public void simpleReadAfterWrite(final FileSystem fs) throws IOException {
private File createDir(String dirPath) throws IOException {  File dir=new File(dirPath);  if (!dir.exists()) {
      try {        InputStream inputStream=this.store.retrieveBlock(this.key,this.readBuffer.getStart(),this.readBuffer.getEnd());        IOUtils.readFully(inputStream,this.readBuffer.getBuffer(),0,readBuffer.getBuffer().length);        inputStream.close();        this.readBuffer.setStatus(CosNInputStream.ReadBuffer.SUCCESS);        break;      } catch (      IOException e) {        this.readBuffer.setStatus(CosNInputStream.ReadBuffer.ERROR);        LOG.warn("Exception occurs when retrieve the block range start: " + String.valueOf(this.readBuffer.getStart()) + " end: "+ this.readBuffer.getEnd());        try {          retryAction=this.retryPolicy.shouldRetry(e,retries++,0,true);          if (retryAction.action == RetryPolicy.RetryAction.RetryDecision.RETRY) {            Thread.sleep(retryAction.delayMillis);          }        } catch (        Exception e1) {          String errMsg=String.format("Exception occurs when retry[%s] " + "to retrieve the block range start: %s, end:%s",this.retryPolicy.toString(),String.valueOf(this.readBuffer.getStart()),String.valueOf(this.readBuffer.getEnd()));
  try {    String userName=System.getProperty("user.name");    String command="id -u " + userName;    if (!getOwnerId) {      command="id -g " + userName;    }    Process child=Runtime.getRuntime().exec(command);    child.waitFor();    InputStream in=child.getInputStream();    StringBuilder strBuffer=new StringBuilder();    int c;    while ((c=in.read()) != -1) {      strBuffer.append((char)c);    }    in.close();    ownerInfoId=strBuffer.toString();  } catch (  IOException|InterruptedException e) {
@Override public boolean delete(Path f,boolean recursive) throws IOException {
    if (!key.endsWith(PATH_DELIMITER)) {      key+=PATH_DELIMITER;    }    if (!recursive && listStatus(f).length > 0) {      String errMsg=String.format("Can not delete the directory: [%s], as" + " it is not empty and option recursive is false.",f);      throw new IOException(errMsg);    }    createParent(f);    String priorLastKey=null;    do {      PartialListing listing=store.list(key,Constants.COS_MAX_LISTING_LENGTH,priorLastKey,true);      for (      FileMetadata file : listing.getFiles()) {        store.delete(file.getKey());      }      for (      FileMetadata commonPrefix : listing.getCommonPrefixes()) {        store.delete(commonPrefix.getKey());      }      priorLastKey=listing.getPriorLastKey();    } while (priorLastKey != null);
    }    if (!recursive && listStatus(f).length > 0) {      String errMsg=String.format("Can not delete the directory: [%s], as" + " it is not empty and option recursive is false.",f);      throw new IOException(errMsg);    }    createParent(f);    String priorLastKey=null;    do {      PartialListing listing=store.list(key,Constants.COS_MAX_LISTING_LENGTH,priorLastKey,true);      for (      FileMetadata file : listing.getFiles()) {        store.delete(file.getKey());      }      for (      FileMetadata commonPrefix : listing.getCommonPrefixes()) {        store.delete(commonPrefix.getKey());      }      priorLastKey=listing.getPriorLastKey();    } while (priorLastKey != null);    try {      store.delete(key);
  }  LOG.debug("Call the getFileStatus to obtain the metadata for " + "the file: [{}].",f);  FileMetadata meta=store.retrieveMetadata(key);  if (meta != null) {    if (meta.isFile()) {      LOG.debug("Path: [{}] is a file. COS key: [{}]",f,key);      return newFile(meta,absolutePath);    } else {      LOG.debug("Path: [{}] is a dir. COS key: [{}]",f,key);      return newDirectory(meta,absolutePath);    }  }  if (!key.endsWith(PATH_DELIMITER)) {    key+=PATH_DELIMITER;  }  LOG.debug("List COS key: [{}] to check the existence of the path.",key);  PartialListing listing=store.list(key,1);  if (listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0) {    if (LOG.isDebugEnabled()) {
  String key=pathToKey(absolutePath);  if (key.length() > 0) {    FileStatus fileStatus=this.getFileStatus(f);    if (fileStatus.isFile()) {      return new FileStatus[]{fileStatus};    }  }  if (!key.endsWith(PATH_DELIMITER)) {    key+=PATH_DELIMITER;  }  URI pathUri=absolutePath.toUri();  Set<FileStatus> status=new TreeSet<>();  String priorLastKey=null;  do {    PartialListing listing=store.list(key,Constants.COS_MAX_LISTING_LENGTH,priorLastKey,false);    for (    FileMetadata fileMetadata : listing.getFiles()) {      Path subPath=keyToPath(fileMetadata.getKey());      if (fileMetadata.getKey().equals(key)) {
  List<Path> paths=new ArrayList<>();  do {    paths.add(absolutePath);    absolutePath=absolutePath.getParent();  } while (absolutePath != null);  for (  Path path : paths) {    if (path.equals(new Path(CosNFileSystem.PATH_DELIMITER))) {      break;    }    try {      FileStatus fileStatus=getFileStatus(path);      if (fileStatus.isFile()) {        throw new FileAlreadyExistsException(String.format("Can't make directory for path: %s, " + "since it is a file.",f));      }      if (fileStatus.isDirectory()) {        break;      }    } catch (    FileNotFoundException e) {
@Override public boolean rename(Path src,Path dst) throws IOException {
    LOG.debug("Recursively find the common parent directory of the source " + "and destination paths. The currently found parent path: {}",dstParentPath);  }  if (null != dstParentPath) {    LOG.debug("It is not allowed to rename a parent directory:[{}] " + "to its subdirectory:[{}].",src,dst);    throw new IOException(String.format("It is not allowed to rename a parent directory: %s " + "to its subdirectory: %s",src,dst));  }  FileStatus dstFileStatus;  try {    dstFileStatus=this.getFileStatus(dst);    if (dstFileStatus.isFile()) {      throw new FileAlreadyExistsException(String.format("File: %s already exists",dstFileStatus.getPath()));    } else {      dst=new Path(dst,src.getName());      FileStatus[] statuses;      try {        statuses=this.listStatus(dst);      } catch (      FileNotFoundException e) {
private void createParent(Path path) throws IOException {  Path parent=path.getParent();  if (parent != null) {    String parentKey=pathToKey(parent);
 else {    PartETag partETag=null;    if (this.blockWritten > 0) {      LOG.info("Upload the last part..., blockId: [{}], written bytes: [{}]",this.currentBlockId,this.blockWritten);      partETag=store.uploadPart(new ByteBufferInputStream(currentBlockBuffer.getByteBuffer()),key,uploadId,currentBlockId + 1,currentBlockBuffer.getByteBuffer().remaining());    }    final List<PartETag> futurePartETagList=this.waitForFinishPartUploads();    if (null == futurePartETagList) {      throw new IOException("Failed to multipart upload to cos, abort it.");    }    List<PartETag> tmpPartEtagList=new LinkedList<>(futurePartETagList);    if (null != partETag) {      tmpPartEtagList.add(partETag);    }    store.completeMultipartUpload(this.key,this.uploadId,tmpPartEtagList);  }  try {    BufferPool.getInstance().returnBuffer(this.currentBlockBuffer);  } catch (  InterruptedException e) {
private void storeFileWithRetry(String key,InputStream inputStream,byte[] md5Hash,long length) throws IOException {  try {    ObjectMetadata objectMetadata=new ObjectMetadata();    objectMetadata.setContentMD5(Base64.encodeAsString(md5Hash));    objectMetadata.setContentLength(length);    PutObjectRequest putObjectRequest=new PutObjectRequest(bucketName,key,inputStream,objectMetadata);    PutObjectResult putObjectResult=(PutObjectResult)callCOSClientWithRetry(putObjectRequest);
@Override public void storeFile(String key,File file,byte[] md5Hash) throws IOException {
@Override public void storeFile(String key,InputStream inputStream,byte[] md5Hash,long contentLength) throws IOException {
public void abortMultipartUpload(String key,String uploadId){
@Override public InputStream retrieve(String key) throws IOException {
@Override public InputStream retrieve(String key,long byteRangeStart) throws IOException {  try {
private PartialListing list(String prefix,String delimiter,int maxListingLength,String priorLastKey) throws IOException {
private PartialListing list(String prefix,String delimiter,int maxListingLength,String priorLastKey) throws IOException {  LOG.debug("List objects. prefix: [{}], delimiter: [{}], " + "maxListLength: [{}], priorLastKey: [{}].",prefix,delimiter,maxListingLength,priorLastKey);  if (!prefix.startsWith(CosNFileSystem.PATH_DELIMITER)) {    prefix+=CosNFileSystem.PATH_DELIMITER;  }  ListObjectsRequest listObjectsRequest=new ListObjectsRequest();  listObjectsRequest.setBucketName(bucketName);  listObjectsRequest.setPrefix(prefix);  listObjectsRequest.setDelimiter(delimiter);  listObjectsRequest.setMarker(priorLastKey);  listObjectsRequest.setMaxKeys(maxListingLength);  ObjectListing objectListing=null;  try {    objectListing=(ObjectListing)callCOSClientWithRetry(listObjectsRequest);  } catch (  Exception e) {    String errMsg=String.format("prefix: [%s], delimiter: [%s], " + "maxListingLength: [%d], priorLastKey: [%s]. " + "List objects occur an exception: [%s].",prefix,(delimiter == null) ? "" : delimiter,maxListingLength,priorLastKey,e.toString());
@Override public void delete(String key) throws IOException {
public void rename(String srcKey,String dstKey) throws IOException {
@Override public void copy(String srcKey,String dstKey) throws IOException {
@Override public long getFileLength(String key) throws IOException {
 else       if (request instanceof ListObjectsRequest) {        sdkMethod="listObjects";        return this.cosClient.listObjects((ListObjectsRequest)request);      } else {        throw new IOException("no such method");      }    } catch (    CosServiceException cse) {      String errMsg=String.format("Call cos sdk failed, " + "retryIndex: [%d / %d], " + "call method: %s, exception: %s",retryIndex,this.maxRetryTimes,sdkMethod,cse.toString());      int statusCode=cse.getStatusCode();      if (statusCode / 100 == 5) {        if (retryIndex <= this.maxRetryTimes) {          LOG.info(errMsg);          long sleepLeast=retryIndex * 300L;          long sleepBound=retryIndex * 500L;          try {            if (request instanceof UploadPartRequest) {
      }    } catch (    CosServiceException cse) {      String errMsg=String.format("Call cos sdk failed, " + "retryIndex: [%d / %d], " + "call method: %s, exception: %s",retryIndex,this.maxRetryTimes,sdkMethod,cse.toString());      int statusCode=cse.getStatusCode();      if (statusCode / 100 == 5) {        if (retryIndex <= this.maxRetryTimes) {          LOG.info(errMsg);          long sleepLeast=retryIndex * 300L;          long sleepBound=retryIndex * 500L;          try {            if (request instanceof UploadPartRequest) {              if (((UploadPartRequest)request).getInputStream() instanceof ByteBufferInputStream) {                ((UploadPartRequest)request).getInputStream().reset();              }            }            Thread.sleep(ThreadLocalRandom.current().nextLong(sleepLeast,sleepBound));            ++retryIndex;          } catch (          InterruptedException e) {
@Test public void testSeek() throws Exception {  Path seekTestFilePath=new Path(this.testRootDir + "/" + "seekTestFile");  long fileSize=5 * Unit.MB;  ContractTestUtils.generateTestFile(this.fs,seekTestFilePath,fileSize,256,255);  LOG.info("5MB file for seek test has created.");  FSDataInputStream inputStream=this.fs.open(seekTestFilePath);  int seekTimes=5;  for (int i=0; i != seekTimes; i++) {    long pos=fileSize / (seekTimes - i) - 1;    inputStream.seek(pos);    assertTrue("expected position at: " + pos + ", but got: "+ inputStream.getPos(),inputStream.getPos() == pos);
  Path seekTestFilePath=new Path(this.testRootDir + "/" + "seekTestFile");  long fileSize=5 * Unit.MB;  ContractTestUtils.generateTestFile(this.fs,seekTestFilePath,fileSize,256,255);  LOG.info("5MB file for seek test has created.");  FSDataInputStream inputStream=this.fs.open(seekTestFilePath);  int seekTimes=5;  for (int i=0; i != seekTimes; i++) {    long pos=fileSize / (seekTimes - i) - 1;    inputStream.seek(pos);    assertTrue("expected position at: " + pos + ", but got: "+ inputStream.getPos(),inputStream.getPos() == pos);    LOG.info("completed seeking at pos: " + inputStream.getPos());  }  LOG.info("begin to random position seeking test...");  Random random=new Random();  for (int i=0; i < seekTimes; i++) {    long pos=Math.abs(random.nextLong()) % fileSize;
  LOG.info("5MB file for seek test has created.");  FSDataInputStream inputStream=this.fs.open(seekTestFilePath);  int seekTimes=5;  for (int i=0; i != seekTimes; i++) {    long pos=fileSize / (seekTimes - i) - 1;    inputStream.seek(pos);    assertTrue("expected position at: " + pos + ", but got: "+ inputStream.getPos(),inputStream.getPos() == pos);    LOG.info("completed seeking at pos: " + inputStream.getPos());  }  LOG.info("begin to random position seeking test...");  Random random=new Random();  for (int i=0; i < seekTimes; i++) {    long pos=Math.abs(random.nextLong()) % fileSize;    LOG.info("seeking for pos: " + pos);    inputStream.seek(pos);    assertTrue("expected position at: " + pos + ", but got: "+ inputStream.getPos(),inputStream.getPos() == pos);
@Test public void testRead() throws Exception {  final int bufLen=256;  Path readTestFilePath=new Path(this.testRootDir + "/" + "testReadSmallFile.txt");  long fileSize=5 * Unit.MB;  ContractTestUtils.generateTestFile(this.fs,readTestFilePath,fileSize,256,255);
  LOG.info("read test file: " + readTestFilePath + " has created.");  FSDataInputStream inputStream=this.fs.open(readTestFilePath);  byte[] buf=new byte[bufLen];  long bytesRead=0;  while (bytesRead < fileSize) {    int bytes=0;    if (fileSize - bytesRead < bufLen) {      int remaining=(int)(fileSize - bytesRead);      bytes=inputStream.read(buf,0,remaining);    } else {      bytes=inputStream.read(buf,0,bufLen);    }    bytesRead+=bytes;    if (bytesRead % (1 * Unit.MB) == 0) {      int available=inputStream.available();      assertTrue("expected remaining: " + (fileSize - bytesRead) + " but got: "+ available,(fileSize - bytesRead) == available);
@Override public void doFilter(ServletRequest request,ServletResponse response,FilterChain filterChain) throws IOException, ServletException {  boolean unauthorizedResponse=true;  int errCode=HttpServletResponse.SC_UNAUTHORIZED;  AuthenticationException authenticationEx=null;  HttpServletRequest httpRequest=(HttpServletRequest)request;  HttpServletResponse httpResponse=(HttpServletResponse)response;  boolean isHttps="https".equals(httpRequest.getScheme());  try {    boolean newToken=false;    AuthenticationToken token;    try {      token=getToken(httpRequest);      if (LOG.isDebugEnabled()) {
  boolean isHttps="https".equals(httpRequest.getScheme());  try {    boolean newToken=false;    AuthenticationToken token;    try {      token=getToken(httpRequest);      if (LOG.isDebugEnabled()) {        LOG.debug("Got token {} from httpRequest {}",token,getRequestURL(httpRequest));      }    } catch (    AuthenticationException ex) {      LOG.warn("AuthenticationToken ignored: " + ex.getMessage());      authenticationEx=ex;      token=null;    }    if (authHandler.managementOperation(token,httpRequest,httpResponse)) {      if (token == null) {        if (LOG.isDebugEnabled()) {
      LOG.warn("AuthenticationToken ignored: " + ex.getMessage());      authenticationEx=ex;      token=null;    }    if (authHandler.managementOperation(token,httpRequest,httpResponse)) {      if (token == null) {        if (LOG.isDebugEnabled()) {          LOG.debug("Request [{}] triggering authentication. handler: {}",getRequestURL(httpRequest),authHandler.getClass());        }        token=authHandler.authenticate(httpRequest,httpResponse);        if (token != null && token != AuthenticationToken.ANONYMOUS) {          if (token.getMaxInactives() > 0) {            token.setMaxInactives(System.currentTimeMillis() + getMaxInactiveInterval() * 1000);          }          if (token.getExpires() != 0) {            token.setExpires(System.currentTimeMillis() + getValidity() * 1000);          }        }        newToken=true;      }      if (token != null) {
      if (token != null) {        unauthorizedResponse=false;        if (LOG.isDebugEnabled()) {          LOG.debug("Request [{}] user [{}] authenticated",getRequestURL(httpRequest),token.getUserName());        }        final AuthenticationToken authToken=token;        httpRequest=new HttpServletRequestWrapper(httpRequest){          @Override public String getAuthType(){            return authToken.getType();          }          @Override public String getRemoteUser(){            return authToken.getUserName();          }          @Override public Principal getUserPrincipal(){            return (authToken != AuthenticationToken.ANONYMOUS) ? authToken : null;          }        };        if (!newToken && !isCookiePersistent() && getMaxInactiveInterval() > 0) {          token.setMaxInactives(System.currentTimeMillis() + getMaxInactiveInterval() * 1000);
@Override public AuthenticationToken alternateAuthenticate(HttpServletRequest request,HttpServletResponse response) throws IOException, AuthenticationException {  AuthenticationToken token=null;  String serializedJWT=null;  HttpServletRequest req=(HttpServletRequest)request;  serializedJWT=getJWTFromCookie(req);  if (serializedJWT == null) {    String loginURL=constructLoginURL(request);
  HttpServletRequest req=(HttpServletRequest)request;  serializedJWT=getJWTFromCookie(req);  if (serializedJWT == null) {    String loginURL=constructLoginURL(request);    LOG.info("sending redirect to: " + loginURL);    ((HttpServletResponse)response).sendRedirect(loginURL);  } else {    String userName=null;    SignedJWT jwtToken=null;    boolean valid=false;    try {      jwtToken=SignedJWT.parse(serializedJWT);      valid=validateToken(jwtToken);      if (valid) {        userName=jwtToken.getJWTClaimsSet().getSubject();
protected String getJWTFromCookie(HttpServletRequest req){  String serializedJWT=null;  Cookie[] cookies=req.getCookies();  if (cookies != null) {    for (    Cookie cookie : cookies) {      if (cookieName.equals(cookie.getName())) {
    if (keytab == null || keytab.trim().length() == 0) {      throw new ServletException("Keytab not defined in configuration");    }    File keytabFile=new File(keytab);    if (!keytabFile.exists()) {      throw new ServletException("Keytab does not exist: " + keytab);    }    final String[] spnegoPrincipals;    if (principal.equals("*")) {      spnegoPrincipals=KerberosUtil.getPrincipalNames(keytab,Pattern.compile("HTTP/.*"));      if (spnegoPrincipals.length == 0) {        throw new ServletException("Principals do not exist in the keytab");      }    } else {      spnegoPrincipals=new String[]{principal};    }    KeyTab keytabInstance=KeyTab.getInstance(keytabFile);    serverSubject.getPrivateCredentials().add(keytabInstance);    for (    String spnegoPrincipal : spnegoPrincipals) {
  env.put(Context.INITIAL_CONTEXT_FACTORY,"com.sun.jndi.ldap.LdapCtxFactory");  env.put(Context.PROVIDER_URL,providerUrl);  try {    ctx=new InitialLdapContext(env,null);    StartTlsResponse tls=(StartTlsResponse)ctx.extendedOperation(new StartTlsRequest());    if (disableHostNameVerification) {      tls.setHostnameVerifier(new HostnameVerifier(){        @Override public boolean verify(        String hostname,        SSLSession session){          return true;        }      });    }    tls.negotiate();    ctx.addToEnvironment(Context.SECURITY_AUTHENTICATION,SECURITY_AUTHENTICATION);    ctx.addToEnvironment(Context.SECURITY_PRINCIPAL,userDN);    ctx.addToEnvironment(Context.SECURITY_CREDENTIALS,password);    ctx.lookup(userDN);
private void authenticateWithoutTlsExtension(String userDN,String password) throws AuthenticationException {  Hashtable<String,Object> env=new Hashtable<String,Object>();  env.put(Context.INITIAL_CONTEXT_FACTORY,"com.sun.jndi.ldap.LdapCtxFactory");  env.put(Context.PROVIDER_URL,providerUrl);  env.put(Context.SECURITY_AUTHENTICATION,SECURITY_AUTHENTICATION);  env.put(Context.SECURITY_PRINCIPAL,userDN);  env.put(Context.SECURITY_CREDENTIALS,password);  try {    Context ctx=new InitialDirContext(env);    ctx.close();
@Override public void init(Properties config) throws ServletException {  for (  Map.Entry prop : config.entrySet()) {
protected AuthenticationHandler initializeAuthHandler(String authHandlerClassName,Properties config) throws ServletException {  try {    Preconditions.checkNotNull(authHandlerClassName);
protected AuthenticationHandler initializeAuthHandler(String authHandlerClassName,Properties config) throws ServletException {  try {    Preconditions.checkNotNull(authHandlerClassName);    logger.debug("Initializing Authentication handler of type " + authHandlerClassName);    Class<?> klass=Thread.currentThread().getContextClassLoader().loadClass(authHandlerClassName);    AuthenticationHandler authHandler=(AuthenticationHandler)klass.newInstance();    authHandler.init(config);
@VisibleForTesting void logDeprecation(String message){
void logDeprecationOnce(String name,String source){  DeprecatedKeyInfo keyInfo=getDeprecatedKeyInfo(name);  if (keyInfo != null && !keyInfo.getAndSetAccessed()) {
public InputStream getConfResourceAsInputStream(String name){  try {    URL url=getResource(name);    if (url == null) {
public Reader getConfResourceAsReader(String name){  try {    URL url=getResource(name);    if (url == null) {
private XMLStreamReader parse(URL url,boolean restricted) throws IOException, XMLStreamException {  if (!quietmode) {    if (LOG.isDebugEnabled()) {
private XMLStreamReader parse(InputStream is,String systemIdStr,boolean restricted) throws IOException, XMLStreamException {  if (!quietmode) {
    XMLStreamReader2 reader=getStreamReader(wrapper,quiet);    if (reader == null) {      if (quiet) {        return null;      }      throw new RuntimeException(resource + " not found");    }    Properties toAddTo=properties;    if (returnCachedProperties) {      toAddTo=new Properties();    }    List<ParsedItem> items=new Parser(reader,wrapper,quiet).parse();    for (    ParsedItem item : items) {      loadProperty(toAddTo,item.name,item.key,item.value,item.isFinal,item.sources);    }    reader.close();    if (returnCachedProperties) {      overlay(properties,toAddTo);      return new Resource(toAddTo,name,wrapper.isParserRestricted());
        return null;      }      throw new RuntimeException(resource + " not found");    }    Properties toAddTo=properties;    if (returnCachedProperties) {      toAddTo=new Properties();    }    List<ParsedItem> items=new Parser(reader,wrapper,quiet).parse();    for (    ParsedItem item : items) {      loadProperty(toAddTo,item.name,item.key,item.value,item.isFinal,item.sources);    }    reader.close();    if (returnCachedProperties) {      overlay(properties,toAddTo);      return new Resource(toAddTo,name,wrapper.isParserRestricted());    }    return null;  } catch (  IOException e) {    LOG.error("error parsing conf " + name,e);
@Override public String get(String name){  String value=super.get(name);
@Override public String get(String name,String defaultValue){  String value=super.get(name,defaultValue);
@Override public boolean getBoolean(String name,boolean defaultValue){  boolean value=super.getBoolean(name,defaultValue);
@Override public float getFloat(String name,float defaultValue){  float value=super.getFloat(name,defaultValue);
@Override public int getInt(String name,int defaultValue){  int value=super.getInt(name,defaultValue);
@Override public long getLong(String name,long defaultValue){  long value=super.getLong(name,defaultValue);
@Override public void set(String name,String value,String source){
@Override public final void reconfigureProperty(String property,String newVal) throws ReconfigurationException {  if (isPropertyReconfigurable(property)) {
private Reconfigurable getReconfigurable(HttpServletRequest req){
private Reconfigurable getReconfigurable(HttpServletRequest req){  LOG.info("servlet path: " + req.getServletPath());
      String rawParam=params.nextElement();      String param=StringEscapeUtils.unescapeHtml4(rawParam);      String value=StringEscapeUtils.unescapeHtml4(req.getParameter(rawParam));      if (value != null) {        if (value.equals(newConf.getRaw(param)) || value.equals("default") || value.equals("null")|| value.isEmpty()) {          if ((value.equals("default") || value.equals("null") || value.isEmpty()) && oldConf.getRaw(param) != null) {            out.println("<p>Changed \"" + StringEscapeUtils.escapeHtml4(param) + "\" from \""+ StringEscapeUtils.escapeHtml4(oldConf.getRaw(param))+ "\" to default</p>");            reconf.reconfigureProperty(param,null);          } else           if (!value.equals("default") && !value.equals("null") && !value.isEmpty()&& (oldConf.getRaw(param) == null || !oldConf.getRaw(param).equals(value))) {            if (oldConf.getRaw(param) == null) {              out.println("<p>Changed \"" + StringEscapeUtils.escapeHtml4(param) + "\" from default to \""+ StringEscapeUtils.escapeHtml4(value)+ "\"</p>");            } else {              out.println("<p>Changed \"" + StringEscapeUtils.escapeHtml4(param) + "\" from \""+ StringEscapeUtils.escapeHtml4(oldConf.getRaw(param))+ "\" to \""+ StringEscapeUtils.escapeHtml4(value)+ "\"</p>");            }            reconf.reconfigureProperty(param,value);          } else {
@Override public void setConf(Configuration conf){  this.conf=conf;  final Class<? extends Random> klass=conf.getClass(HADOOP_SECURITY_SECURE_RANDOM_IMPL_KEY,OpensslSecureRandom.class,Random.class);  try {    random=ReflectionUtils.newInstance(klass,conf);    if (LOG.isDebugEnabled()) {
private FsPermission loadAndReturnPerm(Path pathToLoad,Path pathToDelete) throws NoSuchAlgorithmException, CertificateException, IOException {  FsPermission perm=null;  try {    perm=loadFromPath(pathToLoad,password);    renameOrFail(pathToLoad,path);    if (LOG.isDebugEnabled()) {
private <T>T call(HttpURLConnection conn,Object jsonOutput,int expectedResponse,Class<T> klass,int authRetryCount) throws IOException {  T ret=null;  OutputStream os=null;  try {    if (jsonOutput != null) {      os=conn.getOutputStream();      writeJson(jsonOutput,os);    }  } catch (  IOException ex) {    if (os == null) {      conn.disconnect();    } else {      IOUtils.closeStream(conn.getInputStream());    }    throw ex;  }  if ((conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN && (conn.getResponseMessage().equals(ANONYMOUS_REQUESTS_DISALLOWED) || conn.getResponseMessage().contains(INVALID_SIGNATURE))) || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {    if (LOG.isDebugEnabled()) {
@VisibleForTesting DelegationTokenAuthenticatedURL createAuthenticatedURL(){  return new DelegationTokenAuthenticatedURL(configurator){    @Override public org.apache.hadoop.security.token.Token<? extends TokenIdentifier> selectDelegationToken(    URL url,    Credentials creds){      if (LOG.isDebugEnabled()) {
protected static Token<?> selectDelegationToken(Credentials creds,Text service){  Token<?> token=creds.getToken(service);
@Override public Token<?> getDelegationToken(final String renewer) throws IOException {  final URL url=createURL(null,null,null,null);  final DelegationTokenAuthenticatedURL authUrl=new DelegationTokenAuthenticatedURL(configurator);  Token<?> token=null;  try {    final String doAsUser=getDoAsUser();    token=getActualUgi().doAs(new PrivilegedExceptionAction<Token<?>>(){      @Override public Token<?> run() throws Exception {
@Override public long renewDelegationToken(final Token<?> dToken) throws IOException {  try {    final String doAsUser=getDoAsUser();    final DelegationTokenAuthenticatedURL.Token token=generateDelegationToken(dToken);    final URL url=createURL(null,null,null,null);
@Override public Void cancelDelegationToken(final Token<?> dToken) throws IOException {  try {    final String doAsUser=getDoAsUser();    final DelegationTokenAuthenticatedURL.Token token=generateDelegationToken(dToken);    return getActualUgi().doAs(new PrivilegedExceptionAction<Void>(){      @Override public Void run() throws Exception {        final URL url=createURL(null,null,null,null);
private boolean containsKmsDt(UserGroupInformation ugi) throws IOException {  Credentials creds=ugi.getCredentials();  if (!creds.getAllTokens().isEmpty()) {
@Override public Token<?> getDelegationToken(String renewer) throws IOException {  return doOp(new ProviderCallable<Token<?>>(){    @Override public Token<?> call(    KMSClientProvider provider) throws IOException {      Token<?> token=provider.getDelegationToken(renewer);      token.setService(dtService);
private int readChecksumChunk(byte b[],final int off,final int len) throws IOException {  count=pos=0;  int read=0;  boolean retry=true;  int retriesLeft=numOfRetries;  do {    retriesLeft--;    try {      read=readChunk(chunkPos,b,off,len,checksum);      if (read > 0) {        if (needChecksum()) {          verifySums(b,off,read);        }        chunkPos+=read;      }      retry=false;    } catch (    ChecksumException ce) {
private static void loadFileSystems(){  LOGGER.debug("Loading filesystems");synchronized (FileSystem.class) {    if (!FILE_SYSTEMS_LOADED) {      ServiceLoader<FileSystem> serviceLoader=ServiceLoader.load(FileSystem.class);      Iterator<FileSystem> it=serviceLoader.iterator();      while (it.hasNext()) {        FileSystem fs;        try {          fs=it.next();          try {            SERVICE_FILE_SYSTEMS.put(fs.getScheme(),fs.getClass());            if (LOGGER.isDebugEnabled()) {
  LOGGER.debug("Loading filesystems");synchronized (FileSystem.class) {    if (!FILE_SYSTEMS_LOADED) {      ServiceLoader<FileSystem> serviceLoader=ServiceLoader.load(FileSystem.class);      Iterator<FileSystem> it=serviceLoader.iterator();      while (it.hasNext()) {        FileSystem fs;        try {          fs=it.next();          try {            SERVICE_FILE_SYSTEMS.put(fs.getScheme(),fs.getClass());            if (LOGGER.isDebugEnabled()) {              LOGGER.debug("{}:// = {} from {}",fs.getScheme(),fs.getClass(),ClassUtil.findContainingJar(fs.getClass()));            }          } catch (          Exception e) {            LOGGER.warn("Cannot load: {} from {}",fs,ClassUtil.findContainingJar(fs.getClass()));
public static Class<? extends FileSystem> getFileSystemClass(String scheme,Configuration conf) throws IOException {  if (!FILE_SYSTEMS_LOADED) {    loadFileSystems();  }  LOGGER.debug("Looking for FS supporting {}",scheme);  Class<? extends FileSystem> clazz=null;  if (conf != null) {    String property="fs." + scheme + ".impl";    LOGGER.debug("looking for configuration option {}",property);    clazz=(Class<? extends FileSystem>)conf.getClass(property,null);  } else {    LOGGER.debug("No configuration: skipping check for fs.{}.impl",scheme);  }  if (clazz == null) {    LOGGER.debug("Looking in service filesystems for implementation class");    clazz=SERVICE_FILE_SYSTEMS.get(scheme);  } else {
  LOGGER.debug("Looking for FS supporting {}",scheme);  Class<? extends FileSystem> clazz=null;  if (conf != null) {    String property="fs." + scheme + ".impl";    LOGGER.debug("looking for configuration option {}",property);    clazz=(Class<? extends FileSystem>)conf.getClass(property,null);  } else {    LOGGER.debug("No configuration: skipping check for fs.{}.impl",scheme);  }  if (clazz == null) {    LOGGER.debug("Looking in service filesystems for implementation class");    clazz=SERVICE_FILE_SYSTEMS.get(scheme);  } else {    LOGGER.debug("Filesystem {} defined in configuration option",scheme);  }  if (clazz == null) {    throw new UnsupportedFileSystemException("No FileSystem for scheme " + "\"" + scheme + "\"");
private static void runCommandOnStream(InputStream inputStream,String command) throws IOException, InterruptedException, ExecutionException {  ExecutorService executor=null;  ProcessBuilder builder=new ProcessBuilder();  builder.command(Shell.WINDOWS ? "cmd" : "bash",Shell.WINDOWS ? "/c" : "-c",command);  Process process=builder.start();  int exitCode;  try {    executor=Executors.newFixedThreadPool(2);    Future output=executor.submit(() -> {      try {        if (LOG.isDebugEnabled()) {          try (BufferedReader reader=new BufferedReader(new InputStreamReader(process.getInputStream(),Charset.forName("UTF-8")))){            String line;            while ((line=reader.readLine()) != null) {
  builder.command(Shell.WINDOWS ? "cmd" : "bash",Shell.WINDOWS ? "/c" : "-c",command);  Process process=builder.start();  int exitCode;  try {    executor=Executors.newFixedThreadPool(2);    Future output=executor.submit(() -> {      try {        if (LOG.isDebugEnabled()) {          try (BufferedReader reader=new BufferedReader(new InputStreamReader(process.getInputStream(),Charset.forName("UTF-8")))){            String line;            while ((line=reader.readLine()) != null) {              LOG.debug(line);            }          }         } else {          org.apache.commons.io.IOUtils.copy(process.getInputStream(),new IOUtils.NullOutputStream());        }      } catch (      IOException e) {
          try (BufferedReader reader=new BufferedReader(new InputStreamReader(process.getInputStream(),Charset.forName("UTF-8")))){            String line;            while ((line=reader.readLine()) != null) {              LOG.debug(line);            }          }         } else {          org.apache.commons.io.IOUtils.copy(process.getInputStream(),new IOUtils.NullOutputStream());        }      } catch (      IOException e) {        LOG.debug(e.getMessage());      }    });    Future error=executor.submit(() -> {      try {        if (LOG.isDebugEnabled()) {          try (BufferedReader reader=new BufferedReader(new InputStreamReader(process.getErrorStream(),Charset.forName("UTF-8")))){            String line;            while ((line=reader.readLine()) != null) {
            }          }         } else {          org.apache.commons.io.IOUtils.copy(process.getInputStream(),new IOUtils.NullOutputStream());        }      } catch (      IOException e) {        LOG.debug(e.getMessage());      }    });    Future error=executor.submit(() -> {      try {        if (LOG.isDebugEnabled()) {          try (BufferedReader reader=new BufferedReader(new InputStreamReader(process.getErrorStream(),Charset.forName("UTF-8")))){            String line;            while ((line=reader.readLine()) != null) {              LOG.debug(line);            }          }         } else {          org.apache.commons.io.IOUtils.copy(process.getErrorStream(),new IOUtils.NullOutputStream());        }      } catch (      IOException e) {
  ShellCommandExecutor shExec;  try {    if (Shell.WINDOWS && linkFile.getParentFile() != null && !new Path(target).isAbsolute()) {      shExec=new ShellCommandExecutor(cmd,linkFile.getParentFile());    } else {      shExec=new ShellCommandExecutor(cmd);    }    shExec.execute();  } catch (  Shell.ExitCodeException ec) {    int returnVal=ec.getExitCode();    if (Shell.WINDOWS && returnVal == SYMLINK_NO_PRIVILEGE) {      LOG.warn("Fail to create symbolic links on Windows. " + "The default security settings in Windows disallow non-elevated " + "administrators and all non-administrators from creating symbolic links. "+ "This behavior can be changed in the Local Security Policy management console");    } else     if (returnVal != 0) {      LOG.warn("Command '" + StringUtils.join(" ",cmd) + "' failed "+ returnVal+ " with: "+ ec.getMessage());    }    return returnVal;  }catch (  IOException e) {
  URI srcUri=srcFs.getUri();  URI dstUri=destFs.getUri();  if (srcUri.getScheme() == null) {    return false;  }  if (!srcUri.getScheme().equals(dstUri.getScheme())) {    return false;  }  String srcHost=srcUri.getHost();  String dstHost=dstUri.getHost();  if ((srcHost != null) && (dstHost != null)) {    if (srcHost.equals(dstHost)) {      return srcUri.getPort() == dstUri.getPort();    }    try {      srcHost=InetAddress.getByName(srcHost).getCanonicalHostName();      dstHost=InetAddress.getByName(dstHost).getCanonicalHostName();    } catch (    UnknownHostException ue) {
        if (args.length() > 2048) {          args=args.substring(0,2048);        }        scope.getSpan().addKVAnnotation("args",args);      }      try {        exitCode=instance.run(Arrays.copyOfRange(argv,1,argv.length));      }  finally {        scope.close();      }    } catch (    IllegalArgumentException e) {      if (e.getMessage() == null) {        displayError(cmd,"Null exception message");        e.printStackTrace(System.err);      } else {        displayError(cmd,e.getLocalizedMessage());      }      printUsage(System.err);      if (instance != null) {
@Override public java.net.URLStreamHandler createURLStreamHandler(String protocol){
@Override public java.net.URLStreamHandler createURLStreamHandler(String protocol){  LOG.debug("Creating handler for protocol {}",protocol);  if (!protocols.containsKey(protocol)) {    boolean known=true;    try {      Class<? extends FileSystem> impl=FileSystem.getFileSystemClass(protocol,conf);
private FileStatus[] doGlob() throws IOException {  String scheme=schemeFromPath(pathPattern);  String authority=authorityFromPath(pathPattern);  String pathPatternString=pathPattern.toUri().getPath();  List<String> flattenedPatterns=GlobExpander.expand(pathPatternString);
private FileStatus[] doGlob() throws IOException {  String scheme=schemeFromPath(pathPattern);  String authority=authorityFromPath(pathPattern);  String pathPatternString=pathPattern.toUri().getPath();  List<String> flattenedPatterns=GlobExpander.expand(pathPatternString);  LOG.debug("Filesystem glob {}",pathPatternString);  ArrayList<FileStatus> results=new ArrayList<>(flattenedPatterns.size());  boolean sawWildcard=false;  for (  String flatPattern : flattenedPatterns) {    Path absPattern=fixRelativePart(new Path(flatPattern.isEmpty() ? Path.CUR_DIR : flatPattern));
      ArrayList<FileStatus> newCandidates=new ArrayList<>(candidates.size());      GlobFilter globFilter=new GlobFilter(components.get(componentIdx));      String component=unescapePathComponent(components.get(componentIdx));      if (globFilter.hasPattern()) {        sawWildcard=true;      }      LOG.debug("Component {}, patterned={}",component,sawWildcard);      if (candidates.isEmpty() && sawWildcard) {        break;      }      if ((componentIdx < components.size() - 1) && (!globFilter.hasPattern())) {        for (        FileStatus candidate : candidates) {          candidate.setPath(new Path(candidate.getPath(),component));        }        continue;      }      for (      FileStatus candidate : candidates) {        if (globFilter.hasPattern()) {          FileStatus[] children=listStatus(candidate.getPath());
      if (candidates.isEmpty() && sawWildcard) {        break;      }      if ((componentIdx < components.size() - 1) && (!globFilter.hasPattern())) {        for (        FileStatus candidate : candidates) {          candidate.setPath(new Path(candidate.getPath(),component));        }        continue;      }      for (      FileStatus candidate : candidates) {        if (globFilter.hasPattern()) {          FileStatus[] children=listStatus(candidate.getPath());          if (children.length == 1) {            if (resolveSymlinks) {              LOG.debug("listStatus found one entry; disambiguating {}",children[0]);              Path path=candidate.getPath();              FileStatus status=getFileStatus(path);              if (status == null) {
@VisibleForTesting public final boolean handleEmptyDstDirectoryOnWindows(Path src,File srcFile,Path dst,File dstFile) throws IOException {  try {    FileStatus sdst=this.getFileStatus(dst);    String[] dstFileList=dstFile.list();    if (dstFileList != null) {      if (sdst.isDirectory() && dstFileList.length == 0) {        if (LOG.isDebugEnabled()) {
      }    } catch (    FileAlreadyExistsException e) {      Path existsFilePath=baseTrashPath;      while (!fs.exists(existsFilePath)) {        existsFilePath=existsFilePath.getParent();      }      baseTrashPath=new Path(baseTrashPath.toString().replace(existsFilePath.toString(),existsFilePath.toString() + Time.now()));      trashPath=new Path(baseTrashPath,trashPath.getName());      --i;      continue;    }catch (    IOException e) {      LOG.warn("Can't create trash directory: " + baseTrashPath,e);      cause=e;      break;    }    try {      String orig=trashPath.toString();      while (fs.exists(trashPath)) {
@SuppressWarnings("deprecation") public void createCheckpoint(Date date) throws IOException {  Collection<FileStatus> trashRoots=fs.getTrashRoots(false);  for (  FileStatus trashRoot : trashRoots) {
private void deleteCheckpoint(boolean deleteImmediately) throws IOException {  Collection<FileStatus> trashRoots=fs.getTrashRoots(false);  for (  FileStatus trashRoot : trashRoots) {
private void deleteCheckpoint(Path trashRoot,boolean deleteImmediately) throws IOException {
    return;  }  long now=Time.now();  for (int i=0; i < dirs.length; i++) {    Path path=dirs[i].getPath();    String dir=path.toUri().getPath();    String name=path.getName();    if (name.equals(CURRENT.getName())) {      continue;    }    long time;    try {      time=getTimeFromCheckpoint(name);    } catch (    ParseException e) {      LOG.warn("Unexpected item in trash: " + dir + ". Ignoring.");      continue;    }    if (((now - deletionInterval) > time) || deleteImmediately) {
        continue;      }      int curVersion=higherVersion;      try {        curVersion=Integer.parseInt(nameParts[nameParts.length - 2]);      } catch (      NumberFormatException nfe) {        logInvalidFileNameFormat(cur);        continue;      }      if (curVersion > higherVersion) {        higherVersion=curVersion;        lfs=curLfs;      }    }    if (lfs == null) {      LOGGER.warn("No valid mount-table file exist at: {}. At least one " + "mount-table file should present with the name format: " + "mount-table.<versionNumber>.xml",mountTableConfigPath);      return;    }    Path latestVersionMountTable=lfs.getPath();    if (LOGGER.isDebugEnabled()) {
    }    for (    final MRNflyNode dstNode : mrNodes) {      if (dstNode.status == null || srcNode.compareTo(dstNode) == 0) {        continue;      }      try {        final FileStatus srcStatus=srcNode.cloneStatus();        srcStatus.setPath(f);        final Path tmpPath=getNflyTmpPath(f);        FileUtil.copy(srcNode.getFs(),srcStatus,dstNode.getFs(),tmpPath,false,true,getConf());        dstNode.getFs().delete(f,false);        if (dstNode.getFs().rename(tmpPath,f)) {          try {            dstNode.getFs().setTimes(f,srcNode.status.getModificationTime(),srcNode.status.getAccessTime());          }  finally {            srcStatus.setPath(dstNode.getFs().makeQualified(f));            dstNode.status=srcStatus;
            dstNode.getFs().setTimes(f,srcNode.status.getModificationTime(),srcNode.status.getAccessTime());          }  finally {            srcStatus.setPath(dstNode.getFs().makeQualified(f));            dstNode.status=srcStatus;          }        }      } catch (      IOException ioe) {        LOG.info(f + " " + srcNode+ "->"+ dstNode+ ": Failed to repair",ioe);      }    }  }  if (maxMtime > 0) {    final List<MRNflyNode> mrList=new ArrayList<MRNflyNode>();    for (    final MRNflyNode openNode : mrNodes) {      if (openNode.status != null && openNode.status.getLen() >= 0L) {        if (openNode.status.getModificationTime() == maxMtime) {          mrList.add(openNode);        }      }    }    final MRNflyNode[] readNodes=mrList.toArray(new MRNflyNode[0]);    topology.sortByDistance(myNode,readNodes,readNodes.length);    for (    final MRNflyNode rNode : readNodes) {
@Override public void initialize(URI theUri,Configuration conf) throws IOException {  this.myUri=theUri;  if (LOG.isDebugEnabled()) {
@Override public synchronized void processResult(int rc,String path,Object ctx,String name){  if (isStaleClient(ctx))   return;  if (LOG.isDebugEnabled()) {
  if (LOG.isDebugEnabled()) {    LOG.debug("CreateNode result: " + rc + " for path: "+ path+ " connectionState: "+ zkConnectionState+ "  for "+ this);  }  Code code=Code.get(rc);  if (isSuccess(code)) {    if (becomeActive()) {      monitorActiveStatus();    } else {      reJoinElectionAfterFailureToBecomeActive();    }    return;  }  if (isNodeExists(code)) {    if (createRetryCount == 0) {      becomeStandby();    }    monitorActiveStatus();    return;  }  String errorMessage="Received create error from Zookeeper. code:" + code.toString() + " for path "+ path;
  Code code=Code.get(rc);  if (isSuccess(code)) {    if (becomeActive()) {      monitorActiveStatus();    } else {      reJoinElectionAfterFailureToBecomeActive();    }    return;  }  if (isNodeExists(code)) {    if (createRetryCount == 0) {      becomeStandby();    }    monitorActiveStatus();    return;  }  String errorMessage="Received create error from Zookeeper. code:" + code.toString() + " for path "+ path;  LOG.debug(errorMessage);  if (shouldRetry(code)) {
synchronized void processWatchEvent(ZooKeeper zk,WatchedEvent event){  Event.EventType eventType=event.getType();  if (isStaleClient(zk))   return;  if (LOG.isDebugEnabled()) {
reJoinElection(0);break;case SaslAuthenticated:LOG.info("Successfully authenticated to ZooKeeper using SASL.");break;default:fatalError("Unexpected Zookeeper watch event state: " + event.getState());break;}return;}String path=event.getPath();if (path != null) {switch (eventType) {case NodeDeleted:if (state == State.ACTIVE) {enterNeutralMode();}joinElectionInternal();break;case NodeDataChanged:monitorActiveStatus();
private void fatalError(String errorMessage){
  boolean tryFence=true;  if (tryGracefulFence(fromSvc)) {    tryFence=forceFence;  }  if (tryFence) {    if (!fromSvc.getFencer().fence(fromSvc,toSvc)) {      throw new FailoverFailedException("Unable to fence " + fromSvc + ". Fencing failed.");    }  }  boolean failed=false;  Throwable cause=null;  try {    HAServiceProtocolHelper.transitionToActive(toSvc.getProxy(conf,rpcTimeoutToNewActive),createReqInfo());  } catch (  ServiceFailedException sfe) {    LOG.error("Unable to make {} active ({}). Failing back.",toSvc,sfe.getMessage());    failed=true;    cause=sfe;  }catch (  IOException ioe) {
  Throwable cause=null;  try {    HAServiceProtocolHelper.transitionToActive(toSvc.getProxy(conf,rpcTimeoutToNewActive),createReqInfo());  } catch (  ServiceFailedException sfe) {    LOG.error("Unable to make {} active ({}). Failing back.",toSvc,sfe.getMessage());    failed=true;    cause=sfe;  }catch (  IOException ioe) {    LOG.error("Unable to make {} active (unable to connect). Failing back.",toSvc,ioe);    failed=true;    cause=ioe;  }  if (failed) {    String msg="Unable to failover to " + toSvc;    if (!tryFence) {      try {
private synchronized void enterState(State newState){  if (newState != state) {
  LOG.info("====== Beginning Service Fencing Process... ======");  int i=0;  for (  FenceMethodWithArg method : methods) {    LOG.info("Trying method " + (++i) + "/"+ methods.size()+ ": "+ method);    try {      boolean toSvcFencingFailed=false;      if (toSvc != null) {        toSvcFencingFailed=!method.method.tryFence(toSvc,method.arg);      }      if (toSvcFencingFailed) {        LOG.error("====== Fencing on target failed, skipping fencing " + "on source ======");      } else {        if (method.method.tryFence(fromSvc,method.arg)) {          LOG.info("====== Fencing successful by method " + method + " ======");          return true;        }      }    } catch (    BadFencingConfigurationException e) {
    LOG.info("Trying method " + (++i) + "/"+ methods.size()+ ": "+ method);    try {      boolean toSvcFencingFailed=false;      if (toSvc != null) {        toSvcFencingFailed=!method.method.tryFence(toSvc,method.arg);      }      if (toSvcFencingFailed) {        LOG.error("====== Fencing on target failed, skipping fencing " + "on source ======");      } else {        if (method.method.tryFence(fromSvc,method.arg)) {          LOG.info("====== Fencing successful by method " + method + " ======");          return true;        }      }    } catch (    BadFencingConfigurationException e) {      LOG.error("Fencing method " + method + " misconfigured",e);      continue;    }catch (    Throwable t) {
@Override public void checkArgs(String argStr) throws BadFencingConfigurationException {
private String buildPSScript(final String processName,final String host){
private String buildPSScript(final String processName,final String host){  LOG.info("Building PowerShell script to kill " + processName + " at "+ host);  String ps1script=null;  BufferedWriter writer=null;  try {    File file=File.createTempFile("temp-fence-command",".ps1");    file.deleteOnExit();    FileOutputStream fos=new FileOutputStream(file,false);    OutputStreamWriter osw=new OutputStreamWriter(fos,StandardCharsets.UTF_8);    writer=new BufferedWriter(osw);    String filter=StringUtils.join(" and ",new String[]{"Name LIKE '%java.exe%'","CommandLine LIKE '%" + processName + "%'"});    String cmd="Get-WmiObject Win32_Process";    cmd+=" -Filter \"" + filter + "\"";    cmd+=" -Computer " + host;    cmd+=" |% { $_.Terminate() }";
    File file=File.createTempFile("temp-fence-command",".ps1");    file.deleteOnExit();    FileOutputStream fos=new FileOutputStream(file,false);    OutputStreamWriter osw=new OutputStreamWriter(fos,StandardCharsets.UTF_8);    writer=new BufferedWriter(osw);    String filter=StringUtils.join(" and ",new String[]{"Name LIKE '%java.exe%'","CommandLine LIKE '%" + processName + "%'"});    String cmd="Get-WmiObject Win32_Process";    cmd+=" -Filter \"" + filter + "\"";    cmd+=" -Computer " + host;    cmd+=" |% { $_.Terminate() }";    LOG.info("PowerShell command: " + cmd);    writer.write(cmd);    writer.flush();    ps1script=file.getAbsolutePath();  } catch (  IOException ioe) {
    String cmd="Get-WmiObject Win32_Process";    cmd+=" -Filter \"" + filter + "\"";    cmd+=" -Computer " + host;    cmd+=" |% { $_.Terminate() }";    LOG.info("PowerShell command: " + cmd);    writer.write(cmd);    writer.flush();    ps1script=file.getAbsolutePath();  } catch (  IOException ioe) {    LOG.error("Cannot create PowerShell script",ioe);  } finally {    if (writer != null) {      try {        writer.close();      } catch (      IOException ioe) {
  String cmd=parseArgs(target.getTransitionTargetHAStatus(),args);  if (!Shell.WINDOWS) {    builder=new ProcessBuilder("bash","-e","-c",cmd);  } else {    builder=new ProcessBuilder("cmd.exe","/c",cmd);  }  setConfAsEnvVars(builder.environment());  addTargetInfoAsEnvVars(target,builder.environment());  Process p;  try {    p=builder.start();    p.getOutputStream().close();  } catch (  IOException e) {    LOG.warn("Unable to execute " + cmd,e);    return false;  }  String pid=tryGetPid(p);
private boolean doFence(Session session,InetSocketAddress serviceAddr) throws JSchException {  int port=serviceAddr.getPort();  try {
private boolean doFence(Session session,InetSocketAddress serviceAddr) throws JSchException {  int port=serviceAddr.getPort();  try {    LOG.info("Looking for process running on port " + port);    int rc=execCommand(session,"PATH=$PATH:/sbin:/usr/sbin fuser -v -k -n tcp " + port);    if (rc == 0) {
private int execCommand(Session session,String cmd) throws JSchException, InterruptedException, IOException {
protected void pump() throws IOException {  InputStreamReader inputStreamReader=new InputStreamReader(stream,StandardCharsets.UTF_8);  BufferedReader br=new BufferedReader(inputStreamReader);  String line=null;  while ((line=br.readLine()) != null) {    if (type == StreamType.STDOUT) {
public int run(final String[] args) throws Exception {  if (!localTarget.isAutoFailoverEnabled()) {
            force=true;          } else           if ("-nonInteractive".equals(args[i])) {            interactive=false;          } else {            badArg(args[i]);          }        }        return formatZK(force,interactive);      } else {        badArg(args[0]);      }    }  } catch (  Exception e) {    LOG.error("The failover controller encounters runtime error",e);    throw e;  }  if (!elector.parentZNodeExists()) {    LOG.error("Unable to start failover controller. " + "Parent znode does not exist.\n" + "Run with -formatZK flag to initialize ZooKeeper.");    return ERR_CODE_NO_PARENT_ZNODE;  }  try {
        }        return formatZK(force,interactive);      } else {        badArg(args[0]);      }    }  } catch (  Exception e) {    LOG.error("The failover controller encounters runtime error",e);    throw e;  }  if (!elector.parentZNodeExists()) {    LOG.error("Unable to start failover controller. " + "Parent znode does not exist.\n" + "Run with -formatZK flag to initialize ZooKeeper.");    return ERR_CODE_NO_PARENT_ZNODE;  }  try {    localTarget.checkFencingConfigured();  } catch (  BadFencingConfigurationException e) {    LOG.error("Fencing is not configured for " + localTarget + ".\n"+ "You must configure a fencing method before using automatic "+ "failover.",e);    return ERR_CODE_NO_FENCER;  }  try {
protected void initRPC() throws IOException {  InetSocketAddress bindAddr=getRpcAddressToBindTo();
private synchronized void fatalError(String err){
private synchronized void becomeActive() throws ServiceFailedException {  LOG.info("Trying to make " + localTarget + " active...");  try {    HAServiceProtocolHelper.transitionToActive(localTarget.getProxy(conf,FailoverController.getRpcTimeoutToNewActive(conf)),createReqInfo());    String msg="Successfully transitioned " + localTarget + " to active state";
private synchronized void becomeStandby(){  LOG.info("ZK Election indicated that " + localTarget + " should become standby");  try {    int timeout=FailoverController.getGracefulFenceTimeout(conf);    localTarget.getProxy(conf,timeout).transitionToStandby(createReqInfo());
private void doFence(HAServiceTarget target){
private void doFence(HAServiceTarget target){  LOG.info("Should fence: " + target);  boolean gracefulWorked=new FailoverController(conf,RequestSource.REQUEST_BY_ZKFC).tryGracefulFence(target);  if (gracefulWorked) {
private ZKFCProtocol cedeRemoteActive(HAServiceTarget remote,int timeout) throws IOException {
protected synchronized void setLastHealthState(HealthMonitor.State newState){
  ;  try {    isLog4JLogger=logger instanceof Log4JLogger;  } catch (  NoClassDefFoundError err) {    LOG.debug("Could not load Log4JLogger class",err);    isLog4JLogger=false;  }  if (isLog4JLogger) {    Log4JLogger httpLog4JLog=(Log4JLogger)logger;    org.apache.log4j.Logger httpLogger=httpLog4JLog.getLogger();    Appender appender=null;    try {      appender=httpLogger.getAppender(appenderName);    } catch (    LogConfigurationException e) {      LOG.warn("Http request log for {} could not be created",loggerName);      throw e;
public void addJerseyResourcePackage(final String packageName,final String pathSpec,Map<String,String> params){
public void addInternalServlet(String name,String pathSpec,Class<? extends HttpServlet> clazz,Map<String,String> params){  final ServletHolder sh=new ServletHolder(clazz);  sh.setName(name);  sh.setInitParameters(params);  final ServletMapping[] servletMappings=webAppContext.getServletHandler().getServletMappings();  for (int i=0; i < servletMappings.length; i++) {    if (servletMappings[i].containsPathSpec(pathSpec)) {      if (LOG.isDebugEnabled()) {
@Override public void addFilter(String name,String classname,Map<String,String> parameters){  FilterHolder filterHolder=getFilterHolder(name,classname,parameters);  final String[] userFacingUrls={"/","/*"};  FilterMapping fmap=getFilterMapping(name,userFacingUrls);  defineFilter(webAppContext,filterHolder,fmap);
@Override public void addFilter(String name,String classname,Map<String,String> parameters){  FilterHolder filterHolder=getFilterHolder(name,classname,parameters);  final String[] userFacingUrls={"/","/*"};  FilterMapping fmap=getFilterMapping(name,userFacingUrls);  defineFilter(webAppContext,filterHolder,fmap);  LOG.info("Added filter " + name + " (class="+ classname+ ") to context "+ webAppContext.getDisplayName());  final String[] ALL_URLS={"/*"};  fmap=getFilterMapping(name,ALL_URLS);  for (  Map.Entry<ServletContextHandler,Boolean> e : defaultContexts.entrySet()) {    if (e.getValue()) {      ServletContextHandler ctx=e.getKey();      defineFilter(ctx,filterHolder,fmap);
private static void bindListener(ServerConnector listener) throws Exception {  listener.close();  listener.open();
    try {      c.close();    } catch (    Exception e) {      LOG.error("Error while stopping listener for webapp" + webAppContext.getDisplayName(),e);      exception=addMultiException(exception,e);    }  }  try {    secretProvider.destroy();    webAppContext.clearAttributes();    webAppContext.stop();  } catch (  Exception e) {    LOG.error("Error while stopping web app context for webapp " + webAppContext.getDisplayName(),e);    exception=addMultiException(exception,e);  }  try {    webServer.stop();  } catch (  Exception e) {
public static Compressor getCompressor(CompressionCodec codec,Configuration conf){  Compressor compressor=borrow(compressorPool,codec.getCompressorType());  if (compressor == null) {    compressor=codec.createCompressor();
public static Decompressor getDecompressor(CompressionCodec codec){  Decompressor decompressor=borrow(decompressorPool,codec.getDecompressorType());  if (decompressor == null) {    decompressor=codec.createDecompressor();
@VisibleForTesting void updateCoders(Iterable<RawErasureCoderFactory> coderFactories){  for (  RawErasureCoderFactory coderFactory : coderFactories) {    String codecName=coderFactory.getCodecName();    List<RawErasureCoderFactory> coders=coderMap.get(codecName);    if (coders == null) {      coders=new ArrayList<>();      coders.add(coderFactory);      coderMap.put(codecName,coders);
@Override public synchronized boolean isSupported(){  if (!checked || reinitCodecInTests) {    checked=true;    reinitCodecInTests=conf.getBoolean("test.reload.lzo.codec",false);    clazz=getLzoCodecClass();    try {
public void returnCompressor(Compressor compressor){  if (compressor != null) {    if (LOG.isDebugEnabled()) {
public void returnDecompressor(Decompressor decompressor){  if (decompressor != null) {    if (LOG.isDebugEnabled()) {
@Override protected Object invokeMethod(Method method,Object[] args) throws Throwable {  Object result=super.invokeMethod(method,args);  int retryCount=RetryCount.get();  if (retryCount < this.numToDrop) {    RetryCount.set(++retryCount);    if (LOG.isDebugEnabled()) {
private RetryInfo handleException(final Method method,final int callId,final RetryPolicy policy,final Counters counters,final long expectFailoverCount,final Exception e) throws Exception {  final RetryInfo retryInfo=RetryInfo.newRetryInfo(policy,e,counters,proxyDescriptor.idempotentOrAtMostOnce(method),expectFailoverCount);  if (retryInfo.isFail()) {    if (retryInfo.action.reason != null) {      if (LOG.isDebugEnabled()) {
private void log(final Method method,final boolean isFailover,final int failovers,final long delay,final Exception ex){  boolean info=true;  if (!failedAtLeastOnce.contains(proxyDescriptor.getProxyInfo().toString())) {    failedAtLeastOnce.add(proxyDescriptor.getProxyInfo().toString());    info=hasSuccessfulCall || asyncCallHandler.hasSuccessfulCall();    if (!info && !LOG.isDebugEnabled()) {      return;    }  }  final StringBuilder b=new StringBuilder().append(ex + ", while invoking ").append(proxyDescriptor.getProxyInfo().getString(method.getName()));  if (failovers > 0) {    b.append(" after ").append(failovers).append(" failover attempts");  }  b.append(isFailover ? ". Trying to failover " : ". Retrying ");  b.append(delay > 0 ? "after sleeping for " + delay + "ms." : "immediately.");  if (info) {    LOG.info(b.toString());  } else {
public static RetryPolicy getDefaultRetryPolicy(Configuration conf,String retryPolicyEnabledKey,boolean defaultRetryPolicyEnabled,String retryPolicySpecKey,String defaultRetryPolicySpec,final String remoteExceptionToRetry){  final RetryPolicy multipleLinearRandomRetry=getMultipleLinearRandomRetry(conf,retryPolicyEnabledKey,defaultRetryPolicyEnabled,retryPolicySpecKey,defaultRetryPolicySpec);
private void decayCurrentCosts(){  LOG.debug("Start to decay current costs.");  try {    long totalDecayedCost=0;    long totalRawCost=0;    Iterator<Map.Entry<Object,List<AtomicLong>>> it=callCosts.entrySet().iterator();    while (it.hasNext()) {      Map.Entry<Object,List<AtomicLong>> entry=it.next();      AtomicLong decayedCost=entry.getValue().get(0);      AtomicLong rawCost=entry.getValue().get(1);      totalRawCost+=rawCost.get();      long currentValue=decayedCost.get();      long nextValue=(long)(currentValue * decayFactor);      totalDecayedCost+=nextValue;      decayedCost.set(nextValue);
  try {    long totalDecayedCost=0;    long totalRawCost=0;    Iterator<Map.Entry<Object,List<AtomicLong>>> it=callCosts.entrySet().iterator();    while (it.hasNext()) {      Map.Entry<Object,List<AtomicLong>> entry=it.next();      AtomicLong decayedCost=entry.getValue().get(0);      AtomicLong rawCost=entry.getValue().get(1);      totalRawCost+=rawCost.get();      long currentValue=decayedCost.get();      long nextValue=(long)(currentValue * decayFactor);      totalDecayedCost+=nextValue;      decayedCost.set(nextValue);      LOG.debug("Decaying costs for the user: {}, its decayedCost: {}, rawCost: {}",entry.getKey(),nextValue,rawCost.get());      if (nextValue == 0) {
    while (it.hasNext()) {      Map.Entry<Object,List<AtomicLong>> entry=it.next();      AtomicLong decayedCost=entry.getValue().get(0);      AtomicLong rawCost=entry.getValue().get(1);      totalRawCost+=rawCost.get();      long currentValue=decayedCost.get();      long nextValue=(long)(currentValue * decayFactor);      totalDecayedCost+=nextValue;      decayedCost.set(nextValue);      LOG.debug("Decaying costs for the user: {}, its decayedCost: {}, rawCost: {}",entry.getKey(),nextValue,rawCost.get());      if (nextValue == 0) {        LOG.debug("The decayed cost for the user {} is zero " + "and being cleaned.",entry.getKey());        it.remove();      }    }    totalDecayedCallCost.set(totalDecayedCost);    totalRawCallCost.set(totalRawCost);
      totalRawCost+=rawCost.get();      long currentValue=decayedCost.get();      long nextValue=(long)(currentValue * decayFactor);      totalDecayedCost+=nextValue;      decayedCost.set(nextValue);      LOG.debug("Decaying costs for the user: {}, its decayedCost: {}, rawCost: {}",entry.getKey(),nextValue,rawCost.get());      if (nextValue == 0) {        LOG.debug("The decayed cost for the user {} is zero " + "and being cleaned.",entry.getKey());        it.remove();      }    }    totalDecayedCallCost.set(totalDecayedCost);    totalRawCallCost.set(totalRawCost);    LOG.debug("After decaying the stored costs, totalDecayedCost: {}, " + "totalRawCallCost: {}.",totalDecayedCost,totalRawCost);    recomputeScheduleCache();    updateAverageResponseTime(true);  } catch (  Exception ex) {
private int cachedOrComputedPriorityLevel(Object identity){  Map<Object,Integer> scheduleCache=scheduleCacheRef.get();  if (scheduleCache != null) {    Integer priority=scheduleCache.get(identity);    if (priority != null) {
@Override public boolean shouldBackOff(Schedulable obj){  Boolean backOff=false;  if (backOffByResponseTimeEnabled) {    int priorityLevel=obj.getPriorityLevel();    if (LOG.isDebugEnabled()) {      double[] responseTimes=getAverageResponseTime();      LOG.debug("Current Caller: {}  Priority: {} ",obj.getUserGroupInformation().getUserName(),obj.getPriorityLevel());      for (int i=0; i < numLevels; i++) {
@Override public void addResponseTime(String callName,Schedulable schedulable,ProcessingDetails details){  String user=identityProvider.makeIdentity(schedulable);  long processingCost=costProvider.getCost(details);  addCost(user,processingCost);  int priorityLevel=schedulable.getPriorityLevel();  long queueTime=details.get(Timing.QUEUE,RpcMetrics.TIMEUNIT);  long processingTime=details.get(Timing.PROCESSING,RpcMetrics.TIMEUNIT);  this.decayRpcSchedulerDetailedMetrics.addQueueTime(priorityLevel,queueTime);  this.decayRpcSchedulerDetailedMetrics.addProcessingTime(priorityLevel,processingTime);  responseTimeCountInCurrWindow.getAndIncrement(priorityLevel);  responseTimeTotalInCurrWindow.getAndAdd(priorityLevel,queueTime + processingTime);  if (LOG.isDebugEnabled()) {
    long totalResponseTime=responseTimeTotalInCurrWindow.get(i);    long responseTimeCount=responseTimeCountInCurrWindow.get(i);    if (responseTimeCount > 0) {      averageResponseTime=(double)totalResponseTime / responseTimeCount;    }    final double lastAvg=responseTimeAvgInLastWindow.get(i);    if (lastAvg > PRECISION || averageResponseTime > PRECISION) {      if (enableDecay) {        final double decayed=decayFactor * lastAvg + averageResponseTime;        responseTimeAvgInLastWindow.set(i,decayed);      } else {        responseTimeAvgInLastWindow.set(i,averageResponseTime);      }    } else {      responseTimeAvgInLastWindow.set(i,0);    }    responseTimeCountInLastWindow.set(i,responseTimeCount);    if (LOG.isDebugEnabled()) {
public static void stopProxy(Object proxy){  if (proxy == null) {    throw new HadoopIllegalArgumentException("Cannot close proxy since it is null");  }  try {    if (proxy instanceof Closeable) {      ((Closeable)proxy).close();      return;    } else {      InvocationHandler handler=Proxy.getInvocationHandler(proxy);      if (handler instanceof Closeable) {        ((Closeable)handler).close();        return;      }    }  } catch (  IOException e) {    LOG.error("Closing proxy or invocation handler caused exception",e);  }catch (  IllegalArgumentException e) {
  RpcSaslProto saslResponse=null;  try {    try {      saslResponse=processSaslMessage(saslMessage);    } catch (    IOException e) {      rpcMetrics.incrAuthenticationFailures();      if (LOG.isDebugEnabled()) {        LOG.debug(StringUtils.stringifyException(e));      }      IOException tce=(IOException)getTrueCause(e);      AUDITLOG.warn(AUTH_FAILED_FOR + this.toString() + ":"+ attemptingUser+ " ("+ e.getLocalizedMessage()+ ") with true cause: ("+ tce.getLocalizedMessage()+ ")");      throw tce;    }    if (saslServer != null && saslServer.isComplete()) {      if (LOG.isDebugEnabled()) {        LOG.debug("SASL server context established. Negotiated QoP is " + saslServer.getNegotiatedProperty(Sasl.QOP));      }      user=getAuthorizedUgi(saslServer.getAuthorizationID());
    try {      saslResponse=processSaslMessage(saslMessage);    } catch (    IOException e) {      rpcMetrics.incrAuthenticationFailures();      if (LOG.isDebugEnabled()) {        LOG.debug(StringUtils.stringifyException(e));      }      IOException tce=(IOException)getTrueCause(e);      AUDITLOG.warn(AUTH_FAILED_FOR + this.toString() + ":"+ attemptingUser+ " ("+ e.getLocalizedMessage()+ ") with true cause: ("+ tce.getLocalizedMessage()+ ")");      throw tce;    }    if (saslServer != null && saslServer.isComplete()) {      if (LOG.isDebugEnabled()) {        LOG.debug("SASL server context established. Negotiated QoP is " + saslServer.getNegotiatedProperty(Sasl.QOP));      }      user=getAuthorizedUgi(saslServer.getAuthorizationID());      if (LOG.isDebugEnabled()) {        LOG.debug("SASL server successfully authenticated client: " + user);
private RpcSaslProto buildSaslResponse(SaslState state,byte[] replyToken){  if (LOG.isDebugEnabled()) {
private void unwrapPacketAndProcessRpcs(byte[] inBuf) throws IOException, InterruptedException {  if (LOG.isDebugEnabled()) {
private void processOneRpc(ByteBuffer bb) throws IOException, InterruptedException {  int callId=-1;  int retry=RpcConstants.INVALID_RETRY_COUNT;  try {    final RpcWritable.Buffer buffer=RpcWritable.Buffer.wrap(bb);    final RpcRequestHeaderProto header=getMessage(RpcRequestHeaderProto.getDefaultInstance(),buffer);    callId=header.getCallId();    retry=header.getRetryCount();    if (LOG.isDebugEnabled()) {
    final RpcWritable.Buffer buffer=RpcWritable.Buffer.wrap(bb);    final RpcRequestHeaderProto header=getMessage(RpcRequestHeaderProto.getDefaultInstance(),buffer);    callId=header.getCallId();    retry=header.getRetryCount();    if (LOG.isDebugEnabled()) {      LOG.debug(" got #" + callId);    }    checkRpcHeaders(header);    if (callId < 0) {      processRpcOutOfBandRequest(header,buffer);    } else     if (!connectionContextRead) {      throw new FatalRpcServerException(RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER,"Connection context not established");    } else {      processRpcRequest(header,buffer);    }  } catch (  RpcServerException rse) {    if (LOG.isDebugEnabled()) {
private void moveToNextQueue(){  int thisIdx=this.currentQueueIndex.get();  int nextIdx=(thisIdx + 1) % this.numQueues;  this.currentQueueIndex.set(nextIdx);  this.requestsLeft.set(this.queueWeights[nextIdx]);
public void init(int numLevels){
          return;        }        listBeans(jg,new ObjectName(splitStrings[0]),splitStrings[1],response);        return;      }      String qry=request.getParameter("qry");      if (qry == null) {        qry="*:*";      }      listBeans(jg,new ObjectName(qry),null,response);    }  finally {      if (jg != null) {        jg.close();      }      if (writer != null) {        writer.close();      }    }  } catch (  IOException e) {    LOG.error("Caught an exception while processing JMX request",e);    response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);
private void listBeans(JsonGenerator jg,ObjectName qry,String attribute,HttpServletResponse response) throws IOException {
    MBeanInfo minfo;    String code="";    Object attributeinfo=null;    try {      minfo=mBeanServer.getMBeanInfo(oname);      code=minfo.getClassName();      String prs="";      try {        if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {          prs="modelerType";          code=(String)mBeanServer.getAttribute(oname,prs);        }        if (attribute != null) {          prs=attribute;          attributeinfo=mBeanServer.getAttribute(oname,prs);        }      } catch (      AttributeNotFoundException e) {
    Object attributeinfo=null;    try {      minfo=mBeanServer.getMBeanInfo(oname);      code=minfo.getClassName();      String prs="";      try {        if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {          prs="modelerType";          code=(String)mBeanServer.getAttribute(oname,prs);        }        if (attribute != null) {          prs=attribute;          attributeinfo=mBeanServer.getAttribute(oname,prs);        }      } catch (      AttributeNotFoundException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      MBeanException e) {
      minfo=mBeanServer.getMBeanInfo(oname);      code=minfo.getClassName();      String prs="";      try {        if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {          prs="modelerType";          code=(String)mBeanServer.getAttribute(oname,prs);        }        if (attribute != null) {          prs=attribute;          attributeinfo=mBeanServer.getAttribute(oname,prs);        }      } catch (      AttributeNotFoundException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      MBeanException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      RuntimeException e) {
      String prs="";      try {        if ("org.apache.commons.modeler.BaseModelMBean".equals(code)) {          prs="modelerType";          code=(String)mBeanServer.getAttribute(oname,prs);        }        if (attribute != null) {          prs=attribute;          attributeinfo=mBeanServer.getAttribute(oname,prs);        }      } catch (      AttributeNotFoundException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      MBeanException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      RuntimeException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      ReflectionException e) {
          code=(String)mBeanServer.getAttribute(oname,prs);        }        if (attribute != null) {          prs=attribute;          attributeinfo=mBeanServer.getAttribute(oname,prs);        }      } catch (      AttributeNotFoundException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      MBeanException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      RuntimeException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      ReflectionException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }    } catch (    InstanceNotFoundException e) {      continue;    }catch (    IntrospectionException e) {
          prs=attribute;          attributeinfo=mBeanServer.getAttribute(oname,prs);        }      } catch (      AttributeNotFoundException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      MBeanException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      RuntimeException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }catch (      ReflectionException e) {        LOG.error("getting attribute " + prs + " of "+ oname+ " threw an exception",e);      }    } catch (    InstanceNotFoundException e) {      continue;    }catch (    IntrospectionException e) {      LOG.error("Problem while trying to process JMX query: " + qry + " with MBean "+ oname,e);      continue;
private void writeAttribute(JsonGenerator jg,ObjectName oname,MBeanAttributeInfo attr) throws IOException {  if (!attr.isReadable()) {    return;  }  String attName=attr.getName();  if ("modelerType".equals(attName)) {    return;  }  if (attName.indexOf("=") >= 0 || attName.indexOf(":") >= 0 || attName.indexOf(" ") >= 0) {    return;  }  Object value=null;  try {    value=mBeanServer.getAttribute(oname,attName);  } catch (  RuntimeMBeanException e) {    if (e.getCause() instanceof UnsupportedOperationException) {      LOG.debug("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    } else {
  }  String attName=attr.getName();  if ("modelerType".equals(attName)) {    return;  }  if (attName.indexOf("=") >= 0 || attName.indexOf(":") >= 0 || attName.indexOf(" ") >= 0) {    return;  }  Object value=null;  try {    value=mBeanServer.getAttribute(oname,attName);  } catch (  RuntimeMBeanException e) {    if (e.getCause() instanceof UnsupportedOperationException) {      LOG.debug("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    } else {      LOG.error("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    }    return;  }catch (  RuntimeErrorException e) {
  if (attName.indexOf("=") >= 0 || attName.indexOf(":") >= 0 || attName.indexOf(" ") >= 0) {    return;  }  Object value=null;  try {    value=mBeanServer.getAttribute(oname,attName);  } catch (  RuntimeMBeanException e) {    if (e.getCause() instanceof UnsupportedOperationException) {      LOG.debug("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    } else {      LOG.error("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    }    return;  }catch (  RuntimeErrorException e) {    LOG.error("getting attribute {} of {} threw an exception",attName,oname,e);    return;  }catch (  AttributeNotFoundException e) {
  Object value=null;  try {    value=mBeanServer.getAttribute(oname,attName);  } catch (  RuntimeMBeanException e) {    if (e.getCause() instanceof UnsupportedOperationException) {      LOG.debug("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    } else {      LOG.error("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    }    return;  }catch (  RuntimeErrorException e) {    LOG.error("getting attribute {} of {} threw an exception",attName,oname,e);    return;  }catch (  AttributeNotFoundException e) {    return;  }catch (  MBeanException e) {
  } catch (  RuntimeMBeanException e) {    if (e.getCause() instanceof UnsupportedOperationException) {      LOG.debug("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    } else {      LOG.error("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    }    return;  }catch (  RuntimeErrorException e) {    LOG.error("getting attribute {} of {} threw an exception",attName,oname,e);    return;  }catch (  AttributeNotFoundException e) {    return;  }catch (  MBeanException e) {    LOG.error("getting attribute " + attName + " of "+ oname+ " threw an exception",e);    return;  }catch (  RuntimeException e) {
static MetricsConfig loadFirst(String prefix,String... fileNames){  for (  String fname : fileNames) {    try {      PropertiesConfiguration pcf=new PropertiesConfiguration();      pcf.setListDelimiterHandler(new DefaultListDelimiterHandler(','));      FileHandler fh=new FileHandler(pcf);      fh.setFileName(fname);      fh.load();      Configuration cf=pcf.interpolatedConfiguration();
static MetricsConfig loadFirst(String prefix,String... fileNames){  for (  String fname : fileNames) {    try {      PropertiesConfiguration pcf=new PropertiesConfiguration();      pcf.setListDelimiterHandler(new DefaultListDelimiterHandler(','));      FileHandler fh=new FileHandler(pcf);      fh.setFileName(fname);      fh.load();      Configuration cf=pcf.interpolatedConfiguration();      LOG.info("Loaded properties from {}",fname);      if (LOG.isDebugEnabled()) {
    try {      PropertiesConfiguration pcf=new PropertiesConfiguration();      pcf.setListDelimiterHandler(new DefaultListDelimiterHandler(','));      FileHandler fh=new FileHandler(pcf);      fh.setFileName(fname);      fh.load();      Configuration cf=pcf.interpolatedConfiguration();      LOG.info("Loaded properties from {}",fname);      if (LOG.isDebugEnabled()) {        LOG.debug("Properties: {}",toString(cf));      }      MetricsConfig mc=new MetricsConfig(cf,prefix);      LOG.debug("Metrics Config: {}",mc);      return mc;    } catch (    ConfigurationException e) {      if (e.getMessage().startsWith("Could not locate")) {
@Override public Object getPropertyInternal(String key){  Object value=super.getPropertyInternal(key);  if (value == null) {    if (LOG.isDebugEnabled()) {
String getClassName(String prefix){  String classKey=prefix.isEmpty() ? "class" : prefix.concat(".class");  String clsName=getString(classKey);
  final ClassLoader defaultLoader=getClass().getClassLoader();  Object purls=super.getProperty(PLUGIN_URLS_KEY);  if (purls == null) {    return defaultLoader;  }  Iterable<String> jars=SPLITTER.split((String)purls);  int len=Iterables.size(jars);  if (len > 0) {    final URL[] urls=new URL[len];    try {      int i=0;      for (      String jar : jars) {        LOG.debug("Parsing URL for {}",jar);        urls[i++]=new URL(jar);      }    } catch (    Exception e) {      throw new MetricsConfigException(e);
boolean putMetrics(MetricsBuffer buffer,long logicalTimeMs){  if (logicalTimeMs % periodMs == 0) {
  Random rng=new Random(System.nanoTime());  while (!stopping) {    try {      queue.consumeAll(this);      refreshQueueSizeGauge();      retryDelay=firstRetryDelay;      n=retryCount;      inError=false;    } catch (    InterruptedException e) {      LOG.info(name + " thread interrupted.");    }catch (    Exception e) {      if (n > 0) {        int retryWindow=Math.max(0,1000 / 2 * retryDelay - minDelay);        int awhile=rng.nextInt(retryWindow) + minDelay;        if (!inError) {
      retryDelay=firstRetryDelay;      n=retryCount;      inError=false;    } catch (    InterruptedException e) {      LOG.info(name + " thread interrupted.");    }catch (    Exception e) {      if (n > 0) {        int retryWindow=Math.max(0,1000 / 2 * retryDelay - minDelay);        int awhile=rng.nextInt(retryWindow) + minDelay;        if (!inError) {          LOG.error("Got sink exception, retry in " + awhile + "ms",e);        }        retryDelay*=retryBackoff;        try {          Thread.sleep(awhile);        } catch (        InterruptedException e2) {
 catch (    InterruptedException e) {      LOG.info(name + " thread interrupted.");    }catch (    Exception e) {      if (n > 0) {        int retryWindow=Math.max(0,1000 / 2 * retryDelay - minDelay);        int awhile=rng.nextInt(retryWindow) + minDelay;        if (!inError) {          LOG.error("Got sink exception, retry in " + awhile + "ms",e);        }        retryDelay*=retryBackoff;        try {          Thread.sleep(awhile);        } catch (        InterruptedException e2) {          LOG.info(name + " thread interrupted while waiting for retry",e2);        }        --n;      } else {
@Override public void consume(MetricsBuffer buffer){  long ts=0;  for (  MetricsBuffer.Entry entry : buffer) {    if (sourceFilter == null || sourceFilter.accepts(entry.name())) {      for (      MetricsRecordImpl record : entry.records()) {        if ((context == null || context.equals(record.context())) && (recordFilter == null || recordFilter.accepts(record))) {          if (LOG.isDebugEnabled()) {
@Override public AttributeList getAttributes(String[] attributes){  updateJmxCache();synchronized (this) {    AttributeList ret=new AttributeList();    for (    String key : attributes) {      Attribute attr=attrCache.get(key);      if (LOG.isDebugEnabled()) {
@Override public synchronized void stop(){  if (!monitoring && !DefaultMetricsSystem.inMiniClusterMode()) {    LOG.warn(prefix + " metrics system not yet started!",new MetricsException("Illegal stop"));    return;  }  if (!monitoring) {    LOG.info(prefix + " metrics system stopped (again)");    return;  }  for (  Callback cb : callbacks)   cb.preStop();  for (  Callback cb : namedCallbacks.values())   cb.preStop();  LOG.info("Stopping " + prefix + " metrics system...");  stopTimer();  stopSources();  stopSinks();  clearConfigs();  monitoring=false;
@Override public synchronized <T>T register(String name,String desc,T source){  MetricsSourceBuilder sb=MetricsAnnotations.newSourceBuilder(source);  final MetricsSource s=sb.build();  MetricsInfo si=sb.info();  String name2=name == null ? si.name() : name;  final String finalDesc=desc == null ? si.description() : desc;  final String finalName=DefaultMetricsSystem.sourceName(name2,!monitoring);  allSources.put(finalName,s);
synchronized void registerSource(String name,String desc,MetricsSource source){  checkNotNull(config,"config");  MetricsConfig conf=sourceConfigs.get(name);  MetricsSourceAdapter sa=new MetricsSourceAdapter(prefix,name,desc,source,injectedTags,period,conf != null ? conf : config.subset(SOURCE_KEY));  sources.put(name,sa);  sa.start();
@Override public synchronized <T extends MetricsSink>T register(final String name,final String description,final T sink){
synchronized void registerSink(String name,String desc,MetricsSink sink){  checkNotNull(config,"config");  MetricsConfig conf=sinkConfigs.get(name);  MetricsSinkAdapter sa=conf != null ? newSink(name,desc,sink,conf) : newSink(name,desc,sink,config.subset(SINK_KEY));  sinks.put(name,sa);  sa.start();
private void snapshotMetrics(MetricsSourceAdapter sa,MetricsBufferBuilder bufferBuilder){  long startTime=Time.monotonicNow();  bufferBuilder.add(sa.name(),sa.getMetrics(collector,true));  collector.clear();  snapshotStat.add(Time.monotonicNow() - startTime);
private synchronized void stopSources(){  for (  Entry<String,MetricsSourceAdapter> entry : sources.entrySet()) {    MetricsSourceAdapter sa=entry.getValue();
private synchronized void stopSinks(){  for (  Entry<String,MetricsSinkAdapter> entry : sinks.entrySet()) {    MetricsSinkAdapter sa=entry.getValue();
private InitMode initMode(){  LOG.debug("from system property: " + System.getProperty(MS_INIT_MODE_KEY));
MutableMetric newForField(Field field,Metric annotation,MetricsRegistry registry){  if (LOG.isDebugEnabled()) {
MutableMetric newForMethod(Object source,Method method,Metric annotation,MetricsRegistry registry){  if (LOG.isDebugEnabled()) {
public void init(Class<?> protocol){  if (protocolCache.contains(protocol))   return;  protocolCache.add(protocol);  for (  Method method : protocol.getDeclaredMethods()) {    String name=method.getName();
 else {    try {      hostName=DNS.getDefaultHost(conf.getString("dfs.datanode.dns.interface","default"),conf.getString("dfs.datanode.dns.nameserver","default"));    } catch (    UnknownHostException uhe) {      LOG.error(uhe.toString());      hostName="UNKNOWN.example.com";    }  }  metricsServers=Servers.parse(conf.getString(SERVERS_PROPERTY),DEFAULT_PORT);  multicastEnabled=conf.getBoolean(MULTICAST_ENABLED_PROPERTY,DEFAULT_MULTICAST_ENABLED);  multicastTtl=conf.getInt(MULTICAST_TTL_PROPERTY,DEFAULT_MULTICAST_TTL);  gangliaConfMap=new HashMap<String,GangliaConf>();  loadGangliaConf(GangliaConfType.units);  loadGangliaConf(GangliaConfType.tmax);  loadGangliaConf(GangliaConfType.dmax);  loadGangliaConf(GangliaConfType.slope);  try {
private void loadGangliaConf(GangliaConfType gtype){  String propertyarr[]=conf.getStringArray(gtype.name());  if (propertyarr != null && propertyarr.length > 0) {    for (    String metricNValue : propertyarr) {      String metricNValueArr[]=metricNValue.split(EQUAL);      if (metricNValueArr.length != 2 || metricNValueArr[0].length() == 0) {
static public ObjectName register(String serviceName,String nameName,Map<String,String> properties,Object theMbean){  final MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();  Preconditions.checkNotNull(properties,"JMX bean properties should not be null for " + "bean registration.");  ObjectName name=getMBeanName(serviceName,nameName,properties);  if (name != null) {    try {      mbs.registerMBean(theMbean,name);
static public void unregister(ObjectName mbeanName){
    if (!isChildScope(excludedScope,scope)) {      excludedScope=null;    }  }  Node node=getNode(scope);  if (!(node instanceof InnerNode)) {    return excludedNodes != null && excludedNodes.contains(node) ? null : node;  }  InnerNode innerNode=(InnerNode)node;  int numOfDatanodes=innerNode.getNumOfLeaves();  if (excludedScope == null) {    node=null;  } else {    node=getNode(excludedScope);    if (!(node instanceof InnerNode)) {      numOfDatanodes-=1;    } else {      numOfDatanodes-=((InnerNode)node).getNumOfLeaves();
  } else {    node=getNode(excludedScope);    if (!(node instanceof InnerNode)) {      numOfDatanodes-=1;    } else {      numOfDatanodes-=((InnerNode)node).getNumOfLeaves();    }  }  if (numOfDatanodes <= 0) {    LOG.debug("Failed to find datanode (scope=\"{}\" excludedScope=\"{}\")." + " numOfDatanodes={}",scope,excludedScope,numOfDatanodes);    return null;  }  final int availableNodes;  if (excludedScope == null) {    availableNodes=countNumOfAvailableNodes(scope,excludedNodes);  } else {    netlock.readLock().lock();    try {
  int nthValidToReturn=r.nextInt(availableNodes);  LOG.debug("nthValidToReturn is {}",nthValidToReturn);  Node ret=parentNode.getLeaf(r.nextInt(totalInScopeNodes),excludedScopeNode);  if (!excludedNodes.contains(ret)) {    LOG.debug("Chosen node {} from first random",ret);    return ret;  } else {    ret=null;  }  Node lastValidNode=null;  for (int i=0; i < totalInScopeNodes; ++i) {    ret=parentNode.getLeaf(i,excludedScopeNode);    if (!excludedNodes.contains(ret)) {      if (nthValidToReturn == 0) {        break;      }      --nthValidToReturn;
  if (!excludedNodes.contains(ret)) {    LOG.debug("Chosen node {} from first random",ret);    return ret;  } else {    ret=null;  }  Node lastValidNode=null;  for (int i=0; i < totalInScopeNodes; ++i) {    ret=parentNode.getLeaf(i,excludedScopeNode);    if (!excludedNodes.contains(ret)) {      if (nthValidToReturn == 0) {        break;      }      --nthValidToReturn;      lastValidNode=ret;    } else {      LOG.debug("Node {} is excluded, continuing.",ret);
  if (node == null)   return;  if (node instanceof InnerNode) {    throw new IllegalArgumentException("Not allow to add an inner node: " + NodeBase.getPath(node));  }  netlock.writeLock().lock();  try {    Node rack=null;    if (NetworkTopology.DEFAULT_RACK.equals(node.getNetworkLocation())) {      node.setNetworkLocation(node.getNetworkLocation() + NetworkTopologyWithNodeGroup.DEFAULT_NODEGROUP);    }    Node nodeGroup=getNode(node.getNetworkLocation());    if (nodeGroup == null) {      nodeGroup=new InnerNodeWithNodeGroup(node.getNetworkLocation());    }    rack=getNode(nodeGroup.getNetworkLocation());    if (rack != null && (!(rack instanceof InnerNode) || rack.getParent() == null)) {      throw new IllegalArgumentException("Unexpected data node " + node.toString() + " at an illegal network location");    }    if (clusterMap.add(node)) {
private void loadMappingProviders(){  String[] providerNames=conf.getStrings(MAPPING_PROVIDERS_CONFIG_KEY,new String[]{});  String providerKey;  for (  String name : providerNames) {    providerKey=MAPPING_PROVIDER_CONFIG_PREFIX + "." + name;    Class<?> providerClass=conf.getClass(providerKey,null);    if (providerClass == null) {
@Override @VisibleForTesting public Map<String,String> getServerProperties(InetAddress clientAddress,int ingressPort){
static private void logError(int groupId,String error){
  String keytabName=popOptionWithArgument(ARG_KEYTAB,args);  if (keytabName != null) {    keytab=new File(keytabName);  }  principal=popOptionWithArgument(ARG_PRINCIPAL,args);  String outf=popOptionWithArgument(ARG_OUTPUT,args);  String mkl=popOptionWithArgument(ARG_KEYLEN,args);  if (mkl != null) {    minKeyLength=Integer.parseInt(mkl);  }  securityRequired=popOption(ARG_SECURE,args);  nofail=popOption(ARG_NOFAIL,args);  jaas=popOption(ARG_JAAS,args);  nologin=popOption(ARG_NOLOGIN,args);  checkShortName=popOption(ARG_VERIFYSHORTNAME,args);  String resource;  while (null != (resource=popOptionWithArgument(ARG_RESOURCE,args))) {
  String identity;  if (keytab != null) {    File kt=keytab.getCanonicalFile();    println("Using keytab %s principal %s",kt,principal);    identity=principal;    failif(principal == null,CAT_KERBEROS,"No principal defined");    ugi=loginUserFromKeytabAndReturnUGI(principal,kt.getPath());    dumpUGI(identity,ugi);    validateUGI(principal,ugi);    title("Attempting to relogin");    try {      setShouldRenewImmediatelyForTests(true);      ugi.reloginFromKeytab();    } catch (    IllegalAccessError e) {      warn(CAT_UGI,"Failed to reset UGI -and so could not try to relogin");
Set<String> doGetGroups(String user,int goUpHierarchy) throws NamingException {  DirContext c=getDirContext();  NamingEnumeration<SearchResult> results=c.search(userbaseDN,userSearchFilter,new Object[]{user},SEARCH_CONTROLS);  if (!results.hasMoreElements()) {
    LOG.debug("doGetGroups({}) returned no groups because the " + "user is not found.",user);    return Collections.emptySet();  }  SearchResult result=results.nextElement();  Set<String> groups=Collections.emptySet();  if (useOneQuery) {    try {      Attribute groupDNAttr=result.getAttributes().get(memberOfAttr);      if (groupDNAttr == null) {        throw new NamingException("The user object does not have '" + memberOfAttr + "' attribute."+ "Returned user object: "+ result.toString());      }      groups=new LinkedHashSet<>();      NamingEnumeration groupEnumeration=groupDNAttr.getAll();      while (groupEnumeration.hasMore()) {        String groupDN=groupEnumeration.next().toString();        groups.add(getRelativeDistinguishedName(groupDN));      }    } catch (    NamingException e) {
  Set<String> groups=Collections.emptySet();  if (useOneQuery) {    try {      Attribute groupDNAttr=result.getAttributes().get(memberOfAttr);      if (groupDNAttr == null) {        throw new NamingException("The user object does not have '" + memberOfAttr + "' attribute."+ "Returned user object: "+ result.toString());      }      groups=new LinkedHashSet<>();      NamingEnumeration groupEnumeration=groupDNAttr.getAll();      while (groupEnumeration.hasMore()) {        String groupDN=groupEnumeration.next().toString();        groups.add(getRelativeDistinguishedName(groupDN));      }    } catch (    NamingException e) {      LOG.info("Failed to get groups from the first lookup. Initiating " + "the second LDAP query using the user's DN.",e);    }  }  if (groups.isEmpty() || goUpHierarchy > 0) {    groups=lookupGroup(result,c,goUpHierarchy);
protected boolean failover(int attemptsMadeWithSameLdap,int maxAttemptsBeforeFailover){  if (attemptsMadeWithSameLdap >= maxAttemptsBeforeFailover) {    String previousLdapUrl=currentLdapUrl;    currentLdapUrl=ldapUrls.next();
protected void switchBindUser(AuthenticationException e){  BindUserInfo oldBindUser=this.currentBindUser;  currentBindUser=this.bindUsers.next();  if (!oldBindUser.equals(currentBindUser)) {
@Override public synchronized void setConf(Configuration conf){  this.conf=conf;  String[] urls=conf.getStrings(LDAP_URL_KEY,LDAP_URL_DEFAULT);  if (urls == null || urls.length == 0) {    throw new RuntimeException("LDAP URL(s) are not configured");  }  ldapUrls=Iterators.cycle(urls);  currentLdapUrl=ldapUrls.next();  useSsl=conf.getBoolean(LDAP_USE_SSL_KEY,LDAP_USE_SSL_DEFAULT);  if (useSsl) {    loadSslConf(conf);  }  initializeBindUsers();  String baseDN=conf.getTrimmed(BASE_DN_KEY,BASE_DN_DEFAULT);  userbaseDN=conf.getTrimmed(USER_BASE_DN_KEY,baseDN);  LOG.debug("Usersearch baseDN: {}",userbaseDN);  groupbaseDN=conf.getTrimmed(GROUP_BASE_DN_KEY,baseDN);
  StringBuffer newProviderPath=new StringBuffer();  String[] providers=providerPath.split(",");  Path path=null;  for (  String provider : providers) {    try {      path=unnestUri(new URI(provider));      Class<? extends FileSystem> clazz=null;      try {        String scheme=path.toUri().getScheme();        clazz=FileSystem.getFileSystemClass(scheme,config);      } catch (      IOException ioe) {        if (newProviderPath.length() > 0) {          newProviderPath.append(",");        }        newProviderPath.append(provider);      }      if (clazz != null) {
    }    AuthMethod authMethod=AuthMethod.valueOf(authType.getMethod());    if (authMethod == AuthMethod.SIMPLE) {      switchToSimple=true;    } else {      saslClient=createSaslClient(authType);      if (saslClient == null) {        continue;      }    }    selectedAuthType=authType;    break;  }  if (saslClient == null && !switchToSimple) {    List<String> serverAuthMethods=new ArrayList<String>();    for (    SaslAuth authType : authTypes) {      serverAuthMethods.add(authType.getMethod());    }    throw new AccessControlException("Client cannot authenticate via:" + serverAuthMethods);  }  if (LOG.isDebugEnabled() && selectedAuthType != null) {
      Token<?> token=getServerToken(authType);      if (token == null) {        LOG.debug("tokens aren't supported for this protocol" + " or user doesn't have one");        return null;      }      saslCallback=new SaslClientCallbackHandler(token);      break;    }case KERBEROS:{    if (ugi.getRealAuthenticationMethod().getAuthMethod() != AuthMethod.KERBEROS) {      LOG.debug("client isn't using kerberos");      return null;    }    String serverPrincipal=getServerPrincipal(authType);    if (serverPrincipal == null) {      LOG.debug("protocol doesn't use kerberos");      return null;    }    if (LOG.isDebugEnabled()) {
      saslCallback=new SaslClientCallbackHandler(token);      break;    }case KERBEROS:{    if (ugi.getRealAuthenticationMethod().getAuthMethod() != AuthMethod.KERBEROS) {      LOG.debug("client isn't using kerberos");      return null;    }    String serverPrincipal=getServerPrincipal(authType);    if (serverPrincipal == null) {      LOG.debug("protocol doesn't use kerberos");      return null;    }    if (LOG.isDebugEnabled()) {      LOG.debug("RPC Server's Kerberos principal name for protocol=" + protocol.getCanonicalName() + " is "+ serverPrincipal);    }    break;  }default:throw new IOException("Unknown authentication method " + method);}String mechanism=method.getMechanismName();
private Token<?> getServerToken(SaslAuth authType) throws IOException {  TokenInfo tokenInfo=SecurityUtil.getTokenInfo(protocol,conf);
@VisibleForTesting String getServerPrincipal(SaslAuth authType) throws IOException {  KerberosInfo krbInfo=SecurityUtil.getKerberosInfo(protocol,conf);
  LOG.debug("Get kerberos info proto:" + protocol + " info:"+ krbInfo);  if (krbInfo == null) {    return null;  }  String serverKey=krbInfo.serverPrincipal();  if (serverKey == null) {    throw new IllegalArgumentException("Can't obtain server Kerberos config key from protocol=" + protocol.getCanonicalName());  }  String serverPrincipal=new KerberosPrincipal(authType.getProtocol() + "/" + authType.getServerId(),KerberosPrincipal.KRB_NT_SRV_HST).getName();  String serverKeyPattern=conf.get(serverKey + ".pattern");  if (serverKeyPattern != null && !serverKeyPattern.isEmpty()) {    Pattern pattern=GlobPattern.compile(serverKeyPattern);    if (!pattern.matcher(serverPrincipal).matches()) {      throw new IllegalArgumentException(String.format("Server has invalid Kerberos principal: %s," + " doesn't match the pattern: %s",serverPrincipal,serverKeyPattern));    }  } else {    String confPrincipal=SecurityUtil.getServerPrincipal(conf.get(serverKey),serverAddr.getAddress());    if (LOG.isDebugEnabled()) {
private void sendSaslMessage(OutputStream out,RpcSaslProto message) throws IOException {  if (LOG.isDebugEnabled()) {
    if (serverId.isEmpty()) {      throw new AccessControlException("Kerberos principal name does NOT have the expected " + "hostname part: " + ugi.getUserName());    }    callback=new SaslGssCallbackHandler();    break;  }default:throw new AccessControlException("Server does not support SASL " + authMethod);}final SaslServer saslServer;if (ugi != null) {saslServer=ugi.doAs(new PrivilegedExceptionAction<SaslServer>(){@Override public SaslServer run() throws SaslException {  return saslFactory.createSaslServer(mechanism,protocol,serverId,saslProperties,callback);}});} else {saslServer=saslFactory.createSaslServer(mechanism,protocol,serverId,saslProperties,callback);}if (saslServer == null) {throw new AccessControlException("Unable to find SASL server implementation for " + mechanism);
    } else     if (callback instanceof NameCallback) {      nc=(NameCallback)callback;    } else     if (callback instanceof PasswordCallback) {      pc=(PasswordCallback)callback;    } else     if (callback instanceof RealmCallback) {      continue;    } else {      throw new UnsupportedCallbackException(callback,"Unrecognized SASL DIGEST-MD5 Callback");    }  }  if (pc != null) {    TokenIdentifier tokenIdentifier=getIdentifier(nc.getDefaultName(),secretManager);    char[] password=getPassword(tokenIdentifier);    UserGroupInformation user=null;    user=tokenIdentifier.getUser();    connection.attemptingUser=user;    if (LOG.isDebugEnabled()) {
@InterfaceAudience.Private @VisibleForTesting public static void setTokenServiceUseIp(boolean flag){  if (LOG.isDebugEnabled()) {
public static void setTokenService(Token<?> token,InetSocketAddress addr){  Text service=buildTokenService(addr);  if (token != null) {    token.setService(service);    if (LOG.isDebugEnabled()) {
      LOG.debug("add to " + mapName + "map:"+ nameId[0]+ " id:"+ nameId[1]);      final Integer key=staticMapping.get(parseId(nameId[1]));      final String value=nameId[0];      if (map.containsKey(key)) {        final String prevValue=map.get(key);        if (value.equals(prevValue)) {          continue;        }        reportDuplicateEntry("Got multiple names associated with the same id: ",key,value,key,prevValue);        continue;      }      if (map.containsValue(value)) {        final Integer prevKey=map.inverse().get(value);        reportDuplicateEntry("Got multiple ids associated with the same name: ",key,value,prevKey,value);        continue;      }      map.put(key,value);      updated=true;
      final Integer key=staticMapping.get(parseId(nameId[1]));      final String value=nameId[0];      if (map.containsKey(key)) {        final String prevValue=map.get(key);        if (value.equals(prevValue)) {          continue;        }        reportDuplicateEntry("Got multiple names associated with the same id: ",key,value,key,prevValue);        continue;      }      if (map.containsValue(value)) {        final Integer prevKey=map.inverse().get(value);        reportDuplicateEntry("Got multiple ids associated with the same name: ",key,value,prevKey,value);        continue;      }      map.put(key,value);      updated=true;    }    LOG.debug("Updated " + mapName + " map size: "+ map.size());
private boolean checkSupportedPlatform(){  if (!OS.startsWith("Linux") && !OS.startsWith("Mac") && !OS.equals("SunOS")&& !OS.contains("BSD")) {
private synchronized void updateStaticMapping() throws IOException {  final boolean init=(staticMapping == null);  if (staticMappingFile.exists()) {    long lmTime=staticMappingFile.lastModified();    if (lmTime != lastModificationTimeStaticMap) {
private static UserGroupInformation createLoginUser(Subject subject) throws IOException {  UserGroupInformation realUser=doSubjectLogin(subject,null);  UserGroupInformation loginUser=null;  try {    String proxyUser=System.getenv(HADOOP_PROXY_USER);    if (proxyUser == null) {      proxyUser=System.getProperty(HADOOP_PROXY_USER);    }    loginUser=proxyUser == null ? realUser : createProxyUser(proxyUser,realUser);    final Collection<String> tokenFileLocations=new LinkedHashSet<>();    tokenFileLocations.addAll(getTrimmedStringCollection(System.getProperty(HADOOP_TOKEN_FILES)));    tokenFileLocations.addAll(getTrimmedStringCollection(conf.get(HADOOP_TOKEN_FILES)));    tokenFileLocations.addAll(getTrimmedStringCollection(System.getenv(HADOOP_TOKEN_FILE_LOCATION)));    for (    String tokenFileLocation : tokenFileLocations) {      if (tokenFileLocation != null && tokenFileLocation.length() > 0) {        File tokenFile=new File(tokenFileLocation);
  try {    String proxyUser=System.getenv(HADOOP_PROXY_USER);    if (proxyUser == null) {      proxyUser=System.getProperty(HADOOP_PROXY_USER);    }    loginUser=proxyUser == null ? realUser : createProxyUser(proxyUser,realUser);    final Collection<String> tokenFileLocations=new LinkedHashSet<>();    tokenFileLocations.addAll(getTrimmedStringCollection(System.getProperty(HADOOP_TOKEN_FILES)));    tokenFileLocations.addAll(getTrimmedStringCollection(conf.get(HADOOP_TOKEN_FILES)));    tokenFileLocations.addAll(getTrimmedStringCollection(System.getenv(HADOOP_TOKEN_FILE_LOCATION)));    for (    String tokenFileLocation : tokenFileLocations) {      if (tokenFileLocation != null && tokenFileLocation.length() > 0) {        File tokenFile=new File(tokenFileLocation);        LOG.debug("Reading credentials from location {}",tokenFile.getCanonicalPath());        if (tokenFile.exists() && tokenFile.isFile()) {          Credentials cred=Credentials.readTokenStorageFile(tokenFile,conf);
      proxyUser=System.getProperty(HADOOP_PROXY_USER);    }    loginUser=proxyUser == null ? realUser : createProxyUser(proxyUser,realUser);    final Collection<String> tokenFileLocations=new LinkedHashSet<>();    tokenFileLocations.addAll(getTrimmedStringCollection(System.getProperty(HADOOP_TOKEN_FILES)));    tokenFileLocations.addAll(getTrimmedStringCollection(conf.get(HADOOP_TOKEN_FILES)));    tokenFileLocations.addAll(getTrimmedStringCollection(System.getenv(HADOOP_TOKEN_FILE_LOCATION)));    for (    String tokenFileLocation : tokenFileLocations) {      if (tokenFileLocation != null && tokenFileLocation.length() > 0) {        File tokenFile=new File(tokenFileLocation);        LOG.debug("Reading credentials from location {}",tokenFile.getCanonicalPath());        if (tokenFile.exists() && tokenFile.isFile()) {          Credentials cred=Credentials.readTokenStorageFile(tokenFile,conf);          LOG.debug("Loaded {} tokens from {}",cred.numberOfTokens(),tokenFile.getCanonicalPath());          loginUser.addCredentials(cred);        } else {
 else {          LOG.info("Token file {} does not exist",tokenFile.getCanonicalPath());        }      }    }    final Collection<String> tokensBase64=new LinkedHashSet<>();    tokensBase64.addAll(getTrimmedStringCollection(System.getProperty(HADOOP_TOKENS)));    tokensBase64.addAll(getTrimmedStringCollection(conf.get(HADOOP_TOKENS)));    tokensBase64.addAll(getTrimmedStringCollection(System.getenv(HADOOP_TOKEN)));    int numTokenBase64=0;    for (    String tokenBase64 : tokensBase64) {      if (tokenBase64 != null && tokenBase64.length() > 0) {        try {          Token<TokenIdentifier> token=new Token<>();          token.decodeFromUrlString(tokenBase64);          Credentials cred=new Credentials();          cred.addToken(token.getService(),token);          loginUser.addCredentials(cred);
        }      }    }    final Collection<String> tokensBase64=new LinkedHashSet<>();    tokensBase64.addAll(getTrimmedStringCollection(System.getProperty(HADOOP_TOKENS)));    tokensBase64.addAll(getTrimmedStringCollection(conf.get(HADOOP_TOKENS)));    tokensBase64.addAll(getTrimmedStringCollection(System.getenv(HADOOP_TOKEN)));    int numTokenBase64=0;    for (    String tokenBase64 : tokensBase64) {      if (tokenBase64 != null && tokenBase64.length() > 0) {        try {          Token<TokenIdentifier> token=new Token<>();          token.decodeFromUrlString(tokenBase64);          Credentials cred=new Credentials();          cred.addToken(token.getService(),token);          loginUser.addCredentials(cred);          numTokenBase64++;        } catch (        IOException ioe) {
    }    final Collection<String> tokensBase64=new LinkedHashSet<>();    tokensBase64.addAll(getTrimmedStringCollection(System.getProperty(HADOOP_TOKENS)));    tokensBase64.addAll(getTrimmedStringCollection(conf.get(HADOOP_TOKENS)));    tokensBase64.addAll(getTrimmedStringCollection(System.getenv(HADOOP_TOKEN)));    int numTokenBase64=0;    for (    String tokenBase64 : tokensBase64) {      if (tokenBase64 != null && tokenBase64.length() > 0) {        try {          Token<TokenIdentifier> token=new Token<>();          token.decodeFromUrlString(tokenBase64);          Credentials cred=new Credentials();          cred.addToken(token.getService(),token);          loginUser.addCredentials(cred);          numTokenBase64++;        } catch (        IOException ioe) {
    tokensBase64.addAll(getTrimmedStringCollection(conf.get(HADOOP_TOKENS)));    tokensBase64.addAll(getTrimmedStringCollection(System.getenv(HADOOP_TOKEN)));    int numTokenBase64=0;    for (    String tokenBase64 : tokensBase64) {      if (tokenBase64 != null && tokenBase64.length() > 0) {        try {          Token<TokenIdentifier> token=new Token<>();          token.decodeFromUrlString(tokenBase64);          Credentials cred=new Credentials();          cred.addToken(token.getService(),token);          loginUser.addCredentials(cred);          numTokenBase64++;        } catch (        IOException ioe) {          LOG.error("Cannot add token {}: {}",tokenBase64,ioe.getMessage());        }      }    }    if (numTokenBase64 > 0) {
@InterfaceAudience.Public @InterfaceStability.Evolving public <T>T doAs(PrivilegedAction<T> action){  if (LOG.isDebugEnabled()) {
@InterfaceAudience.Public @InterfaceStability.Evolving public <T>T doAs(PrivilegedExceptionAction<T> action) throws IOException, InterruptedException {  try {    if (LOG.isDebugEnabled()) {
@InterfaceAudience.LimitedPrivate({"HDFS","KMS"}) @InterfaceStability.Unstable public static void logUserInfo(Logger log,String caption,UserGroupInformation ugi) throws IOException {  if (log.isDebugEnabled()) {
@InterfaceAudience.LimitedPrivate({"HDFS","KMS"}) @InterfaceStability.Unstable public static void logUserInfo(Logger log,String caption,UserGroupInformation ugi) throws IOException {  if (log.isDebugEnabled()) {    log.debug(caption + " UGI: " + ugi);    for (    Token<?> token : ugi.getTokens()) {
protected void initFileSystem(URI keystoreUri) throws IOException {  path=ProviderUtils.unnestUri(keystoreUri);  if (LOG.isDebugEnabled()) {
@Override protected void initFileSystem(URI uri) throws IOException {  super.initFileSystem(uri);  try {    file=new File(new URI(getPath().toString()));    if (LOG.isDebugEnabled()) {
@Override protected void initFileSystem(URI uri) throws IOException {  super.initFileSystem(uri);  try {    file=new File(new URI(getPath().toString()));    if (LOG.isDebugEnabled()) {      LOG.debug("initialized local file as '" + file + "'.");      if (file.exists()) {
@Override protected void doFilter(FilterChain filterChain,HttpServletRequest request,HttpServletResponse response) throws IOException, ServletException {  final HttpServletRequest lowerCaseRequest=toLowerCase(request);  String doAsUser=lowerCaseRequest.getParameter(DO_AS);  if (doAsUser != null && !doAsUser.equals(request.getRemoteUser())) {
      String clientKey=krbInfo.clientPrincipal();      if (clientKey != null && !clientKey.isEmpty()) {        try {          clientPrincipal=SecurityUtil.getServerPrincipal(conf.get(clientKey),addr);        } catch (        IOException e) {          throw (AuthorizationException)new AuthorizationException("Can't figure out Kerberos principal name for connection from " + addr + " for user="+ user+ " protocol="+ protocol).initCause(e);        }      }    }  }  if ((clientPrincipal != null && !clientPrincipal.equals(user.getUserName())) || acls.length != 2 || !acls[0].isUserAllowed(user) || acls[1].isUserAllowed(user)) {    String cause=clientPrincipal != null ? ": this service is only accessible by " + clientPrincipal : ": denied by configured ACL";    AUDITLOG.warn(AUTHZ_FAILED_FOR + user + " for protocol="+ protocol+ cause);    throw new AuthorizationException("User " + user + " is not authorized for protocol "+ protocol+ cause);  }  if (addr != null) {    String hostAddress=addr.getHostAddress();    if (hosts.length != 2 || !hosts[0].includes(hostAddress) || hosts[1].includes(hostAddress)) {      AUDITLOG.warn(AUTHZ_FAILED_FOR + " for protocol=" + protocol+ " from host = "+ hostAddress);      throw new AuthorizationException("Host " + hostAddress + " is not authorized for protocol "+ protocol);
    if (LOG.isDebugEnabled()) {      LOG.debug("Header origin is null. Returning");    }    return;  }  if (!areOriginsAllowed(originsList)) {    if (LOG.isDebugEnabled()) {      LOG.debug("Header origins '" + originsList + "' not allowed. Returning");    }    return;  }  String accessControlRequestMethod=req.getHeader(ACCESS_CONTROL_REQUEST_METHOD);  if (!isMethodAllowed(accessControlRequestMethod)) {    if (LOG.isDebugEnabled()) {      LOG.debug("Access control method '" + accessControlRequestMethod + "' not allowed. Returning");    }    return;  }  String accessControlRequestHeaders=req.getHeader(ACCESS_CONTROL_REQUEST_HEADERS);  if (!areHeadersAllowed(accessControlRequestHeaders)) {    if (LOG.isDebugEnabled()) {
private void initializeSSLContext(SSLChannelMode preferredChannelMode) throws NoSuchAlgorithmException, KeyManagementException, IOException {
private String[] alterCipherList(String[] defaultCiphers){  ArrayList<String> preferredSuites=new ArrayList<>();  for (int i=0; i < defaultCiphers.length; i++) {    if (defaultCiphers[i].contains("_GCM_")) {
  String keystoreType=conf.get(resolvePropertyName(mode,SSL_KEYSTORE_TYPE_TPL_KEY),DEFAULT_KEYSTORE_TYPE);  KeyStore keystore=KeyStore.getInstance(keystoreType);  String keystoreKeyPassword=null;  if (requireClientCert || mode == SSLFactory.Mode.SERVER) {    String locationProperty=resolvePropertyName(mode,SSL_KEYSTORE_LOCATION_TPL_KEY);    String keystoreLocation=conf.get(locationProperty,"");    if (keystoreLocation.isEmpty()) {      throw new GeneralSecurityException("The property '" + locationProperty + "' has not been set in the ssl configuration file.");    }    String passwordProperty=resolvePropertyName(mode,SSL_KEYSTORE_PASSWORD_TPL_KEY);    String keystorePassword=getPassword(conf,passwordProperty,"");    if (keystorePassword.isEmpty()) {      throw new GeneralSecurityException("The property '" + passwordProperty + "' has not been set in the ssl configuration file.");    }    String keyPasswordProperty=resolvePropertyName(mode,SSL_KEYSTORE_KEYPASSWORD_TPL_KEY);    keystoreKeyPassword=getPassword(conf,keyPasswordProperty,keystorePassword);    if (LOG.isDebugEnabled()) {
      throw new GeneralSecurityException("The property '" + locationProperty + "' has not been set in the ssl configuration file.");    }    String passwordProperty=resolvePropertyName(mode,SSL_KEYSTORE_PASSWORD_TPL_KEY);    String keystorePassword=getPassword(conf,passwordProperty,"");    if (keystorePassword.isEmpty()) {      throw new GeneralSecurityException("The property '" + passwordProperty + "' has not been set in the ssl configuration file.");    }    String keyPasswordProperty=resolvePropertyName(mode,SSL_KEYSTORE_KEYPASSWORD_TPL_KEY);    keystoreKeyPassword=getPassword(conf,keyPasswordProperty,keystorePassword);    if (LOG.isDebugEnabled()) {      LOG.debug(mode.toString() + " KeyStore: " + keystoreLocation);    }    InputStream is=Files.newInputStream(Paths.get(keystoreLocation));    try {      keystore.load(is,keystorePassword.toCharArray());    }  finally {      is.close();    }    if (LOG.isDebugEnabled()) {
  finally {      is.close();    }    if (LOG.isDebugEnabled()) {      LOG.debug(mode.toString() + " Loaded KeyStore: " + keystoreLocation);    }  } else {    keystore.load(null,null);  }  KeyManagerFactory keyMgrFactory=KeyManagerFactory.getInstance(SSLFactory.SSLCERTIFICATE);  keyMgrFactory.init(keystore,(keystoreKeyPassword != null) ? keystoreKeyPassword.toCharArray() : null);  keyManagers=keyMgrFactory.getKeyManagers();  String truststoreType=conf.get(resolvePropertyName(mode,SSL_TRUSTSTORE_TYPE_TPL_KEY),DEFAULT_KEYSTORE_TYPE);  String locationProperty=resolvePropertyName(mode,SSL_TRUSTSTORE_LOCATION_TPL_KEY);  String truststoreLocation=conf.get(locationProperty,"");  if (!truststoreLocation.isEmpty()) {    String passwordProperty=resolvePropertyName(mode,SSL_TRUSTSTORE_PASSWORD_TPL_KEY);    String truststorePassword=getPassword(conf,passwordProperty,"");
      LOG.debug(mode.toString() + " Loaded KeyStore: " + keystoreLocation);    }  } else {    keystore.load(null,null);  }  KeyManagerFactory keyMgrFactory=KeyManagerFactory.getInstance(SSLFactory.SSLCERTIFICATE);  keyMgrFactory.init(keystore,(keystoreKeyPassword != null) ? keystoreKeyPassword.toCharArray() : null);  keyManagers=keyMgrFactory.getKeyManagers();  String truststoreType=conf.get(resolvePropertyName(mode,SSL_TRUSTSTORE_TYPE_TPL_KEY),DEFAULT_KEYSTORE_TYPE);  String locationProperty=resolvePropertyName(mode,SSL_TRUSTSTORE_LOCATION_TPL_KEY);  String truststoreLocation=conf.get(locationProperty,"");  if (!truststoreLocation.isEmpty()) {    String passwordProperty=resolvePropertyName(mode,SSL_TRUSTSTORE_PASSWORD_TPL_KEY);    String truststorePassword=getPassword(conf,passwordProperty,"");    if (truststorePassword.isEmpty()) {      truststorePassword=null;    }    long truststoreReloadInterval=conf.getLong(resolvePropertyName(mode,SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY),DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);
    keystore.load(null,null);  }  KeyManagerFactory keyMgrFactory=KeyManagerFactory.getInstance(SSLFactory.SSLCERTIFICATE);  keyMgrFactory.init(keystore,(keystoreKeyPassword != null) ? keystoreKeyPassword.toCharArray() : null);  keyManagers=keyMgrFactory.getKeyManagers();  String truststoreType=conf.get(resolvePropertyName(mode,SSL_TRUSTSTORE_TYPE_TPL_KEY),DEFAULT_KEYSTORE_TYPE);  String locationProperty=resolvePropertyName(mode,SSL_TRUSTSTORE_LOCATION_TPL_KEY);  String truststoreLocation=conf.get(locationProperty,"");  if (!truststoreLocation.isEmpty()) {    String passwordProperty=resolvePropertyName(mode,SSL_TRUSTSTORE_PASSWORD_TPL_KEY);    String truststorePassword=getPassword(conf,passwordProperty,"");    if (truststorePassword.isEmpty()) {      truststorePassword=null;    }    long truststoreReloadInterval=conf.getLong(resolvePropertyName(mode,SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY),DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);    if (LOG.isDebugEnabled()) {      LOG.debug(mode.toString() + " TrustStore: " + truststoreLocation);
X509TrustManager loadTrustManager() throws IOException, GeneralSecurityException {  X509TrustManager trustManager=null;  KeyStore ks=KeyStore.getInstance(type);  InputStream in=Files.newInputStream(file.toPath());  try {    ks.load(in,(password == null) ? null : password.toCharArray());    lastLoaded=file.lastModified();
private void disableExcludedCiphers(SSLEngine sslEngine){  String[] cipherSuites=sslEngine.getEnabledCipherSuites();  ArrayList<String> defaultEnabledCipherSuites=new ArrayList<String>(Arrays.asList(cipherSuites));  Iterator iterator=excludeCiphers.iterator();  while (iterator.hasNext()) {    String cipherName=(String)iterator.next();    if (defaultEnabledCipherSuites.contains(cipherName)) {      defaultEnabledCipherSuites.remove(cipherName);
public static void getTokenFile(File tokenFile,String fileFormat,Text alias,Text service,String url,String renewer,Configuration conf) throws Exception {  Token<?> token=null;  Credentials creds=tokenFile.exists() ? Credentials.readTokenStorageFile(tokenFile,conf) : new Credentials();  ServiceLoader<DtFetcher> loader=ServiceLoader.load(DtFetcher.class);  Iterator<DtFetcher> iterator=loader.iterator();  while (iterator.hasNext()) {    DtFetcher fetcher;    try {      fetcher=iterator.next();    } catch (    ServiceConfigurationError e) {      LOG.debug("Failed to load token fetcher implementation",e);      continue;    }    if (matchService(fetcher,service,url)) {      if (!fetcher.isTokenRequired()) {        String message="DtFetcher for service '" + service + "' does not require a token.  Check your configuration.  "+ "Note: security may be disabled or there may be two DtFetcher "+ "providers for the same service designation.";
    DtFetcher fetcher;    try {      fetcher=iterator.next();    } catch (    ServiceConfigurationError e) {      LOG.debug("Failed to load token fetcher implementation",e);      continue;    }    if (matchService(fetcher,service,url)) {      if (!fetcher.isTokenRequired()) {        String message="DtFetcher for service '" + service + "' does not require a token.  Check your configuration.  "+ "Note: security may be disabled or there may be two DtFetcher "+ "providers for the same service designation.";        LOG.error(message);        throw new IllegalArgumentException(message);      }      token=fetcher.addDelegationTokens(conf,creds,renewer,stripPrefix(url));    }  }  if (alias != null) {    if (token == null) {      String message="DtFetcher for service '" + service + "'"+ " does not allow aliasing.  Cannot apply alias '"+ alias+ "'."+ "  Drop alias flag to get token for this service.";
      LOG.debug("Failed to load token fetcher implementation",e);      continue;    }    if (matchService(fetcher,service,url)) {      if (!fetcher.isTokenRequired()) {        String message="DtFetcher for service '" + service + "' does not require a token.  Check your configuration.  "+ "Note: security may be disabled or there may be two DtFetcher "+ "providers for the same service designation.";        LOG.error(message);        throw new IllegalArgumentException(message);      }      token=fetcher.addDelegationTokens(conf,creds,renewer,stripPrefix(url));    }  }  if (alias != null) {    if (token == null) {      String message="DtFetcher for service '" + service + "'"+ " does not allow aliasing.  Cannot apply alias '"+ alias+ "'."+ "  Drop alias flag to get token for this service.";      LOG.error(message);      throw new IOException(message);    }    Token<?> aliasedToken=token.copyToken();    aliasedToken.setService(alias);
public static void removeTokenFromFile(boolean cancel,File tokenFile,String fileFormat,Text alias,Configuration conf) throws IOException, InterruptedException {  Credentials newCreds=new Credentials();  Credentials creds=Credentials.readTokenStorageFile(tokenFile,conf);  for (  Token<?> token : creds.getAllTokens()) {    if (matchAlias(token,alias)) {      if (token.isManaged() && cancel) {        token.cancel(conf);
public static void renewTokenFile(File tokenFile,String fileFormat,Text alias,Configuration conf) throws IOException, InterruptedException {  Credentials creds=Credentials.readTokenStorageFile(tokenFile,conf);  for (  Token<?> token : creds.getAllTokens()) {    if (token.isManaged() && matchAlias(token,alias)) {      long result=token.renew(conf);
@Override protected synchronized byte[] createPassword(TokenIdent identifier){  int sequenceNum;  long now=Time.now();  sequenceNum=incrementDelegationTokenSeqNum();  identifier.setIssueDate(now);  identifier.setMaxDate(now + tokenMaxLifetime);  identifier.setMasterKeyId(currentKey.getKeyId());  identifier.setSequenceNumber(sequenceNum);
public synchronized long renewToken(Token<TokenIdent> token,String renewer) throws InvalidToken, IOException {  ByteArrayInputStream buf=new ByteArrayInputStream(token.getIdentifier());  DataInputStream in=new DataInputStream(buf);  TokenIdent id=createIdentifier();  id.readFields(in);
public synchronized TokenIdent cancelToken(Token<TokenIdent> token,String canceller) throws IOException {  ByteArrayInputStream buf=new ByteArrayInputStream(token.getIdentifier());  DataInputStream in=new DataInputStream(buf);  TokenIdent id=createIdentifier();  id.readFields(in);
protected void logExpireTokens(Collection<TokenIdent> expiredTokens) throws IOException {  for (  TokenIdent ident : expiredTokens) {    logExpireToken(ident);
 catch (    Exception e) {      throw new IOException("Could not start Curator Framework",e);    }  } else {    CuratorFramework nullNsFw=zkClient.usingNamespace(null);    EnsurePath ensureNs=nullNsFw.newNamespaceAwareEnsurePath("/" + zkClient.getNamespace());    try {      ensureNs.ensure(nullNsFw.getZookeeperClient());    } catch (    Exception e) {      throw new IOException("Could not create namespace",e);    }  }  listenerThreadPool=Executors.newSingleThreadExecutor();  try {    delTokSeqCounter=new SharedCount(zkClient,ZK_DTSM_SEQNUM_ROOT,0);    if (delTokSeqCounter != null) {      delTokSeqCounter.start();    }    currentSeqNum=incrSharedCount(delTokSeqCounter,seqNumBatchSize);
private void loadFromZKCache(final boolean isTokenCache){  final String cacheName=isTokenCache ? "token" : "key";
  final String cacheName=isTokenCache ? "token" : "key";  LOG.info("Starting to load {} cache.",cacheName);  final List<ChildData> children;  if (isTokenCache) {    children=tokenCache.getCurrentData();  } else {    children=keyCache.getCurrentData();  }  int count=0;  for (  ChildData child : children) {    try {      if (isTokenCache) {        processTokenAddOrUpdate(child.getData());      } else {        processKeyAddOrUpdate(child.getData());      }    } catch (    Exception e) {
  LOG.info("Starting to load {} cache.",cacheName);  final List<ChildData> children;  if (isTokenCache) {    children=tokenCache.getCurrentData();  } else {    children=keyCache.getCurrentData();  }  int count=0;  for (  ChildData child : children) {    try {      if (isTokenCache) {        processTokenAddOrUpdate(child.getData());      } else {        processKeyAddOrUpdate(child.getData());      }    } catch (    Exception e) {      LOG.info("Ignoring node {} because it failed to load.",child.getPath());
  }  int count=0;  for (  ChildData child : children) {    try {      if (isTokenCache) {        processTokenAddOrUpdate(child.getData());      } else {        processKeyAddOrUpdate(child.getData());      }    } catch (    Exception e) {      LOG.info("Ignoring node {} because it failed to load.",child.getPath());      LOG.debug("Failure exception:",e);      ++count;    }  }  if (isTokenCache) {    syncTokenOwnerStats();  }  if (count > 0) {    LOG.warn("Ignored {} nodes while loading {} cache.",count,cacheName);
  super.stopThreads();  try {    if (tokenCache != null) {      tokenCache.close();    }  } catch (  Exception e) {    LOG.error("Could not stop Delegation Token Cache",e);  }  try {    if (delTokSeqCounter != null) {      delTokSeqCounter.close();    }  } catch (  Exception e) {    LOG.error("Could not stop Delegation Token Counter",e);  }  try {    if (keyIdSeqCounter != null) {      keyIdSeqCounter.close();    }  } catch (  Exception e) {
  } catch (  Exception e) {    LOG.error("Could not stop Delegation Token Cache",e);  }  try {    if (delTokSeqCounter != null) {      delTokSeqCounter.close();    }  } catch (  Exception e) {    LOG.error("Could not stop Delegation Token Counter",e);  }  try {    if (keyIdSeqCounter != null) {      keyIdSeqCounter.close();    }  } catch (  Exception e) {    LOG.error("Could not stop Key Id Counter",e);  }  try {    if (keyCache != null) {      keyCache.close();
    if (delTokSeqCounter != null) {      delTokSeqCounter.close();    }  } catch (  Exception e) {    LOG.error("Could not stop Delegation Token Counter",e);  }  try {    if (keyIdSeqCounter != null) {      keyIdSeqCounter.close();    }  } catch (  Exception e) {    LOG.error("Could not stop Key Id Counter",e);  }  try {    if (keyCache != null) {      keyCache.close();    }  } catch (  Exception e) {    LOG.error("Could not stop KeyCache",e);  }  try {
@Override protected int incrementDelegationTokenSeqNum(){  if (currentSeqNum >= currentMaxSeqNum) {    try {      currentSeqNum=incrSharedCount(delTokSeqCounter,seqNumBatchSize);      currentMaxSeqNum=currentSeqNum + seqNumBatchSize;
    byte[] data=zkClient.getData().forPath(nodePath);    if ((data == null) || (data.length == 0)) {      return null;    }    ByteArrayInputStream bin=new ByteArrayInputStream(data);    DataInputStream din=new DataInputStream(bin);    createIdentifier().readFields(din);    long renewDate=din.readLong();    int pwdLen=din.readInt();    byte[] password=new byte[pwdLen];    int numRead=din.read(password,0,pwdLen);    if (numRead > -1) {      DelegationTokenInformation tokenInfo=new DelegationTokenInformation(renewDate,password);      return tokenInfo;    }  } catch (  KeeperException.NoNodeException e) {    if (!quiet) {
private void addOrUpdateDelegationKey(DelegationKey key,boolean isUpdate) throws IOException {  String nodeCreatePath=getNodePath(ZK_DTSM_MASTER_KEY_ROOT,DELEGATION_KEY_PREFIX + key.getKeyId());  ByteArrayOutputStream os=new ByteArrayOutputStream();  DataOutputStream fsOut=new DataOutputStream(os);  if (LOG.isDebugEnabled()) {
private void addOrUpdateDelegationKey(DelegationKey key,boolean isUpdate) throws IOException {  String nodeCreatePath=getNodePath(ZK_DTSM_MASTER_KEY_ROOT,DELEGATION_KEY_PREFIX + key.getKeyId());  ByteArrayOutputStream os=new ByteArrayOutputStream();  DataOutputStream fsOut=new DataOutputStream(os);  if (LOG.isDebugEnabled()) {    LOG.debug("Storing ZKDTSMDelegationKey_" + key.getKeyId());  }  key.write(fsOut);  try {    if (zkClient.checkExists().forPath(nodeCreatePath) != null) {      zkClient.setData().forPath(nodeCreatePath,os.toByteArray()).setVersion(-1);      if (!isUpdate) {        LOG.debug("Key with path [" + nodeCreatePath + "] already exists.. Updating !!");      }    } else {      zkClient.create().withMode(CreateMode.PERSISTENT).forPath(nodeCreatePath,os.toByteArray());      if (isUpdate) {
  ByteArrayOutputStream os=new ByteArrayOutputStream();  DataOutputStream fsOut=new DataOutputStream(os);  if (LOG.isDebugEnabled()) {    LOG.debug("Storing ZKDTSMDelegationKey_" + key.getKeyId());  }  key.write(fsOut);  try {    if (zkClient.checkExists().forPath(nodeCreatePath) != null) {      zkClient.setData().forPath(nodeCreatePath,os.toByteArray()).setVersion(-1);      if (!isUpdate) {        LOG.debug("Key with path [" + nodeCreatePath + "] already exists.. Updating !!");      }    } else {      zkClient.create().withMode(CreateMode.PERSISTENT).forPath(nodeCreatePath,os.toByteArray());      if (isUpdate) {        LOG.debug("Updating non existent Key path [" + nodeCreatePath + "].. Adding new !!");      }    }  } catch (  KeeperException.NodeExistsException ne) {
@Override protected void removeStoredMasterKey(DelegationKey key){  String nodeRemovePath=getNodePath(ZK_DTSM_MASTER_KEY_ROOT,DELEGATION_KEY_PREFIX + key.getKeyId());  if (LOG.isDebugEnabled()) {
protected void removeStoredToken(TokenIdent ident,boolean checkAgainstZkBeforeDeletion) throws IOException {  String nodeRemovePath=getNodePath(ZK_DTSM_TOKENS_ROOT,DELEGATION_TOKEN_PREFIX + ident.getSequenceNumber());  try {    DelegationTokenInformation dtInfo=getTokenInfoFromZK(ident,true);    if (dtInfo != null) {      if (checkAgainstZkBeforeDeletion && dtInfo.getRenewDate() > now()) {
  String nodeRemovePath=getNodePath(ZK_DTSM_TOKENS_ROOT,DELEGATION_TOKEN_PREFIX + ident.getSequenceNumber());  try {    DelegationTokenInformation dtInfo=getTokenInfoFromZK(ident,true);    if (dtInfo != null) {      if (checkAgainstZkBeforeDeletion && dtInfo.getRenewDate() > now()) {        LOG.info("Node already renewed by peer " + nodeRemovePath + " so this token should not be deleted");        return;      }      if (LOG.isDebugEnabled()) {        LOG.debug("Removing ZKDTSMDelegationToken_" + ident.getSequenceNumber());      }      while (zkClient.checkExists().forPath(nodeRemovePath) != null) {        try {          zkClient.delete().guaranteed().forPath(nodeRemovePath);        } catch (        NoNodeException nne) {          LOG.debug("Node already deleted by peer " + nodeRemovePath);        }      }    } else {
protected void addOrUpdateToken(TokenIdent ident,DelegationTokenInformation info,boolean isUpdate) throws Exception {  String nodeCreatePath=getNodePath(ZK_DTSM_TOKENS_ROOT,DELEGATION_TOKEN_PREFIX + ident.getSequenceNumber());  try (ByteArrayOutputStream tokenOs=new ByteArrayOutputStream();DataOutputStream tokenOut=new DataOutputStream(tokenOs)){    ident.write(tokenOut);    tokenOut.writeLong(info.getRenewDate());    tokenOut.writeInt(info.getPassword().length);    tokenOut.write(info.getPassword());    if (LOG.isDebugEnabled()) {
@SuppressWarnings("unchecked") public HttpURLConnection openConnection(URL url,Token token,String doAs) throws IOException, AuthenticationException {  Preconditions.checkNotNull(url,"url");  Preconditions.checkNotNull(token,"token");  Map<String,String> extraParams=new HashMap<String,String>();  org.apache.hadoop.security.token.Token<? extends TokenIdentifier> dToken=null;
@SuppressWarnings("unchecked") public HttpURLConnection openConnection(URL url,Token token,String doAs) throws IOException, AuthenticationException {  Preconditions.checkNotNull(url,"url");  Preconditions.checkNotNull(token,"token");  Map<String,String> extraParams=new HashMap<String,String>();  org.apache.hadoop.security.token.Token<? extends TokenIdentifier> dToken=null;  LOG.debug("Connecting to url {} with token {} as {}",url,token,doAs);  if (!token.isSet()) {    Credentials creds=UserGroupInformation.getCurrentUser().getCredentials();    if (LOG.isDebugEnabled()) {
@InterfaceAudience.Private public org.apache.hadoop.security.token.Token<? extends TokenIdentifier> selectDelegationToken(URL url,Credentials creds){  final InetSocketAddress serviceAddr=new InetSocketAddress(url.getHost(),url.getPort());  final Text service=SecurityUtil.buildTokenService(serviceAddr);  org.apache.hadoop.security.token.Token<? extends TokenIdentifier> dToken=creds.getToken(service);
  AuthenticationToken authToken=(AuthenticationToken)request.getUserPrincipal();  if (authToken != null && authToken != AuthenticationToken.ANONYMOUS) {    ugi=(UserGroupInformation)request.getAttribute(DelegationTokenAuthenticationHandler.DELEGATION_TOKEN_UGI_ATTRIBUTE);    if (ugi == null) {      String realUser=request.getUserPrincipal().getName();      ugi=UserGroupInformation.createRemoteUser(realUser,handlerAuthMethod);      String doAsUser=getDoAs(request);      if (doAsUser != null) {        ugi=UserGroupInformation.createProxyUser(doAsUser,ugi);        try {          ProxyUsers.authorize(ugi,request.getRemoteAddr());        } catch (        AuthorizationException ex) {          HttpExceptionUtils.createServletExceptionResponse(response,HttpServletResponse.SC_FORBIDDEN,ex);          requestCompleted=true;          if (LOG.isDebugEnabled()) {
@SuppressWarnings("unchecked") @Override public AuthenticationToken authenticate(HttpServletRequest request,HttpServletResponse response) throws IOException, AuthenticationException {  AuthenticationToken token;  String delegationParam=getDelegationToken(request);  if (delegationParam != null) {
  String delegationParam=getDelegationToken(request);  if (delegationParam != null) {    LOG.debug("Authenticating with dt param: {}",delegationParam);    try {      Token<AbstractDelegationTokenIdentifier> dt=new Token();      dt.decodeFromUrlString(delegationParam);      UserGroupInformation ugi=tokenManager.verifyToken(dt);      final String shortName=ugi.getShortUserName();      token=new AuthenticationToken(shortName,ugi.getUserName(),getType());      token.setExpires(0);      request.setAttribute(DELEGATION_TOKEN_UGI_ATTRIBUTE,ugi);    } catch (    Throwable ex) {      token=null;      HttpExceptionUtils.createServletExceptionResponse(response,HttpServletResponse.SC_FORBIDDEN,new AuthenticationException(ex));    }  } else {
@Override public void authenticate(URL url,AuthenticatedURL.Token token) throws IOException, AuthenticationException {  if (!hasDelegationToken(url,token)) {    try {      UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();
@SuppressWarnings("unchecked") public Token<? extends AbstractDelegationTokenIdentifier> createToken(UserGroupInformation ugi,String renewer,String service){
@SuppressWarnings("unchecked") public long renewToken(Token<? extends AbstractDelegationTokenIdentifier> token,String renewer) throws IOException {
@SuppressWarnings("unchecked") public void cancelToken(Token<? extends AbstractDelegationTokenIdentifier> token,String canceler) throws IOException {
protected final void noteFailure(Exception exception){
protected void addService(Service service){  if (LOG.isDebugEnabled()) {
protected boolean removeService(Service service){
protected void serviceInit(Configuration conf) throws Exception {  List<Service> services=getServices();  if (LOG.isDebugEnabled()) {
protected void serviceStart() throws Exception {  List<Service> services=getServices();  if (LOG.isDebugEnabled()) {
protected void serviceStop() throws Exception {  int numOfServicesToStop=serviceList.size();  if (LOG.isDebugEnabled()) {
private void stop(int numOfServicesStarted,boolean stopOnlyStartedServices){  Exception firstException=null;  List<Service> services=getServices();  for (int i=numOfServicesStarted - 1; i >= 0; i--) {    Service service=services.get(i);    if (LOG.isDebugEnabled()) {
@Override public void stateChanged(Service service){
@Override public Configuration bindArgs(Configuration config,List<String> args) throws Exception {  if (LOG.isDebugEnabled()) {
@Override public Configuration bindArgs(Configuration config,List<String> args) throws Exception {  if (LOG.isDebugEnabled()) {    LOG.debug("Service {} passed in {} arguments:",getName(),args.size());    for (    String arg : args) {
@Override public void uncaughtException(Thread thread,Throwable exception){  if (ShutdownHookManager.get().isShutdownInProgress()) {
  if (ShutdownHookManager.get().isShutdownInProgress()) {    LOG.error("Thread {} threw an error during shutdown: {}.",thread.toString(),exception,exception);  } else   if (exception instanceof Error) {    try {      LOG.error("Thread {} threw an error: {}. Shutting down",thread.toString(),exception,exception);    } catch (    Throwable err) {    }    if (exception instanceof OutOfMemoryError) {      try {        System.err.println("Halting due to Out Of Memory Error...");      } catch (      Throwable err) {      }      ExitUtil.haltOnOutOfMemory((OutOfMemoryError)exception);    } else {      ExitUtil.ExitException ee=ServiceLauncher.convertToExitException(exception);      ExitUtil.terminate(ee.status,ee);    }  } else {
@Override public void handle(Signal s){  signalCount.incrementAndGet();  InterruptData data=new InterruptData(s.getName(),s.getNumber());
void noteException(ExitUtil.ExitException exitException){  int exitCode=exitException.getExitCode();  if (exitCode != 0) {
@VisibleForTesting public int loadConfigurationClasses(){  List<String> toCreate=getConfigurationsToCreate();  int loaded=0;  for (  String classname : toCreate) {    try {      Class<?> loadClass=getClassLoader().loadClass(classname);      Object instance=loadClass.getConstructor().newInstance();      if (!(instance instanceof Configuration)) {        throw new ExitUtil.ExitException(EXIT_SERVICE_CREATION_FAILURE,"Could not create " + classname + " because it is not a Configuration class/subclass");      }      loaded++;    } catch (    ClassNotFoundException e) {      LOG.debug("Failed to load {} because it is not on the classpath",classname);    }catch (    ExitUtil.ExitException e) {      throw e;    }catch (    Exception e) {
public ExitUtil.ExitException launchService(Configuration conf,S instance,List<String> processedArgs,boolean addShutdownHook,boolean execute){  ExitUtil.ExitException exitException;  try {    int exitCode=coreServiceLaunch(conf,instance,processedArgs,addShutdownHook,execute);    if (service != null) {      Throwable failure=service.getFailureCause();      if (failure != null) {        Service.STATE failureState=service.getFailureState();        if (failureState == Service.STATE.STOPPED) {
      Throwable failure=service.getFailureCause();      if (failure != null) {        Service.STATE failureState=service.getFailureState();        if (failureState == Service.STATE.STOPPED) {          LOG.debug("Failure during shutdown: {} ",failure,failure);        } else {          throw failure;        }      }    }    String name=getServiceName();    if (exitCode == 0) {      exitException=new ServiceLaunchException(exitCode,"%s succeeded",name);    } else {      exitException=new ServiceLaunchException(exitCode,"%s failed ",name);    }  } catch (  ExitUtil.ExitException ee) {    exitException=ee;  }catch (  Throwable thrown) {
  if (service instanceof LaunchableService) {    LOG.debug("Service {} implements LaunchableService",name);    launchableService=(LaunchableService)service;    if (launchableService.isInState(Service.STATE.INITED)) {      LOG.warn("LaunchableService {}" + " initialized in constructor before CLI arguments passed in",name);    }    Configuration newconf=launchableService.bindArgs(configuration,processedArgs);    if (newconf != null) {      configuration=newconf;    }  }  if (!service.isInState(Service.STATE.INITED)) {    service.init(configuration);  }  int exitCode;  try {    service.start();    exitCode=EXIT_SUCCESS;    if (execute && service.isInState(Service.STATE.STARTED)) {
@Override public void uncaughtException(Thread thread,Throwable exception){
protected void error(String message,Throwable thrown){  String text="Exception: " + message;  if (LOG.isErrorEnabled()) {
    argString.append("\"").append(arg).append("\" ");  }  LOG.debug("Command line: {}",argString);  try {    String[] argArray=args.toArray(new String[args.size()]);    GenericOptionsParser parser=createGenericOptionsParser(conf,argArray);    if (!parser.isParseSuccessful()) {      throw new ServiceLaunchException(EXIT_COMMAND_ARGUMENT_ERROR,E_PARSE_FAILED + " %s",argString);    }    CommandLine line=parser.getCommandLine();    List<String> remainingArgs=Arrays.asList(parser.getRemainingArgs());    LOG.debug("Remaining arguments {}",remainingArgs);    if (line.hasOption(ARG_CONF)) {      String[] filenames=line.getOptionValues(ARG_CONF);      verifyConfigurationFilesExist(filenames);      for (      String filename : filenames) {        File file=new File(filename);
  }  if (StringUtils.popOption("-h",args) || StringUtils.popOption("-help",args)) {    usage();    return 0;  } else   if (args.size() == 0) {    usage();    return 0;  }  String hostPort=StringUtils.popOptionWithArgument("-host",args);  if (hostPort == null) {    System.err.println("You must specify a host with -host.");    return 1;  }  if (args.isEmpty()) {    System.err.println("You must specify an operation.");    return 1;  }  String servicePrincipal=StringUtils.popOptionWithArgument("-principal",args);  if (servicePrincipal != null) {
public synchronized void removeSpanReceiver(long spanReceiverId) throws IOException {  SpanReceiver[] receivers=TracerPool.getGlobalTracerPool().getReceivers();  for (  SpanReceiver receiver : receivers) {    if (receiver.getId() == spanReceiverId) {      TracerPool.getGlobalTracerPool().removeAndCloseReceiver(receiver);
@Override public URL getResource(String name){  URL url=null;  if (!isSystemClass(name,systemClasses)) {    url=findResource(name);    if (url == null && name.startsWith("/")) {      if (LOG.isDebugEnabled()) {
@Override protected synchronized Class<?> loadClass(String name,boolean resolve) throws ClassNotFoundException {  if (LOG.isDebugEnabled()) {
    LOG.debug("Loading class: " + name);  }  Class<?> c=findLoadedClass(name);  ClassNotFoundException ex=null;  if (c == null && !isSystemClass(name,systemClasses)) {    try {      c=findClass(name);      if (LOG.isDebugEnabled() && c != null) {        LOG.debug("Loaded class: " + name + " ");      }    } catch (    ClassNotFoundException e) {      if (LOG.isDebugEnabled()) {        LOG.debug(e.toString());      }      ex=e;    }  }  if (c == null) {    c=parent.loadClass(name);    if (LOG.isDebugEnabled() && c != null) {
public static BlockingThreadPoolExecutorService newInstance(int activeTasks,int waitingTasks,long keepAliveTime,TimeUnit unit,String prefixName){  final BlockingQueue<Runnable> workQueue=new LinkedBlockingQueue<>(waitingTasks + activeTasks);  ThreadPoolExecutor eventProcessingExecutor=new ThreadPoolExecutor(activeTasks,activeTasks,keepAliveTime,unit,workQueue,newDaemonThreadFactory(prefixName),new RejectedExecutionHandler(){    @Override public void rejectedExecution(    Runnable r,    ThreadPoolExecutor executor){
public static synchronized void terminate(ExitException ee) throws ExitException {  int status=ee.getExitCode();  String msg=ee.getMessage();  if (status != 0) {
public static synchronized void terminate(ExitException ee) throws ExitException {  int status=ee.getExitCode();  String msg=ee.getMessage();  if (status != 0) {    LOG.debug("Exiting with status {}: {}",status,msg,ee);
public static synchronized void halt(HaltException ee) throws HaltException {  int status=ee.getExitCode();  String msg=ee.getMessage();  try {    if (status != 0) {
public static synchronized void halt(HaltException ee) throws HaltException {  int status=ee.getExitCode();  String msg=ee.getMessage();  try {    if (status != 0) {      LOG.debug("Halt with status {}: {}",status,msg,ee);
  try {    if (fileName != null) {      File file=new File(fileName);      if (file.exists()) {        try (Reader fileReader=new InputStreamReader(Files.newInputStream(file.toPath()),StandardCharsets.UTF_8);BufferedReader bufferedReader=new BufferedReader(fileReader)){          List<String> lines=new ArrayList<String>();          String line=null;          while ((line=bufferedReader.readLine()) != null) {            lines.add(line);          }          if (LOG.isDebugEnabled()) {            LOG.debug("Loaded IP list of size = " + lines.size() + " from file = "+ fileName);          }          return (lines.toArray(new String[lines.size()]));        }       } else {        LOG.debug("Missing ip list file : " + fileName);      }    }  } catch (  IOException ioe) {
  try {    DocumentBuilder db=builder.newDocumentBuilder();    dom=db.parse(fileInputStream);    Element doc=dom.getDocumentElement();    NodeList nodes=doc.getElementsByTagName("host");    for (int i=0; i < nodes.getLength(); i++) {      Node node=nodes.item(i);      if (node.getNodeType() == Node.ELEMENT_NODE) {        Element e=(Element)node;        String v=readFirstTagValue(e,"name");        String[] hosts=StringUtils.getTrimmedStrings(v);        String str=readFirstTagValue(e,"timeout");        Integer timeout=(str == null) ? null : Integer.parseInt(str);        for (        String host : hosts) {          map.put(host,timeout);
    dom=db.parse(fileInputStream);    Element doc=dom.getDocumentElement();    NodeList nodes=doc.getElementsByTagName("host");    for (int i=0; i < nodes.getLength(); i++) {      Node node=nodes.item(i);      if (node.getNodeType() == Node.ELEMENT_NODE) {        Element e=(Element)node;        String v=readFirstTagValue(e,"name");        String[] hosts=StringUtils.getTrimmedStrings(v);        String str=readFirstTagValue(e,"timeout");        Integer timeout=(str == null) ? null : Integer.parseInt(str);        for (        String host : hosts) {          map.put(host,timeout);          LOG.info("Adding a node \"" + host + "\" to the list of "+ type+ " hosts from "+ filename);        }      }    }  } catch (  IOException|SAXException|ParserConfigurationException e) {
private void refreshInternal(String includesFile,String excludesFile,boolean lazy) throws IOException {
public void setIncludesFile(String includesFile){
public void setExcludesFile(String excludesFile){
public void updateFileNames(String includesFile,String excludesFile){
public void updateFileNames(String includesFile,String excludesFile){  LOG.info("Setting the includes file to " + includesFile);
public static KeyProvider createKeyProvider(final Configuration conf,final String configKeyName) throws IOException {
@VisibleForTesting static int computeCapacity(long maxMemory,double percentage,String mapName){  if (percentage > 100.0 || percentage < 0.0) {    throw new HadoopIllegalArgumentException("Percentage " + percentage + " must be greater than or equal to 0 "+ " and less than or equal to 100");  }  if (maxMemory < 0) {    throw new HadoopIllegalArgumentException("Memory " + maxMemory + " must be greater than or equal to 0");  }  if (percentage == 0.0 || maxMemory == 0) {    return 0;  }  final String vmBit=System.getProperty("sun.arch.data.model");  final double percentDivisor=100.0 / percentage;  final double percentMemory=maxMemory / percentDivisor;  final int e1=(int)(Math.log(percentMemory) / Math.log(2.0) + 0.5);  final int e2=e1 - ("32".equals(vmBit) ? 2 : 3);  final int exponent=e2 < 0 ? 0 : e2 > 30 ? 30 : e2;  final int c=1 << exponent;  LOG.info("Computing capacity for map " + mapName);
  if (percentage > 100.0 || percentage < 0.0) {    throw new HadoopIllegalArgumentException("Percentage " + percentage + " must be greater than or equal to 0 "+ " and less than or equal to 100");  }  if (maxMemory < 0) {    throw new HadoopIllegalArgumentException("Memory " + maxMemory + " must be greater than or equal to 0");  }  if (percentage == 0.0 || maxMemory == 0) {    return 0;  }  final String vmBit=System.getProperty("sun.arch.data.model");  final double percentDivisor=100.0 / percentage;  final double percentMemory=maxMemory / percentDivisor;  final int e1=(int)(Math.log(percentMemory) / Math.log(2.0) + 0.5);  final int e2=e1 - ("32".equals(vmBit) ? 2 : 3);  final int exponent=e2 < 0 ? 0 : e2 > 30 ? 30 : e2;  final int c=1 << exponent;  LOG.info("Computing capacity for map " + mapName);  LOG.info("VM type       = " + vmBit + "-bit");
    throw new HadoopIllegalArgumentException("Percentage " + percentage + " must be greater than or equal to 0 "+ " and less than or equal to 100");  }  if (maxMemory < 0) {    throw new HadoopIllegalArgumentException("Memory " + maxMemory + " must be greater than or equal to 0");  }  if (percentage == 0.0 || maxMemory == 0) {    return 0;  }  final String vmBit=System.getProperty("sun.arch.data.model");  final double percentDivisor=100.0 / percentage;  final double percentMemory=maxMemory / percentDivisor;  final int e1=(int)(Math.log(percentMemory) / Math.log(2.0) + 0.5);  final int e2=e1 - ("32".equals(vmBit) ? 2 : 3);  final int exponent=e2 < 0 ? 0 : e2 > 30 ? 30 : e2;  final int c=1 << exponent;  LOG.info("Computing capacity for map " + mapName);  LOG.info("VM type       = " + vmBit + "-bit");  LOG.info(percentage + "% max memory " + StringUtils.TraditionalBinaryPrefix.long2String(maxMemory,"B",1)+ " = "+ StringUtils.TraditionalBinaryPrefix.long2String((long)percentMemory,"B",1));
public void info(String msg){  if (LOG != null) {
public void debug(Throwable t){  if (LOG != null) {
public void error(String msg){  if (LOG != null) {
    return false;  }  ShellCommandExecutor shexec=null;  boolean setsidSupported=true;  try {    String[] args={"setsid","bash","-c","echo $$"};    shexec=new ShellCommandExecutor(args);    shexec.execute();  } catch (  IOException ioe) {    LOG.debug("setsid is not available on this machine. So not using it.");    setsidSupported=false;  }catch (  SecurityException se) {    LOG.debug("setsid is not allowed to run by the JVM " + "security manager. So not using it.");    setsidSupported=false;  }catch (  Error err) {    if (err.getMessage() != null && err.getMessage().contains("posix_spawn is not " + "a supported process launch mechanism") && (Shell.FREEBSD || Shell.MAC)) {
private static void shutdownExecutor(final Configuration conf){  try {    EXECUTOR.shutdown();    long shutdownTimeout=getShutdownTimeout(conf);    if (!EXECUTOR.awaitTermination(shutdownTimeout,TIME_UNIT_DEFAULT)) {
@Override public void handle(Signal signal){
  if (registered) {    throw new IllegalStateException("Can't re-install the signal handlers.");  }  registered=true;  StringBuilder bld=new StringBuilder();  bld.append("registered UNIX signal handlers for [");  final String SIGNALS[]={"TERM","HUP","INT"};  String separator="";  for (  String signalName : SIGNALS) {    try {      new Handler(signalName,LOG);      bld.append(separator).append(signalName);      separator=", ";    } catch (    Exception e) {      LOG.debug(e);    }  }  bld.append("]");
static void startupShutdownMessage(Class<?> clazz,String[] args,final LogAdapter LOG){  final String hostname=NetUtils.getHostname();  final String classname=clazz.getSimpleName();
  }  try {    executorService.shutdown();    logger.debug("Gracefully shutting down executor service. Waiting max {} {}",timeout,unit);    if (!executorService.awaitTermination(timeout,unit)) {      logger.debug("Executor service has not shutdown yet. Forcing. " + "Will wait up to an additional {} {} for shutdown",timeout,unit);      executorService.shutdownNow();    }    if (executorService.awaitTermination(timeout,unit)) {      logger.debug("Succesfully shutdown executor service");    } else {      logger.error("Unable to shutdown executor service after timeout {} {}",(2 * timeout),unit);    }  } catch (  InterruptedException e) {    logger.error("Interrupted while attempting to shutdown",e);    executorService.shutdownNow();  }catch (  Exception e) {    logger.warn("Exception closing executor service {}",e.getMessage());
private void displayResults(){  LOG.info("Detailed results:");  LOG.info("----------------------------------\n");  for (int i=0; i < testsFromConfigFile.size(); i++) {    CLITestData td=testsFromConfigFile.get(i);    boolean testResult=td.getTestResult();    if (!testResult) {      LOG.info("-------------------------------------------");      LOG.info("                    Test ID: [" + (i + 1) + "]");
private void displayResults(){  LOG.info("Detailed results:");  LOG.info("----------------------------------\n");  for (int i=0; i < testsFromConfigFile.size(); i++) {    CLITestData td=testsFromConfigFile.get(i);    boolean testResult=td.getTestResult();    if (!testResult) {      LOG.info("-------------------------------------------");      LOG.info("                    Test ID: [" + (i + 1) + "]");      LOG.info("           Test Description: [" + td.getTestDesc() + "]");      LOG.info("");      ArrayList<CLICommand> testCommands=td.getTestCommands();      for (      CLICommand cmd : testCommands) {
  LOG.info("----------------------------------\n");  for (int i=0; i < testsFromConfigFile.size(); i++) {    CLITestData td=testsFromConfigFile.get(i);    boolean testResult=td.getTestResult();    if (!testResult) {      LOG.info("-------------------------------------------");      LOG.info("                    Test ID: [" + (i + 1) + "]");      LOG.info("           Test Description: [" + td.getTestDesc() + "]");      LOG.info("");      ArrayList<CLICommand> testCommands=td.getTestCommands();      for (      CLICommand cmd : testCommands) {        LOG.info("              Test Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<CLICommand> cleanupCommands=td.getCleanupCommands();      for (      CLICommand cmd : cleanupCommands) {
      LOG.info("-------------------------------------------");      LOG.info("                    Test ID: [" + (i + 1) + "]");      LOG.info("           Test Description: [" + td.getTestDesc() + "]");      LOG.info("");      ArrayList<CLICommand> testCommands=td.getTestCommands();      for (      CLICommand cmd : testCommands) {        LOG.info("              Test Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<CLICommand> cleanupCommands=td.getCleanupCommands();      for (      CLICommand cmd : cleanupCommands) {        LOG.info("           Cleanup Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<ComparatorData> compdata=td.getComparatorData();      for (      ComparatorData cd : compdata) {        boolean resultBoolean=cd.getTestResult();
      LOG.info("                    Test ID: [" + (i + 1) + "]");      LOG.info("           Test Description: [" + td.getTestDesc() + "]");      LOG.info("");      ArrayList<CLICommand> testCommands=td.getTestCommands();      for (      CLICommand cmd : testCommands) {        LOG.info("              Test Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<CLICommand> cleanupCommands=td.getCleanupCommands();      for (      CLICommand cmd : cleanupCommands) {        LOG.info("           Cleanup Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<ComparatorData> compdata=td.getComparatorData();      for (      ComparatorData cd : compdata) {        boolean resultBoolean=cd.getTestResult();        LOG.info("                 Comparator: [" + cd.getComparatorType() + "]");
      LOG.info("           Test Description: [" + td.getTestDesc() + "]");      LOG.info("");      ArrayList<CLICommand> testCommands=td.getTestCommands();      for (      CLICommand cmd : testCommands) {        LOG.info("              Test Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<CLICommand> cleanupCommands=td.getCleanupCommands();      for (      CLICommand cmd : cleanupCommands) {        LOG.info("           Cleanup Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<ComparatorData> compdata=td.getComparatorData();      for (      ComparatorData cd : compdata) {        boolean resultBoolean=cd.getTestResult();        LOG.info("                 Comparator: [" + cd.getComparatorType() + "]");        LOG.info("         Comparision result:   [" + (resultBoolean ? "pass" : "fail") + "]");
      LOG.info("");      ArrayList<CLICommand> testCommands=td.getTestCommands();      for (      CLICommand cmd : testCommands) {        LOG.info("              Test Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<CLICommand> cleanupCommands=td.getCleanupCommands();      for (      CLICommand cmd : cleanupCommands) {        LOG.info("           Cleanup Commands: [" + expandCommand(cmd.getCmd()) + "]");      }      LOG.info("");      ArrayList<ComparatorData> compdata=td.getComparatorData();      for (      ComparatorData cd : compdata) {        boolean resultBoolean=cd.getTestResult();        LOG.info("                 Comparator: [" + cd.getComparatorType() + "]");        LOG.info("         Comparision result:   [" + (resultBoolean ? "pass" : "fail") + "]");        LOG.info("            Expected output:   [" + expandCommand(cd.getExpectedOutput()) + "]");
  boolean overallResults=true;  int totalPass=0;  int totalFail=0;  int totalComparators=0;  for (int i=0; i < testsFromConfigFile.size(); i++) {    CLITestData td=testsFromConfigFile.get(i);    totalComparators+=testsFromConfigFile.get(i).getComparatorData().size();    boolean resultBoolean=td.getTestResult();    if (resultBoolean) {      totalPass++;    } else {      totalFail++;    }    overallResults&=resultBoolean;  }  LOG.info("               Testing mode: " + testMode);  LOG.info("");
  int totalPass=0;  int totalFail=0;  int totalComparators=0;  for (int i=0; i < testsFromConfigFile.size(); i++) {    CLITestData td=testsFromConfigFile.get(i);    totalComparators+=testsFromConfigFile.get(i).getComparatorData().size();    boolean resultBoolean=td.getTestResult();    if (resultBoolean) {      totalPass++;    } else {      totalFail++;    }    overallResults&=resultBoolean;  }  LOG.info("               Testing mode: " + testMode);  LOG.info("");  LOG.info("             Overall result: " + (overallResults ? "+++ PASS +++" : "--- FAIL ---"));
  int totalComparators=0;  for (int i=0; i < testsFromConfigFile.size(); i++) {    CLITestData td=testsFromConfigFile.get(i);    totalComparators+=testsFromConfigFile.get(i).getComparatorData().size();    boolean resultBoolean=td.getTestResult();    if (resultBoolean) {      totalPass++;    } else {      totalFail++;    }    overallResults&=resultBoolean;  }  LOG.info("               Testing mode: " + testMode);  LOG.info("");  LOG.info("             Overall result: " + (overallResults ? "+++ PASS +++" : "--- FAIL ---"));  if ((totalPass + totalFail) == 0) {    LOG.info("               # Tests pass: " + 0);
  } else {    LOG.info("               # Tests pass: " + totalPass + " ("+ (100 * totalPass / (totalPass + totalFail))+ "%)");    LOG.info("               # Tests fail: " + totalFail + " ("+ (100 * totalFail / (totalPass + totalFail))+ "%)");  }  LOG.info("         # Validations done: " + totalComparators + " (each test may do multiple validations)");  LOG.info("");  LOG.info("Failing tests:");  LOG.info("--------------");  int i=0;  boolean foundTests=false;  for (i=0; i < testsFromConfigFile.size(); i++) {    boolean resultBoolean=testsFromConfigFile.get(i).getTestResult();    if (!resultBoolean) {      LOG.info((i + 1) + ": " + testsFromConfigFile.get(i).getTestDesc());      foundTests=true;    }  }  if (!foundTests) {
private HashMap<String,String> extractMemberVariablesFromConfigurationFields(Field[] fields){  if (fields == null)   return null;  HashMap<String,String> retVal=new HashMap<>();  String propRegex="^[A-Za-z][A-Za-z0-9_-]+(\\.[A-Za-z0-9_-]+)+$";  Pattern p=Pattern.compile(propRegex);  String value;  for (  Field f : fields) {
    }    LOG_CONFIG.debug("  Value: {}",value);    if (value.endsWith(".xml") || value.endsWith(".") || value.endsWith("-"))     continue;    if (configurationPropsToSkipCompare.contains(value)) {      continue;    }    boolean skipPrefix=false;    for (    String cfgPrefix : configurationPrefixToSkipCompare) {      if (value.startsWith(cfgPrefix)) {        skipPrefix=true;        break;      }    }    if (skipPrefix) {      continue;    }    Matcher m=p.matcher(value);    if (!m.find()) {      LOG_CONFIG.debug("  Passes Regex: false");      continue;
private HashMap<String,String> extractPropertiesFromXml(String filename){  if (filename == null) {    return null;  }  Configuration conf=new Configuration(false);  conf.setAllowNullValueProperties(true);  conf.addResource(filename);  HashMap<String,String> retVal=new HashMap<>();  Iterator<Map.Entry<String,String>> kvItr=conf.iterator();  while (kvItr.hasNext()) {    Map.Entry<String,String> entry=kvItr.next();    String key=entry.getKey();    if (xmlPropsToSkipCompare.contains(key)) {      LOG_XML.debug("  Skipping Full Key: {}",key);      continue;    }    if (xmlPrefixToSkipCompare.stream().anyMatch(key::startsWith)) {
  Configuration conf=new Configuration(false);  conf.setAllowNullValueProperties(true);  conf.addResource(filename);  HashMap<String,String> retVal=new HashMap<>();  Iterator<Map.Entry<String,String>> kvItr=conf.iterator();  while (kvItr.hasNext()) {    Map.Entry<String,String> entry=kvItr.next();    String key=entry.getKey();    if (xmlPropsToSkipCompare.contains(key)) {      LOG_XML.debug("  Skipping Full Key: {}",key);      continue;    }    if (xmlPrefixToSkipCompare.stream().anyMatch(key::startsWith)) {      LOG_XML.debug("  Skipping Prefix Key: " + key);      continue;    }    if (conf.onlyKeyExists(key)) {
  Iterator<Map.Entry<String,String>> kvItr=conf.iterator();  while (kvItr.hasNext()) {    Map.Entry<String,String> entry=kvItr.next();    String key=entry.getKey();    if (xmlPropsToSkipCompare.contains(key)) {      LOG_XML.debug("  Skipping Full Key: {}",key);      continue;    }    if (xmlPrefixToSkipCompare.stream().anyMatch(key::startsWith)) {      LOG_XML.debug("  Skipping Prefix Key: " + key);      continue;    }    if (conf.onlyKeyExists(key)) {      retVal.put(key,null);      LOG_XML.debug("  XML Key,Null Value: " + key);    } else {      if (conf.get(key) != null) {
 else         if (f.getType().getName().equals("short")) {          short shValue=(short)f.get(null);          retVal.put(f.getName(),Integer.toString(shValue));        } else         if (f.getType().getName().equals("int")) {          int iValue=(int)f.get(null);          retVal.put(f.getName(),Integer.toString(iValue));        } else         if (f.getType().getName().equals("long")) {          long lValue=(long)f.get(null);          retVal.put(f.getName(),Long.toString(lValue));        } else         if (f.getType().getName().equals("float")) {          float fValue=(float)f.get(null);          retVal.put(f.getName(),Float.toString(fValue));        } else         if (f.getType().getName().equals("double")) {          double dValue=(double)f.get(null);          retVal.put(f.getName(),Double.toString(dValue));
          retVal.put(f.getName(),Integer.toString(shValue));        } else         if (f.getType().getName().equals("int")) {          int iValue=(int)f.get(null);          retVal.put(f.getName(),Integer.toString(iValue));        } else         if (f.getType().getName().equals("long")) {          long lValue=(long)f.get(null);          retVal.put(f.getName(),Long.toString(lValue));        } else         if (f.getType().getName().equals("float")) {          float fValue=(float)f.get(null);          retVal.put(f.getName(),Float.toString(fValue));        } else         if (f.getType().getName().equals("double")) {          double dValue=(double)f.get(null);          retVal.put(f.getName(),Double.toString(dValue));        } else         if (f.getType().getName().equals("boolean")) {          boolean bValue=(boolean)f.get(null);
@Test public void testCompareConfigurationClassAgainstXml(){  assertNotNull(xmlFilename);  assertNotNull(configurationClasses);  final int missingXmlSize=configurationFieldsMissingInXmlFile.size();  for (  Class c : configurationClasses) {
@Test public void testCompareConfigurationClassAgainstXml(){  assertNotNull(xmlFilename);  assertNotNull(configurationClasses);  final int missingXmlSize=configurationFieldsMissingInXmlFile.size();  for (  Class c : configurationClasses) {    LOG.info(c.toString());  }  LOG.info("({} member variables)\n",configurationMemberVariables.size());  StringBuilder xmlErrorMsg=new StringBuilder();  for (  Class c : configurationClasses) {    xmlErrorMsg.append(c);    xmlErrorMsg.append(" ");  }  xmlErrorMsg.append("has ");  xmlErrorMsg.append(missingXmlSize);  xmlErrorMsg.append(" variables missing in ");  xmlErrorMsg.append(xmlFilename);
private void appendMissingEntries(StringBuilder sb,Set<String> missing){  sb.append(" Entries: ");  new TreeSet<>(missing).forEach((s) -> {
@Test public void testCompareXmlAgainstConfigurationClass(){  assertNotNull(xmlFilename);  assertNotNull(configurationClasses);  final int missingConfigSize=xmlFieldsMissingInConfiguration.size();
@Test public void testCompareXmlAgainstConfigurationClass(){  assertNotNull(xmlFilename);  assertNotNull(configurationClasses);  final int missingConfigSize=xmlFieldsMissingInConfiguration.size();  LOG.info("File {} ({} properties)",xmlFilename,xmlKeyValueMap.size());  StringBuilder configErrorMsg=new StringBuilder();  configErrorMsg.append(xmlFilename);  configErrorMsg.append(" has ");  configErrorMsg.append(missingConfigSize);  configErrorMsg.append(" properties missing in");  Arrays.stream(configurationClasses).forEach(c -> configErrorMsg.append("  ").append(c));
      } else       if (defaultValueCheck3 != null) {        defaultConfigName=defaultNameCheck3;        defaultConfigValue=defaultValueCheck3;      }      if (defaultConfigValue != null) {        if (xmlDefaultValue == null) {          xmlPropertiesWithEmptyValue.add(xmlProperty);        } else         if (!xmlDefaultValue.equals(defaultConfigValue)) {          HashMap<String,String> xmlEntry=new HashMap<>();          xmlEntry.put(xmlProperty,xmlDefaultValue);          HashMap<String,String> configEntry=new HashMap<>();          configEntry.put(defaultConfigName,defaultConfigValue);          mismatchingXmlConfig.put(xmlEntry,configEntry);        } else {          xmlPropertiesMatchingConfigDefault.put(xmlProperty,defaultConfigName);        }      } else {
          HashMap<String,String> xmlEntry=new HashMap<>();          xmlEntry.put(xmlProperty,xmlDefaultValue);          HashMap<String,String> configEntry=new HashMap<>();          configEntry.put(defaultConfigName,defaultConfigValue);          mismatchingXmlConfig.put(xmlEntry,configEntry);        } else {          xmlPropertiesMatchingConfigDefault.put(xmlProperty,defaultConfigName);        }      } else {        configPropertiesWithNoDefaultConfig.add(configProperty);      }    }  }  LOG.info("{} has {} properties that do not match the default Config value",xmlFilename,mismatchingXmlConfig.size());  if (mismatchingXmlConfig.isEmpty()) {    LOG.info("  (None)");  } else {    for (    Map.Entry<HashMap<String,String>,HashMap<String,String>> xcEntry : mismatchingXmlConfig.entrySet()) {      xcEntry.getKey().forEach((key,value) -> {
          mismatchingXmlConfig.put(xmlEntry,configEntry);        } else {          xmlPropertiesMatchingConfigDefault.put(xmlProperty,defaultConfigName);        }      } else {        configPropertiesWithNoDefaultConfig.add(configProperty);      }    }  }  LOG.info("{} has {} properties that do not match the default Config value",xmlFilename,mismatchingXmlConfig.size());  if (mismatchingXmlConfig.isEmpty()) {    LOG.info("  (None)");  } else {    for (    Map.Entry<HashMap<String,String>,HashMap<String,String>> xcEntry : mismatchingXmlConfig.entrySet()) {      xcEntry.getKey().forEach((key,value) -> {        LOG.info("XML Property: {}",key);        LOG.info("XML Value:    {}",value);      });      xcEntry.getValue().forEach((key,value) -> {
 else {        configPropertiesWithNoDefaultConfig.add(configProperty);      }    }  }  LOG.info("{} has {} properties that do not match the default Config value",xmlFilename,mismatchingXmlConfig.size());  if (mismatchingXmlConfig.isEmpty()) {    LOG.info("  (None)");  } else {    for (    Map.Entry<HashMap<String,String>,HashMap<String,String>> xcEntry : mismatchingXmlConfig.entrySet()) {      xcEntry.getKey().forEach((key,value) -> {        LOG.info("XML Property: {}",key);        LOG.info("XML Value:    {}",value);      });      xcEntry.getValue().forEach((key,value) -> {        LOG.info("Config Name:  {}",key);        LOG.info("Config Value: {}",value);      });
@Test public void testDefaultValueCollision(){  for (  String filter : filtersForDefaultValueCollisionCheck) {
    RandomDatum key=generator.getKey();    RandomDatum value=generator.getValue();    key.write(data);    value.write(data);  }  LOG.info("Generated " + count + " records");  DataOutputBuffer encryptedDataBuffer=new DataOutputBuffer();  CryptoOutputStream out=new CryptoOutputStream(encryptedDataBuffer,encCodec,bufferSize,key,iv);  out.write(data.getData(),0,data.getLength());  out.flush();  out.close();  LOG.info("Finished encrypting data");  CryptoCodec decCodec=null;  try {    decCodec=(CryptoCodec)ReflectionUtils.newInstance(conf.getClassByName(decCodecClass),conf);  } catch (  ClassNotFoundException cnfe) {
private void waitForRefill(ValueQueue<?> valueQueue,String queueName,int queueSize) throws TimeoutException, InterruptedException {  GenericTestUtils.waitFor(() -> {    int size=valueQueue.getSize(queueName);    if (size != queueSize) {
@After public void tearDown() throws Exception {  if (fc != null) {    final Path testRoot=fileContextTestHelper.getAbsoluteTestRootPath(fc);
protected void describe(String text){
private void cleanupDir(Path p){  try {
protected final Path path(String pathString){  Path p=new Path(pathString).makeQualified(fs.getUri(),getTestBaseDir());
@Test public void testGetLongStatistics(){  Iterator<LongStatistic> iter=storageStatistics.getLongStatistics();  while (iter.hasNext()) {    final LongStatistic longStat=iter.next();    assertNotNull(longStat);    final long expectedStat=getStatisticsValue(longStat.getName());
@Test public void testGetLong(){  for (  String key : STATISTICS_KEYS) {    final long expectedStat=getStatisticsValue(key);    final long storageStat=storageStatistics.getLong(key);
  int errors=0;  for (  Method m : FileSystem.class.getDeclaredMethods()) {    if (Modifier.isStatic(m.getModifiers()) || Modifier.isPrivate(m.getModifiers()) || Modifier.isFinal(m.getModifiers())) {      continue;    }    try {      MustNotImplement.class.getMethod(m.getName(),m.getParameterTypes());      try {        FilterFileSystem.class.getDeclaredMethod(m.getName(),m.getParameterTypes());        LOG.error("FilterFileSystem MUST NOT implement " + m);        errors++;      } catch (      NoSuchMethodException ex) {      }    } catch (    NoSuchMethodException exc) {      try {        FilterFileSystem.class.getDeclaredMethod(m.getName(),m.getParameterTypes());      } catch (      NoSuchMethodException exc2) {
@Test public void testFilterFileSystem() throws Exception {  for (  Method m : AbstractFileSystem.class.getDeclaredMethods()) {    if (Modifier.isStatic(m.getModifiers()))     continue;    if (Modifier.isPrivate(m.getModifiers()))     continue;    if (Modifier.isFinal(m.getModifiers()))     continue;    try {      DontCheck.class.getMethod(m.getName(),m.getParameterTypes());
private int shellRun(String... args) throws Exception {  int exitCode=shell.run(args);
  shell.setConf(conf);  String[] args=new String[2];  args[0]="-ls";  args[1]="file:///";  int res=shell.run(args);  System.out.println("res =" + res);  shell.setConf(conf);  final ByteArrayOutputStream bytes=new ByteArrayOutputStream();  final PrintStream out=new PrintStream(bytes);  final PrintStream oldErr=System.err;  System.setErr(out);  final String results;  try {    int run=shell.run(args);    results=bytes.toString();
private int shellRun(String... args) throws Exception {  int exitCode=shell.run(args);
  int errors=0;  for (  Method m : FileSystem.class.getDeclaredMethods()) {    if (Modifier.isStatic(m.getModifiers()) || Modifier.isPrivate(m.getModifiers()) || Modifier.isFinal(m.getModifiers())) {      continue;    }    try {      MustNotImplement.class.getMethod(m.getName(),m.getParameterTypes());      try {        HarFileSystem.class.getDeclaredMethod(m.getName(),m.getParameterTypes());        LOG.error("HarFileSystem MUST not implement " + m);        errors++;      } catch (      NoSuchMethodException ex) {      }    } catch (    NoSuchMethodException exc) {      try {        HarFileSystem.class.getDeclaredMethod(m.getName(),m.getParameterTypes());      } catch (      NoSuchMethodException exc2) {
@Test public void testLocalFSsetOwner() throws IOException {  assumeNotWindows();  Configuration conf=new Configuration();  conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,"044");  LocalFileSystem localfs=FileSystem.getLocal(conf);  String filename="bar";  Path f=writeFile(localfs,filename);  List<String> groups;  try {    groups=getGroups();
@Test public void testSetUmaskInRealTime() throws Exception {  assumeNotWindows();  LocalFileSystem localfs=FileSystem.getLocal(new Configuration());  Configuration conf=localfs.getConf();  conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,"022");
  String testFilename="teststat2File";  Path path=new Path(testDir,testFilename);  RawLocalFileSystem rfs=new RawLocalFileSystem();  Configuration conf=new Configuration();  rfs.initialize(rfs.getUri(),conf);  rfs.createNewFile(path);  File file=rfs.pathToFile(path);  long defaultBlockSize=rfs.getDefaultBlockSize(path);  RawLocalFileSystem.DeprecatedRawLocalFileStatus fsNIO=new RawLocalFileSystem.DeprecatedRawLocalFileStatus(file,defaultBlockSize,rfs);  fsNIO.loadPermissionInfoByNativeIO();  RawLocalFileSystem.DeprecatedRawLocalFileStatus fsnonNIO=new RawLocalFileSystem.DeprecatedRawLocalFileStatus(file,defaultBlockSize,rfs);  fsnonNIO.loadPermissionInfoByNonNativeIO();  assertEquals(fsNIO.getOwner(),fsnonNIO.getOwner());  assertEquals(fsNIO.getGroup(),fsnonNIO.getGroup());  assertEquals(fsNIO.getPermission(),fsnonNIO.getPermission());
protected void verifyContents(final Path file,final MessageDigest origDigest,final int expectedLength) throws IOException {  ContractTestUtils.NanoTimer timer2=new ContractTestUtils.NanoTimer();  Assertions.assertThat(digest(file)).describedAs("digest of uploaded file %s",file).isEqualTo(origDigest.digest());  timer2.end("Completed digest",file);
  MessageDigest digest1=DigestUtils.getMd5Digest();  digest1.update(payload1);  UploadHandle upload1=startUpload(file);  Map<Integer,PartHandle> partHandles1=new HashMap<>();  int size2=size1 * 2;  int partId2=2;  byte[] payload2=generatePayload(partId1,size2);  MessageDigest digest2=DigestUtils.getMd5Digest();  digest2.update(payload2);  UploadHandle upload2;  try {    upload2=startUpload(file);    Assume.assumeTrue("The Filesystem is unexpectedly supporting concurrent uploads",concurrent);  } catch (  IOException e) {    if (!concurrent) {
@Test public void testRmEmptyRootDirRecursive() throws Throwable {  skipIfUnsupported(TEST_ROOT_TESTS_ENABLED);  Path root=new Path("/");  assertIsDirectory(root);  boolean deleted=getFileSystem().delete(root,true);
  skipIfUnsupported(TEST_ROOT_TESTS_ENABLED);  final Path root=new Path("/");  assertIsDirectory(root);  final FileSystem fs=getFileSystem();  final AtomicInteger iterations=new AtomicInteger(0);  final FileStatus[] originalChildren=listChildren(fs,root);  LambdaTestUtils.eventually(OBJECTSTORE_RETRY_TIMEOUT,new Callable<Void>(){    @Override public Void call() throws Exception {      FileStatus[] deleted=deleteChildren(fs,root,true);      FileStatus[] children=listChildren(fs,root);      if (children.length > 0) {        fail(String.format("After %d attempts: listing after rm /* not empty" + "\n%s\n%s\n%s",iterations.incrementAndGet(),dumpStats("final",children),dumpStats("deleted",deleted),dumpStats("original",originalChildren)));      }      return null;    }  },new LambdaTestUtils.ProportionalRetryInterval(50,1000));  boolean deleted=fs.delete(root,false);
@Test public void testRmRootRecursive() throws Throwable {  skipIfUnsupported(TEST_ROOT_TESTS_ENABLED);  Path root=new Path("/");  assertIsDirectory(root);  Path file=new Path("/testRmRootRecursive");  try {    ContractTestUtils.touch(getFileSystem(),file);    boolean deleted=getFileSystem().delete(root,true);    assertIsDirectory(root);
@Before public void setup() throws Exception {  Thread.currentThread().setName("setup");  LOG.debug("== Setup ==");  contract=createContract(createConfiguration());  contract.init();  assumeEnabled();  fileSystem=contract.getTestFileSystem();  assertNotNull("null filesystem",fileSystem);  URI fsURI=fileSystem.getUri();
protected void describe(String text){
public static FileStatus[] deleteChildren(FileSystem fileSystem,Path path,boolean recursive) throws IOException {
public static FileStatus[] deleteChildren(FileSystem fileSystem,Path path,boolean recursive) throws IOException {  LOG.debug("Deleting children of {} (recursive={})",path,recursive);  FileStatus[] children=listChildren(fileSystem,path);  for (  FileStatus entry : children) {
public static void noteAction(String action){  if (LOG.isDebugEnabled()) {
public static void skip(String message){
public static void bandwidth(NanoTimer timer,long bytes){
public static TreeScanResults treeWalk(FileSystem fs,Path path) throws IOException {  TreeScanResults dirsAndFiles=new TreeScanResults();  FileStatus[] statuses=fs.listStatus(path);  for (  FileStatus status : statuses) {
  barrier();  DFSClientThread[] threads=new DFSClientThread[numOfThreads];  for (int i=0; i < numOfThreads; i++) {    threads[i]=new DFSClientThread(i);    threads[i].start();  }  if (durations[0] > 0) {    if (durations.length == 1) {      while (shouldRun) {        Thread.sleep(2000);        totalTime+=2;        if (totalTime >= durations[0] || stopFileCreated()) {          shouldRun=false;        }      }    } else {      while (shouldRun) {        Thread.sleep(durations[currentIndex] * 1000);
@Test public void testGetMountPoints(){  ViewFileSystem viewfs=(ViewFileSystem)fsView;  MountPoint[] mountPoints=viewfs.getMountPoints();  for (  MountPoint mountPoint : mountPoints) {
  final Path actualMountLinkTarget=fsView.getLinkTarget(mountTargetSymLinkPath);  assertEquals("Resolved link target path not matching!",expectedMountLinkTarget,actualMountLinkTarget);  final String relativeFileName="dir2/../" + targetFileName;  final String link2FileName="dir2/rel.link";  final Path relTargetFile=new Path(targetTestRoot,relativeFileName);  final Path relativeSymLink=new Path(targetTestRoot,link2FileName);  fsTarget.createSymlink(relTargetFile,relativeSymLink,true);  final Path mountTargetRelativeSymLinkPath=new Path(mountTargetRootPath,link2FileName);  final Path expectedMountRelLinkTarget=fsTarget.makeQualified(new Path(targetTestRoot,relativeFileName));  final Path actualMountRelLinkTarget=fsView.getLinkTarget(mountTargetRelativeSymLinkPath);  assertEquals("Resolved relative link target path not matching!",expectedMountRelLinkTarget,actualMountRelLinkTarget);  try {    fsView.getLinkTarget(new Path("/linkToAFile"));    fail("Resolving link target for a ViewFs mount link should fail!");  } catch (  Exception e) {
  fsTarget.createSymlink(relTargetFile,relativeSymLink,true);  final Path mountTargetRelativeSymLinkPath=new Path(mountTargetRootPath,link2FileName);  final Path expectedMountRelLinkTarget=fsTarget.makeQualified(new Path(targetTestRoot,relativeFileName));  final Path actualMountRelLinkTarget=fsView.getLinkTarget(mountTargetRelativeSymLinkPath);  assertEquals("Resolved relative link target path not matching!",expectedMountRelLinkTarget,actualMountRelLinkTarget);  try {    fsView.getLinkTarget(new Path("/linkToAFile"));    fail("Resolving link target for a ViewFs mount link should fail!");  } catch (  Exception e) {    LOG.info("Expected exception: " + e);    assertThat(e.getMessage(),containsString("not a symbolic link"));  }  try {    fsView.getLinkTarget(fsView.makeQualified(new Path(mountTargetRootPath,targetFileName)));    fail("Resolving link target for a non sym link should fail!");  } catch (  Exception e) {
    fsView.getLinkTarget(new Path("/linkToAFile"));    fail("Resolving link target for a ViewFs mount link should fail!");  } catch (  Exception e) {    LOG.info("Expected exception: " + e);    assertThat(e.getMessage(),containsString("not a symbolic link"));  }  try {    fsView.getLinkTarget(fsView.makeQualified(new Path(mountTargetRootPath,targetFileName)));    fail("Resolving link target for a non sym link should fail!");  } catch (  Exception e) {    LOG.info("Expected exception: " + e);    assertThat(e.getMessage(),containsString("not a symbolic link"));  }  try {    fsView.getLinkTarget(new Path("/targetRoot/non-existing-file"));    fail("Resolving link target for a non existing link should fail!");  } catch (  Exception e) {
public static String send4LetterWord(String host,int port,String cmd) throws IOException {
public void expireActiveLockHolder(int idx) throws NoNodeException {  Stat stat=new Stat();  byte[] data=zks.getZKDatabase().getData(DummyZKFC.LOCK_ZNODE,stat,null);  assertArrayEquals(Ints.toByteArray(svcs.get(idx).index),data);  long session=stat.getEphemeralOwner();
private Object runTool(String... args) throws Exception {  errOutBytes.reset();  outBytes.reset();  LOG.info("Running: HAAdmin " + Joiner.on(" ").join(args));  int ret=tool.run(args);  errOutput=new String(errOutBytes.toByteArray(),Charsets.UTF_8);  output=new String(outBytes.toByteArray(),Charsets.UTF_8);
@Test(timeout=(STRESS_RUNTIME_SECS + EXTRA_TIMEOUT_SECS) * 1000) public void testExpireBackAndForth() throws Exception {  cluster.start();  long st=Time.now();  long runFor=STRESS_RUNTIME_SECS * 1000;  int i=0;  while (Time.now() - st < runFor) {    int from=i % 2;    int to=(i + 1) % 2;
@Test(timeout=(STRESS_RUNTIME_SECS + EXTRA_TIMEOUT_SECS) * 1000) public void testRandomExpirations() throws Exception {  cluster.start();  long st=Time.now();  long runFor=STRESS_RUNTIME_SECS * 1000;  Random r=new Random();  while (Time.now() - st < runFor) {    cluster.getTestContext().checkException();    int targetIdx=r.nextInt(2);    ActiveStandbyElector target=cluster.getElector(targetIdx);    long sessId=target.getZKSessionIdForTests();    if (sessId != -1) {
@BeforeClass public static void setup() throws Exception {  Configuration conf=new Configuration();  conf.setInt(HttpServer2.HTTP_MAX_THREADS_KEY,MAX_THREADS);  server=createTestServer(conf);  server.addServlet("echo","/echo",EchoServlet.class);  server.addServlet("echomap","/echomap",EchoMapServlet.class);  server.addServlet("htmlcontent","/htmlcontent",HtmlContentServlet.class);  server.addServlet("longheader","/longheader",LongHeaderServlet.class);  server.addJerseyResourcePackage(JerseyResource.class.getPackage().getName(),"/jersey/*");  server.start();  baseUrl=getServerURL(server);
@Test public void testJersey() throws Exception {  LOG.info("BEGIN testJersey()");  final String js=readOutput(new URL(baseUrl,"/jersey/foo?op=bar"));  final Map<String,Object> m=parse(js);
private void startServer(Configuration conf) throws Exception {  server=createTestServer(conf);  server.addJerseyResourcePackage(JerseyResource.class.getPackage().getName(),"/jersey/*");  server.start();  baseUrl=getServerURL(server);
  turnOnSSLDebugLogging();  storeHttpsCipherSuites();  Configuration conf=new Configuration();  conf.setInt(HttpServer2.HTTP_MAX_THREADS_KEY,10);  File base=new File(BASEDIR);  FileUtil.fullyDelete(base);  base.mkdirs();  keystoreDir=new File(BASEDIR).getAbsolutePath();  sslConfDir=KeyStoreTestUtil.getClasspathDir(TestSSLHttpServer.class);  KeyStoreTestUtil.setupSSLConfig(keystoreDir,sslConfDir,conf,false,true,EXCLUDED_CIPHERS);  Configuration sslConf=KeyStoreTestUtil.getSslConfig();  clientSslFactory=new SSLFactory(SSLFactory.Mode.CLIENT,sslConf);  clientSslFactory.init();  setupServer(conf,sslConf);  baseUrl=new URL("https://" + NetUtils.getHostPortString(server.getConnectorAddress(0)));
static void storeHttpsCipherSuites(){  String cipherSuites=System.getProperty(HTTPS_CIPHER_SUITES_KEY);  if (cipherSuites != null) {
static void restoreHttpsCipherSuites(){  if (cipherSuitesPropertyValue != null) {
private HttpsURLConnection getConnectionWithSSLSocketFactory(URL url,String ciphers) throws IOException, GeneralSecurityException {  HttpsURLConnection conn=(HttpsURLConnection)url.openConnection();  SSLSocketFactory sslSocketFactory=clientSslFactory.createSSLSocketFactory();
private HttpsURLConnection getConnectionWithPreferredProtocolSSLSocketFactory(URL url,String protocols) throws IOException, GeneralSecurityException {  HttpsURLConnection conn=(HttpsURLConnection)url.openConnection();  SSLSocketFactory sslSocketFactory=clientSslFactory.createSSLSocketFactory();
@GET @Path("{" + PATH + ":.*}") @Produces({MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8}) public Response get(@PathParam(PATH) @DefaultValue("UNKNOWN_" + PATH) final String path,@QueryParam(OP) @DefaultValue("UNKNOWN_" + OP) final String op) throws IOException {
private static RandomDatum[] generate(int count){  if (LOG.isDebugEnabled()) {
private static void writeTest(FileSystem fs,RandomDatum[] data,String file) throws IOException {  Configuration conf=new Configuration();  MapFile.delete(fs,file);  if (LOG.isDebugEnabled()) {
private static void readTest(FileSystem fs,RandomDatum[] data,String file,Configuration conf) throws IOException {  RandomDatum v=new RandomDatum();  if (LOG.isDebugEnabled()) {
private static void readTest(FileSystem fs,RandomDatum[] data,String file,Configuration conf) throws IOException {  RandomDatum v=new RandomDatum();  if (LOG.isDebugEnabled()) {    LOG.debug("reading " + data.length + " debug");  }  ArrayFile.Reader reader=new ArrayFile.Reader(fs,file,conf);  try {    for (int i=0; i < data.length; i++) {      reader.get(i,v);      if (!v.equals(data[i])) {        throw new RuntimeException("wrong value at " + i);      }    }    for (int i=data.length - 1; i >= 0; i--) {      reader.get(i,v);      if (!v.equals(data[i])) {        throw new RuntimeException("wrong value at " + i);      }    }    if (LOG.isDebugEnabled()) {
  Path fpath=null;  FileSystem fs=null;  try {    for (; i < args.length; i++) {      if (args[i] == null) {        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else {        file=args[i];        fpath=new Path(file);
  FileSystem fs=null;  try {    for (; i < args.length; i++) {      if (args[i] == null) {        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else {        file=args[i];        fpath=new Path(file);      }    }    fs=fpath.getFileSystem(conf);
  try {    for (; i < args.length; i++) {      if (args[i] == null) {        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else {        file=args[i];        fpath=new Path(file);      }    }    fs=fpath.getFileSystem(conf);    LOG.info("count = " + count);
    for (; i < args.length; i++) {      if (args[i] == null) {        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else {        file=args[i];        fpath=new Path(file);      }    }    fs=fpath.getFileSystem(conf);    LOG.info("count = " + count);    LOG.info("create = " + create);
@Test public void testWithJavaSerialization() throws Exception {  conf.set("io.serializations","org.apache.hadoop.io.serializer.JavaSerialization");  LOG.info("Testing DefaultStringifier with Serializable Integer");  Integer testInt=Integer.valueOf(42);  DefaultStringifier<Integer> stringifier=new DefaultStringifier<Integer>(conf,Integer.class);  String str=stringifier.toString(testInt);  Integer claimedInt=stringifier.fromString(str);
public void compressedSeqFileTest(CompressionCodec codec) throws Exception {  int count=1024 * 10;  int megabytes=1;  int factor=5;  Path file=new Path(GenericTestUtils.getTempPath("test.seq"));  Path recordCompressedFile=new Path(GenericTestUtils.getTempPath("test.rc.seq"));  Path blockCompressedFile=new Path(GenericTestUtils.getTempPath("test.bc.seq"));  int seed=new Random().nextInt();
@SuppressWarnings("deprecation") private void writeTest(FileSystem fs,int count,int seed,Path file,CompressionType compressionType,CompressionCodec codec) throws IOException {  fs.delete(file,true);
@SuppressWarnings("deprecation") private void readTest(FileSystem fs,int count,int seed,Path file) throws IOException {
    RandomDatum key=generator.getKey();    RandomDatum value=generator.getValue();    try {      if ((i % 5) == 0) {        rawKey.reset();        reader.nextRaw(rawKey,rawValue);      } else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {
    RandomDatum value=generator.getValue();    try {      if ((i % 5) == 0) {        rawKey.reset();        reader.nextRaw(rawKey,rawValue);      } else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);
    try {      if ((i % 5) == 0) {        rawKey.reset();        reader.nextRaw(rawKey,rawValue);      } else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);
      if ((i % 5) == 0) {        rawKey.reset();        reader.nextRaw(rawKey,rawValue);      } else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());
        rawKey.reset();        reader.nextRaw(rawKey,rawValue);      } else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());      LOG.info("Actual key = " + k);
        reader.nextRaw(rawKey,rawValue);      } else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());      LOG.info("Actual key = " + k);      LOG.info("Actual len = " + k.getLength());
      } else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());      LOG.info("Actual key = " + k);      LOG.info("Actual len = " + k.getLength());      LOG.info("Expected value = " + value);
 else {        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());      LOG.info("Actual key = " + k);      LOG.info("Actual len = " + k.getLength());      LOG.info("Expected value = " + value);
        if ((i % 2) == 0) {          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());      LOG.info("Actual key = " + k);      LOG.info("Actual len = " + k.getLength());      LOG.info("Expected value = " + value);      LOG.info("Expected len = " + value.getLength());
          reader.next(k);          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());      LOG.info("Actual key = " + k);      LOG.info("Actual len = " + k.getLength());      LOG.info("Expected value = " + value);      LOG.info("Expected len = " + value.getLength());      LOG.info("Actual value = " + v);
          reader.getCurrentValue(v);        } else {          reader.next(k,v);        }        if (!k.equals(key))         throw new RuntimeException("wrong key at " + i);        if (!v.equals(value))         throw new RuntimeException("wrong value at " + i);      }    } catch (    IOException ioe) {      LOG.info("Problem on row " + i);      LOG.info("Expected key = " + key);      LOG.info("Expected len = " + key.getLength());      LOG.info("Actual key = " + k);      LOG.info("Actual len = " + k.getLength());      LOG.info("Expected value = " + value);      LOG.info("Expected len = " + value.getLength());      LOG.info("Actual value = " + v);      LOG.info("Actual len = " + v.getLength());
private void sortTest(FileSystem fs,int count,int megabytes,int factor,boolean fast,Path file) throws IOException {  fs.delete(new Path(file + ".sorted"),true);  SequenceFile.Sorter sorter=newSorter(fs,fast,megabytes,factor);
private void sortTest(FileSystem fs,int count,int megabytes,int factor,boolean fast,Path file) throws IOException {  fs.delete(new Path(file + ".sorted"),true);  SequenceFile.Sorter sorter=newSorter(fs,fast,megabytes,factor);  LOG.debug("sorting " + count + " records");  sorter.sort(file,file.suffix(".sorted"));
@SuppressWarnings("deprecation") private void checkSort(FileSystem fs,int count,int seed,Path file) throws IOException {
    RandomDatum value=generator.getValue();    map.put(key,value);  }  LOG.debug("checking order of " + count + " records");  RandomDatum k=new RandomDatum();  RandomDatum v=new RandomDatum();  Iterator<Map.Entry<RandomDatum,RandomDatum>> iterator=map.entrySet().iterator();  SequenceFile.Reader reader=new SequenceFile.Reader(fs,file.suffix(".sorted"),conf);  for (int i=0; i < count; i++) {    Map.Entry<RandomDatum,RandomDatum> entry=iterator.next();    RandomDatum key=entry.getKey();    RandomDatum value=entry.getValue();    reader.next(k,v);    if (!k.equals(key))     throw new RuntimeException("wrong key at " + i);    if (!v.equals(value))     throw new RuntimeException("wrong value at " + i);  }  reader.close();
  Path file=new Path(GenericTestUtils.getTempPath("test.seq.metadata"));  Path sortedFile=new Path(GenericTestUtils.getTempPath("test.sorted.seq.metadata"));  Path recordCompressedFile=new Path(GenericTestUtils.getTempPath("test.rc.seq.metadata"));  Path blockCompressedFile=new Path(GenericTestUtils.getTempPath("test.bc.seq.metadata"));  FileSystem fs=FileSystem.getLocal(conf);  SequenceFile.Metadata theMetadata=new SequenceFile.Metadata();  theMetadata.set(new Text("name_1"),new Text("value_1"));  theMetadata.set(new Text("name_2"),new Text("value_2"));  theMetadata.set(new Text("name_3"),new Text("value_3"));  theMetadata.set(new Text("name_4"),new Text("value_4"));  int seed=new Random().nextInt();  try {    writeMetadataTest(fs,count,seed,file,CompressionType.NONE,null,theMetadata);    SequenceFile.Metadata aMetadata=readMetadata(fs,file);    if (!theMetadata.equals(aMetadata)) {
  Path sortedFile=new Path(GenericTestUtils.getTempPath("test.sorted.seq.metadata"));  Path recordCompressedFile=new Path(GenericTestUtils.getTempPath("test.rc.seq.metadata"));  Path blockCompressedFile=new Path(GenericTestUtils.getTempPath("test.bc.seq.metadata"));  FileSystem fs=FileSystem.getLocal(conf);  SequenceFile.Metadata theMetadata=new SequenceFile.Metadata();  theMetadata.set(new Text("name_1"),new Text("value_1"));  theMetadata.set(new Text("name_2"),new Text("value_2"));  theMetadata.set(new Text("name_3"),new Text("value_3"));  theMetadata.set(new Text("name_4"),new Text("value_4"));  int seed=new Random().nextInt();  try {    writeMetadataTest(fs,count,seed,file,CompressionType.NONE,null,theMetadata);    SequenceFile.Metadata aMetadata=readMetadata(fs,file);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());
  theMetadata.set(new Text("name_1"),new Text("value_1"));  theMetadata.set(new Text("name_2"),new Text("value_2"));  theMetadata.set(new Text("name_3"),new Text("value_3"));  theMetadata.set(new Text("name_4"),new Text("value_4"));  int seed=new Random().nextInt();  try {    writeMetadataTest(fs,count,seed,file,CompressionType.NONE,null,theMetadata);    SequenceFile.Metadata aMetadata=readMetadata(fs,file);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 1);    }    writeMetadataTest(fs,count,seed,recordCompressedFile,CompressionType.RECORD,codec,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {
  theMetadata.set(new Text("name_2"),new Text("value_2"));  theMetadata.set(new Text("name_3"),new Text("value_3"));  theMetadata.set(new Text("name_4"),new Text("value_4"));  int seed=new Random().nextInt();  try {    writeMetadataTest(fs,count,seed,file,CompressionType.NONE,null,theMetadata);    SequenceFile.Metadata aMetadata=readMetadata(fs,file);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 1);    }    writeMetadataTest(fs,count,seed,recordCompressedFile,CompressionType.RECORD,codec,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());
    writeMetadataTest(fs,count,seed,file,CompressionType.NONE,null,theMetadata);    SequenceFile.Metadata aMetadata=readMetadata(fs,file);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 1);    }    writeMetadataTest(fs,count,seed,recordCompressedFile,CompressionType.RECORD,codec,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 2);    }    writeMetadataTest(fs,count,seed,blockCompressedFile,CompressionType.BLOCK,codec,theMetadata);    aMetadata=readMetadata(fs,blockCompressedFile);    if (!theMetadata.equals(aMetadata)) {
    SequenceFile.Metadata aMetadata=readMetadata(fs,file);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 1);    }    writeMetadataTest(fs,count,seed,recordCompressedFile,CompressionType.RECORD,codec,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 2);    }    writeMetadataTest(fs,count,seed,blockCompressedFile,CompressionType.BLOCK,codec,theMetadata);    aMetadata=readMetadata(fs,blockCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());
    }    writeMetadataTest(fs,count,seed,recordCompressedFile,CompressionType.RECORD,codec,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 2);    }    writeMetadataTest(fs,count,seed,blockCompressedFile,CompressionType.BLOCK,codec,theMetadata);    aMetadata=readMetadata(fs,blockCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 3);    }    sortMetadataTest(fs,file,sortedFile,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {
    writeMetadataTest(fs,count,seed,recordCompressedFile,CompressionType.RECORD,codec,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 2);    }    writeMetadataTest(fs,count,seed,blockCompressedFile,CompressionType.BLOCK,codec,theMetadata);    aMetadata=readMetadata(fs,blockCompressedFile);    if (!theMetadata.equals(aMetadata)) {      LOG.info("The original metadata:\n" + theMetadata.toString());      LOG.info("The retrieved metadata:\n" + aMetadata.toString());      throw new RuntimeException("metadata not match:  " + 3);    }    sortMetadataTest(fs,file,sortedFile,theMetadata);    aMetadata=readMetadata(fs,recordCompressedFile);    if (!theMetadata.equals(aMetadata)) {
@SuppressWarnings("deprecation") private SequenceFile.Metadata readMetadata(FileSystem fs,Path file) throws IOException {
@SuppressWarnings("deprecation") private void writeMetadataTest(FileSystem fs,int count,int seed,Path file,CompressionType compressionType,CompressionCodec codec,SequenceFile.Metadata metadata) throws IOException {  fs.delete(file,true);
private void sortMetadataTest(FileSystem fs,Path unsortedFile,Path sortedFile,SequenceFile.Metadata metadata) throws IOException {  fs.delete(sortedFile,true);
 else       if (args[i].equals("-rwonly")) {        rwonly=true;      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-check")) {        check=true;      } else       if (args[i].equals("-fast")) {        fast=true;      } else       if (args[i].equals("-merge")) {        merge=true;      } else       if (args[i].equals("-compressType")) {        compressType=args[++i];      } else       if (args[i].equals("-codec")) {        compressionCodec=args[++i];      } else {
 else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-check")) {        check=true;      } else       if (args[i].equals("-fast")) {        fast=true;      } else       if (args[i].equals("-merge")) {        merge=true;      } else       if (args[i].equals("-compressType")) {        compressType=args[++i];      } else       if (args[i].equals("-codec")) {        compressionCodec=args[++i];      } else {        file=new Path(args[i]);      }    }    TestSequenceFile test=new TestSequenceFile();
        create=false;      } else       if (args[i].equals("-check")) {        check=true;      } else       if (args[i].equals("-fast")) {        fast=true;      } else       if (args[i].equals("-merge")) {        merge=true;      } else       if (args[i].equals("-compressType")) {        compressType=args[++i];      } else       if (args[i].equals("-codec")) {        compressionCodec=args[++i];      } else {        file=new Path(args[i]);      }    }    TestSequenceFile test=new TestSequenceFile();    fs=file.getFileSystem(test.conf);
      } else       if (args[i].equals("-check")) {        check=true;      } else       if (args[i].equals("-fast")) {        fast=true;      } else       if (args[i].equals("-merge")) {        merge=true;      } else       if (args[i].equals("-compressType")) {        compressType=args[++i];      } else       if (args[i].equals("-codec")) {        compressionCodec=args[++i];      } else {        file=new Path(args[i]);      }    }    TestSequenceFile test=new TestSequenceFile();    fs=file.getFileSystem(test.conf);    LOG.info("count = " + count);
 else       if (args[i].equals("-check")) {        check=true;      } else       if (args[i].equals("-fast")) {        fast=true;      } else       if (args[i].equals("-merge")) {        merge=true;      } else       if (args[i].equals("-compressType")) {        compressType=args[++i];      } else       if (args[i].equals("-codec")) {        compressionCodec=args[++i];      } else {        file=new Path(args[i]);      }    }    TestSequenceFile test=new TestSequenceFile();    fs=file.getFileSystem(test.conf);    LOG.info("count = " + count);
        check=true;      } else       if (args[i].equals("-fast")) {        fast=true;      } else       if (args[i].equals("-merge")) {        merge=true;      } else       if (args[i].equals("-compressType")) {        compressType=args[++i];      } else       if (args[i].equals("-codec")) {        compressionCodec=args[++i];      } else {        file=new Path(args[i]);      }    }    TestSequenceFile test=new TestSequenceFile();    fs=file.getFileSystem(test.conf);    LOG.info("count = " + count);    LOG.info("megabytes = " + megabytes);
      } else       if (args[i].equals("-fast")) {        fast=true;      } else       if (args[i].equals("-merge")) {        merge=true;      } else       if (args[i].equals("-compressType")) {        compressType=args[++i];      } else       if (args[i].equals("-codec")) {        compressionCodec=args[++i];      } else {        file=new Path(args[i]);      }    }    TestSequenceFile test=new TestSequenceFile();    fs=file.getFileSystem(test.conf);    LOG.info("count = " + count);    LOG.info("megabytes = " + megabytes);    LOG.info("factor = " + factor);
private static RandomDatum[] generate(int count){
private static void writeTest(FileSystem fs,RandomDatum[] data,String file,CompressionType compress) throws IOException {  MapFile.delete(fs,file);
private static void readTest(FileSystem fs,RandomDatum[] data,String file) throws IOException {  RandomDatum v=new RandomDatum();  int sample=(int)Math.sqrt(data.length);  Random random=new Random();
  try {    for (; i < args.length; i++) {      if (args[i] == null) {        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else       if (args[i].equals("-compress")) {        compress=args[++i];      } else {        file=args[i];        fpath=new Path(file);
    for (; i < args.length; i++) {      if (args[i] == null) {        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else       if (args[i].equals("-compress")) {        compress=args[++i];      } else {        file=args[i];        fpath=new Path(file);      }    }    fs=fpath.getFileSystem(conf);
      if (args[i] == null) {        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else       if (args[i].equals("-compress")) {        compress=args[++i];      } else {        file=args[i];        fpath=new Path(file);      }    }    fs=fpath.getFileSystem(conf);    LOG.info("count = " + count);
        continue;      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else       if (args[i].equals("-compress")) {        compress=args[++i];      } else {        file=args[i];        fpath=new Path(file);      }    }    fs=fpath.getFileSystem(conf);    LOG.info("count = " + count);    LOG.info("create = " + create);
      } else       if (args[i].equals("-count")) {        count=Integer.parseInt(args[++i]);      } else       if (args[i].equals("-nocreate")) {        create=false;      } else       if (args[i].equals("-nocheck")) {        check=false;      } else       if (args[i].equals("-compress")) {        compress=args[++i];      } else {        file=args[i];        fpath=new Path(file);      }    }    fs=fpath.getFileSystem(conf);    LOG.info("count = " + count);    LOG.info("create = " + create);    LOG.info("check = " + check);
private void testValue(int val,int vintlen) throws IOException {  DataOutputBuffer buf=new DataOutputBuffer();  DataInputBuffer inbuf=new DataInputBuffer();  WritableUtils.writeVInt(buf,val);  if (LOG.isDebugEnabled()) {
private void testValue(int val,int vintlen) throws IOException {  DataOutputBuffer buf=new DataOutputBuffer();  DataInputBuffer inbuf=new DataInputBuffer();  WritableUtils.writeVInt(buf,val);  if (LOG.isDebugEnabled()) {    LOG.debug("Value = " + val);    BytesWritable printer=new BytesWritable();    printer.set(buf.getData(),0,buf.getLength());
private void testSplitableCodec(Class<? extends SplittableCompressionCodec> codecClass) throws IOException {  final long DEFLBYTES=2 * 1024 * 1024;  final Configuration conf=new Configuration();  final Random rand=new Random();  final long seed=rand.nextLong();
private void codecTestMapFile(Class<? extends CompressionCodec> clazz,CompressionType type,int records) throws Exception {  FileSystem fs=FileSystem.get(conf);
private void codecTestMapFile(Class<? extends CompressionCodec> clazz,CompressionType type,int records) throws Exception {  FileSystem fs=FileSystem.get(conf);  LOG.info("Creating MapFiles with " + records + " records using codec "+ clazz.getSimpleName());  Path path=new Path(GenericTestUtils.getTempPath(clazz.getSimpleName() + "-" + type+ "-"+ records));
@Test public void testGzipCompatibility() throws IOException {  Random r=new Random();  long seed=r.nextLong();  r.setSeed(seed);
void GzipConcatTest(Configuration conf,Class<? extends Decompressor> decomClass) throws IOException {  Random r=new Random();  long seed=r.nextLong();  r.setSeed(seed);
  deflateFilter.write(data.getData(),0,data.getLength());  deflateFilter.finish();  deflateFilter.flush();  LOG.info("Finished re-compressing data");  DataInputBuffer deCompressedDataBuffer=new DataInputBuffer();  deCompressedDataBuffer.reset(compressedDataBuffer.getData(),0,compressedDataBuffer.getLength());  CompressionInputStream inflateFilter=codec.createInputStream(deCompressedDataBuffer);  DataInputStream inflateIn=new DataInputStream(new BufferedInputStream(inflateFilter));  for (int i=0; i < count; ++i) {    RandomDatum k1=new RandomDatum();    RandomDatum v1=new RandomDatum();    k1.readFields(originalIn);    v1.readFields(originalIn);    RandomDatum k2=new RandomDatum();    RandomDatum v2=new RandomDatum();
@Test public void testWriteOverride() throws IOException {  Random r=new Random();  long seed=r.nextLong();
@Test public void testSnappyCompressDecompress() throws Exception {  int BYTE_SIZE=1024 * 54;  byte[] bytes=BytesGenerator.get(BYTE_SIZE);  SnappyCompressor compressor=new SnappyCompressor();  compressor.setInput(bytes,0,bytes.length);  assertTrue("SnappyCompressDecompress getBytesRead error !!!",compressor.getBytesRead() > 0);  assertEquals("SnappyCompressDecompress getBytesWritten before compress error !!!",0,compressor.getBytesWritten());  int maxSize=32 + BYTE_SIZE + BYTE_SIZE / 6;  byte[] compressed=new byte[maxSize];  int cSize=compressor.compress(compressed,0,compressed.length);
@Test public void testSnappyCompressDecompress() throws Exception {  int BYTE_SIZE=1024 * 54;  byte[] bytes=BytesGenerator.get(BYTE_SIZE);  SnappyCompressor compressor=new SnappyCompressor();  compressor.setInput(bytes,0,bytes.length);  assertTrue("SnappyCompressDecompress getBytesRead error !!!",compressor.getBytesRead() > 0);  assertEquals("SnappyCompressDecompress getBytesWritten before compress error !!!",0,compressor.getBytesWritten());  int maxSize=32 + BYTE_SIZE + BYTE_SIZE / 6;  byte[] compressed=new byte[maxSize];  int cSize=compressor.compress(compressed,0,compressed.length);  LOG.info("input size: {}",BYTE_SIZE);
@Test(timeout=30000) public void testFstat() throws Exception {  FileOutputStream fos=new FileOutputStream(new File(TEST_DIR,"testfstat"));  NativeIO.POSIX.Stat stat=NativeIO.POSIX.getFstat(fos.getFD());  fos.close();
private boolean doStatTest(String testFilePath) throws Exception {  NativeIO.POSIX.Stat stat=NativeIO.POSIX.getStat(testFilePath);  String owner=stat.getOwner();  String group=stat.getGroup();  int mode=stat.getMode();  String expectedOwner=System.getProperty("user.name");  assertEquals(expectedOwner,owner);  assertNotNull(group);  assertTrue(!group.isEmpty());  StatUtils.Permission expected=StatUtils.getPermissionFromProcess(testFilePath);  StatUtils.Permission permission=new StatUtils.Permission(owner,group,new FsPermission(mode));  assertEquals(expected.getOwner(),permission.getOwner());  assertEquals(expected.getGroup(),permission.getGroup());  assertEquals(expected.getFsPermission(),permission.getFsPermission());
private boolean doStatTest(String testFilePath) throws Exception {  NativeIO.POSIX.Stat stat=NativeIO.POSIX.getStat(testFilePath);  String owner=stat.getOwner();  String group=stat.getGroup();  int mode=stat.getMode();  String expectedOwner=System.getProperty("user.name");  assertEquals(expectedOwner,owner);  assertNotNull(group);  assertTrue(!group.isEmpty());  StatUtils.Permission expected=StatUtils.getPermissionFromProcess(testFilePath);  StatUtils.Permission permission=new StatUtils.Permission(owner,group,new FsPermission(mode));  assertEquals(expected.getOwner(),permission.getOwner());  assertEquals(expected.getGroup(),permission.getGroup());  assertEquals(expected.getFsPermission(),permission.getFsPermission());  LOG.info("Load permission test is successful for path: {}, stat: {}",testFilePath,stat);
@Test(timeout=30000) public void testOpenWithCreate() throws Exception {  assumeNotWindows();  LOG.info("Test creating a file with O_CREAT");  FileDescriptor fd=NativeIO.POSIX.open(new File(TEST_DIR,"testWorkingOpen").getAbsolutePath(),O_WRONLY | O_CREAT,0700);  assertNotNull(true);  assertTrue(fd.valid());  FileOutputStream fos=new FileOutputStream(fd);  fos.write("foo".getBytes());  fos.close();  assertFalse(fd.valid());  LOG.info("Test exclusive create");  try {    fd=NativeIO.POSIX.open(new File(TEST_DIR,"testWorkingOpen").getAbsolutePath(),O_WRONLY | O_CREAT | O_EXCL,0700);    fail("Was able to create existing file with O_EXCL");  } catch (  NativeIOException nioe) {
  assumeTrue(NativeIO.POSIX.isPmdkAvailable());  String filePath="/$:";  long length=0;  long volumnSize=16 * 1024 * 1024* 1024L;  try {    NativeIO.POSIX.Pmem.mapBlock(filePath,length,false);    fail("Illegal length parameter should be detected");  } catch (  Exception e) {    LOG.info(e.getMessage());  }  filePath="/mnt/pmem0/test_native_io";  length=-1L;  try {    NativeIO.POSIX.Pmem.mapBlock(filePath,length,false);    fail("Illegal length parameter should be detected");  } catch (  Exception e) {
@Test(timeout=10000) public void testPmemMapMultipleFiles(){  assumeNotWindows("Native PMDK not supported on Windows");  assumeTrue(NativeIO.POSIX.isPmdkAvailable());  String filePath="/mnt/pmem0/test_native_io";  long length=0;  long volumnSize=16 * 1024 * 1024* 1024L;  length=128 * 1024 * 1024L;  long fileNumber=volumnSize / length;
@Test(timeout=10000) public void testPmemMapMultipleFiles(){  assumeNotWindows("Native PMDK not supported on Windows");  assumeTrue(NativeIO.POSIX.isPmdkAvailable());  String filePath="/mnt/pmem0/test_native_io";  long length=0;  long volumnSize=16 * 1024 * 1024* 1024L;  length=128 * 1024 * 1024L;  long fileNumber=volumnSize / length;  LOG.info("File number = " + fileNumber);  for (int i=0; i < fileNumber; i++) {    String path=filePath + i;
  assumeTrue(NativeIO.POSIX.isPmdkAvailable());  String filePath="/mnt/pmem0/test_native_io";  long length=0;  long volumnSize=16 * 1024 * 1024* 1024L;  length=128 * 1024 * 1024L;  long fileNumber=volumnSize / length;  LOG.info("File number = " + fileNumber);  for (int i=0; i < fileNumber; i++) {    String path=filePath + i;    LOG.info("File path = " + path);    NativeIO.POSIX.Pmem.mapBlock(path,length,false);  }  try {    NativeIO.POSIX.Pmem.mapBlock(filePath,length,false);    fail("Request map extra file when persistent memory is all occupied");  } catch (  Exception e) {
@Test(timeout=10000) public void testPmemMapBigFile(){  assumeNotWindows("Native PMDK not supported on Windows");  assumeTrue(NativeIO.POSIX.isPmdkAvailable());  String filePath="/mnt/pmem0/test_native_io_big";  long length=0;  long volumeSize=16 * 1024 * 1024* 1024L;  length=volumeSize + 1024L;  try {
private static void assertExceptionContains(Throwable t,String substring){  String msg=StringUtils.stringifyException(t);  assertTrue("Exception should contain substring '" + substring + "':\n"+ msg,msg.contains(substring));
@Test(timeout=60000) public void testRTEDuringConnectionSetup() throws IOException {  SocketFactory spyFactory=spy(NetUtils.getDefaultSocketFactory(conf));  Mockito.doAnswer(new Answer<Socket>(){    @Override public Socket answer(    InvocationOnMock invocation){      return new MockSocket();    }  }).when(spyFactory).createSocket();  Server server=new TestServer(1,true);  Client client=new Client(LongWritable.class,conf,spyFactory);  server.start();  try {    InetSocketAddress address=NetUtils.getConnectAddress(server);    try {      call(client,RANDOM.nextLong(),address,conf);      fail("Expected an exception to have been thrown");    } catch (    Exception e) {
  final CountDownLatch callFinishedLatch=new CountDownLatch(clients);  final TestServerQueue server=new TestServerQueue(clients,readers,callQ,handlers,conf);  CallQueueManager<Call> spy=spy((CallQueueManager<Call>)Whitebox.getInternalState(server,"callQueue"));  Whitebox.setInternalState(server,"callQueue",spy);  final InetSocketAddress addr=NetUtils.getConnectAddress(server);  server.start();  Client.setConnectTimeout(conf,10000);  Thread[] threads=new Thread[clients];  for (int i=0; i < clients; i++) {    threads[i]=new Thread(new Runnable(){      @Override public void run(){        Client client=new Client(LongWritable.class,conf);        try {          call(client,new LongWritable(Thread.currentThread().getId()),addr,60000,conf);        } catch (        Throwable e) {
 finally {          callFinishedLatch.countDown();          client.stop();        }      }    });  }  for (int i=0; i < initialClients; i++) {    threads[i].start();    if (i == 0) {      server.firstCallLatch.await();    }    verify(spy,timeout(5000).times(i + 1)).put(any());  }  try {    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        return server.getNumOpenConnections() >= initialClients;      }    },100,3000);  } catch (  TimeoutException e) {
  }  try {    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        return server.getNumOpenConnections() >= initialClients;      }    },100,3000);  } catch (  TimeoutException e) {    fail("timed out while waiting for connections to open.");  }  LOG.info("(initial clients) need:" + initialClients + " connections have:"+ server.getNumOpenConnections());  LOG.info("ipc layer should be blocked");  assertEquals(callQ,server.getCallQueueLen());  assertEquals(initialClients,server.getNumOpenConnections());  for (int i=initialClients; i < clients; i++) {    threads[i].start();  }  Thread.sleep(10);  try {
  final CountDownLatch allCallLatch=new CountDownLatch(clients);  final AtomicBoolean error=new AtomicBoolean();  final TestServer server=new TestServer(clients,false);  Thread[] threads=new Thread[clients];  try {    server.callListener=new Runnable(){      AtomicBoolean first=new AtomicBoolean(true);      @Override public void run(){        try {          allCallLatch.countDown();          if (first.compareAndSet(true,false)) {            firstCallBarrier.await();          } else {            callBarrier.await();          }        } catch (        Throwable t) {
@Test(timeout=20000) public void test() throws Exception {  Configuration conf=new Configuration();  TestProtoBufRpcServerHandoffServer serverImpl=new TestProtoBufRpcServerHandoffServer();  BlockingService blockingService=TestProtobufRpcHandoffProto.newReflectiveBlockingService(serverImpl);  RPC.setProtocolEngine(conf,TestProtoBufRpcServerHandoffProtocol.class,ProtobufRpcEngine2.class);  RPC.Server server=new RPC.Builder(conf).setProtocol(TestProtoBufRpcServerHandoffProtocol.class).setInstance(blockingService).setVerbose(true).setNumHandlers(1).build();  server.start();  InetSocketAddress address=server.getListenerAddress();  long serverStartTime=System.currentTimeMillis();
  RPC.Server server=new RPC.Builder(conf).setProtocol(TestProtoBufRpcServerHandoffProtocol.class).setInstance(blockingService).setVerbose(true).setNumHandlers(1).build();  server.start();  InetSocketAddress address=server.getListenerAddress();  long serverStartTime=System.currentTimeMillis();  LOG.info("Server started at: " + address + " at time: "+ serverStartTime);  final TestProtoBufRpcServerHandoffProtocol client=RPC.getProxy(TestProtoBufRpcServerHandoffProtocol.class,1,address,conf);  ExecutorService executorService=Executors.newFixedThreadPool(2);  CompletionService<ClientInvocationCallable> completionService=new ExecutorCompletionService<ClientInvocationCallable>(executorService);  completionService.submit(new ClientInvocationCallable(client,5000l));  completionService.submit(new ClientInvocationCallable(client,5000l));  long submitTime=System.currentTimeMillis();  Future<ClientInvocationCallable> future1=completionService.take();  Future<ClientInvocationCallable> future2=completionService.take();  ClientInvocationCallable callable1=future1.get();  ClientInvocationCallable callable2=future2.get();
  server.start();  InetSocketAddress address=server.getListenerAddress();  long serverStartTime=System.currentTimeMillis();  LOG.info("Server started at: " + address + " at time: "+ serverStartTime);  final TestProtoBufRpcServerHandoffProtocol client=RPC.getProxy(TestProtoBufRpcServerHandoffProtocol.class,1,address,conf);  ExecutorService executorService=Executors.newFixedThreadPool(2);  CompletionService<ClientInvocationCallable> completionService=new ExecutorCompletionService<ClientInvocationCallable>(executorService);  completionService.submit(new ClientInvocationCallable(client,5000l));  completionService.submit(new ClientInvocationCallable(client,5000l));  long submitTime=System.currentTimeMillis();  Future<ClientInvocationCallable> future1=completionService.take();  Future<ClientInvocationCallable> future2=completionService.take();  ClientInvocationCallable callable1=future1.get();  ClientInvocationCallable callable2=future2.get();  LOG.info(callable1.toString());
    echoRequest2=TestProtos.EchoRequestProto2.newBuilder().addAllMessage(Collections.<String>emptyList()).build();    echoResponse2=proxy.echo2(null,echoRequest2);    assertTrue(Arrays.equals(echoResponse2.getMessageList().toArray(),new String[]{}));    TestProtos.AddRequestProto addRequest=TestProtos.AddRequestProto.newBuilder().setParam1(1).setParam2(2).build();    TestProtos.AddResponseProto addResponse=proxy.add(null,addRequest);    assertThat(addResponse.getResult()).isEqualTo(3);    Integer[] integers=new Integer[]{1,2};    TestProtos.AddRequestProto2 addRequest2=TestProtos.AddRequestProto2.newBuilder().addAllParams(Arrays.asList(integers)).build();    addResponse=proxy.add2(null,addRequest2);    assertThat(addResponse.getResult()).isEqualTo(3);    boolean caught=false;    try {      proxy.error(null,newEmptyRequest());    } catch (    ServiceException e) {      if (LOG.isDebugEnabled()) {
@Test public void testErrorMsgForInsecureClient() throws IOException {  Server server;  TestRpcService proxy=null;  Configuration serverConf=new Configuration(conf);  SecurityUtil.setAuthenticationMethod(AuthenticationMethod.KERBEROS,serverConf);  UserGroupInformation.setConfiguration(serverConf);  server=setupTestServer(serverConf,5);  boolean succeeded=false;  try {    UserGroupInformation.setConfiguration(conf);    proxy=getClient(addr,conf);    proxy.echo(null,newEchoRequest(""));  } catch (  ServiceException e) {    assertTrue(e.getCause() instanceof RemoteException);    RemoteException re=(RemoteException)e.getCause();
    assertTrue(re.unwrapRemoteException() instanceof AccessControlException);    succeeded=true;  } finally {    stop(server,proxy);  }  assertTrue(succeeded);  conf.setInt(CommonConfigurationKeys.IPC_SERVER_RPC_READ_THREADS_KEY,2);  UserGroupInformation.setConfiguration(serverConf);  server=setupTestServer(serverConf,5);  succeeded=false;  proxy=null;  try {    UserGroupInformation.setConfiguration(conf);    proxy=getClient(addr,conf);    proxy.echo(null,newEchoRequest(""));  } catch (  ServiceException e) {
        }      }));      verify(spy,timeout(500).times(i + 1)).addInternal(any(),eq(false));    }    try {      proxy.sleep(null,newSleepRequest(100));    } catch (    ServiceException e) {      RemoteException re=(RemoteException)e.getCause();      IOException unwrapExeption=re.unwrapRemoteException();      if (unwrapExeption instanceof RetriableException) {        succeeded=true;      } else {        lastException=unwrapExeption;      }    }  }  finally {    executorService.shutdown();    stop(server,proxy);  }  if (lastException != null) {
      }));      verify(spy,timeout(500).times(i + 1)).addInternal(any(),eq(false));    }    try {      Thread.sleep(5500);      proxy.sleep(null,newSleepRequest(100));    } catch (    ServiceException e) {      RemoteException re=(RemoteException)e.getCause();      IOException unwrapExeption=re.unwrapRemoteException();      if (unwrapExeption instanceof RetriableException) {        succeeded=true;      } else {        lastException=unwrapExeption;      }    }  }  finally {    executorService.shutdown();    stop(server,proxy);
  final long beginRawCallVolume=MetricsAsserts.getLongCounter("CallVolume",rb1);  final int beginUniqueCaller=MetricsAsserts.getIntCounter("UniqueCallers",rb1);  TestRpcService proxy=getClient(addr,conf);  try {    for (int i=0; i < 2; i++) {      proxy.sleep(null,newSleepRequest(100));    }    GenericTestUtils.waitFor(() -> {      MetricsRecordBuilder rb2=getMetrics("DecayRpcSchedulerMetrics2." + ns);      long decayedCallVolume1=MetricsAsserts.getLongCounter("DecayedCallVolume",rb2);      long rawCallVolume1=MetricsAsserts.getLongCounter("CallVolume",rb2);      int uniqueCaller1=MetricsAsserts.getIntCounter("UniqueCallers",rb2);      long callVolumePriority0=MetricsAsserts.getLongGauge("Priority.0.CompletedCallVolume",rb2);      long callVolumePriority1=MetricsAsserts.getLongGauge("Priority.1.CompletedCallVolume",rb2);      double avgRespTimePriority0=MetricsAsserts.getDoubleGauge("Priority.0.AvgResponseTime",rb2);      double avgRespTimePriority1=MetricsAsserts.getDoubleGauge("Priority.1.AvgResponseTime",rb2);
  final int beginUniqueCaller=MetricsAsserts.getIntCounter("UniqueCallers",rb1);  TestRpcService proxy=getClient(addr,conf);  try {    for (int i=0; i < 2; i++) {      proxy.sleep(null,newSleepRequest(100));    }    GenericTestUtils.waitFor(() -> {      MetricsRecordBuilder rb2=getMetrics("DecayRpcSchedulerMetrics2." + ns);      long decayedCallVolume1=MetricsAsserts.getLongCounter("DecayedCallVolume",rb2);      long rawCallVolume1=MetricsAsserts.getLongCounter("CallVolume",rb2);      int uniqueCaller1=MetricsAsserts.getIntCounter("UniqueCallers",rb2);      long callVolumePriority0=MetricsAsserts.getLongGauge("Priority.0.CompletedCallVolume",rb2);      long callVolumePriority1=MetricsAsserts.getLongGauge("Priority.1.CompletedCallVolume",rb2);      double avgRespTimePriority0=MetricsAsserts.getDoubleGauge("Priority.0.AvgResponseTime",rb2);      double avgRespTimePriority1=MetricsAsserts.getDoubleGauge("Priority.1.AvgResponseTime",rb2);      LOG.info("DecayedCallVolume: {}",decayedCallVolume1);
  TestRpcService proxy=getClient(addr,conf);  try {    for (int i=0; i < 2; i++) {      proxy.sleep(null,newSleepRequest(100));    }    GenericTestUtils.waitFor(() -> {      MetricsRecordBuilder rb2=getMetrics("DecayRpcSchedulerMetrics2." + ns);      long decayedCallVolume1=MetricsAsserts.getLongCounter("DecayedCallVolume",rb2);      long rawCallVolume1=MetricsAsserts.getLongCounter("CallVolume",rb2);      int uniqueCaller1=MetricsAsserts.getIntCounter("UniqueCallers",rb2);      long callVolumePriority0=MetricsAsserts.getLongGauge("Priority.0.CompletedCallVolume",rb2);      long callVolumePriority1=MetricsAsserts.getLongGauge("Priority.1.CompletedCallVolume",rb2);      double avgRespTimePriority0=MetricsAsserts.getDoubleGauge("Priority.0.AvgResponseTime",rb2);      double avgRespTimePriority1=MetricsAsserts.getDoubleGauge("Priority.1.AvgResponseTime",rb2);      LOG.info("DecayedCallVolume: {}",decayedCallVolume1);      LOG.info("CallVolume: {}",rawCallVolume1);
  try {    for (int i=0; i < 2; i++) {      proxy.sleep(null,newSleepRequest(100));    }    GenericTestUtils.waitFor(() -> {      MetricsRecordBuilder rb2=getMetrics("DecayRpcSchedulerMetrics2." + ns);      long decayedCallVolume1=MetricsAsserts.getLongCounter("DecayedCallVolume",rb2);      long rawCallVolume1=MetricsAsserts.getLongCounter("CallVolume",rb2);      int uniqueCaller1=MetricsAsserts.getIntCounter("UniqueCallers",rb2);      long callVolumePriority0=MetricsAsserts.getLongGauge("Priority.0.CompletedCallVolume",rb2);      long callVolumePriority1=MetricsAsserts.getLongGauge("Priority.1.CompletedCallVolume",rb2);      double avgRespTimePriority0=MetricsAsserts.getDoubleGauge("Priority.0.AvgResponseTime",rb2);      double avgRespTimePriority1=MetricsAsserts.getDoubleGauge("Priority.1.AvgResponseTime",rb2);      LOG.info("DecayedCallVolume: {}",decayedCallVolume1);      LOG.info("CallVolume: {}",rawCallVolume1);      LOG.info("UniqueCaller: {}",uniqueCaller1);
    for (int i=0; i < 2; i++) {      proxy.sleep(null,newSleepRequest(100));    }    GenericTestUtils.waitFor(() -> {      MetricsRecordBuilder rb2=getMetrics("DecayRpcSchedulerMetrics2." + ns);      long decayedCallVolume1=MetricsAsserts.getLongCounter("DecayedCallVolume",rb2);      long rawCallVolume1=MetricsAsserts.getLongCounter("CallVolume",rb2);      int uniqueCaller1=MetricsAsserts.getIntCounter("UniqueCallers",rb2);      long callVolumePriority0=MetricsAsserts.getLongGauge("Priority.0.CompletedCallVolume",rb2);      long callVolumePriority1=MetricsAsserts.getLongGauge("Priority.1.CompletedCallVolume",rb2);      double avgRespTimePriority0=MetricsAsserts.getDoubleGauge("Priority.0.AvgResponseTime",rb2);      double avgRespTimePriority1=MetricsAsserts.getDoubleGauge("Priority.1.AvgResponseTime",rb2);      LOG.info("DecayedCallVolume: {}",decayedCallVolume1);      LOG.info("CallVolume: {}",rawCallVolume1);      LOG.info("UniqueCaller: {}",uniqueCaller1);      LOG.info("Priority.0.CompletedCallVolume: {}",callVolumePriority0);
      proxy.sleep(null,newSleepRequest(100));    }    GenericTestUtils.waitFor(() -> {      MetricsRecordBuilder rb2=getMetrics("DecayRpcSchedulerMetrics2." + ns);      long decayedCallVolume1=MetricsAsserts.getLongCounter("DecayedCallVolume",rb2);      long rawCallVolume1=MetricsAsserts.getLongCounter("CallVolume",rb2);      int uniqueCaller1=MetricsAsserts.getIntCounter("UniqueCallers",rb2);      long callVolumePriority0=MetricsAsserts.getLongGauge("Priority.0.CompletedCallVolume",rb2);      long callVolumePriority1=MetricsAsserts.getLongGauge("Priority.1.CompletedCallVolume",rb2);      double avgRespTimePriority0=MetricsAsserts.getDoubleGauge("Priority.0.AvgResponseTime",rb2);      double avgRespTimePriority1=MetricsAsserts.getDoubleGauge("Priority.1.AvgResponseTime",rb2);      LOG.info("DecayedCallVolume: {}",decayedCallVolume1);      LOG.info("CallVolume: {}",rawCallVolume1);      LOG.info("UniqueCaller: {}",uniqueCaller1);      LOG.info("Priority.0.CompletedCallVolume: {}",callVolumePriority0);      LOG.info("Priority.1.CompletedCallVolume: {}",callVolumePriority1);
    }    GenericTestUtils.waitFor(() -> {      MetricsRecordBuilder rb2=getMetrics("DecayRpcSchedulerMetrics2." + ns);      long decayedCallVolume1=MetricsAsserts.getLongCounter("DecayedCallVolume",rb2);      long rawCallVolume1=MetricsAsserts.getLongCounter("CallVolume",rb2);      int uniqueCaller1=MetricsAsserts.getIntCounter("UniqueCallers",rb2);      long callVolumePriority0=MetricsAsserts.getLongGauge("Priority.0.CompletedCallVolume",rb2);      long callVolumePriority1=MetricsAsserts.getLongGauge("Priority.1.CompletedCallVolume",rb2);      double avgRespTimePriority0=MetricsAsserts.getDoubleGauge("Priority.0.AvgResponseTime",rb2);      double avgRespTimePriority1=MetricsAsserts.getDoubleGauge("Priority.1.AvgResponseTime",rb2);      LOG.info("DecayedCallVolume: {}",decayedCallVolume1);      LOG.info("CallVolume: {}",rawCallVolume1);      LOG.info("UniqueCaller: {}",uniqueCaller1);      LOG.info("Priority.0.CompletedCallVolume: {}",callVolumePriority0);      LOG.info("Priority.1.CompletedCallVolume: {}",callVolumePriority1);      LOG.info("Priority.0.AvgResponseTime: {}",avgRespTimePriority0);
      proxy=getClient(addr,c);      proxy.sleep(null,newSleepRequest(3000));      fail("RPC should time out.");    } catch (    ServiceException e) {      assertTrue(e.getCause() instanceof SocketTimeoutException);      LOG.info("got expected timeout.",e);    }    try {      Configuration c=new Configuration(conf);      c.setBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY,false);      c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY,1000);      proxy=getClient(addr,c);      proxy.sleep(null,newSleepRequest(3000));      fail("RPC should time out.");    } catch (    ServiceException e) {      assertTrue(e.getCause() instanceof SocketTimeoutException);
    }    try {      Configuration c=new Configuration(conf);      c.setBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY,false);      c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY,1000);      proxy=getClient(addr,c);      proxy.sleep(null,newSleepRequest(3000));      fail("RPC should time out.");    } catch (    ServiceException e) {      assertTrue(e.getCause() instanceof SocketTimeoutException);      LOG.info("got expected timeout.",e);    }    try {      Configuration c=new Configuration(conf);      c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY,-1);      proxy=getClient(addr,c);      proxy.sleep(null,newSleepRequest(2000));
      LOG.info("got expected timeout.",e);    }    try {      Configuration c=new Configuration(conf);      c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY,-1);      proxy=getClient(addr,c);      proxy.sleep(null,newSleepRequest(2000));    } catch (    ServiceException e) {      LOG.info("got unexpected exception.",e);      fail("RPC should not time out.");    }    try {      Configuration c=new Configuration(conf);      c.setBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY,true);      c.setInt(CommonConfigurationKeys.IPC_PING_INTERVAL_KEY,800);      c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY,1000);      proxy=getClient(addr,c);
      proxy.sleep(null,newSleepRequest(2000));    } catch (    ServiceException e) {      LOG.info("got unexpected exception.",e);      fail("RPC should not time out.");    }    try {      Configuration c=new Configuration(conf);      c.setBoolean(CommonConfigurationKeys.IPC_CLIENT_PING_KEY,true);      c.setInt(CommonConfigurationKeys.IPC_PING_INTERVAL_KEY,800);      c.setInt(CommonConfigurationKeys.IPC_CLIENT_RPC_TIMEOUT_KEY,1000);      proxy=getClient(addr,c);      try {        proxy.sleep(null,newSleepRequest(1300));      } catch (      ServiceException e) {        LOG.info("got unexpected exception.",e);        fail("RPC should not time out.");
  @SuppressWarnings("serial") IOException rseFatal=new RpcServerException("disconnect",expectedIOE){    @Override public RpcStatusProto getRpcStatusProto(){      return RpcStatusProto.FATAL;    }  };  try {    RPC.Builder builder=newServerBuilder(conf).setQueueSizePerHandler(1).setNumHandlers(1).setVerbose(true);    server=setupTestServer(builder);    Whitebox.setInternalState(server,"rpcRequestClass",FakeRequestClass.class);    MutableCounterLong authMetric=(MutableCounterLong)Whitebox.getInternalState(server.getRpcMetrics(),"rpcAuthorizationSuccesses");    proxy=getClient(addr,conf);    boolean isDisconnected=true;    Connection lastConn=null;    long expectedAuths=0;    for (int i=0; i < 128; i++) {      String reqName="request[" + i + "]";
          return null;        }      }));    }    while (server.getCallQueueLen() != 1 || countThreads(CallQueueManager.class.getName()) != 1 || countThreads(PBServerImpl.class.getName()) != 1) {      Thread.sleep(100);    }  }  finally {    try {      stop(server,proxy);      assertEquals("Not enough clients",numClients,res.size());      for (      Future<Void> f : res) {        try {          f.get();          fail("Future get should not return");        } catch (        ExecutionException e) {          ServiceException se=(ServiceException)e.getCause();          assertTrue("Unexpected exception: " + se,se.getCause() instanceof IOException);
          for (int i=0; i < futures.length; i++) {            futures[i]=executor.submit(new Callable<Void>(){              @Override public Void call() throws Exception {                String expect="future" + count.getAndIncrement();                String answer=convert(proxy.echoPostponed(null,newEchoRequest(expect)));                assertEquals(expect,answer);                return null;              }            });            try {              futures[i].get(100,TimeUnit.MILLISECONDS);            } catch (            TimeoutException te) {              continue;            }            Assert.fail("future" + i + " did not block");          }          proxy.sendPostponed(null,newEmptyRequest());          for (int i=0; i < futures.length; i++) {
  UserGroupInformation.setConfiguration(clientConf);  final UserGroupInformation clientUgi=UserGroupInformation.createRemoteUser("client");  clientUgi.setAuthenticationMethod(clientAuth);  final InetSocketAddress addr=NetUtils.getConnectAddress(server);  if (tokenType != UseToken.NONE) {    TestTokenIdentifier tokenId=new TestTokenIdentifier(new Text(clientUgi.getUserName()));    Token<TestTokenIdentifier> token=null;switch (tokenType) {case VALID:      token=new Token<>(tokenId,sm);    SecurityUtil.setTokenService(token,addr);  break;case INVALID:token=new Token<>(tokenId.getBytes(),"bad-password!".getBytes(),tokenId.getKind(),null);SecurityUtil.setTokenService(token,addr);break;case OTHER:token=new Token<>();
private String logOut(String message,Throwable throwable){  StringWriter writer=new StringWriter();  Logger logger=createLogger(writer);
@Test public void testCommon() throws Exception {  String filename=getTestFilename("test-metrics2");  new ConfigBuilder().add("*.foo","default foo").add("p1.*.bar","p1 default bar").add("p1.t1.*.bar","p1.t1 default bar").add("p1.t1.i1.name","p1.t1.i1.name").add("p1.t1.42.bar","p1.t1.42.bar").add("p1.t2.i1.foo","p1.t2.i1.foo").add("p2.*.foo","p2 default foo").save(filename);  MetricsConfig mc=MetricsConfig.create("p1",filename);
private void checkMetricsRecords(List<MetricsRecord> recs){
@Test public void testMutableRatesWithAggregationManyThreads() throws InterruptedException {  final MutableRatesWithAggregation rates=new MutableRatesWithAggregation();  final int n=10;  long[] opCount=new long[n];  double[] opTotalTime=new double[n];  for (int i=0; i < n; i++) {    opCount[i]=0;    opTotalTime[i]=0;    rates.add("metric" + i,0);  }  Thread[] threads=new Thread[n];  final CountDownLatch firstAddsFinished=new CountDownLatch(threads.length);  final CountDownLatch firstSnapshotsFinished=new CountDownLatch(1);  final CountDownLatch secondAddsFinished=new CountDownLatch(threads.length);  final CountDownLatch secondSnapshotsFinished=new CountDownLatch(1);  long seed=new Random().nextLong();
@SuppressWarnings("deprecation") @Test public void testGet(){  MetricsCache cache=new MetricsCache();  assertNull("empty",cache.get("r",Arrays.asList(makeTag("t","t"))));  MetricsRecord mr=makeRecord("r",Arrays.asList(makeTag("t","t")),Arrays.asList(makeMetric("m",1)));  cache.update(mr);  MetricsCache.Record cr=cache.get("r",mr.tags());
@Test public void testRDNS() throws Exception {  InetAddress localhost=getLocalIPAddr();  try {    String s=DNS.reverseDns(localhost,null);
@Test public void testLocalhostResolves() throws Exception {  InetAddress localhost=InetAddress.getByName("localhost");  assertNotNull("localhost is null",localhost);
private IOException verifyExceptionClass(IOException e,Class expectedClass) throws Throwable {  assertNotNull("Null Exception",e);  IOException wrapped=NetUtils.wrapException("desthost",DEST_PORT,"localhost",LOCAL_PORT,e);
    out.write(byteWithHighBit);    doIO(null,out,TIMEOUT);    in.read(readBytes);    assertTrue(Arrays.equals(writeBytes,readBytes));    assertEquals(byteWithHighBit & 0xff,in.read());    doIO(in,null,TIMEOUT);    ((SocketInputStream)in).setTimeout(TIMEOUT * 2);    doIO(in,null,TIMEOUT * 2);    ((SocketInputStream)in).setTimeout(0);    TestingThread thread=new TestingThread(ctx){      @Override public void doWork() throws Exception {        try {          in.read();          fail("Did not fail with interrupt");        } catch (        InterruptedIOException ste) {
@Test public void testAddResolveNodes() throws Throwable {  StaticMapping mapping=newInstance();  StaticMapping.addNodeToRack("n1","/r1");  List<String> queryList=createQueryList();  List<String> resolved=mapping.resolve(queryList);  assertEquals(2,resolved.size());  assertEquals("/r1",resolved.get(0));  assertEquals(NetworkTopology.DEFAULT_RACK,resolved.get(1));  Map<String,String> switchMap=mapping.getSwitchMap();  String topology=mapping.dumpTopology();
  Configuration conf=new Configuration();  conf.set(StaticMapping.KEY_HADOOP_CONFIGURED_NODE_MAPPING,"n1=/r1,n2=/r2");  mapping.setConf(conf);  assertSingleSwitch(mapping);  List<String> l1=new ArrayList<String>(3);  l1.add("n1");  l1.add("unknown");  l1.add("n2");  List<String> resolved=mapping.resolve(l1);  assertEquals(3,resolved.size());  assertEquals("/r1",resolved.get(0));  assertEquals(NetworkTopology.DEFAULT_RACK,resolved.get(1));  assertEquals("/r2",resolved.get(2));  Map<String,String> switchMap=mapping.getSwitchMap();  String topology=mapping.dumpTopology();
@Test public void testCachingRelaysSingleSwitchQueries() throws Throwable {  StaticMapping staticMapping=newInstance(null);  assertSingleSwitch(staticMapping);  CachedDNSToSwitchMapping cachedMap=new CachedDNSToSwitchMapping(staticMapping);
@Test public void testCachingRelaysMultiSwitchQueries() throws Throwable {  StaticMapping staticMapping=newInstance("top");  assertMultiSwitch(staticMapping);  CachedDNSToSwitchMapping cachedMap=new CachedDNSToSwitchMapping(staticMapping);
    @Override public void run(){      try {        for (int i=0; i < SOCKET_NUM; i++) {          DomainSocket pair[]=DomainSocket.socketpair();          watcher.add(pair[1],new DomainSocketWatcher.Handler(){            @Override public boolean handle(            DomainSocket sock){              handled.incrementAndGet();              return true;            }          });          lock.lock();          try {            pairs.add(pair);          }  finally {            lock.unlock();          }        }      } catch (      Throwable e) {
 catch (      Throwable e) {        LOG.error(e.toString());        throw new RuntimeException(e);      }    }  });  final Thread removerThread=new Thread(new Runnable(){    @Override public void run(){      final Random random=new Random();      try {        while (handled.get() != SOCKET_NUM) {          lock.lock();          try {            if (!pairs.isEmpty()) {              int idx=random.nextInt(pairs.size());              DomainSocket pair[]=pairs.remove(idx);              if (random.nextBoolean()) {
      try {        for (int i=0; i < SOCKET_NUM; i++) {          DomainSocket pair[]=DomainSocket.socketpair();          watcher.add(pair[1],new DomainSocketWatcher.Handler(){            @Override public boolean handle(            DomainSocket sock){              handled.incrementAndGet();              return true;            }          });          lock.lock();          try {            pairs.add(pair);          }  finally {            lock.unlock();          }          TimeUnit.MILLISECONDS.sleep(1);        }      } catch (      Throwable e) {
        LOG.error(e.toString());        throw new RuntimeException(e);      }    }  });  final Thread removerThread=new Thread(new Runnable(){    @Override public void run(){      final Random random=new Random();      try {        while (handled.get() != SOCKET_NUM) {          lock.lock();          try {            if (!pairs.isEmpty()) {              int idx=random.nextInt(pairs.size());              DomainSocket pair[]=pairs.remove(idx);              if (random.nextBoolean()) {                pair[0].close();
private void configureSuperUserIPAddresses(Configuration conf,String superUserShortName) throws IOException {  ArrayList<String> ipList=new ArrayList<>();  Enumeration<NetworkInterface> netInterfaceList=NetworkInterface.getNetworkInterfaces();  while (netInterfaceList.hasMoreElements()) {    NetworkInterface inf=netInterfaceList.nextElement();    Enumeration<InetAddress> addrList=inf.getInetAddresses();    while (addrList.hasMoreElements()) {      InetAddress addr=addrList.nextElement();      ipList.add(addr.getHostAddress());    }  }  StringBuilder builder=new StringBuilder();  for (  String ip : ipList) {    builder.append(ip);    builder.append(',');  }  builder.append("127.0.1.1,");  builder.append(InetAddress.getLocalHost().getCanonicalHostName());
@Test public void testGroupShell() throws Exception {  GenericTestUtils.setRootLogLevel(Level.DEBUG);  Configuration conf=new Configuration();  conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping");  Groups groups=new Groups(conf);  String username=System.getProperty("user.name");  List<String> groupList=groups.getGroups(username);
@Test public void testNetgroupShell() throws Exception {  GenericTestUtils.setRootLogLevel(Level.DEBUG);  Configuration conf=new Configuration();  conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING,"org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping");  Groups groups=new Groups(conf);  String username=System.getProperty("user.name");  List<String> groupList=groups.getGroups(username);
@Test public void testGroupWithFallback() throws Exception {  LOG.info("running 'mvn -Pnative -DTestGroupFallback clear test' will " + "test the normal path and 'mvn -DTestGroupFallback clear test' will" + " test the fall back functionality");  GenericTestUtils.setRootLogLevel(Level.DEBUG);  Configuration conf=new Configuration();  conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING,"org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback");  Groups groups=new Groups(conf);  String username=System.getProperty("user.name");  List<String> groupList=groups.getGroups(username);
@Test public void testNetgroupWithFallback() throws Exception {  LOG.info("running 'mvn -Pnative -DTestGroupFallback clear test' will " + "test the normal path and 'mvn -DTestGroupFallback clear test' will" + " test the fall back functionality");  GenericTestUtils.setRootLogLevel(Level.DEBUG);  Configuration conf=new Configuration();  conf.set(CommonConfigurationKeys.HADOOP_SECURITY_GROUP_MAPPING,"org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback");  Groups groups=new Groups(conf);  String username=System.getProperty("user.name");  List<String> groupList=groups.getGroups(username);
@Test public void testGroupsCaching() throws Exception {  conf.setLong(CommonConfigurationKeys.HADOOP_SECURITY_GROUPS_NEGATIVE_CACHE_SECS,0);  Groups groups=new Groups(conf);  groups.cacheGroupsAdd(Arrays.asList(myGroups));  groups.refresh();  FakeGroupMapping.clearBlackList();  FakeGroupMapping.addToBlackList("user1");  assertTrue(groups.getGroups("me").size() == 2);  FakeGroupMapping.addToBlackList("me");  assertTrue(groups.getGroups("me").size() == 2);  try {    TESTLOG.error("We are not supposed to get here." + groups.getGroups("user1").toString());    fail();  } catch (  IOException ioe) {    if (!ioe.getMessage().startsWith("No groups found")) {
void kdiagFailure(String category,String... args) throws Exception {  try {    int ex=exec(conf,args);
@Test public void testFileOutput() throws Throwable {  File f=new File("target/kdiag.txt");  kdiag(ARG_KEYLEN,KEYLEN,ARG_KEYTAB,keytab.getAbsolutePath(),ARG_PRINCIPAL,"foo@EXAMPLE.COM",ARG_OUTPUT,f.getAbsolutePath());
private void dump(File file) throws IOException {  try (FileInputStream in=new FileInputStream(file)){    for (    String line : IOUtils.readLines(in)) {
void kdiagFailure(String category,String... args) throws Exception {  try {    int ex=exec(conf,args);
          try (Socket ignored=serverSock.accept()){            finLatch.await();          }         } catch (        Exception e) {          e.printStackTrace();        }      }    });    ldapServer.start();    final LdapGroupsMapping mapping=new LdapGroupsMapping();    String ldapUrl="ldap://localhost:" + serverSock.getLocalPort();    final Configuration conf=getBaseConf(ldapUrl,null);    conf.setInt(CONNECTION_TIMEOUT,connectionTimeoutMs);    mapping.setConf(conf);    try {      mapping.doGetGroups("hadoop",1);      fail("The LDAP query should have timed out!");    } catch (    NamingException ne) {
            clientSock.getOutputStream().write(AUTHENTICATE_SUCCESS_MSG);            finLatch.await();          }         } catch (        Exception e) {          e.printStackTrace();        }      }    });    ldapServer.start();    final LdapGroupsMapping mapping=new LdapGroupsMapping();    String ldapUrl="ldap://localhost:" + serverSock.getLocalPort();    final Configuration conf=getBaseConf(ldapUrl,null);    conf.setInt(READ_TIMEOUT,readTimeoutMs);    mapping.setConf(conf);    try {      mapping.doGetGroups("hadoop",1);      fail("The LDAP query should have timed out!");    } catch (    NamingException ne) {
  RetryPolicy rp=RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2,1000,TimeUnit.MILLISECONDS);  long lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  assertWithinBounds(UserGroupInformation.metrics.getRenewalFailures().value(),lastRetry,reloginIntervalMs,currentTime);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  assertWithinBounds(UserGroupInformation.metrics.getRenewalFailures().value(),lastRetry,reloginIntervalMs,currentTime);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  assertWithinBounds(UserGroupInformation.metrics.getRenewalFailures().value(),lastRetry,reloginIntervalMs,currentTime);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  assertWithinBounds(UserGroupInformation.metrics.getRenewalFailures().value(),lastRetry,reloginIntervalMs,currentTime);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  String str="5th retry, now:" + currentTime + ", retry:"+ lastRetry;
  assertWithinBounds(UserGroupInformation.metrics.getRenewalFailures().value(),lastRetry,reloginIntervalMs,currentTime);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  assertWithinBounds(UserGroupInformation.metrics.getRenewalFailures().value(),lastRetry,reloginIntervalMs,currentTime);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  assertWithinBounds(UserGroupInformation.metrics.getRenewalFailures().value(),lastRetry,reloginIntervalMs,currentTime);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  String str="5th retry, now:" + currentTime + ", retry:"+ lastRetry;  LOG.info(str);  assertEquals(str,endTime - reloginIntervalMs,lastRetry);  UserGroupInformation.metrics.getRenewalFailures().incr();  lastRetry=UserGroupInformation.getNextTgtRenewalTime(endTime,currentTime,rp);  str="overflow retry, now:" + currentTime + ", retry:"+ lastRetry;
private void assertWithinBounds(final int numFailures,final long lastRetry,final long reloginIntervalMs,long now){  int shift=numFailures + 1;  final long lower=now + reloginIntervalMs * (long)((1 << shift) * 0.5);  final long upper=now + reloginIntervalMs * (long)((1 << shift) * 1.5);  final String str=new String("Retry#" + (numFailures + 1) + ", now:"+ now+ ", lower bound:"+ lower+ ", upper bound:"+ upper+ ", retry:"+ lastRetry);
  SSLEngine serverSSLEngine=serverSSLFactory.createSSLEngine();  SSLEngine clientSSLEngine=clientSSLFactory.createSSLEngine();  clientSSLEngine.setEnabledCipherSuites(StringUtils.getTrimmedStrings(excludeCiphers));  SSLSession session=clientSSLEngine.getSession();  int appBufferMax=session.getApplicationBufferSize();  int netBufferMax=session.getPacketBufferSize();  ByteBuffer clientOut=ByteBuffer.wrap("client".getBytes());  ByteBuffer clientIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer serverOut=ByteBuffer.wrap("server".getBytes());  ByteBuffer serverIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer cTOs=ByteBuffer.allocateDirect(netBufferMax);  ByteBuffer sTOc=ByteBuffer.allocateDirect(netBufferMax);  boolean dataDone=false;  try {    while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {
  SSLEngine clientSSLEngine=clientSSLFactory.createSSLEngine();  clientSSLEngine.setEnabledCipherSuites(StringUtils.getTrimmedStrings(excludeCiphers));  SSLSession session=clientSSLEngine.getSession();  int appBufferMax=session.getApplicationBufferSize();  int netBufferMax=session.getPacketBufferSize();  ByteBuffer clientOut=ByteBuffer.wrap("client".getBytes());  ByteBuffer clientIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer serverOut=ByteBuffer.wrap("server".getBytes());  ByteBuffer serverIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer cTOs=ByteBuffer.allocateDirect(netBufferMax);  ByteBuffer sTOc=ByteBuffer.allocateDirect(netBufferMax);  boolean dataDone=false;  try {    while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {      LOG.info("client wrap " + wrap(clientSSLEngine,clientOut,cTOs));
  int appBufferMax=session.getApplicationBufferSize();  int netBufferMax=session.getPacketBufferSize();  ByteBuffer clientOut=ByteBuffer.wrap("client".getBytes());  ByteBuffer clientIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer serverOut=ByteBuffer.wrap("server".getBytes());  ByteBuffer serverIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer cTOs=ByteBuffer.allocateDirect(netBufferMax);  ByteBuffer sTOc=ByteBuffer.allocateDirect(netBufferMax);  boolean dataDone=false;  try {    while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {      LOG.info("client wrap " + wrap(clientSSLEngine,clientOut,cTOs));      LOG.info("server wrap " + wrap(serverSSLEngine,serverOut,sTOc));      cTOs.flip();      sTOc.flip();
  int netBufferMax=session.getPacketBufferSize();  ByteBuffer clientOut=ByteBuffer.wrap("client".getBytes());  ByteBuffer clientIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer serverOut=ByteBuffer.wrap("server".getBytes());  ByteBuffer serverIn=ByteBuffer.allocate(appBufferMax);  ByteBuffer cTOs=ByteBuffer.allocateDirect(netBufferMax);  ByteBuffer sTOc=ByteBuffer.allocateDirect(netBufferMax);  boolean dataDone=false;  try {    while (!isEngineClosed(clientSSLEngine) || !isEngineClosed(serverSSLEngine)) {      LOG.info("client wrap " + wrap(clientSSLEngine,clientOut,cTOs));      LOG.info("server wrap " + wrap(serverSSLEngine,serverOut,sTOc));      cTOs.flip();      sTOc.flip();      LOG.info("client unwrap " + unwrap(clientSSLEngine,sTOc,clientIn));
protected void assertLaunchOutcome(int expected,String text,String... args){  try {    if (LOG.isDebugEnabled()) {
protected void assertLaunchOutcome(int expected,String text,String... args){  try {    if (LOG.isDebugEnabled()) {      LOG.debug("Launching service with expected outcome {}",expected);      for (      String arg : args) {
@Override public Configuration bindArgs(Configuration config,List<String> args) throws Exception {  Assert.assertEquals(STATE.NOTINITED,getServiceState());  for (  String arg : args) {
  Preconditions.checkArgument(timeoutMillis >= 0,"timeoutMillis must be >= 0");  Preconditions.checkNotNull(timeoutHandler);  long endTime=Time.now() + timeoutMillis;  Throwable ex=null;  boolean running=true;  int iterations=0;  while (running) {    iterations++;    try {      if (check.call()) {        return iterations;      }      ex=null;    } catch (    InterruptedException|FailFastException|VirtualMachineError e) {      throw e;    }catch (    Throwable e) {
    } catch (    InterruptedException|FailFastException|VirtualMachineError e) {      throw e;    }catch (    Throwable e) {      LOG.debug("eventually() iteration {}",iterations,e);      ex=e;    }    running=Time.now() < endTime;    if (running) {      int sleeptime=retry.call();      if (sleeptime >= 0) {        Thread.sleep(sleeptime);      } else {        running=false;      }    }  }  Throwable evaluate;  try {    evaluate=timeoutHandler.evaluate(timeoutMillis,ex);
@Test(timeout=10000) public void testLogCapturer(){  final Logger log=LoggerFactory.getLogger(TestGenericTestUtils.class);  LogCapturer logCapturer=LogCapturer.captureLogs(log);  final String infoMessage="info message";
@Test(timeout=10000) public void testLogCapturer(){  final Logger log=LoggerFactory.getLogger(TestGenericTestUtils.class);  LogCapturer logCapturer=LogCapturer.captureLogs(log);  final String infoMessage="info message";  log.info(infoMessage);  assertTrue(logCapturer.getOutput().endsWith(String.format(infoMessage + "%n")));  logCapturer.clearOutput();  assertTrue(logCapturer.getOutput().isEmpty());  logCapturer.stopCapturing();
@Test(timeout=10000) public void testLogCapturerSlf4jLogger(){  final Logger logger=LoggerFactory.getLogger(TestGenericTestUtils.class);  LogCapturer logCapturer=LogCapturer.captureLogs(logger);  final String infoMessage="info message";
@Test(timeout=10000) public void testLogCapturerSlf4jLogger(){  final Logger logger=LoggerFactory.getLogger(TestGenericTestUtils.class);  LogCapturer logCapturer=LogCapturer.captureLogs(logger);  final String infoMessage="info message";  logger.info(infoMessage);  assertTrue(logCapturer.getOutput().endsWith(String.format(infoMessage + "%n")));  logCapturer.clearOutput();  assertTrue(logCapturer.getOutput().isEmpty());  logCapturer.stopCapturing();
@SuppressWarnings("UseOfSystemOutOrSystemErr") @Test public void testPrintLog4J() throws Throwable {  ByteArrayOutputStream baos=new ByteArrayOutputStream();  PrintStream out=new PrintStream(baos);  FindClass.setOutputStreams(out,System.err);  run(FindClass.SUCCESS,FindClass.A_PRINTRESOURCE,LOG4J_PROPERTIES);  out.flush();  String body=baos.toString("UTF8");
@Test(timeout=60000) public void testAdditionsAndRemovals(){  IdentityHashStore<Key,Integer> store=new IdentityHashStore<Key,Integer>(0);  final int NUM_KEYS=1000;
@POST @Path(KMSRESTConstants.KEYS_RESOURCE) @Consumes(MediaType.APPLICATION_JSON) @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) @SuppressWarnings("unchecked") public Response createKey(Map jsonKey) throws Exception {  try {    LOG.trace("Entering createKey Method.");    KMSWebApp.getAdminCallsMeter().mark();    UserGroupInformation user=HttpUserGroupInformation.get();    final String name=(String)jsonKey.get(KMSRESTConstants.NAME_FIELD);    checkNotEmpty(name,KMSRESTConstants.NAME_FIELD);    assertAccess(KMSACLs.Type.CREATE,user,KMSOp.CREATE_KEY,name);    String cipher=(String)jsonKey.get(KMSRESTConstants.CIPHER_FIELD);    final String material;    material=(String)jsonKey.get(KMSRESTConstants.MATERIAL_FIELD);    int length=(jsonKey.containsKey(KMSRESTConstants.LENGTH_FIELD)) ? (Integer)jsonKey.get(KMSRESTConstants.LENGTH_FIELD) : 0;    String description=(String)jsonKey.get(KMSRESTConstants.DESCRIPTION_FIELD);
@DELETE @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}") public Response deleteKey(@PathParam("name") final String name) throws Exception {  try {    LOG.trace("Entering deleteKey method.");    KMSWebApp.getAdminCallsMeter().mark();    UserGroupInformation user=HttpUserGroupInformation.get();    assertAccess(KMSACLs.Type.DELETE,user,KMSOp.DELETE_KEY,name);    checkNotEmpty(name,"name");
    KMSWebApp.getAdminCallsMeter().mark();    UserGroupInformation user=HttpUserGroupInformation.get();    assertAccess(KMSACLs.Type.DELETE,user,KMSOp.DELETE_KEY,name);    checkNotEmpty(name,"name");    LOG.debug("Deleting key with name {}.",name);    user.doAs(new PrivilegedExceptionAction<Void>(){      @Override public Void run() throws Exception {        provider.deleteKey(name);        provider.flush();        return null;      }    });    kmsAudit.ok(user,KMSOp.DELETE_KEY,name,"");    LOG.trace("Exiting deleteKey method.");    return Response.ok().build();  } catch (  Exception e) {
@POST @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}") @Consumes(MediaType.APPLICATION_JSON) @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response rolloverKey(@PathParam("name") final String name,Map jsonMaterial) throws Exception {  try {    LOG.trace("Entering rolloverKey Method.");    KMSWebApp.getAdminCallsMeter().mark();    UserGroupInformation user=HttpUserGroupInformation.get();    assertAccess(KMSACLs.Type.ROLLOVER,user,KMSOp.ROLL_NEW_VERSION,name);    checkNotEmpty(name,"name");
    if (material != null) {      assertAccess(KMSACLs.Type.SET_KEY_MATERIAL,user,KMSOp.ROLL_NEW_VERSION,name);    }    KeyProvider.KeyVersion keyVersion=user.doAs(new PrivilegedExceptionAction<KeyVersion>(){      @Override public KeyVersion run() throws Exception {        KeyVersion keyVersion=(material != null) ? provider.rollNewVersion(name,Base64.decodeBase64(material)) : provider.rollNewVersion(name);        provider.flush();        return keyVersion;      }    });    kmsAudit.ok(user,KMSOp.ROLL_NEW_VERSION,name,"UserProvidedMaterial:" + (material != null) + " NewVersion:"+ keyVersion.getVersionName());    if (!KMSWebApp.getACLs().hasAccess(KMSACLs.Type.GET,user)) {      keyVersion=removeKeyMaterial(keyVersion);    }    Map json=KMSUtil.toJSON(keyVersion);    LOG.trace("Exiting rolloverKey Method.");    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();  } catch (  Exception e) {
@POST @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}/" + KMSRESTConstants.INVALIDATECACHE_RESOURCE) public Response invalidateCache(@PathParam("name") final String name) throws Exception {  try {    LOG.trace("Entering invalidateCache Method.");    KMSWebApp.getAdminCallsMeter().mark();    checkNotEmpty(name,"name");    UserGroupInformation user=HttpUserGroupInformation.get();    assertAccess(KMSACLs.Type.ROLLOVER,user,KMSOp.INVALIDATE_CACHE,name);
    KMSWebApp.getAdminCallsMeter().mark();    checkNotEmpty(name,"name");    UserGroupInformation user=HttpUserGroupInformation.get();    assertAccess(KMSACLs.Type.ROLLOVER,user,KMSOp.INVALIDATE_CACHE,name);    LOG.debug("Invalidating cache with key name {}.",name);    user.doAs(new PrivilegedExceptionAction<Void>(){      @Override public Void run() throws Exception {        provider.invalidateCache(name);        provider.flush();        return null;      }    });    kmsAudit.ok(user,KMSOp.INVALIDATE_CACHE,name,"");    LOG.trace("Exiting invalidateCache for key name {}.",name);    return Response.ok().build();  } catch (  Exception e) {
  try {    LOG.trace("Entering getKeysMetadata method.");    KMSWebApp.getAdminCallsMeter().mark();    UserGroupInformation user=HttpUserGroupInformation.get();    final String[] keyNames=keyNamesList.toArray(new String[keyNamesList.size()]);    assertAccess(KMSACLs.Type.GET_METADATA,user,KMSOp.GET_KEYS_METADATA);    KeyProvider.Metadata[] keysMeta=user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata[]>(){      @Override public KeyProvider.Metadata[] run() throws Exception {        return provider.getKeysMetadata(keyNames);      }    });    Object json=KMSServerJSONUtils.toJSON(keyNames,keysMeta);    kmsAudit.ok(user,KMSOp.GET_KEYS_METADATA,"");    LOG.trace("Exiting getKeysMetadata method.");    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();  } catch (  Exception e) {
@GET @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}") public Response getKey(@PathParam("name") String name) throws Exception {  try {    LOG.trace("Entering getKey method.");
@GET @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}/" + KMSRESTConstants.METADATA_SUB_RESOURCE) @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response getMetadata(@PathParam("name") final String name) throws Exception {  try {    LOG.trace("Entering getMetadata method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    KMSWebApp.getAdminCallsMeter().mark();    assertAccess(KMSACLs.Type.GET_METADATA,user,KMSOp.GET_METADATA,name);
    LOG.trace("Entering getMetadata method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    KMSWebApp.getAdminCallsMeter().mark();    assertAccess(KMSACLs.Type.GET_METADATA,user,KMSOp.GET_METADATA,name);    LOG.debug("Getting metadata for key with name {}.",name);    KeyProvider.Metadata metadata=user.doAs(new PrivilegedExceptionAction<KeyProvider.Metadata>(){      @Override public KeyProvider.Metadata run() throws Exception {        return provider.getMetadata(name);      }    });    Object json=KMSServerJSONUtils.toJSON(name,metadata);    kmsAudit.ok(user,KMSOp.GET_METADATA,name,"");    LOG.trace("Exiting getMetadata method.");    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();  } catch (  Exception e) {
@GET @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}/" + KMSRESTConstants.CURRENT_VERSION_SUB_RESOURCE) @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response getCurrentVersion(@PathParam("name") final String name) throws Exception {  try {    LOG.trace("Entering getCurrentVersion method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    KMSWebApp.getKeyCallsMeter().mark();    assertAccess(KMSACLs.Type.GET,user,KMSOp.GET_CURRENT_KEY,name);
    LOG.trace("Entering getCurrentVersion method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    KMSWebApp.getKeyCallsMeter().mark();    assertAccess(KMSACLs.Type.GET,user,KMSOp.GET_CURRENT_KEY,name);    LOG.debug("Getting key version for key with name {}.",name);    KeyVersion keyVersion=user.doAs(new PrivilegedExceptionAction<KeyVersion>(){      @Override public KeyVersion run() throws Exception {        return provider.getCurrentKey(name);      }    });    Object json=KMSUtil.toJSON(keyVersion);    kmsAudit.ok(user,KMSOp.GET_CURRENT_KEY,name,"");    LOG.trace("Exiting getCurrentVersion method.");    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();  } catch (  Exception e) {
@GET @Path(KMSRESTConstants.KEY_VERSION_RESOURCE + "/{versionName:.*}") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response getKeyVersion(@PathParam("versionName") final String versionName) throws Exception {  try {    LOG.trace("Entering getKeyVersion method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(versionName,"versionName");    KMSWebApp.getKeyCallsMeter().mark();    assertAccess(KMSACLs.Type.GET,user,KMSOp.GET_KEY_VERSION);
    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(versionName,"versionName");    KMSWebApp.getKeyCallsMeter().mark();    assertAccess(KMSACLs.Type.GET,user,KMSOp.GET_KEY_VERSION);    LOG.debug("Getting key with version name {}.",versionName);    KeyVersion keyVersion=user.doAs(new PrivilegedExceptionAction<KeyVersion>(){      @Override public KeyVersion run() throws Exception {        return provider.getKeyVersion(versionName);      }    });    if (keyVersion != null) {      kmsAudit.ok(user,KMSOp.GET_KEY_VERSION,keyVersion.getName(),"");    }    Object json=KMSUtil.toJSON(keyVersion);    LOG.trace("Exiting getKeyVersion method.");    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();  } catch (  Exception e) {
@SuppressWarnings({"rawtypes","unchecked"}) @GET @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}/" + KMSRESTConstants.EEK_SUB_RESOURCE) @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response generateEncryptedKeys(@PathParam("name") final String name,@QueryParam(KMSRESTConstants.EEK_OP) String edekOp,@DefaultValue("1") @QueryParam(KMSRESTConstants.EEK_NUM_KEYS) final int numKeys) throws Exception {  try {    LOG.trace("Entering generateEncryptedKeys method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    checkNotNull(edekOp,"eekOp");
@SuppressWarnings({"rawtypes","unchecked"}) @GET @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}/" + KMSRESTConstants.EEK_SUB_RESOURCE) @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response generateEncryptedKeys(@PathParam("name") final String name,@QueryParam(KMSRESTConstants.EEK_OP) String edekOp,@DefaultValue("1") @QueryParam(KMSRESTConstants.EEK_NUM_KEYS) final int numKeys) throws Exception {  try {    LOG.trace("Entering generateEncryptedKeys method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    checkNotNull(edekOp,"eekOp");    LOG.debug("Generating encrypted key with name {}," + " the edek Operation is {}.",name,edekOp);    Object retJSON;    if (edekOp.equals(KMSRESTConstants.EEK_GENERATE)) {      LOG.debug("edek Operation is Generate.");      assertAccess(KMSACLs.Type.GENERATE_EEK,user,KMSOp.GENERATE_EEK,name);      final List<EncryptedKeyVersion> retEdeks=new LinkedList<EncryptedKeyVersion>();      try {        user.doAs(new PrivilegedExceptionAction<Void>(){          @Override public Void run() throws Exception {
    LOG.debug("Generating encrypted key with name {}," + " the edek Operation is {}.",name,edekOp);    Object retJSON;    if (edekOp.equals(KMSRESTConstants.EEK_GENERATE)) {      LOG.debug("edek Operation is Generate.");      assertAccess(KMSACLs.Type.GENERATE_EEK,user,KMSOp.GENERATE_EEK,name);      final List<EncryptedKeyVersion> retEdeks=new LinkedList<EncryptedKeyVersion>();      try {        user.doAs(new PrivilegedExceptionAction<Void>(){          @Override public Void run() throws Exception {            LOG.debug("Generated Encrypted key for {} number of " + "keys.",numKeys);            for (int i=0; i < numKeys; i++) {              retEdeks.add(provider.generateEncryptedKey(name));            }            return null;          }        });      } catch (      Exception e) {
          }        });      } catch (      Exception e) {        LOG.error("Exception in generateEncryptedKeys:",e);        throw new IOException(e);      }      kmsAudit.ok(user,KMSOp.GENERATE_EEK,name,"");      retJSON=new ArrayList();      for (      EncryptedKeyVersion edek : retEdeks) {        ((ArrayList)retJSON).add(KMSUtil.toJSON(edek));      }    } else {      StringBuilder error;      error=new StringBuilder("IllegalArgumentException Wrong ");      error.append(KMSRESTConstants.EEK_OP);      error.append(" value, it must be ");      error.append(KMSRESTConstants.EEK_GENERATE);      error.append(" or ");
        throw new IOException(e);      }      kmsAudit.ok(user,KMSOp.GENERATE_EEK,name,"");      retJSON=new ArrayList();      for (      EncryptedKeyVersion edek : retEdeks) {        ((ArrayList)retJSON).add(KMSUtil.toJSON(edek));      }    } else {      StringBuilder error;      error=new StringBuilder("IllegalArgumentException Wrong ");      error.append(KMSRESTConstants.EEK_OP);      error.append(" value, it must be ");      error.append(KMSRESTConstants.EEK_GENERATE);      error.append(" or ");      error.append(KMSRESTConstants.EEK_DECRYPT);      LOG.error(error.toString());      throw new IllegalArgumentException(error.toString());
    }    assertAccess(KMSACLs.Type.GENERATE_EEK,user,KMSOp.REENCRYPT_EEK_BATCH,name);    LOG.debug("Batch reencrypting {} Encrypted Keys for key name {}",jsonPayload.size(),name);    final List<EncryptedKeyVersion> ekvs=KMSUtil.parseJSONEncKeyVersions(name,jsonPayload);    Preconditions.checkArgument(ekvs.size() == jsonPayload.size(),"EncryptedKey size mismatch after parsing from json");    for (    EncryptedKeyVersion ekv : ekvs) {      Preconditions.checkArgument(name.equals(ekv.getEncryptionKeyName()),"All EncryptedKeys must be under the given key name " + name);    }    user.doAs(new PrivilegedExceptionAction<Void>(){      @Override public Void run() throws Exception {        provider.reencryptEncryptedKeys(ekvs);        return null;      }    });    List retJSON=new ArrayList<>(ekvs.size());    for (    EncryptedKeyVersion ekv : ekvs) {      retJSON.add(KMSUtil.toJSON(ekv));    }    kmsAudit.ok(user,KMSOp.REENCRYPT_EEK_BATCH,name,"reencrypted " + ekvs.size() + " keys");
@GET @Path(KMSRESTConstants.KEY_RESOURCE + "/{name:.*}/" + KMSRESTConstants.VERSIONS_SUB_RESOURCE) @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response getKeyVersions(@PathParam("name") final String name) throws Exception {  try {    LOG.trace("Entering getKeyVersions method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    KMSWebApp.getKeyCallsMeter().mark();    assertAccess(KMSACLs.Type.GET,user,KMSOp.GET_KEY_VERSIONS,name);
    LOG.trace("Entering getKeyVersions method.");    UserGroupInformation user=HttpUserGroupInformation.get();    checkNotEmpty(name,"name");    KMSWebApp.getKeyCallsMeter().mark();    assertAccess(KMSACLs.Type.GET,user,KMSOp.GET_KEY_VERSIONS,name);    LOG.debug("Getting key versions for key {}",name);    List<KeyVersion> ret=user.doAs(new PrivilegedExceptionAction<List<KeyVersion>>(){      @Override public List<KeyVersion> run() throws Exception {        return provider.getKeyVersions(name);      }    });    Object json=KMSServerJSONUtils.toJSON(ret);    kmsAudit.ok(user,KMSOp.GET_KEY_VERSIONS,name,"");    LOG.trace("Exiting getKeyVersions method.");    return Response.ok().type(MediaType.APPLICATION_JSON).entity(json).build();  } catch (  Exception e) {
private void setKMSACLs(Configuration conf){  Map<Type,AccessControlList> tempAcls=new HashMap<Type,AccessControlList>();  Map<Type,AccessControlList> tempBlacklist=new HashMap<Type,AccessControlList>();  for (  Type aclType : Type.values()) {    String aclStr=conf.get(aclType.getAclConfigKey(),ACL_DEFAULT);    tempAcls.put(aclType,new AccessControlList(aclStr));    String blacklistStr=conf.get(aclType.getBlacklistConfigKey());    if (blacklistStr != null) {      tempBlacklist.put(aclType,new AccessControlList(blacklistStr));
    } else {      String aclStr=keyAcl.getValue();      String keyName=k.substring(keyNameStarts,keyNameEnds);      String keyOp=k.substring(keyNameEnds + 1);      KeyOpType aclType=null;      try {        aclType=KeyOpType.valueOf(keyOp);      } catch (      IllegalArgumentException e) {        LOG.warn("Invalid key Operation '{}'",keyOp);      }      if (aclType != null) {        HashMap<KeyOpType,AccessControlList> aclMap=tempKeyAcls.get(keyName);        if (aclMap == null) {          aclMap=new HashMap<KeyOpType,AccessControlList>();          tempKeyAcls.put(keyName,aclMap);        }        aclMap.put(aclType,new AccessControlList(aclStr));
public boolean hasAccess(Type type,UserGroupInformation ugi){  boolean access=acls.get(type).isUserAllowed(ugi);  if (LOG.isDebugEnabled()) {
public boolean hasAccess(Type type,UserGroupInformation ugi){  boolean access=acls.get(type).isUserAllowed(ugi);  if (LOG.isDebugEnabled()) {    LOG.debug("Checking user [{}] for: {} {} ",ugi.getShortUserName(),type.toString(),acls.get(type).getAclString());  }  if (access) {    AccessControlList blacklist=blacklistedAcls.get(type);    access=(blacklist == null) || !blacklist.isUserInList(ugi);    if (LOG.isDebugEnabled()) {      if (blacklist == null) {        LOG.debug("No blacklist for {}",type.toString());      } else       if (access) {        LOG.debug("user is not in {}",blacklist.getAclString());      } else {        LOG.debug("user is in {}",blacklist.getAclString());      }    }  }  if (LOG.isDebugEnabled()) {
private boolean checkKeyAccess(String keyName,UserGroupInformation ugi,KeyOpType opType){  Map<KeyOpType,AccessControlList> keyAcl=keyAcls.get(keyName);  if (keyAcl == null) {
private boolean checkKeyAccess(Map<KeyOpType,AccessControlList> keyAcl,UserGroupInformation ugi,KeyOpType opType){  AccessControlList acl=keyAcl.get(opType);  if (acl == null) {
@Override public void contextInitialized(ServletContextEvent sce){  try {    kmsConf=KMSConfiguration.getKMSConf();    UserGroupInformation.setConfiguration(kmsConf);    LOG.info("-------------------------------------------------------------");    LOG.info("  Java runtime version : {}",System.getProperty("java.runtime.version"));
    if (providerString == null) {      throw new IllegalStateException("No KeyProvider has been defined");    }    KeyProvider keyProvider=KeyProviderFactory.get(new URI(providerString),kmsConf);    Preconditions.checkNotNull(keyProvider,String.format("No" + " KeyProvider has been initialized, please" + " check whether %s '%s' is configured correctly in"+ " kms-site.xml.",KMSConfiguration.KEY_PROVIDER_URI,providerString));    if (kmsConf.getBoolean(KMSConfiguration.KEY_CACHE_ENABLE,KMSConfiguration.KEY_CACHE_ENABLE_DEFAULT)) {      long keyTimeOutMillis=kmsConf.getLong(KMSConfiguration.KEY_CACHE_TIMEOUT_KEY,KMSConfiguration.KEY_CACHE_TIMEOUT_DEFAULT);      long currKeyTimeOutMillis=kmsConf.getLong(KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_KEY,KMSConfiguration.CURR_KEY_CACHE_TIMEOUT_DEFAULT);      keyProvider=new CachingKeyProvider(keyProvider,keyTimeOutMillis,currKeyTimeOutMillis);    }    LOG.info("Initialized KeyProvider " + keyProvider);    keyProviderCryptoExtension=KeyProviderCryptoExtension.createKeyProviderCryptoExtension(keyProvider);    keyProviderCryptoExtension=new EagerKeyGeneratorKeyProviderCryptoExtension(kmsConf,keyProviderCryptoExtension);    if (kmsConf.getBoolean(KMSConfiguration.KEY_AUTHORIZATION_ENABLE,KMSConfiguration.KEY_AUTHORIZATION_ENABLE_DEFAULT)) {      keyProviderCryptoExtension=new KeyAuthorizationKeyProvider(keyProviderCryptoExtension,kmsAcls);    }    LOG.info("Initialized KeyProviderCryptoExtension " + keyProviderCryptoExtension);    final int defaultBitlength=kmsConf.getInt(KeyProvider.DEFAULT_BITLENGTH_NAME,KeyProvider.DEFAULT_BITLENGTH);
    printUsage();  }  String type=args.get(1);  boolean runAll=OperationStatsBase.OP_ALL_NAME.equals(type);  List<OperationStatsBase> ops=new ArrayList<OperationStatsBase>();  OperationStatsBase opStat=null;  try {    if (runAll || EncryptKeyStats.OP_ENCRYPT_KEY.equals(type)) {      opStat=new EncryptKeyStats(args);      ops.add(opStat);    }    if (runAll || DecryptKeyStats.OP_DECRYPT_KEY.equals(type)) {      opStat=new DecryptKeyStats(args);      ops.add(opStat);    }    if (ops.isEmpty()) {      printUsage();    }    for (    OperationStatsBase op : ops) {
  File confDir=getTestDir();  conf=createBaseKMSConf(confDir,conf);  conf.set(KeyAuthorizationKeyProvider.KEY_ACL + specialKey + ".ALL","*");  writeConf(confDir,conf);  runServer(null,null,confDir,new KMSCallable<Void>(){    @Override public Void call() throws Exception {      Configuration conf=new Configuration();      URI uri=createKMSUri(getKMSUrl());      KeyProvider kp=createProvider(uri,conf);      Assert.assertTrue(kp.getKeys().isEmpty());      Assert.assertEquals(0,kp.getKeysMetadata().length);      KeyProvider.Options options=new KeyProvider.Options(conf);      options.setCipher("AES/CTR/NoPadding");      options.setBitLength(128);      options.setDescription("l1");
            EncryptedKeyVersion ek1=kpCE.generateEncryptedKey(currKv.getName());            return ek1;          } catch (          Exception ex) {            Assert.fail(ex.toString());          }          return null;        }      });      doAs("GENERATE_EEK",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          KeyProvider kp=createProvider(uri,conf);          KeyProviderCryptoExtension kpCE=KeyProviderCryptoExtension.createKeyProviderCryptoExtension(kp);          kpCE.reencryptEncryptedKey(encKv);          List<EncryptedKeyVersion> ekvs=new ArrayList<>(2);          ekvs.add(encKv);          ekvs.add(encKv);          kpCE.reencryptEncryptedKeys(ekvs);
          KeyProviderCryptoExtension kpCE=KeyProviderCryptoExtension.createKeyProviderCryptoExtension(kp);          kpCE.reencryptEncryptedKey(encKv);          List<EncryptedKeyVersion> ekvs=new ArrayList<>(2);          ekvs.add(encKv);          ekvs.add(encKv);          kpCE.reencryptEncryptedKeys(ekvs);          return null;        }      });      doAs("DECRYPT_EEK",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          KeyProvider kp=createProvider(uri,conf);          try {            KeyProviderCryptoExtension kpCE=KeyProviderCryptoExtension.createKeyProviderCryptoExtension(kp);            kpCE.decryptEncryptedKey(encKv);          } catch (          Exception ex) {
            KeyProviderCryptoExtension kpCE=KeyProviderCryptoExtension.createKeyProviderCryptoExtension(kp);            kpCE.decryptEncryptedKey(encKv);          } catch (          Exception ex) {            Assert.fail(ex.getMessage());          }          return null;        }      });      doAs("GET_KEYS",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          KeyProvider kp=createProvider(uri,conf);          try {            kp.getKeys();          } catch (          Exception ex) {            Assert.fail(ex.getMessage());          }          return null;        }      });
    @Override public Void call() throws Exception {      final Configuration clientConf=new Configuration();      final URI uri=createKMSUri(getKMSUrl());      clientConf.set(KeyProviderFactory.KEY_PROVIDER_PATH,createKMSUri(getKMSUrl()).toString());      doAs("client",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          KeyProvider kp=createProvider(uri,clientConf);          clientConf.unset(HADOOP_SECURITY_KEY_PROVIDER_PATH);          KeyProviderDelegationTokenExtension kpdte=KeyProviderDelegationTokenExtension.createKeyProviderDelegationTokenExtension(kp);          final Credentials credentials=new Credentials();          final Token<?>[] tokens=kpdte.addDelegationTokens("client1",credentials);          Text tokenService=getTokenService(kp);          Assert.assertEquals(1,credentials.getAllTokens().size());          Assert.assertEquals(KMSDelegationToken.TOKEN_KIND,credentials.getToken(tokenService).getKind());          for (          Token<?> token : tokens) {
            LOG.info("Got dt for " + uri + "; "+ token);            try {              token.renew(clientConf);              Assert.fail("client should not be allowed to renew token with" + "renewer=client1");            } catch (            Exception e) {              final DelegationTokenIdentifier identifier=(DelegationTokenIdentifier)token.decodeIdentifier();              GenericTestUtils.assertExceptionContains("tries to renew a token (" + identifier + ") with non-matching renewer",e);            }          }          final UserGroupInformation otherUgi;          if (kerb) {            UserGroupInformation.loginUserFromKeytab("client1",keytab.getAbsolutePath());            otherUgi=UserGroupInformation.getLoginUser();          } else {            otherUgi=UserGroupInformation.createUserForTesting("client1",new String[]{"other group"});            UserGroupInformation.setLoginUser(otherUgi);          }          try {
              Assert.fail("client should not be allowed to renew token with" + "renewer=client1");            } catch (            Exception e) {              final DelegationTokenIdentifier identifier=(DelegationTokenIdentifier)token.decodeIdentifier();              GenericTestUtils.assertExceptionContains("tries to renew a token (" + identifier + ") with non-matching renewer",e);            }          }          final UserGroupInformation otherUgi;          if (kerb) {            UserGroupInformation.loginUserFromKeytab("client1",keytab.getAbsolutePath());            otherUgi=UserGroupInformation.getLoginUser();          } else {            otherUgi=UserGroupInformation.createUserForTesting("client1",new String[]{"other group"});            UserGroupInformation.setLoginUser(otherUgi);          }          try {            otherUgi.doAs(new PrivilegedExceptionAction<Void>(){              @Override public Void run() throws Exception {                boolean renewed=false;
            } catch (            Exception e) {              final DelegationTokenIdentifier identifier=(DelegationTokenIdentifier)token.decodeIdentifier();              GenericTestUtils.assertExceptionContains("tries to renew a token (" + identifier + ") with non-matching renewer",e);            }          }          final UserGroupInformation otherUgi;          if (kerb) {            UserGroupInformation.loginUserFromKeytab("client1",keytab.getAbsolutePath());            otherUgi=UserGroupInformation.getLoginUser();          } else {            otherUgi=UserGroupInformation.createUserForTesting("client1",new String[]{"other group"});            UserGroupInformation.setLoginUser(otherUgi);          }          try {            otherUgi.doAs(new PrivilegedExceptionAction<Void>(){              @Override public Void run() throws Exception {                boolean renewed=false;                for (                Token<?> token : tokens) {
              GenericTestUtils.assertExceptionContains("tries to renew a token (" + identifier + ") with non-matching renewer",e);            }          }          final UserGroupInformation otherUgi;          if (kerb) {            UserGroupInformation.loginUserFromKeytab("client1",keytab.getAbsolutePath());            otherUgi=UserGroupInformation.getLoginUser();          } else {            otherUgi=UserGroupInformation.createUserForTesting("client1",new String[]{"other group"});            UserGroupInformation.setLoginUser(otherUgi);          }          try {            otherUgi.doAs(new PrivilegedExceptionAction<Void>(){              @Override public Void run() throws Exception {                boolean renewed=false;                for (                Token<?> token : tokens) {                  if (!(token.getKind().equals(KMSDelegationToken.TOKEN_KIND))) {                    LOG.info("Skipping token {}",token);
            otherUgi=UserGroupInformation.getLoginUser();          } else {            otherUgi=UserGroupInformation.createUserForTesting("client1",new String[]{"other group"});            UserGroupInformation.setLoginUser(otherUgi);          }          try {            otherUgi.doAs(new PrivilegedExceptionAction<Void>(){              @Override public Void run() throws Exception {                boolean renewed=false;                for (                Token<?> token : tokens) {                  if (!(token.getKind().equals(KMSDelegationToken.TOKEN_KIND))) {                    LOG.info("Skipping token {}",token);                    continue;                  }                  LOG.info("Got dt for " + uri + "; "+ token);                  long tokenLife=token.renew(clientConf);                  LOG.info("Renewed token of kind {}, new lifetime:{}",token.getKind(),tokenLife);
            otherUgi=UserGroupInformation.createUserForTesting("client1",new String[]{"other group"});            UserGroupInformation.setLoginUser(otherUgi);          }          try {            otherUgi.doAs(new PrivilegedExceptionAction<Void>(){              @Override public Void run() throws Exception {                boolean renewed=false;                for (                Token<?> token : tokens) {                  if (!(token.getKind().equals(KMSDelegationToken.TOKEN_KIND))) {                    LOG.info("Skipping token {}",token);                    continue;                  }                  LOG.info("Got dt for " + uri + "; "+ token);                  long tokenLife=token.renew(clientConf);                  LOG.info("Renewed token of kind {}, new lifetime:{}",token.getKind(),tokenLife);                  Thread.sleep(100);                  long newTokenLife=token.renew(clientConf);
            UserGroupInformation.setLoginUser(otherUgi);          }          try {            otherUgi.doAs(new PrivilegedExceptionAction<Void>(){              @Override public Void run() throws Exception {                boolean renewed=false;                for (                Token<?> token : tokens) {                  if (!(token.getKind().equals(KMSDelegationToken.TOKEN_KIND))) {                    LOG.info("Skipping token {}",token);                    continue;                  }                  LOG.info("Got dt for " + uri + "; "+ token);                  long tokenLife=token.renew(clientConf);                  LOG.info("Renewed token of kind {}, new lifetime:{}",token.getKind(),tokenLife);                  Thread.sleep(100);                  long newTokenLife=token.renew(clientConf);                  LOG.info("Renewed token of kind {}, new lifetime:{}",token.getKind(),newTokenLife);
          Token<?> token=UserGroupInformation.getCurrentUser().getCredentials().getToken(tokenService);          Assert.assertNotNull(token);          job1Token.add(token);          ByteArrayInputStream buf=new ByteArrayInputStream(token.getIdentifier());          DataInputStream dis=new DataInputStream(buf);          DelegationTokenIdentifier id=new DelegationTokenIdentifier(token.getKind());          id.readFields(dis);          dis.close();          final long maxTime=id.getMaxDate();          Thread.sleep(5100);          Assert.assertTrue("maxTime " + maxTime + " is not less than now.",maxTime > 0 && maxTime < Time.now());          try {            kp.getKeys();            Assert.fail("Operation should fail since dt is expired.");          } catch (          Exception e) {
      final String lbUri=generateLoadBalancingKeyProviderUriString();      final LoadBalancingKMSClientProvider lbkp=createHAProvider(URI.create(lbUri),uris,conf);      conf.unset(HADOOP_SECURITY_KEY_PROVIDER_PATH);      doAs("SET_KEY_MATERIAL",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          KeyProviderDelegationTokenExtension kpdte=KeyProviderDelegationTokenExtension.createKeyProviderDelegationTokenExtension(lbkp);          kpdte.addDelegationTokens("foo",credentials);          return null;        }      });      nonKerberosUgi.addCredentials(credentials);      nonKerberosUgi.doAs(new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          int i=0;          for (          KMSClientProvider provider : lbkp.getProviders()) {            final String key="k" + i++;
      nonKerberosUgi.doAs(new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          int i=0;          for (          KMSClientProvider provider : lbkp.getProviders()) {            final String key="k" + i++;            LOG.info("Connect to {} to create key {}.",provider,key);            provider.createKey(key,new KeyProvider.Options(conf));          }          return null;        }      });      final Collection<Token<? extends TokenIdentifier>> tokens=credentials.getAllTokens();      doAs("foo",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          assertEquals(1,tokens.size());          Token token=tokens.iterator().next();          assertEquals(KMSDelegationToken.TOKEN_KIND,token.getKind());
          for (          KMSClientProvider provider : lbkp.getProviders()) {            final String key="k" + i++;            LOG.info("Connect to {} to create key {}.",provider,key);            provider.createKey(key,new KeyProvider.Options(conf));          }          return null;        }      });      final Collection<Token<? extends TokenIdentifier>> tokens=credentials.getAllTokens();      doAs("foo",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          assertEquals(1,tokens.size());          Token token=tokens.iterator().next();          assertEquals(KMSDelegationToken.TOKEN_KIND,token.getKind());          LOG.info("Got dt for token: {}",token);          final long tokenLife=token.renew(conf);          LOG.info("Renewed token {}, new lifetime:{}",token,tokenLife);
            LOG.info("Connect to {} to create key {}.",provider,key);            provider.createKey(key,new KeyProvider.Options(conf));          }          return null;        }      });      final Collection<Token<? extends TokenIdentifier>> tokens=credentials.getAllTokens();      doAs("foo",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          assertEquals(1,tokens.size());          Token token=tokens.iterator().next();          assertEquals(KMSDelegationToken.TOKEN_KIND,token.getKind());          LOG.info("Got dt for token: {}",token);          final long tokenLife=token.renew(conf);          LOG.info("Renewed token {}, new lifetime:{}",token,tokenLife);          Thread.sleep(10);          final long newTokenLife=token.renew(conf);
          }          return null;        }      });      final Collection<Token<? extends TokenIdentifier>> tokens=credentials.getAllTokens();      doAs("foo",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          assertEquals(1,tokens.size());          Token token=tokens.iterator().next();          assertEquals(KMSDelegationToken.TOKEN_KIND,token.getKind());          LOG.info("Got dt for token: {}",token);          final long tokenLife=token.renew(conf);          LOG.info("Renewed token {}, new lifetime:{}",token,tokenLife);          Thread.sleep(10);          final long newTokenLife=token.renew(conf);          LOG.info("Renewed token {}, new lifetime:{}",token,newTokenLife);          assertTrue(newTokenLife > tokenLife);
        }      });      final Credentials newCredentials=new Credentials();      doAs("SET_KEY_MATERIAL",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          KeyProviderDelegationTokenExtension kpdte=KeyProviderDelegationTokenExtension.createKeyProviderDelegationTokenExtension(lbkp);          kpdte.addDelegationTokens("foo",newCredentials);          return null;        }      });      doAs("foo",new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          KMSClientProvider kp1=lbkp.getProviders()[0];          URL[] urls=getKMSHAUrl();          final Collection<Token<? extends TokenIdentifier>> tokens=newCredentials.getAllTokens();          assertEquals(1,tokens.size());          Token token=tokens.iterator().next();
@Test public void testKMSJMX() throws Exception {  Configuration conf=new Configuration();  final File confDir=getTestDir();  conf=createBaseKMSConf(confDir,conf);  final String processName="testkmsjmx";  conf.set(KMSConfiguration.METRICS_PROCESS_NAME_KEY,processName);  writeConf(confDir,conf);  runServer(null,null,confDir,new KMSCallable<Void>(){    @Override public Void call() throws Exception {      final URL jmxUrl=new URL(getKMSUrl() + "/jmx?user.name=whatever&qry=Hadoop:service=" + processName+ ",name=JvmMetrics");
public synchronized void createPrincipal(File keytabFile,String... principals) throws Exception {  simpleKdc.createPrincipals(principals);  if (keytabFile.exists() && !keytabFile.delete()) {
private static Match getMatch(String line){  String[] parts=line.split("\\s+");  final String host;  AccessPrivilege privilege=AccessPrivilege.READ_ONLY;switch (parts.length) {case 1:    host=StringUtils.toLowerCase(parts[0]).trim();  break;case 2:host=StringUtils.toLowerCase(parts[0]).trim();String option=parts[1].trim();if ("rw".equalsIgnoreCase(option)) {privilege=AccessPrivilege.READ_WRITE;}break;default:throw new IllegalArgumentException("Incorrectly formatted line '" + line + "'");}if (host.equals("*")) {if (LOG.isDebugEnabled()) {
switch (parts.length) {case 1:    host=StringUtils.toLowerCase(parts[0]).trim();  break;case 2:host=StringUtils.toLowerCase(parts[0]).trim();String option=parts[1].trim();if ("rw".equalsIgnoreCase(option)) {privilege=AccessPrivilege.READ_WRITE;}break;default:throw new IllegalArgumentException("Incorrectly formatted line '" + line + "'");}if (host.equals("*")) {if (LOG.isDebugEnabled()) {LOG.debug("Using match all for '" + host + "' and "+ privilege);}return new AnonymousMatch(privilege);} else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {if (LOG.isDebugEnabled()) {
String option=parts[1].trim();if ("rw".equalsIgnoreCase(option)) {privilege=AccessPrivilege.READ_WRITE;}break;default:throw new IllegalArgumentException("Incorrectly formatted line '" + line + "'");}if (host.equals("*")) {if (LOG.isDebugEnabled()) {LOG.debug("Using match all for '" + host + "' and "+ privilege);}return new AnonymousMatch(privilege);} else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using CIDR match for '" + host + "' and "+ privilege);}return new CIDRMatch(privilege,new SubnetUtils(host).getInfo());} else if (CIDR_FORMAT_LONG.matcher(host).matches()) {if (LOG.isDebugEnabled()) {
default:throw new IllegalArgumentException("Incorrectly formatted line '" + line + "'");}if (host.equals("*")) {if (LOG.isDebugEnabled()) {LOG.debug("Using match all for '" + host + "' and "+ privilege);}return new AnonymousMatch(privilege);} else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using CIDR match for '" + host + "' and "+ privilege);}return new CIDRMatch(privilege,new SubnetUtils(host).getInfo());} else if (CIDR_FORMAT_LONG.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using CIDR match for '" + host + "' and "+ privilege);}String[] pair=host.split("/");return new CIDRMatch(privilege,new SubnetUtils(pair[0],pair[1]).getInfo());} else if (host.contains("*") || host.contains("?") || host.contains("[")|| host.contains("]")|| host.contains("(")|| host.contains(")")) {
LOG.debug("Using match all for '" + host + "' and "+ privilege);}return new AnonymousMatch(privilege);} else if (CIDR_FORMAT_SHORT.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using CIDR match for '" + host + "' and "+ privilege);}return new CIDRMatch(privilege,new SubnetUtils(host).getInfo());} else if (CIDR_FORMAT_LONG.matcher(host).matches()) {if (LOG.isDebugEnabled()) {LOG.debug("Using CIDR match for '" + host + "' and "+ privilege);}String[] pair=host.split("/");return new CIDRMatch(privilege,new SubnetUtils(pair[0],pair[1]).getInfo());} else if (host.contains("*") || host.contains("?") || host.contains("[")|| host.contains("]")|| host.contains("(")|| host.contains(")")) {if (LOG.isDebugEnabled()) {LOG.debug("Using Regex match for '" + host + "' and "+ privilege);}return new RegexMatch(privilege,host);
public void register(int transport,int boundPort){  if (boundPort != port) {
public void unregister(int transport,int boundPort){  if (boundPort != port) {
    factory=new NioServerSocketChannelFactory(Executors.newCachedThreadPool(),Executors.newCachedThreadPool());  } else {    factory=new NioServerSocketChannelFactory(Executors.newCachedThreadPool(),Executors.newCachedThreadPool(),workerCount);  }  server=new ServerBootstrap(factory);  server.setPipelineFactory(new ChannelPipelineFactory(){    @Override public ChannelPipeline getPipeline() throws Exception {      return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),RpcUtil.STAGE_RPC_MESSAGE_PARSER,rpcProgram,RpcUtil.STAGE_RPC_TCP_RESPONSE);    }  });  server.setOption("child.tcpNoDelay",true);  server.setOption("child.keepAlive",true);  server.setOption("child.reuseAddress",true);  server.setOption("reuseAddress",true);  ch=server.bind(new InetSocketAddress(port));  InetSocketAddress socketAddr=(InetSocketAddress)ch.getLocalAddress();  boundPort=socketAddr.getPort();
public void run(){  DatagramChannelFactory f=new NioDatagramChannelFactory(Executors.newCachedThreadPool(),workerCount);  server=new ConnectionlessBootstrap(f);  server.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER,rpcProgram,RpcUtil.STAGE_RPC_UDP_RESPONSE));  server.setOption("broadcast","false");  server.setOption("sendBufferSize",SEND_BUFFER_SIZE);  server.setOption("receiveBufferSize",RECEIVE_BUFFER_SIZE);  server.setOption("reuseAddress",true);  ch=server.bind(new InetSocketAddress(port));  InetSocketAddress socketAddr=(InetSocketAddress)ch.getLocalAddress();  boundPort=socketAddr.getPort();
  tcpServer.setPipelineFactory(new ChannelPipelineFactory(){    private final HashedWheelTimer timer=new HashedWheelTimer();    private final IdleStateHandler idleStateHandler=new IdleStateHandler(timer,0,0,idleTimeMilliSeconds,TimeUnit.MILLISECONDS);    @Override public ChannelPipeline getPipeline() throws Exception {      return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),RpcUtil.STAGE_RPC_MESSAGE_PARSER,idleStateHandler,handler,RpcUtil.STAGE_RPC_TCP_RESPONSE);    }  });  tcpServer.setOption("reuseAddress",true);  tcpServer.setOption("child.reuseAddress",true);  udpServer=new ConnectionlessBootstrap(new NioDatagramChannelFactory(Executors.newCachedThreadPool()));  udpServer.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER,handler,RpcUtil.STAGE_RPC_UDP_RESPONSE));  udpServer.setOption("reuseAddress",true);  tcpChannel=tcpServer.bind(tcpAddress);  udpChannel=udpServer.bind(udpAddress);  allChannels.add(tcpChannel);  allChannels.add(udpChannel);
private XDR set(int xid,XDR in,XDR out){  PortmapMapping mapping=PortmapRequest.mapping(in);  String key=PortmapMapping.key(mapping);  if (LOG.isDebugEnabled()) {
private XDR getport(int xid,XDR in,XDR out){  PortmapMapping mapping=PortmapRequest.mapping(in);  String key=PortmapMapping.key(mapping);  if (LOG.isDebugEnabled()) {
  XDR in=new XDR(info.data().toByteBuffer().asReadOnlyBuffer(),XDR.State.READING);  XDR out=new XDR();  if (portmapProc == PMAPPROC_NULL) {    out=nullOp(xid,in,out);  } else   if (portmapProc == PMAPPROC_SET) {    out=set(xid,in,out);  } else   if (portmapProc == PMAPPROC_UNSET) {    out=unset(xid,in,out);  } else   if (portmapProc == PMAPPROC_DUMP) {    out=dump(xid,in,out);  } else   if (portmapProc == PMAPPROC_GETPORT) {    out=getport(xid,in,out);  } else   if (portmapProc == PMAPPROC_GETVERSADDR) {    out=getport(xid,in,out);  } else {
String analyzeException(String operation,Exception e,List<String> argsList){  String pathArg=!argsList.isEmpty() ? argsList.get(1) : "(none)";  if (LOG.isDebugEnabled()) {
  Map<String,ServiceRecord> results=new HashMap<String,ServiceRecord>(stats.size());  for (  RegistryPathStatus stat : stats) {    if (stat.size > ServiceRecord.RECORD_TYPE.length()) {      String path=join(parentpath,stat.path);      try {        ServiceRecord serviceRecord=operations.resolve(path);        results.put(path,serviceRecord);      } catch (      EOFException ignored) {        if (LOG.isDebugEnabled()) {          LOG.debug("data too short for {}",path);        }      }catch (      InvalidRecordException record) {        if (LOG.isDebugEnabled()) {          LOG.debug("Invalid record at {}",path);        }      }catch (      NoRecordException record) {        if (LOG.isDebugEnabled()) {
@Override protected void serviceInit(Configuration conf) throws Exception {  registryRoot=conf.getTrimmed(KEY_REGISTRY_ZK_ROOT,DEFAULT_ZK_REGISTRY_ROOT);  addService(registrySecurity);  if (LOG.isDebugEnabled()) {
  Configuration conf=getConfig();  createEnsembleProvider();  int sessionTimeout=conf.getInt(KEY_REGISTRY_ZK_SESSION_TIMEOUT,DEFAULT_ZK_SESSION_TIMEOUT);  int connectionTimeout=conf.getInt(KEY_REGISTRY_ZK_CONNECTION_TIMEOUT,DEFAULT_ZK_CONNECTION_TIMEOUT);  int retryTimes=conf.getInt(KEY_REGISTRY_ZK_RETRY_TIMES,DEFAULT_ZK_RETRY_TIMES);  int retryInterval=conf.getInt(KEY_REGISTRY_ZK_RETRY_INTERVAL,DEFAULT_ZK_RETRY_INTERVAL);  int retryCeiling=conf.getInt(KEY_REGISTRY_ZK_RETRY_CEILING,DEFAULT_ZK_RETRY_CEILING);  LOG.info("Creating CuratorService with connection {}",connectionDescription);  CuratorFramework framework;synchronized (CuratorService.class) {    CuratorFrameworkFactory.Builder builder=CuratorFrameworkFactory.builder();    builder.ensembleProvider(ensembleProvider).connectionTimeoutMs(connectionTimeout).sessionTimeoutMs(sessionTimeout).retryPolicy(new BoundedExponentialBackoffRetry(retryInterval,retryCeiling,retryTimes));    registrySecurity.applySecurityEnvironment(builder);    securityConnectionDiagnostics=buildSecurityDiagnostics();    if (LOG.isDebugEnabled()) {
public Stat zkStat(String path) throws IOException {  checkServiceLive();  String fullpath=createFullPath(path);  Stat stat;  try {    if (LOG.isDebugEnabled()) {
public List<ACL> zkGetACLS(String path) throws IOException {  checkServiceLive();  String fullpath=createFullPath(path);  List<ACL> acls;  try {    if (LOG.isDebugEnabled()) {
  checkServiceLive();  path=createFullPath(path);  if (acls == null || acls.isEmpty()) {    throw new NoPathPermissionsException(path,"Empty ACL list");  }  try {    RegistrySecurity.AclListInfo aclInfo=new RegistrySecurity.AclListInfo(acls);    if (LOG.isDebugEnabled()) {      LOG.debug("Creating path {} with mode {} and ACL {}",path,mode,aclInfo);    }    CreateBuilder createBuilder=curator.create();    createBuilder.withMode(mode).withACL(acls);    if (createParents) {      createBuilder.creatingParentsIfNeeded();    }    createBuilder.forPath(path);  } catch (  KeeperException.NodeExistsException e) {    if (LOG.isDebugEnabled()) {
public void zkCreate(String path,CreateMode mode,byte[] data,List<ACL> acls) throws IOException {  Preconditions.checkArgument(data != null,"null data");  checkServiceLive();  String fullpath=createFullPath(path);  try {    if (LOG.isDebugEnabled()) {
public void zkUpdate(String path,byte[] data) throws IOException {  Preconditions.checkArgument(data != null,"null data");  checkServiceLive();  path=createFullPath(path);  try {    if (LOG.isDebugEnabled()) {
public void zkDelete(String path,boolean recursive,BackgroundCallback backgroundCallback) throws IOException {  checkServiceLive();  String fullpath=createFullPath(path);  try {    if (LOG.isDebugEnabled()) {
public List<String> zkList(String path) throws IOException {  checkServiceLive();  String fullpath=createFullPath(path);  try {    if (LOG.isDebugEnabled()) {
public byte[] zkRead(String path) throws IOException {  checkServiceLive();  String fullpath=createFullPath(path);  try {    if (LOG.isDebugEnabled()) {
@Override public void bind(String path,ServiceRecord record,int flags) throws IOException {  Preconditions.checkArgument(record != null,"null record");  validatePath(path);  RegistryTypeUtils.validateServiceRecord(path,record);  if (LOG.isDebugEnabled()) {
@Override public RegistryPathStatus stat(String path) throws IOException {  validatePath(path);  Stat stat=zkStat(path);  String name=RegistryPathUtils.lastPathEntry(path);  RegistryPathStatus status=new RegistryPathStatus(name,stat.getCtime(),stat.getDataLength(),stat.getNumChildren());  if (LOG.isDebugEnabled()) {
private void initSecurity() throws IOException {  secureRegistry=getConfig().getBoolean(KEY_REGISTRY_SECURE,DEFAULT_REGISTRY_SECURE);  systemACLs.clear();  if (secureRegistry) {    addSystemACL(ALL_READ_ACCESS);    kerberosRealm=getConfig().get(KEY_REGISTRY_KERBEROS_REALM,getDefaultRealmInJVM());    String system=getOrFail(KEY_REGISTRY_SYSTEM_ACCOUNTS,DEFAULT_REGISTRY_SYSTEM_ACCOUNTS);    usesRealm=system.contains("@");    systemACLs.addAll(buildACLs(system,kerberosRealm,ZooDefs.Perms.ALL));
    String user=getConfig().get(KEY_REGISTRY_USER_ACCOUNTS,DEFAULT_REGISTRY_USER_ACCOUNTS);    List<ACL> userACLs=buildACLs(user,kerberosRealm,ZooDefs.Perms.ALL);    ACL self;    if (UserGroupInformation.isSecurityEnabled()) {      self=createSaslACLFromCurrentUser(ZooDefs.Perms.ALL);      if (self != null) {        userACLs.add(self);      }    }    LOG.info("Registry User ACLs " + System.lineSeparator() + userACLs);switch (access) {case sasl:      if (!UserGroupInformation.isSecurityEnabled()) {        throw new IOException("Kerberos required for secure registry access");      }    UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();  jaasClientEntry=getOrFail(KEY_REGISTRY_CLIENT_JAAS_CONTEXT,DEFAULT_REGISTRY_CLIENT_JAAS_CONTEXT);jaasClientIdentity=currentUser.getShortUserName();if (LOG.isDebugEnabled()) {
    UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();  jaasClientEntry=getOrFail(KEY_REGISTRY_CLIENT_JAAS_CONTEXT,DEFAULT_REGISTRY_CLIENT_JAAS_CONTEXT);jaasClientIdentity=currentUser.getShortUserName();if (LOG.isDebugEnabled()) {LOG.debug("Auth is SASL user=\"{}\" JAAS context=\"{}\"",jaasClientIdentity,jaasClientEntry);}break;case digest:String id=getOrFail(KEY_REGISTRY_CLIENT_AUTHENTICATION_ID,"");String pass=getOrFail(KEY_REGISTRY_CLIENT_AUTHENTICATION_PASSWORD,"");if (userACLs.isEmpty()) {throw new ServiceStateException(E_NO_USER_DETERMINED_FOR_ACLS);}digest(id,pass);ACL acl=new ACL(ZooDefs.Perms.ALL,toDigestId(id,pass));userACLs.add(acl);digestAuthUser=id;digestAuthPassword=pass;
public boolean addDigestACL(ACL acl){  if (secureRegistry) {    if (LOG.isDebugEnabled()) {
public static void bindJVMtoJAASFile(File jaasFile){  String path=jaasFile.getAbsolutePath();  if (LOG.isDebugEnabled()) {
case anon:      clearZKSaslClientProperties();    break;case digest:  clearZKSaslClientProperties();builder.authorization(SCHEME_DIGEST,digestAuthData);break;case sasl:String existingJaasConf=System.getProperty("java.security.auth.login.config");if (existingJaasConf == null || existingJaasConf.isEmpty()) {if (principal == null || keytab == null) {throw new IOException("SASL is configured for registry, " + "but neither keytab/principal nor java.security.auth.login" + ".config system property are specified");}LOG.info("Enabling ZK sasl client: jaasClientEntry = " + jaasClientEntry + ", principal = "+ principal+ ", keytab = "+ keytab);JaasConfiguration jconf=new JaasConfiguration(jaasClientEntry,principal,keytab);javax.security.auth.login.Configuration.setConfiguration(jconf);setSystemPropertyIfUnset(ZKClientConfig.ENABLE_CLIENT_SASL_KEY,"true");setSystemPropertyIfUnset(ZKClientConfig.LOGIN_CONTEXT_NAME_KEY,jaasClientEntry);} else {
public void logCurrentHadoopUser(){  try {    UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();
public void logCurrentHadoopUser(){  try {    UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();    LOG.info("Current user = {}",currentUser);    UserGroupInformation realUser=currentUser.getRealUser();
public ACL createACLfromUsername(String username,int perms){  if (usesRealm && !username.contains("@")) {    username=username + "@" + kerberosRealm;    if (LOG.isDebugEnabled()) {
@Override public void initTypeToInfoMapping(ServiceRecord serviceRecord) throws Exception {  if (serviceRecord.external.isEmpty()) {
    try {      if (port != 53) {        throw new SocketException("Bypass filtering local DNS server.");      }      Enumeration<NetworkInterface> net=NetworkInterface.getNetworkInterfaces();      while (net.hasMoreElements()) {        NetworkInterface n=(NetworkInterface)net.nextElement();        Enumeration<InetAddress> ee=n.getInetAddresses();        while (ee.hasMoreElements()) {          InetAddress i=(InetAddress)ee.nextElement();          list.add(i);        }      }    } catch (    SocketException e) {    }    ResolverConfig.refresh();    ExtendedResolver resolver;    try {      resolver=new ExtendedResolver();
        }      }    } catch (    SocketException e) {    }    ResolverConfig.refresh();    ExtendedResolver resolver;    try {      resolver=new ExtendedResolver();    } catch (    UnknownHostException e) {      LOG.error("Can not resolve DNS servers: ",e);      return;    }    for (    Resolver check : resolver.getResolvers()) {      if (check instanceof SimpleResolver) {        InetAddress address=((SimpleResolver)check).getAddress().getAddress();        if (list.contains(address)) {          resolver.deleteResolver(check);          continue;        } else {
private void signSiteRecord(Zone zone,Record record) throws DNSSEC.DNSSECException {  RRset rrset=zone.findExactMatch(record.getName(),record.getType());  Calendar cal=Calendar.getInstance();  Date inception=cal.getTime();  cal.add(Calendar.YEAR,1);  Date expiration=cal.getTime();  RRSIGRecord rrsigRecord=DNSSEC.sign(rrset,dnsKeyRecs.get(zone.getOrigin()),privateKey,inception,expiration);
public void nioTCPClient(SocketChannel ch) throws IOException {  try {    ByteBuffer buf=ByteBuffer.allocate(1024);    ch.read(buf);    buf.flip();    int messageLength=getMessgeLength(buf);    byte[] in=new byte[messageLength];    buf.get(in,0,messageLength);    Message query;    byte[] response;    try {      query=new Message(in);
    while (true) {      input.clear();      try {        remoteAddress=channel.receive(input);      } catch (      IOException e) {        LOG.debug("Error during message receipt",e);        continue;      }      Message query;      byte[] response=null;      try {        int position=input.position();        in=new byte[position];        input.flip();        input.get(in);        query=new Message(in);
      Message query;      byte[] response=null;      try {        int position=input.position();        in=new byte[position];        input.flip();        input.get(in);        query=new Message(in);        LOG.info("{}: received UDP query {}",remoteAddress,query.getQuestion());        response=generateReply(query,null);        if (response == null) {          continue;        }      } catch (      IOException e) {        response=formErrorMessage(in);      }      output.clear();
private Message createPrimaryQuery(Message query) throws NameTooLongException, TextParseException {  Name name=query.getQuestion().getName();  if (name.labels() > 0 && name.labels() <= 2) {    int id=query.getHeader().getID();    String queryName=name.getLabelString(0);    Name qualifiedName=Name.concatenate(Name.fromString(queryName),Name.fromString(domainName));
  if (sr != null) {    if (sr.isCNAME()) {      CNAMERecord cname=sr.getCNAME();      RRset rrset=zone.findExactMatch(cname.getName(),Type.CNAME);      addRRset(name,response,rrset,Section.ANSWER,flags);      if (iterations == 0) {        response.getHeader().setFlag(Flags.AA);      }      rcode=addAnswer(response,cname.getTarget(),type,dclass,iterations + 1,flags);    }    if (sr.isNXDOMAIN()) {      response.getHeader().setRcode(Rcode.NXDOMAIN);      if (isDNSSECEnabled()) {        try {          addNXT(response,flags);        } catch (        Exception e) {          LOG.warn("Unable to add NXTRecord to AUTHORITY Section",e);
  try {    parsedRange=Integer.parseInt(range);  } catch (  NumberFormatException e) {    LOG.error("The supplied range is not a valid integer: Supplied range: ",range);    throw e;  }  if (parsedRange < 0) {    String msg=String.format("Range cannot be negative: Supplied range: %d",parsedRange);    LOG.error(msg);    throw new IllegalArgumentException(msg);  }  long ipCount;  try {    SubnetUtils subnetUtils=new SubnetUtils(subnet,mask);    subnetUtils.setInclusiveHostCount(true);    ipCount=subnetUtils.getInfo().getAddressCountLong();  } catch (  IllegalArgumentException e) {
@Override public void processResult(CuratorFramework client,CuratorEvent event) throws Exception {  if (LOG.isDebugEnabled()) {
@Override protected void serviceStart() throws Exception {  setupSecurity();  FileTxnSnapLog ftxn=new FileTxnSnapLog(dataDir,dataDir);  ZooKeeperServer zkServer=new ZooKeeperServer(ftxn,tickTime);  LOG.info("Starting Local Zookeeper service");  factory=ServerCnxnFactory.createFactory();  factory.configure(getAddress(port),-1);  factory.startup(zkServer);  String connectString=getConnectionString();
@Override protected void serviceStart() throws Exception {  setupSecurity();  FileTxnSnapLog ftxn=new FileTxnSnapLog(dataDir,dataDir);  ZooKeeperServer zkServer=new ZooKeeperServer(ftxn,tickTime);  LOG.info("Starting Local Zookeeper service");  factory=ServerCnxnFactory.createFactory();  factory.configure(getAddress(port),-1);  factory.startup(zkServer);  String connectString=getConnectionString();  LOG.info("In memory ZK started at {}\n",connectString);  if (LOG.isDebugEnabled()) {    StringWriter sw=new StringWriter();    PrintWriter pw=new PrintWriter(sw);    zkServer.dumpConf(pw);    pw.flush();
public <V>Future<V> submit(Callable<V> callable){  if (LOG.isDebugEnabled()) {
@Override protected void serviceInit(Configuration conf) throws Exception {  super.serviceInit(conf);  RegistrySecurity registrySecurity=getRegistrySecurity();  if (registrySecurity.isSecureRegistry()) {    ACL sasl=registrySecurity.createSaslACLFromCurrentUser(ZooDefs.Perms.ALL);    registrySecurity.addSystemACL(sasl);
@VisibleForTesting public void createRootRegistryPaths() throws IOException {  List<ACL> systemACLs=getRegistrySecurity().getSystemACLs();
    childEntries=RegistryUtils.statChildren(this,path);    entries=childEntries.values();  } catch (  PathNotFoundException e) {    return 0;  }  try {    RegistryPathStatus registryPathStatus=stat(path);    ServiceRecord serviceRecord=resolve(path);    toDelete=selector.shouldSelect(path,registryPathStatus,serviceRecord);  } catch (  EOFException ignored) {  }catch (  InvalidRecordException ignored) {  }catch (  NoRecordException ignored) {  }catch (  PathNotFoundException e) {    return 0;  }  if (toDelete && !entries.isEmpty()) {    if (LOG.isDebugEnabled()) {
public static void logLoginDetails(String name,LoginContext loginContext){  assertNotNull("Null login context",loginContext);  Subject subject=loginContext.getSubject();
public static void logRecord(String name,ServiceRecord record) throws IOException {
public static void describe(Logger log,String text,Object... args){  log.info("\n=======================================");
public static LoginContext logout(LoginContext login){  try {    if (login != null) {
public static UserGroupInformation loginUGI(String user,File keytab) throws IOException {
@Test public void testRoundTrip() throws Throwable {  String persistence=PersistencePolicies.PERMANENT;  ServiceRecord record=createRecord(persistence);  record.set("customkey","customvalue");  record.set("customkey2","customvalue2");  RegistryTypeUtils.validateServiceRecord("",record);
@Override public void processResult(CuratorFramework client,CuratorEvent event) throws Exception {
@Test public void testBackgroundDelete() throws Throwable {  mkPath("/rm",CreateMode.PERSISTENT);  mkPath("/rm/child",CreateMode.PERSISTENT);  CuratorEventCatcher events=new CuratorEventCatcher();  curatorService.zkDelete("/rm",true,events);  CuratorEvent taken=events.take();
  }  kdcConf=MiniKdc.createConf();  kdcConf.setProperty(MiniKdc.DEBUG,"true");  kdc=new MiniKdc(kdcConf,kdcWorkDir);  kdc.start();  keytab_zk=createKeytab(ZOOKEEPER,"zookeeper.keytab");  keytab_alice=createKeytab(ALICE,"alice.keytab");  keytab_bob=createKeytab(BOB,"bob.keytab");  zkServerPrincipal=Shell.WINDOWS ? ZOOKEEPER_1270001 : ZOOKEEPER_LOCALHOST;  StringBuilder jaas=new StringBuilder(1024);  jaas.append(registrySecurity.createJAASEntry(ZOOKEEPER_CLIENT_CONTEXT,ZOOKEEPER,keytab_zk));  jaas.append(registrySecurity.createJAASEntry(ZOOKEEPER_SERVER_CONTEXT,zkServerPrincipal,keytab_zk));  jaas.append(registrySecurity.createJAASEntry(ALICE_CLIENT_CONTEXT,ALICE_LOCALHOST,keytab_alice));  jaas.append(registrySecurity.createJAASEntry(BOB_CLIENT_CONTEXT,BOB_LOCALHOST,keytab_bob));  jaasFile=new File(kdcWorkDir,"jaas.txt");  FileUtils.write(jaasFile,jaas.toString(),Charset.defaultCharset());
protected LoginContext login(String principal,String context,File keytab) throws LoginException, FileNotFoundException {
@Test public void testDefaultRealm() throws Throwable {  String realm=RegistrySecurity.getDefaultRealmInJVM();
@Test public void testUGIProperties() throws Throwable {  UserGroupInformation user=UserGroupInformation.getCurrentUser();  ACL acl=registrySecurity.createACLForUser(user,ZooDefs.Perms.ALL);  assertFalse(RegistrySecurity.ALL_READWRITE_ACCESS.equals(acl));
@Test public void testClientLogin() throws Throwable {  LoginContext client=login(ALICE_LOCALHOST,ALICE_CLIENT_CONTEXT,keytab_alice);  try {    logLoginDetails(ALICE_LOCALHOST,client);    String confFilename=System.getProperty(Environment.JAAS_CONF_KEY);    assertNotNull("Unset: " + Environment.JAAS_CONF_KEY,confFilename);    String config=FileUtils.readFileToString(new File(confFilename),Charset.defaultCharset());
@Test public void testKerberosAuth() throws Throwable {  File krb5conf=getKdc().getKrb5conf();  String krbConfig=FileUtils.readFileToString(krb5conf,Charset.defaultCharset());
@Test public void testDefaultRealmValid() throws Throwable {  String defaultRealm=KerberosUtil.getDefaultRealm();  assertNotEmpty("No default Kerberos Realm",defaultRealm);
@Test public void testKerberosRulesValid() throws Throwable {  assertTrue("!KerberosName.hasRulesBeenSet()",KerberosName.hasRulesBeenSet());  String rules=KerberosName.getRules();  assertEquals(kerberosRule,rules);
@Test public void testUGILogin() throws Throwable {  UserGroupInformation ugi=loginUGI(ZOOKEEPER,keytab_zk);  RegistrySecurity.UgiInfo ugiInfo=new RegistrySecurity.UgiInfo(ugi);
@Test public void testZookeeperCanWrite() throws Throwable {  System.setProperty("curator-log-events","true");  startSecureZK();  CuratorService curator=null;  LoginContext login=login(ZOOKEEPER_LOCALHOST,ZOOKEEPER_CLIENT_CONTEXT,keytab_zk);  try {    logLoginDetails(ZOOKEEPER,login);    RegistrySecurity.setZKSaslClientProperties(ZOOKEEPER,ZOOKEEPER_CLIENT_CONTEXT);    curator=startCuratorServiceInstance("ZK",true);
protected CuratorService startCuratorServiceInstance(String name,boolean secure){  Configuration clientConf=new Configuration();  clientConf.set(KEY_REGISTRY_ZK_ROOT,"/");  clientConf.setBoolean(KEY_REGISTRY_SECURE,secure);  describe(LOG,"Starting Curator service");  CuratorService curatorService=new CuratorService(name,secureZK);  curatorService.init(clientConf);  curatorService.start();
public void userZookeeperToCreateRoot() throws Throwable {  System.setProperty("curator-log-events","true");  CuratorService curator=null;  LoginContext login=login(ZOOKEEPER_LOCALHOST,ZOOKEEPER_CLIENT_CONTEXT,keytab_zk);  try {    logLoginDetails(ZOOKEEPER,login);    RegistrySecurity.setZKSaslClientProperties(ZOOKEEPER,ZOOKEEPER_CLIENT_CONTEXT);    curator=startCuratorServiceInstance("ZK",true);
public void userZookeeperToCreateRoot() throws Throwable {  System.setProperty("curator-log-events","true");  CuratorService curator=null;  LoginContext login=login(ZOOKEEPER_LOCALHOST,ZOOKEEPER_CLIENT_CONTEXT,keytab_zk);  try {    logLoginDetails(ZOOKEEPER,login);    RegistrySecurity.setZKSaslClientProperties(ZOOKEEPER,ZOOKEEPER_CLIENT_CONTEXT);    curator=startCuratorServiceInstance("ZK",true);    LOG.info(curator.toString());    addToTeardown(curator);    curator.zkMkPath("/",CreateMode.PERSISTENT,false,RegistrySecurity.WorldReadWriteACL);    ZKPathDumper pathDumper=curator.dumpPath(true);
  for (; ; ) {    final long inodeId;    final DFSOutputStream out;synchronized (filesBeingWritten) {      if (filesBeingWritten.isEmpty()) {        return;      }      inodeId=filesBeingWritten.keySet().iterator().next();      out=filesBeingWritten.remove(inodeId);    }    if (out != null) {      try {        if (abort) {          out.abort();        } else {          out.close();        }      } catch (      IOException ie) {
@Deprecated public long renewDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException {
@Deprecated public void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException {
public HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos,FileSystem.Statistics statistics,long startPos) throws IOException {  final FileEncryptionInfo feInfo=dfsos.getFileEncryptionInfo();  if (feInfo != null) {    HdfsKMSUtil.getCryptoProtocolVersion(feInfo);    final CryptoCodec codec=HdfsKMSUtil.getCryptoCodec(conf,feInfo);    KeyVersion decrypted;    try (TraceScope ignored=tracer.newScope("decryptEDEK")){
public HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos,FileSystem.Statistics statistics,long startPos) throws IOException {  final FileEncryptionInfo feInfo=dfsos.getFileEncryptionInfo();  if (feInfo != null) {    HdfsKMSUtil.getCryptoProtocolVersion(feInfo);    final CryptoCodec codec=HdfsKMSUtil.getCryptoCodec(conf,feInfo);    KeyVersion decrypted;    try (TraceScope ignored=tracer.newScope("decryptEDEK")){      LOG.debug("Start decrypting EDEK for file: {}, output stream: 0x{}",dfsos.getSrc(),Integer.toHexString(dfsos.hashCode()));      decrypted=HdfsKMSUtil.decryptEncryptedDataEncryptionKey(feInfo,getKeyProvider());
public DFSOutputStream create(String src,FsPermission permission,EnumSet<CreateFlag> flag,boolean createParent,short replication,long blockSize,Progressable progress,int buffersize,ChecksumOpt checksumOpt,InetSocketAddress[] favoredNodes,String ecPolicyName,String storagePolicy) throws IOException {  checkOpen();  final FsPermission masked=applyUMask(permission);
private DFSOutputStream callAppend(String src,EnumSet<CreateFlag> flag,Progressable progress,String[] favoredNodes) throws IOException {  CreateFlag.validateForAppend(flag);  try {    final LastBlockWithStatus blkWithStatus=callAppend(src,new EnumSetWritable<>(flag,CreateFlag.class));    HdfsFileStatus status=blkWithStatus.getFileStatus();    if (status == null) {
private static synchronized void initThreadsNumForHedgedReads(int num){  if (num <= 0 || HEDGED_READ_THREAD_POOL != null)   return;  HEDGED_READ_THREAD_POOL=new ThreadPoolExecutor(1,num,60,TimeUnit.SECONDS,new SynchronousQueue<Runnable>(),new Daemon.DaemonFactory(){    private final AtomicInteger threadIndex=new AtomicInteger(0);    @Override public Thread newThread(    Runnable r){      Thread t=super.newThread(r);      t.setName("hedgedRead-" + threadIndex.getAndIncrement());      return t;    }  },new ThreadPoolExecutor.CallerRunsPolicy(){    @Override public void rejectedExecution(    Runnable runnable,    ThreadPoolExecutor e){      LOG.info("Execution rejected, Executing in current thread");      HEDGED_READ_METRIC.incHedgedReadOpsInCurThread();      super.rejectedExecution(runnable,e);    }  });  HEDGED_READ_THREAD_POOL.allowCoreThreadTimeOut(true);
public void addNodeToDeadNodeDetector(DFSInputStream dfsInputStream,DatanodeInfo datanodeInfo){  if (!isDeadNodeDetectionEnabled()) {
public void removeNodeFromDeadNodeDetector(DFSInputStream dfsInputStream,DatanodeInfo datanodeInfo){  if (!isDeadNodeDetectionEnabled()) {
public void removeNodeFromDeadNodeDetector(DFSInputStream dfsInputStream,LocatedBlocks locatedBlocks){  if (!isDeadNodeDetectionEnabled() || locatedBlocks == null) {
  try (TraceScope ignored=tracer.newScope("inotifyPoll")){    if (lastReadTxid == -1) {      LOG.debug("poll(): lastReadTxid is -1, reading current txid from NN");      lastReadTxid=namenode.getCurrentEditLogTxid();      return null;    }    if (!it.hasNext()) {      EventBatchList el=namenode.getEditsFromTxid(lastReadTxid + 1);      if (el.getLastTxid() != -1) {        syncTxid=el.getSyncTxid();        it=el.getBatches().iterator();        long formerLastReadTxid=lastReadTxid;        lastReadTxid=el.getLastTxid();        if (el.getFirstTxid() != formerLastReadTxid + 1) {          throw new MissingEventsException(formerLastReadTxid + 1,el.getFirstTxid());        }      } else {
public EventBatch take() throws IOException, InterruptedException, MissingEventsException {  EventBatch next;  try (TraceScope ignored=tracer.newScope("inotifyTake")){    int nextWaitMin=INITIAL_WAIT_MS;    while ((next=poll()) == null) {      int sleepTime=nextWaitMin + rng.nextInt(nextWaitMin);
  long sleeptime=conf.getBlockWriteLocateFollowingInitialDelayMs();  long maxSleepTime=conf.getBlockWriteLocateFollowingMaxDelayMs();  long localstart=Time.monotonicNow();  while (true) {    try {      return dfsClient.namenode.addBlock(src,dfsClient.clientName,prevBlock,excludedNodes,fileId,favoredNodes,allocFlags);    } catch (    RemoteException e) {      IOException ue=e.unwrapRemoteException(FileNotFoundException.class,AccessControlException.class,NSQuotaExceededException.class,DSQuotaExceededException.class,QuotaByStorageTypeExceededException.class,UnresolvedPathException.class);      if (ue != e) {        throw ue;      }      if (NotReplicatedYetException.class.getName().equals(e.getClassName())) {        if (retries == 0) {          throw e;        } else {          --retries;
  final int expectedNum=healthyStreamers.size();  final long socketTimeout=dfsClient.getConf().getSocketTimeout();  long remaingTime=socketTimeout > 0 ? socketTimeout / 2 : Long.MAX_VALUE;  final long waitInterval=1000;synchronized (coordinator) {    while (checkStreamerUpdates(failed,healthyStreamers) < expectedNum && remaingTime > 0) {      try {        long start=Time.monotonicNow();        coordinator.wait(waitInterval);        remaingTime-=Time.monotonicNow() - start;      } catch (      InterruptedException e) {        throw DFSUtilClient.toInterruptedIOException("Interrupted when waiting" + " for results of updating striped streamers",e);      }    }  }synchronized (coordinator) {    for (    StripedDataStreamer streamer : healthyStreamers) {      if (!coordinator.updateStreamerMap.containsKey(streamer)) {
@Override public void hsync(EnumSet<SyncFlag> syncFlags){
public static IOStreamPair connectToDN(DatanodeInfo dn,int timeout,Configuration conf,SaslDataTransferClient saslClient,SocketFactory socketFactory,boolean connectToDnViaHostname,DataEncryptionKeyFactory dekFactory,Token<BlockTokenIdentifier> blockToken) throws IOException {  boolean success=false;  Socket sock=null;  try {    sock=socketFactory.createSocket();    String dnAddr=dn.getXferAddr(connectToDnViaHostname);
static Socket createSocketForPipeline(final DatanodeInfo first,final int length,final DFSClient client) throws IOException {  final DfsClientConf conf=client.getConf();  final String dnAddr=first.getXferAddr(conf.isConnectToDnViaHostname());
void waitForAckedSeqno(long seqno) throws IOException {  try (TraceScope ignored=dfsClient.getTracer().newScope("waitForAckedSeqno")){
  try (TraceScope ignored=dfsClient.getTracer().newScope("waitForAckedSeqno")){    LOG.debug("{} waiting for ack for: {}",this,seqno);    long begin=Time.monotonicNow();    try {synchronized (dataQueue) {        while (!streamerClosed) {          checkClosed();          if (lastAckedSeqno >= seqno) {            break;          }          try {            dataQueue.wait(1000);          } catch (          InterruptedException ie) {            throw new InterruptedIOException("Interrupted while waiting for data to be acknowledged by pipeline");          }        }      }      checkClosed();    } catch (    ClosedChannelException cce) {
            if (span != null) {              span.addTimelineAnnotation("dataQueue.wait");            }            firstWait=false;          }          try {            dataQueue.wait();          } catch (          InterruptedException e) {            Thread.currentThread().interrupt();            break;          }        }      }  finally {        Span span=Tracer.getCurrentSpan();        if ((span != null) && (!firstWait)) {          span.addTimelineAnnotation("end.wait");        }      }      checkClosed();      queuePacket(packet);    } catch (    ClosedChannelException cce) {
void queuePacket(DFSPacket packet){synchronized (dataQueue) {    if (packet == null)     return;    packet.addTraceParent(Tracer.getCurrentSpanId());    dataQueue.addLast(packet);    lastQueuedSeqno=packet.getSeqno();
private static LoadingCache<DatanodeInfo,DatanodeInfo> initExcludedNodes(long excludedNodesCacheExpiry){  return CacheBuilder.newBuilder().expireAfterWrite(excludedNodesCacheExpiry,TimeUnit.MILLISECONDS).removalListener(new RemovalListener<DatanodeInfo,DatanodeInfo>(){    @Override public void onRemoval(    @Nonnull RemovalNotification<DatanodeInfo,DatanodeInfo> notification){
private void scheduleProbe(ProbeType type){
private void scheduleProbe(ProbeType type){  LOG.debug("Schedule probe datanode for probe type: {}.",type);  DatanodeInfo datanodeInfo=null;  if (type == ProbeType.CHECK_DEAD) {    while ((datanodeInfo=deadNodesProbeQueue.poll()) != null) {      if (probeInProg.containsKey(datanodeInfo.getDatanodeUuid())) {
private void probeCallBack(Probe probe,boolean success){
private void probeCallBack(Probe probe,boolean success){  LOG.debug("Probe datanode: {} result: {}, type: {}",probe.getDatanodeInfo(),success,probe.getType());  probeInProg.remove(probe.getDatanodeInfo().getDatanodeUuid());  if (success) {    if (probe.getType() == ProbeType.CHECK_DEAD) {
private void checkDeadNodes(){  Set<DatanodeInfo> datanodeInfos=clearAndGetDetectedDeadNodes();  for (  DatanodeInfo datanodeInfo : datanodeInfos) {
private void checkDeadNodes(){  Set<DatanodeInfo> datanodeInfos=clearAndGetDetectedDeadNodes();  for (  DatanodeInfo datanodeInfo : datanodeInfos) {    LOG.debug("Add dead node to check: {}.",datanodeInfo);    if (!deadNodesProbeQueue.offer(datanodeInfo)) {
public static void cloneDelegationTokenForLogicalUri(UserGroupInformation ugi,URI haUri,Collection<InetSocketAddress> nnAddrs){  Text haService=HAUtilClient.buildTokenServiceForLogicalUri(haUri,HdfsConstants.HDFS_URI_SCHEME);  Token<DelegationTokenIdentifier> haToken=tokenSelector.selectToken(haService,ugi.getTokens());  if (haToken != null) {    for (    InetSocketAddress singleNNAddr : nnAddrs) {      Token<DelegationTokenIdentifier> specificToken=haToken.privateClone(buildTokenService(singleNNAddr));      Text alias=new Text(HAUtilClient.buildTokenServicePrefixForLogicalUri(HdfsConstants.HDFS_URI_SCHEME) + "//" + specificToken.getService());      ugi.addToken(alias,specificToken);      if (LOG.isDebugEnabled()) {
private URI createKeyProviderURI(Configuration conf){  final String providerUriStr=conf.getTrimmed(CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH);  if (providerUriStr == null || providerUriStr.isEmpty()) {
protected static <T>AbstractNNFailoverProxyProvider<T> createFailoverProxyProvider(Configuration conf,URI nameNodeUri,Class<T> xface,boolean checkPort,AtomicBoolean fallbackToSimpleAuth,HAProxyFactory<T> proxyFactory) throws IOException {  Class<FailoverProxyProvider<T>> failoverProxyProviderClass=null;  AbstractNNFailoverProxyProvider<T> providerNN;  try {    failoverProxyProviderClass=getFailoverProxyProviderClass(conf,nameNodeUri);    if (failoverProxyProviderClass == null) {      return null;    }    Constructor<FailoverProxyProvider<T>> ctor=failoverProxyProviderClass.getConstructor(Configuration.class,URI.class,Class.class,HAProxyFactory.class);    FailoverProxyProvider<T> provider=ctor.newInstance(conf,nameNodeUri,xface,proxyFactory);    if (!(provider instanceof AbstractNNFailoverProxyProvider)) {      providerNN=new WrappedFailoverProxyProvider<>(provider);    } else {      providerNN=(AbstractNNFailoverProxyProvider<T>)provider;    }  } catch (  Exception e) {    final String message="Couldn't create proxy provider " + failoverProxyProviderClass;
  }  final Configuration conf=new Configuration();  conf.setBoolean(String.format("fs.%s.impl.disable.cache",scheme),true);  conf.setBoolean(HdfsClientConfigKeys.Retry.POLICY_ENABLED_KEY,false);  conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,0);  DistributedFileSystem fs=null;  try {    fs=(DistributedFileSystem)FileSystem.get(uri,conf);    final boolean safemode=fs.setSafeMode(SafeModeAction.SAFEMODE_GET);    if (LOG.isDebugEnabled()) {      LOG.debug("Is namenode in safemode? " + safemode + "; uri="+ uri);    }    fs.close();    fs=null;    return !safemode;  } catch (  IOException e) {    if (LOG.isDebugEnabled()) {
    ShortCircuitCache cache=clientContext.getShortCircuitCache(block.getBlockId());    try {      MutableBoolean usedPeer=new MutableBoolean(false);      slot=cache.allocShmSlot(datanode,peer,usedPeer,new ExtendedBlockId(block.getBlockId(),block.getBlockPoolId()),clientName);      if (usedPeer.booleanValue()) {        LOG.trace("{}: allocShmSlot used up our previous socket {}.  " + "Allocating a new one...",this,peer.getDomainSocket());        curPeer=nextDomainPeer();        if (curPeer == null)         break;        peer=(DomainPeer)curPeer.peer;      }      ShortCircuitReplicaInfo info=requestFileDescriptors(peer,slot);      clientContext.getPeerCache().put(datanode,peer);      return info;    } catch (    IOException e) {      if (slot != null) {        cache.freeSlot(slot);
sock.getOutputStream().write(0);}replica=new ShortCircuitReplica(key,fis[0],fis[1],cache,Time.monotonicNow(),slot);return new ShortCircuitReplicaInfo(replica);} catch (IOException e) {LOG.warn(this + ": error creating ShortCircuitReplica.",e);return null;} finally {if (replica == null) {IOUtilsClient.cleanupWithLogger(DFSClient.LOG,fis[0],fis[1]);}}case ERROR_UNSUPPORTED:if (!resp.hasShortCircuitAccessVersion()) {LOG.warn("short-circuit read access is disabled for " + "DataNode " + datanode + ".  reason: "+ resp.getMessage());clientContext.getDomainSocketFactory().disableShortCircuitForPath(pathInfo.getPath());} else {LOG.warn("short-circuit read access for the file " + fileName + " is disabled for DataNode "+ datanode+ ".  reason: "+ resp.getMessage());}return null;
  }  LOG.trace("{}: trying to create a remote block reader from the UNIX domain " + "socket at {}",this,pathInfo.getPath());  while (true) {    BlockReaderPeer curPeer=nextDomainPeer();    if (curPeer == null)     break;    if (curPeer.fromCache)     remainingCacheTries--;    DomainPeer peer=(DomainPeer)curPeer.peer;    BlockReader blockReader=null;    try {      blockReader=getRemoteBlockReader(peer);      return blockReader;    } catch (    IOException ioe) {      IOUtilsClient.cleanupWithLogger(LOG,peer);      if (isSecurityException(ioe)) {        LOG.trace("{}: got security exception while constructing a remote " + " block reader from the unix domain socket at {}",this,pathInfo.getPath(),ioe);        throw ioe;
  BlockReader blockReader=null;  while (true) {    BlockReaderPeer curPeer=null;    Peer peer=null;    try {      curPeer=nextTcpPeer();      if (curPeer.fromCache)       remainingCacheTries--;      peer=curPeer.peer;      blockReader=getRemoteBlockReader(peer);      return blockReader;    } catch (    IOException ioe) {      if (isSecurityException(ioe)) {        LOG.trace("{}: got security exception while constructing a remote " + "block reader from {}",this,peer,ioe);        throw ioe;      }      if ((curPeer != null) && curPeer.fromCache) {
static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,UserGroupInformation userGroupInformation,Configuration configuration,String file,ExtendedBlock blk,Token<BlockTokenIdentifier> token,DatanodeInfo node,long startOffset,long length,StorageType storageType) throws IOException {  final ShortCircuitConf scConf=conf.getShortCircuitConf();  LocalDatanodeInfo localDatanodeInfo=getLocalDatanodeInfo(node.getIpcPort());  BlockLocalPathInfo pathinfo=localDatanodeInfo.getBlockLocalPathInfo(blk);  if (pathinfo == null) {    if (userGroupInformation == null) {      userGroupInformation=UserGroupInformation.getCurrentUser();    }    pathinfo=getBlockPathInfo(userGroupInformation,blk,node,configuration,conf.getSocketTimeout(),token,conf.isConnectToDnViaHostname(),storageType);  }  FileInputStream dataIn=null;  FileInputStream checksumIn=null;  BlockReaderLocalLegacy localBlockReader=null;  final boolean skipChecksumCheck=scConf.isSkipShortCircuitChecksums() || storageType.isTransient();  try {    File blkfile=new File(pathinfo.getBlockPath());    dataIn=new FileInputStream(blkfile);
private static BlockLocalPathInfo getBlockPathInfo(UserGroupInformation ugi,ExtendedBlock blk,DatanodeInfo node,Configuration conf,int timeout,Token<BlockTokenIdentifier> token,boolean connectToDnViaHostname,StorageType storageType) throws IOException {  LocalDatanodeInfo localDatanodeInfo=getLocalDatanodeInfo(node.getIpcPort());  BlockLocalPathInfo pathinfo;  ClientDatanodeProtocol proxy=localDatanodeInfo.getDatanodeProxy(ugi,node,conf,timeout,connectToDnViaHostname);  try {    pathinfo=proxy.getBlockLocalPathInfo(blk,token);    if (pathinfo != null && !storageType.isTransient()) {
public synchronized void put(final DFSClient dfsc){  if (dfsc.isClientRunning()) {    if (!isRunning() || isRenewerExpired()) {      final int id=++currentId;      daemon=new Daemon(new Runnable(){        @Override public void run(){          try {            if (LOG.isDebugEnabled()) {
  if (dfsc.isClientRunning()) {    if (!isRunning() || isRenewerExpired()) {      final int id=++currentId;      daemon=new Daemon(new Runnable(){        @Override public void run(){          try {            if (LOG.isDebugEnabled()) {              LOG.debug("Lease renewer daemon for " + clientsString() + " with renew id "+ id+ " started");            }            LeaseRenewer.this.run(id);          } catch (          InterruptedException e) {            LOG.debug("LeaseRenewer is interrupted.",e);          } finally {synchronized (LeaseRenewer.this) {              Factory.INSTANCE.remove(LeaseRenewer.this);            }            if (LOG.isDebugEnabled()) {
private void renew() throws IOException {  final List<DFSClient> copies;synchronized (this) {    copies=new ArrayList<>(dfsclients);  }  Collections.sort(copies,new Comparator<DFSClient>(){    @Override public int compare(    final DFSClient left,    final DFSClient right){      return left.getClientName().compareTo(right.getClientName());    }  });  String previousName="";  for (  final DFSClient c : copies) {    if (!c.getClientName().equals(previousName)) {      if (!c.renewLease()) {        LOG.debug("Did not renew lease for client {}",c);        continue;      }      previousName=c.getClientName();
private void run(final int id) throws InterruptedException {  for (long lastRenewed=Time.monotonicNow(); !Thread.interrupted(); Thread.sleep(getSleepPeriod())) {    final long elapsed=Time.monotonicNow() - lastRenewed;    if (elapsed >= getRenewalTime()) {      try {        renew();        if (LOG.isDebugEnabled()) {
        LOG.warn("Failed to renew lease for " + clientsString() + " for "+ (elapsed / 1000)+ " seconds.  Aborting ...",ie);        List<DFSClient> dfsclientsCopy;synchronized (this) {          DFSClientFaultInjector.get().delayWhenRenewLeaseTimeout();          dfsclientsCopy=new ArrayList<>(dfsclients);          dfsclients.clear();          emptyTime=0;          Factory.INSTANCE.remove(LeaseRenewer.this);        }        for (        DFSClient dfsClient : dfsclientsCopy) {          dfsClient.closeAllFilesBeingWritten(true);        }        break;      }catch (      IOException ie) {        LOG.warn("Failed to renew lease for " + clientsString() + " for "+ (elapsed / 1000)+ " seconds.  Will retry shortly ...",ie);      }    }synchronized (this) {      if (id != currentId || isRenewerExpired()) {
synchronized (this) {          DFSClientFaultInjector.get().delayWhenRenewLeaseTimeout();          dfsclientsCopy=new ArrayList<>(dfsclients);          dfsclients.clear();          emptyTime=0;          Factory.INSTANCE.remove(LeaseRenewer.this);        }        for (        DFSClient dfsClient : dfsclientsCopy) {          dfsClient.closeAllFilesBeingWritten(true);        }        break;      }catch (      IOException ie) {        LOG.warn("Failed to renew lease for " + clientsString() + " for "+ (elapsed / 1000)+ " seconds.  Will retry shortly ...",ie);      }    }synchronized (this) {      if (id != currentId || isRenewerExpired()) {        if (LOG.isDebugEnabled()) {          if (id != currentId) {
public void markZoneForRetry(final Long zoneId){  final ZoneReencryptionStatus zs=zoneStatuses.get(zoneId);  Preconditions.checkNotNull(zs,"Cannot find zone " + zoneId);
public void markZoneStarted(final Long zoneId){  final ZoneReencryptionStatus zs=zoneStatuses.get(zoneId);  Preconditions.checkNotNull(zs,"Cannot find zone " + zoneId);
public void markZoneCompleted(final Long zoneId){  final ZoneReencryptionStatus zs=zoneStatuses.get(zoneId);  Preconditions.checkNotNull(zs,"Cannot find zone " + zoneId);
private boolean addZoneIfNecessary(final Long zoneId,final String name,final ReencryptionInfoProto reProto){  if (!zoneStatuses.containsKey(zoneId)) {
public boolean removeZone(final Long zoneId){
public static SaslPropertiesResolver getSaslPropertiesResolver(Configuration conf){  String qops=conf.get(DFS_DATA_TRANSFER_PROTECTION_KEY);  if (qops == null || qops.isEmpty()) {
private IOStreamPair checkTrustAndSend(InetAddress addr,OutputStream underlyingOut,InputStream underlyingIn,DataEncryptionKeyFactory encryptionKeyFactory,Token<BlockTokenIdentifier> accessToken,DatanodeID datanodeId,SecretKey secretKey) throws IOException {  boolean localTrusted=trustedChannelResolver.isTrusted();  boolean remoteTrusted=trustedChannelResolver.isTrusted(addr);
private IOStreamPair send(InetAddress addr,OutputStream underlyingOut,InputStream underlyingIn,DataEncryptionKey encryptionKey,Token<BlockTokenIdentifier> accessToken,DatanodeID datanodeId,SecretKey secretKey) throws IOException {  if (encryptionKey != null) {
    LOG.debug("SASL client doing encrypted handshake for addr = {}, " + "datanodeId = {}",addr,datanodeId);    return getEncryptedStreams(addr,underlyingOut,underlyingIn,encryptionKey,accessToken,secretKey);  } else   if (!UserGroupInformation.isSecurityEnabled()) {    LOG.debug("SASL client skipping handshake in unsecured configuration for " + "addr = {}, datanodeId = {}",addr,datanodeId);    return null;  } else   if (SecurityUtil.isPrivilegedPort(datanodeId.getXferPort())) {    LOG.debug("SASL client skipping handshake in secured configuration with " + "privileged port for addr = {}, datanodeId = {}",addr,datanodeId);    return null;  } else   if (fallbackToSimpleAuth != null && fallbackToSimpleAuth.get()) {    LOG.debug("SASL client skipping handshake in secured configuration with " + "unsecured cluster for addr = {}, datanodeId = {}",addr,datanodeId);    return null;  } else   if (saslPropsResolver != null) {    LOG.debug("SASL client doing general handshake for addr = {}, datanodeId = {}",addr,datanodeId);    return getSaslStreams(addr,underlyingOut,underlyingIn,accessToken,secretKey);  } else {
static ClientDatanodeProtocolPB createClientDatanodeProtocolProxy(DatanodeID datanodeid,Configuration conf,int socketTimeout,boolean connectToDnViaHostname,LocatedBlock locatedBlock) throws IOException {  final String dnAddr=datanodeid.getIpcAddr(connectToDnViaHostname);  InetSocketAddress addr=NetUtils.createSocketAddr(dnAddr);
private static URI getFailoverVirtualIP(Configuration conf,String nameServiceID){  String configKey=IPFAILOVER_CONFIG_PREFIX + "." + nameServiceID;  String virtualIP=conf.get(configKey);
private void logProxyException(Exception ex,String proxyInfo){  if (isStandbyException(ex)) {
  final String feature;  if (conf.isShortCircuitLocalReads() && (!conf.isUseLegacyBlockReaderLocal())) {    feature="The short-circuit local reads feature";  } else   if (conf.isDomainSocketDataTraffic()) {    feature="UNIX domain socket data traffic";  } else {    feature=null;  }  if (feature == null) {    PerformanceAdvisory.LOG.debug("Both short-circuit local reads and UNIX domain socket are disabled.");  } else {    if (conf.getDomainSocketPath().isEmpty()) {      throw new HadoopIllegalArgumentException(feature + " is enabled but " + HdfsClientConfigKeys.DFS_DOMAIN_SOCKET_PATH_KEY+ " is not set.");    } else     if (DomainSocket.getLoadingFailureReason() != null) {      LOG.warn(feature + " cannot be used because " + DomainSocket.getLoadingFailureReason());    } else {
public ShortCircuitReplicaInfo fetchOrCreate(ExtendedBlockId key,ShortCircuitReplicaCreator creator){  Waitable<ShortCircuitReplicaInfo> newWaitable;  lock.lock();  try {    ShortCircuitReplicaInfo info=null;    for (int i=0; i < FETCH_OR_CREATE_RETRY_TIMES; i++) {      if (closed) {        LOG.trace("{}: can't fethchOrCreate {} because the cache is closed.",this,key);        return null;      }      Waitable<ShortCircuitReplicaInfo> waitable=replicaInfoMap.get(key);      if (waitable != null) {        try {          info=fetch(key,waitable);          break;        } catch (        RetriableException e) {
    info=waitable.await();  } catch (  InterruptedException e) {    LOG.info(this + ": interrupted while waiting for " + key);    Thread.currentThread().interrupt();    throw new RetriableException("interrupted");  }  if (info.getInvalidTokenException() != null) {    LOG.info(this + ": could not get " + key+ " due to InvalidToken "+ "exception.",info.getInvalidTokenException());    return info;  }  ShortCircuitReplica replica=info.getReplica();  if (replica == null) {    LOG.warn(this + ": failed to get " + key);    return info;  }  if (replica.purged) {    throw new RetriableException("Ignoring purged replica " + replica + ".  Retrying.");  }  if (replica.isStale()) {
  try {    LOG.trace("{}: loading {}",this,key);    info=creator.createShortCircuitReplicaInfo();  } catch (  RuntimeException e) {    LOG.warn(this + ": failed to load " + key,e);  }  if (info == null)   info=new ShortCircuitReplicaInfo();  lock.lock();  try {    if (info.getReplica() != null) {      LOG.trace("{}: successfully loaded {}",this,info.getReplica());      startCacheCleanerThreadIfNeeded();    } else {      Waitable<ShortCircuitReplicaInfo> waitableInMap=replicaInfoMap.get(key);      if (waitableInMap == newWaitable)       replicaInfoMap.remove(key);      if (info.getInvalidTokenException() != null) {
private void startCacheCleanerThreadIfNeeded(){  if (cacheCleaner == null) {    cacheCleaner=new CacheCleaner();    long rateMs=cacheCleaner.getRateInMs();    ScheduledFuture<?> future=cleanerExecutor.scheduleAtFixedRate(cacheCleaner,rateMs,rateMs,TimeUnit.MILLISECONDS);    cacheCleaner.setFuture(future);
      purge((ShortCircuitReplica)evictable.get(eldestKey));    }    while (!evictableMmapped.isEmpty()) {      Object eldestKey=evictableMmapped.firstKey();      purge((ShortCircuitReplica)evictableMmapped.get(eldestKey));    }  }  finally {    lock.unlock();  }  releaserExecutor.shutdown();  cleanerExecutor.shutdown();  try {    if (!releaserExecutor.awaitTermination(30,TimeUnit.SECONDS)) {      LOG.error("Forcing SlotReleaserThreadPool to shutdown!");      releaserExecutor.shutdownNow();    }  } catch (  InterruptedException e) {    releaserExecutor.shutdownNow();    Thread.currentThread().interrupt();
    lock.unlock();  }  releaserExecutor.shutdown();  cleanerExecutor.shutdown();  try {    if (!releaserExecutor.awaitTermination(30,TimeUnit.SECONDS)) {      LOG.error("Forcing SlotReleaserThreadPool to shutdown!");      releaserExecutor.shutdownNow();    }  } catch (  InterruptedException e) {    releaserExecutor.shutdownNow();    Thread.currentThread().interrupt();    LOG.error("Interrupted while waiting for SlotReleaserThreadPool " + "to terminate",e);  }  try {    if (!cleanerExecutor.awaitTermination(30,TimeUnit.SECONDS)) {      LOG.error("Forcing CleanerThreadPool to shutdown!");      cleanerExecutor.shutdownNow();
private static void logDebugMessage(){  final StringBuilder b=DEBUG_MESSAGE.get();
private List<ErasureCodingPolicy> loadECPolicies(File policyFile) throws ParserConfigurationException, IOException, SAXException {
  Future<BlockReadStats> future=null;  try {    if (timeoutMillis > 0) {      future=readService.poll(timeoutMillis,TimeUnit.MILLISECONDS);    } else {      future=readService.take();    }    if (future != null) {      final BlockReadStats stats=future.get();      return new StripingChunkReadResult(futures.remove(future),StripingChunkReadResult.SUCCESSFUL,stats);    } else {      return new StripingChunkReadResult(StripingChunkReadResult.TIMEOUT);    }  } catch (  ExecutionException e) {    LOG.debug("Exception during striped read task",e);    return new StripingChunkReadResult(futures.remove(future),StripingChunkReadResult.FAILED);  }catch (  CancellationException e) {
synchronized void ensureTokenInitialized() throws IOException {  if (!hasInitedToken || (action != null && !action.isValid())) {    Token<?> token=fs.getDelegationToken(null);    if (token != null) {      fs.setDelegationToken(token);      addRenewAction(fs);
synchronized void initDelegationToken(UserGroupInformation ugi){  Token<?> token=selectDelegationToken(ugi);  if (token != null) {
public URLConnection openConnection(URL url,boolean isSpnego) throws IOException, AuthenticationException {  if (isSpnego) {
protected synchronized Token<?> getDelegationToken() throws IOException {  if (delegationToken == null) {    Token<?> token=tokenSelector.selectToken(new Text(getCanonicalServiceName()),ugi.getTokens());    if (token != null) {
@VisibleForTesting synchronized boolean replaceExpiredDelegationToken() throws IOException {  boolean replaced=false;  if (canRefreshDelegationToken) {    Token<?> token=getDelegationToken(null);
  final int numThreads=10;  final ExecutorService threadPool=newFixedThreadPool(numThreads);  try {    final CountDownLatch allReady=new CountDownLatch(numThreads);    final CountDownLatch startBlocker=new CountDownLatch(1);    final CountDownLatch allDone=new CountDownLatch(numThreads);    final AtomicReference<Throwable> childError=new AtomicReference<>();    for (int i=0; i < numThreads; i++) {      threadPool.submit(new Runnable(){        @Override public void run(){          allReady.countDown();          try {            startBlocker.await();            incrementOpsCountByRandomNumbers();          } catch (          Throwable t) {
  for (int i=0; i < runners.length; i++) {    runners[i]=new Runner(i,countThreshold,countLimit,pool,i,bam);    threads[i]=runners[i].start(num);  }  final List<Exception> exceptions=new ArrayList<Exception>();  final Thread randomRecycler=new Thread(){    @Override public void run(){      LOG.info("randomRecycler start");      for (int i=0; shouldRun(); i++) {        final int j=ThreadLocalRandom.current().nextInt(runners.length);        try {          runners[j].recycle();        } catch (        Exception e) {          e.printStackTrace();          exceptions.add(new Exception(this + " has an exception",e));        }        if ((i & 0xFF) == 0) {
          LOG.info("randomRecycler sleep, i=" + i);          sleepMs(100);        }      }      LOG.info("randomRecycler done");    }    boolean shouldRun(){      for (int i=0; i < runners.length; i++) {        if (threads[i].isAlive()) {          return true;        }        if (!runners[i].isEmpty()) {          return true;        }      }      return false;    }  };  randomRecycler.start();  randomRecycler.join();  Assert.assertTrue(exceptions.isEmpty());  Assert.assertNull(counters.get(0,false));
      if (noRedirect) {        URI redirectURL=createOpenRedirectionURL(uriInfo);        final String js=JsonUtil.toJsonString("Location",redirectURL);        response=Response.ok(js).type(MediaType.APPLICATION_JSON).build();      } else {        final FSOperations.FSOpen command=new FSOperations.FSOpen(path);        final FileSystem fs=createFileSystem(user);        InputStream is=null;        UserGroupInformation ugi=UserGroupInformation.createProxyUser(user.getShortUserName(),UserGroupInformation.getLoginUser());        try {          is=ugi.doAs(new PrivilegedExceptionAction<InputStream>(){            @Override public InputStream run() throws Exception {              return command.execute(fs);            }          });        } catch (        InterruptedException ie) {
          is=ugi.doAs(new PrivilegedExceptionAction<InputStream>(){            @Override public InputStream run() throws Exception {              return command.execute(fs);            }          });        } catch (        InterruptedException ie) {          LOG.info("Open interrupted.",ie);          Thread.currentThread().interrupt();        }        Long offset=params.get(OffsetParam.NAME,OffsetParam.class);        Long len=params.get(LenParam.NAME,LenParam.class);        AUDIT_LOG.info("[{}] offset [{}] len [{}]",new Object[]{path,offset,len});        InputStreamEntity entity=new InputStreamEntity(is,offset,len);        response=Response.ok(entity).type(MediaType.APPLICATION_OCTET_STREAM).build();      }      break;    }case GETFILESTATUS:{    FSOperations.FSFileStatus command=new FSOperations.FSFileStatus(path);
 catch (        InterruptedException ie) {          LOG.info("Open interrupted.",ie);          Thread.currentThread().interrupt();        }        Long offset=params.get(OffsetParam.NAME,OffsetParam.class);        Long len=params.get(LenParam.NAME,LenParam.class);        AUDIT_LOG.info("[{}] offset [{}] len [{}]",new Object[]{path,offset,len});        InputStreamEntity entity=new InputStreamEntity(is,offset,len);        response=Response.ok(entity).type(MediaType.APPLICATION_OCTET_STREAM).build();      }      break;    }case GETFILESTATUS:{    FSOperations.FSFileStatus command=new FSOperations.FSFileStatus(path);    Map json=fsExecute(user,command);    AUDIT_LOG.info("[{}]",path);    response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();    break;
  AUDIT_LOG.info("[{}] filter [{}]",path,(filter != null) ? filter : "-");  response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();  break;}case GETHOMEDIRECTORY:{enforceRootPath(op.value(),path);FSOperations.FSHomeDir command=new FSOperations.FSHomeDir();JSONObject json=fsExecute(user,command);AUDIT_LOG.info("");response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case INSTRUMENTATION:{enforceRootPath(op.value(),path);Groups groups=HttpFSServerWebApp.get().get(Groups.class);Set<String> userGroups=groups.getGroupsSet(user.getShortUserName());if (!userGroups.contains(HttpFSServerWebApp.get().getAdminGroup())) {
FSOperations.FSHomeDir command=new FSOperations.FSHomeDir();JSONObject json=fsExecute(user,command);AUDIT_LOG.info("");response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case INSTRUMENTATION:{enforceRootPath(op.value(),path);Groups groups=HttpFSServerWebApp.get().get(Groups.class);Set<String> userGroups=groups.getGroupsSet(user.getShortUserName());if (!userGroups.contains(HttpFSServerWebApp.get().getAdminGroup())) {throw new AccessControlException("User not in HttpFSServer admin group");}Instrumentation instrumentation=HttpFSServerWebApp.get().get(Instrumentation.class);Map snapshot=instrumentation.getSnapshot();response=Response.ok(snapshot).build();break;
case INSTRUMENTATION:{enforceRootPath(op.value(),path);Groups groups=HttpFSServerWebApp.get().get(Groups.class);Set<String> userGroups=groups.getGroupsSet(user.getShortUserName());if (!userGroups.contains(HttpFSServerWebApp.get().getAdminGroup())) {throw new AccessControlException("User not in HttpFSServer admin group");}Instrumentation instrumentation=HttpFSServerWebApp.get().get(Instrumentation.class);Map snapshot=instrumentation.getSnapshot();response=Response.ok(snapshot).build();break;}case GETCONTENTSUMMARY:{FSOperations.FSContentSummary command=new FSOperations.FSContentSummary(path);Map json=fsExecute(user,command);AUDIT_LOG.info("[{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();
AUDIT_LOG.info("[{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETQUOTAUSAGE:{FSOperations.FSQuotaUsage command=new FSOperations.FSQuotaUsage(path);Map json=fsExecute(user,command);AUDIT_LOG.info("[{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETFILECHECKSUM:{FSOperations.FSFileChecksum command=new FSOperations.FSFileChecksum(path);Boolean noRedirect=params.get(NoRedirectParam.NAME,NoRedirectParam.class);AUDIT_LOG.info("[{}]",path);if (noRedirect) {URI redirectURL=createOpenRedirectionURL(uriInfo);
response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETFILECHECKSUM:{FSOperations.FSFileChecksum command=new FSOperations.FSFileChecksum(path);Boolean noRedirect=params.get(NoRedirectParam.NAME,NoRedirectParam.class);AUDIT_LOG.info("[{}]",path);if (noRedirect) {URI redirectURL=createOpenRedirectionURL(uriInfo);final String js=JsonUtil.toJsonString("Location",redirectURL);response=Response.ok(js).type(MediaType.APPLICATION_JSON).build();} else {Map json=fsExecute(user,command);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();}break;}case GETFILEBLOCKLOCATIONS:{
AUDIT_LOG.info("[{}]",path);if (noRedirect) {URI redirectURL=createOpenRedirectionURL(uriInfo);final String js=JsonUtil.toJsonString("Location",redirectURL);response=Response.ok(js).type(MediaType.APPLICATION_JSON).build();} else {Map json=fsExecute(user,command);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();}break;}case GETFILEBLOCKLOCATIONS:{response=Response.status(Response.Status.BAD_REQUEST).build();break;}case GETACLSTATUS:{FSOperations.FSAclStatus command=new FSOperations.FSAclStatus(path);Map json=fsExecute(user,command);
break;}case GETFILEBLOCKLOCATIONS:{response=Response.status(Response.Status.BAD_REQUEST).build();break;}case GETACLSTATUS:{FSOperations.FSAclStatus command=new FSOperations.FSAclStatus(path);Map json=fsExecute(user,command);AUDIT_LOG.info("ACL status for [{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETXATTRS:{List<String> xattrNames=params.getValues(XAttrNameParam.NAME,XAttrNameParam.class);XAttrCodec encoding=params.get(XAttrEncodingParam.NAME,XAttrEncodingParam.class);FSOperations.FSGetXAttrs command=new FSOperations.FSGetXAttrs(path,xattrNames,encoding);@SuppressWarnings("rawtypes") Map json=fsExecute(user,command);
case GETACLSTATUS:{FSOperations.FSAclStatus command=new FSOperations.FSAclStatus(path);Map json=fsExecute(user,command);AUDIT_LOG.info("ACL status for [{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETXATTRS:{List<String> xattrNames=params.getValues(XAttrNameParam.NAME,XAttrNameParam.class);XAttrCodec encoding=params.get(XAttrEncodingParam.NAME,XAttrEncodingParam.class);FSOperations.FSGetXAttrs command=new FSOperations.FSGetXAttrs(path,xattrNames,encoding);@SuppressWarnings("rawtypes") Map json=fsExecute(user,command);AUDIT_LOG.info("XAttrs for [{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case LISTXATTRS:{
}case GETXATTRS:{List<String> xattrNames=params.getValues(XAttrNameParam.NAME,XAttrNameParam.class);XAttrCodec encoding=params.get(XAttrEncodingParam.NAME,XAttrEncodingParam.class);FSOperations.FSGetXAttrs command=new FSOperations.FSGetXAttrs(path,xattrNames,encoding);@SuppressWarnings("rawtypes") Map json=fsExecute(user,command);AUDIT_LOG.info("XAttrs for [{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case LISTXATTRS:{FSOperations.FSListXAttrs command=new FSOperations.FSListXAttrs(path);@SuppressWarnings("rawtypes") Map json=fsExecute(user,command);AUDIT_LOG.info("XAttr names for [{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case LISTSTATUS_BATCH:{
AUDIT_LOG.info("XAttrs for [{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case LISTXATTRS:{FSOperations.FSListXAttrs command=new FSOperations.FSListXAttrs(path);@SuppressWarnings("rawtypes") Map json=fsExecute(user,command);AUDIT_LOG.info("XAttr names for [{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case LISTSTATUS_BATCH:{String startAfter=params.get(HttpFSParametersProvider.StartAfterParam.NAME,HttpFSParametersProvider.StartAfterParam.class);byte[] token=HttpFSUtils.EMPTY_BYTES;if (startAfter != null) {token=startAfter.getBytes(Charsets.UTF_8);}FSOperations.FSListStatusBatch command=new FSOperations.FSListStatusBatch(path,token);
response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case LISTSTATUS_BATCH:{String startAfter=params.get(HttpFSParametersProvider.StartAfterParam.NAME,HttpFSParametersProvider.StartAfterParam.class);byte[] token=HttpFSUtils.EMPTY_BYTES;if (startAfter != null) {token=startAfter.getBytes(Charsets.UTF_8);}FSOperations.FSListStatusBatch command=new FSOperations.FSListStatusBatch(path,token);@SuppressWarnings("rawtypes") Map json=fsExecute(user,command);AUDIT_LOG.info("[{}] token [{}]",path,token);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETTRASHROOT:{FSOperations.FSTrashRoot command=new FSOperations.FSTrashRoot(path);JSONObject json=fsExecute(user,command);
response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETSTORAGEPOLICY:{FSOperations.FSGetStoragePolicy command=new FSOperations.FSGetStoragePolicy(path);JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}]",path);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case GETSNAPSHOTDIFF:{String oldSnapshotName=params.get(OldSnapshotNameParam.NAME,OldSnapshotNameParam.class);String snapshotName=params.get(SnapshotNameParam.NAME,SnapshotNameParam.class);FSOperations.FSGetSnapshotDiff command=new FSOperations.FSGetSnapshotDiff(path,oldSnapshotName,snapshotName);String js=fsExecute(user,command);AUDIT_LOG.info("[{}]",path);response=Response.ok(js).type(MediaType.APPLICATION_JSON).build();
@DELETE @Path("{path:.*}") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Response delete(@PathParam("path") String path,@QueryParam(OperationParam.NAME) OperationParam op,@Context Parameters params,@Context HttpServletRequest request) throws IOException, FileSystemAccessException {  UserGroupInformation user=HttpUserGroupInformation.get();  Response response;  path=makeAbsolute(path);  MDC.put(HttpFSFileSystem.OP_PARAM,op.value().name());  MDC.put("hostname",request.getRemoteAddr());switch (op.value()) {case DELETE:{      Boolean recursive=params.get(RecursiveParam.NAME,RecursiveParam.class);
  path=makeAbsolute(path);  MDC.put(HttpFSFileSystem.OP_PARAM,op.value().name());  MDC.put("hostname",request.getRemoteAddr());switch (op.value()) {case DELETE:{      Boolean recursive=params.get(RecursiveParam.NAME,RecursiveParam.class);      AUDIT_LOG.info("[{}] recursive [{}]",path,recursive);      FSOperations.FSDelete command=new FSOperations.FSDelete(path,recursive);      JSONObject json=fsExecute(user,command);      response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();      break;    }case DELETESNAPSHOT:{    String snapshotName=params.get(SnapshotNameParam.NAME,SnapshotNameParam.class);    FSOperations.FSDeleteSnapshot command=new FSOperations.FSDeleteSnapshot(path,snapshotName);    fsExecute(user,command);
  Response response;  path=makeAbsolute(path);  MDC.put(HttpFSFileSystem.OP_PARAM,op.value().name());  MDC.put("hostname",request.getRemoteAddr());switch (op.value()) {case APPEND:{      Boolean hasData=params.get(DataParam.NAME,DataParam.class);      URI redirectURL=createUploadRedirectionURL(uriInfo,HttpFSFileSystem.Operation.APPEND);      Boolean noRedirect=params.get(NoRedirectParam.NAME,NoRedirectParam.class);      if (noRedirect) {        final String js=JsonUtil.toJsonString("Location",redirectURL);        response=Response.ok(js).type(MediaType.APPLICATION_JSON).build();      } else       if (hasData) {        FSOperations.FSAppend command=new FSOperations.FSAppend(is,path);        fsExecute(user,command);
        final String js=JsonUtil.toJsonString("Location",redirectURL);        response=Response.ok(js).type(MediaType.APPLICATION_JSON).build();      } else       if (hasData) {        FSOperations.FSAppend command=new FSOperations.FSAppend(is,path);        fsExecute(user,command);        AUDIT_LOG.info("[{}]",path);        response=Response.ok().type(MediaType.APPLICATION_JSON).build();      } else {        response=Response.temporaryRedirect(redirectURL).build();      }      break;    }case CONCAT:{    System.out.println("HTTPFS SERVER CONCAT");    String sources=params.get(SourcesParam.NAME,SourcesParam.class);    FSOperations.FSConcat command=new FSOperations.FSConcat(path,sources.split(","));    fsExecute(user,command);
      } else {        response=Response.temporaryRedirect(redirectURL).build();      }      break;    }case CONCAT:{    System.out.println("HTTPFS SERVER CONCAT");    String sources=params.get(SourcesParam.NAME,SourcesParam.class);    FSOperations.FSConcat command=new FSOperations.FSConcat(path,sources.split(","));    fsExecute(user,command);    AUDIT_LOG.info("[{}]",path);    System.out.println("SENT RESPONSE");    response=Response.ok().build();    break;  }case TRUNCATE:{  Long newLength=params.get(NewLengthParam.NAME,NewLengthParam.class);  FSOperations.FSTruncate command=new FSOperations.FSTruncate(path,newLength);
case CONCAT:{    System.out.println("HTTPFS SERVER CONCAT");    String sources=params.get(SourcesParam.NAME,SourcesParam.class);    FSOperations.FSConcat command=new FSOperations.FSConcat(path,sources.split(","));    fsExecute(user,command);    AUDIT_LOG.info("[{}]",path);    System.out.println("SENT RESPONSE");    response=Response.ok().build();    break;  }case TRUNCATE:{  Long newLength=params.get(NewLengthParam.NAME,NewLengthParam.class);  FSOperations.FSTruncate command=new FSOperations.FSTruncate(path,newLength);  JSONObject json=fsExecute(user,command);  AUDIT_LOG.info("Truncate [{}] to length [{}]",path,newLength);  response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();
    System.out.println("SENT RESPONSE");    response=Response.ok().build();    break;  }case TRUNCATE:{  Long newLength=params.get(NewLengthParam.NAME,NewLengthParam.class);  FSOperations.FSTruncate command=new FSOperations.FSTruncate(path,newLength);  JSONObject json=fsExecute(user,command);  AUDIT_LOG.info("Truncate [{}] to length [{}]",path,newLength);  response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();  break;}case UNSETSTORAGEPOLICY:{FSOperations.FSUnsetStoragePolicy command=new FSOperations.FSUnsetStoragePolicy(path);fsExecute(user,command);AUDIT_LOG.info("Unset storage policy [{}]",path);response=Response.ok().build();
        Short permission=params.get(PermissionParam.NAME,PermissionParam.class);        Short unmaskedPermission=params.get(UnmaskedPermissionParam.NAME,UnmaskedPermissionParam.class);        Boolean override=params.get(OverwriteParam.NAME,OverwriteParam.class);        Short replication=params.get(ReplicationParam.NAME,ReplicationParam.class);        Long blockSize=params.get(BlockSizeParam.NAME,BlockSizeParam.class);        FSOperations.FSCreate command=new FSOperations.FSCreate(is,path,permission,override,replication,blockSize,unmaskedPermission);        fsExecute(user,command);        AUDIT_LOG.info("[{}] permission [{}] override [{}] " + "replication [{}] blockSize [{}] unmaskedpermission [{}]",new Object[]{path,permission,override,replication,blockSize,unmaskedPermission});        final String js=JsonUtil.toJsonString("Location",uriInfo.getAbsolutePath());        response=Response.created(uriInfo.getAbsolutePath()).type(MediaType.APPLICATION_JSON).entity(js).build();      } else {        response=Response.temporaryRedirect(redirectURL).build();      }      break;    }case ALLOWSNAPSHOT:{    FSOperations.FSAllowSnapshot command=new FSOperations.FSAllowSnapshot(path);
        fsExecute(user,command);        AUDIT_LOG.info("[{}] permission [{}] override [{}] " + "replication [{}] blockSize [{}] unmaskedpermission [{}]",new Object[]{path,permission,override,replication,blockSize,unmaskedPermission});        final String js=JsonUtil.toJsonString("Location",uriInfo.getAbsolutePath());        response=Response.created(uriInfo.getAbsolutePath()).type(MediaType.APPLICATION_JSON).entity(js).build();      } else {        response=Response.temporaryRedirect(redirectURL).build();      }      break;    }case ALLOWSNAPSHOT:{    FSOperations.FSAllowSnapshot command=new FSOperations.FSAllowSnapshot(path);    fsExecute(user,command);    AUDIT_LOG.info("[{}] allowed snapshot",path);    response=Response.ok().build();    break;  }case DISALLOWSNAPSHOT:{  FSOperations.FSDisallowSnapshot command=new FSOperations.FSDisallowSnapshot(path);
      }      break;    }case ALLOWSNAPSHOT:{    FSOperations.FSAllowSnapshot command=new FSOperations.FSAllowSnapshot(path);    fsExecute(user,command);    AUDIT_LOG.info("[{}] allowed snapshot",path);    response=Response.ok().build();    break;  }case DISALLOWSNAPSHOT:{  FSOperations.FSDisallowSnapshot command=new FSOperations.FSDisallowSnapshot(path);  fsExecute(user,command);  AUDIT_LOG.info("[{}] disallowed snapshot",path);  response=Response.ok().build();  break;}case CREATESNAPSHOT:{String snapshotName=params.get(SnapshotNameParam.NAME,SnapshotNameParam.class);
  }case DISALLOWSNAPSHOT:{  FSOperations.FSDisallowSnapshot command=new FSOperations.FSDisallowSnapshot(path);  fsExecute(user,command);  AUDIT_LOG.info("[{}] disallowed snapshot",path);  response=Response.ok().build();  break;}case CREATESNAPSHOT:{String snapshotName=params.get(SnapshotNameParam.NAME,SnapshotNameParam.class);FSOperations.FSCreateSnapshot command=new FSOperations.FSCreateSnapshot(path,snapshotName);String json=fsExecute(user,command);AUDIT_LOG.info("[{}] snapshot created as [{}]",path,snapshotName);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case SETXATTR:{String xattrName=params.get(XAttrNameParam.NAME,XAttrNameParam.class);
case CREATESNAPSHOT:{String snapshotName=params.get(SnapshotNameParam.NAME,SnapshotNameParam.class);FSOperations.FSCreateSnapshot command=new FSOperations.FSCreateSnapshot(path,snapshotName);String json=fsExecute(user,command);AUDIT_LOG.info("[{}] snapshot created as [{}]",path,snapshotName);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case SETXATTR:{String xattrName=params.get(XAttrNameParam.NAME,XAttrNameParam.class);String xattrValue=params.get(XAttrValueParam.NAME,XAttrValueParam.class);EnumSet<XAttrSetFlag> flag=params.get(XAttrSetFlagParam.NAME,XAttrSetFlagParam.class);FSOperations.FSSetXAttr command=new FSOperations.FSSetXAttr(path,xattrName,xattrValue,flag);fsExecute(user,command);AUDIT_LOG.info("[{}] to xAttr [{}]",path,xattrName);response=Response.ok().build();
}case SETXATTR:{String xattrName=params.get(XAttrNameParam.NAME,XAttrNameParam.class);String xattrValue=params.get(XAttrValueParam.NAME,XAttrValueParam.class);EnumSet<XAttrSetFlag> flag=params.get(XAttrSetFlagParam.NAME,XAttrSetFlagParam.class);FSOperations.FSSetXAttr command=new FSOperations.FSSetXAttr(path,xattrName,xattrValue,flag);fsExecute(user,command);AUDIT_LOG.info("[{}] to xAttr [{}]",path,xattrName);response=Response.ok().build();break;}case RENAMESNAPSHOT:{String oldSnapshotName=params.get(OldSnapshotNameParam.NAME,OldSnapshotNameParam.class);String snapshotName=params.get(SnapshotNameParam.NAME,SnapshotNameParam.class);FSOperations.FSRenameSnapshot command=new FSOperations.FSRenameSnapshot(path,oldSnapshotName,snapshotName);fsExecute(user,command);AUDIT_LOG.info("[{}] renamed snapshot [{}] to [{}]",path,oldSnapshotName,snapshotName);
response=Response.ok().build();break;}case RENAMESNAPSHOT:{String oldSnapshotName=params.get(OldSnapshotNameParam.NAME,OldSnapshotNameParam.class);String snapshotName=params.get(SnapshotNameParam.NAME,SnapshotNameParam.class);FSOperations.FSRenameSnapshot command=new FSOperations.FSRenameSnapshot(path,oldSnapshotName,snapshotName);fsExecute(user,command);AUDIT_LOG.info("[{}] renamed snapshot [{}] to [{}]",path,oldSnapshotName,snapshotName);response=Response.ok().build();break;}case REMOVEXATTR:{String xattrName=params.get(XAttrNameParam.NAME,XAttrNameParam.class);FSOperations.FSRemoveXAttr command=new FSOperations.FSRemoveXAttr(path,xattrName);fsExecute(user,command);AUDIT_LOG.info("[{}] removed xAttr [{}]",path,xattrName);
fsExecute(user,command);AUDIT_LOG.info("[{}] renamed snapshot [{}] to [{}]",path,oldSnapshotName,snapshotName);response=Response.ok().build();break;}case REMOVEXATTR:{String xattrName=params.get(XAttrNameParam.NAME,XAttrNameParam.class);FSOperations.FSRemoveXAttr command=new FSOperations.FSRemoveXAttr(path,xattrName);fsExecute(user,command);AUDIT_LOG.info("[{}] removed xAttr [{}]",path,xattrName);response=Response.ok().build();break;}case MKDIRS:{Short permission=params.get(PermissionParam.NAME,PermissionParam.class);Short unmaskedPermission=params.get(UnmaskedPermissionParam.NAME,UnmaskedPermissionParam.class);FSOperations.FSMkdirs command=new FSOperations.FSMkdirs(path,permission,unmaskedPermission);
fsExecute(user,command);AUDIT_LOG.info("[{}] removed xAttr [{}]",path,xattrName);response=Response.ok().build();break;}case MKDIRS:{Short permission=params.get(PermissionParam.NAME,PermissionParam.class);Short unmaskedPermission=params.get(UnmaskedPermissionParam.NAME,UnmaskedPermissionParam.class);FSOperations.FSMkdirs command=new FSOperations.FSMkdirs(path,permission,unmaskedPermission);JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}] permission [{}] unmaskedpermission [{}]",path,permission,unmaskedPermission);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case RENAME:{String toPath=params.get(DestinationParam.NAME,DestinationParam.class);FSOperations.FSRename command=new FSOperations.FSRename(path,toPath);
Short unmaskedPermission=params.get(UnmaskedPermissionParam.NAME,UnmaskedPermissionParam.class);FSOperations.FSMkdirs command=new FSOperations.FSMkdirs(path,permission,unmaskedPermission);JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}] permission [{}] unmaskedpermission [{}]",path,permission,unmaskedPermission);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case RENAME:{String toPath=params.get(DestinationParam.NAME,DestinationParam.class);FSOperations.FSRename command=new FSOperations.FSRename(path,toPath);JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}] to [{}]",path,toPath);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case SETOWNER:{String owner=params.get(OwnerParam.NAME,OwnerParam.class);
case RENAME:{String toPath=params.get(DestinationParam.NAME,DestinationParam.class);FSOperations.FSRename command=new FSOperations.FSRename(path,toPath);JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}] to [{}]",path,toPath);response=Response.ok(json).type(MediaType.APPLICATION_JSON).build();break;}case SETOWNER:{String owner=params.get(OwnerParam.NAME,OwnerParam.class);String group=params.get(GroupParam.NAME,GroupParam.class);FSOperations.FSSetOwner command=new FSOperations.FSSetOwner(path,owner,group);fsExecute(user,command);AUDIT_LOG.info("[{}] to (O/G)[{}]",path,owner + ":" + group);response=Response.ok().build();break;
case SETOWNER:{String owner=params.get(OwnerParam.NAME,OwnerParam.class);String group=params.get(GroupParam.NAME,GroupParam.class);FSOperations.FSSetOwner command=new FSOperations.FSSetOwner(path,owner,group);fsExecute(user,command);AUDIT_LOG.info("[{}] to (O/G)[{}]",path,owner + ":" + group);response=Response.ok().build();break;}case SETPERMISSION:{Short permission=params.get(PermissionParam.NAME,PermissionParam.class);FSOperations.FSSetPermission command=new FSOperations.FSSetPermission(path,permission);fsExecute(user,command);AUDIT_LOG.info("[{}] to [{}]",path,permission);response=Response.ok().build();break;
break;}case SETPERMISSION:{Short permission=params.get(PermissionParam.NAME,PermissionParam.class);FSOperations.FSSetPermission command=new FSOperations.FSSetPermission(path,permission);fsExecute(user,command);AUDIT_LOG.info("[{}] to [{}]",path,permission);response=Response.ok().build();break;}case SETREPLICATION:{Short replication=params.get(ReplicationParam.NAME,ReplicationParam.class);FSOperations.FSSetReplication command=new FSOperations.FSSetReplication(path,replication);JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}] to [{}]",path,replication);response=Response.ok(json).build();break;
AUDIT_LOG.info("[{}] to [{}]",path,permission);response=Response.ok().build();break;}case SETREPLICATION:{Short replication=params.get(ReplicationParam.NAME,ReplicationParam.class);FSOperations.FSSetReplication command=new FSOperations.FSSetReplication(path,replication);JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}] to [{}]",path,replication);response=Response.ok(json).build();break;}case SETTIMES:{Long modifiedTime=params.get(ModifiedTimeParam.NAME,ModifiedTimeParam.class);Long accessTime=params.get(AccessTimeParam.NAME,AccessTimeParam.class);FSOperations.FSSetTimes command=new FSOperations.FSSetTimes(path,modifiedTime,accessTime);fsExecute(user,command);
JSONObject json=fsExecute(user,command);AUDIT_LOG.info("[{}] to [{}]",path,replication);response=Response.ok(json).build();break;}case SETTIMES:{Long modifiedTime=params.get(ModifiedTimeParam.NAME,ModifiedTimeParam.class);Long accessTime=params.get(AccessTimeParam.NAME,AccessTimeParam.class);FSOperations.FSSetTimes command=new FSOperations.FSSetTimes(path,modifiedTime,accessTime);fsExecute(user,command);AUDIT_LOG.info("[{}] to (M/A)[{}]",path,modifiedTime + ":" + accessTime);response=Response.ok().build();break;}case SETACL:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);FSOperations.FSSetAcl command=new FSOperations.FSSetAcl(path,aclSpec);
Long accessTime=params.get(AccessTimeParam.NAME,AccessTimeParam.class);FSOperations.FSSetTimes command=new FSOperations.FSSetTimes(path,modifiedTime,accessTime);fsExecute(user,command);AUDIT_LOG.info("[{}] to (M/A)[{}]",path,modifiedTime + ":" + accessTime);response=Response.ok().build();break;}case SETACL:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);FSOperations.FSSetAcl command=new FSOperations.FSSetAcl(path,aclSpec);fsExecute(user,command);AUDIT_LOG.info("[{}] to acl [{}]",path,aclSpec);response=Response.ok().build();break;}case REMOVEACL:{FSOperations.FSRemoveAcl command=new FSOperations.FSRemoveAcl(path);
}case SETACL:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);FSOperations.FSSetAcl command=new FSOperations.FSSetAcl(path,aclSpec);fsExecute(user,command);AUDIT_LOG.info("[{}] to acl [{}]",path,aclSpec);response=Response.ok().build();break;}case REMOVEACL:{FSOperations.FSRemoveAcl command=new FSOperations.FSRemoveAcl(path);fsExecute(user,command);AUDIT_LOG.info("[{}] removed acl",path);response=Response.ok().build();break;}case MODIFYACLENTRIES:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);
break;}case REMOVEACL:{FSOperations.FSRemoveAcl command=new FSOperations.FSRemoveAcl(path);fsExecute(user,command);AUDIT_LOG.info("[{}] removed acl",path);response=Response.ok().build();break;}case MODIFYACLENTRIES:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);FSOperations.FSModifyAclEntries command=new FSOperations.FSModifyAclEntries(path,aclSpec);fsExecute(user,command);AUDIT_LOG.info("[{}] modify acl entry with [{}]",path,aclSpec);response=Response.ok().build();break;}case REMOVEACLENTRIES:{
break;}case MODIFYACLENTRIES:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);FSOperations.FSModifyAclEntries command=new FSOperations.FSModifyAclEntries(path,aclSpec);fsExecute(user,command);AUDIT_LOG.info("[{}] modify acl entry with [{}]",path,aclSpec);response=Response.ok().build();break;}case REMOVEACLENTRIES:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);FSOperations.FSRemoveAclEntries command=new FSOperations.FSRemoveAclEntries(path,aclSpec);fsExecute(user,command);AUDIT_LOG.info("[{}] remove acl entry [{}]",path,aclSpec);response=Response.ok().build();break;
AUDIT_LOG.info("[{}] modify acl entry with [{}]",path,aclSpec);response=Response.ok().build();break;}case REMOVEACLENTRIES:{String aclSpec=params.get(AclPermissionParam.NAME,AclPermissionParam.class);FSOperations.FSRemoveAclEntries command=new FSOperations.FSRemoveAclEntries(path,aclSpec);fsExecute(user,command);AUDIT_LOG.info("[{}] remove acl entry [{}]",path,aclSpec);response=Response.ok().build();break;}case REMOVEDEFAULTACL:{FSOperations.FSRemoveDefaultAcl command=new FSOperations.FSRemoveDefaultAcl(path);fsExecute(user,command);AUDIT_LOG.info("[{}] remove default acl",path);response=Response.ok().build();
public void init() throws ServerException {  if (status != Status.UNDEF) {    throw new IllegalStateException("Server already initialized");  }  status=Status.BOOTING;  verifyDir(homeDir);  verifyDir(tempDir);  Properties serverInfo=new Properties();  try {    InputStream is=getResource(name + ".properties");    serverInfo.load(is);    is.close();  } catch (  IOException ex) {    throw new RuntimeException("Could not load server information file: " + name + ".properties");  }  initLog();  log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");
    throw new IllegalStateException("Server already initialized");  }  status=Status.BOOTING;  verifyDir(homeDir);  verifyDir(tempDir);  Properties serverInfo=new Properties();  try {    InputStream is=getResource(name + ".properties");    serverInfo.load(is);    is.close();  } catch (  IOException ex) {    throw new RuntimeException("Could not load server information file: " + name + ".properties");  }  initLog();  log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");  log.info("Server [{}] starting",name);  log.info("  Built information:");
  }  status=Status.BOOTING;  verifyDir(homeDir);  verifyDir(tempDir);  Properties serverInfo=new Properties();  try {    InputStream is=getResource(name + ".properties");    serverInfo.load(is);    is.close();  } catch (  IOException ex) {    throw new RuntimeException("Could not load server information file: " + name + ".properties");  }  initLog();  log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");  log.info("Server [{}] starting",name);  log.info("  Built information:");  log.info("    Version           : {}",serverInfo.getProperty(name + ".version","undef"));
  status=Status.BOOTING;  verifyDir(homeDir);  verifyDir(tempDir);  Properties serverInfo=new Properties();  try {    InputStream is=getResource(name + ".properties");    serverInfo.load(is);    is.close();  } catch (  IOException ex) {    throw new RuntimeException("Could not load server information file: " + name + ".properties");  }  initLog();  log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");  log.info("Server [{}] starting",name);  log.info("  Built information:");  log.info("    Version           : {}",serverInfo.getProperty(name + ".version","undef"));
  verifyDir(homeDir);  verifyDir(tempDir);  Properties serverInfo=new Properties();  try {    InputStream is=getResource(name + ".properties");    serverInfo.load(is);    is.close();  } catch (  IOException ex) {    throw new RuntimeException("Could not load server information file: " + name + ".properties");  }  initLog();  log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");  log.info("Server [{}] starting",name);  log.info("  Built information:");  log.info("    Version           : {}",serverInfo.getProperty(name + ".version","undef"));  log.info("    Source Repository : {}",serverInfo.getProperty(name + ".source.repository","undef"));
  verifyDir(tempDir);  Properties serverInfo=new Properties();  try {    InputStream is=getResource(name + ".properties");    serverInfo.load(is);    is.close();  } catch (  IOException ex) {    throw new RuntimeException("Could not load server information file: " + name + ".properties");  }  initLog();  log.info("++++++++++++++++++++++++++++++++++++++++++++++++++++++");  log.info("Server [{}] starting",name);  log.info("  Built information:");  log.info("    Version           : {}",serverInfo.getProperty(name + ".version","undef"));  log.info("    Source Repository : {}",serverInfo.getProperty(name + ".source.repository","undef"));  log.info("    Source Revision   : {}",serverInfo.getProperty(name + ".source.revision","undef"));
 else {    try {      defaultConf=new Configuration(false);      ConfigurationUtils.load(defaultConf,inputStream);    } catch (    Exception ex) {      throw new ServerException(ServerException.ERROR.S03,defaultConfig,ex.getMessage(),ex);    }  }  if (config == null) {    Configuration siteConf;    File siteFile=new File(file,name + "-site.xml");    if (!siteFile.exists()) {      log.warn("Site configuration file [{}] not found in config directory",siteFile);      siteConf=new Configuration(false);    } else {      if (!siteFile.isFile()) {        throw new ServerException(ServerException.ERROR.S05,siteFile.getAbsolutePath());
    } else {      if (!siteFile.isFile()) {        throw new ServerException(ServerException.ERROR.S05,siteFile.getAbsolutePath());      }      try {        log.debug("Loading site configuration from [{}]",siteFile);        inputStream=Files.newInputStream(siteFile.toPath());        siteConf=new Configuration(false);        ConfigurationUtils.load(siteConf,inputStream);      } catch (      IOException ex) {        throw new ServerException(ServerException.ERROR.S06,siteFile,ex.getMessage(),ex);      }    }    config=new Configuration(false);    ConfigurationUtils.copy(siteConf,config);  }  ConfigurationUtils.injectDefaults(defaultConf,config);  ConfigRedactor redactor=new ConfigRedactor(config);  for (  String name : System.getProperties().stringPropertyNames()) {
        inputStream=Files.newInputStream(siteFile.toPath());        siteConf=new Configuration(false);        ConfigurationUtils.load(siteConf,inputStream);      } catch (      IOException ex) {        throw new ServerException(ServerException.ERROR.S06,siteFile,ex.getMessage(),ex);      }    }    config=new Configuration(false);    ConfigurationUtils.copy(siteConf,config);  }  ConfigurationUtils.injectDefaults(defaultConf,config);  ConfigRedactor redactor=new ConfigRedactor(config);  for (  String name : System.getProperties().stringPropertyNames()) {    String value=System.getProperty(name);    if (name.startsWith(getPrefix() + ".")) {      config.set(name,value);      String redacted=redactor.redact(name,value);      log.info("System property sets  {}: {}",name,redacted);
private void loadServices(Class[] classes,List<Service> list) throws ServerException {  for (  Class klass : classes) {    try {      Service service=(Service)klass.newInstance();
protected List<Service> loadServices() throws ServerException {  try {    Map<Class,Service> map=new LinkedHashMap<Class,Service>();    Class[] classes=getConfig().getClasses(getPrefixedName(CONF_SERVICES));    Class[] classesExt=getConfig().getClasses(getPrefixedName(CONF_SERVICES_EXT));    List<Service> list=new ArrayList<Service>();    loadServices(classes,list);    loadServices(classesExt,list);    for (    Service service : list) {      if (map.containsKey(service.getInterface())) {
protected void initServices(List<Service> services) throws ServerException {  for (  Service service : services) {
protected void destroyServices(){  List<Service> list=new ArrayList<Service>(services.values());  Collections.reverse(list);  for (  Service service : list) {    try {
  ensureOperational();  Check.notNull(klass,"serviceKlass");  if (getStatus() == Status.SHUTTING_DOWN) {    throw new IllegalStateException("Server shutting down");  }  try {    Service newService=klass.newInstance();    Service oldService=services.get(newService.getInterface());    if (oldService != null) {      try {        oldService.destroy();      } catch (      Throwable ex) {        log.error("Could not destroy service [{}], {}",new Object[]{oldService.getInterface(),ex.getMessage(),ex});      }    }    newService.init(this);    services.put(newService.getInterface(),newService);  } catch (  Exception ex) {
@Override public void schedule(final Callable<?> callable,long delay,long interval,TimeUnit unit){  Check.notNull(callable,"callable");  if (!scheduler.isShutdown()) {    LOG.debug("Scheduling callable [{}], interval [{}] seconds, delay [{}] in [{}]",new Object[]{callable,delay,interval,unit});    Runnable r=new Runnable(){      @Override public void run(){        String instrName=callable.getClass().getSimpleName();        Instrumentation instr=getServer().get(Instrumentation.class);        if (getServer().getStatus() == Server.Status.HALTED) {
protected void log(Response.Status status,Throwable throwable){
private static void execWaitRet(String cmd) throws IOException {
private static void execIgnoreRet(String cmd) throws IOException {
private static void execAssertSucceeds(String cmd) throws IOException {
private static void execAssertFails(String cmd) throws IOException {
private static Process establishMount(URI uri) throws IOException {  Runtime r=Runtime.getRuntime();  String cp=System.getProperty("java.class.path");  String buildTestDir=System.getProperty("build.test");  String fuseCmd=buildTestDir + "/../fuse_dfs";  String libHdfs=buildTestDir + "/../../../c++/lib";  String arch=System.getProperty("os.arch");  String jvm=System.getProperty("java.home") + "/lib/" + arch+ "/server";  String lp=System.getProperty("LD_LIBRARY_PATH") + ":" + libHdfs+ ":"+ jvm;
private void addExports() throws IOException {  FileSystem fs=FileSystem.get(config);  String[] exportsPath=config.getStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY,NfsConfigKeys.DFS_NFS_EXPORT_POINT_DEFAULT);  for (  String exportPath : exportsPath) {    URI exportURI=Nfs3Utils.getResolvedURI(fs,exportPath);
@Override public XDR nullOp(XDR out,int xid,InetAddress client){  if (LOG.isDebugEnabled()) {
    return MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES,out,xid,null);  }  String path=xdr.readString();  if (LOG.isDebugEnabled()) {    LOG.debug("MOUNT MNT path: " + path + " client: "+ client);  }  String host=client.getHostName();  if (LOG.isDebugEnabled()) {    LOG.debug("Got host: " + host + " path: "+ path);  }  URI exportURI=exports.get(path);  if (exportURI == null) {    LOG.info("Path " + path + " is not shared.");    MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT,out,xid,null);    return out;  }  DFSClient dfsClient=null;  try {    dfsClient=new DFSClient(exportURI,config);
    LOG.debug("Got host: " + host + " path: "+ path);  }  URI exportURI=exports.get(path);  if (exportURI == null) {    LOG.info("Path " + path + " is not shared.");    MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT,out,xid,null);    return out;  }  DFSClient dfsClient=null;  try {    dfsClient=new DFSClient(exportURI,config);  } catch (  Exception e) {    LOG.error("Can't get handle for export:" + path,e);    MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT,out,xid,null);    return out;  }  FileHandle handle=null;  try {
    LOG.info("Path " + path + " is not shared.");    MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT,out,xid,null);    return out;  }  DFSClient dfsClient=null;  try {    dfsClient=new DFSClient(exportURI,config);  } catch (  Exception e) {    LOG.error("Can't get handle for export:" + path,e);    MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_NOENT,out,xid,null);    return out;  }  FileHandle handle=null;  try {    HdfsFileStatus exFileStatus=dfsClient.getFileInfo(exportURI.getPath());    handle=new FileHandle(exFileStatus.getFileId(),Nfs3Utils.getNamenodeId(config,exportURI));  } catch (  IOException e) {
@Override public XDR dump(XDR out,int xid,InetAddress client){  if (LOG.isDebugEnabled()) {
@Override public XDR umnt(XDR xdr,XDR out,int xid,InetAddress client){  String path=xdr.readString();  if (LOG.isDebugEnabled()) {
@Override public XDR umntall(XDR out,int xid,InetAddress client){  if (LOG.isDebugEnabled()) {
private void prepareAddressMap() throws IOException {  FileSystem fs=FileSystem.get(config);  String[] exportsPath=config.getStrings(NfsConfigKeys.DFS_NFS_EXPORT_POINT_KEY,NfsConfigKeys.DFS_NFS_EXPORT_POINT_DEFAULT);  for (  String exportPath : exportsPath) {    URI exportURI=Nfs3Utils.getResolvedURI(fs,exportPath);    int namenodeId=Nfs3Utils.getNamenodeId(config,exportURI);    URI value=namenodeUriMap.get(namenodeId);    if (value == null) {
UserGroupInformation getUserGroupInformation(String effectiveUser,UserGroupInformation realUser) throws IOException {  Preconditions.checkNotNull(effectiveUser);  Preconditions.checkNotNull(realUser);  realUser.checkTGTAndReloginFromKeytab();  UserGroupInformation ugi=UserGroupInformation.createProxyUser(effectiveUser,realUser);
private long updateNonSequentialWriteInMemory(long count){  long newValue=nonSequentialWriteInMemory.addAndGet(count);
@VisibleForTesting public static void alterWriteRequest(WRITE3Request request,long cachedOffset){  long offset=request.getOffset();  int count=request.getCount();  long smallerCount=offset + count - cachedOffset;
private synchronized WriteCtx addWritesToCache(WRITE3Request request,Channel channel,int xid){  long offset=request.getOffset();  int count=request.getCount();  long cachedOffset=nextOffset.get();  int originalCount=WriteCtx.INVALID_ORIGINAL_COUNT;
    LOG.warn(String.format("Got overwrite [%d-%d) smaller than" + " current offset %d," + " drop the request.",offset,(offset + count),cachedOffset));    return null;  }  if ((offset < cachedOffset) && (offset + count > cachedOffset)) {    LOG.warn(String.format("Got overwrite with appended data [%d-%d)," + " current offset %d," + " drop the overlapped section [%d-%d)"+ " and append new data [%d-%d).",offset,(offset + count),cachedOffset,offset,cachedOffset,cachedOffset,(offset + count)));    LOG.warn("Modify this write to write only the appended data");    alterWriteRequest(request,cachedOffset);    originalCount=count;    offset=request.getOffset();    count=request.getCount();  }  if (offset < cachedOffset) {    LOG.warn("(offset,count,nextOffset): ({},{},{})",offset,count,nextOffset);    return null;  } else {    DataState dataState=offset == cachedOffset ? WriteCtx.DataState.NO_DUMP : WriteCtx.DataState.ALLOW_DUMP;    WriteCtx writeCtx=new WriteCtx(request.getHandle(),request.getOffset(),request.getCount(),originalCount,request.getStableHow(),request.getData(),channel,xid,false,dataState);
    alterWriteRequest(request,cachedOffset);    originalCount=count;    offset=request.getOffset();    count=request.getCount();  }  if (offset < cachedOffset) {    LOG.warn("(offset,count,nextOffset): ({},{},{})",offset,count,nextOffset);    return null;  } else {    DataState dataState=offset == cachedOffset ? WriteCtx.DataState.NO_DUMP : WriteCtx.DataState.ALLOW_DUMP;    WriteCtx writeCtx=new WriteCtx(request.getHandle(),request.getOffset(),request.getCount(),originalCount,request.getStableHow(),request.getData(),channel,xid,false,dataState);    LOG.debug("Add new write to the list with nextOffset {}" + " and requested offset={}",cachedOffset,offset);    if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {      updateNonSequentialWriteInMemory(count);    }    WriteCtx oldWriteCtx=checkRepeatedWriteRequest(request,channel,xid);    if (oldWriteCtx == null) {
private synchronized boolean checkAndStartWrite(AsyncDataService asyncDataService,WriteCtx writeCtx){  if (writeCtx.getOffset() == nextOffset.get()) {    if (!asyncStatus) {
  WRITE3Response response;  byte[] readbuffer=new byte[count];  int readCount=0;  FSDataInputStream fis=null;  try {    fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));  } catch (  ClosedChannelException closedException) {    LOG.info("The FSDataOutputStream has been closed. " + "Continue processing the perfect overwrite.");  }catch (  IOException e) {    LOG.info("hsync failed when processing possible perfect overwrite, " + "path={} error: {}",path,e.toString());    return new WRITE3Response(Nfs3Status.NFS3ERR_IO,wccData,0,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);  }  try {    fis=dfsClient.createWrappedInputStream(dfsClient.open(path));    readCount=fis.read(offset,readbuffer,0,count);    if (readCount < count) {
  FSDataInputStream fis=null;  try {    fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));  } catch (  ClosedChannelException closedException) {    LOG.info("The FSDataOutputStream has been closed. " + "Continue processing the perfect overwrite.");  }catch (  IOException e) {    LOG.info("hsync failed when processing possible perfect overwrite, " + "path={} error: {}",path,e.toString());    return new WRITE3Response(Nfs3Status.NFS3ERR_IO,wccData,0,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);  }  try {    fis=dfsClient.createWrappedInputStream(dfsClient.open(path));    readCount=fis.read(offset,readbuffer,0,count);    if (readCount < count) {      LOG.error("Can't read back {} bytes, partial read size: {}",count,readCount);      return new WRITE3Response(Nfs3Status.NFS3ERR_IO,wccData,0,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);    }  } catch (  IOException e) {
      LOG.error("Can't read back {} bytes, partial read size: {}",count,readCount);      return new WRITE3Response(Nfs3Status.NFS3ERR_IO,wccData,0,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);    }  } catch (  IOException e) {    LOG.info("Read failed when processing possible perfect overwrite, " + "path={}",path,e);    return new WRITE3Response(Nfs3Status.NFS3ERR_IO,wccData,0,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);  } finally {    IOUtils.cleanupWithLogger(LOG,fis);  }  Comparator comparator=new Comparator();  if (comparator.compare(readbuffer,0,readCount,data,0,count) != 0) {    LOG.info("Perfect overwrite has different content");    response=new WRITE3Response(Nfs3Status.NFS3ERR_INVAL,wccData,0,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);  } else {    LOG.info("Perfect overwrite has same content," + " updating the mtime, then return success");    Nfs3FileAttributes postOpAttr=null;    try {
    Preconditions.checkState(channel != null && preOpAttr != null);    updateLastAccessTime();  }  Preconditions.checkState(commitOffset >= 0);  COMMIT_STATUS ret=checkCommitInternal(commitOffset,channel,xid,preOpAttr,fromRead);  LOG.debug("Got commit status: {}",ret.name());  if (ret == COMMIT_STATUS.COMMIT_DO_SYNC || ret == COMMIT_STATUS.COMMIT_FINISHED) {    try {      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));      ret=COMMIT_STATUS.COMMIT_FINISHED;    } catch (    ClosedChannelException cce) {      if (pendingWrites.isEmpty()) {        ret=COMMIT_STATUS.COMMIT_FINISHED;      } else {        ret=COMMIT_STATUS.COMMIT_ERROR;      }    }catch (    IOException e) {
  if (pendingWrites.isEmpty()) {    LOG.debug("The async write task has no pending writes, fileId: {}",latestAttr.getFileId());    processCommits(nextOffset.get());    this.asyncStatus=false;    return null;  }  Entry<OffsetRange,WriteCtx> lastEntry=pendingWrites.lastEntry();  OffsetRange range=lastEntry.getKey();  WriteCtx toWrite=lastEntry.getValue();  LOG.trace("range.getMin()={} nextOffset={}",range.getMin(),nextOffset);  long offset=nextOffset.get();  if (range.getMin() > offset) {    LOG.debug("The next sequential write has not arrived yet");    processCommits(nextOffset.get());    this.asyncStatus=false;  } else   if (range.getMax() <= offset) {
  OffsetRange range=lastEntry.getKey();  WriteCtx toWrite=lastEntry.getValue();  LOG.trace("range.getMin()={} nextOffset={}",range.getMin(),nextOffset);  long offset=nextOffset.get();  if (range.getMin() > offset) {    LOG.debug("The next sequential write has not arrived yet");    processCommits(nextOffset.get());    this.asyncStatus=false;  } else   if (range.getMax() <= offset) {    LOG.debug("Remove write {} which is already written from the list",range);    pendingWrites.remove(range);  } else   if (range.getMin() < offset && range.getMax() > offset) {    LOG.warn("Got an overlapping write {}, nextOffset={}. " + "Remove and trim it",range,offset);    pendingWrites.remove(range);    trimWriteRequest(toWrite,offset);
  long offset=nextOffset.get();  if (range.getMin() > offset) {    LOG.debug("The next sequential write has not arrived yet");    processCommits(nextOffset.get());    this.asyncStatus=false;  } else   if (range.getMax() <= offset) {    LOG.debug("Remove write {} which is already written from the list",range);    pendingWrites.remove(range);  } else   if (range.getMin() < offset && range.getMax() > offset) {    LOG.warn("Got an overlapping write {}, nextOffset={}. " + "Remove and trim it",range,offset);    pendingWrites.remove(range);    trimWriteRequest(toWrite,offset);    nextOffset.addAndGet(toWrite.getCount());    LOG.debug("Change nextOffset (after trim) to {}",nextOffset.get());    return toWrite;
    processCommits(nextOffset.get());    this.asyncStatus=false;  } else   if (range.getMax() <= offset) {    LOG.debug("Remove write {} which is already written from the list",range);    pendingWrites.remove(range);  } else   if (range.getMin() < offset && range.getMax() > offset) {    LOG.warn("Got an overlapping write {}, nextOffset={}. " + "Remove and trim it",range,offset);    pendingWrites.remove(range);    trimWriteRequest(toWrite,offset);    nextOffset.addAndGet(toWrite.getCount());    LOG.debug("Change nextOffset (after trim) to {}",nextOffset.get());    return toWrite;  } else {    LOG.debug("Remove write {} from the list",range);    pendingWrites.remove(range);
  try {    while (activeState) {      WriteCtx toWrite=offerNextToWrite();      if (toWrite != null) {        doSingleWrite(toWrite);        updateLastAccessTime();      } else {        break;      }    }    if (!activeState) {      LOG.debug("The openFileCtx is not active anymore, fileId: {}",latestAttr.getFileId());    }  }  finally {synchronized (this) {      if (startOffset == asyncWriteBackStartOffset) {        asyncStatus=false;      } else {
private void processCommits(long offset){  Preconditions.checkState(offset > 0);  long flushedOffset=getFlushedOffset();  Entry<Long,CommitCtx> entry=pendingCommits.firstEntry();  if (entry == null || entry.getValue().offset > flushedOffset) {    return;  }  int status=Nfs3Status.NFS3ERR_IO;  try {    fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));    status=Nfs3Status.NFS3_OK;  } catch (  ClosedChannelException cce) {    if (!pendingWrites.isEmpty()) {      LOG.error("Can't sync for fileId: {}. " + "Channel closed with writes pending",latestAttr.getFileId(),cce);    }    status=Nfs3Status.NFS3ERR_IO;  }catch (  IOException e) {
    return;  }  int status=Nfs3Status.NFS3ERR_IO;  try {    fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));    status=Nfs3Status.NFS3_OK;  } catch (  ClosedChannelException cce) {    if (!pendingWrites.isEmpty()) {      LOG.error("Can't sync for fileId: {}. " + "Channel closed with writes pending",latestAttr.getFileId(),cce);    }    status=Nfs3Status.NFS3ERR_IO;  }catch (  IOException e) {    LOG.error("Got stream error during data sync: ",e);    status=Nfs3Status.NFS3ERR_IO;  }  try {    latestAttr=Nfs3Utils.getFileAttr(client,Nfs3Utils.getFileIdPath(latestAttr.getFileId()),iug);  } catch (  IOException e) {
  try {    fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));    status=Nfs3Status.NFS3_OK;  } catch (  ClosedChannelException cce) {    if (!pendingWrites.isEmpty()) {      LOG.error("Can't sync for fileId: {}. " + "Channel closed with writes pending",latestAttr.getFileId(),cce);    }    status=Nfs3Status.NFS3ERR_IO;  }catch (  IOException e) {    LOG.error("Got stream error during data sync: ",e);    status=Nfs3Status.NFS3ERR_IO;  }  try {    latestAttr=Nfs3Utils.getFileAttr(client,Nfs3Utils.getFileIdPath(latestAttr.getFileId()),iug);  } catch (  IOException e) {    LOG.error("Can't get new file attr, fileId: " + latestAttr.getFileId(),e);    status=Nfs3Status.NFS3ERR_IO;
catch (  IOException e) {    LOG.error("Got stream error during data sync: ",e);    status=Nfs3Status.NFS3ERR_IO;  }  try {    latestAttr=Nfs3Utils.getFileAttr(client,Nfs3Utils.getFileIdPath(latestAttr.getFileId()),iug);  } catch (  IOException e) {    LOG.error("Can't get new file attr, fileId: " + latestAttr.getFileId(),e);    status=Nfs3Status.NFS3ERR_IO;  }  if (latestAttr.getSize() != offset) {    LOG.error("After sync, the expect file size: {}, " + "however actual file size is: {}",offset,latestAttr.getSize());    status=Nfs3Status.NFS3ERR_IO;  }  WccData wccData=new WccData(Nfs3Utils.getWccAttr(latestAttr),latestAttr);  while (entry != null && entry.getValue().offset <= flushedOffset) {    pendingCommits.remove(entry.getKey());    CommitCtx commit=entry.getValue();
private void doSingleWrite(final WriteCtx writeCtx){  Channel channel=writeCtx.getChannel();  int xid=writeCtx.getXid();  long offset=writeCtx.getOffset();  int count=writeCtx.getCount();  WriteStableHow stableHow=writeCtx.getStableHow();  FileHandle handle=writeCtx.getHandle();  if (LOG.isDebugEnabled()) {
  FileHandle handle=writeCtx.getHandle();  if (LOG.isDebugEnabled()) {    LOG.debug("do write, fileHandle {} offset: {} length: {} stableHow: {}",handle.dumpFileHandle(),offset,count,stableHow.name());  }  try {    writeCtx.writeData(fos);    RpcProgramNfs3.metrics.incrBytesWritten(writeCtx.getCount());    long flushedOffset=getFlushedOffset();    if (flushedOffset != (offset + count)) {      throw new IOException("output stream is out of sync, pos=" + flushedOffset + " and nextOffset should be"+ (offset + count));    }    if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {synchronized (writeCtx) {        if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {          writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);          updateNonSequentialWriteInMemory(-count);          if (LOG.isDebugEnabled()) {
  }  try {    writeCtx.writeData(fos);    RpcProgramNfs3.metrics.incrBytesWritten(writeCtx.getCount());    long flushedOffset=getFlushedOffset();    if (flushedOffset != (offset + count)) {      throw new IOException("output stream is out of sync, pos=" + flushedOffset + " and nextOffset should be"+ (offset + count));    }    if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {synchronized (writeCtx) {        if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {          writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);          updateNonSequentialWriteInMemory(-count);          if (LOG.isDebugEnabled()) {            LOG.debug("After writing {} at offset {}, " + "updated the memory count, new value: {}",handle.dumpFileHandle(),offset,nonSequentialWriteInMemory.get());          }        }      }    }    if (!writeCtx.getReplied()) {      if (stableHow != WriteStableHow.UNSTABLE) {
    if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {synchronized (writeCtx) {        if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {          writeCtx.setDataState(WriteCtx.DataState.NO_DUMP);          updateNonSequentialWriteInMemory(-count);          if (LOG.isDebugEnabled()) {            LOG.debug("After writing {} at offset {}, " + "updated the memory count, new value: {}",handle.dumpFileHandle(),offset,nonSequentialWriteInMemory.get());          }        }      }    }    if (!writeCtx.getReplied()) {      if (stableHow != WriteStableHow.UNSTABLE) {        LOG.info("Do sync for stable write: {}",writeCtx);        try {          if (stableHow == WriteStableHow.DATA_SYNC) {            fos.hsync();          } else {            Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC,"Unknown WriteStableHow: " + stableHow);
      if (stableHow != WriteStableHow.UNSTABLE) {        LOG.info("Do sync for stable write: {}",writeCtx);        try {          if (stableHow == WriteStableHow.DATA_SYNC) {            fos.hsync();          } else {            Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC,"Unknown WriteStableHow: " + stableHow);            fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));          }        } catch (        IOException e) {          LOG.error("hsync failed with writeCtx: {}",writeCtx,e);          throw e;        }      }      WccAttr preOpAttr=latestAttr.getWccAttr();      WccData fileWcc=new WccData(preOpAttr,latestAttr);      if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {        LOG.warn("Return original count: {} instead of real data count: {}",writeCtx.getOriginalCount(),count);
          } else {            Preconditions.checkState(stableHow == WriteStableHow.FILE_SYNC,"Unknown WriteStableHow: " + stableHow);            fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));          }        } catch (        IOException e) {          LOG.error("hsync failed with writeCtx: {}",writeCtx,e);          throw e;        }      }      WccAttr preOpAttr=latestAttr.getWccAttr();      WccData fileWcc=new WccData(preOpAttr,latestAttr);      if (writeCtx.getOriginalCount() != WriteCtx.INVALID_ORIGINAL_COUNT) {        LOG.warn("Return original count: {} instead of real data count: {}",writeCtx.getOriginalCount(),count);        count=writeCtx.getOriginalCount();      }      WRITE3Response response=new WRITE3Response(Nfs3Status.NFS3_OK,fileWcc,count,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);      RpcProgramNfs3.metrics.addWrite(Nfs3Utils.getElapsedTime(writeCtx.startTime));      Nfs3Utils.writeChannel(channel,response.serialize(new XDR(),xid,new VerifierNone()),xid);    }    processCommits(writeCtx.getOffset() + writeCtx.getCount());
  }  activeState=false;  if (dumpThread != null && dumpThread.isAlive()) {    dumpThread.interrupt();    try {      dumpThread.join(3000);    } catch (    InterruptedException ignored) {    }  }  try {    if (fos != null) {      fos.close();    }  } catch (  IOException e) {    LOG.info("Can't close stream for fileId: {}, error: {}",latestAttr.getFileId(),e.toString());  }  LOG.info("There are {} pending writes.",pendingWrites.size());  WccAttr preOpAttr=latestAttr.getWccAttr();  while (!pendingWrites.isEmpty()) {    OffsetRange key=pendingWrites.firstKey();
  try {    if (fos != null) {      fos.close();    }  } catch (  IOException e) {    LOG.info("Can't close stream for fileId: {}, error: {}",latestAttr.getFileId(),e.toString());  }  LOG.info("There are {} pending writes.",pendingWrites.size());  WccAttr preOpAttr=latestAttr.getWccAttr();  while (!pendingWrites.isEmpty()) {    OffsetRange key=pendingWrites.firstKey();    LOG.info("Fail pending write: {}, nextOffset={}",key,nextOffset.get());    WriteCtx writeCtx=pendingWrites.remove(key);    if (!writeCtx.getReplied()) {      WccData fileWcc=new WccData(preOpAttr,latestAttr);      WRITE3Response response=new WRITE3Response(Nfs3Status.NFS3ERR_IO,fileWcc,0,writeCtx.getStableHow(),Nfs3Constant.WRITE_COMMIT_VERF);      Nfs3Utils.writeChannel(writeCtx.getChannel(),response.serialize(new XDR(),writeCtx.getXid(),new VerifierNone()),writeCtx.getXid());
  LOG.info("There are {} pending writes.",pendingWrites.size());  WccAttr preOpAttr=latestAttr.getWccAttr();  while (!pendingWrites.isEmpty()) {    OffsetRange key=pendingWrites.firstKey();    LOG.info("Fail pending write: {}, nextOffset={}",key,nextOffset.get());    WriteCtx writeCtx=pendingWrites.remove(key);    if (!writeCtx.getReplied()) {      WccData fileWcc=new WccData(preOpAttr,latestAttr);      WRITE3Response response=new WRITE3Response(Nfs3Status.NFS3ERR_IO,fileWcc,0,writeCtx.getStableHow(),Nfs3Constant.WRITE_COMMIT_VERF);      Nfs3Utils.writeChannel(writeCtx.getChannel(),response.serialize(new XDR(),writeCtx.getXid(),new VerifierNone()),writeCtx.getXid());    }  }  if (dumpOut != null) {    try {      dumpOut.close();    } catch (    IOException e) {      LOG.error("Failed to close outputstream of dump file {}",dumpFilePath,e);
        LOG.debug("Got one inactive stream: " + ctx);      }      return pairs;    }    if (ctx.hasPendingWork()) {      continue;    }    if (idlest == null) {      idlest=pairs;    } else {      if (ctx.getLastAccessTime() < idlest.getValue().getLastAccessTime()) {        idlest=pairs;      }    }  }  if (idlest == null) {    LOG.warn("No eviction candidate. All streams have pending work.");    return null;  } else {    long idleTime=Time.monotonicNow() - idlest.getValue().getLastAccessTime();    if (idleTime < NfsConfigKeys.DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT) {
  Iterator<Entry<FileHandle,OpenFileCtx>> it=openFileMap.entrySet().iterator();  if (LOG.isTraceEnabled()) {    LOG.trace("openFileMap size:" + size());  }  while (it.hasNext()) {    Entry<FileHandle,OpenFileCtx> pairs=it.next();    FileHandle handle=pairs.getKey();    OpenFileCtx ctx=pairs.getValue();    if (!ctx.streamCleanup(handle,streamTimeout)) {      continue;    }synchronized (this) {      OpenFileCtx ctx2=openFileMap.get(handle);      if (ctx2 != null) {        if (ctx2.streamCleanup(handle,streamTimeout)) {          openFileMap.remove(handle);          if (LOG.isDebugEnabled()) {
private void clearDirectory(String writeDumpDir) throws IOException {  File dumpDir=new File(writeDumpDir);  if (dumpDir.exists()) {
@VisibleForTesting GETATTR3Response getattr(XDR xdr,SecurityHandler securityHandler,SocketAddress remoteAddress){  GETATTR3Response response=new GETATTR3Response(Nfs3Status.NFS3_OK);  if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_ONLY)) {    response.setStatus(Nfs3Status.NFS3ERR_ACCES);    return response;  }  GETATTR3Request request;  try {    request=GETATTR3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid GETATTR request");    response.setStatus(Nfs3Status.NFS3ERR_INVAL);    return response;  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {
  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("GETATTR for fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  Nfs3FileAttributes attrs=null;  try {    attrs=writeManager.getFileAttr(dfsClient,handle,iug);  } catch (  RemoteException r) {    LOG.warn("Exception",r);    IOException io=r.unwrapRemoteException();    if (io instanceof AuthorizationException) {      return new GETATTR3Response(Nfs3Status.NFS3ERR_ACCES);
  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  Nfs3FileAttributes attrs=null;  try {    attrs=writeManager.getFileAttr(dfsClient,handle,iug);  } catch (  RemoteException r) {    LOG.warn("Exception",r);    IOException io=r.unwrapRemoteException();    if (io instanceof AuthorizationException) {      return new GETATTR3Response(Nfs3Status.NFS3ERR_ACCES);    } else {      return new GETATTR3Response(Nfs3Status.NFS3ERR_IO);    }  }catch (  IOException e) {    LOG.info("Can't get file attribute, fileId={}",handle.getFileId(),e);
private void setattrInternal(DFSClient dfsClient,String fileIdPath,SetAttr3 newAttr,boolean setMode) throws IOException {  EnumSet<SetAttrField> updateFields=newAttr.getUpdateFields();  if (setMode && updateFields.contains(SetAttrField.MODE)) {
  try {    request=SETATTR3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid SETATTR request");    response.setStatus(Nfs3Status.NFS3ERR_INVAL);    return response;  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS SETATTR fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (request.getAttr().getUpdateFields().contains(SetAttrField.SIZE)) {
  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS SETATTR fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (request.getAttr().getUpdateFields().contains(SetAttrField.SIZE)) {    LOG.error("Setting file size is not supported when setattr, fileId: {}",handle.getFileId());    response.setStatus(Nfs3Status.NFS3ERR_INVAL);    return response;  }  String fileIdPath=Nfs3Utils.getFileIdPath(handle);  Nfs3FileAttributes preOpAttr=null;  try {
      LOG.info("Can't get path for fileId: {}",handle.getFileId());      response.setStatus(Nfs3Status.NFS3ERR_STALE);      return response;    }    WccAttr preOpWcc=Nfs3Utils.getWccAttr(preOpAttr);    if (request.isCheck()) {      if (!preOpAttr.getCtime().equals(request.getCtime())) {        WccData wccData=new WccData(preOpWcc,preOpAttr);        return new SETATTR3Response(Nfs3Status.NFS3ERR_NOT_SYNC,wccData);      }    }    if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_WRITE)) {      return new SETATTR3Response(Nfs3Status.NFS3ERR_ACCES,new WccData(preOpWcc,preOpAttr));    }    setattrInternal(dfsClient,fileIdPath,request.getAttr(),true);    Nfs3FileAttributes postOpAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);    WccData wccData=new WccData(preOpWcc,postOpAttr);    return new SETATTR3Response(Nfs3Status.NFS3_OK,wccData);  } catch (  IOException e) {
@VisibleForTesting LOOKUP3Response lookup(XDR xdr,SecurityHandler securityHandler,SocketAddress remoteAddress){  LOOKUP3Response response=new LOOKUP3Response(Nfs3Status.NFS3_OK);  if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_ONLY)) {    response.setStatus(Nfs3Status.NFS3ERR_ACCES);    return response;  }  LOOKUP3Request request;  try {    request=LOOKUP3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid LOOKUP request");    return new LOOKUP3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle dirHandle=request.getHandle();  String fileName=request.getName();  int namenodeId=dirHandle.getNamenodeId();  if (LOG.isDebugEnabled()) {
  } catch (  IOException e) {    LOG.error("Invalid LOOKUP request");    return new LOOKUP3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle dirHandle=request.getHandle();  String fileName=request.getName();  int namenodeId=dirHandle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS LOOKUP dir fileHandle: {} name: {} client: {}",dirHandle.dumpFileHandle(),fileName,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  try {    String dirFileIdPath=Nfs3Utils.getFileIdPath(dirHandle);    Nfs3FileAttributes postOpObjAttr=writeManager.getFileAttr(dfsClient,dirHandle,fileName,namenodeId);
  FileHandle dirHandle=request.getHandle();  String fileName=request.getName();  int namenodeId=dirHandle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS LOOKUP dir fileHandle: {} name: {} client: {}",dirHandle.dumpFileHandle(),fileName,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  try {    String dirFileIdPath=Nfs3Utils.getFileIdPath(dirHandle);    Nfs3FileAttributes postOpObjAttr=writeManager.getFileAttr(dfsClient,dirHandle,fileName,namenodeId);    if (postOpObjAttr == null) {      LOG.debug("NFS LOOKUP fileId: {} name: {} does not exist",dirHandle.getFileId(),fileName);      Nfs3FileAttributes postOpDirAttr=Nfs3Utils.getFileAttr(dfsClient,dirFileIdPath,iug);
    response.setStatus(Nfs3Status.NFS3ERR_ACCES);    return response;  }  ACCESS3Request request;  try {    request=ACCESS3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid ACCESS request");    return new ACCESS3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (LOG.isDebugEnabled()) {
    request=ACCESS3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid ACCESS request");    return new ACCESS3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (LOG.isDebugEnabled()) {    LOG.debug("NFS ACCESS fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  Nfs3FileAttributes attrs;  try {    attrs=writeManager.getFileAttr(dfsClient,handle,iug);
  } catch (  IOException e) {    LOG.error("Invalid READLINK request");    return new READLINK3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READLINK fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  String fileIdPath=Nfs3Utils.getFileIdPath(handle);  try {    String target=dfsClient.getLinkTarget(fileIdPath);    Nfs3FileAttributes postOpAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);
    return new READLINK3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READLINK fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  String fileIdPath=Nfs3Utils.getFileIdPath(handle);  try {    String target=dfsClient.getLinkTarget(fileIdPath);    Nfs3FileAttributes postOpAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);    if (postOpAttr == null) {      LOG.info("Can't get path for fileId: {}",handle.getFileId());
  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READLINK fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  String fileIdPath=Nfs3Utils.getFileIdPath(handle);  try {    String target=dfsClient.getLinkTarget(fileIdPath);    Nfs3FileAttributes postOpAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);    if (postOpAttr == null) {      LOG.info("Can't get path for fileId: {}",handle.getFileId());      return new READLINK3Response(Nfs3Status.NFS3ERR_STALE);    }    if (postOpAttr.getType() != NfsFileType.NFSLNK.toValue()) {
  final String userName=securityHandler.getUser();  if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_ONLY)) {    response.setStatus(Nfs3Status.NFS3ERR_ACCES);    return response;  }  READ3Request request;  try {    request=READ3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid READ request");    return new READ3Response(Nfs3Status.NFS3ERR_INVAL);  }  long offset=request.getOffset();  int count=request.getCount();  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {
    return new READ3Response(Nfs3Status.NFS3ERR_INVAL);  }  long offset=request.getOffset();  int count=request.getCount();  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READ fileHandle: {} offset: {} count: {} client: {}",handle.dumpFileHandle(),offset,count,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(userName,namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  Nfs3FileAttributes attrs;  boolean eof;  if (count == 0) {    try {
  int count=request.getCount();  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READ fileHandle: {} offset: {} count: {} client: {}",handle.dumpFileHandle(),offset,count,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(userName,namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  Nfs3FileAttributes attrs;  boolean eof;  if (count == 0) {    try {      attrs=Nfs3Utils.getFileAttr(dfsClient,Nfs3Utils.getFileIdPath(handle),iug);    } catch (    IOException e) {
      return new READ3Response(Nfs3Status.NFS3ERR_ACCES);    }  }  int ret=writeManager.commitBeforeRead(dfsClient,handle,offset + count);  if (ret != Nfs3Status.NFS3_OK) {    LOG.warn("commitBeforeRead didn't succeed with ret={}. " + "Read may not get most recent data.",ret);  }  try {    int rtmax=config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);    int buffSize=Math.min(rtmax,count);    byte[] readbuffer=new byte[buffSize];    int readCount=0;    for (int i=0; i < 1; ++i) {      FSDataInputStream fis=clientCache.getDfsInputStream(userName,Nfs3Utils.getFileIdPath(handle),namenodeId);      if (fis == null) {        return new READ3Response(Nfs3Status.NFS3ERR_ACCES);      }      try {        readCount=fis.read(offset,readbuffer,0,count);
  try {    request=WRITE3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid WRITE request");    return new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);  }  long offset=request.getOffset();  int count=request.getCount();  WriteStableHow stableHow=request.getStableHow();  byte[] data=request.getData().array();  if (data.length < count) {    LOG.error("Invalid argument, data size is less than count in request");    return new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {
  WriteStableHow stableHow=request.getStableHow();  byte[] data=request.getData().array();  if (data.length < count) {    LOG.error("Invalid argument, data size is less than count in request");    return new WRITE3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS WRITE fileHandle: {} offset: {} length: {} " + "stableHow: {} xid: {} client: {}",handle.dumpFileHandle(),offset,count,stableHow.getValue(),xid,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  Nfs3FileAttributes preOpAttr=null;  try {
  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS WRITE fileHandle: {} offset: {} length: {} " + "stableHow: {} xid: {} client: {}",handle.dumpFileHandle(),offset,count,stableHow.getValue(),xid,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  Nfs3FileAttributes preOpAttr=null;  try {    preOpAttr=writeManager.getFileAttr(dfsClient,handle,iug);    if (preOpAttr == null) {      LOG.error("Can't get path for fileId: {}",handle.getFileId());      return new WRITE3Response(Nfs3Status.NFS3ERR_STALE);    }    if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_WRITE)) {
  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS WRITE fileHandle: {} offset: {} length: {} " + "stableHow: {} xid: {} client: {}",handle.dumpFileHandle(),offset,count,stableHow.getValue(),xid,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  Nfs3FileAttributes preOpAttr=null;  try {    preOpAttr=writeManager.getFileAttr(dfsClient,handle,iug);    if (preOpAttr == null) {      LOG.error("Can't get path for fileId: {}",handle.getFileId());      return new WRITE3Response(Nfs3Status.NFS3ERR_STALE);    }    if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_WRITE)) {      return new WRITE3Response(Nfs3Status.NFS3ERR_ACCES,new WccData(Nfs3Utils.getWccAttr(preOpAttr),preOpAttr),0,stableHow,Nfs3Constant.WRITE_COMMIT_VERF);
    request=CREATE3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid CREATE request");    return new CREATE3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle dirHandle=request.getHandle();  String fileName=request.getName();  int namenodeId=dirHandle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS CREATE dir fileHandle: {} filename: {} client: {}",dirHandle.dumpFileHandle(),fileName,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  int createMode=request.getMode();  if ((createMode != Nfs3Constant.CREATE_EXCLUSIVE) && request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE) && request.getObjAttr().getSize() != 0) {
  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  int createMode=request.getMode();  if ((createMode != Nfs3Constant.CREATE_EXCLUSIVE) && request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE) && request.getObjAttr().getSize() != 0) {    LOG.error("Setting file size is not supported when creating file: {} " + "dir fileId: {}",fileName,dirHandle.getFileId());    return new CREATE3Response(Nfs3Status.NFS3ERR_INVAL);  }  HdfsDataOutputStream fos=null;  String dirFileIdPath=Nfs3Utils.getFileIdPath(dirHandle);  Nfs3FileAttributes preOpDirAttr=null;  Nfs3FileAttributes postOpObjAttr=null;  FileHandle fileHandle=null;  WccData dirWcc=null;  try {
      return new CREATE3Response(Nfs3Status.NFS3ERR_ACCES,null,preOpDirAttr,new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),preOpDirAttr));    }    String fileIdPath=Nfs3Utils.getFileIdPath(dirHandle) + "/" + fileName;    SetAttr3 setAttr3=request.getObjAttr();    assert(setAttr3 != null);    FsPermission permission=setAttr3.getUpdateFields().contains(SetAttrField.MODE) ? new FsPermission((short)setAttr3.getMode()) : FsPermission.getDefault().applyUMask(umask);    EnumSet<CreateFlag> flag=(createMode != Nfs3Constant.CREATE_EXCLUSIVE) ? EnumSet.of(CreateFlag.CREATE,CreateFlag.OVERWRITE) : EnumSet.of(CreateFlag.CREATE);    fos=dfsClient.createWrappedOutputStream(dfsClient.create(fileIdPath,permission,flag,false,replication,blockSize,null,bufferSize,null),null);    if ((createMode == Nfs3Constant.CREATE_UNCHECKED) || (createMode == Nfs3Constant.CREATE_GUARDED)) {      if (!setAttr3.getUpdateFields().contains(SetAttrField.GID)) {        setAttr3.getUpdateFields().add(SetAttrField.GID);        setAttr3.setGid(securityHandler.getGid());      }      setattrInternal(dfsClient,fileIdPath,setAttr3,false);    }    postOpObjAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);    dirWcc=Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr),dfsClient,dirFileIdPath,iug);    OpenFileCtx openFileCtx=new OpenFileCtx(fos,postOpObjAttr,writeDumpDir + "/" + postOpObjAttr.getFileId(),dfsClient,iug,aixCompatMode,config);
  MKDIR3Response response=new MKDIR3Response(Nfs3Status.NFS3_OK);  MKDIR3Request request;  try {    request=MKDIR3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid MKDIR request");    return new MKDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle dirHandle=request.getHandle();  String fileName=request.getName();  int namenodeId=dirHandle.getNamenodeId();  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (LOG.isDebugEnabled()) {
  try {    request=MKDIR3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid MKDIR request");    return new MKDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle dirHandle=request.getHandle();  String fileName=request.getName();  int namenodeId=dirHandle.getNamenodeId();  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (LOG.isDebugEnabled()) {    LOG.debug("NFS MKDIR dirHandle: {} filename: {} client: {}",dirHandle.dumpFileHandle(),fileName,remoteAddress);  }  if (request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE)) {
  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (LOG.isDebugEnabled()) {    LOG.debug("NFS MKDIR dirHandle: {} filename: {} client: {}",dirHandle.dumpFileHandle(),fileName,remoteAddress);  }  if (request.getObjAttr().getUpdateFields().contains(SetAttrField.SIZE)) {    LOG.error("Setting file size is not supported when mkdir: " + "{} in dirHandle {}",fileName,dirHandle);    return new MKDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  String dirFileIdPath=Nfs3Utils.getFileIdPath(dirHandle);  Nfs3FileAttributes preOpDirAttr=null;  Nfs3FileAttributes postOpDirAttr=null;  Nfs3FileAttributes postOpObjAttr=null;  FileHandle objFileHandle=null;  try {
      return new MKDIR3Response(Nfs3Status.NFS3ERR_ACCES,null,preOpDirAttr,new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),preOpDirAttr));    }    final String fileIdPath=dirFileIdPath + "/" + fileName;    SetAttr3 setAttr3=request.getObjAttr();    FsPermission permission=setAttr3.getUpdateFields().contains(SetAttrField.MODE) ? new FsPermission((short)setAttr3.getMode()) : FsPermission.getDefault().applyUMask(umask);    if (!dfsClient.mkdirs(fileIdPath,permission,false)) {      WccData dirWcc=Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr),dfsClient,dirFileIdPath,iug);      return new MKDIR3Response(Nfs3Status.NFS3ERR_IO,null,null,dirWcc);    }    if (!setAttr3.getUpdateFields().contains(SetAttrField.GID)) {      setAttr3.getUpdateFields().add(SetAttrField.GID);      setAttr3.setGid(securityHandler.getGid());    }    setattrInternal(dfsClient,fileIdPath,setAttr3,false);    postOpObjAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);    objFileHandle=new FileHandle(postOpObjAttr.getFileId(),namenodeId);    WccData dirWcc=Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr),dfsClient,dirFileIdPath,iug);    return new MKDIR3Response(Nfs3Status.NFS3_OK,new FileHandle(postOpObjAttr.getFileId(),namenodeId),postOpObjAttr,dirWcc);
    return new REMOVE3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle dirHandle=request.getHandle();  int namenodeId=dirHandle.getNamenodeId();  String fileName=request.getName();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS REMOVE dir fileHandle: {} fileName: {} client: {}",dirHandle.dumpFileHandle(),fileName,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  String dirFileIdPath=Nfs3Utils.getFileIdPath(dirHandle);  Nfs3FileAttributes preOpDirAttr=null;  Nfs3FileAttributes postOpDirAttr=null;  try {    preOpDirAttr=Nfs3Utils.getFileAttr(dfsClient,dirFileIdPath,iug);
    WccData errWcc=new WccData(Nfs3Utils.getWccAttr(preOpDirAttr),preOpDirAttr);    if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_WRITE)) {      return new REMOVE3Response(Nfs3Status.NFS3ERR_ACCES,errWcc);    }    String fileIdPath=dirFileIdPath + "/" + fileName;    HdfsFileStatus fstat=Nfs3Utils.getFileStatus(dfsClient,fileIdPath);    if (fstat == null) {      return new REMOVE3Response(Nfs3Status.NFS3ERR_NOENT,errWcc);    }    if (fstat.isDirectory()) {      return new REMOVE3Response(Nfs3Status.NFS3ERR_ISDIR,errWcc);    }    boolean result=dfsClient.delete(fileIdPath,false);    WccData dirWcc=Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr),dfsClient,dirFileIdPath,iug);    if (!result) {      return new REMOVE3Response(Nfs3Status.NFS3ERR_ACCES,dirWcc);    }    return new REMOVE3Response(Nfs3Status.NFS3_OK,dirWcc);  } catch (  IOException e) {
    return new RMDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle dirHandle=request.getHandle();  String fileName=request.getName();  int namenodeId=dirHandle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS RMDIR dir fileHandle: {} fileName: {} client: {}",dirHandle.dumpFileHandle(),fileName,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  String dirFileIdPath=Nfs3Utils.getFileIdPath(dirHandle);  Nfs3FileAttributes preOpDirAttr=null;  Nfs3FileAttributes postOpDirAttr=null;  try {    preOpDirAttr=Nfs3Utils.getFileAttr(dfsClient,dirFileIdPath,iug);
      return new RMDIR3Response(Nfs3Status.NFS3ERR_ACCES,errWcc);    }    String fileIdPath=dirFileIdPath + "/" + fileName;    HdfsFileStatus fstat=Nfs3Utils.getFileStatus(dfsClient,fileIdPath);    if (fstat == null) {      return new RMDIR3Response(Nfs3Status.NFS3ERR_NOENT,errWcc);    }    if (!fstat.isDirectory()) {      return new RMDIR3Response(Nfs3Status.NFS3ERR_NOTDIR,errWcc);    }    if (fstat.getChildrenNum() > 0) {      return new RMDIR3Response(Nfs3Status.NFS3ERR_NOTEMPTY,errWcc);    }    boolean result=dfsClient.delete(fileIdPath,false);    WccData dirWcc=Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(preOpDirAttr),dfsClient,dirFileIdPath,iug);    if (!result) {      return new RMDIR3Response(Nfs3Status.NFS3ERR_ACCES,dirWcc);    }    return new RMDIR3Response(Nfs3Status.NFS3_OK,dirWcc);  } catch (  IOException e) {
@VisibleForTesting RENAME3Response rename(XDR xdr,SecurityHandler securityHandler,SocketAddress remoteAddress){  RENAME3Response response=new RENAME3Response(Nfs3Status.NFS3_OK);  RENAME3Request request=null;  try {    request=RENAME3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid RENAME request");    return new RENAME3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle fromHandle=request.getFromDirHandle();  int fromNamenodeId=fromHandle.getNamenodeId();  String fromName=request.getFromName();  FileHandle toHandle=request.getToDirHandle();  int toNamenodeId=toHandle.getNamenodeId();  String toName=request.getToName();  if (LOG.isDebugEnabled()) {
    LOG.debug("NFS RENAME from: {}/{} to: {}/{} client: {}",fromHandle.dumpFileHandle(),fromName,toHandle.dumpFileHandle(),toName,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),fromNamenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (fromNamenodeId != toNamenodeId) {    response.setStatus(Nfs3Status.NFS3ERR_INVAL);    return response;  }  String fromDirFileIdPath=Nfs3Utils.getFileIdPath(fromHandle);  String toDirFileIdPath=Nfs3Utils.getFileIdPath(toHandle);  Nfs3FileAttributes fromPreOpAttr=null;  Nfs3FileAttributes toPreOpAttr=null;  WccData fromDirWcc=null;  WccData toDirWcc=null;  try {
    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  if (fromNamenodeId != toNamenodeId) {    response.setStatus(Nfs3Status.NFS3ERR_INVAL);    return response;  }  String fromDirFileIdPath=Nfs3Utils.getFileIdPath(fromHandle);  String toDirFileIdPath=Nfs3Utils.getFileIdPath(toHandle);  Nfs3FileAttributes fromPreOpAttr=null;  Nfs3FileAttributes toPreOpAttr=null;  WccData fromDirWcc=null;  WccData toDirWcc=null;  try {    fromPreOpAttr=Nfs3Utils.getFileAttr(dfsClient,fromDirFileIdPath,iug);    if (fromPreOpAttr == null) {      LOG.info("Can't get path for fromHandle fileId: {}",fromHandle.getFileId());
    }    toPreOpAttr=Nfs3Utils.getFileAttr(dfsClient,toDirFileIdPath,iug);    if (toPreOpAttr == null) {      LOG.info("Can't get path for toHandle fileId: {}",toHandle.getFileId());      return new RENAME3Response(Nfs3Status.NFS3ERR_STALE);    }    if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_WRITE)) {      WccData fromWcc=new WccData(Nfs3Utils.getWccAttr(fromPreOpAttr),fromPreOpAttr);      WccData toWcc=new WccData(Nfs3Utils.getWccAttr(toPreOpAttr),toPreOpAttr);      return new RENAME3Response(Nfs3Status.NFS3ERR_ACCES,fromWcc,toWcc);    }    String src=fromDirFileIdPath + "/" + fromName;    String dst=toDirFileIdPath + "/" + toName;    dfsClient.rename(src,dst,Options.Rename.NONE);    fromDirWcc=Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(fromPreOpAttr),dfsClient,fromDirFileIdPath,iug);    toDirWcc=Nfs3Utils.createWccData(Nfs3Utils.getWccAttr(toPreOpAttr),dfsClient,toDirFileIdPath,iug);    return new RENAME3Response(Nfs3Status.NFS3_OK,fromDirWcc,toDirWcc);  } catch (  IOException e) {
    response.setStatus(Nfs3Status.NFS3ERR_ACCES);    return response;  }  SYMLINK3Request request;  try {    request=SYMLINK3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid SYMLINK request");    response.setStatus(Nfs3Status.NFS3ERR_INVAL);    return response;  }  FileHandle dirHandle=request.getHandle();  String name=request.getName();  String symData=request.getSymData();  String linkDirIdPath=Nfs3Utils.getFileIdPath(dirHandle);  int namenodeId=dirHandle.getNamenodeId();  String linkIdPath=linkDirIdPath + "/" + name;
public READDIR3Response readdir(XDR xdr,SecurityHandler securityHandler,SocketAddress remoteAddress){  READDIR3Response response=new READDIR3Response(Nfs3Status.NFS3_OK);  if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_ONLY)) {    response.setStatus(Nfs3Status.NFS3ERR_ACCES);    return response;  }  READDIR3Request request;  try {    request=READDIR3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid READDIR request");    return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  long cookie=request.getCookie();  if (cookie < 0) {
    return response;  }  READDIR3Request request;  try {    request=READDIR3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid READDIR request");    return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  long cookie=request.getCookie();  if (cookie < 0) {    LOG.error("Invalid READDIR request, with negative cookie: {}",cookie);    return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  long count=request.getCount();  if (count <= 0) {
  try {    request=READDIR3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid READDIR request");    return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  long cookie=request.getCookie();  if (cookie < 0) {    LOG.error("Invalid READDIR request, with negative cookie: {}",cookie);    return new READDIR3Response(Nfs3Status.NFS3ERR_INVAL);  }  long count=request.getCount();  if (count <= 0) {    LOG.info("Nonpositive count in invalid READDIR request: {}",count);    return new READDIR3Response(Nfs3Status.NFS3_OK);
  }  long count=request.getCount();  if (count <= 0) {    LOG.info("Nonpositive count in invalid READDIR request: {}",count);    return new READDIR3Response(Nfs3Status.NFS3_OK);  }  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READDIR fileHandle: {} cookie: {} count: {} client: {}",handle.dumpFileHandle(),cookie,count,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  HdfsFileStatus dirStatus;  DirectoryListing dlisting;  Nfs3FileAttributes postOpAttr;  long dotdotFileId=0;  try {
    LOG.info("Nonpositive count in invalid READDIR request: {}",count);    return new READDIR3Response(Nfs3Status.NFS3_OK);  }  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READDIR fileHandle: {} cookie: {} count: {} client: {}",handle.dumpFileHandle(),cookie,count,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  HdfsFileStatus dirStatus;  DirectoryListing dlisting;  Nfs3FileAttributes postOpAttr;  long dotdotFileId=0;  try {    String dirFileIdPath=Nfs3Utils.getFileIdPath(handle);    dirStatus=dfsClient.getFileInfo(dirFileIdPath);
  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  HdfsFileStatus dirStatus;  DirectoryListing dlisting;  Nfs3FileAttributes postOpAttr;  long dotdotFileId=0;  try {    String dirFileIdPath=Nfs3Utils.getFileIdPath(handle);    dirStatus=dfsClient.getFileInfo(dirFileIdPath);    if (dirStatus == null) {      LOG.info("Can't get path for fileId: {}",handle.getFileId());      return new READDIR3Response(Nfs3Status.NFS3ERR_STALE);    }    if (!dirStatus.isDirectory()) {      LOG.error("Can't readdir for regular file, fileId: {}",handle.getFileId());
    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_ACCES);  }  READDIRPLUS3Request request=null;  try {    request=READDIRPLUS3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid READDIRPLUS request");    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  long cookie=request.getCookie();  if (cookie < 0) {    LOG.error("Invalid READDIRPLUS request, with negative cookie: {}",cookie);    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  long dirCount=request.getDirCount();  if (dirCount <= 0) {
    request=READDIRPLUS3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid READDIRPLUS request");    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  long cookie=request.getCookie();  if (cookie < 0) {    LOG.error("Invalid READDIRPLUS request, with negative cookie: {}",cookie);    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  long dirCount=request.getDirCount();  if (dirCount <= 0) {    LOG.info("Nonpositive dircount in invalid READDIRPLUS request: {}",dirCount);    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  int maxCount=request.getMaxCount();
    LOG.error("Invalid READDIRPLUS request");    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  long cookie=request.getCookie();  if (cookie < 0) {    LOG.error("Invalid READDIRPLUS request, with negative cookie: {}",cookie);    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  long dirCount=request.getDirCount();  if (dirCount <= 0) {    LOG.info("Nonpositive dircount in invalid READDIRPLUS request: {}",dirCount);    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  int maxCount=request.getMaxCount();  if (maxCount <= 0) {    LOG.info("Nonpositive maxcount in invalid READDIRPLUS request: {}",maxCount);
    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  int maxCount=request.getMaxCount();  if (maxCount <= 0) {    LOG.info("Nonpositive maxcount in invalid READDIRPLUS request: {}",maxCount);    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READDIRPLUS fileHandle: {} cookie: {} dirCount: {} " + "maxCount: {} client: {}",handle.dumpFileHandle(),cookie,dirCount,maxCount,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_SERVERFAULT);  }  HdfsFileStatus dirStatus;  DirectoryListing dlisting;  Nfs3FileAttributes postOpDirAttr;  long dotdotFileId=0;  HdfsFileStatus dotdotStatus=null;
  if (maxCount <= 0) {    LOG.info("Nonpositive maxcount in invalid READDIRPLUS request: {}",maxCount);    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_INVAL);  }  if (LOG.isDebugEnabled()) {    LOG.debug("NFS READDIRPLUS fileHandle: {} cookie: {} dirCount: {} " + "maxCount: {} client: {}",handle.dumpFileHandle(),cookie,dirCount,maxCount,remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_SERVERFAULT);  }  HdfsFileStatus dirStatus;  DirectoryListing dlisting;  Nfs3FileAttributes postOpDirAttr;  long dotdotFileId=0;  HdfsFileStatus dotdotStatus=null;  try {    String dirFileIdPath=Nfs3Utils.getFileIdPath(handle);
  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_SERVERFAULT);  }  HdfsFileStatus dirStatus;  DirectoryListing dlisting;  Nfs3FileAttributes postOpDirAttr;  long dotdotFileId=0;  HdfsFileStatus dotdotStatus=null;  try {    String dirFileIdPath=Nfs3Utils.getFileIdPath(handle);    dirStatus=dfsClient.getFileInfo(dirFileIdPath);    if (dirStatus == null) {      LOG.info("Can't get path for fileId: {}",handle.getFileId());      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_STALE);    }    if (!dirStatus.isDirectory()) {
      LOG.error("Can't readdirplus for regular file, fileId: {}",handle.getFileId());      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_NOTDIR);    }    long cookieVerf=request.getCookieVerf();    if ((cookieVerf != 0) && (cookieVerf != dirStatus.getModificationTime())) {      if (aixCompatMode) {        LOG.warn("AIX compatibility mode enabled, ignoring cookieverf " + "mismatches.");      } else {        LOG.error("cookieverf mismatch. request cookieverf: {} " + "dir cookieverf: {}",cookieVerf,dirStatus.getModificationTime());        return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_BAD_COOKIE,Nfs3Utils.getFileAttr(dfsClient,dirFileIdPath,iug),0,null);      }    }    if (cookie == 0) {      String dotdotFileIdPath=dirFileIdPath + "/..";      dotdotStatus=dfsClient.getFileInfo(dotdotFileIdPath);      if (dotdotStatus == null) {        throw new IOException("Can't get path for handle path: " + dotdotFileIdPath);      }      dotdotFileId=dotdotStatus.getFileId();
    byte[] startAfter;    if (cookie == 0) {      startAfter=HdfsFileStatus.EMPTY_NAME;    } else {      String inodeIdPath=Nfs3Utils.getFileIdPath(cookie);      startAfter=inodeIdPath.getBytes(Charset.forName("UTF-8"));    }    dlisting=listPaths(dfsClient,dirFileIdPath,startAfter);    postOpDirAttr=Nfs3Utils.getFileAttr(dfsClient,dirFileIdPath,iug);    if (postOpDirAttr == null) {      LOG.info("Can't get path for fileId: {}",handle.getFileId());      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_STALE);    }  } catch (  IOException e) {    LOG.warn("Exception",e);    int status=mapErrorStatus(e);    return new READDIRPLUS3Response(status);
      return new READDIRPLUS3Response(Nfs3Status.NFS3ERR_STALE);    }  } catch (  IOException e) {    LOG.warn("Exception",e);    int status=mapErrorStatus(e);    return new READDIRPLUS3Response(status);  }  HdfsFileStatus[] fstatus=dlisting.getPartialListing();  int n=(int)Math.min(fstatus.length,dirCount - 2);  boolean eof=(n >= fstatus.length) && !dlisting.hasMore();  READDIRPLUS3Response.EntryPlus3[] entries;  if (cookie == 0) {    entries=new READDIRPLUS3Response.EntryPlus3[n + 2];    entries[0]=new READDIRPLUS3Response.EntryPlus3(postOpDirAttr.getFileId(),".",0,postOpDirAttr,new FileHandle(postOpDirAttr.getFileId(),namenodeId));    entries[1]=new READDIRPLUS3Response.EntryPlus3(dotdotFileId,"..",dotdotFileId,Nfs3Utils.getNfs3FileAttrFromFileStatus(dotdotStatus,iug),new FileHandle(dotdotFileId,namenodeId));    for (int i=2; i < n + 2; i++) {      long fileId=fstatus[i - 2].getFileId();
 catch (  IOException e) {    LOG.error("Invalid FSSTAT request");    return new FSSTAT3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS FSSTAT fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  try {    FsStatus fsStatus=dfsClient.getDiskStatus();    long totalBytes=fsStatus.getCapacity();    long freeBytes=fsStatus.getRemaining();
 catch (  IOException e) {    LOG.error("Invalid FSINFO request");    return new FSINFO3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS FSINFO fileHandle: {} client: {}",remoteAddress,handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  try {    int rtmax=config.getInt(NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY,NfsConfigKeys.DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT);    int wtmax=config.getInt(NfsConfigKeys.DFS_NFS_MAX_WRITE_TRANSFER_SIZE_KEY,NfsConfigKeys.DFS_NFS_MAX_WRITE_TRANSFER_SIZE_DEFAULT);    int dtperf=config.getInt(NfsConfigKeys.DFS_NFS_MAX_READDIR_TRANSFER_SIZE_KEY,NfsConfigKeys.DFS_NFS_MAX_READDIR_TRANSFER_SIZE_DEFAULT);
@VisibleForTesting PATHCONF3Response pathconf(XDR xdr,SecurityHandler securityHandler,SocketAddress remoteAddress){  PATHCONF3Response response=new PATHCONF3Response(Nfs3Status.NFS3_OK);  if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_ONLY)) {    response.setStatus(Nfs3Status.NFS3ERR_ACCES);    return response;  }  PATHCONF3Request request;  try {    request=PATHCONF3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid PATHCONF request");    return new PATHCONF3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  Nfs3FileAttributes attrs;  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {
    request=PATHCONF3Request.deserialize(xdr);  } catch (  IOException e) {    LOG.error("Invalid PATHCONF request");    return new PATHCONF3Response(Nfs3Status.NFS3ERR_INVAL);  }  FileHandle handle=request.getHandle();  Nfs3FileAttributes attrs;  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS PATHCONF fileHandle: {} client: {}",handle.dumpFileHandle(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  try {    attrs=Nfs3Utils.getFileAttr(dfsClient,Nfs3Utils.getFileIdPath(handle),iug);
    LOG.error("Invalid COMMIT request");    response.setStatus(Nfs3Status.NFS3ERR_INVAL);    return response;  }  FileHandle handle=request.getHandle();  int namenodeId=handle.getNamenodeId();  if (LOG.isDebugEnabled()) {    LOG.debug("NFS COMMIT fileHandle: {} offset={} count={} client: {}",handle.dumpFileHandle(),request.getOffset(),request.getCount(),remoteAddress);  }  DFSClient dfsClient=clientCache.getDfsClient(securityHandler.getUser(),namenodeId);  if (dfsClient == null) {    response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);    return response;  }  String fileIdPath=Nfs3Utils.getFileIdPath(handle);  Nfs3FileAttributes preOpAttr=null;  try {    preOpAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);
  }  String fileIdPath=Nfs3Utils.getFileIdPath(handle);  Nfs3FileAttributes preOpAttr=null;  try {    preOpAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);    if (preOpAttr == null) {      LOG.info("Can't get path for fileId: {}",handle.getFileId());      return new COMMIT3Response(Nfs3Status.NFS3ERR_STALE);    }    if (!checkAccessPrivilege(remoteAddress,AccessPrivilege.READ_WRITE)) {      return new COMMIT3Response(Nfs3Status.NFS3ERR_ACCES,new WccData(Nfs3Utils.getWccAttr(preOpAttr),preOpAttr),Nfs3Constant.WRITE_COMMIT_VERF);    }    long commitOffset=(request.getCount() == 0) ? 0 : (request.getOffset() + request.getCount());    writeManager.handleCommit(dfsClient,handle,commitOffset,channel,xid,preOpAttr,namenodeId);    return null;  } catch (  IOException e) {    LOG.warn("Exception",e);    Nfs3FileAttributes postOpAttr=null;
@Override public void handleInternal(ChannelHandlerContext ctx,RpcInfo info){  RpcCall rpcCall=(RpcCall)info.header();  final NFSPROC3 nfsproc3=NFSPROC3.fromValue(rpcCall.getProcedure());  int xid=rpcCall.getXid();  byte[] data=new byte[info.data().readableBytes()];  info.data().readBytes(data);  XDR xdr=new XDR(data);  XDR out=new XDR();  InetAddress client=((InetSocketAddress)info.remoteAddress()).getAddress();  Credentials credentials=rpcCall.getCredential();  if (nfsproc3 != NFSPROC3.NULL) {    if (credentials.getFlavor() != AuthFlavor.AUTH_SYS && credentials.getFlavor() != AuthFlavor.RPCSEC_GSS) {
  Credentials credentials=rpcCall.getCredential();  if (nfsproc3 != NFSPROC3.NULL) {    if (credentials.getFlavor() != AuthFlavor.AUTH_SYS && credentials.getFlavor() != AuthFlavor.RPCSEC_GSS) {      LOG.info("Wrong RPC AUTH flavor, {} is not AUTH_SYS or RPCSEC_GSS.",credentials.getFlavor());      XDR reply=new XDR();      RpcDeniedReply rdr=new RpcDeniedReply(xid,RpcReply.ReplyState.MSG_ACCEPTED,RpcDeniedReply.RejectState.AUTH_ERROR,new VerifierNone());      rdr.write(reply);      ChannelBuffer buf=ChannelBuffers.wrappedBuffer(reply.asReadOnlyWrap().buffer());      RpcResponse rsp=new RpcResponse(buf,info.remoteAddress());      RpcUtil.sendRpcResponse(ctx,rsp);      return;    }  }  if (!isIdempotent(rpcCall)) {    RpcCallCache.CacheEntry entry=rpcCallCache.checkOrAddToCache(client,xid);    if (entry != null) {      if (entry.isCompleted()) {
      XDR reply=new XDR();      RpcDeniedReply rdr=new RpcDeniedReply(xid,RpcReply.ReplyState.MSG_ACCEPTED,RpcDeniedReply.RejectState.AUTH_ERROR,new VerifierNone());      rdr.write(reply);      ChannelBuffer buf=ChannelBuffers.wrappedBuffer(reply.asReadOnlyWrap().buffer());      RpcResponse rsp=new RpcResponse(buf,info.remoteAddress());      RpcUtil.sendRpcResponse(ctx,rsp);      return;    }  }  if (!isIdempotent(rpcCall)) {    RpcCallCache.CacheEntry entry=rpcCallCache.checkOrAddToCache(client,xid);    if (entry != null) {      if (entry.isCompleted()) {        LOG.info("Sending the cached reply to retransmitted request {}",xid);        RpcUtil.sendRpcResponse(ctx,entry.getResponse());        return;      } else {
  }  final long startTime=System.nanoTime();  NFS3Response response=null;  if (nfsproc3 == NFSPROC3.NULL) {    response=nullProcedure();  } else   if (nfsproc3 == NFSPROC3.GETATTR) {    response=getattr(xdr,info);    metrics.addGetattr(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.SETATTR) {    response=setattr(xdr,info);    metrics.addSetattr(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.LOOKUP) {    response=lookup(xdr,info);    metrics.addLookup(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.ACCESS) {    response=access(xdr,info);
  NFS3Response response=null;  if (nfsproc3 == NFSPROC3.NULL) {    response=nullProcedure();  } else   if (nfsproc3 == NFSPROC3.GETATTR) {    response=getattr(xdr,info);    metrics.addGetattr(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.SETATTR) {    response=setattr(xdr,info);    metrics.addSetattr(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.LOOKUP) {    response=lookup(xdr,info);    metrics.addLookup(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.ACCESS) {    response=access(xdr,info);    metrics.addAccess(Nfs3Utils.getElapsedTime(startTime));
  } else   if (nfsproc3 == NFSPROC3.GETATTR) {    response=getattr(xdr,info);    metrics.addGetattr(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.SETATTR) {    response=setattr(xdr,info);    metrics.addSetattr(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.LOOKUP) {    response=lookup(xdr,info);    metrics.addLookup(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.ACCESS) {    response=access(xdr,info);    metrics.addAccess(Nfs3Utils.getElapsedTime(startTime));  } else   if (nfsproc3 == NFSPROC3.READLINK) {    response=readlink(xdr,info);    metrics.addReadlink(Nfs3Utils.getElapsedTime(startTime));
public void trimWrite(int delta){  Preconditions.checkState(delta < count);  if (LOG.isDebugEnabled()) {
  OpenFileCtx openFileCtx=fileContextCache.get(fileHandle);  if (openFileCtx == null) {    LOG.info("No opened stream for fileHandle: " + fileHandle.dumpFileHandle());    String fileIdPath=Nfs3Utils.getFileIdPath(fileHandle.getFileId());    HdfsDataOutputStream fos=null;    Nfs3FileAttributes latestAttr=null;    try {      int bufferSize=config.getInt(CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY,CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_DEFAULT);      fos=dfsClient.append(fileIdPath,bufferSize,EnumSet.of(CreateFlag.APPEND),null,null);      latestAttr=Nfs3Utils.getFileAttr(dfsClient,fileIdPath,iug);    } catch (    RemoteException e) {      IOException io=e.unwrapRemoteException();      if (io instanceof AlreadyBeingCreatedException) {        LOG.warn("Can't append file: " + fileIdPath + ". Possibly the file is being closed. Drop the request: "+ request+ ", wait for the client to retry...");        return;
        LOG.warn("Can't append file: " + fileIdPath + ". Possibly the file is being closed. Drop the request: "+ request+ ", wait for the client to retry...");        return;      }      throw e;    }catch (    IOException e) {      LOG.error("Can't append to file: " + fileIdPath,e);      if (fos != null) {        fos.close();      }      WccData fileWcc=new WccData(Nfs3Utils.getWccAttr(preOpAttr),preOpAttr);      WRITE3Response response=new WRITE3Response(Nfs3Status.NFS3ERR_IO,fileWcc,count,request.getStableHow(),Nfs3Constant.WRITE_COMMIT_VERF);      Nfs3Utils.writeChannel(channel,response.serialize(new XDR(),xid,new VerifierNone()),xid);      return;    }    String writeDumpDir=config.get(NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_KEY,NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_DEFAULT);    openFileCtx=new OpenFileCtx(fos,latestAttr,writeDumpDir + "/" + fileHandle.getFileId(),dfsClient,iug,aixCompatMode,config);    if (!addOpenFileStream(fileHandle,openFileCtx)) {      LOG.info("Can't add new stream. Close it. Tell client to retry.");
      LOG.error("Can't append to file: " + fileIdPath,e);      if (fos != null) {        fos.close();      }      WccData fileWcc=new WccData(Nfs3Utils.getWccAttr(preOpAttr),preOpAttr);      WRITE3Response response=new WRITE3Response(Nfs3Status.NFS3ERR_IO,fileWcc,count,request.getStableHow(),Nfs3Constant.WRITE_COMMIT_VERF);      Nfs3Utils.writeChannel(channel,response.serialize(new XDR(),xid,new VerifierNone()),xid);      return;    }    String writeDumpDir=config.get(NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_KEY,NfsConfigKeys.DFS_NFS_FILE_DUMP_DIR_DEFAULT);    openFileCtx=new OpenFileCtx(fos,latestAttr,writeDumpDir + "/" + fileHandle.getFileId(),dfsClient,iug,aixCompatMode,config);    if (!addOpenFileStream(fileHandle,openFileCtx)) {      LOG.info("Can't add new stream. Close it. Tell client to retry.");      try {        fos.close();      } catch (      IOException e) {        LOG.error("Can't close stream for fileHandle: " + handle.dumpFileHandle(),e);
int commitBeforeRead(DFSClient dfsClient,FileHandle fileHandle,long commitOffset){  int status;  OpenFileCtx openFileCtx=fileContextCache.get(fileHandle);  if (openFileCtx == null) {    if (LOG.isDebugEnabled()) {
void handleCommit(DFSClient dfsClient,FileHandle fileHandle,long commitOffset,Channel channel,int xid,Nfs3FileAttributes preOpAttr,int namenodeId){  long startTime=System.nanoTime();  int status;  OpenFileCtx openFileCtx=fileContextCache.get(fileHandle);  if (openFileCtx == null) {
switch (ret) {case COMMIT_FINISHED:case COMMIT_INACTIVE_CTX:      status=Nfs3Status.NFS3_OK;    break;case COMMIT_INACTIVE_WITH_PENDING_WRITE:case COMMIT_ERROR:  status=Nfs3Status.NFS3ERR_IO;break;case COMMIT_WAIT:return;case COMMIT_SPECIAL_WAIT:status=Nfs3Status.NFS3ERR_JUKEBOX;break;case COMMIT_SPECIAL_SUCCESS:status=Nfs3Status.NFS3_OK;break;default:LOG.error("Should not get commit return code: " + ret.name());throw new RuntimeException("Should not get commit return code: " + ret.name());}}Nfs3FileAttributes postOpAttr=null;try {postOpAttr=getFileAttr(dfsClient,new FileHandle(preOpAttr.getFileId(),namenodeId),iug);
@Override public void init(Configuration configuration,RouterRpcServer rpcServer,StateStoreService stateStore){  this.conf=configuration;  this.server=rpcServer;  this.store=stateStore;  this.metrics=FederationRPCMetrics.create(conf,server);  ThreadFactory threadFactory=new ThreadFactoryBuilder().setNameFormat("Federation RPC Performance Monitor-%d").build();  this.executor=Executors.newFixedThreadPool(1,threadFactory);  try {    StandardMBean bean=new StandardMBean(this.metrics,FederationRPCMBean.class);    registeredBean=MBeans.register("Router","FederationRPC",bean);
      innerinfo.put("adminState",node.getAdminState().toString());      innerinfo.put("nonDfsUsedSpace",node.getNonDfsUsed());      innerinfo.put("capacity",node.getCapacity());      innerinfo.put("numBlocks",-1);      innerinfo.put("version",(node.getSoftwareVersion() == null ? "UNKNOWN" : node.getSoftwareVersion()));      innerinfo.put("used",node.getDfsUsed());      innerinfo.put("remaining",node.getRemaining());      innerinfo.put("blockScheduled",-1);      innerinfo.put("blockPoolUsed",node.getBlockPoolUsed());      innerinfo.put("blockPoolUsedPercent",node.getBlockPoolUsedPercent());      innerinfo.put("volfails",-1);      info.put(node.getHostName() + ":" + node.getXferPort(),Collections.unmodifiableMap(innerinfo));    }  } catch (  StandbyException e) {    LOG.error("Cannot get {} nodes, Router in safe mode",type);  }catch (  SubClusterTimeoutException e) {
      innerinfo.put("capacity",node.getCapacity());      innerinfo.put("numBlocks",-1);      innerinfo.put("version",(node.getSoftwareVersion() == null ? "UNKNOWN" : node.getSoftwareVersion()));      innerinfo.put("used",node.getDfsUsed());      innerinfo.put("remaining",node.getRemaining());      innerinfo.put("blockScheduled",-1);      innerinfo.put("blockPoolUsed",node.getBlockPoolUsed());      innerinfo.put("blockPoolUsedPercent",node.getBlockPoolUsedPercent());      innerinfo.put("volfails",-1);      info.put(node.getHostName() + ":" + node.getXferPort(),Collections.unmodifiableMap(innerinfo));    }  } catch (  StandbyException e) {    LOG.error("Cannot get {} nodes, Router in safe mode",type);  }catch (  SubClusterTimeoutException e) {    LOG.error("Cannot get {} nodes, subclusters timed out responding",type);  }catch (  IOException e) {
    final List<MembershipState> namenodes=getActiveNamenodeRegistrations();    List<MembershipState> namenodesOrder=new ArrayList<>(namenodes);    Collections.sort(namenodesOrder,MembershipState.NAME_COMPARATOR);    for (    MembershipState namenode : namenodesOrder) {      Map<String,Object> innerInfo=new HashMap<>();      Map<String,Object> map=getJson(namenode);      innerInfo.putAll(map);      long dateModified=namenode.getDateModified();      long lastHeartbeat=getSecondsSince(dateModified);      innerInfo.put("lastHeartbeat",lastHeartbeat);      MembershipStats stats=namenode.getStats();      long used=stats.getTotalSpace() - stats.getAvailableSpace();      innerInfo.put("used",used);      info.put(namenode.getNamenodeKey(),Collections.unmodifiableMap(innerInfo));    }  } catch (  IOException e) {
      float totalDfsUsed=0;      float[] usages=new float[live.length];      int i=0;      for (      DatanodeInfo dn : live) {        usages[i++]=dn.getDfsUsedPercent();        totalDfsUsed+=dn.getDfsUsedPercent();      }      totalDfsUsed/=live.length;      Arrays.sort(usages);      median=usages[usages.length / 2];      max=usages[usages.length - 1];      min=usages[0];      for (i=0; i < usages.length; i++) {        dev+=(usages[i] - totalDfsUsed) * (usages[i] - totalDfsUsed);      }      dev=(float)Math.sqrt(dev / usages.length);    }  } catch (  IOException e) {
    MembershipState partial=MembershipState.newInstance();    String rpcAddress=address.getHostName() + ":" + address.getPort();    partial.setRpcAddress(rpcAddress);    partial.setNameserviceId(nsId);    GetNamenodeRegistrationsRequest request=GetNamenodeRegistrationsRequest.newInstance(partial);    MembershipStore membership=getMembershipStore();    GetNamenodeRegistrationsResponse response=membership.getNamenodeRegistrations(request);    List<MembershipState> records=response.getNamenodeMemberships();    if (records != null && records.size() == 1) {      MembershipState record=records.get(0);      UpdateNamenodeRegistrationRequest updateRequest=UpdateNamenodeRegistrationRequest.newInstance(record.getNameserviceId(),record.getNamenodeId(),ACTIVE);      membership.updateNamenodeRegistration(updateRequest);      cacheNS.remove(nsId);      cacheBP.clear();    }  } catch (  StateStoreUnavailableException e) {
@Override public List<? extends FederationNamenodeContext> getNamenodesForBlockPoolId(final String bpId) throws IOException {  List<? extends FederationNamenodeContext> ret=cacheBP.get(bpId);  if (ret == null) {    try {      MembershipState partial=MembershipState.newInstance();      partial.setBlockPoolId(bpId);      GetNamenodeRegistrationsRequest request=GetNamenodeRegistrationsRequest.newInstance(partial);      final List<MembershipState> result=getRecentRegistrationForQuery(request,true,false);      if (result == null || result.isEmpty()) {
private List<MembershipState> getRecentRegistrationForQuery(GetNamenodeRegistrationsRequest request,boolean addUnavailable,boolean addExpired) throws IOException {  MembershipStore membershipStore=getMembershipStore();  GetNamenodeRegistrationsResponse response=membershipStore.getNamenodeRegistrations(request);  List<MembershipState> memberships=response.getNamenodeMemberships();  if (!addExpired || !addUnavailable) {    Iterator<MembershipState> iterator=memberships.iterator();    while (iterator.hasNext()) {      MembershipState membership=iterator.next();      if (membership.getState() == EXPIRED && !addExpired) {        iterator.remove();      } else       if (membership.getState() == UNAVAILABLE && !addUnavailable) {        iterator.remove();      }    }  }  List<MembershipState> priorityList=new ArrayList<>();  priorityList.addAll(memberships);  Collections.sort(priorityList,new NamenodePriorityComparator());
private void invalidateLocationCache(final String path){
  }  ConcurrentMap<String,PathLocation> map=locationCache.asMap();  Set<Entry<String,PathLocation>> entries=map.entrySet();  Iterator<Entry<String,PathLocation>> it=entries.iterator();  while (it.hasNext()) {    Entry<String,PathLocation> entry=it.next();    String key=entry.getKey();    PathLocation loc=entry.getValue();    String src=loc.getSourcePath();    if (src != null) {      if (isParentEntry(key,path)) {        LOG.debug("Removing {}",src);        it.remove();      }    } else {      String dest=loc.getDefaultLocation().getDest();      if (dest.startsWith(path)) {
@VisibleForTesting public void refreshEntries(final Collection<MountTable> entries){  writeLock.lock();  try {    Map<String,MountTable> newEntries=new ConcurrentHashMap<>();    for (    MountTable entry : entries) {      String srcPath=entry.getSourcePath();      newEntries.put(srcPath,entry);    }    Set<String> oldEntries=new TreeSet<>(Collections.reverseOrder());    for (    MountTable entry : getTreeValues("/")) {      String srcPath=entry.getSourcePath();      oldEntries.add(srcPath);    }    for (    String srcPath : oldEntries) {      if (!newEntries.containsKey(srcPath)) {        this.tree.remove(srcPath);        invalidateLocationCache(srcPath);
      newEntries.put(srcPath,entry);    }    Set<String> oldEntries=new TreeSet<>(Collections.reverseOrder());    for (    MountTable entry : getTreeValues("/")) {      String srcPath=entry.getSourcePath();      oldEntries.add(srcPath);    }    for (    String srcPath : oldEntries) {      if (!newEntries.containsKey(srcPath)) {        this.tree.remove(srcPath);        invalidateLocationCache(srcPath);        LOG.info("Removed stale mount point {} from resolver",srcPath);      }    }    for (    MountTable entry : entries) {      String srcPath=entry.getSourcePath();      if (!oldEntries.contains(srcPath)) {        this.tree.put(srcPath,entry);        invalidateLocationCache(srcPath);
      String srcPath=entry.getSourcePath();      oldEntries.add(srcPath);    }    for (    String srcPath : oldEntries) {      if (!newEntries.containsKey(srcPath)) {        this.tree.remove(srcPath);        invalidateLocationCache(srcPath);        LOG.info("Removed stale mount point {} from resolver",srcPath);      }    }    for (    MountTable entry : entries) {      String srcPath=entry.getSourcePath();      if (!oldEntries.contains(srcPath)) {        this.tree.put(srcPath,entry);        invalidateLocationCache(srcPath);        LOG.info("Added new mount point {} to resolver",srcPath);      } else {        MountTable existingEntry=this.tree.get(srcPath);
    for (    String srcPath : oldEntries) {      if (!newEntries.containsKey(srcPath)) {        this.tree.remove(srcPath);        invalidateLocationCache(srcPath);        LOG.info("Removed stale mount point {} from resolver",srcPath);      }    }    for (    MountTable entry : entries) {      String srcPath=entry.getSourcePath();      if (!oldEntries.contains(srcPath)) {        this.tree.put(srcPath,entry);        invalidateLocationCache(srcPath);        LOG.info("Added new mount point {} to resolver",srcPath);      } else {        MountTable existingEntry=this.tree.get(srcPath);        if (existingEntry != null && !existingEntry.equals(entry)) {          LOG.info("Entry has changed from \"{}\" to \"{}\"",existingEntry,entry);
private PathLocation buildLocation(final String path,final MountTable entry) throws IOException {  String srcPath=entry.getSourcePath();  if (!path.startsWith(srcPath)) {
@Override public PathLocation getDestinationForPath(String path) throws IOException {  PathLocation mountTableResult=super.getDestinationForPath(path);  if (mountTableResult == null) {
@Override public PathLocation getDestinationForPath(String path) throws IOException {  PathLocation mountTableResult=super.getDestinationForPath(path);  if (mountTableResult == null) {    LOG.error("The {} cannot find a location for {}",super.getClass().getSimpleName(),path);  } else   if (mountTableResult.hasMultipleDestinations()) {    DestinationOrder order=mountTableResult.getDestinationOrder();    OrderedResolver orderedResolver=orderedResolvers.get(order);    if (orderedResolver == null) {      LOG.error("Cannot find resolver for order {}",order);    } else {      String firstNamespace=orderedResolver.getFirstNamespace(path,mountTableResult);      if (firstNamespace != null) {        mountTableResult=new PathLocation(mountTableResult,firstNamespace);        LOG.debug("Ordered locations following {} are {}",order,mountTableResult);      } else {
@Override public String getFirstNamespace(final String path,final PathLocation loc){  String srcPath=loc.getSourcePath();  String trimmedPath=trimPathToChild(path,srcPath);
@Override public String getFirstNamespace(final String path,final PathLocation loc){  String finalPath=extractTempFileName(path);  Set<String> namespaces=loc.getNamespaces();  ConsistentHashRing locator=getHashResolver(namespaces);  String hashedSubcluster=locator.getLocation(finalPath);  if (hashedSubcluster == null) {    String srcPath=loc.getSourcePath();
@Override protected String chooseFirstNamespace(String path,PathLocation loc){  String localSubcluster=null;  String clientAddr=getClientAddr();  Map<String,String> subclusterInfo=getSubclusterMapping();  if (subclusterInfo != null) {    localSubcluster=subclusterInfo.get(clientAddr);    if (localSubcluster != null) {
    Map<String,DatanodeStorageReport[]> dnMap=loginUser.doAs(new PrivilegedAction<Map<String,DatanodeStorageReport[]>>(){      @Override public Map<String,DatanodeStorageReport[]> run(){        try {          return rpcServer.getDatanodeStorageReportMap(DatanodeReportType.ALL);        } catch (        IOException e) {          LOG.error("Cannot get the datanodes from the RPC server",e);          return null;        }      }    });    for (    Entry<String,DatanodeStorageReport[]> entry : dnMap.entrySet()) {      String nsId=entry.getKey();      DatanodeStorageReport[] dns=entry.getValue();      for (      DatanodeStorageReport dn : dns) {        DatanodeInfo dnInfo=dn.getDatanodeInfo();        String ipAddr=dnInfo.getIpAddr();        ret.put(ipAddr,nsId);
  Map<String,String> ret=new HashMap<>();  try {    GetNamenodeRegistrationsRequest request=GetNamenodeRegistrationsRequest.newInstance();    GetNamenodeRegistrationsResponse response=membershipStore.getNamenodeRegistrations(request);    final List<MembershipState> nns=response.getNamenodeMemberships();    for (    MembershipState nn : nns) {      try {        String nsId=nn.getNameserviceId();        String rpcAddress=nn.getRpcAddress();        String hostname=HostAndPort.fromString(rpcAddress).getHost();        ret.put(hostname,nsId);        if (hostname.equals(localHostname)) {          ret.put(localIp,nsId);        }        InetAddress addr=InetAddress.getByName(hostname);        String ipAddr=addr.getHostAddress();
public String getFirstNamespace(final String path,final PathLocation loc){  final Set<String> namespaces=(loc == null) ? null : loc.getNamespaces();  if (CollectionUtils.isEmpty(namespaces)) {
public ConnectionContext getConnection(UserGroupInformation ugi,String nnAddress,Class<?> protocol) throws IOException {  if (!this.running) {
  }  finally {    readLock.unlock();  }  if (pool == null) {    writeLock.lock();    try {      pool=this.pools.get(connectionId);      if (pool == null) {        pool=new ConnectionPool(this.conf,nnAddress,ugi,this.minSize,this.maxSize,this.minActiveRatio,protocol);        this.pools.put(connectionId,pool);      }    }  finally {      writeLock.unlock();    }  }  ConnectionContext conn=pool.getConnection();  if (conn == null || !conn.isUsable()) {    if (!this.creatorQueue.offer(pool)) {      LOG.error("Cannot add more than {} connections at the same time",this.creatorQueueMaxSize);
protected synchronized void close(){  long timeSinceLastActive=TimeUnit.MILLISECONDS.toSeconds(Time.now() - getLastActiveTime());
protected static <T>ConnectionContext newConnection(Configuration conf,String nnAddress,UserGroupInformation ugi,Class<T> proto) throws IOException {  if (!PROTO_MAP.containsKey(proto)) {    String msg="Unsupported protocol for connection to NameNode: " + ((proto != null) ? proto.getName() : "null");
    URL jmxURL=new URL(scheme,host,port,"/jmx?qry=" + beanQuery);    LOG.debug("JMX URL: {}",jmxURL);    URLConnection conn=connectionFactory.openConnection(jmxURL,UserGroupInformation.isSecurityEnabled());    conn.setConnectTimeout(5 * 1000);    conn.setReadTimeout(5 * 1000);    InputStream in=conn.getInputStream();    InputStreamReader isr=new InputStreamReader(in,"UTF-8");    reader=new BufferedReader(isr);    StringBuilder sb=new StringBuilder();    String line=null;    while ((line=reader.readLine()) != null) {      sb.append(line);    }    String jmxOutput=sb.toString();    JSONObject json=new JSONObject(jmxOutput);    ret=json.getJSONArray("beans");
    URLConnection conn=connectionFactory.openConnection(jmxURL,UserGroupInformation.isSecurityEnabled());    conn.setConnectTimeout(5 * 1000);    conn.setReadTimeout(5 * 1000);    InputStream in=conn.getInputStream();    InputStreamReader isr=new InputStreamReader(in,"UTF-8");    reader=new BufferedReader(isr);    StringBuilder sb=new StringBuilder();    String line=null;    while ((line=reader.readLine()) != null) {      sb.append(line);    }    String jmxOutput=sb.toString();    JSONObject json=new JSONObject(jmxOutput);    ret=json.getJSONArray("beans");  } catch (  IOException e) {    LOG.error("Cannot read JMX bean {} from server {}",beanQuery,webAddress,e);
    conn.setReadTimeout(5 * 1000);    InputStream in=conn.getInputStream();    InputStreamReader isr=new InputStreamReader(in,"UTF-8");    reader=new BufferedReader(isr);    StringBuilder sb=new StringBuilder();    String line=null;    while ((line=reader.readLine()) != null) {      sb.append(line);    }    String jmxOutput=sb.toString();    JSONObject json=new JSONObject(jmxOutput);    ret=json.getJSONArray("beans");  } catch (  IOException e) {    LOG.error("Cannot read JMX bean {} from server {}",beanQuery,webAddress,e);  }catch (  JSONException e) {    LOG.error("Cannot parse JMX output for {} from server {}: {}",beanQuery,webAddress,e.getMessage());
    while ((line=reader.readLine()) != null) {      sb.append(line);    }    String jmxOutput=sb.toString();    JSONObject json=new JSONObject(jmxOutput);    ret=json.getJSONArray("beans");  } catch (  IOException e) {    LOG.error("Cannot read JMX bean {} from server {}",beanQuery,webAddress,e);  }catch (  JSONException e) {    LOG.error("Cannot parse JMX output for {} from server {}: {}",beanQuery,webAddress,e.getMessage());  }catch (  Exception e) {    LOG.error("Cannot parse JMX output for {} from server {}",beanQuery,webAddress,e);  } finally {    if (reader != null) {      try {        reader.close();
    nnDesc+="-" + namenodeId;  } else {    this.localTarget=null;  }  this.rpcAddress=getRpcAddress(conf,nameserviceId,namenodeId);  LOG.info("{} RPC address: {}",nnDesc,rpcAddress);  this.serviceAddress=DFSUtil.getNamenodeServiceAddr(conf,nameserviceId,namenodeId);  if (this.serviceAddress == null) {    LOG.error("Cannot locate RPC service address for NN {}, " + "using RPC address {}",nnDesc,this.rpcAddress);    this.serviceAddress=this.rpcAddress;  }  LOG.info("{} Service RPC address: {}",nnDesc,serviceAddress);  this.lifelineAddress=DFSUtil.getNamenodeLifelineAddr(conf,nameserviceId,namenodeId);  if (this.lifelineAddress == null) {    this.lifelineAddress=this.serviceAddress;  }  LOG.info("{} Lifeline RPC address: {}",nnDesc,lifelineAddress);  this.webAddress=DFSUtil.getNamenodeWebAddr(conf,nameserviceId,namenodeId);
  NamenodeStatusReport report=getNamenodeStatusReport();  if (!report.registrationValid()) {    LOG.error("Namenode is not operational: {}",getNamenodeDesc());  } else   if (report.haStateValid()) {    LOG.debug("Received service state: {} from HA namenode: {}",report.getState(),getNamenodeDesc());  } else   if (localTarget == null) {    LOG.debug("Reporting non-HA namenode as operational: " + getNamenodeDesc());  } else {    return;  }  try {    if (!resolver.registerNamenode(report)) {      LOG.warn("Cannot register namenode {}",report);    }  } catch (  IOException e) {    LOG.info("Cannot register namenode in the State Store");  }catch (  Exception ex) {
protected NamenodeStatusReport getNamenodeStatusReport(){  NamenodeStatusReport report=new NamenodeStatusReport(nameserviceId,namenodeId,rpcAddress,serviceAddress,lifelineAddress,scheme,webAddress);  try {
    LOG.debug("Probing NN at service address: {}",serviceAddress);    URI serviceURI=new URI("hdfs://" + serviceAddress);    NamenodeProtocol nn=NameNodeProxies.createProxy(this.conf,serviceURI,NamenodeProtocol.class).getProxy();    if (nn != null) {      NamespaceInfo info=nn.versionRequest();      if (info != null) {        report.setNamespaceInfo(info);      }    }    if (!report.registrationValid()) {      return report;    }    try {      ClientProtocol client=NameNodeProxies.createProxy(this.conf,serviceURI,ClientProtocol.class).getProxy();      if (client != null) {        boolean isSafeMode=client.setSafeMode(SafeModeAction.SAFEMODE_GET,false);        report.setSafeMode(isSafeMode);      }    } catch (    Exception e) {
      ClientProtocol client=NameNodeProxies.createProxy(this.conf,serviceURI,ClientProtocol.class).getProxy();      if (client != null) {        boolean isSafeMode=client.setSafeMode(SafeModeAction.SAFEMODE_GET,false);        report.setSafeMode(isSafeMode);      }    } catch (    Exception e) {      LOG.error("Cannot fetch safemode state for {}",getNamenodeDesc(),e);    }    updateJMXParameters(webAddress,report);    if (localTarget != null) {      try {        if (localTargetHAProtocol == null) {          localTargetHAProtocol=localTarget.getProxy(conf,30 * 1000);        }        HAServiceStatus status=localTargetHAProtocol.getServiceStatus();        report.setHAServiceState(status.getState());      } catch (      Throwable e) {        if (e.getMessage().startsWith("HA for namenode is not enabled")) {
        report.setSafeMode(isSafeMode);      }    } catch (    Exception e) {      LOG.error("Cannot fetch safemode state for {}",getNamenodeDesc(),e);    }    updateJMXParameters(webAddress,report);    if (localTarget != null) {      try {        if (localTargetHAProtocol == null) {          localTargetHAProtocol=localTarget.getProxy(conf,30 * 1000);        }        HAServiceStatus status=localTargetHAProtocol.getServiceStatus();        report.setHAServiceState(status.getState());      } catch (      Throwable e) {        if (e.getMessage().startsWith("HA for namenode is not enabled")) {          LOG.error("HA for {} is not enabled",getNamenodeDesc());          localTarget=null;        } else {
    } catch (    Exception e) {      LOG.error("Cannot fetch safemode state for {}",getNamenodeDesc(),e);    }    updateJMXParameters(webAddress,report);    if (localTarget != null) {      try {        if (localTargetHAProtocol == null) {          localTargetHAProtocol=localTarget.getProxy(conf,30 * 1000);        }        HAServiceStatus status=localTargetHAProtocol.getServiceStatus();        report.setHAServiceState(status.getState());      } catch (      Throwable e) {        if (e.getMessage().startsWith("HA for namenode is not enabled")) {          LOG.error("HA for {} is not enabled",getNamenodeDesc());          localTarget=null;        } else {          LOG.error("Cannot fetch HA status for {}: {}",getNamenodeDesc(),e.getMessage(),e);
    RemoteLocation loc=entry.getKey();    QuotaUsage usage=entry.getValue();    if (isMountEntry) {      nsCount+=usage.getFileAndDirectoryCount();      ssCount+=usage.getSpaceConsumed();      eachByStorageType(t -> typeCount[t.ordinal()]+=usage.getTypeConsumed(t));    } else     if (usage != null) {      if (!RouterQuotaManager.isQuotaSet(usage)) {        hasQuotaUnset=true;      }      nsQuota=usage.getQuota();      ssQuota=usage.getSpaceQuota();      eachByStorageType(t -> typeQuota[t.ordinal()]=usage.getTypeQuota(t));      nsCount+=usage.getFileAndDirectoryCount();      ssCount+=usage.getSpaceConsumed();      eachByStorageType(t -> typeCount[t.ordinal()]+=usage.getTypeConsumed(t));
    NamenodeHeartbeatService localHeartbeatService=createLocalNamenodeHeartbeatService();    if (localHeartbeatService != null) {      String nnDesc=localHeartbeatService.getNamenodeDesc();      ret.put(nnDesc,localHeartbeatService);    }  }  Collection<String> namenodes=this.conf.getTrimmedStringCollection(RBFConfigKeys.DFS_ROUTER_MONITOR_NAMENODE);  for (  String namenode : namenodes) {    String[] namenodeSplit=namenode.split("\\.");    String nsId=null;    String nnId=null;    if (namenodeSplit.length == 2) {      nsId=namenodeSplit[0];      nnId=namenodeSplit[1];    } else     if (namenodeSplit.length == 1) {      nsId=namenode;    } else {
protected NamenodeHeartbeatService createLocalNamenodeHeartbeatService(){  String nsId=DFSUtil.getNamenodeNameServiceId(conf);  String nnId=null;  if (HAUtil.isHAEnabled(conf,nsId)) {    nnId=HAUtil.getNameNodeId(conf,nsId);    if (nnId == null) {
protected NamenodeHeartbeatService createNamenodeHeartbeatService(String nsId,String nnId){
@Override public DisableNameserviceResponse disableNameservice(DisableNameserviceRequest request) throws IOException {  checkSuperuserPrivilege();  String nsId=request.getNameServiceId();  boolean success=false;  if (namespaceExists(nsId)) {    success=getDisabledNameserviceStore().disableNameservice(nsId);    if (success) {
@Override public EnableNameserviceResponse enableNameservice(EnableNameserviceRequest request) throws IOException {  checkSuperuserPrivilege();  String nsId=request.getNameServiceId();  DisabledNameserviceStore store=getDisabledNameserviceStore();  Set<String> disabled=store.getDisabledNameservices();  boolean success=false;  if (disabled.contains(nsId)) {    success=store.enableNameservice(nsId);    if (success) {
@Override public HdfsFileStatus create(String src,FsPermission masked,String clientName,EnumSetWritable<CreateFlag> flag,boolean createParent,short replication,long blockSize,CryptoProtocolVersion[] supportedVersions,String ecPolicyName,String storagePolicy) throws IOException {  rpcServer.checkOperation(NameNode.OperationCategory.WRITE);  if (createParent && rpcServer.isPathAll(src)) {    int index=src.lastIndexOf(Path.SEPARATOR);    String parent=src.substring(0,index);
@Override public HdfsFileStatus create(String src,FsPermission masked,String clientName,EnumSetWritable<CreateFlag> flag,boolean createParent,short replication,long blockSize,CryptoProtocolVersion[] supportedVersions,String ecPolicyName,String storagePolicy) throws IOException {  rpcServer.checkOperation(NameNode.OperationCategory.WRITE);  if (createParent && rpcServer.isPathAll(src)) {    int index=src.lastIndexOf(Path.SEPARATOR);    String parent=src.substring(0,index);    LOG.debug("Creating {} requires creating parent {}",src,parent);    FsPermission parentPermissions=getParentPermission(masked);    boolean success=mkdirs(parent,parentPermissions,createParent);    if (!success) {
@Override public DirectoryListing getListing(String src,byte[] startAfter,boolean needLocation) throws IOException {  rpcServer.checkOperation(NameNode.OperationCategory.READ);  List<RemoteResult<RemoteLocation,DirectoryListing>> listings=getListingInt(src,startAfter,needLocation);  TreeMap<String,HdfsFileStatus> nnListing=new TreeMap<>();  int totalRemainingEntries=0;  int remainingEntries=0;  boolean namenodeListingExists=false;  String lastName=null;  if (listings != null) {    for (    RemoteResult<RemoteLocation,DirectoryListing> result : listings) {      if (result.hasException()) {        IOException ioe=result.getException();        if (ioe instanceof FileNotFoundException) {          RemoteLocation location=result.getLocation();
      if (ioe instanceof FileNotFoundException) {        notFoundException=(FileNotFoundException)ioe;      } else       if (!allowPartialList) {        throw ioe;      }    } else     if (result.getResult() != null) {      summaries.add(result.getResult());    }  }  final List<String> children=subclusterResolver.getMountPoints(path);  if (children != null) {    for (    String child : children) {      Path childPath=new Path(path,child);      try {        ContentSummary mountSummary=getContentSummary(childPath.toString());        if (mountSummary != null) {          summaries.add(mountSummary);        }      } catch (      Exception e) {
      MountTableResolver mountTable=(MountTableResolver)subclusterResolver;      MountTable entry=mountTable.getMountPoint(mName);      if (entry != null) {        permission=entry.getMode();        owner=entry.getOwnerName();        group=entry.getGroupName();        RemoteMethod method=new RemoteMethod("getFileInfo",new Class<?>[]{String.class},new RemoteParam());        HdfsFileStatus fInfo=getFileInfoAll(entry.getDestinations(),method,mountStatusTimeOut);        if (fInfo != null) {          permission=fInfo.getPermission();          owner=fInfo.getOwner();          group=fInfo.getGroup();          childrenNum=fInfo.getChildrenNum();          flags=DFSUtil.getFlags(fInfo.isEncrypted(),fInfo.isErasureCoded(),fInfo.isSnapshotEnabled(),fInfo.hasAcl());        }      }    } catch (    IOException e) {
          permission=fInfo.getPermission();          owner=fInfo.getOwner();          group=fInfo.getGroup();          childrenNum=fInfo.getChildrenNum();          flags=DFSUtil.getFlags(fInfo.isEncrypted(),fInfo.isErasureCoded(),fInfo.isSnapshotEnabled(),fInfo.hasAcl());        }      }    } catch (    IOException e) {      LOG.error("Cannot get mount point: {}",e.getMessage());    }  } else {    try {      UserGroupInformation ugi=RouterRpcServer.getRemoteUser();      owner=ugi.getUserName();      group=ugi.getPrimaryGroupName();    } catch (    IOException e) {      String msg="Cannot get remote user: " + e.getMessage();      if (UserGroupInformation.isSecurityEnabled()) {
          group=fInfo.getGroup();          childrenNum=fInfo.getChildrenNum();          flags=DFSUtil.getFlags(fInfo.isEncrypted(),fInfo.isErasureCoded(),fInfo.isSnapshotEnabled(),fInfo.hasAcl());        }      }    } catch (    IOException e) {      LOG.error("Cannot get mount point: {}",e.getMessage());    }  } else {    try {      UserGroupInformation ugi=RouterRpcServer.getRemoteUser();      owner=ugi.getUserName();      group=ugi.getPrimaryGroupName();    } catch (    IOException e) {      String msg="Cannot get remote user: " + e.getMessage();      if (UserGroupInformation.isSecurityEnabled()) {        LOG.error(msg);      } else {
  if (path.equals(Path.SEPARATOR)) {    srcPath=Path.SEPARATOR + child;  } else {    srcPath=path + Path.SEPARATOR + child;  }  Long modTime=0L;  try {    MountTable entry=mountTable.getMountPoint(srcPath);    if (entry == null) {      List<MountTable> entries=mountTable.getMounts(srcPath);      for (      MountTable eachEntry : entries) {        if (ret.get(child) == null || ret.get(child) < eachEntry.getDateModified()) {          modTime=eachEntry.getDateModified();        }      }    } else {      modTime=entry.getDateModified();    }  } catch (  IOException e) {
public void fsck(){  final long startTime=Time.monotonicNow();  try {    String warnMsg="Now FSCK to DFSRouter is unstable feature. " + "There may be incompatible changes between releases.";    LOG.warn(warnMsg);    out.println(warnMsg);    String msg="Federated FSCK started by " + UserGroupInformation.getCurrentUser() + " from "+ remoteAddress+ " at "+ new Date();
    LOG.error("Cannot heartbeat for router: unknown router id");    return;  }  if (isStoreAvailable()) {    RouterStore routerStore=router.getRouterStateManager();    try {      RouterState record=RouterState.newInstance(routerId,router.getStartTime(),router.getRouterState());      StateStoreVersion stateStoreVersion=StateStoreVersion.newInstance(getStateStoreVersion(MembershipStore.class),getStateStoreVersion(MountTableStore.class));      record.setStateStoreVersion(stateStoreVersion);      String hostPort=StateStoreUtils.getHostPortString(router.getAdminServerAddress());      record.setAdminAddress(hostPort);      RouterHeartbeatRequest request=RouterHeartbeatRequest.newInstance(record);      RouterHeartbeatResponse response=routerStore.routerHeartbeat(request);      if (!response.getStatus()) {        LOG.warn("Cannot heartbeat router {}",routerId);      } else {
  }  if (isStoreAvailable()) {    RouterStore routerStore=router.getRouterStateManager();    try {      RouterState record=RouterState.newInstance(routerId,router.getStartTime(),router.getRouterState());      StateStoreVersion stateStoreVersion=StateStoreVersion.newInstance(getStateStoreVersion(MembershipStore.class),getStateStoreVersion(MountTableStore.class));      record.setStateStoreVersion(stateStoreVersion);      String hostPort=StateStoreUtils.getHostPortString(router.getAdminServerAddress());      record.setAdminAddress(hostPort);      RouterHeartbeatRequest request=RouterHeartbeatRequest.newInstance(record);      RouterHeartbeatResponse response=routerStore.routerHeartbeat(request);      if (!response.getStatus()) {        LOG.warn("Cannot heartbeat router {}",routerId);      } else {        LOG.debug("Router heartbeat for router {}",routerId);      }    } catch (    IOException e) {
      long ssQuota=oldQuota.getSpaceQuota();      long[] typeQuota=new long[StorageType.values().length];      Quota.eachByStorageType(t -> typeQuota[t.ordinal()]=oldQuota.getTypeQuota(t));      QuotaUsage currentQuotaUsage=null;      HdfsFileStatus ret=this.rpcServer.getFileInfo(src);      if (ret == null || ret.getModificationTime() == 0) {        long[] zeroConsume=new long[StorageType.values().length];        currentQuotaUsage=new RouterQuotaUsage.Builder().fileAndDirectoryCount(0).quota(nsQuota).spaceConsumed(0).spaceQuota(ssQuota).typeConsumed(zeroConsume).typeQuota(typeQuota).build();      } else {        try {          Quota quotaModule=this.rpcServer.getQuotaModule();          Map<RemoteLocation,QuotaUsage> usageMap=quotaModule.getEachQuotaUsage(src);          currentQuotaUsage=quotaModule.aggregateQuota(src,usageMap);          remoteQuotaUsage.putAll(usageMap);        } catch (        IOException ioe) {
private void fixGlobalQuota(RemoteLocation location,QuotaUsage remoteQuota) throws IOException {  QuotaUsage gQuota=this.rpcServer.getQuotaModule().getGlobalQuota(location.getSrc());  if (remoteQuota.getQuota() != gQuota.getQuota() || remoteQuota.getSpaceQuota() != gQuota.getSpaceQuota()) {    this.rpcServer.getQuotaModule().setQuotaInternal(location.getSrc(),Arrays.asList(location),gQuota.getQuota(),gQuota.getSpaceQuota(),null);
      ioes.put(namenode,ioe);      if (ioe instanceof StandbyException) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpFailureStandby();        }        failover=true;      } else       if (isUnavailableException(ioe)) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpFailureCommunicate();        }        failover=true;      } else       if (ioe instanceof RemoteException) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpComplete(true);        }        RemoteException re=(RemoteException)ioe;        ioe=re.unwrapRemoteException();        ioe=getCleanException(ioe);
 else       if (isUnavailableException(ioe)) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpFailureCommunicate();        }        failover=true;      } else       if (ioe instanceof RemoteException) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpComplete(true);        }        RemoteException re=(RemoteException)ioe;        ioe=re.unwrapRemoteException();        ioe=getCleanException(ioe);        throw ioe;      } else       if (ioe instanceof ConnectionNullException) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpFailureCommunicate();        }        LOG.error("Get connection for {} {} error: {}",nsId,rpcAddress,ioe.getMessage());
        StandbyException se=new StandbyException(ioe.getMessage());        se.initCause(ioe);        throw se;      } else       if (ioe instanceof NoNamenodesAvailableException) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpNoNamenodes();        }        LOG.error("Cannot get available namenode for {} {} error: {}",nsId,rpcAddress,ioe.getMessage());        throw new RetriableException(ioe);      } else {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpFailureCommunicate();          this.rpcMonitor.proxyOpComplete(false);        }        throw ioe;      }    } finally {      if (connection != null) {
      } else       if (ioe instanceof NoNamenodesAvailableException) {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpNoNamenodes();        }        LOG.error("Cannot get available namenode for {} {} error: {}",nsId,rpcAddress,ioe.getMessage());        throw new RetriableException(ioe);      } else {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpFailureCommunicate();          this.rpcMonitor.proxyOpComplete(false);        }        throw ioe;      }    } finally {      if (connection != null) {        connection.release();      }    }  }  if (this.rpcMonitor != null) {    this.rpcMonitor.proxyOpComplete(false);
        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpNoNamenodes();        }        LOG.error("Cannot get available namenode for {} {} error: {}",nsId,rpcAddress,ioe.getMessage());        throw new RetriableException(ioe);      } else {        if (this.rpcMonitor != null) {          this.rpcMonitor.proxyOpFailureCommunicate();          this.rpcMonitor.proxyOpComplete(false);        }        throw ioe;      }    } finally {      if (connection != null) {        connection.release();      }    }  }  if (this.rpcMonitor != null) {    this.rpcMonitor.proxyOpComplete(false);  }  String msg="No namenode available to invoke " + method.getName() + " "+ Arrays.deepToString(params)+ " in "+ namenodes+ " from "+ router.getRouterId();
        String methodName=matcher.group(2);        String fileName=matcher.group(3);        int lineNumber=Integer.parseInt(matcher.group(4));        StackTraceElement element=new StackTraceElement(declaringClass,methodName,fileName,lineNumber);        elements.add(element);      }    }    stackTrace=elements.toArray(new StackTraceElement[elements.size()]);  }  if (ioe instanceof RemoteException) {    RemoteException re=(RemoteException)ioe;    ret=new RemoteException(re.getClassName(),msg);  } else {    Class<? extends IOException> ioeClass=ioe.getClass();    try {      Constructor<? extends IOException> constructor=ioeClass.getDeclaredConstructor(String.class);      ret=constructor.newInstance(msg);    } catch (    ReflectiveOperationException e) {
    List<? extends FederationNamenodeContext> namenodes=getNamenodesForNameservice(ns);    try {      Class<?> proto=remoteMethod.getProtocol();      Object[] params=remoteMethod.getParams(loc);      Object result=invokeMethod(ugi,namenodes,proto,m,params);      if (isExpectedClass(expectedResultClass,result) && isExpectedValue(expectedResultValue,result)) {        @SuppressWarnings("unchecked") R location=(R)loc;        @SuppressWarnings("unchecked") T ret=(T)result;        return new RemoteResult<>(location,ret);      }      if (firstResult == null) {        firstResult=result;      }    } catch (    IOException ioe) {      ioe=processException(ioe,loc);      thrownExceptions.add(ioe);    }catch (    Exception e) {
  }  if (rpcMonitor != null) {    rpcMonitor.proxyOp();  }  try {    List<Future<Object>> futures=null;    if (timeOutMs > 0) {      futures=executorService.invokeAll(callables,timeOutMs,TimeUnit.MILLISECONDS);    } else {      futures=executorService.invokeAll(callables);    }    List<RemoteResult<T,R>> results=new ArrayList<>();    for (int i=0; i < futures.size(); i++) {      T location=orderedLocations.get(i);      try {        Future<Object> future=futures.get(i);        R result=(R)future.get();        results.add(new RemoteResult<>(location,result));
      T location=orderedLocations.get(i);      try {        Future<Object> future=futures.get(i);        R result=(R)future.get();        results.add(new RemoteResult<>(location,result));      } catch (      CancellationException ce) {        T loc=orderedLocations.get(i);        String msg="Invocation to \"" + loc + "\" for \""+ method.getMethodName()+ "\" timed out";        LOG.error(msg);        IOException ioe=new SubClusterTimeoutException(msg);        results.add(new RemoteResult<>(location,ioe));      }catch (      ExecutionException ex) {        Throwable cause=ex.getCause();        LOG.debug("Canot execute {} in {}: {}",m.getName(),location,cause.getMessage());        IOException ioe=null;
        R result=(R)future.get();        results.add(new RemoteResult<>(location,result));      } catch (      CancellationException ce) {        T loc=orderedLocations.get(i);        String msg="Invocation to \"" + loc + "\" for \""+ method.getMethodName()+ "\" timed out";        LOG.error(msg);        IOException ioe=new SubClusterTimeoutException(msg);        results.add(new RemoteResult<>(location,ioe));      }catch (      ExecutionException ex) {        Throwable cause=ex.getCause();        LOG.debug("Canot execute {} in {}: {}",m.getName(),location,cause.getMessage());        IOException ioe=null;        if (cause instanceof IOException) {          ioe=(IOException)cause;        } else {
private DatanodeInfo[] getCachedDatanodeReportImpl(final DatanodeReportType type) throws IOException {  UserGroupInformation loginUser=UserGroupInformation.getLoginUser();  RouterRpcServer.setCurrentUser(loginUser);  try {    DatanodeInfo[] dns=clientProto.getDatanodeReport(type);
  checkOperation(OperationCategory.UNCHECKED);  Map<String,DatanodeInfo> datanodesMap=new LinkedHashMap<>();  RemoteMethod method=new RemoteMethod("getDatanodeReport",new Class<?>[]{DatanodeReportType.class},type);  Set<FederationNamespaceInfo> nss=namenodeResolver.getNamespaces();  Map<FederationNamespaceInfo,DatanodeInfo[]> results=rpcClient.invokeConcurrent(nss,method,requireResponse,false,timeOutMs,DatanodeInfo[].class);  for (  Entry<FederationNamespaceInfo,DatanodeInfo[]> entry : results.entrySet()) {    FederationNamespaceInfo ns=entry.getKey();    DatanodeInfo[] result=entry.getValue();    for (    DatanodeInfo node : result) {      String nodeId=node.getXferAddr();      DatanodeInfo dn=datanodesMap.get(nodeId);      if (dn == null || node.getLastUpdate() > dn.getLastUpdate()) {        node.setNetworkLocation(NodeBase.PATH_SEPARATOR_STR + ns.getNameserviceId() + node.getNetworkLocation());        datanodesMap.put(nodeId,node);      } else {
private void leave(){  long timeInSafemode=now() - enterSafeModeTime;
@Override public void periodicInvoke(){  long now=Time.now();  long delta=now - startupTime;  if (delta < startupInterval) {
@Override public String[] getGroupsForUser(String user) throws IOException {
public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer) throws IOException {
public void cancelDelegationToken(Token<DelegationTokenIdentifier> token) throws IOException {  LOG.debug("Cancel delegation token");  final String operationName="cancelDelegationToken";  boolean success=false;  String tokenId="";  try {    String canceller=getRemoteUser().getUserName();
void logAuditEvent(boolean succeeded,String cmd,String tokenId) throws IOException {
    LOG.info("Watcher for tokens is disabled in this secret manager");    try {      checkAgainstZkBeforeDeletion.set(true);      if (zkClient.checkExists().forPath(ZK_DTSM_TOKENS_ROOT) == null) {        zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(ZK_DTSM_TOKENS_ROOT);      }      try {        zookeeper=zkClient.getZookeeperClient().getZooKeeper();      } catch (      Exception e) {        LOG.info("Cannot get zookeeper client ",e);      } finally {        if (zookeeper == null) {          throw new IOException("Zookeeper client is null");        }      }      LOG.info("Start loading token cache");      long start=Time.now();      rebuildTokenCache(true);
 catch (      Exception e) {        LOG.info("Cannot get zookeeper client ",e);      } finally {        if (zookeeper == null) {          throw new IOException("Zookeeper client is null");        }      }      LOG.info("Start loading token cache");      long start=Time.now();      rebuildTokenCache(true);      LOG.info("Loaded token cache in {} milliseconds",Time.now() - start);      int syncInterval=conf.getInt(ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL,ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL_DEFAULT);      scheduler.scheduleAtFixedRate(new Runnable(){        @Override public void run(){          try {            rebuildTokenCache(false);          } catch (          Exception e) {
  List<R> newRecords=query.getRecords();  long currentDriverTime=query.getTimestamp();  if (newRecords == null || currentDriverTime <= 0) {    LOG.error("Cannot check overrides for record");    return;  }  for (  R record : newRecords) {    if (record.shouldBeDeleted(currentDriverTime)) {      String recordName=StateStoreUtils.getRecordName(record.getClass());      if (getDriver().remove(record)) {        deleteRecords.add(record);        LOG.info("Deleted State Store record {}: {}",recordName,record);      } else {        LOG.warn("Couldn't delete State Store record {}: {}",recordName,record);      }    } else     if (record.checkExpired(currentDriverTime)) {      String recordName=StateStoreUtils.getRecordName(record.getClass());
  this.addService(monitorService);  MembershipState.setExpirationMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS,RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS_DEFAULT,TimeUnit.MILLISECONDS));  MembershipState.setDeletionMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_DELETION_MS,RBFConfigKeys.FEDERATION_STORE_MEMBERSHIP_EXPIRATION_DELETION_MS_DEFAULT,TimeUnit.MILLISECONDS));  RouterState.setExpirationMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_MS,RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_MS_DEFAULT,TimeUnit.MILLISECONDS));  RouterState.setDeletionMs(conf.getTimeDuration(RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_DELETION_MS,RBFConfigKeys.FEDERATION_STORE_ROUTER_EXPIRATION_DELETION_MS_DEFAULT,TimeUnit.MILLISECONDS));  this.cacheUpdater=new StateStoreCacheUpdateService(this);  addService(this.cacheUpdater);  if (conf.getBoolean(RBFConfigKeys.DFS_ROUTER_METRICS_ENABLE,RBFConfigKeys.DFS_ROUTER_METRICS_ENABLE_DEFAULT)) {    this.metrics=StateStoreMetrics.create(conf);    try {      StandardMBean bean=new StandardMBean(metrics,StateStoreMBean.class);      ObjectName registeredObject=MBeans.register("Router","StateStore",bean);      LOG.info("Registered StateStoreMBean: {}",registeredObject);    } catch (    NotCompliantMBeanException e) {      throw new RuntimeException("Bad StateStoreMBean setup",e);
public void loadDriver(){synchronized (this.driver) {    if (!isDriverReady()) {      String driverName=this.driver.getClass().getSimpleName();      if (this.driver.init(conf,getIdentifier(),getSupportedRecords(),metrics)) {
  boolean success=true;  if (isDriverReady()) {    List<StateStoreCache> cachesToUpdate=new LinkedList<>();    cachesToUpdate.addAll(cachesToUpdateInternal);    cachesToUpdate.addAll(cachesToUpdateExternal);    for (    StateStoreCache cachedStore : cachesToUpdate) {      String cacheName=cachedStore.getClass().getSimpleName();      boolean result=false;      try {        result=cachedStore.loadCache(force);      } catch (      IOException e) {        LOG.error("Error updating cache for {}",cacheName,e);        result=false;      }      if (!result) {        success=false;
@Override public <T extends BaseRecord>boolean initRecordStorage(String className,Class<T> recordClass){  String dataDirPath=getRootDir() + "/" + className;  try {    if (!exists(dataDirPath)) {
@Override public <T extends BaseRecord>boolean initRecordStorage(String className,Class<T> recordClass){  String dataDirPath=getRootDir() + "/" + className;  try {    if (!exists(dataDirPath)) {      LOG.info("{} data directory doesn't exist, creating it",dataDirPath);      if (!mkdir(dataDirPath)) {
@Override public <T extends BaseRecord>QueryResult<T> get(Class<T> clazz) throws IOException {  verifyDriverReady();  long start=monotonicNow();  StateStoreMetrics metrics=getMetrics();  List<T> ret=new ArrayList<>();  try {    String path=getPathForClass(clazz);    List<String> children=getChildren(path);    for (    String child : children) {      String pathRecord=path + "/" + child;      if (child.endsWith(TMP_MARK)) {
    List<String> children=getChildren(path);    for (    String child : children) {      String pathRecord=path + "/" + child;      if (child.endsWith(TMP_MARK)) {        LOG.debug("There is a temporary file {} in {}",child,path);        if (isOldTempRecord(child)) {          LOG.warn("Removing {} as it's an old temporary record",child);          remove(pathRecord);        }      } else {        T record=getRecord(pathRecord,clazz);        ret.add(record);      }    }  } catch (  Exception e) {    if (metrics != null) {      metrics.addFailure(monotonicNow() - start);    }    String msg="Cannot fetch records for " + clazz.getSimpleName();
  if (records.isEmpty()) {    return true;  }  long start=monotonicNow();  StateStoreMetrics metrics=getMetrics();  Map<String,T> toWrite=new HashMap<>();  for (  T record : records) {    Class<? extends BaseRecord> recordClass=record.getClass();    String path=getPathForClass(recordClass);    String primaryKey=getPrimaryKey(record);    String recordPath=path + "/" + primaryKey;    if (exists(recordPath)) {      if (allowUpdate) {        record.setDateModified(this.getTime());        toWrite.put(recordPath,record);      } else       if (errorIfExists) {
  Map<String,T> toWrite=new HashMap<>();  for (  T record : records) {    Class<? extends BaseRecord> recordClass=record.getClass();    String path=getPathForClass(recordClass);    String primaryKey=getPrimaryKey(record);    String recordPath=path + "/" + primaryKey;    if (exists(recordPath)) {      if (allowUpdate) {        record.setDateModified(this.getTime());        toWrite.put(recordPath,record);      } else       if (errorIfExists) {        LOG.error("Attempt to insert record {} that already exists",recordPath);        if (metrics != null) {          metrics.addFailure(monotonicNow() - start);        }        return false;
        if (metrics != null) {          metrics.addFailure(monotonicNow() - start);        }        return false;      } else {        LOG.debug("Not updating {}",record);      }    } else {      toWrite.put(recordPath,record);    }  }  boolean success=true;  for (  Entry<String,T> entry : toWrite.entrySet()) {    String recordPath=entry.getKey();    String recordPathTemp=recordPath + "." + now()+ TMP_MARK;    BufferedWriter writer=getWriter(recordPathTemp);    try {      T record=entry.getValue();      String line=serializeString(record);
      }    } else {      toWrite.put(recordPath,record);    }  }  boolean success=true;  for (  Entry<String,T> entry : toWrite.entrySet()) {    String recordPath=entry.getKey();    String recordPathTemp=recordPath + "." + now()+ TMP_MARK;    BufferedWriter writer=getWriter(recordPathTemp);    try {      T record=entry.getValue();      String line=serializeString(record);      writer.write(line);    } catch (    IOException e) {      LOG.error("Cannot write {}",recordPathTemp,e);      success=false;    } finally {
 else {      toWrite.put(recordPath,record);    }  }  boolean success=true;  for (  Entry<String,T> entry : toWrite.entrySet()) {    String recordPath=entry.getKey();    String recordPathTemp=recordPath + "." + now()+ TMP_MARK;    BufferedWriter writer=getWriter(recordPathTemp);    try {      T record=entry.getValue();      String line=serializeString(record);      writer.write(line);    } catch (    IOException e) {      LOG.error("Cannot write {}",recordPathTemp,e);      success=false;    } finally {
  }  long start=Time.monotonicNow();  StateStoreMetrics metrics=getMetrics();  int removed=0;  try {    final QueryResult<T> result=get(clazz);    final List<T> existingRecords=result.getRecords();    final List<T> recordsToRemove=filterMultiple(query,existingRecords);    boolean success=true;    for (    T recordToRemove : recordsToRemove) {      String path=getPathForClass(clazz);      String primaryKey=getPrimaryKey(recordToRemove);      String recordToRemovePath=path + "/" + primaryKey;      if (remove(recordToRemovePath)) {        removed++;      } else {
  int removed=0;  try {    final QueryResult<T> result=get(clazz);    final List<T> existingRecords=result.getRecords();    final List<T> recordsToRemove=filterMultiple(query,existingRecords);    boolean success=true;    for (    T recordToRemove : recordsToRemove) {      String path=getPathForClass(clazz);      String primaryKey=getPrimaryKey(recordToRemove);      String recordToRemovePath=path + "/" + primaryKey;      if (remove(recordToRemovePath)) {        removed++;      } else {        LOG.error("Cannot remove record {}",recordToRemovePath);        success=false;
    final List<T> recordsToRemove=filterMultiple(query,existingRecords);    boolean success=true;    for (    T recordToRemove : recordsToRemove) {      String path=getPathForClass(clazz);      String primaryKey=getPrimaryKey(recordToRemove);      String recordToRemovePath=path + "/" + primaryKey;      if (remove(recordToRemovePath)) {        removed++;      } else {        LOG.error("Cannot remove record {}",recordToRemovePath);        success=false;      }    }    if (!success) {      LOG.error("Cannot remove records {} query {}",clazz,query);      if (metrics != null) {        metrics.addFailure(monotonicNow() - start);
@Override protected <T extends BaseRecord>BufferedReader getReader(String filename){  BufferedReader reader=null;  try {
@Override protected <T extends BaseRecord>BufferedWriter getWriter(String filename){  BufferedWriter writer=null;  try {
  try {    List<String> children=zkManager.getChildren(znode);    for (    String child : children) {      try {        String path=getNodePath(znode,child);        Stat stat=new Stat();        String data=zkManager.getStringData(path,stat);        boolean corrupted=false;        if (data == null || data.equals("")) {          corrupted=true;        } else {          try {            T record=createRecord(data,stat,clazz);            ret.add(record);          } catch (          IOException e) {
      try {        String path=getNodePath(znode,child);        Stat stat=new Stat();        String data=zkManager.getStringData(path,stat);        boolean corrupted=false;        if (data == null || data.equals("")) {          corrupted=true;        } else {          try {            T record=createRecord(data,stat,clazz);            ret.add(record);          } catch (          IOException e) {            LOG.error("Cannot create record type \"{}\" from \"{}\": {}",clazz.getSimpleName(),data,e.getMessage());            corrupted=true;          }        }        if (corrupted) {
        String data=zkManager.getStringData(path,stat);        boolean corrupted=false;        if (data == null || data.equals("")) {          corrupted=true;        } else {          try {            T record=createRecord(data,stat,clazz);            ret.add(record);          } catch (          IOException e) {            LOG.error("Cannot create record type \"{}\" from \"{}\": {}",clazz.getSimpleName(),data,e.getMessage());            corrupted=true;          }        }        if (corrupted) {          LOG.error("Cannot get data for {} at {}, cleaning corrupted data",child,path);          zkManager.delete(path);        }      } catch (      Exception e) {
        } else {          try {            T record=createRecord(data,stat,clazz);            ret.add(record);          } catch (          IOException e) {            LOG.error("Cannot create record type \"{}\" from \"{}\": {}",clazz.getSimpleName(),data,e.getMessage());            corrupted=true;          }        }        if (corrupted) {          LOG.error("Cannot get data for {} at {}, cleaning corrupted data",child,path);          zkManager.delete(path);        }      } catch (      Exception e) {        LOG.error("Cannot get data for {}: {}",child,e.getMessage());      }    }  } catch (  Exception e) {    getMetrics().addFailure(monotonicNow() - start);    String msg="Cannot get children for \"" + znode + "\": "+ e.getMessage();
  if (query == null) {    return 0;  }  long start=monotonicNow();  List<T> records=null;  try {    QueryResult<T> result=get(clazz);    records=result.getRecords();  } catch (  IOException ex) {    LOG.error("Cannot get existing records",ex);    getMetrics().addFailure(monotonicNow() - start);    return 0;  }  String znode=getZNodeForClass(clazz);  List<T> recordsToRemove=filterMultiple(query,records);  int removed=0;  for (  T existingRecord : recordsToRemove) {
    records=result.getRecords();  } catch (  IOException ex) {    LOG.error("Cannot get existing records",ex);    getMetrics().addFailure(monotonicNow() - start);    return 0;  }  String znode=getZNodeForClass(clazz);  List<T> recordsToRemove=filterMultiple(query,records);  int removed=0;  for (  T existingRecord : recordsToRemove) {    LOG.info("Removing \"{}\"",existingRecord);    try {      String primaryKey=getPrimaryKey(existingRecord);      String path=getNodePath(znode,primaryKey);      if (zkManager.delete(path)) {        removed++;
 catch (  IOException ex) {    LOG.error("Cannot get existing records",ex);    getMetrics().addFailure(monotonicNow() - start);    return 0;  }  String znode=getZNodeForClass(clazz);  List<T> recordsToRemove=filterMultiple(query,records);  int removed=0;  for (  T existingRecord : recordsToRemove) {    LOG.info("Removing \"{}\"",existingRecord);    try {      String primaryKey=getPrimaryKey(existingRecord);      String path=getNodePath(znode,primaryKey);      if (zkManager.delete(path)) {        removed++;      } else {
@Override public <T extends BaseRecord>boolean removeAll(Class<T> clazz) throws IOException {  long start=monotonicNow();  boolean status=true;  String znode=getZNodeForClass(clazz);
@Override public <T extends BaseRecord>boolean removeAll(Class<T> clazz) throws IOException {  long start=monotonicNow();  boolean status=true;  String znode=getZNodeForClass(clazz);  LOG.info("Deleting all children under {}",znode);  try {    List<String> children=zkManager.getChildren(znode);    for (    String child : children) {      String path=getNodePath(znode,child);
private boolean writeNode(String znode,byte[] bytes,boolean update,boolean error){  try {    boolean created=zkManager.create(znode);    if (!update && !created && error) {
@Override public NamenodeHeartbeatResponse namenodeHeartbeat(NamenodeHeartbeatRequest request) throws IOException {  MembershipState record=request.getNamenodeMembership();  String nnId=record.getNamenodeKey();  MembershipState existingEntry=null;  cacheReadLock.lock();  try {    existingEntry=this.activeRegistrations.get(nnId);  }  finally {    cacheReadLock.unlock();  }  if (existingEntry != null) {    if (existingEntry.getState() != record.getState()) {      LOG.info("NN registration state has changed: {} -> {}",existingEntry,record);    } else {      LOG.debug("Updating NN registration: {} -> {}",existingEntry,record);    }  } else {
  for (  MembershipState record : records) {    FederationNamenodeServiceState state=record.getState();    TreeSet<MembershipState> matchingSet=occurenceMap.get(state);    if (matchingSet == null) {      matchingSet=new TreeSet<>();      occurenceMap.put(state,matchingSet);    }    matchingSet.add(record);  }  TreeSet<MembershipState> largestSet=new TreeSet<>();  for (  TreeSet<MembershipState> matchingSet : occurenceMap.values()) {    if (largestSet.size() < matchingSet.size()) {      largestSet=matchingSet;    }  }  if (largestSet.size() > records.size() / 2) {    return largestSet.first();  } else   if (records.size() > 0) {    TreeSet<MembershipState> sortedList=new TreeSet<>(records);
public static void simulateSlowNamenode(final NameNode nn,final int seconds) throws Exception {  FSNamesystem namesystem=nn.getNamesystem();  HAContext haContext=namesystem.getHAContext();  HAContext spyHAContext=spy(haContext);  doAnswer(new Answer<Object>(){    @Override public Object answer(    InvocationOnMock invocation) throws Throwable {
public static void simulateThrowExceptionRouterRpcServer(final RouterRpcServer server) throws IOException {  RouterRpcClient rpcClient=server.getRPCClient();  ConnectionManager connectionManager=new ConnectionManager(server.getConfig());  ConnectionManager spyConnectionManager=spy(connectionManager);  doAnswer(new Answer(){    @Override public Object answer(    InvocationOnMock invocation) throws Throwable {
public void waitRouterRegistrationQuorum(RouterContext router,FederationNamenodeServiceState state,String nsId,String nnId) throws Exception {
public void stopRouter(RouterContext router){  try {    router.router.shutDown();    int loopCount=0;    while (router.router.getServiceState() != STATE.STOPPED) {      loopCount++;      Thread.sleep(1000);      if (loopCount > 20) {
public void addFileSystemMock() throws IOException {  final SortedMap<String,String> fs=new ConcurrentSkipListMap<String,String>();  DirectoryListing l=mockNn.getListing(anyString(),any(),anyBoolean());  when(l).thenAnswer(invocation -> {    String src=getSrc(invocation);
    if (fs.get(src) == null) {      throw new FileNotFoundException("File does not exist " + src);    }    if (!src.endsWith("/")) {      src+="/";    }    Map<String,String> files=fs.subMap(src,src + Character.MAX_VALUE);    List<HdfsFileStatus> list=new ArrayList<>();    for (    String file : files.keySet()) {      if (file.substring(src.length()).indexOf('/') < 0) {        HdfsFileStatus fileStatus=getMockHdfsFileStatus(file,fs.get(file));        list.add(fileStatus);      }    }    HdfsFileStatus[] array=list.toArray(new HdfsFileStatus[list.size()]);    return new DirectoryListing(array,0);  });  when(mockNn.getFileInfo(anyString())).thenAnswer(invocation -> {    String src=getSrc(invocation);
    Map<String,String> files=fs.subMap(src,src + Character.MAX_VALUE);    List<HdfsFileStatus> list=new ArrayList<>();    for (    String file : files.keySet()) {      if (file.substring(src.length()).indexOf('/') < 0) {        HdfsFileStatus fileStatus=getMockHdfsFileStatus(file,fs.get(file));        list.add(fileStatus);      }    }    HdfsFileStatus[] array=list.toArray(new HdfsFileStatus[list.size()]);    return new DirectoryListing(array,0);  });  when(mockNn.getFileInfo(anyString())).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} getFileInfo({})",nsId,src);    return getMockHdfsFileStatus(src,fs.get(src));  });  HdfsFileStatus c=mockNn.create(anyString(),any(),anyString(),any(),anyBoolean(),anyShort(),anyLong(),any(),any(),any());
        list.add(fileStatus);      }    }    HdfsFileStatus[] array=list.toArray(new HdfsFileStatus[list.size()]);    return new DirectoryListing(array,0);  });  when(mockNn.getFileInfo(anyString())).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} getFileInfo({})",nsId,src);    return getMockHdfsFileStatus(src,fs.get(src));  });  HdfsFileStatus c=mockNn.create(anyString(),any(),anyString(),any(),anyBoolean(),anyShort(),anyLong(),any(),any(),any());  when(c).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} create({})",nsId,src);    boolean createParent=(boolean)invocation.getArgument(4);    if (createParent) {
    LOG.info("{} getFileInfo({})",nsId,src);    return getMockHdfsFileStatus(src,fs.get(src));  });  HdfsFileStatus c=mockNn.create(anyString(),any(),anyString(),any(),anyBoolean(),anyShort(),anyLong(),any(),any(),any());  when(c).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} create({})",nsId,src);    boolean createParent=(boolean)invocation.getArgument(4);    if (createParent) {      Path path=new Path(src).getParent();      while (!path.isRoot()) {        LOG.info("{} create parent {}",nsId,path);        fs.put(path.toString(),"DIRECTORY");        path=path.getParent();      }    }    fs.put(src,"FILE");
  });  HdfsFileStatus c=mockNn.create(anyString(),any(),anyString(),any(),anyBoolean(),anyShort(),anyLong(),any(),any(),any());  when(c).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} create({})",nsId,src);    boolean createParent=(boolean)invocation.getArgument(4);    if (createParent) {      Path path=new Path(src).getParent();      while (!path.isRoot()) {        LOG.info("{} create parent {}",nsId,path);        fs.put(path.toString(),"DIRECTORY");        path=path.getParent();      }    }    fs.put(src,"FILE");    return getMockHdfsFileStatus(src,"FILE");  });
      Path path=new Path(src).getParent();      while (!path.isRoot()) {        LOG.info("{} create parent {}",nsId,path);        fs.put(path.toString(),"DIRECTORY");        path=path.getParent();      }    }    fs.put(src,"FILE");    return getMockHdfsFileStatus(src,"FILE");  });  LocatedBlocks b=mockNn.getBlockLocations(anyString(),anyLong(),anyLong());  when(b).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} getBlockLocations({})",nsId,src);    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for getBlockLocations",nsId,src);      throw new FileNotFoundException("File does not exist " + src);
    return getMockHdfsFileStatus(src,"FILE");  });  LocatedBlocks b=mockNn.getBlockLocations(anyString(),anyLong(),anyLong());  when(b).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} getBlockLocations({})",nsId,src);    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for getBlockLocations",nsId,src);      throw new FileNotFoundException("File does not exist " + src);    }    return mock(LocatedBlocks.class);  });  boolean f=mockNn.complete(anyString(),anyString(),any(),anyLong());  when(f).thenAnswer(invocation -> {    String src=getSrc(invocation);    if (!fs.containsKey(src)) {
    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for getBlockLocations",nsId,src);      throw new FileNotFoundException("File does not exist " + src);    }    return mock(LocatedBlocks.class);  });  boolean f=mockNn.complete(anyString(),anyString(),any(),anyLong());  when(f).thenAnswer(invocation -> {    String src=getSrc(invocation);    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for complete",nsId,src);      throw new FileNotFoundException("File does not exist " + src);    }    return true;  });  LocatedBlock a=mockNn.addBlock(anyString(),anyString(),any(),any(),anyLong(),any(),any());  when(a).thenAnswer(invocation -> {
  });  boolean f=mockNn.complete(anyString(),anyString(),any(),anyLong());  when(f).thenAnswer(invocation -> {    String src=getSrc(invocation);    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for complete",nsId,src);      throw new FileNotFoundException("File does not exist " + src);    }    return true;  });  LocatedBlock a=mockNn.addBlock(anyString(),anyString(),any(),any(),anyLong(),any(),any());  when(a).thenAnswer(invocation -> {    String src=getSrc(invocation);    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for addBlock",nsId,src);      throw new FileNotFoundException("File does not exist " + src);
      throw new FileNotFoundException("File does not exist " + src);    }    return true;  });  LocatedBlock a=mockNn.addBlock(anyString(),anyString(),any(),any(),anyLong(),any(),any());  when(a).thenAnswer(invocation -> {    String src=getSrc(invocation);    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for addBlock",nsId,src);      throw new FileNotFoundException("File does not exist " + src);    }    return getMockLocatedBlock(nsId);  });  boolean m=mockNn.mkdirs(anyString(),any(),anyBoolean());  when(m).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} mkdirs({})",nsId,src);
    if (!fs.containsKey(src)) {      LOG.error("{} cannot find {} for addBlock",nsId,src);      throw new FileNotFoundException("File does not exist " + src);    }    return getMockLocatedBlock(nsId);  });  boolean m=mockNn.mkdirs(anyString(),any(),anyBoolean());  when(m).thenAnswer(invocation -> {    String src=getSrc(invocation);    LOG.info("{} mkdirs({})",nsId,src);    boolean createParent=(boolean)invocation.getArgument(2);    if (createParent) {      Path path=new Path(src).getParent();      while (!path.isRoot()) {        LOG.info("{} mkdir parent {}",nsId,path);        fs.put(path.toString(),"DIRECTORY");
public void addDatanodeMock() throws IOException {  when(mockNn.getDatanodeReport(any(DatanodeReportType.class))).thenAnswer(invocation -> {
public static void registerSubclusters(List<Router> routers,Collection<MockNamenode> namenodes,Set<String> unavailableSubclusters) throws IOException {  for (  final Router router : routers) {    MembershipNamenodeResolver resolver=(MembershipNamenodeResolver)router.getNamenodeResolver();    for (    final MockNamenode nn : namenodes) {      String nsId=nn.getNameserviceId();      String rpcAddress="localhost:" + nn.getRPCPort();      String httpAddress="localhost:" + nn.getHTTPPort();      String scheme="http";      NamenodeStatusReport report=new NamenodeStatusReport(nsId,null,rpcAddress,rpcAddress,rpcAddress,scheme,httpAddress);      if (unavailableSubclusters.contains(nsId)) {
@Test public void testMountTableScalability() throws IOException {  List<MountTable> emptyList=new ArrayList<>();  mountTable.refreshEntries(emptyList);  for (int i=0; i < 100000; i++) {    Map<String,String> map=getMountTableEntry("1","/" + i);    MountTable record=MountTable.newInstance("/" + i,map);    mountTable.addEntry(record);    if (i % 10000 == 0) {
 finally {        if (routerClient != null) {          try {            routerClient.close();          } catch (          IOException e) {            LOG.error("Cannot close the client");          }        }      }    });    GenericTestUtils.waitFor(() -> {      try {        Map<String,Integer> newResult=objectMapper.readValue(metrics.getAsyncCallerPool(),Map.class);        if (newResult.get("active") != 1) {          return false;        }        if (newResult.get("max") != 4) {          return false;        }        int total=newResult.get("total");
private void testWriteWithFailedSubcluster(final DestinationOrder order) throws Exception {  final FileSystem router0Fs=getFileSystem(routers.get(0));  final FileSystem router1Fs=getFileSystem(routers.get(1));  final FileSystem ns0Fs=getFileSystem(namenodes.get("ns0").getRPCPort());  final String mountPoint="/" + order + "-failsubcluster";  final Path mountPath=new Path(mountPoint);
private void testWriteWithFailedSubcluster(final DestinationOrder order) throws Exception {  final FileSystem router0Fs=getFileSystem(routers.get(0));  final FileSystem router1Fs=getFileSystem(routers.get(1));  final FileSystem ns0Fs=getFileSystem(namenodes.get("ns0").getRPCPort());  final String mountPoint="/" + order + "-failsubcluster";  final Path mountPath=new Path(mountPoint);  LOG.info("Setup {} with order {}",mountPoint,order);  createMountTableEntry(getRandomRouter(),mountPoint,order,namenodes.keySet());  refreshRoutersCaches(routers);
private void testWriteWithFailedSubcluster(final DestinationOrder order) throws Exception {  final FileSystem router0Fs=getFileSystem(routers.get(0));  final FileSystem router1Fs=getFileSystem(routers.get(1));  final FileSystem ns0Fs=getFileSystem(namenodes.get("ns0").getRPCPort());  final String mountPoint="/" + order + "-failsubcluster";  final Path mountPath=new Path(mountPoint);  LOG.info("Setup {} with order {}",mountPoint,order);  createMountTableEntry(getRandomRouter(),mountPoint,order,namenodes.keySet());  refreshRoutersCaches(routers);  LOG.info("Write in {} should succeed writing in ns0 and fail for ns1",mountPath);  checkDirectoriesFaultTolerant(mountPath,order,router0Fs,router1Fs,ns0Fs,false);  checkFilesFaultTolerant(mountPath,order,router0Fs,router1Fs,ns0Fs,false);
private void checkDirectoriesFaultTolerant(Path mountPoint,DestinationOrder order,FileSystem router0Fs,FileSystem router1Fs,FileSystem ns0Fs,boolean faultTolerant) throws Exception {  final FileStatus[] dirs0=listStatus(router1Fs,mountPoint);
private void checkFilesFaultTolerant(Path mountPoint,DestinationOrder order,FileSystem router0Fs,FileSystem router1Fs,FileSystem ns0Fs,boolean faultTolerant) throws Exception {  final FileStatus[] dirs0=listStatus(router1Fs,mountPoint);  final Path dir0=Path.getPathWithoutSchemeAndAuthority(dirs0[0].getPath());
private TaskResults collectResults(final String tag,final Collection<Callable<Boolean>> tasks) throws Exception {  final TaskResults results=new TaskResults();  service.invokeAll(tasks).forEach(task -> {    try {      boolean succeeded=task.get();      if (succeeded) {
@Test public void testReadWithFailedSubcluster() throws Exception {  DestinationOrder order=DestinationOrder.HASH_ALL;  final String mountPoint="/" + order + "-testread";  final Path mountPath=new Path(mountPoint);
  FSDataInputStream fsdis=fs.open(fileexisting);  assertNotNull("We should be able to read the file",fsdis);  LambdaTestUtils.intercept(FileNotFoundException.class,() -> fs.open(filenotexisting));  String nsIdWithFile=null;  for (  Entry<String,MockNamenode> entry : namenodes.entrySet()) {    String nsId=entry.getKey();    MockNamenode nn=entry.getValue();    int rpc=nn.getRPCPort();    FileSystem nnfs=getFileSystem(rpc);    try {      FileStatus fileStatus=nnfs.getFileStatus(fileexisting);      assertNotNull(fileStatus);      assertNull("The file cannot be in two subclusters",nsIdWithFile);      nsIdWithFile=nsId;    } catch (    FileNotFoundException fnfe) {
  LambdaTestUtils.intercept(FileNotFoundException.class,() -> fs.open(filenotexisting));  String nsIdWithFile=null;  for (  Entry<String,MockNamenode> entry : namenodes.entrySet()) {    String nsId=entry.getKey();    MockNamenode nn=entry.getValue();    int rpc=nn.getRPCPort();    FileSystem nnfs=getFileSystem(rpc);    try {      FileStatus fileStatus=nnfs.getFileStatus(fileexisting);      assertNotNull(fileStatus);      assertNull("The file cannot be in two subclusters",nsIdWithFile);      nsIdWithFile=nsId;    } catch (    FileNotFoundException fnfe) {      LOG.debug("File not found in {}",nsId);    }  }  assertNotNull("The file has to be in one subcluster",nsIdWithFile);
@Test public void testFsck() throws Exception {  MountTable addEntry=MountTable.newInstance("/testdir",Collections.singletonMap("ns0","/testdir"));  assertTrue(addMountTable(addEntry));  addEntry=MountTable.newInstance("/testdir2",Collections.singletonMap("ns1","/testdir2"));  assertTrue(addMountTable(addEntry));  routerFs.createNewFile(new Path("/testdir/testfile"));  routerFs.createNewFile(new Path("/testdir2/testfile2"));  routerFs.createNewFile(new Path("/testdir2/testfile3"));  routerFs.createNewFile(new Path("/testdir2/testfile4"));  try (CloseableHttpClient httpClient=HttpClients.createDefault()){    HttpGet httpGet=new HttpGet("http://" + webAddress.getHostName() + ":"+ webAddress.getPort()+ "/fsck");    try (CloseableHttpResponse httpResponse=httpClient.execute(httpGet)){      assertEquals(HttpStatus.SC_OK,httpResponse.getStatusLine().getStatusCode());      String out=EntityUtils.toString(httpResponse.getEntity(),StandardCharsets.UTF_8);
      LOG.info(out);      assertTrue(out.contains("Federated FSCK started"));      assertTrue(out.contains("Total files:\t1"));      assertTrue(out.contains("Total files:\t3"));      assertTrue(out.contains("Federated FSCK ended"));      int nnCount=0;      for (      MembershipState nn : memberships) {        if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {          assertTrue(out.contains("Checking " + nn + " at "+ nn.getWebAddress()+ "\n"));          nnCount++;        }      }      assertEquals(2,nnCount);    }     httpGet=new HttpGet("http://" + webAddress.getHostName() + ":"+ webAddress.getPort()+ "/fsck?path=/testdir");    try (CloseableHttpResponse httpResponse=httpClient.execute(httpGet)){      assertEquals(HttpStatus.SC_OK,httpResponse.getStatusLine().getStatusCode());      String out=EntityUtils.toString(httpResponse.getEntity(),StandardCharsets.UTF_8);
@Test public void testProxyGetStats() throws Exception {  Supplier<Boolean> check=new Supplier<Boolean>(){    @Override public Boolean get(){      try {        long[] combinedData=routerProtocol.getStats();        long[] individualData=getAggregateStats();        int len=Math.min(combinedData.length,individualData.length);        for (int i=0; i < len; i++) {          if (combinedData[i] != individualData[i]) {
@Test public void testManageSnapshot() throws Exception {  final String mountPoint="/mntsnapshot";  final String snapshotFolder=mountPoint + "/folder";
@Test public void testErasureCoding() throws Exception {  LOG.info("List the available erasurce coding policies");  ErasureCodingPolicyInfo[] policies=checkErasureCodingPolicies();  for (  ErasureCodingPolicyInfo policy : policies) {
  Map<String,String> codecsNamenode=nnProtocol.getErasureCodingCodecs();  assertTrue(Maps.difference(codecsRouter,codecsNamenode).areEqual());  for (  Entry<String,String> entry : codecsRouter.entrySet()) {    LOG.info("  {}: {}",entry.getKey(),entry.getValue());  }  LOG.info("Create a testing directory via the router at the root level");  String dirPath="/testec";  String filePath1=dirPath + "/testfile1";  FsPermission permission=new FsPermission("755");  routerProtocol.mkdirs(dirPath,permission,false);  createFile(routerFS,filePath1,32);  assertTrue(verifyFileExists(routerFS,filePath1));  DFSClient file1Protocol=getFileDFSClient(filePath1);  LOG.info("The policy for the new file should not be set");  assertNull(routerProtocol.getErasureCodingPolicy(filePath1));  assertNull(file1Protocol.getErasureCodingPolicy(filePath1));
  String filePath1=dirPath + "/testfile1";  FsPermission permission=new FsPermission("755");  routerProtocol.mkdirs(dirPath,permission,false);  createFile(routerFS,filePath1,32);  assertTrue(verifyFileExists(routerFS,filePath1));  DFSClient file1Protocol=getFileDFSClient(filePath1);  LOG.info("The policy for the new file should not be set");  assertNull(routerProtocol.getErasureCodingPolicy(filePath1));  assertNull(file1Protocol.getErasureCodingPolicy(filePath1));  String policyName="RS-6-3-1024k";  LOG.info("Set policy \"{}\" for \"{}\"",policyName,dirPath);  routerProtocol.setErasureCodingPolicy(dirPath,policyName);  String filePath2=dirPath + "/testfile2";  LOG.info("Create {} in the path with the new EC policy",filePath2);  createFile(routerFS,filePath2,32);
private DFSClient getFileDFSClient(final String path){  for (  String nsId : cluster.getNameservices()) {
  when(ugi1.getRealUser()).thenReturn(suUgi);  when(ugi2.getRealUser()).thenReturn(suUgi);  when(suUgi.getShortUserName()).thenReturn(superUser);  when(suUgi.getUserName()).thenReturn(superUser + "L");  when(ugi1.getShortUserName()).thenReturn("user1");  when(ugi2.getShortUserName()).thenReturn("user2");  when(ugi1.getUserName()).thenReturn("userL1");  when(ugi2.getUserName()).thenReturn("userL2");  when(ugi1.getGroups()).thenReturn(groupNames1);  when(ugi2.getGroups()).thenReturn(groupNames2);  when(ugi1.getGroupsSet()).thenReturn(groupNamesSet1);  when(ugi2.getGroupsSet()).thenReturn(groupNamesSet2);  LambdaTestUtils.intercept(AuthorizationException.class,() -> ProxyUsers.authorize(ugi1,LOOPBACK_ADDRESS));  try {    ProxyUsers.authorize(ugi2,LOOPBACK_ADDRESS);
  LambdaTestUtils.intercept(AuthorizationException.class,() -> ProxyUsers.authorize(ugi1,LOOPBACK_ADDRESS));  try {    ProxyUsers.authorize(ugi2,LOOPBACK_ADDRESS);    LOG.info("auth for {} succeeded",ugi2.getUserName());  } catch (  AuthorizationException e) {    fail("first auth for " + ugi2.getShortUserName() + " should've succeeded: "+ e.getLocalizedMessage());  }  String rsrc="testGroupMappingRefresh_rsrc.xml";  tempResource=addNewConfigResource(rsrc,userKeyGroups,"gr2",userKeyHosts,LOOPBACK_ADDRESS);  conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY,defaultFs);  DFSAdmin admin=new DFSAdmin(conf);  String[] args=new String[]{"-refreshSuperUserGroupsConfiguration"};  admin.run(args);  LambdaTestUtils.intercept(AuthorizationException.class,() -> ProxyUsers.authorize(ugi2,LOOPBACK_ADDRESS));  try {    ProxyUsers.authorize(ugi1,LOOPBACK_ADDRESS);
private void testGroupMappingRefreshInternal(String defaultFs) throws Exception {  Groups groups=Groups.getUserToGroupsMappingService(conf);  String user="test_user123";  LOG.info("First attempt:");  List<String> g1=groups.getGroups(user);
private void testGroupMappingRefreshInternal(String defaultFs) throws Exception {  Groups groups=Groups.getUserToGroupsMappingService(conf);  String user="test_user123";  LOG.info("First attempt:");  List<String> g1=groups.getGroups(user);  LOG.info("Group 1 :{}",g1);  LOG.info("Second attempt, should be the same:");  List<String> g2=groups.getGroups(user);
  String user="test_user123";  LOG.info("First attempt:");  List<String> g1=groups.getGroups(user);  LOG.info("Group 1 :{}",g1);  LOG.info("Second attempt, should be the same:");  List<String> g2=groups.getGroups(user);  LOG.info("Group 2 :{}",g2);  for (int i=0; i < g2.size(); i++) {    assertEquals("Should be same group ",g1.get(i),g2.get(i));  }  conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY,defaultFs);  DFSAdmin admin=new DFSAdmin(conf);  String[] args=new String[]{"-refreshUserToGroupsMappings"};  admin.run(args);  LOG.info("Third attempt(after refresh command), should be different:");  List<String> g3=groups.getGroups(user);
  }  conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY,defaultFs);  DFSAdmin admin=new DFSAdmin(conf);  String[] args=new String[]{"-refreshUserToGroupsMappings"};  admin.run(args);  LOG.info("Third attempt(after refresh command), should be different:");  List<String> g3=groups.getGroups(user);  LOG.info("Group 3:{}",g3);  for (int i=0; i < g3.size(); i++) {    assertNotEquals("Should be different group: " + g1.get(i) + " and "+ g3.get(i),g1.get(i),g3.get(i));  }  LOG.info("Fourth attempt(after timeout), should be different:");  GenericTestUtils.waitFor(() -> {    List<String> g4;    try {      g4=groups.getGroups(user);    } catch (    IOException e) {
  String[] args=new String[]{"-refreshUserToGroupsMappings"};  admin.run(args);  LOG.info("Third attempt(after refresh command), should be different:");  List<String> g3=groups.getGroups(user);  LOG.info("Group 3:{}",g3);  for (int i=0; i < g3.size(); i++) {    assertNotEquals("Should be different group: " + g1.get(i) + " and "+ g3.get(i),g1.get(i),g3.get(i));  }  LOG.info("Fourth attempt(after timeout), should be different:");  GenericTestUtils.waitFor(() -> {    List<String> g4;    try {      g4=groups.getGroups(user);    } catch (    IOException e) {      LOG.debug("Failed to get groups for user:{}",user);      return false;
    ZKDelegationTokenSecretManagerImpl dtsm1=new ZKDelegationTokenSecretManagerImpl(conf);    ZKDelegationTokenSecretManagerImpl dtsm2=new ZKDelegationTokenSecretManagerImpl(conf);    ZKDelegationTokenSecretManagerImpl dtsm3=new ZKDelegationTokenSecretManagerImpl(conf);    DelegationTokenManager tm1, tm2, tm3;    tm1=new DelegationTokenManager(conf,new Text("bla"));    tm1.setExternalDelegationTokenSecretManager(dtsm1);    tm2=new DelegationTokenManager(conf,new Text("bla"));    tm2.setExternalDelegationTokenSecretManager(dtsm2);    tm3=new DelegationTokenManager(conf,new Text("bla"));    tm3.setExternalDelegationTokenSecretManager(dtsm3);    Token<DelegationTokenIdentifier> token=(Token<DelegationTokenIdentifier>)tm1.createToken(UserGroupInformation.getCurrentUser(),"foo");    Assert.assertNotNull(token);    tm2.verifyToken(token);    Thread.sleep(9 * 1000);    long renewalTime=tm2.renewToken(token,"foo");
  String filterInitializerConfKey="hadoop.http.filter.initializers";  String initializers=conf.get(filterInitializerConfKey,"");  String[] parts=initializers.split(",");  Set<String> target=new LinkedHashSet<String>();  for (  String filterInitializer : parts) {    filterInitializer=filterInitializer.trim();    if (filterInitializer.equals(AuthenticationFilterInitializer.class.getName()) || filterInitializer.equals(ProxyUserAuthenticationFilterInitializer.class.getName()) || filterInitializer.isEmpty()) {      continue;    }    target.add(filterInitializer);  }  target.add(AuthFilterInitializer.class.getName());  initializers=StringUtils.join(target,",");  conf.set(filterInitializerConfKey,initializers);  LOG.info("Filter initializers set : " + initializers);  HttpServer2.Builder builder=new HttpServer2.Builder().setName(name).setConf(conf).setACL(new AccessControlList(conf.get(DFS_ADMIN," "))).setSecurityEnabled(UserGroupInformation.isSecurityEnabled()).setUsernameConfKey(spnegoUserNameKey).setKeytabConfKey(getSpnegoKeytabKey(conf,spnegoKeytabFileKey));  if (UserGroupInformation.isSecurityEnabled()) {
    if (filterInitializer.equals(AuthenticationFilterInitializer.class.getName()) || filterInitializer.equals(ProxyUserAuthenticationFilterInitializer.class.getName()) || filterInitializer.isEmpty()) {      continue;    }    target.add(filterInitializer);  }  target.add(AuthFilterInitializer.class.getName());  initializers=StringUtils.join(target,",");  conf.set(filterInitializerConfKey,initializers);  LOG.info("Filter initializers set : " + initializers);  HttpServer2.Builder builder=new HttpServer2.Builder().setName(name).setConf(conf).setACL(new AccessControlList(conf.get(DFS_ADMIN," "))).setSecurityEnabled(UserGroupInformation.isSecurityEnabled()).setUsernameConfKey(spnegoUserNameKey).setKeytabConfKey(getSpnegoKeytabKey(conf,spnegoKeytabFileKey));  if (UserGroupInformation.isSecurityEnabled()) {    LOG.info("Starting web server as: " + SecurityUtil.getServerPrincipal(conf.get(spnegoUserNameKey),httpAddr.getHostName()));  }  if (policy.isHttpEnabled()) {    if (httpAddr.getPort() == 0) {      builder.setFindPort(true);    }    URI uri=URI.create("http://" + NetUtils.getHostPortString(httpAddr));    builder.addEndpoint(uri);
  LOG.info("Filter initializers set : " + initializers);  HttpServer2.Builder builder=new HttpServer2.Builder().setName(name).setConf(conf).setACL(new AccessControlList(conf.get(DFS_ADMIN," "))).setSecurityEnabled(UserGroupInformation.isSecurityEnabled()).setUsernameConfKey(spnegoUserNameKey).setKeytabConfKey(getSpnegoKeytabKey(conf,spnegoKeytabFileKey));  if (UserGroupInformation.isSecurityEnabled()) {    LOG.info("Starting web server as: " + SecurityUtil.getServerPrincipal(conf.get(spnegoUserNameKey),httpAddr.getHostName()));  }  if (policy.isHttpEnabled()) {    if (httpAddr.getPort() == 0) {      builder.setFindPort(true);    }    URI uri=URI.create("http://" + NetUtils.getHostPortString(httpAddr));    builder.addEndpoint(uri);    LOG.info("Starting Web-server for " + name + " at: "+ uri);  }  if (policy.isHttpsEnabled() && httpsAddr != null) {    Configuration sslConf=loadSslConfiguration(conf);    loadSslConfToHttpServerBuilder(builder,sslConf);    if (httpsAddr.getPort() == 0) {      builder.setFindPort(true);
  InetSocketAddress inAddr=null;  if (!(fs instanceof DistributedFileSystem)) {    throw new IllegalArgumentException("FileSystem " + fs + " is not a DFS.");  }  fs.exists(new Path("/"));  DistributedFileSystem dfs=(DistributedFileSystem)fs;  Configuration dfsConf=dfs.getConf();  URI dfsUri=dfs.getUri();  String nsId=dfsUri.getHost();  if (isHAEnabled(dfsConf,nsId)) {    List<ClientProtocol> namenodes=getProxiesForAllNameNodesInNameservice(dfsConf,nsId);    for (    ClientProtocol proxy : namenodes) {      try {        if (proxy.getHAServiceState().equals(HAServiceState.ACTIVE)) {          inAddr=RPC.getServerAddress(proxy);        }      } catch (      Exception e) {
      searchScope=NodeBase.ROOT;      excludedScope=scope.substring(1);    } else {      searchScope=scope;      excludedScope=null;    }    Node n=chooseRandom(searchScope,excludedScope,excludedNodes);    if (n == null) {      if (LOG.isDebugEnabled()) {        LOG.debug("No node to choose.");      }      return null;    }    Preconditions.checkArgument(n instanceof DatanodeDescriptor);    DatanodeDescriptor dnDescriptor=(DatanodeDescriptor)n;    if (dnDescriptor.hasStorageType(type)) {      return dnDescriptor;    } else {
  int availableCount=root.getSubtreeStorageCount(type);  if (excludeRoot != null && root.isAncestor(excludeRoot)) {    if (excludeRoot instanceof DFSTopologyNodeImpl) {      availableCount-=((DFSTopologyNodeImpl)excludeRoot).getSubtreeStorageCount(type);    } else {      availableCount-=((DatanodeDescriptor)excludeRoot).hasStorageType(type) ? 1 : 0;    }  }  if (excludedNodes != null) {    for (    Node excludedNode : excludedNodes) {      if (excludeRoot != null && isNodeInScope(excludedNode,excludedScope)) {        continue;      }      if (excludedNode instanceof DatanodeDescriptor) {        availableCount-=((DatanodeDescriptor)excludedNode).hasStorageType(type) ? 1 : 0;      } else       if (excludedNode instanceof DFSTopologyNodeImpl) {        availableCount-=((DFSTopologyNodeImpl)excludedNode).getSubtreeStorageCount(type);      } else       if (excludedNode instanceof DatanodeInfo) {
@Override public boolean add(Node n){
@Override public boolean remove(Node n){
public synchronized void childAddStorage(String childName,StorageType type){
public synchronized void childRemoveStorage(String childName,StorageType type){
public IOStreamPair receive(Peer peer,OutputStream underlyingOut,InputStream underlyingIn,int xferPort,DatanodeID datanodeId) throws IOException {  if (dnConf.getEncryptDataTransfer()) {
  Collection<InMemoryAliasMapProtocol> aliasMaps=new ArrayList<>();  for (  String nsId : getNameServiceIds(conf)) {    try {      URI namenodeURI=null;      Configuration newConf=new Configuration(conf);      if (HAUtil.isHAEnabled(conf,nsId)) {        newConf.setClass(addKeySuffixes(PROXY_PROVIDER_KEY_PREFIX,nsId),InMemoryAliasMapFailoverProxyProvider.class,AbstractNNFailoverProxyProvider.class);        namenodeURI=new URI(HdfsConstants.HDFS_URI_SCHEME + "://" + nsId);      } else {        String key=addKeySuffixes(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS,nsId);        String addr=conf.get(key);        if (addr != null) {          namenodeURI=createUri(HdfsConstants.HDFS_URI_SCHEME,NetUtils.createSocketAddr(addr));        }      }      if (namenodeURI != null) {        aliasMaps.add(NameNodeProxies.createProxy(newConf,namenodeURI,InMemoryAliasMapProtocol.class).getProxy());
        namenodeURI=new URI(HdfsConstants.HDFS_URI_SCHEME + "://" + nsId);      } else {        String key=addKeySuffixes(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS,nsId);        String addr=conf.get(key);        if (addr != null) {          namenodeURI=createUri(HdfsConstants.HDFS_URI_SCHEME,NetUtils.createSocketAddr(addr));        }      }      if (namenodeURI != null) {        aliasMaps.add(NameNodeProxies.createProxy(newConf,namenodeURI,InMemoryAliasMapProtocol.class).getProxy());        LOG.info("Connected to InMemoryAliasMap at {}",namenodeURI);      }    } catch (    IOException|URISyntaxException e) {      LOG.warn("Exception in connecting to InMemoryAliasMap for nameservice " + "{}: {}",nsId,e);    }  }  if (conf.get(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS) != null) {    URI uri=createUri("hdfs",NetUtils.createSocketAddr(conf.get(DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS)));    try {      aliasMaps.add(NameNodeProxies.createProxy(conf,uri,InMemoryAliasMapProtocol.class).getProxy());
@Override public void purgeLogsOlderThan(long minTxIdToKeep) throws IOException {
@Override public void recoverUnfinalizedSegments() throws IOException {  Preconditions.checkState(!isActiveWriter,"already active writer");  LOG.info("Starting recovery process for unclosed journal segments...");  Map<AsyncLogger,NewEpochResponseProto> resps=createNewUniqueEpoch();  LOG.info("Successfully started new epoch " + loggers.getEpoch());  if (LOG.isDebugEnabled()) {
@Override public void selectInputStreams(Collection<EditLogInputStream> streams,long fromTxnId,boolean inProgressOk,boolean onlyDurableTxns) throws IOException {  if (inProgressOk && inProgressTailingEnabled) {    if (LOG.isDebugEnabled()) {
File getOrCreatePaxosDir(){  File paxosDir=new File(sd.getCurrentDir(),"paxos");  if (!paxosDir.exists()) {
File getOrCreatePaxosDir(){  File paxosDir=new File(sd.getCurrentDir(),"paxos");  if (!paxosDir.exists()) {    LOG.info("Creating paxos dir: {}",paxosDir.toPath());    if (!paxosDir.mkdir()) {
private static void purgeMatching(File dir,List<Pattern> patterns,long minTxIdToKeep) throws IOException {  for (  File f : FileUtil.listFiles(dir)) {    if (!f.isFile())     continue;    for (    Pattern p : patterns) {      Matcher matcher=p.matcher(f.getName());      if (matcher.matches()) {        long txid=Long.parseLong(matcher.group(1));        if (txid < minTxIdToKeep) {
void format(NamespaceInfo nsInfo,boolean force) throws IOException {  Preconditions.checkState(nsInfo.getNamespaceID() != 0,"can't format with uninitialized namespace info: %s",nsInfo);
private void updateLastPromisedEpoch(long newEpoch) throws IOException {
  checkRequest(reqInfo);  boolean needsValidation=true;  if (startTxId == curSegmentTxId) {    if (curSegment != null) {      curSegment.close();      curSegment=null;      curSegmentTxId=HdfsServerConstants.INVALID_TXID;      curSegmentLayoutVersion=0;    }    checkSync(nextTxId == endTxId + 1,"Trying to finalize in-progress log segment %s to end at " + "txid %s but only written up to txid %s ; journal id: %s",startTxId,endTxId,nextTxId - 1,journalId);    needsValidation=false;  }  FileJournalManager.EditLogFile elf=fjm.getLogFile(startTxId);  if (elf == null) {    throw new JournalOutOfSyncException("No log file to finalize at " + "transaction ID " + startTxId + " ; journal id: "+ journalId);  }  if (elf.isInProgress()) {    if (needsValidation) {
  checkRequest(reqInfo);  abortCurSegment();  long segmentTxId=segment.getStartTxId();  Preconditions.checkArgument(segment.getEndTxId() > 0 && segment.getEndTxId() >= segmentTxId,"bad recovery state for segment %s: %s ; journal id: %s",segmentTxId,TextFormat.shortDebugString(segment),journalId);  PersistedRecoveryPaxosData oldData=getPersistedPaxosData(segmentTxId);  PersistedRecoveryPaxosData newData=PersistedRecoveryPaxosData.newBuilder().setAcceptedInEpoch(reqInfo.getEpoch()).setSegmentState(segment).build();  if (oldData != null) {    alwaysAssert(oldData.getAcceptedInEpoch() <= reqInfo.getEpoch(),"Bad paxos transition, out-of-order epochs.\nOld: %s\nNew: " + "%s\nJournalId: %s\n",oldData,newData,journalId);  }  File syncedFile=null;  SegmentStateProto currentSegment=getSegmentInfo(segmentTxId);  if (currentSegment == null || currentSegment.getEndTxId() != segment.getEndTxId()) {    if (currentSegment == null) {      LOG.info("Synchronizing log " + TextFormat.shortDebugString(segment) + ": no current segment in place ; journal id: "+ journalId);      updateHighestWrittenTxId(Math.max(segment.getEndTxId(),highestWrittenTxId));    } else {
  }  File syncedFile=null;  SegmentStateProto currentSegment=getSegmentInfo(segmentTxId);  if (currentSegment == null || currentSegment.getEndTxId() != segment.getEndTxId()) {    if (currentSegment == null) {      LOG.info("Synchronizing log " + TextFormat.shortDebugString(segment) + ": no current segment in place ; journal id: "+ journalId);      updateHighestWrittenTxId(Math.max(segment.getEndTxId(),highestWrittenTxId));    } else {      LOG.info("Synchronizing log " + TextFormat.shortDebugString(segment) + ": old segment "+ TextFormat.shortDebugString(currentSegment)+ " is not the right length ; journal id: "+ journalId);      if (txnRange(currentSegment).contains(committedTxnId.get()) && !txnRange(segment).contains(committedTxnId.get())) {        throw new AssertionError("Cannot replace segment " + TextFormat.shortDebugString(currentSegment) + " with new segment "+ TextFormat.shortDebugString(segment)+ ": would discard already-committed txn "+ committedTxnId.get()+ " ; journal id: "+ journalId);      }      alwaysAssert(currentSegment.getIsInProgress(),"Should never be asked to synchronize a different log on top of " + "an already-finalized segment ; journal id: " + journalId);      if (txnRange(currentSegment).contains(highestWrittenTxId)) {        updateHighestWrittenTxId(segment.getEndTxId());      }    }    syncedFile=syncLog(reqInfo,segment,fromUrl);  } else {
    } else {      LOG.info("Synchronizing log " + TextFormat.shortDebugString(segment) + ": old segment "+ TextFormat.shortDebugString(currentSegment)+ " is not the right length ; journal id: "+ journalId);      if (txnRange(currentSegment).contains(committedTxnId.get()) && !txnRange(segment).contains(committedTxnId.get())) {        throw new AssertionError("Cannot replace segment " + TextFormat.shortDebugString(currentSegment) + " with new segment "+ TextFormat.shortDebugString(segment)+ ": would discard already-committed txn "+ committedTxnId.get()+ " ; journal id: "+ journalId);      }      alwaysAssert(currentSegment.getIsInProgress(),"Should never be asked to synchronize a different log on top of " + "an already-finalized segment ; journal id: " + journalId);      if (txnRange(currentSegment).contains(highestWrittenTxId)) {        updateHighestWrittenTxId(segment.getEndTxId());      }    }    syncedFile=syncLog(reqInfo,segment,fromUrl);  } else {    LOG.info("Skipping download of log " + TextFormat.shortDebugString(segment) + ": already have up-to-date logs ; journal id: "+ journalId);  }  JournalFaultInjector.get().beforePersistPaxosData();  persistPaxosData(segmentTxId,newData);  JournalFaultInjector.get().afterPersistPaxosData();  if (syncedFile != null) {    FileUtil.replaceFile(syncedFile,storage.getInProgressEditLog(segmentTxId));
public synchronized void doUpgrade(StorageInfo sInfo) throws IOException {  long oldCTime=storage.getCTime();  storage.cTime=sInfo.cTime;  int oldLV=storage.getLayoutVersion();  storage.layoutVersion=sInfo.layoutVersion;
synchronized boolean moveTmpSegmentToCurrent(File tmpFile,File finalFile,long endTxId) throws IOException {  final boolean success;  if (endTxId <= committedTxnId.get()) {    if (!finalFile.getParentFile().exists()) {
synchronized Journal getOrCreateJournal(String jid,String nameServiceId,StartupOption startOpt) throws IOException {  QuorumJournalManager.checkJournalId(jid);  Journal journal=journalsById.get(jid);  if (journal == null) {    File logDir=getLogDir(jid,nameServiceId);
  Preconditions.checkState(!isStarted(),"JN already running");  try {    for (    File journalDir : localDir) {      validateAndCreateJournalDir(journalDir);    }    DefaultMetricsSystem.initialize("JournalNode");    JvmMetrics.create("JournalNode",conf.get(DFSConfigKeys.DFS_METRICS_SESSION_ID_KEY),DefaultMetricsSystem.instance());    InetSocketAddress socAddr=JournalNodeRpcServer.getAddress(conf);    SecurityUtil.login(conf,DFSConfigKeys.DFS_JOURNALNODE_KEYTAB_FILE_KEY,DFSConfigKeys.DFS_JOURNALNODE_KERBEROS_PRINCIPAL_KEY,socAddr.getHostName());    registerJNMXBean();    httpServer=new JournalNodeHttpServer(conf,this,getHttpServerBindAddress(conf));    httpServer.start();    httpServerURI=httpServer.getServerURI().toString();    rpcServer=new JournalNodeRpcServer(conf,this);    rpcServer.start();  } catch (  IOException ioe) {
private boolean createEditsSyncDir(){  File editsSyncDir=journal.getStorage().getEditsSyncDir();  if (editsSyncDir.exists()) {
        if (!journal.isFormatted()) {          LOG.warn("Journal cannot sync. Not formatted.");        } else {          syncJournals();        }      } catch (      Throwable t) {        if (!shouldSync) {          if (t instanceof InterruptedException) {            LOG.info("Stopping JournalNode Sync.");            Thread.currentThread().interrupt();            return;          } else {            LOG.warn("JournalNodeSyncer received an exception while " + "shutting down.",t);          }          break;        } else {          if (t instanceof InterruptedException) {
  LOG.info("Syncing Journal " + jn.getBoundIpcAddress().getAddress() + ":"+ jn.getBoundIpcAddress().getPort()+ " with "+ otherJNProxies.get(index)+ ", journal id: "+ jid);  final InterQJournalProtocol jnProxy=otherJNProxies.get(index).jnProxy;  if (jnProxy == null) {    LOG.error("JournalNode Proxy not found.");    return;  }  List<RemoteEditLog> thisJournalEditLogs;  try {    thisJournalEditLogs=journal.getEditLogManifest(0,false).getLogs();  } catch (  IOException e) {    LOG.error("Exception in getting local edit log manifest",e);    return;  }  GetEditLogManifestResponseProto editLogManifest;  try {    editLogManifest=jnProxy.getEditLogManifestFromJournal(jid,nameServiceId,0,false);  } catch (  IOException e) {
  try {    uriStr=conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY);    if (uriStr == null || uriStr.isEmpty()) {      if (nameServiceId != null) {        uriStr=conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + "." + nameServiceId);      }    }    if (uriStr == null || uriStr.isEmpty()) {      HashSet<String> sharedEditsUri=Sets.newHashSet();      if (nameServiceId != null) {        Collection<String> nnIds=DFSUtilClient.getNameNodeIds(conf,nameServiceId);        for (        String nnId : nnIds) {          String suffix=nameServiceId + "." + nnId;          uriStr=conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + "." + suffix);          sharedEditsUri.add(uriStr);        }        if (sharedEditsUri.size() > 1) {          uriStr=null;
    if (uriStr == null || uriStr.isEmpty()) {      HashSet<String> sharedEditsUri=Sets.newHashSet();      if (nameServiceId != null) {        Collection<String> nnIds=DFSUtilClient.getNameNodeIds(conf,nameServiceId);        for (        String nnId : nnIds) {          String suffix=nameServiceId + "." + nnId;          uriStr=conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + "." + suffix);          sharedEditsUri.add(uriStr);        }        if (sharedEditsUri.size() > 1) {          uriStr=null;          LOG.error("The conf property " + DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + " not set properly, "+ "it has been configured with different journalnode values "+ sharedEditsUri.toString()+ " for a"+ " single nameserviceId"+ nameServiceId);        }      }    }    if (uriStr == null || uriStr.isEmpty()) {      LOG.error("Could not construct Shared Edits Uri");      return null;    } else {
      if (nameServiceId != null) {        Collection<String> nnIds=DFSUtilClient.getNameNodeIds(conf,nameServiceId);        for (        String nnId : nnIds) {          String suffix=nameServiceId + "." + nnId;          uriStr=conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + "." + suffix);          sharedEditsUri.add(uriStr);        }        if (sharedEditsUri.size() > 1) {          uriStr=null;          LOG.error("The conf property " + DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY + " not set properly, "+ "it has been configured with different journalnode values "+ sharedEditsUri.toString()+ " for a"+ " single nameserviceId"+ nameServiceId);        }      }    }    if (uriStr == null || uriStr.isEmpty()) {      LOG.error("Could not construct Shared Edits Uri");      return null;    } else {      return getJournalAddrList(uriStr);    }  } catch (  URISyntaxException e) {
    for (    RemoteEditLog missingLog : missingLogs) {      URL url=null;      boolean success=false;      try {        if (remoteJNproxy.httpServerUrl == null) {          if (response.hasFromURL()) {            remoteJNproxy.httpServerUrl=getHttpServerURI(response.getFromURL(),remoteJNproxy.jnAddr.getHostName());          } else {            LOG.error("EditLogManifest response does not have fromUrl " + "field set. Aborting current sync attempt");            break;          }        }        String urlPath=GetJournalEditServlet.buildPath(jid,missingLog.getStartTxId(),nsInfo,false);        url=new URL(remoteJNproxy.httpServerUrl,urlPath);        success=downloadMissingLogSegment(url,missingLog);      } catch (      URISyntaxException e) {        LOG.error("EditLogManifest's fromUrl field syntax incorrect",e);
      boolean success=false;      try {        if (remoteJNproxy.httpServerUrl == null) {          if (response.hasFromURL()) {            remoteJNproxy.httpServerUrl=getHttpServerURI(response.getFromURL(),remoteJNproxy.jnAddr.getHostName());          } else {            LOG.error("EditLogManifest response does not have fromUrl " + "field set. Aborting current sync attempt");            break;          }        }        String urlPath=GetJournalEditServlet.buildPath(jid,missingLog.getStartTxId(),nsInfo,false);        url=new URL(remoteJNproxy.httpServerUrl,urlPath);        success=downloadMissingLogSegment(url,missingLog);      } catch (      URISyntaxException e) {        LOG.error("EditLogManifest's fromUrl field syntax incorrect",e);      }catch (      MalformedURLException e) {        LOG.error("MalformedURL when download missing log segment",e);
public void checkAccess(BlockTokenIdentifier id,String userId,ExtendedBlock block,BlockTokenIdentifier.AccessMode mode) throws InvalidToken {  if (LOG.isDebugEnabled()) {
private static void addFileToTarGzRecursively(TarArchiveOutputStream tOut,File file,String prefix,Configuration conf) throws IOException {  String entryName=prefix + file.getName();  TarArchiveEntry tarEntry=new TarArchiveEntry(file,entryName);  tOut.putArchiveEntry(tarEntry);
public void start() throws IOException {  RPC.setProtocolEngine(getConf(),AliasMapProtocolPB.class,ProtobufRpcEngine2.class);  AliasMapProtocolServerSideTranslatorPB aliasMapProtocolXlator=new AliasMapProtocolServerSideTranslatorPB(this);  BlockingService aliasMapProtocolService=AliasMapProtocolService.newReflectiveBlockingService(aliasMapProtocolXlator);  InetSocketAddress rpcAddress=getBindAddress(conf,DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS,DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS_DEFAULT,DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_BIND_HOST);  boolean setVerbose=conf.getBoolean(DFS_PROVIDED_ALIASMAP_INMEMORY_SERVER_LOG,DFS_PROVIDED_ALIASMAP_INMEMORY_SERVER_LOG_DEFAULT);  aliasMapServer=new RPC.Builder(conf).setProtocol(AliasMapProtocolPB.class).setInstance(aliasMapProtocolService).setBindAddress(rpcAddress.getHostName()).setPort(rpcAddress.getPort()).setNumHandlers(1).setVerbose(setVerbose).build();
static long getLong(Configuration conf,String key,long defaultValue){  final long v=conf.getLong(key,defaultValue);
static long getLongBytes(Configuration conf,String key,long defaultValue){  final long v=conf.getLongBytes(key,defaultValue);
static int getInt(Configuration conf,String key,int defaultValue){  final int v=conf.getInt(key,defaultValue);
private static <T extends StorageGroup>void logUtilizationCollection(String name,Collection<T> items){
private void chooseStorageGroups(final Matcher matcher){
private void chooseStorageGroups(final Matcher matcher){  LOG.info("chooseStorageGroups for " + matcher + ": overUtilized => underUtilized");  chooseStorageGroups(overUtilized,underUtilized,matcher);
private void chooseStorageGroups(final Matcher matcher){  LOG.info("chooseStorageGroups for " + matcher + ": overUtilized => underUtilized");  chooseStorageGroups(overUtilized,underUtilized,matcher);  LOG.info("chooseStorageGroups for " + matcher + ": overUtilized => belowAvgUtilized");  chooseStorageGroups(overUtilized,belowAvgUtilized,matcher);
private void matchSourceWithTargetToMove(Source source,StorageGroup target){  long size=Math.min(source.availableSizeToMove(),target.availableSizeToMove());  final Task task=new Task(target,size);  source.addTask(task);  target.incScheduledSize(task.getSize());  dispatcher.add(source,target);
    final List<DatanodeStorageReport> reports=dispatcher.init();    final long bytesLeftToMove=init(reports);    if (bytesLeftToMove == 0) {      return newResult(ExitStatus.SUCCESS,bytesLeftToMove,0);    } else {      LOG.info("Need to move " + StringUtils.byteDesc(bytesLeftToMove) + " to make the cluster balanced.");    }    if (!runDuringUpgrade && nnc.isUpgrading()) {      System.err.println("Balancer exiting as upgrade is not finalized, " + "please finalize the HDFS upgrade before running the balancer.");      LOG.error("Balancer exiting as upgrade is not finalized, " + "please finalize the HDFS upgrade before running the balancer.");      return newResult(ExitStatus.UNFINALIZED_UPGRADE,bytesLeftToMove,-1);    }    final long bytesBeingMoved=chooseStorageGroups();    if (bytesBeingMoved == 0) {      System.out.println("No block can be moved. Exiting...");      return newResult(ExitStatus.NO_MOVE_BLOCK,bytesLeftToMove,bytesBeingMoved);    } else {
static private int doBalance(Collection<URI> namenodes,Collection<String> nsIds,final BalancerParameters p,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS) * 2 + conf.getTimeDuration(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);
static private int doBalance(Collection<URI> namenodes,Collection<String> nsIds,final BalancerParameters p,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS) * 2 + conf.getTimeDuration(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);  LOG.info("namenodes  = " + namenodes);
static private int doBalance(Collection<URI> namenodes,Collection<String> nsIds,final BalancerParameters p,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS) * 2 + conf.getTimeDuration(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);  LOG.info("namenodes  = " + namenodes);  LOG.info("parameters = " + p);
static private int doBalance(Collection<URI> namenodes,Collection<String> nsIds,final BalancerParameters p,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS) * 2 + conf.getTimeDuration(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);  LOG.info("namenodes  = " + namenodes);  LOG.info("parameters = " + p);  LOG.info("included nodes = " + p.getIncludedNodes());
static private int doBalance(Collection<URI> namenodes,Collection<String> nsIds,final BalancerParameters p,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS) * 2 + conf.getTimeDuration(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);  LOG.info("namenodes  = " + namenodes);  LOG.info("parameters = " + p);  LOG.info("included nodes = " + p.getIncludedNodes());  LOG.info("excluded nodes = " + p.getExcludedNodes());
    boolean done=false;    for (int iteration=0; !done; iteration++) {      done=true;      Collections.shuffle(connectors);      for (      NameNodeConnector nnc : connectors) {        if (p.getBlockPools().size() == 0 || p.getBlockPools().contains(nnc.getBlockpoolID())) {          final Balancer b=new Balancer(nnc,p,conf);          final Result r=b.runOneIteration();          r.print(iteration,System.out);          b.resetData(conf);          if (r.exitStatus == ExitStatus.IN_PROGRESS) {            done=false;          } else           if (r.exitStatus != ExitStatus.SUCCESS) {            return r.exitStatus.getExitCode();          }        } else {
  }  long scheduleInterval=conf.getTimeDuration(DFSConfigKeys.DFS_BALANCER_SERVICE_INTERVAL_KEY,DFSConfigKeys.DFS_BALANCER_SERVICE_INTERVAL_DEFAULT,TimeUnit.MILLISECONDS);  int retryOnException=conf.getInt(DFSConfigKeys.DFS_BALANCER_SERVICE_RETRIES_ON_EXCEPTION,DFSConfigKeys.DFS_BALANCER_SERVICE_RETRIES_ON_EXCEPTION_DEFAULT);  while (serviceRunning) {    try {      int retCode=doBalance(namenodes,nsIds,p,conf);      if (retCode < 0) {        LOG.info("Balance failed, error code: " + retCode);        failedTimesSinceLastSuccessfulBalance++;      } else {        LOG.info("Balance succeed!");        failedTimesSinceLastSuccessfulBalance=0;      }      exceptionsSinceLastBalance=0;    } catch (    Exception e) {      if (++exceptionsSinceLastBalance > retryOnException) {        throw e;
  if (getBlocksRateLimiter != null) {    getBlocksRateLimiter.acquire();  }  boolean isRequestStandby=false;  NamenodeProtocol nnproxy=null;  try {    if (requestToStandby && nsId != null && HAUtil.isHAEnabled(config,nsId)) {      List<ClientProtocol> namenodes=HAUtil.getProxiesForAllNameNodesInNameservice(config,nsId);      for (      ClientProtocol proxy : namenodes) {        try {          if (proxy.getHAServiceState().equals(HAServiceProtocol.HAServiceState.STANDBY)) {            NamenodeProtocol sbn=NameNodeProxies.createNonHAProxy(config,RPC.getServerAddress(proxy),NamenodeProtocol.class,UserGroupInformation.getCurrentUser(),false).getProxy();            nnproxy=sbn;            isRequestStandby=true;            break;          }        } catch (        Exception e) {
@Override public void initialize(Configuration conf,FSClusterStats stats,NetworkTopology clusterMap,Host2NodesMap host2datanodeMap){  super.initialize(conf,stats,clusterMap,host2datanodeMap);  float balancedPreferencePercent=conf.getFloat(DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);
@Override public void initialize(Configuration conf,FSClusterStats stats,NetworkTopology clusterMap,Host2NodesMap host2datanodeMap){  super.initialize(conf,stats,clusterMap,host2datanodeMap);  float balancedPreferencePercent=conf.getFloat(DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_RACK_FAULT_TOLERANT_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);
private static BlockTokenSecretManager createBlockTokenSecretManager(final Configuration conf) throws IOException {  final boolean isEnabled=conf.getBoolean(DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY,DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_DEFAULT);
private void markBlockAsCorrupt(BlockToMarkCorrupt b,DatanodeStorageInfo storageInfo,DatanodeDescriptor node) throws IOException {  if (b.getStored().isDeleted()) {
private boolean invalidateBlock(BlockToMarkCorrupt b,DatanodeInfo dn,NumberReplicas nr) throws IOException {
    }    final BlockPlacementPolicy placementPolicy=placementPolicies.getPolicy(rw.getBlock().getBlockType());    rw.chooseTargets(placementPolicy,storagePolicySuite,excludedNodes);  }  namesystem.writeLock();  try {    for (    BlockReconstructionWork rw : reconWork) {      final DatanodeStorageInfo[] targets=rw.getTargets();      if (targets == null || targets.length == 0) {        rw.resetTargets();        continue;      }synchronized (neededReconstruction) {        if (validateReconstructionWork(rw)) {          scheduledWork++;        }      }    }  }  finally {    namesystem.writeUnlock();  }  if (blockLog.isDebugEnabled()) {
    final BlockPlacementPolicy placementPolicy=placementPolicies.getPolicy(rw.getBlock().getBlockType());    rw.chooseTargets(placementPolicy,storagePolicySuite,excludedNodes);  }  namesystem.writeLock();  try {    for (    BlockReconstructionWork rw : reconWork) {      final DatanodeStorageInfo[] targets=rw.getTargets();      if (targets == null || targets.length == 0) {        rw.resetTargets();        continue;      }synchronized (neededReconstruction) {        if (validateReconstructionWork(rw)) {          scheduledWork++;        }      }    }  }  finally {    namesystem.writeUnlock();  }  if (blockLog.isDebugEnabled()) {
  final int pendingNum=pendingReconstruction.getNumReplicas(block);  if (hasEnoughEffectiveReplicas(block,numReplicas,pendingNum)) {    neededReconstruction.remove(block,priority);    rw.resetTargets();    blockLog.debug("BLOCK* Removing {} from neededReconstruction as" + " it has enough replicas",block);    return false;  }  DatanodeStorageInfo[] targets=rw.getTargets();  BlockPlacementStatus placementStatus=getBlockPlacementStatus(block);  if ((numReplicas.liveReplicas() >= requiredRedundancy) && (!placementStatus.isPlacementPolicySatisfied())) {    BlockPlacementStatus newPlacementStatus=getBlockPlacementStatus(block,targets);    if (!newPlacementStatus.isPlacementPolicySatisfied() && (newPlacementStatus.getAdditionalReplicasRequired() >= placementStatus.getAdditionalReplicasRequired())) {      return false;    }    rw.setNotEnoughRack();  }  rw.addTaskToDatanode(numReplicas);  DatanodeStorageInfo.incrementBlocksScheduled(targets);
public boolean processReport(final DatanodeID nodeID,final DatanodeStorage storage,final BlockListAsLongs newReport,BlockReportContext context) throws IOException {  namesystem.writeLock();  final long startTime=Time.monotonicNow();  final long endTime;  DatanodeDescriptor node;  Collection<Block> invalidatedBlocks=Collections.emptyList();  String strBlockReportId=context != null ? Long.toHexString(context.getReportId()) : "";  try {    node=datanodeManager.getDatanode(nodeID);    if (node == null || !node.isRegistered()) {      throw new IOException("ProcessReport from dead or unregistered node: " + nodeID);    }    DatanodeStorageInfo storageInfo=providedStorageMap.getStorage(node,storage);    if (storageInfo == null) {      storageInfo=node.updateStorage(storage);    }    if (namesystem.isInStartupSafeMode() && !StorageType.PROVIDED.equals(storageInfo.getStorageType()) && storageInfo.getBlockReportCount() > 0) {
  DatanodeDescriptor node;  Collection<Block> invalidatedBlocks=Collections.emptyList();  String strBlockReportId=context != null ? Long.toHexString(context.getReportId()) : "";  try {    node=datanodeManager.getDatanode(nodeID);    if (node == null || !node.isRegistered()) {      throw new IOException("ProcessReport from dead or unregistered node: " + nodeID);    }    DatanodeStorageInfo storageInfo=providedStorageMap.getStorage(node,storage);    if (storageInfo == null) {      storageInfo=node.updateStorage(storage);    }    if (namesystem.isInStartupSafeMode() && !StorageType.PROVIDED.equals(storageInfo.getStorageType()) && storageInfo.getBlockReportCount() > 0) {      blockLog.info("BLOCK* processReport 0x{}: " + "discarded non-initial block report from {}" + " because namenode still in startup phase",strBlockReportId,nodeID);      blockReportLeaseManager.removeLease(node);      return !node.hasStaleStorages();    }    if (storageInfo.getBlockReportCount() == 0) {
      storageInfo=node.updateStorage(storage);    }    if (namesystem.isInStartupSafeMode() && !StorageType.PROVIDED.equals(storageInfo.getStorageType()) && storageInfo.getBlockReportCount() > 0) {      blockLog.info("BLOCK* processReport 0x{}: " + "discarded non-initial block report from {}" + " because namenode still in startup phase",strBlockReportId,nodeID);      blockReportLeaseManager.removeLease(node);      return !node.hasStaleStorages();    }    if (storageInfo.getBlockReportCount() == 0) {      blockLog.info("BLOCK* processReport 0x{}: Processing first " + "storage report for {} from datanode {}",strBlockReportId,storageInfo.getStorageID(),nodeID.getDatanodeUuid());      processFirstBlockReport(storageInfo,newReport);    } else {      if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {        invalidatedBlocks=processReport(storageInfo,newReport,context);      }    }    storageInfo.receivedBlockReport();  }  finally {    endTime=Time.monotonicNow();    namesystem.writeUnlock();
      return !node.hasStaleStorages();    }    if (storageInfo.getBlockReportCount() == 0) {      blockLog.info("BLOCK* processReport 0x{}: Processing first " + "storage report for {} from datanode {}",strBlockReportId,storageInfo.getStorageID(),nodeID.getDatanodeUuid());      processFirstBlockReport(storageInfo,newReport);    } else {      if (!StorageType.PROVIDED.equals(storageInfo.getStorageType())) {        invalidatedBlocks=processReport(storageInfo,newReport,context);      }    }    storageInfo.receivedBlockReport();  }  finally {    endTime=Time.monotonicNow();    namesystem.writeUnlock();  }  for (  Block b : invalidatedBlocks) {    blockLog.debug("BLOCK* processReport 0x{}: {} on node {} size {} does not" + " belong to any file",strBlockReportId,b,node,b.getNumBytes());  }  final NameNodeMetrics metrics=NameNode.getNameNodeMetrics();  if (metrics != null) {
  if (getPostponedMisreplicatedBlocksCount() == 0) {    return;  }  namesystem.writeLock();  long startTime=Time.monotonicNow();  long startSize=postponedMisreplicatedBlocks.size();  try {    Iterator<Block> it=postponedMisreplicatedBlocks.iterator();    for (int i=0; i < blocksPerPostpondedRescan && it.hasNext(); i++) {      Block b=it.next();      it.remove();      BlockInfo bi=getStoredBlock(b);      if (bi == null) {        LOG.debug("BLOCK* rescanPostponedMisreplicatedBlocks: " + "Postponed mis-replicated block {} no longer found " + "in block map.",b);        continue;      }      MisReplicationResult res=processMisReplicatedBlock(bi);
    for (int i=0; i < blocksPerPostpondedRescan && it.hasNext(); i++) {      Block b=it.next();      it.remove();      BlockInfo bi=getStoredBlock(b);      if (bi == null) {        LOG.debug("BLOCK* rescanPostponedMisreplicatedBlocks: " + "Postponed mis-replicated block {} no longer found " + "in block map.",b);        continue;      }      MisReplicationResult res=processMisReplicatedBlock(bi);      LOG.debug("BLOCK* rescanPostponedMisreplicatedBlocks: " + "Re-scanned block {}, result is {}",b,res);      if (res == MisReplicationResult.POSTPONE) {        rescannedMisreplicatedBlocks.add(b);      }    }  }  finally {    postponedMisreplicatedBlocks.addAll(rescannedMisreplicatedBlocks);    rescannedMisreplicatedBlocks.clear();    long endSize=postponedMisreplicatedBlocks.size();
    for (    BlockReportReplica iblk : report) {      set.add(new BlockReportReplica(iblk));    }    sortedReport=set;  } else {    sortedReport=report;  }  reportDiffSorted(storageInfo,sortedReport,toAdd,toRemove,toInvalidate,toCorrupt,toUC);  DatanodeDescriptor node=storageInfo.getDatanodeDescriptor();  for (  StatefulBlockInfo b : toUC) {    addStoredBlockUnderConstruction(b,storageInfo);  }  for (  BlockInfo b : toRemove) {    removeStoredBlock(b,node);  }  int numBlocksLogged=0;  for (  BlockInfoToAdd b : toAdd) {    addStoredBlock(b.stored,b.reported,storageInfo,null,numBlocksLogged < maxNumBlocksToLog);    numBlocksLogged++;
void processFirstBlockReport(final DatanodeStorageInfo storageInfo,final BlockListAsLongs report) throws IOException {  if (report == null)   return;  assert(namesystem.hasWriteLock());  assert(storageInfo.getBlockReportCount() == 0);  for (  BlockReportReplica iblk : report) {    ReplicaState reportedState=iblk.getState();    if (LOG.isDebugEnabled()) {
private void queueReportedBlock(DatanodeStorageInfo storageInfo,Block block,ReplicaState reportedState,String reason){  assert shouldPostponeBlocksFromFuture;
private void processQueuedMessages(Iterable<ReportedBlockInfo> rbis) throws IOException {  boolean isPreviousMessageProcessed=true;  for (  ReportedBlockInfo rbi : rbis) {
nrPostponed++;postponeBlock(block);break;case UNDER_CONSTRUCTION:LOG.trace("under construction block {}: {}",block,res);nrUnderConstruction++;break;case OK:break;default:throw new AssertionError("Invalid enum value: " + res);}processed++;}totalProcessed+=processed;reconstructionQueuesInitProgress=Math.min((double)totalProcessed / totalBlocks,1.0);if (!blocksItr.hasNext()) {LOG.info("Total number of blocks            = {}",blocksMap.size());LOG.info("Number of invalid blocks          = {}",nrInvalid);LOG.info("Number of under-replicated blocks = {}",nrUnderReplicated);
postponeBlock(block);break;case UNDER_CONSTRUCTION:LOG.trace("under construction block {}: {}",block,res);nrUnderConstruction++;break;case OK:break;default:throw new AssertionError("Invalid enum value: " + res);}processed++;}totalProcessed+=processed;reconstructionQueuesInitProgress=Math.min((double)totalProcessed / totalBlocks,1.0);if (!blocksItr.hasNext()) {LOG.info("Total number of blocks            = {}",blocksMap.size());LOG.info("Number of invalid blocks          = {}",nrInvalid);LOG.info("Number of under-replicated blocks = {}",nrUnderReplicated);LOG.info("Number of  over-replicated blocks = {}{}",nrOverReplicated,((nrPostponed > 0) ? (" (" + nrPostponed + " postponed)") : ""));
public int processMisReplicatedBlocks(List<BlockInfo> blocks){  int processed=0;  Iterator<BlockInfo> iter=blocks.iterator();  try {    while (isPopulatingReplQueues() && namesystem.isRunning() && !Thread.currentThread().isInterrupted()&& iter.hasNext()) {      int limit=processed + numBlocksPerIteration;      namesystem.writeLockInterruptibly();      try {        while (iter.hasNext() && processed < limit) {          BlockInfo blk=iter.next();          MisReplicationResult r=processMisReplicatedBlock(blk);          processed++;
private void processChosenExcessRedundancy(final Collection<DatanodeStorageInfo> nonExcess,final DatanodeStorageInfo chosen,BlockInfo storedBlock){  nonExcess.remove(chosen);  excessRedundancyMap.add(chosen.getDatanodeDescriptor(),storedBlock);  final Block blockToInvalidate=getBlockOnStorage(storedBlock,chosen);  addToInvalidates(blockToInvalidate,chosen.getDatanodeDescriptor());
public void removeStoredBlock(BlockInfo storedBlock,DatanodeDescriptor node){
private boolean processAndHandleReportedBlock(DatanodeStorageInfo storageInfo,Block block,ReplicaState reportedState,DatanodeDescriptor delHintNode) throws IOException {  final DatanodeDescriptor node=storageInfo.getDatanodeDescriptor();
    while (it.hasNext()) {      final BlockInfo block=it.next();      if (block.isDeleted()) {        continue;      }      int expectedReplication=this.getExpectedRedundancyNum(block);      NumberReplicas num=countNodes(block);      if (shouldProcessExtraRedundancy(num,expectedReplication)) {        processExtraRedundancyBlock(block,(short)expectedReplication,null,null);        numExtraRedundancy++;      }    }    if (namesystem.hasWriteLock()) {      namesystem.writeUnlock();      try {        Thread.sleep(1);      } catch (      InterruptedException e) {        Thread.currentThread().interrupt();
boolean isNodeHealthyForDecommissionOrMaintenance(DatanodeDescriptor node){  if (!node.checkBlockReportReceived()) {
    final EnumMap<StorageType,Integer> storageTypes=getRequiredStorageTypes(requiredStorageTypes);    List<DatanodeStorageInfo> results=new ArrayList<>();    boolean avoidStaleNodes=stats != null && stats.isAvoidingStaleDataNodesForWrite();    int maxNodesAndReplicas[]=getMaxNodesPerRack(0,numOfReplicas);    numOfReplicas=maxNodesAndReplicas[0];    int maxNodesPerRack=maxNodesAndReplicas[1];    chooseFavouredNodes(src,numOfReplicas,favoredNodes,favoriteAndExcludedNodes,blocksize,maxNodesPerRack,results,avoidStaleNodes,storageTypes);    if (results.size() < numOfReplicas) {      numOfReplicas-=results.size();      for (      DatanodeStorageInfo storage : results) {        addToExcludedNodes(storage.getDatanodeDescriptor(),favoriteAndExcludedNodes);      }      DatanodeStorageInfo[] remainingTargets=chooseTarget(src,numOfReplicas,writer,new ArrayList<DatanodeStorageInfo>(numOfReplicas),false,favoriteAndExcludedNodes,blocksize,storagePolicy,flags,storageTypes);      for (int i=0; i < remainingTargets.length; i++) {        results.add(remainingTargets[i]);      }    }    return getPipeline(writer,results.toArray(new DatanodeStorageInfo[results.size()]));
  final Map<String,List<DatanodeStorageInfo>> rackMap=new HashMap<>();  final List<DatanodeStorageInfo> moreThanOne=new ArrayList<>();  final List<DatanodeStorageInfo> exactlyOne=new ArrayList<>();  splitNodesWithRack(availableReplicas,delCandidates,rackMap,moreThanOne,exactlyOne);  boolean firstOne=true;  final DatanodeStorageInfo delNodeHintStorage=DatanodeStorageInfo.getDatanodeStorageInfo(delCandidates,delNodeHint);  final DatanodeStorageInfo addedNodeStorage=DatanodeStorageInfo.getDatanodeStorageInfo(delCandidates,addedNode);  while (delCandidates.size() - expectedNumOfReplicas > excessReplicas.size()) {    final DatanodeStorageInfo cur;    if (firstOne && useDelHint(delNodeHintStorage,addedNodeStorage,moreThanOne,exactlyOne,excessTypes)) {      cur=delNodeHintStorage;    } else {      cur=chooseReplicaToDelete(moreThanOne,exactlyOne,excessTypes,rackMap);    }    firstOne=false;    if (cur == null) {
    final Set<Node> newExcludeNodes=new HashSet<>();    for (    DatanodeStorageInfo resultStorage : results) {      addToExcludedNodes(resultStorage.getDatanodeDescriptor(),newExcludeNodes);    }    LOG.trace("Chosen nodes: {}",results);    LOG.trace("Excluded nodes: {}",excludedNodes);    LOG.trace("New Excluded nodes: {}",newExcludeNodes);    final int numOfReplicas=totalReplicaExpected - results.size();    numResultsOflastChoose=results.size();    try {      chooseOnce(numOfReplicas,writer,newExcludeNodes,blocksize,++bestEffortMaxNodesPerRack,results,avoidStaleNodes,storageTypes);    } catch (    NotEnoughReplicasException nere) {      lastException=nere;    } finally {      excludedNodes.addAll(newExcludeNodes);    }  }  if (numResultsOflastChoose != totalReplicaExpected) {
private synchronized NodeData registerNode(DatanodeDescriptor dn){  if (nodes.containsKey(dn.getDatanodeUuid())) {
public synchronized void unregister(DatanodeDescriptor dn){  NodeData node=nodes.remove(dn.getDatanodeUuid());  if (node == null) {
  remove(node);  long monotonicNowMs=Time.monotonicNow();  pruneExpiredPending(monotonicNowMs);  if (numPending >= maxPending) {    if (LOG.isDebugEnabled()) {      StringBuilder allLeases=new StringBuilder();      String prefix="";      for (NodeData cur=pendingHead.next; cur != pendingHead; cur=cur.next) {        allLeases.append(prefix).append(cur.datanodeUuid);        prefix=", ";      }      LOG.debug("Can't create a new BR lease for DN {}, because " + "numPending equals maxPending at {}.  Current leases: {}",dn.getDatanodeUuid(),numPending,allLeases.toString());    }    return 0;  }  numPending++;  node.leaseId=getNextId();  node.leaseTimeMs=monotonicNowMs;
public synchronized boolean checkLease(DatanodeDescriptor dn,long monotonicNowMs,long id){  if (id == 0) {
public synchronized long removeLease(DatanodeDescriptor dn){  NodeData node=nodes.get(dn.getDatanodeUuid());  if (node == null) {
  LOG.info("Starting CacheReplicationMonitor with interval " + intervalMs + " milliseconds");  try {    long curTimeMs=Time.monotonicNow();    while (true) {      lock.lock();      try {        while (true) {          if (shutdown) {            LOG.info("Shutting down CacheReplicationMonitor");            return;          }          if (completedScanCount < neededScanCount) {            LOG.debug("Rescanning because of pending operations");            break;          }          long delta=(startTimeMs + intervalMs) - curTimeMs;          if (delta <= 0) {
            break;          }          doRescan.await(delta,TimeUnit.MILLISECONDS);          curTimeMs=Time.monotonicNow();        }      }  finally {        lock.unlock();      }      startTimeMs=curTimeMs;      mark=!mark;      rescan();      curTimeMs=Time.monotonicNow();      lock.lock();      try {        completedScanCount=curScanCount;        curScanCount=-1;        scanFinished.signalAll();      }  finally {
private void rescanCacheDirectives(){  FSDirectory fsDir=namesystem.getFSDirectory();  final long now=new Date().getTime();  for (  CacheDirective directive : cacheManager.getCacheDirectives()) {    scannedDirectives++;    if (directive.getExpiryTime() > 0 && directive.getExpiryTime() <= now) {
  FSDirectory fsDir=namesystem.getFSDirectory();  final long now=new Date().getTime();  for (  CacheDirective directive : cacheManager.getCacheDirectives()) {    scannedDirectives++;    if (directive.getExpiryTime() > 0 && directive.getExpiryTime() <= now) {      LOG.debug("Directive {}: the directive expired at {} (now = {})",directive.getId(),directive.getExpiryTime(),now);      continue;    }    String path=directive.getPath();    INode node;    try {      node=fsDir.getINode(path,DirOp.READ);    } catch (    IOException e) {      LOG.debug("Directive {}: Failed to resolve path {} ({})",directive.getId(),path,e.getMessage());      continue;    }    if (node == null) {
    try {      node=fsDir.getINode(path,DirOp.READ);    } catch (    IOException e) {      LOG.debug("Directive {}: Failed to resolve path {} ({})",directive.getId(),path,e.getMessage());      continue;    }    if (node == null) {      LOG.debug("Directive {}: No inode found at {}",directive.getId(),path);    } else     if (node.isDirectory()) {      INodeDirectory dir=node.asDirectory();      ReadOnlyList<INode> children=dir.getChildrenList(Snapshot.CURRENT_STATE_ID);      for (      INode child : children) {        if (child.isFile()) {          rescanFile(directive,child.asFile());        }      }    } else     if (node.isFile()) {      rescanFile(directive,node.asFile());
private void rescanFile(CacheDirective directive,INodeFile file){  BlockInfo[] blockInfos=file.getBlocks();  directive.addFilesNeeded(1);  long neededTotal=file.computeFileSizeNotIncludingLastUcBlock() * directive.getReplication();  directive.addBytesNeeded(neededTotal);  CachePool pool=directive.getPool();  if (pool.getBytesNeeded() > pool.getLimit()) {
    Block block=new Block(blockInfo.getBlockId());    CachedBlock ncblock=new CachedBlock(block.getBlockId(),directive.getReplication(),mark);    CachedBlock ocblock=cachedBlocks.get(ncblock);    if (ocblock == null) {      cachedBlocks.put(ncblock);      ocblock=ncblock;    } else {      List<DatanodeDescriptor> cachedOn=ocblock.getDatanodes(Type.CACHED);      long cachedByBlock=Math.min(cachedOn.size(),directive.getReplication()) * blockInfo.getNumBytes();      cachedTotal+=cachedByBlock;      if ((mark != ocblock.getMark()) || (ocblock.getReplication() < directive.getReplication())) {        ocblock.setReplicationAndMark(directive.getReplication(),mark);      }    }    LOG.trace("Directive {}: setting replication for block {} to {}",directive.getId(),blockInfo,ocblock.getReplication());  }  directive.addBytesCached(cachedTotal);  if (cachedTotal == neededTotal) {
private void rescanCachedBlockMap(){  Set<DatanodeDescriptor> datanodes=blockManager.getDatanodeManager().getDatanodes();  for (  DatanodeDescriptor dn : datanodes) {    long remaining=dn.getCacheRemaining();    for (Iterator<CachedBlock> it=dn.getPendingCached().iterator(); it.hasNext(); ) {      CachedBlock cblock=it.next();      BlockInfo blockInfo=blockManager.getStoredBlock(new Block(cblock.getBlockId()));      if (blockInfo == null) {
private void addNewPendingCached(final int neededCached,CachedBlock cachedBlock,List<DatanodeDescriptor> cached,List<DatanodeDescriptor> pendingCached){  BlockInfo blockInfo=blockManager.getStoredBlock(new Block(cachedBlock.getBlockId()));  if (blockInfo == null) {
      if (info != null) {        pendingBytes-=info.getNumBytes();      }    }    it=datanode.getPendingUncached().iterator();    while (it.hasNext()) {      CachedBlock cBlock=it.next();      BlockInfo info=blockManager.getStoredBlock(new Block(cBlock.getBlockId()));      if (info != null) {        pendingBytes+=info.getNumBytes();      }    }    long pendingCapacity=pendingBytes + datanode.getCacheRemaining();    if (pendingCapacity < blockInfo.getNumBytes()) {      LOG.trace("Block {}: DataNode {} is not a valid possibility " + "because the block has size {}, but the DataNode only has {} " + "bytes of cache remaining ({} pending bytes, {} already cached.)",blockInfo.getBlockId(),datanode.getDatanodeUuid(),blockInfo.getNumBytes(),pendingCapacity,pendingBytes,datanode.getCacheRemaining());      outOfCapacity++;      continue;    }    possibilities.add(datanode);  }  List<DatanodeDescriptor> chosen=chooseDatanodesForCaching(possibilities,neededCached,blockManager.getDatanodeManager().getStaleInterval());
@Override protected void processConf(){  this.pendingRepLimit=conf.getInt(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BACKOFF_MONITOR_PENDING_LIMIT,DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BACKOFF_MONITOR_PENDING_LIMIT_DEFAULT);  if (this.pendingRepLimit < 1) {
  if (!namesystem.isRunning()) {    LOG.info("Namesystem is not running, skipping " + "decommissioning/maintenance checks.");    return;  }  numBlocksChecked=0;  try {    namesystem.writeLock();    try {      processCancelledNodes();      processPendingNodes();    }  finally {      namesystem.writeUnlock();    }    check();  } catch (  Exception e) {    LOG.warn("DatanodeAdminMonitor caught exception when processing node.",e);  }  if (numBlocksChecked + outOfServiceNodeBlocks.size() > 0) {
  if (toRemove.size() == 0) {    return;  }  namesystem.writeLock();  try {    for (    DatanodeDescriptor dn : toRemove) {      final boolean isHealthy=blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);      if (isHealthy) {        if (dn.isDecommissionInProgress()) {          dnAdmin.setDecommissioned(dn);          outOfServiceNodeBlocks.remove(dn);          pendingRep.remove(dn);        } else         if (dn.isEnteringMaintenance()) {          dnAdmin.setInMaintenance(dn);          pendingRep.remove(dn);        } else         if (dn.isInService()) {
    for (    DatanodeDescriptor dn : toRemove) {      final boolean isHealthy=blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);      if (isHealthy) {        if (dn.isDecommissionInProgress()) {          dnAdmin.setDecommissioned(dn);          outOfServiceNodeBlocks.remove(dn);          pendingRep.remove(dn);        } else         if (dn.isEnteringMaintenance()) {          dnAdmin.setInMaintenance(dn);          pendingRep.remove(dn);        } else         if (dn.isInService()) {          LOG.info("Node {} completed decommission and maintenance " + "but has been moved back to in service",dn);          pendingRep.remove(dn);          outOfServiceNodeBlocks.remove(dn);          continue;
          outOfServiceNodeBlocks.remove(dn);          pendingRep.remove(dn);        } else         if (dn.isEnteringMaintenance()) {          dnAdmin.setInMaintenance(dn);          pendingRep.remove(dn);        } else         if (dn.isInService()) {          LOG.info("Node {} completed decommission and maintenance " + "but has been moved back to in service",dn);          pendingRep.remove(dn);          outOfServiceNodeBlocks.remove(dn);          continue;        } else {          LOG.error("Node {} is in an unexpected state {} and has been " + "removed from tracking for decommission or maintenance",dn,dn.getAdminState());          pendingRep.remove(dn);          outOfServiceNodeBlocks.remove(dn);          continue;
          pendingRep.remove(dn);        } else         if (dn.isEnteringMaintenance()) {          dnAdmin.setInMaintenance(dn);          pendingRep.remove(dn);        } else         if (dn.isInService()) {          LOG.info("Node {} completed decommission and maintenance " + "but has been moved back to in service",dn);          pendingRep.remove(dn);          outOfServiceNodeBlocks.remove(dn);          continue;        } else {          LOG.error("Node {} is in an unexpected state {} and has been " + "removed from tracking for decommission or maintenance",dn,dn.getAdminState());          pendingRep.remove(dn);          outOfServiceNodeBlocks.remove(dn);          continue;        }        LOG.info("Node {} is sufficiently replicated and healthy, " + "marked as {}.",dn,dn.getAdminState());
private void checkForCompletedNodes(List<DatanodeDescriptor> removeList){  for (  DatanodeDescriptor dn : outOfServiceNodeBlocks.keySet()) {    if (dn.isInMaintenance()) {
      DatanodeDescriptor dn=nodeIter.next();      Iterator<BlockInfo> blockIt=iterators.get(dn);      while (blockIt.hasNext()) {        if (blocksProcessed >= blocksPerLock) {          blocksProcessed=0;          namesystem.writeUnlock();          namesystem.writeLock();        }        blocksProcessed++;        if (nextBlockAddedToPending(blockIt,dn)) {          pendingCount++;          break;        }      }      if (!blockIt.hasNext()) {        nodeIter.remove();      }      if (pendingCount >= pendingRepLimit) {        break;
@Override protected void processConf(){  numBlocksPerCheck=conf.getInt(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY,DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_DEFAULT);  if (numBlocksPerCheck <= 0) {
  if (!namesystem.isRunning()) {    LOG.info("Namesystem is not running, skipping " + "decommissioning/maintenance checks.");    return;  }  numBlocksChecked=0;  numBlocksCheckedPerLock=0;  numNodesChecked=0;  namesystem.writeLock();  try {    processPendingNodes();    check();  } catch (  Exception e) {    LOG.warn("DatanodeAdminMonitor caught exception when processing node.",e);  } finally {    namesystem.writeUnlock();  }  if (numBlocksChecked + numNodesChecked > 0) {
  final List<DatanodeDescriptor> toRemove=new ArrayList<>();  while (it.hasNext() && !exceededNumBlocksPerCheck() && namesystem.isRunning()) {    numNodesChecked++;    final Map.Entry<DatanodeDescriptor,AbstractList<BlockInfo>> entry=it.next();    final DatanodeDescriptor dn=entry.getKey();    try {      AbstractList<BlockInfo> blocks=entry.getValue();      boolean fullScan=false;      if (dn.isMaintenance() && dn.maintenanceExpired()) {        dnAdmin.stopMaintenance(dn);        toRemove.add(dn);        continue;      }      if (dn.isInMaintenance()) {        continue;      }      if (blocks == null) {
    try {      AbstractList<BlockInfo> blocks=entry.getValue();      boolean fullScan=false;      if (dn.isMaintenance() && dn.maintenanceExpired()) {        dnAdmin.stopMaintenance(dn);        toRemove.add(dn);        continue;      }      if (dn.isInMaintenance()) {        continue;      }      if (blocks == null) {        LOG.debug("Newly-added node {}, doing full scan to find " + "insufficiently-replicated blocks.",dn);        blocks=handleInsufficientlyStored(dn);        outOfServiceNodeBlocks.put(dn,blocks);        fullScan=true;      } else {
        dnAdmin.stopMaintenance(dn);        toRemove.add(dn);        continue;      }      if (dn.isInMaintenance()) {        continue;      }      if (blocks == null) {        LOG.debug("Newly-added node {}, doing full scan to find " + "insufficiently-replicated blocks.",dn);        blocks=handleInsufficientlyStored(dn);        outOfServiceNodeBlocks.put(dn,blocks);        fullScan=true;      } else {        LOG.debug("Processing {} node {}",dn.getAdminState(),dn);        pruneReliableBlocks(dn,blocks);      }      if (blocks.size() == 0) {        if (!fullScan) {
 else {        LOG.debug("Processing {} node {}",dn.getAdminState(),dn);        pruneReliableBlocks(dn,blocks);      }      if (blocks.size() == 0) {        if (!fullScan) {          LOG.debug("Node {} has finished replicating current set of " + "blocks, checking with the full block map.",dn);          blocks=handleInsufficientlyStored(dn);          outOfServiceNodeBlocks.put(dn,blocks);        }        final boolean isHealthy=blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);        if (blocks.size() == 0 && isHealthy) {          if (dn.isDecommissionInProgress()) {            dnAdmin.setDecommissioned(dn);            toRemove.add(dn);          } else           if (dn.isEnteringMaintenance()) {            dnAdmin.setInMaintenance(dn);
        LOG.debug("Processing {} node {}",dn.getAdminState(),dn);        pruneReliableBlocks(dn,blocks);      }      if (blocks.size() == 0) {        if (!fullScan) {          LOG.debug("Node {} has finished replicating current set of " + "blocks, checking with the full block map.",dn);          blocks=handleInsufficientlyStored(dn);          outOfServiceNodeBlocks.put(dn,blocks);        }        final boolean isHealthy=blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);        if (blocks.size() == 0 && isHealthy) {          if (dn.isDecommissionInProgress()) {            dnAdmin.setDecommissioned(dn);            toRemove.add(dn);          } else           if (dn.isEnteringMaintenance()) {            dnAdmin.setInMaintenance(dn);          } else {
      }      if (blocks.size() == 0) {        if (!fullScan) {          LOG.debug("Node {} has finished replicating current set of " + "blocks, checking with the full block map.",dn);          blocks=handleInsufficientlyStored(dn);          outOfServiceNodeBlocks.put(dn,blocks);        }        final boolean isHealthy=blockManager.isNodeHealthyForDecommissionOrMaintenance(dn);        if (blocks.size() == 0 && isHealthy) {          if (dn.isDecommissionInProgress()) {            dnAdmin.setDecommissioned(dn);            toRemove.add(dn);          } else           if (dn.isEnteringMaintenance()) {            dnAdmin.setInMaintenance(dn);          } else {            Preconditions.checkState(false,"Node %s is in an invalid state! " + "Invalid state: %s %s blocks are on this dn.",dn,dn.getAdminState(),blocks.size());          }          LOG.debug("Node {} is sufficiently replicated and healthy, " + "marked as {}.",dn,dn.getAdminState());
    LOG.warn("Deprecated configuration key {} will be ignored.",deprecatedKey);    LOG.warn("Please update your configuration to use {} instead.",DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY);  }  checkArgument(blocksPerInterval > 0,"Must set a positive value for " + DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY);  final int maxConcurrentTrackedNodes=conf.getInt(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES,DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT);  checkArgument(maxConcurrentTrackedNodes >= 0,"Cannot set a negative " + "value for " + DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES);  Class cls=null;  try {    cls=conf.getClass(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MONITOR_CLASS,Class.forName(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MONITOR_CLASS_DEFAULT));    monitor=(DatanodeAdminMonitorInterface)ReflectionUtils.newInstance(cls,conf);    monitor.setBlockManager(blockManager);    monitor.setNameSystem(namesystem);    monitor.setDatanodeAdminManager(this);  } catch (  Exception e) {    throw new RuntimeException("Unable to create the Decommission monitor " + "from " + cls,e);  }  executor.scheduleAtFixedRate(monitor,intervalSecs,intervalSecs,TimeUnit.SECONDS);
@VisibleForTesting public void startDecommission(DatanodeDescriptor node){  if (!node.isDecommissionInProgress() && !node.isDecommissioned()) {    hbManager.startDecommission(node);    if (node.isDecommissionInProgress()) {      for (      DatanodeStorageInfo storage : node.getStorageInfos()) {
@VisibleForTesting public void startMaintenance(DatanodeDescriptor node,long maintenanceExpireTimeInMS){  node.setMaintenanceExpireTimeInMS(maintenanceExpireTimeInMS);  if (!node.isMaintenance()) {    hbManager.startMaintenance(node);    if (node.isEnteringMaintenance()) {      for (      DatanodeStorageInfo storage : node.getStorageInfos()) {
protected void setDecommissioned(DatanodeDescriptor dn){  dn.setDecommissioned();
protected void setInMaintenance(DatanodeDescriptor dn){  dn.setInMaintenance();
@Override public void setConf(Configuration conf){  this.conf=conf;  this.maxConcurrentTrackedNodes=conf.getInt(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES,DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT);  if (this.maxConcurrentTrackedNodes < 0) {
private void pruneStorageMap(final StorageReport[] reports){synchronized (storageMap) {
private void pruneStorageMap(final StorageReport[] reports){synchronized (storageMap) {    LOG.debug("Number of storages reported in heartbeat={};" + " Number of storages in storageMap={}",reports.length,storageMap.size());    HashMap<String,DatanodeStorageInfo> excessStorages;    excessStorages=new HashMap<>(storageMap);    for (    final StorageReport report : reports) {      excessStorages.remove(report.getStorage().getStorageID());    }    for (    final DatanodeStorageInfo storageInfo : excessStorages.values()) {      if (storageInfo.numBlocks() == 0) {        DatanodeStorageInfo info=storageMap.remove(storageInfo.getStorageID());        if (!hasStorageType(info.getStorageType())) {          if (getParent() instanceof DFSTopologyNodeImpl) {            ((DFSTopologyNodeImpl)getParent()).childRemoveStorage(getName(),info.getStorageType());          }        }        LOG.info("Removed storage {} from DataNode {}",storageInfo,this);      } else {
private void updateFailedStorage(Set<DatanodeStorageInfo> failedStorageInfos){  for (  DatanodeStorageInfo storageInfo : failedStorageInfos) {    if (storageInfo.getState() != DatanodeStorage.State.FAILED) {
private List<String> getNetworkDependencies(DatanodeInfo node) throws UnresolvedTopologyException {  List<String> dependencies=Collections.emptyList();  if (dnsToSwitchMapping instanceof DNSToSwitchMappingWithDependency) {    dependencies=((DNSToSwitchMappingWithDependency)dnsToSwitchMapping).getDependency(node.getHostName());    if (dependencies == null) {
@VisibleForTesting void checkIfClusterIsNowMultiRack(DatanodeDescriptor node){  if (!hasClusterEverBeenMultiRack && networktopology.getNumOfRacks() > 1) {    String message="DN " + node + " joining cluster has expanded a formerly "+ "single-rack cluster to be multi-rack. ";    if (blockManager.isPopulatingReplQueues()) {      message+="Re-checking all blocks for replication, since they should " + "now be replicated cross-rack";
      final boolean isInMaintenance=dn.isInMaintenance();      if (((listLiveNodes && !isDead) || (listDeadNodes && isDead) || (listDecommissioningNodes && isDecommissioning)|| (listEnteringMaintenanceNodes && isEnteringMaintenance)|| (listInMaintenanceNodes && isInMaintenance)) && hostConfigManager.isIncluded(dn)) {        nodes.add(dn);      }      foundNodes.add(dn.getResolvedAddress());    }  }  Collections.sort(nodes);  if (listDeadNodes) {    for (    InetSocketAddress addr : includedNodes) {      if (foundNodes.matchedBy(addr)) {        continue;      }      DatanodeDescriptor dn=new DatanodeDescriptor(new DatanodeID(addr.getAddress().getHostAddress(),addr.getHostName(),"",addr.getPort() == 0 ? defaultXferPort : addr.getPort(),defaultInfoPort,defaultInfoSecurePort,defaultIpcPort));      setDatanodeDead(dn);      if (hostConfigManager.isExcluded(dn)) {        dn.setDecommissioned();      }      nodes.add(dn);    }  }  if (LOG.isDebugEnabled()) {
    throw new DisallowedDatanodeException(nodeinfo);  }  if (nodeinfo == null || !nodeinfo.isRegistered()) {    return new DatanodeCommand[]{RegisterCommand.REGISTER};  }  heartbeatManager.updateHeartbeat(nodeinfo,reports,cacheCapacity,cacheUsed,xceiverCount,failedVolumes,volumeFailureSummary);  if (namesystem.isInSafeMode()) {    return new DatanodeCommand[0];  }  final BlockRecoveryCommand brCommand=getBlockRecoveryCommand(blockPoolId,nodeinfo);  if (brCommand != null) {    return new DatanodeCommand[]{brCommand};  }  final List<DatanodeCommand> cmds=new ArrayList<>();  int totalReplicateBlocks=nodeinfo.getNumberOfReplicateBlocks();  int totalECBlocks=nodeinfo.getNumberOfBlocksToBeErasureCoded();  int totalBlocks=totalReplicateBlocks + totalECBlocks;  if (totalBlocks > 0) {    int numReplicationTasks=(int)Math.ceil((double)(totalReplicateBlocks * maxTransfers) / totalBlocks);
public void handleLifeline(DatanodeRegistration nodeReg,StorageReport[] reports,String blockPoolId,long cacheCapacity,long cacheUsed,int xceiverCount,int maxTransfers,int failedVolumes,VolumeFailureSummary volumeFailureSummary) throws IOException {  if (LOG.isDebugEnabled()) {
private void createReplicationWork(int sourceIndex,DatanodeStorageInfo target){  BlockInfoStriped stripedBlk=(BlockInfoStriped)getBlock();  final byte blockIndex=liveBlockIndicies[sourceIndex];  final DatanodeDescriptor source=getSrcNodes()[sourceIndex];  final long internBlkLen=StripedBlockUtil.getInternalBlockLength(stripedBlk.getNumBytes(),stripedBlk.getCellSize(),stripedBlk.getDataBlockNum(),blockIndex);  final Block targetBlk=new Block(stripedBlk.getBlockId() + blockIndex,internBlkLen,stripedBlk.getGenerationStamp());  source.addBlockToBeReplicated(targetBlk,new DatanodeStorageInfo[]{target});
synchronized void startDecommission(final DatanodeDescriptor node){  if (!node.isAlive()) {
synchronized void startMaintenance(final DatanodeDescriptor node){  if (!node.isAlive()) {
synchronized void stopMaintenance(final DatanodeDescriptor node){
synchronized void stopDecommission(final DatanodeDescriptor node){
private boolean removeNodeFromStaleList(DatanodeDescriptor d,boolean isDead){  boolean result=false;  result=staleDataNodes.remove(d);  if (enableLogStaleNodes && result) {
boolean decrement(BlockInfo block,DatanodeStorageInfo dn){  boolean removed=false;synchronized (pendingReconstructions) {    PendingBlockInfo found=pendingReconstructions.get(block);    if (found != null) {
DatanodeStorageInfo getStorage(DatanodeDescriptor dn,DatanodeStorage s) throws IOException {  if (providedEnabled && storageId.equals(s.getStorageID())) {    if (StorageType.PROVIDED.equals(s.getStorageType())) {      if (providedStorageInfo.getState() == State.FAILED && s.getState() == State.NORMAL) {        providedStorageInfo.setState(State.NORMAL);
private static ECTopologyVerifierResult verifyECWithTopology(final int minDN,final int minRack,final int numOfRacks,final int numOfDataNodes,String readablePolicies){  String resultMessage;  if (numOfDataNodes < minDN) {    resultMessage=String.format("%d DataNodes are required for " + "the erasure coding policies: %s. " + "The number of DataNodes is only %d.",minDN,readablePolicies,numOfDataNodes);
  user=(user != null ? user : "");  path=(path != null ? path : "");  LOG.trace("Got user: {}, remoteIp: {}, path: {}",user,remoteIp,path);  if (remoteIp == null) {    LOG.trace("Returned false due to null rempteIp");    return false;  }  List<Rule> userRules=((userRules=rulemap.get(user)) != null) ? userRules : new ArrayList<Rule>();  List<Rule> anyRules=((anyRules=rulemap.get("*")) != null) ? anyRules : new ArrayList<Rule>();  List<Rule> rules=Stream.of(userRules,anyRules).flatMap(l -> l.stream()).collect(Collectors.toList());  for (  Rule rule : rules) {    SubnetUtils.SubnetInfo subnet=rule.getSubnet();    String rulePath=rule.getPath();    LOG.trace("Evaluating rule, subnet: {}, path: {}",subnet != null ? subnet.getCidrSignature() : "*",rulePath);    try {      if ((subnet == null || subnet.isInRange(remoteIp)) && FilenameUtils.directoryContains(rulePath,path)) {
  } else {    Pattern comma_split=Pattern.compile(",");    Pattern rule_split=Pattern.compile("\\||\n");    Map<Integer,List<String[]>> splits=rule_split.splitAsStream(ruleString).map(x -> comma_split.split(x,3)).collect(Collectors.groupingBy(x -> x.length));    if (!splits.keySet().equals(Collections.singleton(3))) {      String bad_lines=rule_split.splitAsStream(ruleString).filter(x -> comma_split.split(x,3).length != 3).collect(Collectors.joining("\n"));      throw new IllegalArgumentException("Bad rule definition: " + bad_lines);    }    int user=0;    int cidr=1;    int path=2;    BiFunction<CopyOnWriteArrayList<Rule>,CopyOnWriteArrayList<Rule>,CopyOnWriteArrayList<Rule>> arrayListMerge=(v1,v2) -> {      v1.addAll(v2);      return v1;    };    for (    String[] split : splits.get(3)) {
@Override public void run(){  if (!metricsLog.isInfoEnabled() || !hasAppenders(metricsLog) || objectName == null) {    return;  }  metricsLog.info(" >> Begin " + nodeName + " metrics dump");  final MBeanServer server=ManagementFactory.getPlatformMBeanServer();  for (  final ObjectName mbeanName : server.queryNames(objectName,null)) {    try {      MBeanInfo mBeanInfo=server.getMBeanInfo(mbeanName);      final String mBeanNameName=MBeans.getMbeanNameName(mbeanName);      final Set<String> attributeNames=getFilteredAttributes(mBeanInfo);      final AttributeList attributes=server.getAttributes(mbeanName,attributeNames.toArray(new String[attributeNames.size()]));      for (      Object o : attributes) {        final Attribute attribute=(Attribute)o;        final Object value=attribute.getValue();        final String valueStr=(value != null) ? value.toString() : "null";
    return;  }  metricsLog.info(" >> Begin " + nodeName + " metrics dump");  final MBeanServer server=ManagementFactory.getPlatformMBeanServer();  for (  final ObjectName mbeanName : server.queryNames(objectName,null)) {    try {      MBeanInfo mBeanInfo=server.getMBeanInfo(mbeanName);      final String mBeanNameName=MBeans.getMbeanNameName(mbeanName);      final Set<String> attributeNames=getFilteredAttributes(mBeanInfo);      final AttributeList attributes=server.getAttributes(mbeanName,attributeNames.toArray(new String[attributeNames.size()]));      for (      Object o : attributes) {        final Attribute attribute=(Attribute)o;        final Object value=attribute.getValue();        final String valueStr=(value != null) ? value.toString() : "null";        metricsLog.info(mBeanNameName + ":" + attribute.getName()+ "="+ trimLine(valueStr));      }    } catch (    Exception e) {
  metricsLog.info(" >> Begin " + nodeName + " metrics dump");  final MBeanServer server=ManagementFactory.getPlatformMBeanServer();  for (  final ObjectName mbeanName : server.queryNames(objectName,null)) {    try {      MBeanInfo mBeanInfo=server.getMBeanInfo(mbeanName);      final String mBeanNameName=MBeans.getMbeanNameName(mbeanName);      final Set<String> attributeNames=getFilteredAttributes(mBeanInfo);      final AttributeList attributes=server.getAttributes(mbeanName,attributeNames.toArray(new String[attributeNames.size()]));      for (      Object o : attributes) {        final Attribute attribute=(Attribute)o;        final Object value=attribute.getValue();        final String valueStr=(value != null) ? value.toString() : "null";        metricsLog.info(mBeanNameName + ":" + attribute.getName()+ "="+ trimLine(valueStr));      }    } catch (    Exception e) {      metricsLog.error("Failed to get " + nodeName + " metrics for mbean "+ mbeanName.toString(),e);
public static void checkVersionUpgradable(int oldVersion) throws IOException {  if (oldVersion > LAST_UPGRADABLE_LAYOUT_VERSION) {    String msg="*********** Upgrade is not supported from this " + " older version " + oldVersion + " of storage to the current version."+ " Please upgrade to "+ LAST_UPGRADABLE_HADOOP_VERSION+ " or a later version and then upgrade to current"+ " version. Old layout version is "+ (oldVersion == 0 ? "'too old'" : ("" + oldVersion))+ " and latest layout version this software version can"+ " upgrade from is "+ LAST_UPGRADABLE_LAYOUT_VERSION+ ". ************";
    throw new IOException("Source '" + srcFile + "' and destination '"+ destFile+ "' are the same");  }  File parentFile=destFile.getParentFile();  if (parentFile != null) {    if (!parentFile.mkdirs() && !parentFile.isDirectory()) {      throw new IOException("Destination '" + parentFile + "' directory cannot be created");    }  }  if (destFile.exists()) {    if (FileUtil.canWrite(destFile) == false) {      throw new IOException("Destination '" + destFile + "' exists but is read-only");    } else {      if (destFile.delete() == false) {        throw new IOException("Destination '" + destFile + "' exists but cannot be deleted");      }    }  }  try {    NativeIO.copyFileUnbuffered(srcFile,destFile);  } catch (  NativeIOException e) {    throw new IOException("Failed to copy " + srcFile.getCanonicalPath() + " to "+ destFile.getCanonicalPath()+ " due to failure in NativeIO#copyFileUnbuffered(). "+ e.toString());
    while (num > 0) {      num=stream.read(buf);      if (num > 0) {        received+=num;        for (        FileOutputStream fos : outputStreams) {          fos.write(buf,0,num);        }        if (throttler != null) {          throttler.throttle(num);        }      }    }    finishedReceiving=true;    double xferSec=Math.max(((float)(Time.monotonicNow() - startTime)) / 1000.0,0.001);    long xferKb=received / 1024;    xferCombined+=xferSec;    xferStats.append(String.format(" The file download took %.2fs at %.2f KB/s.",xferSec,xferKb / xferSec));  }  finally {    stream.close();
public static boolean isDiskStatsEnabled(int fileIOSamplingPercentage){  final boolean isEnabled;  if (fileIOSamplingPercentage <= 0) {
@Override public Reader<FileRegion> getReader(Reader.Options opts,String blockPoolID) throws IOException {  InMemoryAliasMapProtocol aliasMap=getAliasMap(blockPoolID);
@Override public Writer<FileRegion> getWriter(Writer.Options opts,String blockPoolID) throws IOException {  InMemoryAliasMapProtocol aliasMap=getAliasMap(blockPoolID);
public BlockMovementStatus moveBlock(BlockMovingInfo blkMovingInfo,SaslDataTransferClient saslClient,ExtendedBlock eb,Socket sock,DataEncryptionKeyFactory km,Token<BlockTokenIdentifier> accessToken){
public BlockMovementStatus moveBlock(BlockMovingInfo blkMovingInfo,SaslDataTransferClient saslClient,ExtendedBlock eb,Socket sock,DataEncryptionKeyFactory km,Token<BlockTokenIdentifier> accessToken){  LOG.info("Start moving block:{} from src:{} to destin:{} to satisfy " + "storageType, sourceStoragetype:{} and destinStoragetype:{}",blkMovingInfo.getBlock(),blkMovingInfo.getSource(),blkMovingInfo.getTarget(),blkMovingInfo.getSourceStorageType(),blkMovingInfo.getTargetStorageType());  DataOutputStream out=null;  DataInputStream in=null;  try {    NetUtils.connect(sock,NetUtils.createSocketAddr(blkMovingInfo.getTarget().getXferAddr(connectToDnViaHostname)),socketTimeout);    sock.setSoTimeout(socketTimeout * 5);    sock.setKeepAlive(true);    OutputStream unbufOut=sock.getOutputStream();    InputStream unbufIn=sock.getInputStream();
  DataInputStream in=null;  try {    NetUtils.connect(sock,NetUtils.createSocketAddr(blkMovingInfo.getTarget().getXferAddr(connectToDnViaHostname)),socketTimeout);    sock.setSoTimeout(socketTimeout * 5);    sock.setKeepAlive(true);    OutputStream unbufOut=sock.getOutputStream();    InputStream unbufIn=sock.getInputStream();    LOG.debug("Connecting to datanode {}",blkMovingInfo.getTarget());    IOStreamPair saslStreams=saslClient.socketSend(sock,unbufOut,unbufIn,km,accessToken,blkMovingInfo.getTarget());    unbufOut=saslStreams.out;    unbufIn=saslStreams.in;    out=new DataOutputStream(new BufferedOutputStream(unbufOut,ioFileBufferSize));    in=new DataInputStream(new BufferedInputStream(unbufIn,ioFileBufferSize));    sendRequest(out,eb,accessToken,blkMovingInfo.getSource(),blkMovingInfo.getTargetStorageType());    receiveResponse(in);
    sock.setSoTimeout(socketTimeout * 5);    sock.setKeepAlive(true);    OutputStream unbufOut=sock.getOutputStream();    InputStream unbufIn=sock.getInputStream();    LOG.debug("Connecting to datanode {}",blkMovingInfo.getTarget());    IOStreamPair saslStreams=saslClient.socketSend(sock,unbufOut,unbufIn,km,accessToken,blkMovingInfo.getTarget());    unbufOut=saslStreams.out;    unbufIn=saslStreams.in;    out=new DataOutputStream(new BufferedOutputStream(unbufOut,ioFileBufferSize));    in=new DataInputStream(new BufferedInputStream(unbufIn,ioFileBufferSize));    sendRequest(out,eb,accessToken,blkMovingInfo.getSource(),blkMovingInfo.getTargetStorageType());    receiveResponse(in);    LOG.info("Successfully moved block:{} from src:{} to destin:{} for" + " satisfying storageType:{}",blkMovingInfo.getBlock(),blkMovingInfo.getSource(),blkMovingInfo.getTarget(),blkMovingInfo.getTargetStorageType());    return BlockMovementStatus.DN_BLK_STORAGE_MOVEMENT_SUCCESS;  } catch (  BlockPinningException e) {
@Override public void run(){  while (running) {    try {      Future<BlockMovementAttemptFinished> future=moverCompletionService.take();      if (future != null) {        BlockMovementAttemptFinished result=future.get();
void verifyAndSetNamespaceInfo(BPServiceActor actor,NamespaceInfo nsInfo) throws IOException {  writeLock();  if (nsInfo.getState() == HAServiceState.ACTIVE && bpServiceToActive == null) {
@VisibleForTesting NamespaceInfo retrieveNamespaceInfo() throws IOException {  NamespaceInfo nsInfo=null;  while (shouldRun()) {    try {      nsInfo=bpNamenode.versionRequest();
        cmds.add(cmd);      }    } else {      for (int r=0; r < reports.length; r++) {        StorageBlockReport singleReport[]={reports[r]};        DatanodeCommand cmd=bpNamenode.blockReport(bpRegistration,bpos.getBlockPoolId(),singleReport,new BlockReportContext(reports.length,r,reportId,fullBrLeaseId,true));        blockReportSizes.add(calculateBlockReportPBSize(useBlocksBuffer,singleReport));        numReportsSent++;        numRPCs++;        if (cmd != null) {          cmds.add(cmd);        }      }    }    success=true;  }  finally {    long brSendCost=monotonicNow() - brSendStartTime;    long brCreateCost=brSendStartTime - brCreateStartTime;    dn.getMetrics().addBlockReport(brSendCost,getRpcMetricSuffix());
  }  DatanodeCommand cmd=null;  final long startTime=monotonicNow();  if (startTime - lastCacheReport > dnConf.cacheReportInterval) {    if (LOG.isDebugEnabled()) {      LOG.debug("Sending cacheReport from service actor: " + this);    }    lastCacheReport=startTime;    String bpid=bpos.getBlockPoolId();    List<Long> blockIds=dn.getFSDataset().getCacheReport(bpid);    long createTime=monotonicNow();    cmd=bpNamenode.cacheReport(bpRegistration,bpid,blockIds);    long sendTime=monotonicNow();    long createCost=createTime - startTime;    long sendCost=sendTime - createTime;    dn.getMetrics().addCacheReport(sendCost);    if (LOG.isDebugEnabled()) {
HeartbeatResponse sendHeartBeat(boolean requestBlockReportLease) throws IOException {  scheduler.scheduleNextHeartbeat();  StorageReport[] reports=dn.getFSDataset().getStorageReports(bpos.getBlockPoolId());  if (LOG.isDebugEnabled()) {
private void handleRollingUpgradeStatus(HeartbeatResponse resp) throws IOException {  RollingUpgradeStatus rollingUpgradeStatus=resp.getRollingUpdateStatus();  if (rollingUpgradeStatus != null && rollingUpgradeStatus.getBlockPoolId().compareTo(bpos.getBlockPoolId()) != 0) {
 catch (      IOException ioe) {        runningState=RunningState.INIT_FAILED;        if (shouldRetryInit()) {          LOG.error("Initialization failed for " + this + " "+ ioe.getLocalizedMessage());          sleepAndLogInterrupts(5000,"initializing");        } else {          runningState=RunningState.FAILED;          LOG.error("Initialization failed for " + this + ". Exiting. ",ioe);          return;        }      }    }    runningState=RunningState.RUNNING;    if (initialRegistrationComplete != null) {      initialRegistrationComplete.countDown();    }    while (shouldRun()) {      try {        offerService();
List<StorageDirectory> recoverTransitionRead(NamespaceInfo nsInfo,StorageLocation location,StartupOption startOpt,List<Callable<StorageDirectory>> callables,Configuration conf) throws IOException {
private void format(StorageDirectory bpSdir,NamespaceInfo nsInfo) throws IOException {
void remove(File absPathToRemove){  Preconditions.checkArgument(absPathToRemove.isAbsolute());
  int filesRestored=0;  File[] children=trashRoot.exists() ? trashRoot.listFiles() : null;  if (children == null) {    return 0;  }  File restoreDirectory=null;  for (  File child : children) {    if (child.isDirectory()) {      filesRestored+=restoreBlockFilesFromTrash(child);      continue;    }    if (restoreDirectory == null) {      restoreDirectory=new File(getRestoreDirectory(child));      if (!restoreDirectory.exists() && !restoreDirectory.mkdirs()) {        throw new IOException("Failed to create directory " + restoreDirectory);      }    }    final File newChild=new File(restoreDirectory,child.getName());    if (newChild.exists() && newChild.length() >= child.length()) {
private static void linkAllBlocks(File fromDir,File toDir,int diskLayoutVersion,Configuration conf) throws IOException {  HardLink hardLink=new HardLink();  DataStorage.linkBlocks(fromDir,toDir,DataStorage.STORAGE_DIR_FINALIZED,diskLayoutVersion,hardLink,conf);  DataStorage.linkBlocks(fromDir,toDir,DataStorage.STORAGE_DIR_RBW,diskLayoutVersion,hardLink,conf);
@VisibleForTesting String getRestoreDirectory(File blockFile){  Matcher matcher=BLOCK_POOL_TRASH_PATH_PATTERN.matcher(blockFile.getParent());  String restoreDirectory=matcher.replaceFirst("$1$2" + STORAGE_DIR_CURRENT + "$4");
boolean packetSentInTime(){  final long diff=Time.monotonicNow() - this.lastSentTime.get();  final boolean allowedIdleTime=(diff <= this.maxSendIdleTime);
private void handleMirrorOutError(IOException ioe) throws IOException {  String bpid=block.getBlockPoolId();
private int receivePacket() throws IOException {  packetReceiver.receiveNextPacket(in);  PacketHeader header=packetReceiver.getHeader();  if (LOG.isDebugEnabled()) {
        boolean alignedOnDisk=partialChunkSizeOnDisk == 0;        boolean alignedInPacket=firstByteInBlock % bytesPerChecksum == 0;        boolean overwriteLastCrc=!alignedOnDisk && !shouldNotWriteChecksum;        boolean doCrcRecalc=overwriteLastCrc && (lastChunkBoundary != firstByteInBlock);        if (!alignedInPacket && len > bytesPerChecksum) {          throw new IOException("Unexpected packet data length for " + block + " from "+ inAddr+ ": a partial chunk must be "+ " sent in an individual packet (data length = "+ len+ " > bytesPerChecksum = "+ bytesPerChecksum+ ")");        }        Checksum partialCrc=null;        if (doCrcRecalc) {          if (LOG.isDebugEnabled()) {            LOG.debug("receivePacket for " + block + ": previous write did not end at the chunk boundary."+ " onDiskLen="+ onDiskLen);          }          long offsetInChecksum=BlockMetadataHeader.getHeaderSize() + onDiskLen / bytesPerChecksum * checksumSize;          partialCrc=computePartialChunkCrc(onDiskLen,offsetInChecksum);        }        int startByteToDisk=(int)(onDiskLen - firstByteInBlock) + dataBuf.arrayOffset() + dataBuf.position();        int numBytesToDisk=(int)(offsetInBlock - onDiskLen);        long begin=Time.monotonicNow();
    while (receivePacket() >= 0) {    }    if (responder != null) {      ((PacketResponder)responder.getRunnable()).close();      responderClosed=true;    }    if (isDatanode || isTransfer) {      try (ReplicaHandler handler=claimReplicaHandler()){        close();        block.setNumBytes(replicaInfo.getNumBytes());        if (stage == BlockConstructionStage.TRANSFER_RBW) {          datanode.data.convertTemporaryToRbw(block);        } else {          datanode.data.finalizeBlock(block,dirSyncOnFinalize);        }      }       datanode.metrics.incrBlocksWritten();    }  } catch (  IOException ioe) {    replicaInfo.releaseAllBytesReserved();
    if (responder != null) {      ((PacketResponder)responder.getRunnable()).close();      responderClosed=true;    }    if (isDatanode || isTransfer) {      try (ReplicaHandler handler=claimReplicaHandler()){        close();        block.setNumBytes(replicaInfo.getNumBytes());        if (stage == BlockConstructionStage.TRANSFER_RBW) {          datanode.data.convertTemporaryToRbw(block);        } else {          datanode.data.finalizeBlock(block,dirSyncOnFinalize);        }      }       datanode.metrics.incrBlocksWritten();    }  } catch (  IOException ioe) {    replicaInfo.releaseAllBytesReserved();    if (datanode.isRestarting()) {
private void initPerfMonitoring(DatanodeInfo[] downstreams){  if (downstreams != null && downstreams.length > 0) {    downstreamDNs=downstreams;    isPenultimateNode=(downstreams.length == 1);    if (isPenultimateNode && datanode.getPeerMetrics() != null) {      mirrorNameForMetrics=(downstreams[0].getInfoSecurePort() != 0 ? downstreams[0].getInfoSecureAddr() : downstreams[0].getInfoAddr());
private Checksum computePartialChunkCrc(long blkoff,long ckoff) throws IOException {  int sizePartialChunk=(int)(blkoff % bytesPerChecksum);  blkoff=blkoff - sizePartialChunk;  if (LOG.isDebugEnabled()) {
private static void logRecoverBlock(String who,RecoveringBlock rb){  ExtendedBlock block=rb.getBlock();  DatanodeInfo[] targets=rb.getLocations();
public synchronized void addVolumeScanner(FsVolumeReference ref){  boolean success=false;  try {    FsVolumeSpi volume=ref.getVolume();    if (!isEnabled()) {
public synchronized void removeVolumeScanner(FsVolumeSpi volume){  if (!isEnabled()) {
synchronized void markSuspectBlock(String storageId,ExtendedBlock block){  if (!isEnabled()) {
    }  }  try {    if (transferTo) {      SocketOutputStream sockOut=(SocketOutputStream)out;      sockOut.write(buf,headerOff,dataOff - headerOff);      FileChannel fileCh=((FileInputStream)ris.getDataIn()).getChannel();      LongWritable waitTime=new LongWritable();      LongWritable transferTime=new LongWritable();      fileIoProvider.transferToSocketFully(ris.getVolumeRef().getVolume(),sockOut,fileCh,blockInPosition,dataLen,waitTime,transferTime);      datanode.metrics.addSendDataPacketBlockedOnNetworkNanos(waitTime.get());      datanode.metrics.addSendDataPacketTransferNanos(transferTime.get());      blockInPosition+=dataLen;    } else {      out.write(buf,headerOff,dataOff + dataLen - headerOff);    }  } catch (  IOException e) {    if (e instanceof SocketTimeoutException) {
      pktBufSize+=(chunkSize + checksumSize) * maxChunksPerPacket;    }    ByteBuffer pktBuf=ByteBuffer.allocate(pktBufSize);    while (endOffset > offset && !Thread.currentThread().isInterrupted()) {      manageOsCache();      long len=sendPacket(pktBuf,maxChunksPerPacket,streamForSendChunks,transferTo,throttler);      offset+=len;      totalRead+=len + (numberOfChunks(len) * checksumSize);      seqno++;    }    if (!Thread.currentThread().isInterrupted()) {      try {        sendPacket(pktBuf,maxChunksPerPacket,streamForSendChunks,transferTo,throttler);        out.flush();      } catch (      IOException e) {        throw ioeToSocketException(e);      }      sentEntireByteRange=true;
@Override public String reconfigurePropertyImpl(String property,String newVal) throws ReconfigurationException {switch (property) {case DFS_DATANODE_DATA_DIR_KEY:{      IOException rootException=null;      try {
 catch (      IOException e) {        rootException=e;      } finally {        try {          triggerBlockReport(new BlockReportOptions.Factory().setIncremental(false).build());        } catch (        IOException e) {          LOG.warn("Exception while sending the block report after refreshing" + " volumes {} to {}",property,newVal,e);          if (rootException == null) {            rootException=e;          }        } finally {          if (rootException != null) {            throw new ReconfigurationException(property,newVal,getConf().get(property),rootException);          }        }      }      break;    }case DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY:{    ReconfigurationException rootException=null;
  }  ChangedVolumes results=new ChangedVolumes();  results.newLocations.addAll(newStorageLocations);  for (Iterator<Storage.StorageDirectory> it=storage.dirIterator(); it.hasNext(); ) {    Storage.StorageDirectory dir=it.next();    boolean found=false;    for (Iterator<StorageLocation> newLocationItr=results.newLocations.iterator(); newLocationItr.hasNext(); ) {      StorageLocation newLocation=newLocationItr.next();      if (newLocation.matchesStorageDirectory(dir)) {        StorageLocation oldLocation=existingStorageLocations.get(newLocation.getNormalizedUri().toString());        if (oldLocation != null && oldLocation.getStorageType() != newLocation.getStorageType()) {          throw new IOException("Changing storage type is not allowed.");        }        newLocationItr.remove();        results.unchangedLocations.add(newLocation);        found=true;        break;
          throw new IOException("Changing storage type is not allowed.");        }        newLocationItr.remove();        results.unchangedLocations.add(newLocation);        found=true;        break;      }    }    if (!found) {      LOG.info("Deactivation request received for active volume: {}",dir.getRoot());      results.deactivateLocations.add(StorageLocation.parse(dir.getRoot().toString()));    }  }  if (getFSDataset().getNumFailedVolumes() > 0) {    for (    String failedStorageLocation : getFSDataset().getVolumeFailureSummary().getFailedStorageLocations()) {      boolean found=false;      for (Iterator<StorageLocation> newLocationItr=results.newLocations.iterator(); newLocationItr.hasNext(); ) {        StorageLocation newLocation=newLocationItr.next();        if (newLocation.getNormalizedUri().toString().equals(failedStorageLocation)) {          found=true;
        service=Executors.newFixedThreadPool(changedVolumes.newLocations.size());        List<Future<IOException>> exceptions=Lists.newArrayList();        for (        final StorageLocation location : changedVolumes.newLocations) {          exceptions.add(service.submit(new Callable<IOException>(){            @Override public IOException call(){              try {                data.addVolume(location,nsInfos);              } catch (              IOException e) {                return e;              }              return null;            }          }));        }        for (int i=0; i < changedVolumes.newLocations.size(); i++) {          StorageLocation volume=changedVolumes.newLocations.get(i);          Future<IOException> ioExceptionFuture=exceptions.get(i);          try {
          exceptions.add(service.submit(new Callable<IOException>(){            @Override public IOException call(){              try {                data.addVolume(location,nsInfos);              } catch (              IOException e) {                return e;              }              return null;            }          }));        }        for (int i=0; i < changedVolumes.newLocations.size(); i++) {          StorageLocation volume=changedVolumes.newLocations.get(i);          Future<IOException> ioExceptionFuture=exceptions.get(i);          try {            IOException ioe=ioExceptionFuture.get();            if (ioe != null) {              errorMessageBuilder.append(String.format("FAILED TO ADD: %s: %s%n",volume,ioe.getMessage()));
                data.addVolume(location,nsInfos);              } catch (              IOException e) {                return e;              }              return null;            }          }));        }        for (int i=0; i < changedVolumes.newLocations.size(); i++) {          StorageLocation volume=changedVolumes.newLocations.get(i);          Future<IOException> ioExceptionFuture=exceptions.get(i);          try {            IOException ioe=ioExceptionFuture.get();            if (ioe != null) {              errorMessageBuilder.append(String.format("FAILED TO ADD: %s: %s%n",volume,ioe.getMessage()));              LOG.error("Failed to add volume: {}",volume,ioe);            } else {              effectiveVolumes.add(volume.toString());
              return null;            }          }));        }        for (int i=0; i < changedVolumes.newLocations.size(); i++) {          StorageLocation volume=changedVolumes.newLocations.get(i);          Future<IOException> ioExceptionFuture=exceptions.get(i);          try {            IOException ioe=ioExceptionFuture.get();            if (ioe != null) {              errorMessageBuilder.append(String.format("FAILED TO ADD: %s: %s%n",volume,ioe.getMessage()));              LOG.error("Failed to add volume: {}",volume,ioe);            } else {              effectiveVolumes.add(volume.toString());              LOG.info("Successfully added volume: {}",volume);            }          } catch (          Exception e) {            errorMessageBuilder.append(String.format("FAILED to ADD: %s: %s%n",volume,e.toString()));
private void initIpcServer() throws IOException {  InetSocketAddress ipcAddr=NetUtils.createSocketAddr(getConf().getTrimmed(DFS_DATANODE_IPC_ADDRESS_KEY));  RPC.setProtocolEngine(getConf(),ClientDatanodeProtocolPB.class,ProtobufRpcEngine2.class);  ClientDatanodeProtocolServerSideTranslatorPB clientDatanodeProtocolXlator=new ClientDatanodeProtocolServerSideTranslatorPB(this);  BlockingService service=ClientDatanodeProtocolService.newReflectiveBlockingService(clientDatanodeProtocolXlator);  ipcServer=new RPC.Builder(getConf()).setProtocol(ClientDatanodeProtocolPB.class).setInstance(service).setBindAddress(ipcAddr.getHostName()).setPort(ipcAddr.getPort()).setNumHandlers(getConf().getInt(DFS_DATANODE_HANDLER_COUNT_KEY,DFS_DATANODE_HANDLER_COUNT_DEFAULT)).setVerbose(false).setSecretManager(blockPoolTokenSecretManager).build();  ReconfigurationProtocolServerSideTranslatorPB reconfigurationProtocolXlator=new ReconfigurationProtocolServerSideTranslatorPB(this);  service=ReconfigurationProtocolService.newReflectiveBlockingService(reconfigurationProtocolXlator);  DFSUtil.addPBProtocol(getConf(),ReconfigurationProtocolPB.class,service,ipcServer);  InterDatanodeProtocolServerSideTranslatorPB interDatanodeProtocolXlator=new InterDatanodeProtocolServerSideTranslatorPB(this);  service=InterDatanodeProtocolService.newReflectiveBlockingService(interDatanodeProtocolXlator);  DFSUtil.addPBProtocol(getConf(),InterDatanodeProtocolPB.class,service,ipcServer);  TraceAdminProtocolServerSideTranslatorPB traceAdminXlator=new TraceAdminProtocolServerSideTranslatorPB(this);  BlockingService traceAdminService=TraceAdminService.newReflectiveBlockingService(traceAdminXlator);  DFSUtil.addPBProtocol(getConf(),TraceAdminProtocolPB.class,traceAdminService,ipcServer);
  } else {    int backlogLength=getConf().getInt(CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_KEY,CommonConfigurationKeysPublic.IPC_SERVER_LISTEN_QUEUE_SIZE_DEFAULT);    tcpPeerServer=new TcpPeerServer(dnConf.socketWriteTimeout,DataNode.getStreamingAddr(getConf()),backlogLength);  }  if (dnConf.getTransferSocketRecvBufferSize() > 0) {    tcpPeerServer.setReceiveBufferSize(dnConf.getTransferSocketRecvBufferSize());  }  streamingAddr=tcpPeerServer.getStreamingAddr();  LOG.info("Opened streaming server at {}",streamingAddr);  this.threadGroup=new ThreadGroup("dataXceiverServer");  xserver=new DataXceiverServer(tcpPeerServer,getConf(),this);  this.dataXceiverServer=new Daemon(threadGroup,xserver);  this.threadGroup.setDaemon(true);  if (getConf().getBoolean(HdfsClientConfigKeys.Read.ShortCircuit.KEY,HdfsClientConfigKeys.Read.ShortCircuit.DEFAULT) || getConf().getBoolean(HdfsClientConfigKeys.DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC,HdfsClientConfigKeys.DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC_DEFAULT)) {    DomainPeerServer domainPeerServer=getDomainPeerServer(getConf(),streamingAddr.getPort());    if (domainPeerServer != null) {      this.localDataXceiverServer=new Daemon(threadGroup,new DataXceiverServer(domainPeerServer,getConf(),this));
synchronized void checkDatanodeUuid() throws IOException {  if (storage.getDatanodeUuid() == null) {    storage.setDatanodeUuid(generateUuid());    storage.writeAll();
public static InterDatanodeProtocol createInterDataNodeProtocolProxy(DatanodeID datanodeid,final Configuration conf,final int socketTimeout,final boolean connectToDnViaHostname) throws IOException {  final String dnAddr=datanodeid.getIpcAddr(connectToDnViaHostname);  final InetSocketAddress addr=NetUtils.createSocketAddr(dnAddr);
private void checkBlockToken(ExtendedBlock block,Token<BlockTokenIdentifier> token,AccessMode accessMode) throws IOException {  if (isBlockTokenEnabled) {    BlockTokenIdentifier id=new BlockTokenIdentifier();    ByteArrayInputStream buf=new ByteArrayInputStream(token.getIdentifier());    DataInputStream in=new DataInputStream(buf);    id.readFields(in);
public void shutdown(){  stopMetricsLogger();  if (plugins != null) {    for (    ServicePlugin p : plugins) {      try {        p.stop();
  boolean lengthTooShort=false;  try {    data.checkBlock(block,block.getNumBytes(),ReplicaState.FINALIZED);  } catch (  ReplicaNotFoundException e) {    replicaNotExist=true;  }catch (  UnexpectedReplicaStateException e) {    replicaStateNotFinalized=true;  }catch (  FileNotFoundException e) {    blockFileNotExist=true;  }catch (  EOFException e) {    lengthTooShort=true;  }catch (  IOException e) {    blockFileNotExist=true;  }  if (replicaNotExist || replicaStateNotFinalized) {    String errStr="Can't send invalid block " + block;
  }catch (  EOFException e) {    lengthTooShort=true;  }catch (  IOException e) {    blockFileNotExist=true;  }  if (replicaNotExist || replicaStateNotFinalized) {    String errStr="Can't send invalid block " + block;    LOG.info(errStr);    bpos.trySendErrorReport(DatanodeProtocol.INVALID_BLOCK,errStr);    return;  }  if (blockFileNotExist) {    reportBadBlock(bpos,block,"Can't replicate block " + block + " because the block file doesn't exist, or is not accessible");    return;  }  if (lengthTooShort) {    reportBadBlock(bpos,block,"Can't replicate block " + block + " because on-disk length "+ data.getLength(block)+ " is shorter than NameNode recorded length "+ block.getNumBytes());    return;
@Override public void deleteBlockPool(String blockPoolId,boolean force) throws IOException {  checkSuperuserPrivilege();
@Override public synchronized void shutdownDatanode(boolean forUpgrade) throws IOException {  checkSuperuserPrivilege();
public void enableTrash(String bpid){  if (trashEnabledBpids.add(bpid)) {    getBPStorage(bpid).stopTrashCleaner();
public void clearTrash(String bpid){  if (trashEnabledBpids.contains(bpid)) {    getBPStorage(bpid).clearTrash();    trashEnabledBpids.remove(bpid);
  final List<UpgradeTask> tasks=Lists.newArrayList();  for (  StorageLocation dataDir : dataDirs) {    if (!containsStorageDir(dataDir)) {      try {        final List<Callable<StorageDirectory>> callables=Lists.newArrayList();        final StorageDirectory sd=loadStorageDirectory(datanode,nsInfo,dataDir,startOpt,callables);        if (callables.isEmpty()) {          addStorageDir(sd);          success.add(dataDir);        } else {          for (          Callable<StorageDirectory> c : callables) {            tasks.add(new UpgradeTask(dataDir,executor.submit(c)));          }        }      } catch (      IOException e) {        LOG.warn("Failed to add storage directory {}",dataDir,e);      }    } else {
      try {        final List<Callable<StorageDirectory>> callables=Lists.newArrayList();        final StorageDirectory sd=loadStorageDirectory(datanode,nsInfo,dataDir,startOpt,callables);        if (callables.isEmpty()) {          addStorageDir(sd);          success.add(dataDir);        } else {          for (          Callable<StorageDirectory> c : callables) {            tasks.add(new UpgradeTask(dataDir,executor.submit(c)));          }        }      } catch (      IOException e) {        LOG.warn("Failed to add storage directory {}",dataDir,e);      }    } else {      LOG.info("Storage directory {} has already been used.",dataDir);      success.add(dataDir);    }  }  if (!tasks.isEmpty()) {
    dataDir.makeBlockPoolDir(bpid,null);    try {      final List<Callable<StorageDirectory>> sdCallables=Lists.newArrayList();      final List<StorageDirectory> dirs=bpStorage.recoverTransitionRead(nsInfo,dataDir,startOpt,sdCallables,datanode.getConf());      if (sdCallables.isEmpty()) {        for (        StorageDirectory sd : dirs) {          success.add(sd);        }      } else {        upgradeCallableMap.put(dataDir,sdCallables);      }    } catch (    IOException e) {      LOG.warn("Failed to add storage directory {} for block pool {}",dataDir,bpid,e);    }  }  for (  Map.Entry<StorageLocation,List<Callable<StorageDirectory>>> entry : upgradeCallableMap.entrySet()) {    for (    Callable<StorageDirectory> c : entry.getValue()) {      tasks.add(new UpgradeTask(entry.getKey(),executor.submit(c)));    }  }  if (!tasks.isEmpty()) {
private void doUpgrade(final StorageDirectory sd,final NamespaceInfo nsInfo,final File prevDir,final File tmpDir,final File bbwDir,final File toDir,final int oldLV,Configuration conf) throws IOException {  linkAllBlocks(tmpDir,bbwDir,toDir,oldLV,conf);  clusterID=nsInfo.getClusterID();  upgradeProperties(sd,conf);  rename(tmpDir,prevDir);
void upgradeProperties(StorageDirectory sd,Configuration conf) throws IOException {  createStorageID(sd,layoutVersion,conf);
private static void linkBlocks(File from,File to,int oldLV,HardLink hl,Configuration conf) throws IOException {
        datanode.shortCircuitRegistry.registerSlot(ExtendedBlockId.fromExtendedBlock(blk),slotId,isCached);        registeredSlotId=slotId;      }      fis=datanode.requestShortCircuitFdsForRead(blk,token,maxVersion);      Preconditions.checkState(fis != null);      bld.setStatus(SUCCESS);      bld.setShortCircuitAccessVersion(DataNode.CURRENT_BLOCK_FORMAT_VERSION);    } catch (    ShortCircuitFdsVersionException e) {      bld.setStatus(ERROR_UNSUPPORTED);      bld.setShortCircuitAccessVersion(DataNode.CURRENT_BLOCK_FORMAT_VERSION);      bld.setMessage(e.getMessage());    }catch (    ShortCircuitFdsUnsupportedException e) {      bld.setStatus(ERROR_UNSUPPORTED);      bld.setMessage(e.getMessage());    }catch (    IOException e) {      bld.setStatus(ERROR);
@Override public void readBlock(final ExtendedBlock block,final Token<BlockTokenIdentifier> blockToken,final String clientName,final long blockOffset,final long length,final boolean sendChecksum,final CachingStrategy cachingStrategy) throws IOException {  previousOpClientName=clientName;  long read=0;  updateCurrentThreadName("Sending block " + block);  OutputStream baseStream=getOutputStream();  DataOutputStream out=getBufferedOutputStream();  checkAccess(out,true,block,blockToken,Op.READ_BLOCK,BlockTokenIdentifier.AccessMode.READ);  BlockSender blockSender=null;  DatanodeRegistration dnR=datanode.getDNRegistrationForBP(block.getBlockPoolId());  final String clientTraceFmt=clientName.length() > 0 && ClientTraceLog.isInfoEnabled() ? String.format(DN_CLIENTTRACE_FORMAT,localAddress,remoteAddress,"%d","HDFS_READ",clientName,"%d",dnR.getDatanodeUuid(),block,"%d") : dnR + " Served block " + block+ " to "+ remoteAddress;  try {    try {      blockSender=new BlockSender(block,blockOffset,length,true,false,sendChecksum,datanode,clientTraceFmt,cachingStrategy);    } catch (    IOException e) {      String msg="opReadBlock " + block + " received exception "+ e;
  if (targetStorageTypes.length > 0) {    System.arraycopy(targetStorageTypes,0,storageTypes,1,nst);  }  final int nsi=targetStorageIds.length;  final String[] storageIds;  if (nsi > 0) {    storageIds=new String[nsi + 1];    storageIds[0]=storageId;    if (targetStorageTypes.length > 0) {      System.arraycopy(targetStorageIds,0,storageIds,1,nsi);    }  } else {    storageIds=new String[0];  }  checkAccess(replyOut,isClient,block,blockToken,Op.WRITE_BLOCK,BlockTokenIdentifier.AccessMode.WRITE,storageTypes,storageIds);  if (isTransfer && targets.length > 0) {    throw new IOException(stage + " does not support multiple targets " + Arrays.asList(targets));  }  if (LOG.isDebugEnabled()) {
    System.arraycopy(targetStorageTypes,0,storageTypes,1,nst);  }  final int nsi=targetStorageIds.length;  final String[] storageIds;  if (nsi > 0) {    storageIds=new String[nsi + 1];    storageIds[0]=storageId;    if (targetStorageTypes.length > 0) {      System.arraycopy(targetStorageIds,0,storageIds,1,nsi);    }  } else {    storageIds=new String[0];  }  checkAccess(replyOut,isClient,block,blockToken,Op.WRITE_BLOCK,BlockTokenIdentifier.AccessMode.WRITE,storageTypes,storageIds);  if (isTransfer && targets.length > 0) {    throw new IOException(stage + " does not support multiple targets " + Arrays.asList(targets));  }  if (LOG.isDebugEnabled()) {    LOG.debug("opWriteBlock: stage={}, clientname={}\n  " + "block  ={}, newGs={}, bytesRcvd=[{}, {}]\n  " + "targets={}; pipelineSize={}, srcDataNode={}, pinning={}",stage,clientname,block,latestGenerationStamp,minBytesRcvd,maxBytesRcvd,Arrays.asList(targets),pipelineSize,srcDataNode,pinning);
    storageIds[0]=storageId;    if (targetStorageTypes.length > 0) {      System.arraycopy(targetStorageIds,0,storageIds,1,nsi);    }  } else {    storageIds=new String[0];  }  checkAccess(replyOut,isClient,block,blockToken,Op.WRITE_BLOCK,BlockTokenIdentifier.AccessMode.WRITE,storageTypes,storageIds);  if (isTransfer && targets.length > 0) {    throw new IOException(stage + " does not support multiple targets " + Arrays.asList(targets));  }  if (LOG.isDebugEnabled()) {    LOG.debug("opWriteBlock: stage={}, clientname={}\n  " + "block  ={}, newGs={}, bytesRcvd=[{}, {}]\n  " + "targets={}; pipelineSize={}, srcDataNode={}, pinning={}",stage,clientname,block,latestGenerationStamp,minBytesRcvd,maxBytesRcvd,Arrays.asList(targets),pipelineSize,srcDataNode,pinning);    LOG.debug("isDatanode={}, isClient={}, isTransfer={}",isDatanode,isClient,isTransfer);    LOG.debug("writeBlock receive buf size {} tcp no delay {}",peer.getReceiveBufferSize(),peer.getTcpNoDelay());  }  final ExtendedBlock originalBlock=new ExtendedBlock(block);  if (block.getNumBytes() == 0) {    block.setNumBytes(dataXceiverServer.estimateBlockSize);
  }  LOG.info("Receiving {} src: {} dest: {}",block,remoteAddress,localAddress);  DataOutputStream mirrorOut=null;  DataInputStream mirrorIn=null;  Socket mirrorSock=null;  String mirrorNode=null;  String firstBadLink="";  Status mirrorInStatus=SUCCESS;  final String storageUuid;  final boolean isOnTransientStorage;  try {    final Replica replica;    if (isDatanode || stage != BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {      setCurrentBlockReceiver(getBlockReceiver(block,storageType,in,peer.getRemoteAddressString(),peer.getLocalAddressString(),stage,latestGenerationStamp,minBytesRcvd,maxBytesRcvd,clientname,srcDataNode,datanode,requestedChecksum,cachingStrategy,allowLazyPersist,pinning,storageId));      replica=blockReceiver.getReplica();    } else {
        DataEncryptionKeyFactory keyFactory=datanode.getDataEncryptionKeyFactoryForBlock(block);        SecretKey secretKey=null;        if (dnConf.overwriteDownstreamDerivedQOP) {          String bpid=block.getBlockPoolId();          BlockKey blockKey=datanode.blockPoolTokenSecretManager.get(bpid).getCurrentKey();          secretKey=blockKey.getKey();        }        IOStreamPair saslStreams=datanode.saslClient.socketSend(mirrorSock,unbufMirrorOut,unbufMirrorIn,keyFactory,blockToken,targets[0],secretKey);        unbufMirrorOut=saslStreams.out;        unbufMirrorIn=saslStreams.in;        mirrorOut=new DataOutputStream(new BufferedOutputStream(unbufMirrorOut,smallBufferSize));        mirrorIn=new DataInputStream(unbufMirrorIn);        String targetStorageId=null;        if (targetStorageIds.length > 0) {          targetStorageId=targetStorageIds[0];        }        if (targetPinnings != null && targetPinnings.length > 0) {
        }        if (targetPinnings != null && targetPinnings.length > 0) {          new Sender(mirrorOut).writeBlock(originalBlock,targetStorageTypes[0],blockToken,clientname,targets,targetStorageTypes,srcDataNode,stage,pipelineSize,minBytesRcvd,maxBytesRcvd,latestGenerationStamp,requestedChecksum,cachingStrategy,allowLazyPersist,targetPinnings[0],targetPinnings,targetStorageId,targetStorageIds);        } else {          new Sender(mirrorOut).writeBlock(originalBlock,targetStorageTypes[0],blockToken,clientname,targets,targetStorageTypes,srcDataNode,stage,pipelineSize,minBytesRcvd,maxBytesRcvd,latestGenerationStamp,requestedChecksum,cachingStrategy,allowLazyPersist,false,targetPinnings,targetStorageId,targetStorageIds);        }        mirrorOut.flush();        DataNodeFaultInjector.get().writeBlockAfterFlush();        if (isClient) {          BlockOpResponseProto connectAck=BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(mirrorIn));          mirrorInStatus=connectAck.getStatus();          firstBadLink=connectAck.getFirstBadLink();          if (mirrorInStatus != SUCCESS) {            LOG.debug("Datanode {} got response for connect" + "ack  from downstream datanode with firstbadlink as {}",targets.length,firstBadLink);          }        }      } catch (      IOException e) {        if (isClient) {          BlockOpResponseProto.newBuilder().setStatus(ERROR).setFirstBadLink(targets[0].getXferAddr()).build().writeDelimitedTo(replyOut);
 else {          new Sender(mirrorOut).writeBlock(originalBlock,targetStorageTypes[0],blockToken,clientname,targets,targetStorageTypes,srcDataNode,stage,pipelineSize,minBytesRcvd,maxBytesRcvd,latestGenerationStamp,requestedChecksum,cachingStrategy,allowLazyPersist,false,targetPinnings,targetStorageId,targetStorageIds);        }        mirrorOut.flush();        DataNodeFaultInjector.get().writeBlockAfterFlush();        if (isClient) {          BlockOpResponseProto connectAck=BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(mirrorIn));          mirrorInStatus=connectAck.getStatus();          firstBadLink=connectAck.getFirstBadLink();          if (mirrorInStatus != SUCCESS) {            LOG.debug("Datanode {} got response for connect" + "ack  from downstream datanode with firstbadlink as {}",targets.length,firstBadLink);          }        }      } catch (      IOException e) {        if (isClient) {          BlockOpResponseProto.newBuilder().setStatus(ERROR).setFirstBadLink(targets[0].getXferAddr()).build().writeDelimitedTo(replyOut);          replyOut.flush();        }        IOUtils.closeStream(mirrorOut);
 catch (      IOException e) {        if (isClient) {          BlockOpResponseProto.newBuilder().setStatus(ERROR).setFirstBadLink(targets[0].getXferAddr()).build().writeDelimitedTo(replyOut);          replyOut.flush();        }        IOUtils.closeStream(mirrorOut);        mirrorOut=null;        IOUtils.closeStream(mirrorIn);        mirrorIn=null;        IOUtils.closeSocket(mirrorSock);        mirrorSock=null;        if (isClient) {          LOG.error("{}:Exception transfering block {} to mirror {}",datanode,block,mirrorNode,e);          throw e;        } else {          LOG.info("{}:Exception transfering {} to mirror {}- continuing " + "without the mirror",datanode,block,mirrorNode,e);
        }        IOUtils.closeStream(mirrorOut);        mirrorOut=null;        IOUtils.closeStream(mirrorIn);        mirrorIn=null;        IOUtils.closeSocket(mirrorSock);        mirrorSock=null;        if (isClient) {          LOG.error("{}:Exception transfering block {} to mirror {}",datanode,block,mirrorNode,e);          throw e;        } else {          LOG.info("{}:Exception transfering {} to mirror {}- continuing " + "without the mirror",datanode,block,mirrorNode,e);          incrDatanodeNetworkErrors();        }      }    }    if (isClient && !isTransfer) {      if (mirrorInStatus != SUCCESS) {        LOG.debug("Datanode {} forwarding connect ack to upstream " + "firstbadlink is {}",targets.length,firstBadLink);
@Override public void copyBlock(final ExtendedBlock block,final Token<BlockTokenIdentifier> blockToken) throws IOException {  updateCurrentThreadName("Copying block " + block);  DataOutputStream reply=getBufferedOutputStream();  checkAccess(reply,true,block,blockToken,Op.COPY_BLOCK,BlockTokenIdentifier.AccessMode.COPY);  if (datanode.data.getPinning(block)) {    String msg="Not able to copy block " + block.getBlockId() + " "+ "to "+ peer.getRemoteAddressString()+ " because it's pinned ";
    String msg="Not able to copy block " + block.getBlockId() + " "+ "to "+ peer.getRemoteAddressString()+ " because threads "+ "quota is exceeded.";    LOG.info(msg);    sendResponse(ERROR,msg);    return;  }  BlockSender blockSender=null;  boolean isOpSuccess=true;  try {    blockSender=new BlockSender(block,0,-1,false,false,true,datanode,null,CachingStrategy.newDropBehind());    OutputStream baseStream=getOutputStream();    writeSuccessWithChecksumInfo(blockSender,reply);    long beginRead=Time.monotonicNow();    long read=blockSender.sendBlock(reply,baseStream,dataXceiverServer.balanceThrottler);    long duration=Time.monotonicNow() - beginRead;    datanode.metrics.incrBytesRead((int)read);    datanode.metrics.incrBlocksRead();
    return;  }  BlockSender blockSender=null;  boolean isOpSuccess=true;  try {    blockSender=new BlockSender(block,0,-1,false,false,true,datanode,null,CachingStrategy.newDropBehind());    OutputStream baseStream=getOutputStream();    writeSuccessWithChecksumInfo(blockSender,reply);    long beginRead=Time.monotonicNow();    long read=blockSender.sendBlock(reply,baseStream,dataXceiverServer.balanceThrottler);    long duration=Time.monotonicNow() - beginRead;    datanode.metrics.incrBytesRead((int)read);    datanode.metrics.incrBlocksRead();    datanode.metrics.incrTotalReadTime(duration);    LOG.info("Copied {} to {}",block,peer.getRemoteAddressString());  } catch (  IOException ioe) {
  if (!dataXceiverServer.balanceThrottler.acquire()) {    String msg="Not able to receive block " + block.getBlockId() + " from "+ peer.getRemoteAddressString()+ " because threads "+ "quota is exceeded.";    LOG.warn(msg);    sendResponse(ERROR,msg);    return;  }  Socket proxySock=null;  DataOutputStream proxyOut=null;  Status opStatus=SUCCESS;  String errMsg=null;  DataInputStream proxyReply=null;  boolean IoeDuringCopyBlockOperation=false;  try {    if (proxySource.equals(datanode.getDatanodeId())) {      ReplicaInfo oldReplica=datanode.data.moveBlockAcrossStorage(block,storageType,storageId);      if (oldReplica != null) {
    return;  }  Socket proxySock=null;  DataOutputStream proxyOut=null;  Status opStatus=SUCCESS;  String errMsg=null;  DataInputStream proxyReply=null;  boolean IoeDuringCopyBlockOperation=false;  try {    if (proxySource.equals(datanode.getDatanodeId())) {      ReplicaInfo oldReplica=datanode.data.moveBlockAcrossStorage(block,storageType,storageId);      if (oldReplica != null) {        LOG.info("Moved {} from StorageType {} to {}",block,oldReplica.getVolume().getStorageType(),storageType);      }    } else {      block.setNumBytes(dataXceiverServer.estimateBlockSize);      final String dnAddr=proxySource.getXferAddr(connectToDnViaHostname);
      DataEncryptionKeyFactory keyFactory=datanode.getDataEncryptionKeyFactoryForBlock(block);      IOStreamPair saslStreams=datanode.saslClient.socketSend(proxySock,unbufProxyOut,unbufProxyIn,keyFactory,blockToken,proxySource);      unbufProxyOut=saslStreams.out;      unbufProxyIn=saslStreams.in;      proxyOut=new DataOutputStream(new BufferedOutputStream(unbufProxyOut,smallBufferSize));      proxyReply=new DataInputStream(new BufferedInputStream(unbufProxyIn,ioFileBufferSize));      IoeDuringCopyBlockOperation=true;      new Sender(proxyOut).copyBlock(block,blockToken);      IoeDuringCopyBlockOperation=false;      BlockOpResponseProto copyResponse=BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(proxyReply));      String logInfo="copy block " + block + " from "+ proxySock.getRemoteSocketAddress();      DataTransferProtoUtil.checkBlockOpStatus(copyResponse,logInfo,true);      ReadOpChecksumInfoProto checksumInfo=copyResponse.getReadOpChecksumInfo();      DataChecksum remoteChecksum=DataTransferProtoUtil.fromProto(checksumInfo.getChecksum());      setCurrentBlockReceiver(getBlockReceiver(block,storageType,proxyReply,proxySock.getRemoteSocketAddress().toString(),proxySock.getLocalSocketAddress().toString(),null,0,0,0,"",null,datanode,remoteChecksum,CachingStrategy.newDropBehind(),false,false,storageId));
private void checkAccess(OutputStream out,final boolean reply,final ExtendedBlock blk,final Token<BlockTokenIdentifier> t,final Op op,final BlockTokenIdentifier.AccessMode mode,final StorageType[] storageTypes,final String[] storageIds) throws IOException {  checkAndWaitForBP(blk);  if (datanode.isBlockTokenEnabled) {
    try {      peer=peerServer.accept();      int curXceiverCount=datanode.getXceiverCount();      if (curXceiverCount > maxXceiverCount) {        throw new IOException("Xceiver count " + curXceiverCount + " exceeds the limit of concurrent xceivers: "+ maxXceiverCount);      }      new Daemon(datanode.threadGroup,DataXceiver.create(peer,datanode,this)).start();    } catch (    SocketTimeoutException ignored) {    }catch (    AsynchronousCloseException ace) {      if (datanode.shouldRun && !datanode.shutdownForUpgrade) {        LOG.warn("{}:DataXceiverServer",datanode.getDisplayName(),ace);      }    }catch (    IOException ie) {      IOUtils.closeQuietly(peer);      LOG.warn("{}:DataXceiverServer",datanode.getDisplayName(),ie);    }catch (    OutOfMemoryError ie) {      IOUtils.closeQuietly(peer);
      }      new Daemon(datanode.threadGroup,DataXceiver.create(peer,datanode,this)).start();    } catch (    SocketTimeoutException ignored) {    }catch (    AsynchronousCloseException ace) {      if (datanode.shouldRun && !datanode.shutdownForUpgrade) {        LOG.warn("{}:DataXceiverServer",datanode.getDisplayName(),ace);      }    }catch (    IOException ie) {      IOUtils.closeQuietly(peer);      LOG.warn("{}:DataXceiverServer",datanode.getDisplayName(),ie);    }catch (    OutOfMemoryError ie) {      IOUtils.closeQuietly(peer);      LOG.error("DataNode is out of memory. Will retry in 30 seconds.",ie);      try {        Thread.sleep(TimeUnit.SECONDS.toMillis(30L));      } catch (      InterruptedException e) {      }    }catch (    Throwable te) {
void start(){  shouldRun.set(true);  long firstScanTime=ThreadLocalRandom.current().nextLong(scanPeriodMsecs);
  if (!shouldRun.getAndSet(false)) {    LOG.warn("Shutdown has been called, but periodic scanner not started");  }  if (masterThread != null) {    masterThread.shutdown();  }  if (reportCompileThreadPool != null) {    reportCompileThreadPool.shutdownNow();  }  if (masterThread != null) {    try {      masterThread.awaitTermination(1,TimeUnit.MINUTES);    } catch (    InterruptedException e) {      LOG.error("interrupted while waiting for masterThread to " + "terminate",e);    }  }  if (reportCompileThreadPool != null) {    try {      reportCompileThreadPool.awaitTermination(1,TimeUnit.MINUTES);    } catch (    InterruptedException e) {
        } else         if (memBlock.compareWith(info) != 0) {          statsRecord.duplicateBlocks++;          addDifference(diffRecord,statsRecord,info);        }        d++;        if (d < blockpoolReport.size()) {          ScanInfo nextInfo=blockpoolReport.get(d);          if (nextInfo.getBlockId() != info.getBlockId()) {            ++m;          }        } else {          ++m;        }      }      while (m < bl.size()) {        ReplicaInfo current=bl.get(m++);        addDifference(diffRecord,statsRecord,current.getBlockId(),current.getVolume());      }      while (d < blockpoolReport.size()) {        if (!dataset.isDeletingBlock(bpid,blockpoolReport.get(d).getBlockId())) {
  try (FsDatasetSpi.FsVolumeReferences volumes=dataset.getFsVolumeReferences()){    for (    final FsVolumeSpi volume : volumes) {      if (volume.getStorageType() != StorageType.PROVIDED) {        ReportCompiler reportCompiler=new ReportCompiler(volume);        Future<ScanInfoVolumeReport> result=reportCompileThreadPool.submit(reportCompiler);        compilersInProgress.add(result);      }    }    for (    Future<ScanInfoVolumeReport> future : compilersInProgress) {      try {        final ScanInfoVolumeReport result=future.get();        if (!CollectionUtils.addIgnoreNull(volReports,result)) {          volReports.clear();          break;        }      } catch (      Exception ex) {        LOG.warn("Error compiling report. Continuing.",ex);      }    }  } catch (  IOException e) {
public void cancelPlan(String planID) throws DiskBalancerException {  lock.lock();  boolean needShutdown=false;  try {    checkDiskBalancerEnabled();    if (this.planID == null || !this.planID.equals(planID) || this.planID.isEmpty()) {
private void createWorkPlan(NodePlan plan) throws DiskBalancerException {  Preconditions.checkState(lock.isHeldByCurrentThread());  workMap.clear();  Map<String,String> storageIDToVolBasePathMap=getStorageIDToVolumeBasePathMap();  for (  Step step : plan.getVolumeSetPlans()) {    String sourceVolUuid=step.getSourceVolume().getUuid();    String destVolUuid=step.getDestinationVolume().getUuid();    String sourceVolBasePath=storageIDToVolBasePathMap.get(sourceVolUuid);    if (sourceVolBasePath == null) {      final String errMsg="Disk Balancer - Unable to find volume: " + step.getSourceVolume().getPath() + ". SubmitPlan failed.";
private void createWorkPlan(NodePlan plan) throws DiskBalancerException {  Preconditions.checkState(lock.isHeldByCurrentThread());  workMap.clear();  Map<String,String> storageIDToVolBasePathMap=getStorageIDToVolumeBasePathMap();  for (  Step step : plan.getVolumeSetPlans()) {    String sourceVolUuid=step.getSourceVolume().getUuid();    String destVolUuid=step.getDestinationVolume().getUuid();    String sourceVolBasePath=storageIDToVolBasePathMap.get(sourceVolUuid);    if (sourceVolBasePath == null) {      final String errMsg="Disk Balancer - Unable to find volume: " + step.getSourceVolume().getPath() + ". SubmitPlan failed.";      LOG.error(errMsg);      throw new DiskBalancerException(errMsg,DiskBalancerException.Result.INVALID_VOLUME);    }    String destVolBasePath=storageIDToVolBasePathMap.get(destVolUuid);    if (destVolBasePath == null) {      final String errMsg="Disk Balancer - Unable to find volume: " + step.getDestinationVolume().getPath() + ". SubmitPlan failed.";
@Override public void bumpReplicaGS(long newGS) throws IOException {  long oldGS=getGenerationStamp();  final File oldmeta=getMetaFile();  setGenerationStamp(newGS);  final File newmeta=getMetaFile();  if (LOG.isDebugEnabled()) {
public static void truncateBlock(FsVolumeSpi volume,File blockFile,File metaFile,long oldlen,long newlen,FileIoProvider fileIoProvider) throws IOException {
  if (numBlockIters == 0) {    LOG.debug("{}: no block pools are registered.",this);    return Long.MAX_VALUE;  }  int curIdx;  if (curBlockIter == null) {    curIdx=0;  } else {    curIdx=blockIters.indexOf(curBlockIter);    Preconditions.checkState(curIdx >= 0);  }  long nowMs=Time.now();  long minTimeoutMs=Long.MAX_VALUE;  for (int i=0; i < numBlockIters; i++) {    int idx=(curIdx + i + 1) % numBlockIters;    BlockIterator iter=blockIters.get(idx);    if (!iter.atEnd()) {
  } else {    curIdx=blockIters.indexOf(curBlockIter);    Preconditions.checkState(curIdx >= 0);  }  long nowMs=Time.now();  long minTimeoutMs=Long.MAX_VALUE;  for (int i=0; i < numBlockIters; i++) {    int idx=(curIdx + i + 1) % numBlockIters;    BlockIterator iter=blockIters.get(idx);    if (!iter.atEnd()) {      LOG.info("Now scanning bpid {} on volume {}",iter.getBlockPoolId(),volume);      curBlockIter=iter;      return 0L;    }    long iterStartMs=iter.getIterStartMs();    long waitMs=(iterStartMs + conf.scanPeriodMs) - nowMs;    if (waitMs <= 0) {
  long nowMs=Time.now();  long minTimeoutMs=Long.MAX_VALUE;  for (int i=0; i < numBlockIters; i++) {    int idx=(curIdx + i + 1) % numBlockIters;    BlockIterator iter=blockIters.get(idx);    if (!iter.atEnd()) {      LOG.info("Now scanning bpid {} on volume {}",iter.getBlockPoolId(),volume);      curBlockIter=iter;      return 0L;    }    long iterStartMs=iter.getIterStartMs();    long waitMs=(iterStartMs + conf.scanPeriodMs) - nowMs;    if (waitMs <= 0) {      iter.rewind();      LOG.info("Now rescanning bpid {} on volume {}, after more than " + "{} hour(s)",iter.getBlockPoolId(),volume,TimeUnit.HOURS.convert(conf.scanPeriodMs,TimeUnit.MILLISECONDS));      curBlockIter=iter;
private long scanBlock(ExtendedBlock cblock,long bytesPerSec){  ExtendedBlock block=null;  try {    Block b=volume.getDataset().getStoredBlock(cblock.getBlockPoolId(),cblock.getBlockId());    if (b == null) {
    block=curBlockIter.nextBlock();  } catch (  IOException e) {    LOG.warn("{}: nextBlock error on {}",this,curBlockIter);    return null;  }  if (block == null) {    LOG.info("{}: finished scanning block pool {}",this,curBlockIter.getBlockPoolId());    saveBlockIterator(curBlockIter);    return null;  } else   if (conf.skipRecentAccessed) {    try {      BlockLocalPathInfo blockLocalPathInfo=volume.getDataset().getBlockLocalPathInfo(block);      BasicFileAttributes attr=Files.readAttributes(new File(blockLocalPathInfo.getBlockPath()).toPath(),BasicFileAttributes.class);      if (System.currentTimeMillis() - attr.lastAccessTime().to(TimeUnit.MILLISECONDS) < conf.scanPeriodMs) {        return null;      }    } catch (    IOException ioe) {
        long timeout=findNextUsableBlockIter();        if (timeout > 0) {          LOG.trace("{}: no block pools are ready to scan yet.  Waiting " + "{} ms.",this,timeout);synchronized (stats) {            stats.nextBlockPoolScanStartMs=Time.monotonicNow() + timeout;          }          return timeout;        }synchronized (stats) {          stats.scansSinceRestart++;          stats.blocksScannedInCurrentPeriod=0;          stats.nextBlockPoolScanStartMs=-1;        }        return 0L;      }      block=getNextBlockToScan();      if (block == null) {        return 0L;      }    }    if (curBlockIter != null) {
      while (true) {        ExtendedBlock suspectBlock=null;synchronized (this) {          if (stopping) {            break;          }          if (timeout > 0) {            LOG.debug("{}: wait for {} milliseconds",this,timeout);            wait(timeout);            if (stopping) {              break;            }          }          suspectBlock=popNextSuspectBlock();        }        timeout=runLoop(suspectBlock);      }    } catch (    InterruptedException e) {      LOG.trace("{} exiting because of InterruptedException.",this);    }catch (    Throwable e) {
public synchronized void markSuspectBlock(ExtendedBlock block){  if (stopping) {
    LOG.trace("Skipped checking all volumes, time since last check {} is less " + "than the minimum gap between checks ({} ms).",gap,minDiskCheckGapMs);    return Collections.emptySet();  }  final FsDatasetSpi.FsVolumeReferences references=dataset.getFsVolumeReferences();  if (references.size() == 0) {    LOG.warn("checkAllVolumesAsync - no volumes can be referenced");    return Collections.emptySet();  }  lastAllVolumesCheck=timer.monotonicNow();  final Set<FsVolumeSpi> healthyVolumes=new HashSet<>();  final Set<FsVolumeSpi> failedVolumes=new HashSet<>();  final Set<FsVolumeSpi> allVolumes=new HashSet<>();  final AtomicLong numVolumes=new AtomicLong(references.size());  final CountDownLatch latch=new CountDownLatch(1);  for (int i=0; i < references.size(); ++i) {    final FsVolumeReference reference=references.getReference(i);    Optional<ListenableFuture<VolumeCheckResult>> olf=delegateChecker.schedule(reference.getVolume(),IGNORED_CONTEXT);
private void initializeStripedBlkReconstructionThreadPool(int numThreads){
@Override public void setConf(Configuration conf){  balancedSpaceThreshold=conf.getLongBytes(DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY,DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_DEFAULT);  balancedPreferencePercent=conf.getFloat(DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);
private V doChooseVolume(final List<V> volumes,long replicaSize,String storageId) throws IOException {  AvailableSpaceVolumeList volumesWithSpaces=new AvailableSpaceVolumeList(volumes);  if (volumesWithSpaces.areAllVolumesWithinFreeSpaceThreshold()) {    V volume=roundRobinPolicyBalanced.chooseVolume(volumes,replicaSize,storageId);    if (LOG.isDebugEnabled()) {
  if (volumesWithSpaces.areAllVolumesWithinFreeSpaceThreshold()) {    V volume=roundRobinPolicyBalanced.chooseVolume(volumes,replicaSize,storageId);    if (LOG.isDebugEnabled()) {      LOG.debug("All volumes are within the configured free space balance " + "threshold. Selecting " + volume + " for write of block size "+ replicaSize);    }    return volume;  } else {    V volume=null;    long mostAvailableAmongLowVolumes=volumesWithSpaces.getMostAvailableSpaceAmongVolumesWithLowAvailableSpace();    List<V> highAvailableVolumes=extractVolumesFromPairs(volumesWithSpaces.getVolumesWithHighAvailableSpace());    List<V> lowAvailableVolumes=extractVolumesFromPairs(volumesWithSpaces.getVolumesWithLowAvailableSpace());    float preferencePercentScaler=(highAvailableVolumes.size() * balancedPreferencePercent) + (lowAvailableVolumes.size() * (1 - balancedPreferencePercent));    float scaledPreferencePercent=(highAvailableVolumes.size() * balancedPreferencePercent) / preferencePercentScaler;    if (mostAvailableAmongLowVolumes < replicaSize || random.nextFloat() < scaledPreferencePercent) {      volume=roundRobinPolicyHighAvailable.chooseVolume(highAvailableVolumes,replicaSize,storageId);      if (LOG.isDebugEnabled()) {
    }    return volume;  } else {    V volume=null;    long mostAvailableAmongLowVolumes=volumesWithSpaces.getMostAvailableSpaceAmongVolumesWithLowAvailableSpace();    List<V> highAvailableVolumes=extractVolumesFromPairs(volumesWithSpaces.getVolumesWithHighAvailableSpace());    List<V> lowAvailableVolumes=extractVolumesFromPairs(volumesWithSpaces.getVolumesWithLowAvailableSpace());    float preferencePercentScaler=(highAvailableVolumes.size() * balancedPreferencePercent) + (lowAvailableVolumes.size() * (1 - balancedPreferencePercent));    float scaledPreferencePercent=(highAvailableVolumes.size() * balancedPreferencePercent) / preferencePercentScaler;    if (mostAvailableAmongLowVolumes < replicaSize || random.nextFloat() < scaledPreferencePercent) {      volume=roundRobinPolicyHighAvailable.chooseVolume(highAvailableVolumes,replicaSize,storageId);      if (LOG.isDebugEnabled()) {        LOG.debug("Volumes are imbalanced. Selecting " + volume + " from high available space volumes for write of block size "+ replicaSize);      }    } else {      volume=roundRobinPolicyLowAvailable.chooseVolume(lowAvailableVolumes,replicaSize,storageId);      if (LOG.isDebugEnabled()) {
@VisibleForTesting static ReplicaInfo selectReplicaToDelete(final ReplicaInfo replica1,final ReplicaInfo replica2){  ReplicaInfo replicaToKeep;  ReplicaInfo replicaToDelete;  if (replica1.getBlockURI().equals(replica2.getBlockURI())) {    return null;  }  if (replica1.getGenerationStamp() != replica2.getGenerationStamp()) {    replicaToKeep=replica1.getGenerationStamp() > replica2.getGenerationStamp() ? replica1 : replica2;  } else   if (replica1.getNumBytes() != replica2.getNumBytes()) {    replicaToKeep=replica1.getNumBytes() > replica2.getNumBytes() ? replica1 : replica2;  } else   if (replica1.getVolume().isTransientStorage() && !replica2.getVolume().isTransientStorage()) {    replicaToKeep=replica2;  } else {    replicaToKeep=replica1;  }  replicaToDelete=(replicaToKeep == replica1) ? replica2 : replica1;  if (LOG.isDebugEnabled()) {
private boolean readReplicasFromCache(ReplicaMap volumeMap,final RamDiskReplicaTracker lazyWriteReplicaMap){  ReplicaMap tmpReplicaMap=new ReplicaMap(new ReentrantReadWriteLock());  File replicaFile=new File(replicaCacheDir,REPLICA_CACHE_FILE);  if (!replicaFile.exists()) {
  FileInputStream inputStream=null;  try {    inputStream=fileIoProvider.getFileInputStream(volume,replicaFile);    BlockListAsLongs blocksList=BlockListAsLongs.readFrom(inputStream,maxDataLength);    if (blocksList == null) {      return false;    }    for (    BlockReportReplica replica : blocksList) {switch (replica.getState()) {case FINALIZED:        addReplicaToReplicasMap(replica,tmpReplicaMap,lazyWriteReplicaMap,true);      break;case RUR:case RBW:case RWR:    addReplicaToReplicasMap(replica,tmpReplicaMap,lazyWriteReplicaMap,false);  break;default:break;}}for (Iterator<ReplicaInfo> iter=tmpReplicaMap.replicas(bpid).iterator(); iter.hasNext(); ) {ReplicaInfo info=iter.next();
    inputStream=fileIoProvider.getFileInputStream(volume,replicaFile);    BlockListAsLongs blocksList=BlockListAsLongs.readFrom(inputStream,maxDataLength);    if (blocksList == null) {      return false;    }    for (    BlockReportReplica replica : blocksList) {switch (replica.getState()) {case FINALIZED:        addReplicaToReplicasMap(replica,tmpReplicaMap,lazyWriteReplicaMap,true);      break;case RUR:case RBW:case RWR:    addReplicaToReplicasMap(replica,tmpReplicaMap,lazyWriteReplicaMap,false);  break;default:break;}}for (Iterator<ReplicaInfo> iter=tmpReplicaMap.replicas(bpid).iterator(); iter.hasNext(); ) {ReplicaInfo info=iter.next();iter.remove();volumeMap.add(bpid,info);
    for (    BlockReportReplica replica : blocksList) {switch (replica.getState()) {case FINALIZED:        addReplicaToReplicasMap(replica,tmpReplicaMap,lazyWriteReplicaMap,true);      break;case RUR:case RBW:case RWR:    addReplicaToReplicasMap(replica,tmpReplicaMap,lazyWriteReplicaMap,false);  break;default:break;}}for (Iterator<ReplicaInfo> iter=tmpReplicaMap.replicas(bpid).iterator(); iter.hasNext(); ) {ReplicaInfo info=iter.next();iter.remove();volumeMap.add(bpid,info);}LOG.info("Successfully read replica from cache file : " + replicaFile.getPath());return true;} catch (Exception e) {LOG.info("Exception occurred while reading the replicas cache file: " + replicaFile.getPath(),e);
void deleteAsync(FsVolumeReference volumeRef,ReplicaInfo replicaToDelete,ExtendedBlock block,String trashDirectory){
void deleteSync(FsVolumeReference volumeRef,ReplicaInfo replicaToDelete,ExtendedBlock block,String trashDirectory){
synchronized void cacheBlock(long blockId,String bpid,String blockFileName,long length,long genstamp,Executor volumeExecutor){  ExtendedBlockId key=new ExtendedBlockId(blockId,bpid);  Value prevValue=mappableBlockMap.get(key);  if (prevValue != null) {
  Value prevValue=mappableBlockMap.get(key);  boolean deferred=false;  if (cacheLoader.isTransientCache() && !dataset.datanode.getShortCircuitRegistry().processBlockMunlockRequest(key)) {    deferred=true;  }  if (prevValue == null) {    LOG.debug("Block with id {}, pool {} does not need to be uncached, " + "because it is not currently in the mappableBlockMap.",blockId,bpid);    numBlocksFailedToUncache.incrementAndGet();    return;  }switch (prevValue.state) {case CACHING:    LOG.debug("Cancelling caching for block with id {}, pool {}.",blockId,bpid);  mappableBlockMap.put(key,new Value(prevValue.mappableBlock,State.CACHING_CANCELLED));break;case CACHED:mappableBlockMap.put(key,new Value(prevValue.mappableBlock,State.UNCACHING));if (deferred) {if (LOG.isDebugEnabled()) {
    deferred=true;  }  if (prevValue == null) {    LOG.debug("Block with id {}, pool {} does not need to be uncached, " + "because it is not currently in the mappableBlockMap.",blockId,bpid);    numBlocksFailedToUncache.incrementAndGet();    return;  }switch (prevValue.state) {case CACHING:    LOG.debug("Cancelling caching for block with id {}, pool {}.",blockId,bpid);  mappableBlockMap.put(key,new Value(prevValue.mappableBlock,State.CACHING_CANCELLED));break;case CACHED:mappableBlockMap.put(key,new Value(prevValue.mappableBlock,State.UNCACHING));if (deferred) {if (LOG.isDebugEnabled()) {LOG.debug("{} is anchored, and can't be uncached now.  Scheduling it " + "for uncaching in {} ",key,DurationFormatUtils.formatDurationHMS(revocationPollingMs));}deferredUncachingExecutor.schedule(new UncachingTask(key,revocationMs),revocationPollingMs,TimeUnit.MILLISECONDS);} else {
private void activateVolume(ReplicaMap replicaMap,Storage.StorageDirectory sd,StorageType storageType,FsVolumeReference ref) throws IOException {  try (AutoCloseableLock lock=datasetWriteLock.acquire()){    DatanodeStorage dnStorage=storageMap.get(sd.getStorageUuid());    if (dnStorage != null) {      final String errorMsg=String.format("Found duplicated storage UUID: %s in %s.",sd.getStorageUuid(),sd.getVersionFile());
private void addVolume(Storage.StorageDirectory sd) throws IOException {  final StorageLocation storageLocation=sd.getStorageLocation();  FsVolumeImpl fsVolume=new FsVolumeImplBuilder().setDataset(this).setStorageID(sd.getStorageUuid()).setStorageDirectory(sd).setFileIoProvider(datanode.getFileIoProvider()).setConf(this.conf).build();  FsVolumeReference ref=fsVolume.obtainReference();  ReplicaMap tempVolumeMap=new ReplicaMap(datasetReadLock,datasetWriteLock);  fsVolume.getVolumeMap(tempVolumeMap,ramDiskReplicaTracker);  activateVolume(tempVolumeMap,sd,storageLocation.getStorageType(),ref);
@Override public void removeVolumes(final Collection<StorageLocation> storageLocsToRemove,boolean clearFailure){  Collection<StorageLocation> storageLocationsToRemove=new ArrayList<>(storageLocsToRemove);  Map<String,List<ReplicaInfo>> blkToInvalidate=new HashMap<>();  List<String> storageToRemove=new ArrayList<>();  try (AutoCloseableLock lock=datasetWriteLock.acquire()){    for (int idx=0; idx < dataStorage.getNumStorageDirs(); idx++) {      Storage.StorageDirectory sd=dataStorage.getStorageDir(idx);      final StorageLocation sdLocation=sd.getStorageLocation();
@Override public void removeVolumes(final Collection<StorageLocation> storageLocsToRemove,boolean clearFailure){  Collection<StorageLocation> storageLocationsToRemove=new ArrayList<>(storageLocsToRemove);  Map<String,List<ReplicaInfo>> blkToInvalidate=new HashMap<>();  List<String> storageToRemove=new ArrayList<>();  try (AutoCloseableLock lock=datasetWriteLock.acquire()){    for (int idx=0; idx < dataStorage.getNumStorageDirs(); idx++) {      Storage.StorageDirectory sd=dataStorage.getStorageDir(idx);      final StorageLocation sdLocation=sd.getStorageLocation();      LOG.info("Checking removing StorageLocation " + sdLocation + " with id "+ sd.getStorageUuid());      if (storageLocationsToRemove.contains(sdLocation)) {
  if (calculateChecksum) {    computeChecksum(srcReplica,dstMeta,smallBufferSize,conf);  } else {    try {      srcReplica.copyMetadata(dstMeta.toURI());    } catch (    IOException e) {      throw new IOException("Failed to copy " + srcReplica + " metadata to "+ dstMeta,e);    }  }  try {    srcReplica.copyBlockdata(dstFile.toURI());  } catch (  IOException e) {    throw new IOException("Failed to copy " + srcReplica + " block file to "+ dstFile,e);  }  if (LOG.isDebugEnabled()) {    if (calculateChecksum) {      LOG.debug("Copied " + srcReplica.getMetadataURI() + " meta to "+ dstMeta+ " and calculated checksum");    } else {
@Override public ReplicaHandler recoverAppend(ExtendedBlock b,long newGS,long expectedBlockLen) throws IOException {
@Override public Replica recoverClose(ExtendedBlock b,long newGS,long expectedBlockLen) throws IOException {
@Override public ReplicaHandler recoverRbw(ExtendedBlock b,long newGS,long minBytesRcvd,long maxBytesRcvd) throws IOException {
@Override public ReplicaInPipeline convertTemporaryToRbw(final ExtendedBlock b) throws IOException {  long startTimeMs=Time.monotonicNow();  try (AutoCloseableLock lock=datasetWriteLock.acquire()){    final long blockId=b.getBlockId();    final long expectedGs=b.getGenerationStamp();    final long visible=b.getNumBytes();
@Override public void adjustCrcChannelPosition(ExtendedBlock b,ReplicaOutputStreams streams,int checksumSize) throws IOException {  FileOutputStream file=(FileOutputStream)streams.getChecksumOut();  FileChannel channel=file.getChannel();  long oldPos=channel.position();  long newPos=oldPos - checksumSize;  if (LOG.isDebugEnabled()) {
          errors.add("Failed to delete replica " + invalidBlks[i] + ": GenerationStamp not matched, existing replica is "+ Block.toString(infoByBlockId));        }        continue;      }      v=(FsVolumeImpl)info.getVolume();      if (v == null) {        errors.add("Failed to delete replica " + invalidBlks[i] + ". No volume for replica "+ info);        continue;      }      try {        File blockFile=new File(info.getBlockURI());        if (blockFile != null && blockFile.getParentFile() == null) {          errors.add("Failed to delete replica " + invalidBlks[i] + ". Parent not found for block file: "+ blockFile);          continue;        }      } catch (      IllegalArgumentException e) {        LOG.warn("Parent directory check failed; replica " + info + " is not backed by a local file");      }      removing=volumeMap.remove(bpid,invalidBlks[i]);      addDeletingBlock(bpid,removing.getBlockId());
static ReplicaRecoveryInfo initReplicaRecoveryImpl(String bpid,ReplicaMap map,Block block,long recoveryId) throws IOException, MustStopExistingWriter {  final ReplicaInfo replica=map.get(bpid,block.getBlockId());
    if (!rip.attemptToSetWriter(null,Thread.currentThread())) {      throw new MustStopExistingWriter(rip);    }    if (replica.getBytesOnDisk() < replica.getVisibleLength()) {      throw new IOException("getBytesOnDisk() < getVisibleLength(), rip=" + replica);    }    checkReplicaFiles(replica);  }  if (replica.getGenerationStamp() < block.getGenerationStamp()) {    throw new IOException("replica.getGenerationStamp() < block.getGenerationStamp(), block=" + block + ", replica="+ replica);  }  if (replica.getGenerationStamp() >= recoveryId) {    throw new IOException("THIS IS NOT SUPPOSED TO HAPPEN:" + " replica.getGenerationStamp() >= recoveryId = " + recoveryId + ", block="+ block+ ", replica="+ replica);  }  final ReplicaInfo rur;  if (replica.getState() == ReplicaState.RUR) {    rur=replica;    if (rur.getRecoveryID() >= recoveryId) {      throw new RecoveryInProgressException("rur.getRecoveryID() >= recoveryId = " + recoveryId + ", block="+ block+ ", rur="+ rur);    }    final long oldRecoveryID=rur.getRecoveryID();
      throw new IOException("getBytesOnDisk() < getVisibleLength(), rip=" + replica);    }    checkReplicaFiles(replica);  }  if (replica.getGenerationStamp() < block.getGenerationStamp()) {    throw new IOException("replica.getGenerationStamp() < block.getGenerationStamp(), block=" + block + ", replica="+ replica);  }  if (replica.getGenerationStamp() >= recoveryId) {    throw new IOException("THIS IS NOT SUPPOSED TO HAPPEN:" + " replica.getGenerationStamp() >= recoveryId = " + recoveryId + ", block="+ block+ ", replica="+ replica);  }  final ReplicaInfo rur;  if (replica.getState() == ReplicaState.RUR) {    rur=replica;    if (rur.getRecoveryID() >= recoveryId) {      throw new RecoveryInProgressException("rur.getRecoveryID() >= recoveryId = " + recoveryId + ", block="+ block+ ", rur="+ rur);    }    final long oldRecoveryID=rur.getRecoveryID();    rur.setRecoveryID(recoveryId);    LOG.info("initReplicaRecovery: update recovery id for " + block + " from "+ oldRecoveryID+ " to "+ recoveryId);  } else {
@Override public void addBlockPool(String bpid,Configuration conf) throws IOException {
@Override public void shutdownBlockPool(String bpid){  try (AutoCloseableLock lock=datasetWriteLock.acquire()){
@Override public void onCompleteLazyPersist(String bpId,long blockId,long creationTime,File[] savedFiles,FsVolumeImpl targetVolume){  try (AutoCloseableLock lock=datasetWriteLock.acquire()){    ramDiskReplicaTracker.recordEndLazyPersist(bpId,blockId,savedFiles);    targetVolume.incDfsUsedAndNumBlocks(bpId,savedFiles[0].length() + savedFiles[1].length());    datanode.getMetrics().incrRamDiskBlocksLazyPersisted();    datanode.getMetrics().incrRamDiskBytesLazyPersisted(savedFiles[1].length());    datanode.getMetrics().addRamDiskBlocksLazyPersistWindowMs(Time.monotonicNow() - creationTime);    if (LOG.isDebugEnabled()) {
@Override MappableBlock load(long length,FileInputStream blockIn,FileInputStream metaIn,String blockFileName,ExtendedBlockId key) throws IOException {  PmemMappedBlock mappableBlock=null;  String cachePath=null;  FileChannel blockChannel=null;  RandomAccessFile cacheFile=null;  try {    blockChannel=blockIn.getChannel();    if (blockChannel == null) {      throw new IOException("Block InputStream has no FileChannel.");    }    cachePath=pmemVolumeManager.getCachePath(key);    cacheFile=new RandomAccessFile(cachePath,"rw");    blockChannel.transferTo(0,length,cacheFile.getChannel());    cacheFile.getChannel().position(0);    verifyChecksum(length,metaIn,cacheFile.getChannel(),blockFileName);    mappableBlock=new PmemMappedBlock(length,key);
  try {    blockChannel=blockIn.getChannel();    if (blockChannel == null) {      throw new IOException("Block InputStream has no FileChannel.");    }    cachePath=pmemVolumeManager.getCachePath(key);    cacheFile=new RandomAccessFile(cachePath,"rw");    blockChannel.transferTo(0,length,cacheFile.getChannel());    cacheFile.getChannel().position(0);    verifyChecksum(length,metaIn,cacheFile.getChannel(),blockFileName);    mappableBlock=new PmemMappedBlock(length,key);    LOG.info("Successfully cached one replica:{} into persistent memory" + ", [cached path={}, length={}]",key,cachePath,length);  }  finally {    IOUtils.closeQuietly(blockChannel);    IOUtils.closeQuietly(cacheFile);    if (mappableBlock == null) {
@Override public MappableBlock getRecoveredMappableBlock(File cacheFile,String bpid,byte volumeIndex) throws IOException {  ExtendedBlockId key=new ExtendedBlockId(getBlockId(cacheFile),bpid);  MappableBlock mappableBlock=new PmemMappedBlock(cacheFile.length(),key);  PmemVolumeManager.getInstance().recoverBlockKeyToVolume(key,volumeIndex);  String path=PmemVolumeManager.getInstance().getCachePath(key);  long length=mappableBlock.getLength();
@Override public void close(){  String cacheFilePath=null;  try {    cacheFilePath=PmemVolumeManager.getInstance().getCachePath(key);    FsDatasetUtil.deleteMappedFile(cacheFilePath);
private void loadVolumes(String[] volumes) throws IOException {  for (byte n=0; n < volumes.length; n++) {    try {      File pmemDir=new File(volumes[n]);      File realPmemDir=verifyIfValidPmemVolume(pmemDir);      if (!cacheRecoveryEnabled) {        cleanup(realPmemDir);      }      this.pmemVolumes.add(realPmemDir.getPath());      long maxBytes;      if (maxBytesPerPmem == -1) {        maxBytes=realPmemDir.getUsableSpace();      } else {        maxBytes=maxBytesPerPmem;      }      UsedBytesCount usedBytesCount=new UsedBytesCount(maxBytes);      this.usedBytesCounts.add(usedBytesCount);
    try {      File pmemDir=new File(volumes[n]);      File realPmemDir=verifyIfValidPmemVolume(pmemDir);      if (!cacheRecoveryEnabled) {        cleanup(realPmemDir);      }      this.pmemVolumes.add(realPmemDir.getPath());      long maxBytes;      if (maxBytesPerPmem == -1) {        maxBytes=realPmemDir.getUsableSpace();      } else {        maxBytes=maxBytesPerPmem;      }      UsedBytesCount usedBytesCount=new UsedBytesCount(maxBytes);      this.usedBytesCounts.add(usedBytesCount);      LOG.info("Added persistent memory - {} with size={}",volumes[n],maxBytes);    } catch (    IllegalArgumentException e) {
      if (!cacheRecoveryEnabled) {        cleanup(realPmemDir);      }      this.pmemVolumes.add(realPmemDir.getPath());      long maxBytes;      if (maxBytesPerPmem == -1) {        maxBytes=realPmemDir.getUsableSpace();      } else {        maxBytes=maxBytesPerPmem;      }      UsedBytesCount usedBytesCount=new UsedBytesCount(maxBytes);      this.usedBytesCounts.add(usedBytesCount);      LOG.info("Added persistent memory - {} with size={}",volumes[n],maxBytes);    } catch (    IllegalArgumentException e) {      LOG.error("Failed to parse persistent memory volume " + volumes[n],e);      continue;    }catch (    IOException e) {
@VisibleForTesting protected static String getSuffix(final Path prefix,final Path fullPath){  String prefixStr=prefix.toString();  String pathStr=fullPath.toString();  if (!pathStr.startsWith(prefixStr)) {
@Override void addBlockPool(String bpid,Configuration conf,Timer timer) throws IOException {
@Override public void compileReport(String bpid,Collection<ScanInfo> report,ReportCompiler reportCompiler) throws InterruptedException, IOException {
void submitLazyPersistTask(String bpId,long blockId,long genStamp,long creationTime,ReplicaInfo replica,FsVolumeReference target) throws IOException {  if (LOG.isDebugEnabled()) {
@Override protected void refresh(){  long start=Time.monotonicNow();  long dfsUsed=0;  long count=0;  FsDatasetSpi fsDataset=volume.getDataset();  try {    Collection<ReplicaInfo> replicaInfos=(Collection<ReplicaInfo>)fsDataset.deepCopyReplica(bpid);    long cost=Time.monotonicNow() - start;    if (cost > DEEP_COPY_REPLICA_THRESHOLD_MS) {
  FsDatasetSpi fsDataset=volume.getDataset();  try {    Collection<ReplicaInfo> replicaInfos=(Collection<ReplicaInfo>)fsDataset.deepCopyReplica(bpid);    long cost=Time.monotonicNow() - start;    if (cost > DEEP_COPY_REPLICA_THRESHOLD_MS) {      LOG.debug("Copy replica infos, blockPoolId: {}, replicas size: {}, " + "duration: {}ms",bpid,replicaInfos.size(),Time.monotonicNow() - start);    }    if (CollectionUtils.isNotEmpty(replicaInfos)) {      for (      ReplicaInfo replicaInfo : replicaInfos) {        if (Objects.equals(replicaInfo.getVolume().getStorageID(),volume.getStorageID())) {          dfsUsed+=replicaInfo.getBytesOnDisk();          dfsUsed+=replicaInfo.getMetadataLength();          count++;        }      }    }    this.used.set(dfsUsed);    cost=Time.monotonicNow() - start;    if (cost > REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS) {
    Collection<ReplicaInfo> replicaInfos=(Collection<ReplicaInfo>)fsDataset.deepCopyReplica(bpid);    long cost=Time.monotonicNow() - start;    if (cost > DEEP_COPY_REPLICA_THRESHOLD_MS) {      LOG.debug("Copy replica infos, blockPoolId: {}, replicas size: {}, " + "duration: {}ms",bpid,replicaInfos.size(),Time.monotonicNow() - start);    }    if (CollectionUtils.isNotEmpty(replicaInfos)) {      for (      ReplicaInfo replicaInfo : replicaInfos) {        if (Objects.equals(replicaInfo.getVolume().getStorageID(),volume.getStorageID())) {          dfsUsed+=replicaInfo.getBytesOnDisk();          dfsUsed+=replicaInfo.getMetadataLength();          count++;        }      }    }    this.used.set(dfsUsed);    cost=Time.monotonicNow() - start;    if (cost > REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS) {      LOG.debug("Refresh dfs used, bpid: {}, replicas size: {}, dfsUsed: {} " + "on volume: {}, duration: {}ms",bpid,count,used,volume.getStorageID(),Time.monotonicNow() - start);    }  } catch (  Exception e) {
          try {            fsVolumeReferences=dn.getFSDataset().getFsVolumeReferences();            Iterator<FsVolumeSpi> volumeIterator=fsVolumeReferences.iterator();            while (volumeIterator.hasNext()) {              FsVolumeSpi volume=volumeIterator.next();              DataNodeVolumeMetrics metrics=volume.getMetrics();              String volumeName=volume.getBaseURI().getPath();              metadataOpStats.put(volumeName,metrics.getMetadataOperationMean());              readIoStats.put(volumeName,metrics.getReadIoMean());              writeIoStats.put(volumeName,metrics.getWriteIoMean());            }          }  finally {            if (fsVolumeReferences != null) {              try {                fsVolumeReferences.close();              } catch (              IOException e) {
              readIoStats.put(volumeName,metrics.getReadIoMean());              writeIoStats.put(volumeName,metrics.getWriteIoMean());            }          }  finally {            if (fsVolumeReferences != null) {              try {                fsVolumeReferences.close();              } catch (              IOException e) {                LOG.error("Error in releasing FS Volume references",e);              }            }          }          if (metadataOpStats.isEmpty() && readIoStats.isEmpty() && writeIoStats.isEmpty()) {            LOG.debug("No disk stats available for detecting outliers.");            continue;          }          detectAndUpdateDiskOutliers(metadataOpStats,readIoStats,writeIoStats);        }        try {          Thread.sleep(detectionInterval);        } catch (        InterruptedException e) {
public Map<String,Double> getOutliers(Map<String,Double> stats){  if (stats.size() < minNumResources) {
@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause){
@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause){
      p.addLast(new HttpRequestEncoder(),new Forwarder(uri,client));    }  });  ChannelFuture f=proxiedServer.connect(host);  proxiedChannel=f.channel();  f.addListener(new ChannelFutureListener(){    @Override public void operationComplete(    ChannelFuture future) throws Exception {      if (future.isSuccess()) {        ctx.channel().pipeline().remove(HttpResponseEncoder.class);        HttpRequest newReq=new DefaultFullHttpRequest(HTTP_1_1,req.getMethod(),req.getUri());        newReq.headers().add(req.headers());        newReq.headers().set(CONNECTION,Values.CLOSE);        future.channel().writeAndFlush(newReq);      } else {        DefaultHttpResponse resp=new DefaultHttpResponse(HTTP_1_1,INTERNAL_SERVER_ERROR);        resp.headers().set(CONNECTION,Values.CLOSE);
@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause){  if (LOG.isDebugEnabled()) {
@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause){  releaseDfsResources();  DefaultHttpResponse resp=ExceptionHandler.exceptionCaught(cause);  resp.headers().set(CONNECTION,CLOSE);  ctx.writeAndFlush(resp).addListener(ChannelFutureListener.CLOSE);  if (LOG != null && LOG.isDebugEnabled()) {
@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause){
protected void recordOutput(final TextStringBuilder result,final String outputLine){
protected int parseTopNodes(final CommandLine cmd,final TextStringBuilder result) throws IllegalArgumentException {  String outputLine="";  int nodes=0;  final String topVal=cmd.getOptionValue(DiskBalancerCLI.TOP);  if (StringUtils.isBlank(topVal)) {    outputLine=String.format("No top limit specified, using default top value %d.",getDefaultTop());
@Override public void execute(CommandLine cmd) throws Exception {  LOG.info("Executing \"query plan\" command.");  Preconditions.checkState(cmd.hasOption(DiskBalancerCLI.QUERY));  verifyCommandOptions(DiskBalancerCLI.QUERY,cmd);  String nodeName=cmd.getOptionValue(DiskBalancerCLI.QUERY);  Preconditions.checkNotNull(nodeName);  nodeName=nodeName.trim();  String nodeAddress=nodeName;  if (!nodeName.matches("[^\\:]+:[0-9]{2,5}")) {    int defaultIPC=NetUtils.createSocketAddr(getConf().getTrimmed(DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_KEY,DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_DEFAULT)).getPort();    nodeAddress=nodeName + ":" + defaultIPC;
  String nodeName=cmd.getOptionValue(DiskBalancerCLI.QUERY);  Preconditions.checkNotNull(nodeName);  nodeName=nodeName.trim();  String nodeAddress=nodeName;  if (!nodeName.matches("[^\\:]+:[0-9]{2,5}")) {    int defaultIPC=NetUtils.createSocketAddr(getConf().getTrimmed(DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_KEY,DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_DEFAULT)).getPort();    nodeAddress=nodeName + ":" + defaultIPC;    LOG.debug("Using default data node port :  {}",nodeAddress);  }  ClientDatanodeProtocol dataNode=getDataNodeProxy(nodeAddress);  try {    DiskBalancerWorkStatus workStatus=dataNode.queryDiskBalancerPlan();    System.out.printf("Plan File: %s%nPlan ID: %s%nResult: %s%n",workStatus.getPlanFile(),workStatus.getPlanID(),workStatus.getResult().toString());    if (cmd.hasOption(DiskBalancerCLI.VERBOSE)) {      System.out.printf("%s",workStatus.currentStateString());    }  } catch (  DiskBalancerException ex) {
public static ClusterConnector getCluster(URI clusterURI,Configuration conf) throws IOException, URISyntaxException {
public static ClusterConnector getCluster(URI clusterURI,Configuration conf) throws IOException, URISyntaxException {  LOG.debug("Cluster URI : {}",clusterURI);
@Override public List<DiskBalancerDataNode> getNodes() throws Exception {  Preconditions.checkNotNull(this.clusterURI);  String dataFilePath=this.clusterURI.getPath();
@Override public List<DiskBalancerDataNode> getNodes() throws Exception {  Preconditions.checkNotNull(this.clusterURI);  String dataFilePath=this.clusterURI.getPath();  LOG.info("Reading cluster info from file : " + dataFilePath);  DiskBalancerCluster cluster=READER.readValue(new File(dataFilePath));  String message=String.format("Found %d node(s)",cluster.getNodes().size());
public void readClusterInfo() throws Exception {  Preconditions.checkNotNull(clusterConnector);
private void skipMisConfiguredVolume(DiskBalancerVolume volume){  String errMessage=String.format("Real capacity is negative." + "This usually points to some " + "kind of mis-configuration.%n"+ "Capacity : %d Reserved : %d "+ "realCap = capacity - "+ "reserved = %d.%n"+ "Skipping this volume from "+ "all processing. type : %s id"+ " :%s",volume.getCapacity(),volume.getReserved(),volume.computeEffectiveCapacity(),volume.getStorageType(),volume.getUuid());
@Override public NodePlan plan(DiskBalancerDataNode node) throws Exception {  final long startTime=Time.monotonicNow();  NodePlan plan=new NodePlan(node.getDataNodeName(),node.getDataNodePort());
  Preconditions.checkNotNull(vSet);  Preconditions.checkNotNull(plan);  Preconditions.checkNotNull(node);  DiskBalancerVolumeSet currentSet=new DiskBalancerVolumeSet(vSet);  while (currentSet.isBalancingNeeded(this.threshold)) {    removeSkipVolumes(currentSet);    DiskBalancerVolume lowVolume=currentSet.getSortedQueue().first();    DiskBalancerVolume highVolume=currentSet.getSortedQueue().last();    Step nextStep=null;    if (!lowVolume.isSkip() && !highVolume.isSkip()) {      nextStep=computeMove(currentSet,lowVolume,highVolume);    } else {      LOG.debug("Skipping compute move. lowVolume: {} highVolume: {}",lowVolume.getPath(),highVolume.getPath());    }    applyStep(nextStep,currentSet,lowVolume,highVolume);    if (nextStep != null) {
private Step computeMove(DiskBalancerVolumeSet currentSet,DiskBalancerVolume lowVolume,DiskBalancerVolume highVolume){  long maxLowVolumeCanReceive=(long)((currentSet.getIdealUsed() * lowVolume.computeEffectiveCapacity()) - lowVolume.getUsed());  if (maxLowVolumeCanReceive <= 0) {
private void skipVolume(DiskBalancerVolumeSet currentSet,DiskBalancerVolume volume){  if (LOG.isDebugEnabled()) {    String message=String.format("Skipping volume. Volume : %s " + "Type : %s Target " + "Number of bytes : %f lowVolume dfsUsed : %d. Skipping this "+ "volume from all future balancing calls.",volume.getPath(),volume.getStorageType(),currentSet.getIdealUsed() * volume.getCapacity(),volume.getUsed());
private void printQueue(TreeSet<DiskBalancerVolume> queue){  if (LOG.isDebugEnabled()) {    String format=String.format("First Volume : %s, DataDensity : %f, " + "Last Volume : %s, DataDensity : %f",queue.first().getPath(),queue.first().getVolumeDataDensity(),queue.last().getPath(),queue.last().getVolumeDataDensity());
public static Planner getPlanner(String plannerName,DiskBalancerDataNode node,double threshold){  if (plannerName.equals(GREEDY_PLANNER)) {    if (LOG.isDebugEnabled()) {      String message=String.format("Creating a %s for Node : %s IP : %s ID : %s",GREEDY_PLANNER,node.getDataNodeName(),node.getDataNodeIP(),node.getDataNodeUUID());
static int run(Map<URI,List<Path>> namenodes,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getTimeDuration(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS) * 2 + conf.getTimeDuration(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);  AtomicInteger retryCount=new AtomicInteger(0);  Map<Long,Set<DatanodeInfo>> excludedPinnedBlocks=new HashMap<>();
private synchronized void setState(BNState newState){  if (LOG.isDebugEnabled()) {
  NamenodeRegistration nnReg=null;  while (!isStopRequested()) {    try {      nnReg=namenode.registerSubordinateNamenode(getRegistration());      break;    } catch (    SocketTimeoutException e) {      LOG.info("Problem connecting to name-node: " + nnRpcAddress);      try {        Thread.sleep(1000);      } catch (      InterruptedException ie) {        LOG.warn("Encountered exception ",e);      }    }  }  String msg=null;  if (nnReg == null)   msg="Registration rejected by " + nnRpcAddress; else   if (!nnReg.isRole(NamenodeRole.NAMENODE)) {    msg="Name-node " + nnRpcAddress + " is not active";
public final void processCacheReport(final DatanodeID datanodeID,final List<Long> blockIds) throws IOException {  if (!enabled) {
private void initialize(Configuration conf) throws IOException {  shouldRun=true;  checkpointConf=new CheckpointConf(conf);  String fullInfoAddr=conf.get(DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY,DFS_NAMENODE_BACKUP_HTTP_ADDRESS_DEFAULT);  infoBindAddress=fullInfoAddr.substring(0,fullInfoAddr.indexOf(":"));  LOG.info("Checkpoint Period : " + checkpointConf.getPeriod() + " secs "+ "("+ checkpointConf.getPeriod() / 60 + " min)");
  }  while (shouldRun) {    try {      long now=monotonicNow();      boolean shouldCheckpoint=false;      if (now >= lastCheckpointTime + checkpointPeriodMSec) {        shouldCheckpoint=true;      } else       if (now >= lastEditLogCheckTime + periodMSec) {        long txns=countUncheckpointedTxns();        lastEditLogCheckTime=now;        if (txns >= checkpointConf.getTxnCount())         shouldCheckpoint=true;      }      if (shouldCheckpoint) {        doCheckpoint();        lastCheckpointTime=now;        lastEditLogCheckTime=now;      }    } catch (    IOException e) {
    try {      long now=monotonicNow();      boolean shouldCheckpoint=false;      if (now >= lastCheckpointTime + checkpointPeriodMSec) {        shouldCheckpoint=true;      } else       if (now >= lastEditLogCheckTime + periodMSec) {        long txns=countUncheckpointedTxns();        lastEditLogCheckTime=now;        if (txns >= checkpointConf.getTxnCount())         shouldCheckpoint=true;      }      if (shouldCheckpoint) {        doCheckpoint();        lastCheckpointTime=now;        lastEditLogCheckTime=now;      }    } catch (    IOException e) {      LOG.error("Exception in doCheckpoint: ",e);
  NNStorage bnStorage=bnImage.getStorage();  long startTime=monotonicNow();  bnImage.freezeNamespaceAtNextRoll();  NamenodeCommand cmd=getRemoteNamenodeProxy().startCheckpoint(backupNode.getRegistration());  CheckpointCommand cpCmd=null;switch (cmd.getAction()) {case NamenodeProtocol.ACT_SHUTDOWN:    shutdown();  throw new IOException("Name-node " + backupNode.nnRpcAddress + " requested shutdown.");case NamenodeProtocol.ACT_CHECKPOINT:cpCmd=(CheckpointCommand)cmd;break;default:throw new IOException("Unsupported NamenodeCommand: " + cmd.getAction());}bnImage.waitUntilNamespaceFrozen();CheckpointSignature sig=cpCmd.getSignature();sig.validateStorageInfo(bnImage);long lastApplied=bnImage.getLastAppliedTxId();
case NamenodeProtocol.ACT_SHUTDOWN:    shutdown();  throw new IOException("Name-node " + backupNode.nnRpcAddress + " requested shutdown.");case NamenodeProtocol.ACT_CHECKPOINT:cpCmd=(CheckpointCommand)cmd;break;default:throw new IOException("Unsupported NamenodeCommand: " + cmd.getAction());}bnImage.waitUntilNamespaceFrozen();CheckpointSignature sig=cpCmd.getSignature();sig.validateStorageInfo(bnImage);long lastApplied=bnImage.getLastAppliedTxId();LOG.debug("Doing checkpoint. Last applied: " + lastApplied);RemoteEditLogManifest manifest=getRemoteNamenodeProxy().getEditLogManifest(bnImage.getLastAppliedTxId() + 1);boolean needReloadImage=false;if (!manifest.getLogs().isEmpty()) {RemoteEditLog firstRemoteLog=manifest.getLogs().get(0);if (firstRemoteLog.getStartTxId() > lastApplied + 1) {
LOG.debug("Doing checkpoint. Last applied: " + lastApplied);RemoteEditLogManifest manifest=getRemoteNamenodeProxy().getEditLogManifest(bnImage.getLastAppliedTxId() + 1);boolean needReloadImage=false;if (!manifest.getLogs().isEmpty()) {RemoteEditLog firstRemoteLog=manifest.getLogs().get(0);if (firstRemoteLog.getStartTxId() > lastApplied + 1) {LOG.info("Unable to roll forward using only logs. Downloading " + "image with txid " + sig.mostRecentCheckpointTxId);MD5Hash downloadedHash=TransferFsImage.downloadImageToStorage(backupNode.nnHttpAddress,sig.mostRecentCheckpointTxId,bnStorage,true,false);bnImage.saveDigestAndRenameCheckpointImage(NameNodeFile.IMAGE,sig.mostRecentCheckpointTxId,downloadedHash);lastApplied=sig.mostRecentCheckpointTxId;needReloadImage=true;}if (firstRemoteLog.getStartTxId() > lastApplied + 1) {throw new IOException("No logs to roll forward from " + lastApplied);}for (RemoteEditLog log : manifest.getLogs()) {TransferFsImage.downloadEditsToStorage(backupNode.nnHttpAddress,log,bnStorage);
      init(true);    } catch (    Throwable e) {      LOG.error("caught exception initializing " + this,e);      if (skipBrokenEdits) {        return null;      }      Throwables.propagateIfPossible(e,IOException.class);    }  Preconditions.checkState(state != State.UNINIT);return nextOpImpl(skipBrokenEdits);case OPEN:op=reader.readOp(skipBrokenEdits);if ((op != null) && (op.hasTransactionId())) {long txId=op.getTransactionId();if ((txId >= lastTxId) && (lastTxId != HdfsServerConstants.INVALID_TXID)) {long skipAmt=log.length() - tracker.getPos();if (skipAmt > 0) {  if (LOG.isDebugEnabled()) {
  long size=fc.size();  int bufSize=doubleBuf.getReadyBuf().getLength();  long need=bufSize - (size - position);  if (need <= 0) {    return;  }  long oldSize=size;  long total=0;  long fillCapacity=fill.capacity();  while (need > 0) {    fill.position(0);    IOUtils.writeFully(fc,fill,size);    need-=fillCapacity;    size+=fillCapacity;    total+=fillCapacity;  }  if (LOG.isDebugEnabled()) {
void updateCountForQuota(int initThreads){  writeLock();  try {    int threads=(initThreads < 1) ? 1 : initThreads;
void updateCountForQuota(int initThreads){  writeLock();  try {    int threads=(initThreads < 1) ? 1 : initThreads;    LOG.info("Initializing quota with " + threads + " thread(s)");    long start=Time.monotonicNow();    QuotaCounts counts=new QuotaCounts.Builder().build();    ForkJoinPool p=new ForkJoinPool(threads);    RecursiveAction task=new InitQuotaTask(getBlockStoragePolicySuite(),rootDir.getStoragePolicyID(),rootDir,counts);    p.execute(task);    task.join();    p.shutdown();
private void copyINodeDefaultAcl(INode child,FsPermission modes){  if (LOG.isDebugEnabled()) {
static FSEditLog newInstance(Configuration conf,NNStorage storage,List<URI> editsDirs){  boolean asyncEditLogging=conf.getBoolean(DFSConfigKeys.DFS_NAMENODE_EDITS_ASYNC_LOGGING,DFSConfigKeys.DFS_NAMENODE_EDITS_ASYNC_LOGGING_DEFAULT);
void logSyncAll(){  long lastWrittenTxId=getLastWrittenTxId();
void logSyncAll(){  long lastWrittenTxId=getLastWrittenTxId();  LOG.info("logSyncAll toSyncToTxId=" + lastWrittenTxId + " lastSyncedTxid="+ synctxid+ " mostRecentTxid="+ txid);  logSync(lastWrittenTxId);  lastWrittenTxId=getLastWrittenTxId();
          try {            wait(1000);          } catch (          InterruptedException ie) {          }        }        if (mytxid <= synctxid) {          return;        }        editsBatchedInSync=txid - synctxid - 1;        syncStart=txid;        isSyncRunning=true;        sync=true;        try {          if (journalSet.isEmpty()) {            throw new IOException("No journals available to flush");          }          editLogStream.setReadyToFlush();        } catch (        IOException e) {          final String msg="Could not sync enough journals to persistent storage " + "due to " + e.getMessage() + ". "+ "Unsynced transactions: "+ (txid - synctxid);
          if (journalSet.isEmpty()) {            throw new IOException("No journals available to flush");          }          editLogStream.setReadyToFlush();        } catch (        IOException e) {          final String msg="Could not sync enough journals to persistent storage " + "due to " + e.getMessage() + ". "+ "Unsynced transactions: "+ (txid - synctxid);          LOG.error(msg,new Exception());synchronized (journalSetLock) {            IOUtils.cleanupWithLogger(LOG,journalSet);          }          terminate(1,msg);        }      }  finally {        doneWithAutoSyncScheduling();      }      logStream=editLogStream;    }    long start=monotonicNow();    try {      if (logStream != null) {
public synchronized void startLogSegment(long txid,boolean abortCurrentLogSegment,int layoutVersion) throws IOException {
synchronized void registerBackupNode(NamenodeRegistration bnReg,NamenodeRegistration nnReg) throws IOException {  if (bnReg.isRole(NamenodeRole.CHECKPOINT))   return;  JournalManager jas=findBackupJournal(bnReg);  if (jas != null) {
synchronized void releaseBackupStream(NamenodeRegistration registration) throws IOException {  BackupJournalManager bjm=this.findBackupJournal(registration);  if (bjm != null) {
@Override public void logSync(){  Edit edit=THREAD_EDIT.get();  if (edit != null) {    THREAD_EDIT.set(null);    if (LOG.isDebugEnabled()) {
private void enqueueEdit(Edit edit){  if (LOG.isDebugEnabled()) {
private void terminate(Throwable t){  String message="Exception while edit logging: " + t.getMessage();
void format(FSNamesystem fsn,String clusterId,boolean force) throws IOException {  long fileCount=fsn.getFilesTotal();  Preconditions.checkState(fileCount == 1,"FSImage.format should be called with an uninitialized namesystem, has " + fileCount + " files");  NamespaceInfo ns=NNStorage.newNamespaceInfo();
    for (Iterator<StorageDirectory> it=storage.dirIterator(false); it.hasNext(); ) {      StorageDirectory sd=it.next();      if (!NNUpgradeUtil.canRollBack(sd,storage,prevState.getStorage(),HdfsServerConstants.NAMENODE_LAYOUT_VERSION)) {        continue;      }      LOG.info("Can perform rollback for " + sd);      canRollback=true;    }    if (fsns.isHaEnabled()) {      editLog.initJournalsForWrite();      boolean canRollBackSharedEditLog=editLog.canRollBackSharedLog(prevState.getStorage(),HdfsServerConstants.NAMENODE_LAYOUT_VERSION);      if (canRollBackSharedEditLog) {        LOG.info("Can perform rollback for shared edit log.");        canRollback=true;      }    }    if (!canRollback)     throw new IOException("Cannot rollback. None of the storage " + "directories contain previous fs state.");    for (Iterator<StorageDirectory> it=storage.dirIterator(false); it.hasNext(); ) {      StorageDirectory sd=it.next();
void reloadFromImageFile(File file,FSNamesystem target) throws IOException {  target.clear();
  } else {    editStreams=FSImagePreTransactionalStorageInspector.getEditLogStreams(storage);  }  int maxOpSize=conf.getInt(DFSConfigKeys.DFS_NAMENODE_MAX_OP_SIZE_KEY,DFSConfigKeys.DFS_NAMENODE_MAX_OP_SIZE_DEFAULT);  for (  EditLogInputStream elis : editStreams) {    elis.setMaxOpSize(maxOpSize);  }  for (  EditLogInputStream l : editStreams) {    LOG.debug("Planning to load edit log stream: " + l);  }  if (!editStreams.iterator().hasNext()) {    LOG.info("No edit log streams selected.");  }  FSImageFile imageFile=null;  for (int i=0; i < imageFiles.size(); i++) {    try {      imageFile=imageFiles.get(i);      loadFSImageFile(target,recovery,imageFile,startOpt);      break;
void loadFSImageFile(FSNamesystem target,MetaRecoveryContext recovery,FSImageFile imageFile,StartupOption startupOption) throws IOException {
void saveFSImage(SaveNamespaceContext context,StorageDirectory sd,NameNodeFile dstType) throws IOException {  long txid=context.getTxId();  File newFile=NNStorage.getStorageFile(sd,NameNodeFile.IMAGE_NEW,txid);  File dstFile=NNStorage.getStorageFile(sd,dstType,txid);  FSImageFormatProtobuf.Saver saver=new FSImageFormatProtobuf.Saver(context,conf);  FSImageCompression compression=FSImageCompression.createCompression(conf);  long numErrors=saver.save(newFile,compression);  if (numErrors > 0) {
private void renameImageFileInDir(StorageDirectory sd,NameNodeFile fromNnf,NameNodeFile toNnf,long txid,boolean renameMD5) throws IOException {  final File fromFile=NNStorage.getStorageFile(sd,fromNnf,txid);  final File toFile=NNStorage.getStorageFile(sd,toNnf,txid);  if (LOG.isDebugEnabled()) {
NamenodeCommand startCheckpoint(NamenodeRegistration bnReg,NamenodeRegistration nnReg,int layoutVersion) throws IOException {  LOG.info("Start checkpoint at txid " + getEditLog().getLastWrittenTxId());  String msg=null;  if (bnReg.getNamespaceID() != storage.getNamespaceID())   msg="Name node " + bnReg.getAddress() + " has incompatible namespace id: "+ bnReg.getNamespaceID()+ " expected: "+ storage.getNamespaceID(); else   if (bnReg.isRole(NamenodeRole.NAMENODE))   msg="Name node " + bnReg.getAddress() + " role "+ bnReg.getRole()+ ": checkpoint is not allowed."; else   if (bnReg.getLayoutVersion() < storage.getLayoutVersion() || (bnReg.getLayoutVersion() == storage.getLayoutVersion() && bnReg.getCTime() > storage.getCTime()))   msg="Name node " + bnReg.getAddress() + " has newer image layout version: LV = "+ bnReg.getLayoutVersion()+ " cTime = "+ bnReg.getCTime()+ ". Current version: LV = "+ storage.getLayoutVersion()+ " cTime = "+ storage.getCTime();  if (msg != null) {
private static void setRenameReservedMapInternal(String renameReserved){  Collection<String> pairs=StringUtils.getTrimmedStringCollection(renameReserved);  for (  String p : pairs) {    String[] pair=StringUtils.split(p,'/','=');    Preconditions.checkArgument(pair.length == 2,"Could not parse key-value pair " + p);    String key=pair[0];    String value=pair[1];    Preconditions.checkArgument(DFSUtil.isReservedPathComponent(key),"Unknown reserved path " + key);    Preconditions.checkArgument(DFSUtil.isValidNameForComponent(value),"Invalid rename path for " + key + ": "+ value);
static String renameReservedPathsOnUpgrade(String path,final int layoutVersion) throws IllegalReservedPathException {  final String oldPath=path;  if (!NameNodeLayoutVersion.supports(Feature.ADD_INODE_ID,layoutVersion)) {    String[] components=INode.getPathNames(path);    if (components.length > 1) {      components[1]=DFSUtil.bytes2String(renameReservedRootComponentOnUpgrade(DFSUtil.string2Bytes(components[1]),layoutVersion));      path=DFSUtil.strings2PathString(components);    }  }  if (!NameNodeLayoutVersion.supports(Feature.SNAPSHOT,layoutVersion)) {    String[] components=INode.getPathNames(path);    if (components.length == 0) {      return path;    }    for (int i=0; i < components.length; i++) {      components[i]=DFSUtil.bytes2String(renameReservedComponentOnUpgrade(DFSUtil.string2Bytes(components[i]),layoutVersion));    }    path=DFSUtil.strings2PathString(components);  }  if (!path.equals(oldPath)) {
@Override List<FSImageFile> getLatestImages() throws IOException {  if (latestNameSD == null)   throw new IOException("Image file is not found in " + imageDirs);  if (latestEditsSD == null)   throw new IOException("Edits file is not found in " + editsDirs);  if (latestNameCheckpointTime > latestEditsCheckpointTime && latestNameSD != latestEditsSD && latestNameSD.getStorageDirType() == NameNodeDirType.IMAGE && latestEditsSD.getStorageDirType() == NameNodeDirType.EDITS) {    LOG.error("This is a rare failure scenario!!!");
@Override public void inspectDirectory(StorageDirectory sd) throws IOException {  if (!sd.getVersionFile().exists()) {
    needToSave|=true;    return;  }  try {    maxSeenTxId=Math.max(maxSeenTxId,NNStorage.readTransactionIdFile(sd));  } catch (  IOException ioe) {    LOG.warn("Unable to determine the max transaction ID seen by " + sd,ioe);    return;  }  File currentDir=sd.getCurrentDir();  File filesInStorage[];  try {    filesInStorage=FileUtil.listFiles(currentDir);  } catch (  IOException ioe) {    LOG.warn("Unable to inspect storage directory " + currentDir,ioe);    return;  }  for (  File f : filesInStorage) {
  }  File currentDir=sd.getCurrentDir();  File filesInStorage[];  try {    filesInStorage=FileUtil.listFiles(currentDir);  } catch (  IOException ioe) {    LOG.warn("Unable to inspect storage directory " + currentDir,ioe);    return;  }  for (  File f : filesInStorage) {    LOG.debug("Checking file " + f);    String name=f.getName();    Matcher imageMatch=this.matchPattern(name);    if (imageMatch != null) {      if (sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)) {        try {          long txid=Long.parseLong(imageMatch.group(1));
static FSNamesystem loadFromDisk(Configuration conf) throws IOException {  checkConfiguration(conf);  FSImage fsImage=new FSImage(conf,FSNamesystem.getNamespaceDirs(conf),FSNamesystem.getNamespaceEditsDirs(conf));  FSNamesystem namesystem=new FSNamesystem(conf,fsImage,false);  StartupOption startOpt=NameNode.getStartupOption(conf);  if (startOpt == StartupOption.RECOVER) {    namesystem.setSafeMode(SafeModeAction.SAFEMODE_ENTER);  }  long loadStart=monotonicNow();  try {    namesystem.loadFSImage(startOpt);  } catch (  IOException ioe) {    LOG.warn("Encountered exception loading fsimage",ioe);    fsImage.close();    throw ioe;  }  long timeTakenToLoadFSImage=monotonicNow() - loadStart;
@VisibleForTesting static RetryCache initRetryCache(Configuration conf){  boolean enable=conf.getBoolean(DFS_NAMENODE_ENABLE_RETRY_CACHE_KEY,DFS_NAMENODE_ENABLE_RETRY_CACHE_DEFAULT);
@VisibleForTesting static RetryCache initRetryCache(Configuration conf){  boolean enable=conf.getBoolean(DFS_NAMENODE_ENABLE_RETRY_CACHE_KEY,DFS_NAMENODE_ENABLE_RETRY_CACHE_DEFAULT);  LOG.info("Retry cache on namenode is " + (enable ? "enabled" : "disabled"));  if (enable) {    float heapPercent=conf.getFloat(DFS_NAMENODE_RETRY_CACHE_HEAP_PERCENT_KEY,DFS_NAMENODE_RETRY_CACHE_HEAP_PERCENT_DEFAULT);    long entryExpiryMillis=conf.getLong(DFS_NAMENODE_RETRY_CACHE_EXPIRYTIME_MILLIS_KEY,DFS_NAMENODE_RETRY_CACHE_EXPIRYTIME_MILLIS_DEFAULT);
  List<AuditLogger> auditLoggers=Lists.newArrayList();  boolean topAuditLoggerAdded=false;  if (alClasses != null && !alClasses.isEmpty()) {    for (    String className : alClasses) {      try {        AuditLogger logger;        if (DFS_NAMENODE_DEFAULT_AUDIT_LOGGER_NAME.equals(className)) {          logger=new FSNamesystemAuditLogger();        } else {          logger=(AuditLogger)Class.forName(className).newInstance();          if (TopAuditLogger.class.getName().equals(logger.getClass().getName())) {            topAuditLoggerAdded=true;          }        }        logger.initialize(conf);        auditLoggers.add(logger);      } catch (      InstantiationException e) {
      editLog.initJournalsForWrite();      editLog.recoverUnclosedStreams();      LOG.info("Catching up to latest edits from old active before " + "taking over writer role in edits logs");      editLogTailer.catchupDuringFailover();      blockManager.setPostponeBlocksFromFuture(false);      blockManager.getDatanodeManager().markAllDatanodesStale();      blockManager.clearQueues();      blockManager.processAllPendingDNMessages();      blockManager.getBlockIdManager().applyImpendingGenerationStamp();      if (!isInSafeMode()) {        LOG.info("Reprocessing replication and invalidation queues");        blockManager.initializeReplQueues();      }      if (LOG.isDebugEnabled()) {        LOG.debug("NameNode metadata after re-processing " + "replication and invalidation queues during failover:\n" + metaSaveAsString());      }      long nextTxId=getFSImage().getLastAppliedTxId() + 1;
void startStandbyServices(final Configuration conf,boolean isObserver) throws IOException {
void stopStandbyServices() throws IOException {  HAServiceState curState=getState() == OBSERVER ? OBSERVER : STANDBY;
CryptoProtocolVersion chooseProtocolVersion(EncryptionZone zone,CryptoProtocolVersion[] supportedVersions) throws UnknownCryptoProtocolVersionException, UnresolvedLinkException, SnapshotAccessControlException {  Preconditions.checkNotNull(zone);  Preconditions.checkNotNull(supportedVersions);  final CryptoProtocolVersion required=zone.getVersion();  for (  CryptoProtocolVersion c : supportedVersions) {    if (c.equals(CryptoProtocolVersion.UNKNOWN)) {
private boolean checkBlocksComplete(String src,boolean allowCommittedBlock,BlockInfo... blocks){  final int n=allowCommittedBlock ? numCommittedAllowed : 0;  for (int i=0; i < blocks.length; i++) {    final short min=blockManager.getMinStorageNum(blocks[i]);    final String err=INodeFile.checkBlockComplete(blocks,i,n,min);    if (err != null) {      final int numNodes=blocks[i].numNodes();
void commitBlockSynchronization(ExtendedBlock oldBlock,long newgenerationstamp,long newlength,boolean closeFile,boolean deleteblock,DatanodeID[] newtargets,String[] newtargetstorages) throws IOException {
void commitBlockSynchronization(ExtendedBlock oldBlock,long newgenerationstamp,long newlength,boolean closeFile,boolean deleteblock,DatanodeID[] newtargets,String[] newtargetstorages) throws IOException {  LOG.info("commitBlockSynchronization(oldBlock=" + oldBlock + ", newgenerationstamp="+ newgenerationstamp+ ", newlength="+ newlength+ ", newtargets="+ Arrays.asList(newtargets)+ ", closeFile="+ closeFile+ ", deleteBlock="+ deleteblock+ ")");  checkOperation(OperationCategory.WRITE);  final String src;  writeLock();  boolean copyTruncate=false;  BlockInfo truncatedBlock=null;  try {    checkOperation(OperationCategory.WRITE);    checkNameNodeSafeMode("Cannot commitBlockSynchronization while in safe mode");    final BlockInfo storedBlock=getStoredBlock(ExtendedBlock.getLocalBlock(oldBlock));    if (storedBlock == null) {      if (deleteblock) {
      if (deleteblock) {        LOG.debug("Block (={}) not found",oldBlock);        return;      } else {        throw new IOException("Block (=" + oldBlock + ") not found");      }    }    final long oldGenerationStamp=storedBlock.getGenerationStamp();    final long oldNumBytes=storedBlock.getNumBytes();    if (storedBlock.isDeleted()) {      throw new IOException("The blockCollection of " + storedBlock + " is null, likely because the file owning this block was"+ " deleted and the block removal is delayed");    }    final INodeFile iFile=getBlockCollection(storedBlock);    src=iFile.getFullPathName();    if (isFileDeleted(iFile)) {      throw new FileNotFoundException("File not found: " + src + ", likely due to delayed block removal");    }    if ((!iFile.isUnderConstruction() || storedBlock.isComplete()) && iFile.getLastBlock().isComplete()) {      if (LOG.isDebugEnabled()) {
              dsInfos[i].addBlock(truncatedBlock,truncatedBlock);            } else {              Block bi=new Block(storedBlock);              if (storedBlock.isStriped()) {                bi.setBlockId(bi.getBlockId() + i);              }              dsInfos[i].addBlock(storedBlock,bi);            }          }        }      }      if (copyTruncate) {        iFile.convertLastBlockToUC(truncatedBlock,dsInfos);      } else {        iFile.convertLastBlockToUC(storedBlock,dsInfos);        if (closeFile) {          blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),storedBlock,oldGenerationStamp,oldNumBytes,dsInfos);        }      }    }    if (closeFile) {      if (copyTruncate) {        closeFileCommitBlocks(src,iFile,truncatedBlock);
 else {              Block bi=new Block(storedBlock);              if (storedBlock.isStriped()) {                bi.setBlockId(bi.getBlockId() + i);              }              dsInfos[i].addBlock(storedBlock,bi);            }          }        }      }      if (copyTruncate) {        iFile.convertLastBlockToUC(truncatedBlock,dsInfos);      } else {        iFile.convertLastBlockToUC(storedBlock,dsInfos);        if (closeFile) {          blockManager.markBlockReplicasAsCorrupt(oldBlock.getLocalBlock(),storedBlock,oldGenerationStamp,oldNumBytes,dsInfos);        }      }    }    if (closeFile) {      if (copyTruncate) {        closeFileCommitBlocks(src,iFile,truncatedBlock);        if (!iFile.isBlockInLatestSnapshot(storedBlock)) {
    LinkedHashMap<Integer,HdfsPartialListing> listings=Maps.newLinkedHashMap();    DirectoryListing lastListing=null;    int numEntries=0;    for (; srcsIndex < srcs.length; srcsIndex++) {      String src=srcs[srcsIndex];      HdfsPartialListing listing;      try {        DirectoryListing dirListing=getListingInt(dir,pc,src,indexStartAfter,needLocation);        if (dirListing == null) {          throw new FileNotFoundException("Path " + src + " does not exist");        }        listing=new HdfsPartialListing(srcsIndex,Lists.newArrayList(dirListing.getPartialListing()));        numEntries+=listing.getPartialListing().size();        lastListing=dirListing;      } catch (      Exception e) {        if (e instanceof AccessControlException) {
CheckpointSignature rollEditLog() throws IOException {  String operationName="rollEditLog";  CheckpointSignature result=null;  checkSuperuserPrivilege(operationName);  checkOperation(OperationCategory.JOURNAL);  writeLock();  try {    checkOperation(OperationCategory.JOURNAL);    checkNameNodeSafeMode("Log not rolled");    if (Server.isRpcInvocation()) {
NamenodeCommand startCheckpoint(NamenodeRegistration backupNode,NamenodeRegistration activeNamenode) throws IOException {  checkOperation(OperationCategory.CHECKPOINT);  writeLock();  try {    checkOperation(OperationCategory.CHECKPOINT);    checkNameNodeSafeMode("Checkpoint not started");
void endCheckpoint(NamenodeRegistration registration,CheckpointSignature sig) throws IOException {  checkOperation(OperationCategory.CHECKPOINT);  readLock();  try {    checkOperation(OperationCategory.CHECKPOINT);    checkNameNodeSafeMode("Checkpoint not ended");
boolean disableErasureCodingPolicy(String ecPolicyName,final boolean logRetryCache) throws IOException {  final String operationName="disableErasureCodingPolicy";  checkOperation(OperationCategory.WRITE);  checkErasureCodingSupported(operationName);  boolean success=false;
void checkPermission(INodesInPath inodesInPath,boolean doCheckOwner,FsAction ancestorAccess,FsAction parentAccess,FsAction access,FsAction subAccess,boolean ignoreEmptyDir) throws AccessControlException {  if (LOG.isDebugEnabled()) {
private static void checkNotSymlink(INode inode,byte[][] components,int i) throws UnresolvedPathException {  if (inode != null && inode.isSymlink()) {    final int last=components.length - 1;    final String path=getPath(components,0,last);    final String preceding=getPath(components,0,i - 1);    final String remainder=getPath(components,i + 1,last);    final String target=inode.asSymlink().getSymlinkString();    if (LOG.isDebugEnabled()) {      final String link=inode.getLocalName();
@Override synchronized public void finalizeLogSegment(long firstTxId,long lastTxId) throws IOException {  File inprogressFile=NNStorage.getInProgressEditsFile(sd,firstTxId);  File dstFile=NNStorage.getFinalizedEditsFile(sd,firstTxId,lastTxId);
@Override public void purgeLogsOlderThan(long minTxIdToKeep) throws IOException {
private void discardEditLogSegments(long startTxId) throws IOException {  File currentDir=sd.getCurrentDir();  List<EditLogFile> allLogFiles=matchEditLogs(currentDir);  List<EditLogFile> toTrash=Lists.newArrayList();
    if (editsMatch.matches()) {      try {        long startTxId=Long.parseLong(editsMatch.group(1));        long endTxId=Long.parseLong(editsMatch.group(2));        ret.add(new EditLogFile(f,startTxId,endTxId));        continue;      } catch (      NumberFormatException nfe) {        LOG.error("Edits file " + f + " has improperly formatted "+ "transaction ID");      }    }    Matcher inProgressEditsMatch=EDITS_INPROGRESS_REGEX.matcher(name);    if (inProgressEditsMatch.matches()) {      try {        long startTxId=Long.parseLong(inProgressEditsMatch.group(1));        ret.add(new EditLogFile(f,startTxId,HdfsServerConstants.INVALID_TXID,true));        continue;      } catch (      NumberFormatException nfe) {
      }    }    Matcher inProgressEditsMatch=EDITS_INPROGRESS_REGEX.matcher(name);    if (inProgressEditsMatch.matches()) {      try {        long startTxId=Long.parseLong(inProgressEditsMatch.group(1));        ret.add(new EditLogFile(f,startTxId,HdfsServerConstants.INVALID_TXID,true));        continue;      } catch (      NumberFormatException nfe) {        LOG.error("In-progress edits file " + f + " has improperly "+ "formatted transaction ID");      }    }    if (forPurging) {      Matcher staleInprogressEditsMatch=EDITS_INPROGRESS_STALE_REGEX.matcher(name);      if (staleInprogressEditsMatch.matches()) {        try {          long startTxId=Long.parseLong(staleInprogressEditsMatch.group(1));          ret.add(new EditLogFile(f,startTxId,HdfsServerConstants.INVALID_TXID,true));          continue;
@Override synchronized public void selectInputStreams(Collection<EditLogInputStream> streams,long fromTxId,boolean inProgressOk,boolean onlyDurableTxns) throws IOException {  List<EditLogFile> elfs=matchEditLogs(sd.getCurrentDir());  if (LOG.isDebugEnabled()) {
@Override synchronized public void recoverUnfinalizedSegments() throws IOException {  File currentDir=sd.getCurrentDir();
  List<EditLogFile> allLogFiles=matchEditLogs(currentDir);  for (  EditLogFile elf : allLogFiles) {    if (elf.getFile().equals(currentInProgress)) {      continue;    }    if (elf.isInProgress()) {      if (elf.getFile().length() == 0) {        LOG.info("Deleting zero-length edit log file " + elf);        if (!elf.getFile().delete()) {          throw new IOException("Unable to delete file " + elf.getFile());        }        continue;      }      elf.scanLog(getLastReadableTxId(),true);      if (elf.hasCorruptHeader()) {        elf.moveAsideCorruptFile();        throw new CorruptionException("In-progress edit log file is corrupt: " + elf);      }      if (elf.getLastTxId() == HdfsServerConstants.INVALID_TXID) {
int run(Configuration conf,AtomicInteger errorCount) throws Exception {  final int initCount=errorCount.get();  LOG.info(Util.memoryInfo());  initConf(conf);  final FSNamesystem namesystem=checkINodeReference(conf,errorCount);  INodeMapValidation.run(namesystem.getFSDirectory(),errorCount);
FSNamesystem checkINodeReference(Configuration conf,AtomicInteger errorCount) throws Exception {  INodeReferenceValidation.start();  final FSNamesystem namesystem=loadImage(conf);  LOG.info(Util.memoryInfo());  INodeReferenceValidation.end(errorCount);
    return false;  }  Set<String> validRequestors=new HashSet<String>();  validRequestors.add(SecurityUtil.getServerPrincipal(conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSUtilClient.getNNAddress(conf).getHostName()));  try {    validRequestors.add(SecurityUtil.getServerPrincipal(conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY),SecondaryNameNode.getHttpAddress(conf).getHostName()));  } catch (  Exception e) {    LOG.debug("SecondaryNameNode principal could not be added",e);    String msg=String.format("SecondaryNameNode principal not considered, %s = %s, %s = %s",DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY,conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));    LOG.warn(msg);  }  if (HAUtil.isHAEnabled(conf,DFSUtil.getNamenodeNameServiceId(conf))) {    List<Configuration> otherNnConfs=HAUtil.getConfForOtherNodes(conf);    for (    Configuration otherNnConf : otherNnConfs) {      validRequestors.add(SecurityUtil.getServerPrincipal(otherNnConf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSUtilClient.getNNAddress(otherNnConf).getHostName()));    }  }  for (  String v : validRequestors) {    if (v != null && v.equals(remoteUser)) {
  validRequestors.add(SecurityUtil.getServerPrincipal(conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSUtilClient.getNNAddress(conf).getHostName()));  try {    validRequestors.add(SecurityUtil.getServerPrincipal(conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY),SecondaryNameNode.getHttpAddress(conf).getHostName()));  } catch (  Exception e) {    LOG.debug("SecondaryNameNode principal could not be added",e);    String msg=String.format("SecondaryNameNode principal not considered, %s = %s, %s = %s",DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY,conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));    LOG.warn(msg);  }  if (HAUtil.isHAEnabled(conf,DFSUtil.getNamenodeNameServiceId(conf))) {    List<Configuration> otherNnConfs=HAUtil.getConfForOtherNodes(conf);    for (    Configuration otherNnConf : otherNnConfs) {      validRequestors.add(SecurityUtil.getServerPrincipal(otherNnConf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSUtilClient.getNNAddress(otherNnConf).getHostName()));    }  }  for (  String v : validRequestors) {    if (v != null && v.equals(remoteUser)) {      LOG.info("ImageServlet allowing checkpointer: " + remoteUser);      return true;
  } catch (  Exception e) {    LOG.debug("SecondaryNameNode principal could not be added",e);    String msg=String.format("SecondaryNameNode principal not considered, %s = %s, %s = %s",DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY,conf.get(DFSConfigKeys.DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,conf.getTrimmed(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT));    LOG.warn(msg);  }  if (HAUtil.isHAEnabled(conf,DFSUtil.getNamenodeNameServiceId(conf))) {    List<Configuration> otherNnConfs=HAUtil.getConfForOtherNodes(conf);    for (    Configuration otherNnConf : otherNnConfs) {      validRequestors.add(SecurityUtil.getServerPrincipal(otherNnConf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY),DFSUtilClient.getNNAddress(otherNnConf).getHostName()));    }  }  for (  String v : validRequestors) {    if (v != null && v.equals(remoteUser)) {      LOG.info("ImageServlet allowing checkpointer: " + remoteUser);      return true;    }  }  if (HttpServer2.userHasAdministratorAccess(context,remoteUser)) {    LOG.info("ImageServlet allowing administrator: " + remoteUser);    return true;
          response.sendError(HttpServletResponse.SC_CONFLICT,"Another checkpointer is already in the process of uploading a" + " checkpoint made up to transaction ID " + larger.last());          return null;        }        if (!currentlyDownloadingCheckpoints.add(imageRequest)) {          response.sendError(HttpServletResponse.SC_CONFLICT,"Either current namenode is checkpointing or another" + " checkpointer is already in the process of " + "uploading a checkpoint made at transaction ID "+ txid);          return null;        }        long now=System.currentTimeMillis();        long lastCheckpointTime=nnImage.getStorage().getMostRecentCheckpointTime();        long lastCheckpointTxid=nnImage.getStorage().getMostRecentCheckpointTxId();        long checkpointPeriod=conf.getTimeDuration(DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT,TimeUnit.SECONDS);        checkpointPeriod=Math.round(checkpointPeriod * recentImageCheckTimePrecision);        long checkpointTxnCount=conf.getLong(DFS_NAMENODE_CHECKPOINT_TXNS_KEY,DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT);        long timeDelta=TimeUnit.MILLISECONDS.toSeconds(now - lastCheckpointTime);        if (checkRecentImageEnable && NameNodeFile.IMAGE.equals(parsedParams.getNameNodeFile()) && timeDelta < checkpointPeriod && txid - lastCheckpointTxid < checkpointTxnCount) {          String message="Rejecting a fsimage due to small time delta " + "and txnid delta. Time since previous checkpoint is " + timeDelta + " expecting at least "+ checkpointPeriod+ " txnid delta since previous checkpoint is "+ (txid - lastCheckpointTxid)+ " expecting at least "+ checkpointTxnCount;          LOG.info(message);
@Override public void selectInputStreams(Collection<EditLogInputStream> streams,long fromTxId,boolean inProgressOk,boolean onlyDurableTxns){  final PriorityQueue<EditLogInputStream> allStreams=new PriorityQueue<EditLogInputStream>(64,EDIT_LOG_INPUT_STREAM_COMPARATOR);  for (  JournalAndStream jas : journals) {    if (jas.isDisabled()) {
  for (  JournalAndStream jas : journals) {    try {      closure.apply(jas);    } catch (    Throwable t) {      if (jas.isRequired()) {        final String msg="Error: " + status + " failed for required journal ("+ jas+ ")";        LOG.error(msg,t);        abortAllJournals();        terminate(1,msg);      } else {        LOG.error("Error: " + status + " failed for (journal "+ jas+ ")",t);        badJAS.add(jas);      }    }  }  disableAndReportErrorOnJournals(badJAS);  if (!NameNodeResourcePolicy.areResourcesAvailable(journals,minimumRedundantJournals)) {    String message=status + " failed for too many journals";
      }    }  }  final Map<Long,List<RemoteEditLog>> logsByStartTxId=new HashMap<>();  allLogs.forEach(input -> {    long key=RemoteEditLog.GET_START_TXID.apply(input);    logsByStartTxId.computeIfAbsent(key,k -> new ArrayList<>()).add(input);  });  long curStartTxId=fromTxId;  List<RemoteEditLog> logs=new ArrayList<>();  while (true) {    List<RemoteEditLog> logGroup=logsByStartTxId.getOrDefault(curStartTxId,Collections.emptyList());    if (logGroup.isEmpty()) {      SortedSet<Long> startTxIds=Sets.newTreeSet(logsByStartTxId.keySet());      startTxIds=startTxIds.tailSet(curStartTxId);      if (startTxIds.isEmpty()) {        break;      } else {
  long curStartTxId=fromTxId;  List<RemoteEditLog> logs=new ArrayList<>();  while (true) {    List<RemoteEditLog> logGroup=logsByStartTxId.getOrDefault(curStartTxId,Collections.emptyList());    if (logGroup.isEmpty()) {      SortedSet<Long> startTxIds=Sets.newTreeSet(logsByStartTxId.keySet());      startTxIds=startTxIds.tailSet(curStartTxId);      if (startTxIds.isEmpty()) {        break;      } else {        if (LOG.isDebugEnabled()) {          LOG.debug("Found gap in logs at " + curStartTxId + ": "+ "not returning previous logs in manifest.");        }        logs.clear();        curStartTxId=startTxIds.first();        continue;
private synchronized void removeLease(Lease lease,long inodeId){  leasesById.remove(inodeId);  if (!lease.removeFile(inodeId)) {
private void format(StorageDirectory sd) throws IOException {  sd.clearDirectory();  writeProperties(sd);  writeTransactionIdFile(sd,0);
private void reportErrorsOnDirectory(StorageDirectory sd){
private void reportErrorsOnDirectory(StorageDirectory sd){  LOG.error("Error reported on storage directory {}",sd);  if (LOG.isDebugEnabled()) {    String lsd=listStorageDirectories();
static boolean canRollBack(StorageDirectory sd,StorageInfo storage,StorageInfo prevStorage,int targetLayoutVersion) throws IOException {  File prevDir=sd.getPreviousDir();  if (!prevDir.exists()) {
static void doFinalize(StorageDirectory sd) throws IOException {  File prevDir=sd.getPreviousDir();  if (!prevDir.exists()) {
static void doFinalize(StorageDirectory sd) throws IOException {  File prevDir=sd.getPreviousDir();  if (!prevDir.exists()) {    LOG.info("Directory " + prevDir + " does not exist.");
static void doPreUpgrade(Configuration conf,StorageDirectory sd) throws IOException {
public static void doUpgrade(StorageDirectory sd,Storage storage) throws IOException {
public static void setServiceAddress(Configuration conf,String address){
void setRpcLifelineServerAddress(Configuration conf,InetSocketAddress lifelineRPCAddress){
      httpServer.setAliasMap(levelDBAliasMapServer.getAliasMap());    }  }  rpcServer.start();  try {    plugins=conf.getInstances(DFS_NAMENODE_PLUGINS_KEY,ServicePlugin.class);  } catch (  RuntimeException e) {    String pluginsValue=conf.get(DFS_NAMENODE_PLUGINS_KEY);    LOG.error("Unable to load NameNode plugins. Specified list of plugins: " + pluginsValue,e);    throw e;  }  for (  ServicePlugin p : plugins) {    try {      p.start(this);    } catch (    Throwable t) {      LOG.warn("ServicePlugin " + p + " could not be started",t);    }  }  LOG.info(getRole() + " RPC up at: " + getNameNodeAddress());  if (rpcServer.getServiceRpcAddress() != null) {
private static boolean initializeSharedEdits(Configuration conf,boolean force,boolean interactive) throws IOException {  String nsId=DFSUtil.getNamenodeNameServiceId(conf);  String namenodeId=HAUtil.getNameNodeId(conf,nsId);  initializeGenericKeys(conf,nsId,namenodeId);  if (conf.get(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY) == null) {
private static void copyEditLogSegmentsToSharedDir(FSNamesystem fsns,Collection<URI> sharedEditsDirs,NNStorage newSharedStorage,Configuration conf) throws IOException {  Preconditions.checkArgument(!sharedEditsDirs.isEmpty(),"No shared edits specified");  List<URI> sharedEditsUris=new ArrayList<URI>(sharedEditsDirs);  FSEditLog newSharedEditLog=new FSEditLog(conf,newSharedStorage,sharedEditsUris);  newSharedEditLog.initJournalsForWrite();  newSharedEditLog.recoverUnclosedStreams();  FSEditLog sourceEditLog=fsns.getFSImage().editLog;  long fromTxId=fsns.getFSImage().getMostRecentCheckpointTxId();  Collection<EditLogInputStream> streams=null;  try {    streams=sourceEditLog.selectInputStreams(fromTxId + 1,0);    newSharedEditLog.setNextTxId(fromTxId + 1);    for (    EditLogInputStream stream : streams) {
  try {    streams=sourceEditLog.selectInputStreams(fromTxId + 1,0);    newSharedEditLog.setNextTxId(fromTxId + 1);    for (    EditLogInputStream stream : streams) {      LOG.debug("Beginning to copy stream {} to shared edits",stream);      FSEditLogOp op;      boolean segmentOpen=false;      while ((op=stream.readOp()) != null) {        LOG.trace("copying op: {}",op);        if (!segmentOpen) {          newSharedEditLog.startLogSegment(op.txid,false,fsns.getEffectiveLayoutVersion());          segmentOpen=true;        }        newSharedEditLog.logEdit(op);        if (op.opCode == FSEditLogOpCodes.OP_END_LOG_SEGMENT) {          newSharedEditLog.endCurrentLogSegment(false);
    for (    EditLogInputStream stream : streams) {      LOG.debug("Beginning to copy stream {} to shared edits",stream);      FSEditLogOp op;      boolean segmentOpen=false;      while ((op=stream.readOp()) != null) {        LOG.trace("copying op: {}",op);        if (!segmentOpen) {          newSharedEditLog.startLogSegment(op.txid,false,fsns.getEffectiveLayoutVersion());          segmentOpen=true;        }        newSharedEditLog.logEdit(op);        if (op.opCode == FSEditLogOpCodes.OP_END_LOG_SEGMENT) {          newSharedEditLog.endCurrentLogSegment(false);          LOG.debug("ending log segment because of END_LOG_SEGMENT op in {}",stream);          segmentOpen=false;        }      }      if (segmentOpen) {
      startOpt=StartupOption.BACKUP;    } else     if (StartupOption.CHECKPOINT.getName().equalsIgnoreCase(cmd)) {      startOpt=StartupOption.CHECKPOINT;    } else     if (StartupOption.OBSERVER.getName().equalsIgnoreCase(cmd)) {      startOpt=StartupOption.OBSERVER;    } else     if (StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd) || StartupOption.UPGRADEONLY.getName().equalsIgnoreCase(cmd)) {      startOpt=StartupOption.UPGRADE.getName().equalsIgnoreCase(cmd) ? StartupOption.UPGRADE : StartupOption.UPGRADEONLY;      while (i + 1 < argsLen) {        String flag=args[i + 1];        if (flag.equalsIgnoreCase(StartupOption.CLUSTERID.getName())) {          if (i + 2 < argsLen) {            i+=2;            startOpt.setClusterId(args[i]);          } else {            LOG.error("Must specify a valid cluster ID after the " + StartupOption.CLUSTERID.getName() + " flag");
public static NameNode createNameNode(String argv[],Configuration conf) throws IOException {
  LOG.info("createNameNode " + Arrays.asList(argv));  if (conf == null)   conf=new HdfsConfiguration();  GenericOptionsParser hParser=new GenericOptionsParser(conf,argv);  argv=hParser.getRemainingArgs();  StartupOption startOpt=parseArguments(argv);  if (startOpt == null) {    printUsage(System.err);    return null;  }  setStartupOption(conf,startOpt);  boolean aborted=false;switch (startOpt) {case FORMAT:    aborted=format(conf,startOpt.getForceFormat(),startOpt.getInteractiveFormat());  terminate(aborted ? 1 : 0);return null;case GENCLUSTERID:String clusterID=NNStorage.newClusterID();
protected synchronized void doImmediateShutdown(Throwable t) throws ExitException {  try {
@Override public void errorReport(NamenodeRegistration registration,int errorCode,String msg) throws IOException {  checkNNStartup();  namesystem.checkOperation(OperationCategory.UNCHECKED);  namesystem.checkSuperuserPrivilege();  verifyRequest(registration);
@Override public HdfsFileStatus create(String src,FsPermission masked,String clientName,EnumSetWritable<CreateFlag> flag,boolean createParent,short replication,long blockSize,CryptoProtocolVersion[] supportedVersions,String ecPolicyName,String storagePolicy) throws IOException {  checkNNStartup();  String clientMachine=getClientMachine();  if (stateChangeLog.isDebugEnabled()) {
@Override public LastBlockWithStatus append(String src,String clientName,EnumSetWritable<CreateFlag> flag) throws IOException {  checkNNStartup();  String clientMachine=getClientMachine();  if (stateChangeLog.isDebugEnabled()) {
@Override public LocatedBlock getAdditionalDatanode(final String src,final long fileId,final ExtendedBlock blk,final DatanodeInfo[] existings,final String[] existingStorageIDs,final DatanodeInfo[] excludes,final int numAdditionalNodes,final String clientName) throws IOException {  checkNNStartup();  if (LOG.isDebugEnabled()) {
@Deprecated @Override public boolean rename(String src,String dst) throws IOException {  checkNNStartup();  if (stateChangeLog.isDebugEnabled()) {
@Override public void rename2(String src,String dst,Options.Rename... options) throws IOException {  checkNNStartup();  if (stateChangeLog.isDebugEnabled()) {
@Override public boolean truncate(String src,long newLength,String clientName) throws IOException {  checkNNStartup();
@Override public boolean delete(String src,boolean recursive) throws IOException {  checkNNStartup();  if (stateChangeLog.isDebugEnabled()) {
@Override public boolean mkdirs(String src,FsPermission masked,boolean createParent) throws IOException {  checkNNStartup();  if (stateChangeLog.isDebugEnabled()) {
@Override public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException {  checkNNStartup();
@Override public DatanodeCommand blockReport(final DatanodeRegistration nodeReg,String poolId,final StorageBlockReport[] reports,final BlockReportContext context) throws IOException {  checkNNStartup();  verifyRequest(nodeReg);  if (blockStateChangeLog.isDebugEnabled()) {
@Override public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,String poolId,List<Long> blockIds) throws IOException {  checkNNStartup();  verifyRequest(nodeReg);  if (blockStateChangeLog.isDebugEnabled()) {
@Override public void blockReceivedAndDeleted(final DatanodeRegistration nodeReg,String poolId,StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks) throws IOException {  checkNNStartup();  verifyRequest(nodeReg);  metrics.incrBlockReceivedAndDeletedOps();  if (blockStateChangeLog.isDebugEnabled()) {
@Override public void errorReport(DatanodeRegistration nodeReg,int errorCode,String msg) throws IOException {  checkNNStartup();  String dnName=(nodeReg == null) ? "Unknown DataNode" : nodeReg.toString();  if (errorCode == DatanodeProtocol.NOTIFY) {
@Override public String[] getGroupsForUser(String user) throws IOException {  if (LOG.isDebugEnabled()) {
  String dnVersion=dnReg.getSoftwareVersion();  if (VersionUtil.compareVersions(dnVersion,minimumDataNodeVersion) < 0) {    IncorrectVersionException ive=new IncorrectVersionException(minimumDataNodeVersion,dnVersion,"DataNode","NameNode");    LOG.warn(ive.getMessage() + " DN: " + dnReg);    throw ive;  }  String nnVersion=VersionInfo.getVersion();  if (!dnVersion.equals(nnVersion)) {    String messagePrefix="Reported DataNode version '" + dnVersion + "' of DN "+ dnReg+ " does not match NameNode version '"+ nnVersion+ "'";    long nnCTime=nn.getFSImage().getStorage().getCTime();    long dnCTime=dnReg.getStorageInfo().getCTime();    if (nnCTime != dnCTime) {      IncorrectVersionException ive=new IncorrectVersionException(messagePrefix + " and CTime of DN ('" + dnCTime+ "') does not match CTime of NN ('"+ nnCTime+ "')");      LOG.warn(ive.toString(),ive);      throw ive;    } else {
      sb.append("FSCK started by " + UserGroupInformation.getCurrentUser() + " from "+ remoteAddress+ " at "+ new Date());      out.println(sb);      sb.append(" for blockIds: \n");      for (      String blk : blocks) {        if (blk == null || !blk.contains(Block.BLOCK_FILE_PREFIX)) {          out.println("Incorrect blockId format: " + blk);          continue;        }        out.print("\n");        blockIdCK(blk);        sb.append(blk + "\n");      }      LOG.info("{}",sb.toString());      namenode.getNamesystem().logFsckEvent("/",remoteAddress);      out.flush();      return;    }    String msg="FSCK started by " + UserGroupInformation.getCurrentUser() + " from "+ remoteAddress+ " for path "+ path+ " at "+ new Date();
private void deleteCorruptedFile(String path){  try {    namenode.getRpcServer().delete(path,true);
      throw new IOException("failed to create directory " + target);    }    int chain=0;    boolean copyError=false;    for (    LocatedBlock lBlk : blocks.getLocatedBlocks()) {      LocatedBlock lblock=lBlk;      DatanodeInfo[] locs=lblock.getLocations();      if (locs == null || locs.length == 0) {        if (fos != null) {          fos.flush();          fos.close();          fos=null;        }        continue;      }      if (fos == null) {        fos=dfs.create(target + "/" + chain,true);        chain++;
          fos=null;        }        continue;      }      if (fos == null) {        fos=dfs.create(target + "/" + chain,true);        chain++;      }      try {        copyBlock(dfs,lblock,fos);      } catch (      Exception e) {        LOG.error("Fsck: could not copy block " + lblock.getBlock() + " to "+ target,e);        fos.flush();        fos.close();        fos=null;        internalError=true;        copyError=true;      }    }    if (copyError) {
      deadNodes.clear();      failures++;      continue;    }    try {      String file=BlockReaderFactory.getFileName(targetAddr,block.getBlockPoolId(),block.getBlockId());      blockReader=new BlockReaderFactory(dfs.getConf()).setFileName(file).setBlock(block).setBlockToken(lblock.getBlockToken()).setStartOffset(0).setLength(block.getNumBytes()).setVerifyChecksum(true).setClientName("fsck").setDatanodeInfo(chosenNode).setInetSocketAddress(targetAddr).setCachingStrategy(CachingStrategy.newDropBehind()).setClientCacheContext(dfs.getClientContext()).setConfiguration(namenode.getConf()).setRemotePeerFactory(new RemotePeerFactory(){        @Override public Peer newConnectedPeer(        InetSocketAddress addr,        Token<BlockTokenIdentifier> blockToken,        DatanodeID datanodeId) throws IOException {          Peer peer=null;          Socket s=NetUtils.getDefaultSocketFactory(conf).createSocket();          try {            s.connect(addr,HdfsConstants.READ_TIMEOUT);            s.setSoTimeout(HdfsConstants.READ_TIMEOUT);            peer=DFSUtilClient.peerFromSocketAndKey(dfs.getSaslDataTransferClient(),s,NamenodeFsck.this,blockToken,datanodeId,HdfsConstants.READ_TIMEOUT);          }  finally {            if (peer == null) {
  if (op == null) {    state=State.EOF;    if (streams[curIdx].getLastTxId() == prevTxId) {      return null;    } else {      throw new PrematureEOFException("got premature end-of-file " + "at txid " + prevTxId + "; expected file to go up to "+ streams[curIdx].getLastTxId());    }  }  prevTxId=op.getTransactionId();  return op;} catch (IOException e) {  prevException=e;  state=State.STREAM_FAILED;}break;case STREAM_FAILED:if (curIdx + 1 == streams.length) {throw prevException;}long oldLast=streams[curIdx].getLastTxId();
        continue;      }      LOG.info("Executing re-encrypt commands on zone {}. Current zones:{}",zoneId,getReencryptionStatus());      getReencryptionStatus().markZoneStarted(zoneId);      resetSubmissionTracker(zoneId);    }  finally {      dir.getFSNamesystem().readUnlock();    }    try {      reencryptEncryptionZone(zoneId);    } catch (    RetriableException|SafeModeException re) {      LOG.info("Re-encryption caught exception, will retry",re);      getReencryptionStatus().markZoneForRetry(zoneId);    }catch (    IOException ioe) {      LOG.warn("IOException caught when re-encrypting zone {}",zoneId,ioe);    }catch (    InterruptedException ie) {      LOG.info("Re-encrypt handler interrupted. Exiting.");
  final Future<ReencryptionTask> completed=batchService.take();  throttle();  checkPauseForTesting();  if (completed.isCancelled()) {    LOG.debug("Skipped a canceled re-encryption task");    return;  }  final ReencryptionTask task=completed.get();  boolean shouldRetry;  do {    dir.getFSNamesystem().writeLock();    try {      throttleTimerLocked.start();      processTask(task);      shouldRetry=false;    } catch (    RetriableException|SafeModeException re) {
  shouldRun=true;  nameNodeAddr=NameNode.getServiceAddress(conf,true);  this.conf=conf;  this.namenode=NameNodeProxies.createNonHAProxy(conf,nameNodeAddr,NamenodeProtocol.class,UserGroupInformation.getCurrentUser(),true).getProxy();  fsName=getInfoServer();  checkpointDirs=FSImage.getCheckpointDirs(conf,"/tmp/hadoop/dfs/namesecondary");  checkpointEditsDirs=FSImage.getCheckpointEditsDirs(conf,"/tmp/hadoop/dfs/namesecondary");  checkpointImage=new CheckpointStorage(conf,checkpointDirs,checkpointEditsDirs);  checkpointImage.recoverCreate(commandLineOpts.shouldFormat());  checkpointImage.deleteTempEdits();  namesystem=new FSNamesystem(conf,checkpointImage,true);  namesystem.dir.disableQuotaChecks();  checkpointConf=new CheckpointConf(conf);  nameNodeStatusBeanName=MBeans.register("SecondaryNameNode","SecondaryNameNodeInfo",this);  legacyOivImageDir=conf.get(DFSConfigKeys.DFS_NAMENODE_LEGACY_OIV_IMAGE_DIR_KEY);
  while (shouldRun) {    try {      Thread.sleep(1000 * period);    } catch (    InterruptedException ie) {    }    if (!shouldRun) {      break;    }    try {      if (UserGroupInformation.isSecurityEnabled())       UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();      final long monotonicNow=Time.monotonicNow();      final long now=Time.now();      if (shouldCheckpointBasedOnCount() || monotonicNow >= lastCheckpointTime + 1000 * checkpointConf.getPeriod()) {        doCheckpoint();        lastCheckpointTime=monotonicNow;        lastCheckpointWallclockTime=now;      }    } catch (    IOException e) {
    } catch (    InterruptedException ie) {    }    if (!shouldRun) {      break;    }    try {      if (UserGroupInformation.isSecurityEnabled())       UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();      final long monotonicNow=Time.monotonicNow();      final long now=Time.now();      if (shouldCheckpointBasedOnCount() || monotonicNow >= lastCheckpointTime + 1000 * checkpointConf.getPeriod()) {        doCheckpoint();        lastCheckpointTime=monotonicNow;        lastCheckpointWallclockTime=now;      }    } catch (    IOException e) {      LOG.error("Exception in doCheckpoint",e);      e.printStackTrace();      if (checkpointImage.getMergeErrorCount() > maxRetries) {
    if (!shouldRun) {      break;    }    try {      if (UserGroupInformation.isSecurityEnabled())       UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();      final long monotonicNow=Time.monotonicNow();      final long now=Time.now();      if (shouldCheckpointBasedOnCount() || monotonicNow >= lastCheckpointTime + 1000 * checkpointConf.getPeriod()) {        doCheckpoint();        lastCheckpointTime=monotonicNow;        lastCheckpointWallclockTime=now;      }    } catch (    IOException e) {      LOG.error("Exception in doCheckpoint",e);      e.printStackTrace();      if (checkpointImage.getMergeErrorCount() > maxRetries) {        LOG.error("Merging failed " + checkpointImage.getMergeErrorCount() + " times.");
      doCheckpoint();    } else {      System.err.println("EditLog size " + count + " transactions is "+ "smaller than configured checkpoint "+ "interval "+ checkpointConf.getTxnCount()+ " transactions.");      System.err.println("Skipping checkpoint.");    }  break;case GETEDITSIZE:long uncheckpointed=countUncheckpointedTxns();System.out.println("NameNode has " + uncheckpointed + " uncheckpointed transactions");break;default:throw new AssertionError("bad command enum: " + opts.getCommand());}} catch (RemoteException e) {exitCode=1;try {String[] content;content=e.getLocalizedMessage().split("\n");LOG.error(cmd + ": " + content[0]);
      System.err.println("EditLog size " + count + " transactions is "+ "smaller than configured checkpoint "+ "interval "+ checkpointConf.getTxnCount()+ " transactions.");      System.err.println("Skipping checkpoint.");    }  break;case GETEDITSIZE:long uncheckpointed=countUncheckpointedTxns();System.out.println("NameNode has " + uncheckpointed + " uncheckpointed transactions");break;default:throw new AssertionError("bad command enum: " + opts.getCommand());}} catch (RemoteException e) {exitCode=1;try {String[] content;content=e.getLocalizedMessage().split("\n");LOG.error(cmd + ": " + content[0]);} catch (Exception ex) {LOG.error(cmd + ": " + ex.getLocalizedMessage());
public static void downloadAliasMap(URL fsName,File aliasMap,boolean isBootstrapStandby) throws IOException {  String paramString=ImageServlet.getParamStringForAliasMap(isBootstrapStandby);  getFileClient(fsName,paramString,Arrays.asList(aliasMap),null,false);
      }      if (CheckpointFaultInjector.getInstance().shouldCorruptAByte(localfile)) {        LOG.warn("SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!");        buf[0]++;      }      out.write(buf,0,num);      total+=num;      if (throttler != null) {        throttler.throttle(num,canceler);      }    }  } catch (  EofException e) {    reportStr+=" Connection closed by client.";    ioe=e;    out=null;  }catch (  IOException ie) {    ioe=ie;    throw ie;  } finally {
        LOG.warn("SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!");        buf[0]++;      }      out.write(buf,0,num);      total+=num;      if (throttler != null) {        throttler.throttle(num,canceler);      }    }  } catch (  EofException e) {    reportStr+=" Connection closed by client.";    ioe=e;    out=null;  }catch (  IOException ie) {    ioe=ie;    throw ie;  } finally {    reportStr+=" Sent total: " + total + " bytes. Size of last segment intended to send: "+ num+ " bytes.";
static MD5Hash getFileClient(URL infoServer,String queryString,List<File> localPaths,Storage dstStorage,boolean getChecksum) throws IOException {  URL url=new URL(infoServer,ImageServlet.PATH_SPEC + "?" + queryString);
private static void setTimeout(HttpURLConnection connection){  if (timeout <= 0) {    Configuration conf=new HdfsConfiguration();    timeout=conf.getInt(DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_KEY,DFSConfigKeys.DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT);
  NamenodeProtocol proxy=null;  NamespaceInfo nsInfo=null;  boolean isUpgradeFinalized=false;  RemoteNameNodeInfo proxyInfo=null;  for (int i=0; i < remoteNNs.size(); i++) {    proxyInfo=remoteNNs.get(i);    InetSocketAddress otherIpcAddress=proxyInfo.getIpcAddress();    proxy=createNNProtocolProxy(otherIpcAddress);    try {      nsInfo=proxy.versionRequest();      isUpgradeFinalized=proxy.isUpgradeFinalized();      break;    } catch (    IOException ioe) {      LOG.warn("Unable to fetch namespace information from remote NN at " + otherIpcAddress + ": "+ ioe.getMessage());      if (LOG.isDebugEnabled()) {
  boolean isUpgradeFinalized=false;  RemoteNameNodeInfo proxyInfo=null;  for (int i=0; i < remoteNNs.size(); i++) {    proxyInfo=remoteNNs.get(i);    InetSocketAddress otherIpcAddress=proxyInfo.getIpcAddress();    proxy=createNNProtocolProxy(otherIpcAddress);    try {      nsInfo=proxy.versionRequest();      isUpgradeFinalized=proxy.isUpgradeFinalized();      break;    } catch (    IOException ioe) {      LOG.warn("Unable to fetch namespace information from remote NN at " + otherIpcAddress + ": "+ ioe.getMessage());      if (LOG.isDebugEnabled()) {        LOG.debug("Full exception trace",ioe);      }    }  }  if (nsInfo == null) {
    proxyInfo=remoteNNs.get(i);    InetSocketAddress otherIpcAddress=proxyInfo.getIpcAddress();    proxy=createNNProtocolProxy(otherIpcAddress);    try {      nsInfo=proxy.versionRequest();      isUpgradeFinalized=proxy.isUpgradeFinalized();      break;    } catch (    IOException ioe) {      LOG.warn("Unable to fetch namespace information from remote NN at " + otherIpcAddress + ": "+ ioe.getMessage());      if (LOG.isDebugEnabled()) {        LOG.debug("Full exception trace",ioe);      }    }  }  if (nsInfo == null) {    LOG.error("Unable to fetch namespace information from any remote NN. Possible NameNodes: " + remoteNNs);    return ERR_CODE_FAILED_CONNECT;  }  if (!checkLayoutVersion(nsInfo)) {
    if (dataDirStates.values().contains(StorageState.NOT_FORMATTED)) {      isFormatted=false;      System.err.println("The original storage directory is not formatted.");    }  } catch (  InconsistentFSStateException e) {    LOG.warn("The storage directory is in an inconsistent state",e);  } finally {    storage.unlockAll();  }  if (!isFormatted && !format(storage,nsInfo)) {    return false;  }  FSImage.checkUpgrade(storage);  for (Iterator<StorageDirectory> it=storage.dirIterator(false); it.hasNext(); ) {    StorageDirectory sd=it.next();    try {      NNUpgradeUtil.renameCurToTmp(sd);    } catch (    IOException e) {
private void parseConfAndFindOtherNN() throws IOException {  Configuration conf=getConf();  nsId=DFSUtil.getNamenodeNameServiceId(conf);  if (!HAUtil.isHAEnabled(conf,nsId)) {    throw new HadoopIllegalArgumentException("HA is not enabled for this namenode.");  }  nnId=HAUtil.getNameNodeId(conf,nsId);  NameNode.initializeGenericKeys(conf,nsId,nnId);  if (!HAUtil.usesSharedEditsDir(conf)) {    throw new HadoopIllegalArgumentException("Shared edits storage is not enabled for this namenode.");  }  remoteNNs=RemoteNameNodeInfo.getRemoteNameNodes(conf,nsId);  List<RemoteNameNodeInfo> remove=new ArrayList<RemoteNameNodeInfo>(remoteNNs.size());  for (  RemoteNameNodeInfo info : remoteNNs) {    InetSocketAddress address=info.getIpcAddress();    LOG.info("Found nn: " + info.getNameNodeID() + ", ipc: "+ info.getIpcAddress());    if (address.getPort() == 0 || address.getAddress().isAnyLocalAddress()) {
private int formatAndDownloadAliasMap(String pathAliasMap,RemoteNameNodeInfo proxyInfo) throws IOException {
@VisibleForTesting public long doTailEdits() throws IOException, InterruptedException {  namesystem.writeLockInterruptibly();  try {    FSImage image=namesystem.getFSImage();    long lastTxnId=image.getLastAppliedTxId();    if (LOG.isDebugEnabled()) {
  try {    FSImage image=namesystem.getFSImage();    long lastTxnId=image.getLastAppliedTxId();    if (LOG.isDebugEnabled()) {      LOG.debug("lastTxnId: " + lastTxnId);    }    Collection<EditLogInputStream> streams;    long startTime=Time.monotonicNow();    try {      streams=editLog.selectInputStreams(lastTxnId + 1,0,null,inProgressOk,true);    } catch (    IOException ioe) {      LOG.warn("Edits tailer failed to find any streams. Will try again " + "later.",ioe);      return 0;    } finally {      NameNode.getNameNodeMetrics().addEditLogFetchTime(Time.monotonicNow() - startTime);    }    if (LOG.isDebugEnabled()) {
      streams=editLog.selectInputStreams(lastTxnId + 1,0,null,inProgressOk,true);    } catch (    IOException ioe) {      LOG.warn("Edits tailer failed to find any streams. Will try again " + "later.",ioe);      return 0;    } finally {      NameNode.getNameNodeMetrics().addEditLogFetchTime(Time.monotonicNow() - startTime);    }    if (LOG.isDebugEnabled()) {      LOG.debug("edit streams to load from: " + streams.size());    }    long editsLoaded=0;    try {      editsLoaded=image.loadEdits(streams,namesystem,maxTxnsPerLock,null,null);    } catch (    EditLogInputException elie) {      editsLoaded=elie.getNumEditsLoaded();      throw elie;    } finally {
public static void init(int interval,int maxSkipLevels,Logger log){  DirectoryDiffListFactory.skipInterval=interval;  DirectoryDiffListFactory.maxLevels=maxSkipLevels;  if (maxLevels > 0) {    constructor=c -> new DiffListBySkipList(c);
private void gcDeletedSnapshot(String name){  final Snapshot.Root deleted;  namesystem.readLock();  try {    deleted=namesystem.getSnapshotManager().chooseDeletedSnapshot();  } catch (  Throwable e) {    LOG.error("Failed to chooseDeletedSnapshot",e);    throw e;  } finally {    namesystem.readUnlock();  }  if (deleted == null) {    LOG.trace("{}: no snapshots are marked as deleted.",name);    return;  }  final String snapshotRoot=deleted.getRootFullPathName();  final String snapshotName=deleted.getLocalName();
    deleted=namesystem.getSnapshotManager().chooseDeletedSnapshot();  } catch (  Throwable e) {    LOG.error("Failed to chooseDeletedSnapshot",e);    throw e;  } finally {    namesystem.readUnlock();  }  if (deleted == null) {    LOG.trace("{}: no snapshots are marked as deleted.",name);    return;  }  final String snapshotRoot=deleted.getRootFullPathName();  final String snapshotName=deleted.getLocalName();  LOG.info("{}: delete snapshot {} from {}",name,snapshotName,snapshotRoot);  try {    namesystem.gcDeletedSnapshot(snapshotRoot,snapshotName);  } catch (  Throwable e) {
  if (blkLocs == null) {    return;  }  for (  StorageTypeNodePair dn : blkLocs) {    boolean foundDn=dn.getDatanodeInfo().compareTo(reportedDn) == 0 ? true : false;    boolean foundType=dn.getStorageType().equals(type);    if (foundDn && foundType) {      blkLocs.remove(dn);      Block[] mFinishedBlocks=new Block[1];      mFinishedBlocks[0]=reportedBlock;      context.notifyMovementTriedBlocks(mFinishedBlocks);      if (blkLocs.size() <= 0) {        movementFinishedBlocks.add(reportedBlock);        scheduledBlkLocs.remove(reportedBlock);      }      return;    }  }  if (LOG.isDebugEnabled()) {
@VisibleForTesting void blocksStorageMovementUnReportedItemsCheck(){synchronized (storageMovementAttemptedItems) {    Iterator<AttemptedItemInfo> iter=storageMovementAttemptedItems.iterator();    long now=monotonicNow();    while (iter.hasNext()) {      AttemptedItemInfo itemInfo=iter.next();      if (now > itemInfo.getLastAttemptedOrReportedTime() + selfRetryTimeout) {        long file=itemInfo.getFile();        ItemInfo candidate=new ItemInfo(itemInfo.getStartPath(),file,itemInfo.getRetryCount() + 1);        blockStorageMovementNeeded.add(candidate);        iter.remove();
public DatanodeMap getLiveDatanodeStorageReport(Context spsContext) throws IOException {  long now=Time.monotonicNow();  long elapsedTimeMs=now - lastAccessedTime;  boolean refreshNeeded=elapsedTimeMs >= refreshIntervalMs;  lastAccessedTime=now;  if (refreshNeeded) {    if (LOG.isDebugEnabled()) {
  if (refreshNeeded) {    if (LOG.isDebugEnabled()) {      LOG.debug("elapsedTimeMs > refreshIntervalMs : {} > {}," + " so refreshing cache",elapsedTimeMs,refreshIntervalMs);    }    datanodeMap.reset();    DatanodeStorageReport[] liveDns=spsContext.getLiveDatanodeStorageReport();    for (    DatanodeStorageReport storage : liveDns) {      StorageReport[] storageReports=storage.getStorageReports();      List<StorageType> storageTypes=new ArrayList<>();      List<Long> remainingSizeList=new ArrayList<>();      for (      StorageReport t : storageReports) {        if (t.getRemaining() > 0) {          storageTypes.add(t.getStorage().getStorageType());          remainingSizeList.add(t.getRemaining());        }      }      datanodeMap.addTarget(storage.getDatanodeInfo(),storageTypes,remainingSizeList);    }    if (LOG.isDebugEnabled()) {
@Override public synchronized void start(StoragePolicySatisfierMode serviceMode){  if (serviceMode == StoragePolicySatisfierMode.NONE) {
          if (itemInfo.getRetryCount() >= blockMovementMaxRetry) {            LOG.info("Failed to satisfy the policy after " + blockMovementMaxRetry + " retries. Removing inode "+ itemInfo.getFile()+ " from the queue");            storageMovementNeeded.removeItemTrackInfo(itemInfo,false);            continue;          }          long trackId=itemInfo.getFile();          BlocksMovingAnalysis status=null;          BlockStoragePolicy existingStoragePolicy;          HdfsFileStatus fileStatus=ctxt.getFileInfo(trackId);          if (fileStatus == null || fileStatus.isDir()) {            storageMovementNeeded.removeItemTrackInfo(itemInfo,true);          } else {            byte existingStoragePolicyID=fileStatus.getStoragePolicy();            existingStoragePolicy=ctxt.getStoragePolicy(existingStoragePolicyID);            HdfsLocatedFileStatus file=(HdfsLocatedFileStatus)fileStatus;            status=analyseBlocksStorageMovementsAndAssignToDN(file,existingStoragePolicy);
          }          long trackId=itemInfo.getFile();          BlocksMovingAnalysis status=null;          BlockStoragePolicy existingStoragePolicy;          HdfsFileStatus fileStatus=ctxt.getFileInfo(trackId);          if (fileStatus == null || fileStatus.isDir()) {            storageMovementNeeded.removeItemTrackInfo(itemInfo,true);          } else {            byte existingStoragePolicyID=fileStatus.getStoragePolicy();            existingStoragePolicy=ctxt.getStoragePolicy(existingStoragePolicyID);            HdfsLocatedFileStatus file=(HdfsLocatedFileStatus)fileStatus;            status=analyseBlocksStorageMovementsAndAssignToDN(file,existingStoragePolicy);switch (status.status) {case ANALYSIS_SKIPPED_FOR_RETRY:case BLOCKS_TARGETS_PAIRED:              if (LOG.isDebugEnabled()) {                LOG.debug("Block analysis status:{} for the file id:{}." + " Adding to attempt monitor queue for the storage " + "movement attempt finished report",status.status,fileStatus.getFileId());              }            this.storageMovementsMonitor.add(itemInfo.getStartPath(),itemInfo.getFile(),monotonicNow(),status.assignedBlocks,itemInfo.getRetryCount());
          HdfsFileStatus fileStatus=ctxt.getFileInfo(trackId);          if (fileStatus == null || fileStatus.isDir()) {            storageMovementNeeded.removeItemTrackInfo(itemInfo,true);          } else {            byte existingStoragePolicyID=fileStatus.getStoragePolicy();            existingStoragePolicy=ctxt.getStoragePolicy(existingStoragePolicyID);            HdfsLocatedFileStatus file=(HdfsLocatedFileStatus)fileStatus;            status=analyseBlocksStorageMovementsAndAssignToDN(file,existingStoragePolicy);switch (status.status) {case ANALYSIS_SKIPPED_FOR_RETRY:case BLOCKS_TARGETS_PAIRED:              if (LOG.isDebugEnabled()) {                LOG.debug("Block analysis status:{} for the file id:{}." + " Adding to attempt monitor queue for the storage " + "movement attempt finished report",status.status,fileStatus.getFileId());              }            this.storageMovementsMonitor.add(itemInfo.getStartPath(),itemInfo.getFile(),monotonicNow(),status.assignedBlocks,itemInfo.getRetryCount());          break;case NO_BLOCKS_TARGETS_PAIRED:        if (LOG.isDebugEnabled()) {          LOG.debug("Adding trackID:{} for the file id:{} back to" + " retry queue as none of the blocks found its eligible" + " targets.",trackId,fileStatus.getFileId());
 else {            byte existingStoragePolicyID=fileStatus.getStoragePolicy();            existingStoragePolicy=ctxt.getStoragePolicy(existingStoragePolicyID);            HdfsLocatedFileStatus file=(HdfsLocatedFileStatus)fileStatus;            status=analyseBlocksStorageMovementsAndAssignToDN(file,existingStoragePolicy);switch (status.status) {case ANALYSIS_SKIPPED_FOR_RETRY:case BLOCKS_TARGETS_PAIRED:              if (LOG.isDebugEnabled()) {                LOG.debug("Block analysis status:{} for the file id:{}." + " Adding to attempt monitor queue for the storage " + "movement attempt finished report",status.status,fileStatus.getFileId());              }            this.storageMovementsMonitor.add(itemInfo.getStartPath(),itemInfo.getFile(),monotonicNow(),status.assignedBlocks,itemInfo.getRetryCount());          break;case NO_BLOCKS_TARGETS_PAIRED:        if (LOG.isDebugEnabled()) {          LOG.debug("Adding trackID:{} for the file id:{} back to" + " retry queue as none of the blocks found its eligible" + " targets.",trackId,fileStatus.getFileId());        }      retryItem=true;    break;case FEW_LOW_REDUNDANCY_BLOCKS:  if (LOG.isDebugEnabled()) {
private BlocksMovingAnalysis analyseBlocksStorageMovementsAndAssignToDN(HdfsLocatedFileStatus fileInfo,BlockStoragePolicy existingStoragePolicy) throws IOException {  BlocksMovingAnalysis.Status status=BlocksMovingAnalysis.Status.BLOCKS_ALREADY_SATISFIED;  final ErasureCodingPolicy ecPolicy=fileInfo.getErasureCodingPolicy();  final LocatedBlocks locatedBlocks=fileInfo.getLocatedBlocks();  final boolean lastBlkComplete=locatedBlocks.isLastBlockComplete();  if (!lastBlkComplete) {
@Override public void addFileToProcess(ItemInfo trackInfo,boolean scanCompleted){  storageMovementNeeded.add(trackInfo,scanCompleted);  if (LOG.isDebugEnabled()) {
public void start(){  if (!storagePolicyEnabled) {
public void changeModeEvent(StoragePolicySatisfierMode newMode){  if (!storagePolicyEnabled) {
  if (LOG.isDebugEnabled()) {    LOG.debug("Updating SPS service status, current mode:{}, new mode:{}",mode,newMode);  }switch (newMode) {case EXTERNAL:    if (mode == newMode) {      LOG.info("Storage policy satisfier is already in mode:{}," + " so ignoring change mode event.",newMode);      return;    }  spsService.stopGracefully();break;case NONE:if (mode == newMode) {LOG.info("Storage policy satisfier is already disabled, mode:{}" + " so ignoring change mode event.",newMode);return;}LOG.info("Disabling StoragePolicySatisfier, mode:{}",newMode);spsService.stop(true);clearPathIds();break;
public void verifyOutstandingPathQLimit() throws IOException {  long size=pathsToBeTraveresed.size();  if (outstandingPathsLimit - size <= 0) {
private static void logConf(Configuration conf){  LOG.info("NNTop conf: " + DFSConfigKeys.NNTOP_BUCKETS_PER_WINDOW_KEY + " = "+ conf.get(DFSConfigKeys.NNTOP_BUCKETS_PER_WINDOW_KEY));
private static void logConf(Configuration conf){  LOG.info("NNTop conf: " + DFSConfigKeys.NNTOP_BUCKETS_PER_WINDOW_KEY + " = "+ conf.get(DFSConfigKeys.NNTOP_BUCKETS_PER_WINDOW_KEY));  LOG.info("NNTop conf: " + DFSConfigKeys.NNTOP_NUM_USERS_KEY + " = "+ conf.get(DFSConfigKeys.NNTOP_NUM_USERS_KEY));
public void report(long currTime,String userName,String cmd){
public TopWindow snapshot(long time){  TopWindow window=new TopWindow(windowLenMs);  Set<String> metricNames=metricMap.keySet();
private TopN getTopUsersForMetric(long time,String metricName,RollingWindowMap rollingWindows){  TopN topN=new TopN(topUsersCnt);  Iterator<Map.Entry<String,RollingWindow>> iterator=rollingWindows.entrySet().iterator();  while (iterator.hasNext()) {    Map.Entry<String,RollingWindow> entry=iterator.next();    String userName=entry.getKey();    RollingWindow aWindow=entry.getValue();    long windowSum=aWindow.getSum(time);    if (windowSum == 0) {
  if (fsn == null) {    throw new IOException("Namesystem has not been initialized yet.");  }  final BlockManager bm=fsn.getBlockManager();  HashSet<Node> excludes=new HashSet<Node>();  if (excludeDatanodes != null) {    for (    String host : StringUtils.getTrimmedStringCollection(excludeDatanodes)) {      int idx=host.indexOf(":");      Node excludeNode=null;      if (idx != -1) {        excludeNode=bm.getDatanodeManager().getDatanodeByXferAddr(host.substring(0,idx),Integer.parseInt(host.substring(idx + 1)));      } else {        excludeNode=bm.getDatanodeManager().getDatanodeByHost(host);      }      if (excludeNode != null) {        excludes.add(excludeNode);      } else {
private ThreadPoolExecutor initializeBlockMoverThreadPool(int num){
@Override public void submitMoveTask(BlockMovingInfo blkMovingInfo) throws IOException {
public static Configuration addSecurityConfiguration(Configuration conf){  conf=new HdfsConfiguration(conf);  String nameNodePrincipal=conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY,"");  if (LOG.isDebugEnabled()) {
@Override protected void checkRpcAdminAccess() throws IOException, AccessControlException {  UserGroupInformation ugi=UserGroupInformation.getCurrentUser();  UserGroupInformation zkfcUgi=UserGroupInformation.getLoginUser();  if (adminAcl.isUserAllowed(ugi) || ugi.getShortUserName().equals(zkfcUgi.getShortUserName())) {
  isThreadDumpCaptured=false;  int httpTimeOut=conf.getInt(DFSConfigKeys.DFS_HA_ZKFC_NN_HTTP_TIMEOUT_KEY,DFSConfigKeys.DFS_HA_ZKFC_NN_HTTP_TIMEOUT_KEY_DEFAULT);  if (httpTimeOut == 0) {    return;  }  try {    String stacksUrl=DFSUtil.getInfoServer(localNNTarget.getAddress(),conf,DFSUtil.getHttpClientScheme(conf)) + "/stacks";    URL url=new URL(stacksUrl);    HttpURLConnection conn=(HttpURLConnection)url.openConnection();    conn.setReadTimeout(httpTimeOut);    conn.setConnectTimeout(httpTimeOut);    conn.connect();    ByteArrayOutputStream out=new ByteArrayOutputStream();    IOUtils.copyBytes(conn.getInputStream(),out,4096,true);    StringBuilder localNNThreadDumpContent=new StringBuilder("-- Local NN thread dump -- \n");    localNNThreadDumpContent.append(out).append("\n -- Local NN thread dump -- ");
@VisibleForTesting static void cancelTokens(final Configuration conf,final Path tokenFile) throws IOException, InterruptedException {  for (  Token<?> token : readTokens(tokenFile,conf)) {    if (token.isManaged()) {      token.cancel(conf);      if (LOG.isDebugEnabled()) {
@VisibleForTesting static void renewTokens(final Configuration conf,final Path tokenFile) throws IOException, InterruptedException {  for (  Token<?> token : readTokens(tokenFile,conf)) {    if (token.isManaged()) {      long result=token.renew(conf);      if (LOG.isDebugEnabled()) {
@VisibleForTesting static void saveDelegationToken(Configuration conf,FileSystem fs,final String renewer,final Path tokenFile) throws IOException {  Token<?> token=fs.getDelegationToken(renewer);  if (null != token) {    Credentials cred=new Credentials();    cred.addToken(token.getService(),token);    cred.writeTokenStorageFile(tokenFile,conf,Credentials.SerializedFormat.WRITABLE);    if (LOG.isDebugEnabled()) {
@Override public void setConf(Configuration conf){  conf=new HdfsConfiguration(conf);  String nameNodePrincipal=conf.get(DFSConfigKeys.DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY,"");  if (LOG.isDebugEnabled()) {
    visitor.start(inputStream.getVersion(true));    while (true) {      try {        FSEditLogOp op=inputStream.readOp();        if (op == null)         break;        if (fixTxIds) {          if (nextTxId <= 0) {            nextTxId=op.getTransactionId();            if (nextTxId <= 0) {              nextTxId=1;            }          }          op.setTransactionId(nextTxId);          nextTxId++;        }        visitor.visitOp(op);      } catch (      IOException e) {        if (!recoveryMode) {
            nextTxId=op.getTransactionId();            if (nextTxId <= 0) {              nextTxId=1;            }          }          op.setTransactionId(nextTxId);          nextTxId++;        }        visitor.visitOp(op);      } catch (      IOException e) {        if (!recoveryMode) {          LOG.error("Got IOException at position " + inputStream.getPosition());          visitor.close(e);          throw e;        }        LOG.error("Got IOException while reading stream!  Resyncing.",e);        inputStream.resync();      }catch (      RuntimeException e) {        if (!recoveryMode) {
          }          op.setTransactionId(nextTxId);          nextTxId++;        }        visitor.visitOp(op);      } catch (      IOException e) {        if (!recoveryMode) {          LOG.error("Got IOException at position " + inputStream.getPosition());          visitor.close(e);          throw e;        }        LOG.error("Got IOException while reading stream!  Resyncing.",e);        inputStream.resync();      }catch (      RuntimeException e) {        if (!recoveryMode) {          LOG.error("Got RuntimeException at position " + inputStream.getPosition());          visitor.close(e);          throw e;
switch (op) {case "GETFILESTATUS":    content=image.getFileStatus(path);  break;case "LISTSTATUS":content=image.listStatus(path);break;case "GETACLSTATUS":content=image.getAclStatus(path);break;case "GETXATTRS":List<String> names=getXattrNames(decoder);String encoder=getEncoder(decoder);content=image.getXAttrs(path,names,encoder);break;case "LISTXATTRS":content=image.listXAttrs(path);break;case "GETCONTENTSUMMARY":content=image.getContentSummary(path);break;
    ArrayList<FsImageProto.FileSummary.Section> sections=Lists.newArrayList(summary.getSectionsList());    Collections.sort(sections,new Comparator<FsImageProto.FileSummary.Section>(){      @Override public int compare(      FsImageProto.FileSummary.Section s1,      FsImageProto.FileSummary.Section s2){        FSImageFormatProtobuf.SectionName n1=FSImageFormatProtobuf.SectionName.fromString(s1.getName());        FSImageFormatProtobuf.SectionName n2=FSImageFormatProtobuf.SectionName.fromString(s2.getName());        if (n1 == null) {          return n2 == null ? 0 : -1;        } else         if (n2 == null) {          return -1;        } else {          return n1.ordinal() - n2.ordinal();        }      }    });    for (    FsImageProto.FileSummary.Section s : sections) {      fin.getChannel().position(s.getOffset());      InputStream is=FSImageUtil.wrapInputStreamForCompression(conf,summary.getCodec(),new BufferedInputStream(new LimitInputStream(fin,s.getLength())));
private static byte[][] loadINodeSection(InputStream in) throws IOException {  FsImageProto.INodeSection s=FsImageProto.INodeSection.parseDelimitedFrom(in);
static SerialNumberManager.StringTable loadStringTable(InputStream in) throws IOException {  FsImageProto.StringTableSection s=FsImageProto.StringTableSection.parseDelimitedFrom(in);
  } catch (  IOException e) {    throw new IOException("No <version> section found at the top of " + "the fsimage XML.  This XML file is too old to be processed " + "by oiv.",e);  }  Node version=new Node();  loadNodeChildren(version,"version fields");  Integer onDiskVersion=version.removeChildInt("onDiskVersion");  if (onDiskVersion == null) {    throw new IOException("The <version> section doesn't contain " + "the onDiskVersion.");  }  Integer layoutVersion=version.removeChildInt("layoutVersion");  if (layoutVersion == null) {    throw new IOException("The <version> section doesn't contain " + "the layoutVersion.");  }  if (layoutVersion.intValue() != NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION) {    throw new IOException("Layout version mismatch.  This oiv tool " + "handles layout version " + NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION + ", but the "+ "XML file has <layoutVersion> "+ layoutVersion+ ".  Please "+ "either re-generate the XML file with the proper layout version, "+ "or manually edit the XML file to be usable with this version "+ "of the oiv tool.");  }  fileSummaryBld.setOndiskVersion(onDiskVersion);  fileSummaryBld.setLayoutVersion(layoutVersion);  if (LOG.isDebugEnabled()) {
      }      throw new IOException("Got unexpected tag end event for " + ev.asEndElement().getName().getLocalPart() + " while looking "+ "for section header tag.");    } else     if (ev.getEventType() != XMLStreamConstants.START_ELEMENT) {      throw new IOException("Expected section header START_ELEMENT; " + "got event of type " + ev.getEventType());    }    String sectionName=ev.asStartElement().getName().getLocalPart();    if (!unprocessedSections.contains(sectionName)) {      throw new IOException("Unknown or duplicate section found for " + sectionName);    }    SectionProcessor sectionProcessor=sections.get(sectionName);    if (sectionProcessor == null) {      throw new IOException("Unknown FSImage section " + sectionName + ".  Valid section names are ["+ StringUtils.join(", ",sections.keySet())+ "]");    }    unprocessedSections.remove(sectionName);    sectionProcessor.process();  }  writeStringTableSection();  long prevOffset=out.getCount();  FileSummary fileSummary=fileSummaryBld.build();  if (LOG.isDebugEnabled()) {
  DataInputStream in=null;  PositionTrackingInputStream tracker=null;  ImageLoader fsip=null;  boolean done=false;  try {    tracker=new PositionTrackingInputStream(new BufferedInputStream(Files.newInputStream(Paths.get(inputFile))));    in=new DataInputStream(tracker);    int imageVersionFile=findImageVersion(in);    fsip=ImageLoader.LoaderFactory.getLoader(imageVersionFile);    if (fsip == null)     throw new IOException("No image processor to read version " + imageVersionFile + " is available.");    fsip.loadImage(in,processor,skipBlocks);    done=true;  }  finally {    if (!done) {      if (tracker != null) {
@Override protected void buildNamespace(InputStream in,List<Long> refIdList) throws IOException {  corrChecker.saveNodeRefIds(refIdList);
    count++;    if (LOG.isDebugEnabled() && count % 10000 == 0) {      LOG.debug("Scanned {} directories.",count);    }    long parentId=e.getParent();    if (!corrChecker.isNodeIdExist(parentId)) {      LOG.debug("Corruption detected! Parent node is not contained " + "in the list of known ids!");      addCorruptedNode(parentId);    }    int numOfCorruption=0;    for (int i=0; i < e.getChildrenCount(); i++) {      long childId=e.getChildren(i);      putDirChildToMetadataMap(parentId,childId);      if (!corrChecker.isNodeIdExist(childId)) {        addCorruptedNode(childId);        numOfCorruption++;      }    }    if (numOfCorruption > 0) {
@Override public void afterOutput() throws IOException {  if (!corruptionsMap.isEmpty()) {
private void loadDirectoriesInINodeSection(InputStream in) throws IOException {  INodeSection s=INodeSection.parseDelimitedFrom(in);  LOG.info("Loading directories in INode section.");  AtomicInteger numDirs=new AtomicInteger(0);  for (int i=0; i < s.getNumInodes(); ++i) {    INode p=INode.parseDelimitedFrom(in);    if (LOG.isDebugEnabled() && i % 10000 == 0) {
private void outputINodes(InputStream in) throws IOException {  INodeSection s=INodeSection.parseDelimitedFrom(in);
  LOG.info("Found {} INodes in the INode section",s.getNumInodes());  long ignored=0;  long ignoredSnapshots=0;  for (int i=0; i < s.getNumInodes(); ++i) {    INode p=INode.parseDelimitedFrom(in);    try {      String parentPath=metadataMap.getParentPath(p.getId());      printIfNotEmpty(getEntry(parentPath,p));    } catch (    IOException ioe) {      ignored++;      if (!(ioe instanceof IgnoreSnapshotException)) {        LOG.warn("Exception caught, ignoring node:{}",p.getId(),ioe);      } else {        ignoredSnapshots++;        if (LOG.isDebugEnabled()) {
  long ignoredSnapshots=0;  for (int i=0; i < s.getNumInodes(); ++i) {    INode p=INode.parseDelimitedFrom(in);    try {      String parentPath=metadataMap.getParentPath(p.getId());      printIfNotEmpty(getEntry(parentPath,p));    } catch (    IOException ioe) {      ignored++;      if (!(ioe instanceof IgnoreSnapshotException)) {        LOG.warn("Exception caught, ignoring node:{}",p.getId(),ioe);      } else {        ignoredSnapshots++;        if (LOG.isDebugEnabled()) {          LOG.debug("Exception caught, ignoring node:{}.",p.getId(),ioe);        }      }    }    if (LOG.isDebugEnabled() && i % 100000 == 0) {
private static IgnoreSnapshotException createIgnoredSnapshotException(long inode){  if (LOG.isDebugEnabled()) {
private static void saveMD5File(File dataFile,String digestString) throws IOException {  File md5File=getDigestFileForFile(dataFile);  String md5Line=digestString + " *" + dataFile.getName()+ "\n";  AtomicFileOutputStream afos=new AtomicFileOutputStream(md5File);  afos.write(md5Line.getBytes(Charsets.UTF_8));  afos.close();  if (LOG.isDebugEnabled()) {
@BeforeClass public static void init(){  sockDir=new TemporarySocketDirectory();  DomainSocket.disableBindPathValidation();  prevCacheManipulator=NativeIO.POSIX.getCacheManipulator();  NativeIO.POSIX.setCacheManipulator(new CacheManipulator(){    @Override public void mlock(    String identifier,    ByteBuffer mmap,    long length) throws IOException {
private void waitForReplicaAnchorStatus(final ShortCircuitCache cache,final ExtendedBlock block,final boolean expectedIsAnchorable,final boolean expectedIsAnchored,final int expectedOutstandingMmaps) throws Exception {  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      final MutableBoolean result=new MutableBoolean(false);      cache.accept(new CacheVisitor(){        @Override public void visit(        int numOutstandingMmaps,        Map<ExtendedBlockId,ShortCircuitReplica> replicas,        Map<ExtendedBlockId,InvalidToken> failedLoads,        LinkedMap evictable,        LinkedMap evictableMmapped){          Assert.assertEquals(expectedOutstandingMmaps,numOutstandingMmaps);          ShortCircuitReplica replica=replicas.get(ExtendedBlockId.fromExtendedBlock(block));          Assert.assertNotNull(replica);          Slot slot=replica.getSlot();          if ((expectedIsAnchorable != slot.isAnchorable()) || (expectedIsAnchored != slot.isAnchored())) {
@Test public void testOpenManyFilesViaTcp() throws Exception {  final int NUM_OPENS=500;  Configuration conf=new Configuration();  conf.setBoolean(HdfsClientConfigKeys.Read.ShortCircuit.KEY,false);  MiniDFSCluster cluster=null;  FSDataInputStream[] streams=new FSDataInputStream[NUM_OPENS];  try {    cluster=new MiniDFSCluster.Builder(conf).build();    DistributedFileSystem dfs=cluster.getFileSystem();    final Path TEST_PATH=new Path("/testFile");    DFSTestUtil.createFile(dfs,TEST_PATH,131072,(short)1,1);    for (int i=0; i < NUM_OPENS; i++) {      streams[i]=dfs.open(TEST_PATH);
@Test public void testStickyBitReset() throws Exception {  Path sbExplicitTestDir=new Path("/DirToTestExplicitStickyBit");  Path sbOmittedTestDir=new Path("/DirToTestOmittedStickyBit");  hdfs.mkdirs(sbExplicitTestDir);  hdfs.mkdirs(sbOmittedTestDir);  assertTrue(hdfs.exists(sbExplicitTestDir));  assertTrue(hdfs.exists(sbOmittedTestDir));  hdfs.setPermission(sbExplicitTestDir,new FsPermission((short)01777));
@Test public void testStickyBitReset() throws Exception {  Path sbExplicitTestDir=new Path("/DirToTestExplicitStickyBit");  Path sbOmittedTestDir=new Path("/DirToTestOmittedStickyBit");  hdfs.mkdirs(sbExplicitTestDir);  hdfs.mkdirs(sbOmittedTestDir);  assertTrue(hdfs.exists(sbExplicitTestDir));  assertTrue(hdfs.exists(sbOmittedTestDir));  hdfs.setPermission(sbExplicitTestDir,new FsPermission((short)01777));  LOG.info("Dir: {}, permission: {}",sbExplicitTestDir.getName(),hdfs.getFileStatus(sbExplicitTestDir).getPermission());  assertTrue(hdfs.getFileStatus(sbExplicitTestDir).getPermission().getStickyBit());  hdfs.setPermission(sbOmittedTestDir,new FsPermission((short)0777));
@Test public void testStickyBitReset() throws Exception {  Path sbExplicitTestDir=new Path("/DirToTestExplicitStickyBit");  Path sbOmittedTestDir=new Path("/DirToTestOmittedStickyBit");  hdfs.mkdirs(sbExplicitTestDir);  hdfs.mkdirs(sbOmittedTestDir);  assertTrue(hdfs.exists(sbExplicitTestDir));  assertTrue(hdfs.exists(sbOmittedTestDir));  hdfs.setPermission(sbExplicitTestDir,new FsPermission((short)01777));  LOG.info("Dir: {}, permission: {}",sbExplicitTestDir.getName(),hdfs.getFileStatus(sbExplicitTestDir).getPermission());  assertTrue(hdfs.getFileStatus(sbExplicitTestDir).getPermission().getStickyBit());  hdfs.setPermission(sbOmittedTestDir,new FsPermission((short)0777));  LOG.info("Dir: {}, permission: {}",sbOmittedTestDir.getName(),hdfs.getFileStatus(sbOmittedTestDir).getPermission());  assertFalse(hdfs.getFileStatus(sbOmittedTestDir).getPermission().getStickyBit());  hdfs.setPermission(sbExplicitTestDir,new FsPermission((short)00777));
  hdfs.mkdirs(sbExplicitTestDir);  hdfs.mkdirs(sbOmittedTestDir);  assertTrue(hdfs.exists(sbExplicitTestDir));  assertTrue(hdfs.exists(sbOmittedTestDir));  hdfs.setPermission(sbExplicitTestDir,new FsPermission((short)01777));  LOG.info("Dir: {}, permission: {}",sbExplicitTestDir.getName(),hdfs.getFileStatus(sbExplicitTestDir).getPermission());  assertTrue(hdfs.getFileStatus(sbExplicitTestDir).getPermission().getStickyBit());  hdfs.setPermission(sbOmittedTestDir,new FsPermission((short)0777));  LOG.info("Dir: {}, permission: {}",sbOmittedTestDir.getName(),hdfs.getFileStatus(sbOmittedTestDir).getPermission());  assertFalse(hdfs.getFileStatus(sbOmittedTestDir).getPermission().getStickyBit());  hdfs.setPermission(sbExplicitTestDir,new FsPermission((short)00777));  LOG.info("Dir: {}, permission: {}",sbExplicitTestDir.getName(),hdfs.getFileStatus(sbExplicitTestDir).getPermission());  assertFalse(hdfs.getFileStatus(sbExplicitTestDir).getPermission().getStickyBit());  hdfs.setPermission(sbOmittedTestDir,new FsPermission((short)01777));  hdfs.setPermission(sbOmittedTestDir,new FsPermission((short)0777));
  fsTarget.createNewFile(testBaseFile);  FSDataOutputStream dataOutputStream=fsTarget.append(testBaseFile);  dataOutputStream.write(1);  dataOutputStream.close();  fsTarget.createNewFile(testLevel2File);  dataOutputStream=fsTarget.append(testLevel2File);  dataOutputStream.write("test link fallback".toString().getBytes());  dataOutputStream.close();  String clusterName="ClusterFallback";  URI viewFsUri=new URI(FsConstants.VIEWFS_SCHEME,clusterName,"/",null,null);  Configuration conf=new Configuration();  ConfigUtil.addLinkFallback(conf,clusterName,fsTarget.getUri());  FileSystem vfs=FileSystem.get(viewFsUri,conf);  assertEquals(ViewFileSystem.class,vfs.getClass());  FileStatus baseFileStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testBaseFile.toUri().toString()));
  dataOutputStream.write(1);  dataOutputStream.close();  fsTarget.createNewFile(testLevel2File);  dataOutputStream=fsTarget.append(testLevel2File);  dataOutputStream.write("test link fallback".toString().getBytes());  dataOutputStream.close();  String clusterName="ClusterFallback";  URI viewFsUri=new URI(FsConstants.VIEWFS_SCHEME,clusterName,"/",null,null);  Configuration conf=new Configuration();  ConfigUtil.addLinkFallback(conf,clusterName,fsTarget.getUri());  FileSystem vfs=FileSystem.get(viewFsUri,conf);  assertEquals(ViewFileSystem.class,vfs.getClass());  FileStatus baseFileStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testBaseFile.toUri().toString()));  LOG.info("BaseFileStat: " + baseFileStat);  FileStatus baseFileRelStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testBaseFileRelative.toUri().toString()));
  dataOutputStream.write("test link fallback".toString().getBytes());  dataOutputStream.close();  String clusterName="ClusterFallback";  URI viewFsUri=new URI(FsConstants.VIEWFS_SCHEME,clusterName,"/",null,null);  Configuration conf=new Configuration();  ConfigUtil.addLinkFallback(conf,clusterName,fsTarget.getUri());  FileSystem vfs=FileSystem.get(viewFsUri,conf);  assertEquals(ViewFileSystem.class,vfs.getClass());  FileStatus baseFileStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testBaseFile.toUri().toString()));  LOG.info("BaseFileStat: " + baseFileStat);  FileStatus baseFileRelStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testBaseFileRelative.toUri().toString()));  LOG.info("BaseFileRelStat: " + baseFileRelStat);  Assert.assertEquals("Unexpected file length for " + testBaseFile,1,baseFileStat.getLen());  Assert.assertEquals("Unexpected file length for " + testBaseFileRelative,baseFileStat.getLen(),baseFileRelStat.getLen());  FileStatus level2FileStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testLevel2File.toUri().toString()));
  FSDataOutputStream dataOutputStream=fsTarget.append(testLevel2File);  dataOutputStream.write("test link fallback".toString().getBytes());  dataOutputStream.close();  String clusterName="ClusterFallback";  URI viewFsUri=new URI(FsConstants.VIEWFS_SCHEME,clusterName,"/",null,null);  Configuration conf=new Configuration();  ConfigUtil.addLink(conf,clusterName,"/internalDir/linkToDir2",new Path(targetTestRoot,"dir2").toUri());  ConfigUtil.addLink(conf,clusterName,"/internalDir/internalDirB/linkToDir3",new Path(targetTestRoot,"dir3").toUri());  ConfigUtil.addLink(conf,clusterName,"/danglingLink",new Path(targetTestRoot,"missingTarget").toUri());  ConfigUtil.addLink(conf,clusterName,"/linkToAFile",new Path(targetTestRoot,"aFile").toUri());  System.out.println("ViewFs link fallback " + fsTarget.getUri());  ConfigUtil.addLinkFallback(conf,clusterName,targetTestRoot.toUri());  FileSystem vfs=FileSystem.get(viewFsUri,conf);  assertEquals(ViewFileSystem.class,vfs.getClass());  FileStatus baseFileStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testBaseFile.toUri().toString()));
  String clusterName="ClusterFallback";  URI viewFsUri=new URI(FsConstants.VIEWFS_SCHEME,clusterName,"/",null,null);  Configuration conf=new Configuration();  ConfigUtil.addLink(conf,clusterName,"/internalDir/linkToDir2",new Path(targetTestRoot,"dir2").toUri());  ConfigUtil.addLink(conf,clusterName,"/internalDir/internalDirB/linkToDir3",new Path(targetTestRoot,"dir3").toUri());  ConfigUtil.addLink(conf,clusterName,"/danglingLink",new Path(targetTestRoot,"missingTarget").toUri());  ConfigUtil.addLink(conf,clusterName,"/linkToAFile",new Path(targetTestRoot,"aFile").toUri());  System.out.println("ViewFs link fallback " + fsTarget.getUri());  ConfigUtil.addLinkFallback(conf,clusterName,targetTestRoot.toUri());  FileSystem vfs=FileSystem.get(viewFsUri,conf);  assertEquals(ViewFileSystem.class,vfs.getClass());  FileStatus baseFileStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testBaseFile.toUri().toString()));  LOG.info("BaseFileStat: " + baseFileStat);  Assert.assertEquals("Unexpected file length for " + testBaseFile,0,baseFileStat.getLen());  FileStatus level2FileStat=vfs.getFileStatus(new Path(viewFsUri.toString() + testLevel2File.toUri().toString()));
  File infile=new File(TEST_DIR,testFileName);  final byte[] content="HelloWorld".getBytes();  FileOutputStream fos=null;  try {    fos=new FileOutputStream(infile);    fos.write(content);  }  finally {    if (fos != null) {      fos.close();    }  }  assertEquals((long)content.length,infile.length());  Configuration conf=new Configuration();  ConfigUtil.addLinkMergeSlash(conf,clusterName,TEST_DIR.toURI());  FileSystem vfs=FileSystem.get(viewFsUri,conf);  assertEquals(ViewFileSystem.class,vfs.getClass());  FileStatus stat=vfs.getFileStatus(new Path(viewFsUri.toString() + testFileName));
static protected FSDataOutputStream writeFile(FileSystem fileSys,Path name,int repl,int numOfBlocks,boolean completeFile) throws IOException {  FSDataOutputStream stm=fileSys.create(name,true,fileSys.getConf().getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY,4096),(short)repl,blockSize);  byte[] buffer=new byte[blockSize * numOfBlocks];  Random rand=new Random(seed);  rand.nextBytes(buffer);  stm.write(buffer);
protected void putNodeInService(int nnIndex,DatanodeInfo outOfServiceNode) throws IOException {
protected void waitNodeState(List<DatanodeInfo> nodes,AdminStates state){  for (  DatanodeInfo node : nodes) {    boolean done=(state == node.getAdminState());    while (!done) {
public static byte[] randomBytes(long seed,int size){
public static long verifyExpectedCacheUsage(final long expectedCacheUsed,final long expectedBlocks,final FsDatasetSpi<?> fsd) throws Exception {  GenericTestUtils.waitFor(new Supplier<Boolean>(){    private int tries=0;    @Override public Boolean get(){      long curCacheUsed=fsd.getCacheUsed();      long curBlocks=fsd.getNumBlocksCached();      if ((curCacheUsed != expectedCacheUsed) || (curBlocks != expectedBlocks)) {        if (tries++ > 10) {
public static boolean verifyFileReplicasOnStorageType(FileSystem fs,DFSClient client,Path path,StorageType storageType) throws IOException {  if (!fs.exists(path)) {
public static HashSet<Path> closeOpenFiles(HashMap<Path,FSDataOutputStream> openFilesMap,int numFilesToClose) throws IOException {  HashSet<Path> closedFiles=new HashSet<>();  for (Iterator<Entry<Path,FSDataOutputStream>> it=openFilesMap.entrySet().iterator(); it.hasNext(); ) {    Entry<Path,FSDataOutputStream> entry=it.next();
public static void verifySnapshotDiffReport(DistributedFileSystem fs,Path dir,String from,String to,DiffReportEntry... entries) throws IOException {  SnapshotDiffReport report=fs.getSnapshotDiffReport(dir,from,to);  SnapshotDiffReport inverseReport=fs.getSnapshotDiffReport(dir,to,from);
public static void verifySnapshotDiffReport(DistributedFileSystem fs,Path dir,String from,String to,DiffReportEntry... entries) throws IOException {  SnapshotDiffReport report=fs.getSnapshotDiffReport(dir,from,to);  SnapshotDiffReport inverseReport=fs.getSnapshotDiffReport(dir,to,from);  LOG.info(report.toString());
@Test public void testAppend() throws IOException {  final int maxOldFileLen=2 * BLOCK_SIZE + 1;  final int maxFlushedBytes=BLOCK_SIZE;  byte[] contents=AppendTestUtil.initBuffer(maxOldFileLen + 2 * maxFlushedBytes);  for (int oldFileLen=0; oldFileLen <= maxOldFileLen; oldFileLen++) {    for (int flushedBytes1=0; flushedBytes1 <= maxFlushedBytes; flushedBytes1++) {      for (int flushedBytes2=0; flushedBytes2 <= maxFlushedBytes; flushedBytes2++) {        final int fileLen=oldFileLen + flushedBytes1 + flushedBytes2;        final Path p=new Path("foo" + oldFileLen + "_"+ flushedBytes1+ "_"+ flushedBytes2);
public void shutdown(boolean deleteDfsDir,boolean closeFileSystem){  LOG.info("Shutting down the Mini HDFS Cluster");  if (checkExitOnShutdown) {    if (ExitUtil.terminateCalled()) {
public void shutdownDataNode(int dnIndex){
public synchronized boolean restartDataNodes(boolean keepPort) throws IOException {  for (int i=dataNodes.size() - 1; i >= 0; i--) {    if (!restartDataNode(i,keepPort))     return false;
public void printNNs(){  for (int i=0; i < namenodes.size(); i++) {
public static void verifyRead(DistributedFileSystem dfs,Path testPath,int length,byte[] expected) throws IOException {
public static void verifyRead(DistributedFileSystem dfs,Path testPath,int length,byte[] expected) throws IOException {  LOG.info("verifyRead on path {}",testPath);  byte[] buffer=new byte[length + 100];
public static void verifyRead(DistributedFileSystem dfs,Path testPath,int length,byte[] expected) throws IOException {  LOG.info("verifyRead on path {}",testPath);  byte[] buffer=new byte[length + 100];  LOG.info("verifyRead verifyLength on path {}",testPath);  StripedFileTestUtil.verifyLength(dfs,testPath,length);
public static void verifyRead(DistributedFileSystem dfs,Path testPath,int length,byte[] expected) throws IOException {  LOG.info("verifyRead on path {}",testPath);  byte[] buffer=new byte[length + 100];  LOG.info("verifyRead verifyLength on path {}",testPath);  StripedFileTestUtil.verifyLength(dfs,testPath,length);  LOG.info("verifyRead verifyPread on path {}",testPath);  StripedFileTestUtil.verifyPread(dfs,testPath,length,expected,buffer);
public static void verifyRead(DistributedFileSystem dfs,Path testPath,int length,byte[] expected) throws IOException {  LOG.info("verifyRead on path {}",testPath);  byte[] buffer=new byte[length + 100];  LOG.info("verifyRead verifyLength on path {}",testPath);  StripedFileTestUtil.verifyLength(dfs,testPath,length);  LOG.info("verifyRead verifyPread on path {}",testPath);  StripedFileTestUtil.verifyPread(dfs,testPath,length,expected,buffer);  LOG.info("verifyRead verifyStatefulRead on path {}",testPath);  StripedFileTestUtil.verifyStatefulRead(dfs,testPath,length,expected,buffer);
public static void verifyRead(DistributedFileSystem dfs,Path testPath,int length,byte[] expected) throws IOException {  LOG.info("verifyRead on path {}",testPath);  byte[] buffer=new byte[length + 100];  LOG.info("verifyRead verifyLength on path {}",testPath);  StripedFileTestUtil.verifyLength(dfs,testPath,length);  LOG.info("verifyRead verifyPread on path {}",testPath);  StripedFileTestUtil.verifyPread(dfs,testPath,length,expected,buffer);  LOG.info("verifyRead verifyStatefulRead on path {}",testPath);  StripedFileTestUtil.verifyStatefulRead(dfs,testPath,length,expected,buffer);  LOG.info("verifyRead verifyStatefulRead2 on path {}",testPath);  StripedFileTestUtil.verifyStatefulRead(dfs,testPath,length,expected,ByteBuffer.allocate(length + 100));
public static void testReadWithDNFailure(MiniDFSCluster cluster,DistributedFileSystem dfs,int fileLength,int dnFailureNum) throws Exception {  String fileType=fileLength < (BLOCK_SIZE * NUM_DATA_UNITS) ? "smallFile" : "largeFile";  String src="/dnFailure_" + dnFailureNum + "_"+ fileType;
public static void testReadWithBlockCorrupted(MiniDFSCluster cluster,DistributedFileSystem dfs,String src,int fileNumBytes,int dataBlkDelNum,int parityBlkDelNum,boolean deleteBlockFile) throws IOException {
public static void corruptBlocks(MiniDFSCluster cluster,DistributedFileSystem dfs,Path srcPath,int dataBlkDelNum,int parityBlkDelNum,boolean deleteBlockFile) throws IOException {
  LOG.info("corruptBlocks on path {}",srcPath);  int recoverBlkNum=dataBlkDelNum + parityBlkDelNum;  LocatedBlocks locatedBlocks=getLocatedBlocks(dfs,srcPath);  LocatedStripedBlock lastBlock=(LocatedStripedBlock)locatedBlocks.getLastLocatedBlock();  int[] delDataBlkIndices=StripedFileTestUtil.randomArray(0,NUM_DATA_UNITS,dataBlkDelNum);  Assert.assertNotNull(delDataBlkIndices);  int[] delParityBlkIndices=StripedFileTestUtil.randomArray(NUM_DATA_UNITS,NUM_DATA_UNITS + NUM_PARITY_UNITS,parityBlkDelNum);  Assert.assertNotNull(delParityBlkIndices);  int[] delBlkIndices=new int[recoverBlkNum];  System.arraycopy(delDataBlkIndices,0,delBlkIndices,0,delDataBlkIndices.length);  System.arraycopy(delParityBlkIndices,0,delBlkIndices,delDataBlkIndices.length,delParityBlkIndices.length);  ExtendedBlock[] delBlocks=new ExtendedBlock[recoverBlkNum];  for (int i=0; i < recoverBlkNum; i++) {    delBlocks[i]=StripedBlockUtil.constructInternalBlock(lastBlock.getBlock(),CELL_SIZE,NUM_DATA_UNITS,delBlkIndices[i]);    if (deleteBlockFile) {
  LocatedStripedBlock lastBlock=(LocatedStripedBlock)locatedBlocks.getLastLocatedBlock();  int[] delDataBlkIndices=StripedFileTestUtil.randomArray(0,NUM_DATA_UNITS,dataBlkDelNum);  Assert.assertNotNull(delDataBlkIndices);  int[] delParityBlkIndices=StripedFileTestUtil.randomArray(NUM_DATA_UNITS,NUM_DATA_UNITS + NUM_PARITY_UNITS,parityBlkDelNum);  Assert.assertNotNull(delParityBlkIndices);  int[] delBlkIndices=new int[recoverBlkNum];  System.arraycopy(delDataBlkIndices,0,delBlkIndices,0,delDataBlkIndices.length);  System.arraycopy(delParityBlkIndices,0,delBlkIndices,delDataBlkIndices.length,delParityBlkIndices.length);  ExtendedBlock[] delBlocks=new ExtendedBlock[recoverBlkNum];  for (int i=0; i < recoverBlkNum; i++) {    delBlocks[i]=StripedBlockUtil.constructInternalBlock(lastBlock.getBlock(),CELL_SIZE,NUM_DATA_UNITS,delBlkIndices[i]);    if (deleteBlockFile) {      LOG.info("Deleting block file {}",delBlocks[i]);      cluster.corruptBlockOnDataNodesByDeletingBlockFile(delBlocks[i]);    } else {
public static void waitBlockGroupsReported(DistributedFileSystem fs,String src,int numDeadDNs) throws Exception {  boolean success;  final int ATTEMPTS=40;  int count=0;  final ErasureCodingPolicy ecPolicy=fs.getErasureCodingPolicy(new Path(src));  do {    success=true;    count++;    LocatedBlocks lbs=fs.getClient().getLocatedBlocks(src,0);    for (    LocatedBlock lb : lbs.getLocatedBlocks()) {      short expected=(short)(getRealTotalBlockNum((int)lb.getBlockSize(),ecPolicy) - numDeadDNs);      int reported=lb.getLocations().length;      if (reported < expected) {        success=false;
  int count=0;  final ErasureCodingPolicy ecPolicy=fs.getErasureCodingPolicy(new Path(src));  do {    success=true;    count++;    LocatedBlocks lbs=fs.getClient().getLocatedBlocks(src,0);    for (    LocatedBlock lb : lbs.getLocatedBlocks()) {      short expected=(short)(getRealTotalBlockNum((int)lb.getBlockSize(),ecPolicy) - numDeadDNs);      int reported=lb.getLocations().length;      if (reported < expected) {        success=false;        LOG.info("blockGroup " + lb.getBlock() + " of file "+ src+ " has reported internalBlocks "+ reported+ " (desired "+ expected+ "); locations "+ Joiner.on(' ').join(lb.getLocations()));        Thread.sleep(1000);        break;      }    }    if (success) {
  List<List<LocatedBlock>> blockGroupList=new ArrayList<>();  LocatedBlocks lbs=dfs.getClient().getLocatedBlocks(srcPath.toString(),0L,Long.MAX_VALUE);  if (length > 0) {    int expectedNumGroup=(length - 1) / blkGroupSize + 1;    assertEquals(expectedNumGroup,lbs.getLocatedBlocks().size());  }  final ErasureCodingPolicy ecPolicy=dfs.getErasureCodingPolicy(srcPath);  final int cellSize=ecPolicy.getCellSize();  final int dataBlkNum=ecPolicy.getNumDataUnits();  final int parityBlkNum=ecPolicy.getNumParityUnits();  int index=0;  for (  LocatedBlock firstBlock : lbs.getLocatedBlocks()) {    Assert.assertTrue(firstBlock instanceof LocatedStripedBlock);    final long gs=firstBlock.getBlock().getGenerationStamp();    final long oldGS=oldGSList != null ? oldGSList.get(index++) : -1L;    final String s="gs=" + gs + ", oldGS="+ oldGS;
    final int numCellInGroup=(groupSize - 1) / cellSize + 1;    final int lastCellIndex=(numCellInGroup - 1) % dataBlkNum;    final int lastCellSize=groupSize - (numCellInGroup - 1) * cellSize;    List<LocatedBlock> blockList=blockGroupList.get(group);    byte[][] dataBlockBytes=new byte[dataBlkNum][];    byte[][] parityBlockBytes=new byte[parityBlkNum][];    Set<Integer> checkSet=new HashSet<>();    for (int i=0; i < blockList.size(); i++) {      final int j=i >= dataBlkNum ? 0 : i;      final int numCellInBlock=(numCellInGroup - 1) / dataBlkNum + (j <= lastCellIndex ? 1 : 0);      final int blockSize=numCellInBlock * cellSize + (isLastGroup && j == lastCellIndex ? lastCellSize - cellSize : 0);      final byte[] blockBytes=new byte[blockSize];      if (i < dataBlkNum) {        dataBlockBytes[i]=blockBytes;      } else {
public static LocatedBlocks waitForReconstructionFinished(Path file,DistributedFileSystem fs,int groupSize) throws Exception {
public static void waitForAllReconstructionFinished(Path file,DistributedFileSystem fs,long expectedBlocks) throws Exception {
void setErrorState(Throwable t){  checkErrorState();
    DFSClientFaultInjector.set(new DFSClientFaultInjector(){      public boolean skipRollingRestartWait(){        return true;      }    });    final DFSOutputStream out=(DFSOutputStream)fileSys.append(file).getWrappedStream();    final AtomicBoolean running=new AtomicBoolean(true);    final AtomicBoolean failed=new AtomicBoolean(false);    Thread t=new Thread(){      public void run(){        while (running.get()) {          try {            out.write("test".getBytes());            out.hflush();            Thread.sleep(1000);          } catch (          IOException|InterruptedException e) {
        }      }    });    Random r=new Random();    byte[] b=new byte[oneWriteSize];    while (count < totalSize) {      r.nextBytes(b);      o.write(b);      count+=oneWriteSize;      o.hflush();    }    assertTrue("Expected a failure in the pipeline",failed.get());    DatanodeInfo[] newNodes=dfsO.getStreamer().getNodes();    o.close();    for (    DataNode d : cluster.getDataNodes()) {      DataNodeTestUtils.triggerBlockReport(d);    }    List<DatanodeInfo> pipelineList=Arrays.asList(pipeline);    DatanodeInfo newNode=null;
private void createAFileWithCorruptedBlockReplicas(Path filePath,short repl,int corruptBlockCount) throws IOException, AccessControlException, FileNotFoundException, UnresolvedLinkException, InterruptedException, TimeoutException {  DFSTestUtil.createFile(dfs,filePath,BLOCK_SIZE,repl,0);  DFSTestUtil.waitReplication(dfs,filePath,repl);  final LocatedBlocks locatedblocks=dfs.dfs.getNamenode().getBlockLocations(filePath.toString(),0L,BLOCK_SIZE);  Assert.assertEquals(repl,locatedblocks.get(0).getLocations().length);  LocatedBlock lblock=locatedblocks.get(0);  DatanodeInfo[] datanodeinfos=lblock.getLocations();  ExtendedBlock block=lblock.getBlock();  for (int i=0; i < corruptBlockCount; i++) {    DatanodeInfo dninfo=datanodeinfos[i];    final DataNode dn=cluster.getDataNode(dninfo.getIpcPort());    cluster.corruptReplica(dn,block);
private static void verifyFsckHealth(String expected) throws Exception {  String outStr=runFsck(conf,0,true,"/");
private static void verifyFsckBlockCorrupted() throws Exception {  String outStr=runFsck(conf,1,true,"/");
private static void testFsckListCorruptFilesBlocks(Path filePath,int errorCode) throws Exception {  String outStr=runFsck(conf,errorCode,true,filePath.toString(),"-list-corruptfileblocks");
private void pread(DFSInputStream in,long pos,byte[] buffer,int offset,int length,byte[] authenticData) throws IOException {  Assert.assertTrue("Test buffer too small",buffer.length >= offset + length);  if (pos >= 0)   in.seek(pos);
@Test public void testReadFromOneDN() throws Exception {  HdfsConfiguration configuration=new HdfsConfiguration();  final String contextName="testReadFromOneDNContext";  configuration.set(HdfsClientConfigKeys.DFS_CLIENT_CONTEXT,contextName);  configuration.setLong(HdfsClientConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY,100000000L);  BlockReaderTestUtil util=new BlockReaderTestUtil(1,configuration);  final Path testFile=new Path("/testConnCache.dat");  byte authenticData[]=util.writeFile(testFile,FILE_SIZE / 1024);  DFSClient client=new DFSClient(new InetSocketAddress("localhost",util.getCluster().getNameNodePort()),util.getConf());  DFSInputStream in=client.open(testFile.toString());
  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();    cluster.waitActive();    FileSystem fs=cluster.getFileSystem();    util.createFiles(fs,"/srcdat",replFactor);    util.waitReplication(fs,"/srcdat",(short)2);    final int dnIdx=0;    final DataNode dn=cluster.getDataNodes().get(dnIdx);    final String bpid=cluster.getNamesystem().getBlockPoolId();    List<ReplicaInfo> replicas=dn.getFSDataset().getFinalizedBlocks(bpid);    assertTrue("Replicas do not exist",!replicas.isEmpty());    for (int idx=0; idx < replicas.size(); idx++) {      ReplicaInfo replica=replicas.get(idx);      ExtendedBlock eb=new ExtendedBlock(bpid,replica);      if (idx % 3 == 0) {
    util.createFiles(fs,"/srcdat",replFactor);    util.waitReplication(fs,"/srcdat",(short)2);    final int dnIdx=0;    final DataNode dn=cluster.getDataNodes().get(dnIdx);    final String bpid=cluster.getNamesystem().getBlockPoolId();    List<ReplicaInfo> replicas=dn.getFSDataset().getFinalizedBlocks(bpid);    assertTrue("Replicas do not exist",!replicas.isEmpty());    for (int idx=0; idx < replicas.size(); idx++) {      ReplicaInfo replica=replicas.get(idx);      ExtendedBlock eb=new ExtendedBlock(bpid,replica);      if (idx % 3 == 0) {        LOG.info("Deliberately removing meta for block " + eb);        cluster.deleteMeta(dnIdx,eb);      } else       if (idx % 3 == 1) {        final int newSize=2;
  Path file=new Path(src);  conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,4096);  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();  try {    cluster.waitActive();    FileSystem fs=cluster.getFileSystem();    NamenodeProtocols preSpyNN=cluster.getNameNodeRpc();    NamenodeProtocols spyNN=spy(preSpyNN);    DFSClient client=new DFSClient(null,spyNN,conf,null);    doAnswer(new Answer<LocatedBlock>(){      private int getBlockCount(      LocatedBlock ret) throws IOException {        LocatedBlocks lb=cluster.getNameNodeRpc().getBlockLocations(src,0,Long.MAX_VALUE);        assertEquals(lb.getLastLocatedBlock().getBlock(),ret.getBlock());        return lb.getLocatedBlocks().size();      }      @Override public LocatedBlock answer(      InvocationOnMock invocation) throws Throwable {
        final int blockCount=getBlockCount(ret);        final LocatedBlock ret2;        try {          ret2=(LocatedBlock)invocation.callRealMethod();        } catch (        NotReplicatedYetException e) {          throw new AssertionError("Unexpected exception",e);        }        final int blockCount2=getBlockCount(ret2);        assertEquals(blockCount,blockCount2);        return ret2;      }    }).when(spyNN).addBlock(Mockito.anyString(),Mockito.anyString(),Mockito.<ExtendedBlock>any(),Mockito.<DatanodeInfo[]>any(),Mockito.anyLong(),Mockito.<String[]>any(),Mockito.<EnumSet<AddBlockFlag>>any());    doAnswer(new Answer<Boolean>(){      @Override public Boolean answer(      InvocationOnMock invocation) throws Throwable {        LOG.info("Called complete:");        if (!(Boolean)invocation.callRealMethod()) {          LOG.info("Complete call returned false, not faking a retry RPC");
static void parseMultipleLinearRandomRetry(String expected,String s){  final MultipleLinearRandomRetry r=MultipleLinearRandomRetry.parseCommaSeparatedString(s);
@Test public void testDefaultSendBufferSize() throws IOException {  final int sendBufferSize=getSendBufferSize(new Configuration());
@Test public void testSpecifiedSendBufferSize() throws IOException {  final Configuration conf1=new Configuration();  conf1.setInt(DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY,256 * 1024);  final int sendBufferSize1=getSendBufferSize(conf1);  final Configuration conf2=new Configuration();  conf2.setInt(DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY,1024);  final int sendBufferSize2=getSendBufferSize(conf2);
@Test public void testAutoTuningSendBufferSize() throws IOException {  final Configuration conf=new Configuration();  conf.setInt(DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY,0);  final int sendBufferSize=getSendBufferSize(conf);
    client.removeXAttr("/file5","user.field");    client.setAcl("/file5",AclEntry.parseAclSpec("user::rwx,user:foo:rw-,group::r--,other::---",true));    client.removeAcl("/file5");    client.rename("/file5","/dir");    client.truncate("/truncate_file",BLOCK_SIZE);    client.create("/file_ec_test1",false);    EventBatch batch=null;    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    long txid=batch.getTxid();    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.RENAME);    Event.RenameEvent re=(Event.RenameEvent)batch.getEvents()[0];    Assert.assertEquals("/file4",re.getDstPath());    Assert.assertEquals("/file",re.getSrcPath());    Assert.assertTrue(re.getTimestamp() > 0);
    Event.RenameEvent re=(Event.RenameEvent)batch.getEvents()[0];    Assert.assertEquals("/file4",re.getDstPath());    Assert.assertEquals("/file",re.getSrcPath());    Assert.assertTrue(re.getTimestamp() > 0);    LOG.info(re.toString());    Assert.assertTrue(re.toString().startsWith("RenameEvent [srcPath="));    long eventsBehind=eis.getTxidsBehindEstimate();    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.RENAME);    Event.RenameEvent re2=(Event.RenameEvent)batch.getEvents()[0];    Assert.assertTrue(re2.getDstPath().equals("/file2"));    Assert.assertTrue(re2.getSrcPath().equals("/file4"));    Assert.assertTrue(re2.getTimestamp() > 0);
    LOG.info(re2.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CREATE);    Event.CreateEvent ce=(Event.CreateEvent)batch.getEvents()[0];    Assert.assertTrue(ce.getiNodeType() == Event.CreateEvent.INodeType.FILE);    Assert.assertTrue(ce.getPath().equals("/file2"));    Assert.assertTrue(ce.getCtime() > 0);    Assert.assertTrue(ce.getReplication() > 0);    Assert.assertTrue(ce.getSymlinkTarget() == null);    Assert.assertTrue(ce.getOverwrite());    Assert.assertEquals(BLOCK_SIZE,ce.getDefaultBlockSize());    Assert.assertTrue(ce.isErasureCoded().isPresent());    Assert.assertFalse(ce.isErasureCoded().get());
    Assert.assertTrue(ce.getSymlinkTarget() == null);    Assert.assertTrue(ce.getOverwrite());    Assert.assertEquals(BLOCK_SIZE,ce.getDefaultBlockSize());    Assert.assertTrue(ce.isErasureCoded().isPresent());    Assert.assertFalse(ce.isErasureCoded().get());    LOG.info(ce.toString());    Assert.assertTrue(ce.toString().startsWith("CreateEvent [INodeType="));    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CLOSE);    Event.CloseEvent ce2=(Event.CloseEvent)batch.getEvents()[0];    Assert.assertTrue(ce2.getPath().equals("/file2"));    Assert.assertTrue(ce2.getFileSize() > 0);    Assert.assertTrue(ce2.getTimestamp() > 0);
    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CLOSE);    Event.CloseEvent ce2=(Event.CloseEvent)batch.getEvents()[0];    Assert.assertTrue(ce2.getPath().equals("/file2"));    Assert.assertTrue(ce2.getFileSize() > 0);    Assert.assertTrue(ce2.getTimestamp() > 0);    LOG.info(ce2.toString());    Assert.assertTrue(ce2.toString().startsWith("CloseEvent [path="));    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.APPEND);    Event.AppendEvent append2=(Event.AppendEvent)batch.getEvents()[0];    Assert.assertEquals("/file2",append2.getPath());    Assert.assertFalse(append2.toNewBlock());
    Assert.assertFalse(append2.toNewBlock());    LOG.info(append2.toString());    Assert.assertTrue(append2.toString().startsWith("AppendEvent [path="));    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CLOSE);    Assert.assertTrue(((Event.CloseEvent)batch.getEvents()[0]).getPath().equals("/file2"));    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue.getPath().equals("/file2"));    Assert.assertTrue(mue.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.TIMES);
    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue.getPath().equals("/file2"));    Assert.assertTrue(mue.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.TIMES);    LOG.info(mue.toString());    Assert.assertTrue(mue.toString().startsWith("MetadataUpdateEvent [path="));    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue2=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue2.getPath().equals("/file2"));    Assert.assertTrue(mue2.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.REPLICATION);    Assert.assertTrue(mue2.getReplication() == 1);
    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue2=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue2.getPath().equals("/file2"));    Assert.assertTrue(mue2.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.REPLICATION);    Assert.assertTrue(mue2.getReplication() == 1);    LOG.info(mue2.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(3,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.APPEND);    Assert.assertTrue(((Event.AppendEvent)batch.getEvents()[0]).getPath().equals("/file2"));    Assert.assertTrue(batch.getEvents()[1].getEventType() == Event.EventType.UNLINK);    Event.UnlinkEvent ue2=(Event.UnlinkEvent)batch.getEvents()[1];    Assert.assertTrue(ue2.getPath().equals("/file3"));    Assert.assertTrue(ue2.getTimestamp() > 0);
    Assert.assertTrue(ue2.getPath().equals("/file3"));    Assert.assertTrue(ue2.getTimestamp() > 0);    LOG.info(ue2.toString());    Assert.assertTrue(ue2.toString().startsWith("UnlinkEvent [path="));    Assert.assertTrue(batch.getEvents()[2].getEventType() == Event.EventType.CLOSE);    Event.CloseEvent ce3=(Event.CloseEvent)batch.getEvents()[2];    Assert.assertTrue(ce3.getPath().equals("/file2"));    Assert.assertTrue(ce3.getTimestamp() > 0);    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.UNLINK);    Event.UnlinkEvent ue=(Event.UnlinkEvent)batch.getEvents()[0];    Assert.assertTrue(ue.getPath().equals("/file2"));    Assert.assertTrue(ue.getTimestamp() > 0);
    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.UNLINK);    Event.UnlinkEvent ue=(Event.UnlinkEvent)batch.getEvents()[0];    Assert.assertTrue(ue.getPath().equals("/file2"));    Assert.assertTrue(ue.getTimestamp() > 0);    LOG.info(ue.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CREATE);    Event.CreateEvent ce4=(Event.CreateEvent)batch.getEvents()[0];    Assert.assertTrue(ce4.getiNodeType() == Event.CreateEvent.INodeType.DIRECTORY);    Assert.assertTrue(ce4.getPath().equals("/dir"));    Assert.assertTrue(ce4.getCtime() > 0);    Assert.assertTrue(ce4.getReplication() == 0);    Assert.assertTrue(ce4.getSymlinkTarget() == null);
    Event.CreateEvent ce4=(Event.CreateEvent)batch.getEvents()[0];    Assert.assertTrue(ce4.getiNodeType() == Event.CreateEvent.INodeType.DIRECTORY);    Assert.assertTrue(ce4.getPath().equals("/dir"));    Assert.assertTrue(ce4.getCtime() > 0);    Assert.assertTrue(ce4.getReplication() == 0);    Assert.assertTrue(ce4.getSymlinkTarget() == null);    LOG.info(ce4.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue3=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue3.getPath().equals("/dir"));    Assert.assertTrue(mue3.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.PERMS);    Assert.assertTrue(mue3.getPerms().toString().contains("rw-rw-rw-"));
    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue3=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue3.getPath().equals("/dir"));    Assert.assertTrue(mue3.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.PERMS);    Assert.assertTrue(mue3.getPerms().toString().contains("rw-rw-rw-"));    LOG.info(mue3.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue4=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue4.getPath().equals("/dir"));    Assert.assertTrue(mue4.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.OWNER);    Assert.assertTrue(mue4.getOwnerName().equals("username"));    Assert.assertTrue(mue4.getGroupName().equals("groupname"));
    Assert.assertTrue(mue4.getPath().equals("/dir"));    Assert.assertTrue(mue4.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.OWNER);    Assert.assertTrue(mue4.getOwnerName().equals("username"));    Assert.assertTrue(mue4.getGroupName().equals("groupname"));    LOG.info(mue4.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CREATE);    Event.CreateEvent ce5=(Event.CreateEvent)batch.getEvents()[0];    Assert.assertTrue(ce5.getiNodeType() == Event.CreateEvent.INodeType.SYMLINK);    Assert.assertTrue(ce5.getPath().equals("/dir2"));    Assert.assertTrue(ce5.getCtime() > 0);    Assert.assertTrue(ce5.getReplication() == 0);    Assert.assertTrue(ce5.getSymlinkTarget().equals("/dir"));
    Assert.assertTrue(ce5.getPath().equals("/dir2"));    Assert.assertTrue(ce5.getCtime() > 0);    Assert.assertTrue(ce5.getReplication() == 0);    Assert.assertTrue(ce5.getSymlinkTarget().equals("/dir"));    LOG.info(ce5.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue5=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue5.getPath().equals("/file5"));    Assert.assertTrue(mue5.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.XATTRS);    Assert.assertTrue(mue5.getxAttrs().size() == 1);    Assert.assertTrue(mue5.getxAttrs().get(0).getName().contains("field"));    Assert.assertTrue(!mue5.isxAttrsRemoved());
    Assert.assertTrue(mue5.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.XATTRS);    Assert.assertTrue(mue5.getxAttrs().size() == 1);    Assert.assertTrue(mue5.getxAttrs().get(0).getName().contains("field"));    Assert.assertTrue(!mue5.isxAttrsRemoved());    LOG.info(mue5.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue6=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue6.getPath().equals("/file5"));    Assert.assertTrue(mue6.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.XATTRS);    Assert.assertTrue(mue6.getxAttrs().size() == 1);    Assert.assertTrue(mue6.getxAttrs().get(0).getName().contains("field"));    Assert.assertTrue(mue6.isxAttrsRemoved());
    Event.MetadataUpdateEvent mue6=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue6.getPath().equals("/file5"));    Assert.assertTrue(mue6.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.XATTRS);    Assert.assertTrue(mue6.getxAttrs().size() == 1);    Assert.assertTrue(mue6.getxAttrs().get(0).getName().contains("field"));    Assert.assertTrue(mue6.isxAttrsRemoved());    LOG.info(mue6.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue7=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue7.getPath().equals("/file5"));    Assert.assertTrue(mue7.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.ACLS);    Assert.assertTrue(mue7.getAcls().contains(AclEntry.parseAclEntry("user::rwx",true)));
    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue7=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue7.getPath().equals("/file5"));    Assert.assertTrue(mue7.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.ACLS);    Assert.assertTrue(mue7.getAcls().contains(AclEntry.parseAclEntry("user::rwx",true)));    LOG.info(mue7.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue8=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue8.getPath().equals("/file5"));    Assert.assertTrue(mue8.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.ACLS);    Assert.assertTrue(mue8.getAcls() == null);
    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.METADATA);    Event.MetadataUpdateEvent mue8=(Event.MetadataUpdateEvent)batch.getEvents()[0];    Assert.assertTrue(mue8.getPath().equals("/file5"));    Assert.assertTrue(mue8.getMetadataType() == Event.MetadataUpdateEvent.MetadataType.ACLS);    Assert.assertTrue(mue8.getAcls() == null);    LOG.info(mue8.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.RENAME);    Event.RenameEvent re3=(Event.RenameEvent)batch.getEvents()[0];    Assert.assertTrue(re3.getDstPath().equals("/dir/file5"));    Assert.assertTrue(re3.getSrcPath().equals("/file5"));    Assert.assertTrue(re3.getTimestamp() > 0);
    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.RENAME);    Event.RenameEvent re3=(Event.RenameEvent)batch.getEvents()[0];    Assert.assertTrue(re3.getDstPath().equals("/dir/file5"));    Assert.assertTrue(re3.getSrcPath().equals("/file5"));    Assert.assertTrue(re3.getTimestamp() > 0);    LOG.info(re3.toString());    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.TRUNCATE);    Event.TruncateEvent et=((Event.TruncateEvent)batch.getEvents()[0]);    Assert.assertTrue(et.getPath().equals("/truncate_file"));    Assert.assertTrue(et.getFileSize() == BLOCK_SIZE);    Assert.assertTrue(et.getTimestamp() > 0);
    Assert.assertTrue(et.toString().startsWith("TruncateEvent [path="));    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    txid=checkTxid(batch,txid);    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CREATE);    ce=(Event.CreateEvent)batch.getEvents()[0];    Assert.assertTrue(ce.getiNodeType() == Event.CreateEvent.INodeType.FILE);    Assert.assertTrue(ce.getPath().equals("/file_ec_test1"));    Assert.assertTrue(ce.getCtime() > 0);    Assert.assertTrue(ce.getReplication() > 0);    Assert.assertTrue(ce.getSymlinkTarget() == null);    Assert.assertFalse(ce.getOverwrite());    Assert.assertEquals(BLOCK_SIZE,ce.getDefaultBlockSize());    Assert.assertTrue(ce.isErasureCoded().isPresent());    Assert.assertFalse(ce.isErasureCoded().get());
    batch=waitForNextEvents(eis);    Assert.assertEquals(1,batch.getEvents().length);    long txid=batch.getTxid();    long eventsBehind=eis.getTxidsBehindEstimate();    Assert.assertTrue(batch.getEvents()[0].getEventType() == Event.EventType.CREATE);    Event.CreateEvent ce=(Event.CreateEvent)batch.getEvents()[0];    Assert.assertTrue(ce.getiNodeType() == Event.CreateEvent.INodeType.FILE);    Assert.assertTrue(ce.getPath().equals("/ecdir/file_ec_test2"));    Assert.assertTrue(ce.getCtime() > 0);    Assert.assertEquals(1,ce.getReplication());    Assert.assertTrue(ce.getSymlinkTarget() == null);    Assert.assertTrue(ce.getOverwrite());    Assert.assertEquals(ecPolicy.getCellSize(),ce.getDefaultBlockSize());    Assert.assertTrue(ce.isErasureCoded().isPresent());    Assert.assertTrue(ce.isErasureCoded().get());
@Test public void testWithKerberizedCluster() throws Exception {  conf=new HdfsConfiguration(baseConf);  conf.setInt(HADOOP_KERBEROS_MIN_SECONDS_BEFORE_RELOGIN,3);  conf.setInt(IPC_CLIENT_CONNECTION_IDLESCANINTERVAL_KEY,100);  conf.setInt(IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,2000);  Client.setConnectTimeout(conf,2000);  cluster=new MiniQJMHACluster.Builder(conf).setForceRemoteEditsOnly(true).build();  cluster.getDfsCluster().waitActive();  cluster.getDfsCluster().transitionToActive(0);  final UserGroupInformation ugi=UserGroupInformation.loginUserFromKeytabAndReturnUGI("hdfs",generalHDFSKeytabFile.getAbsolutePath());  UserGroupInformation.setShouldRenewImmediatelyForTests(true);  ugi.doAs(new PrivilegedExceptionAction<Void>(){    @Override public Void run() throws Exception {
  cluster=new MiniQJMHACluster.Builder(conf).setForceRemoteEditsOnly(true).build();  cluster.getDfsCluster().waitActive();  cluster.getDfsCluster().transitionToActive(0);  final UserGroupInformation ugi=UserGroupInformation.loginUserFromKeytabAndReturnUGI("hdfs",generalHDFSKeytabFile.getAbsolutePath());  UserGroupInformation.setShouldRenewImmediatelyForTests(true);  ugi.doAs(new PrivilegedExceptionAction<Void>(){    @Override public Void run() throws Exception {      LOG.info("Current user is: " + UserGroupInformation.getCurrentUser() + " login user is:"+ UserGroupInformation.getLoginUser());      Configuration clientConf=new Configuration(cluster.getDfsCluster().getConfiguration(0));      try (DistributedFileSystem clientFs=(DistributedFileSystem)FileSystem.get(clientConf)){        clientFs.mkdirs(new Path("/test"));        LOG.info("mkdir /test success");        final DFSInotifyEventInputStream eis=clientFs.getInotifyEventStream();        EventBatch batch;        while ((batch=eis.poll()) != null) {
    final Path file=new Path(dir,"file");    writeFile(dfs,file);    final Path file2=new Path(dir,"file2");    writeFile(dfs,file2);    final Long fileLength=dfs.getFileStatus(file).getLen();    final Long fileDiskUsed=fileLength * replication;    final Long file2Length=dfs.getFileStatus(file2).getLen();    final Long file2DiskUsed=file2Length * replication;    int ret=-1;    try {      ret=shell.run(new String[]{"-du",dir.toString()});    } catch (    Exception e) {      System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());    }    assertEquals(0,ret);    String returnString=out.toString();
    } catch (    Exception e) {      System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());    }    assertEquals(0,ret);    returnString=out.toString();    LOG.info("-du -s return is:\n" + returnString);    Long combinedLength=fileLength + file2Length + newFileLength;    Long combinedDiskUsed=fileDiskUsed + file2DiskUsed + newFileDiskUsed;    assertTrue(returnString.contains(combinedLength.toString()));    assertTrue(returnString.contains(combinedDiskUsed.toString()));    out.reset();    ret=-1;    try {      ret=shell.run(new String[]{"-du",parent.toString()});    } catch (    Exception e) {      System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());
    out.reset();    ret=-1;    try {      ret=shell.run(new String[]{"-du",parent.toString()});    } catch (    Exception e) {      System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());    }    assertEquals(0,ret);    returnString=out.toString();    LOG.info("-du return is:\n" + returnString);    assertTrue(returnString.contains(combinedLength.toString()));    assertTrue(returnString.contains(combinedDiskUsed.toString()));    out.reset();    ret=-1;    try {      ret=shell.run(new String[]{"-du","-s","-x",parent.toString()});
    out.reset();    ret=-1;    try {      ret=shell.run(new String[]{"-du","-s","-x",parent.toString()});    } catch (    Exception e) {      System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());    }    assertEquals(0,ret);    returnString=out.toString();    LOG.info("-du -s -x return is:\n" + returnString);    Long exludeSnapshotLength=file2Length + newFileLength;    Long excludeSnapshotDiskUsed=file2DiskUsed + newFileDiskUsed;    assertTrue(returnString.contains(exludeSnapshotLength.toString()));    assertTrue(returnString.contains(excludeSnapshotDiskUsed.toString()));    out.reset();    ret=-1;
    final Path snapshotPath=new Path(parent,".snapshot/" + snapshotName);    dfs.allowSnapshot(parent);    assertThat(dfs.createSnapshot(parent,snapshotName),is(snapshotPath));    rmr(dfs,file);    rmr(dfs,dir2);    final Path newFile=new Path(dir,"new file");    writeFile(dfs,newFile);    final Long newFileLength=dfs.getFileStatus(newFile).getLen();    int val=-1;    try {      val=shell.run(new String[]{"-count","-v",parent.toString()});    } catch (    Exception e) {      System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());    }    assertEquals(0,val);    String returnString=out.toString();
    }    assertEquals(0,val);    String returnString=out.toString();    LOG.info("-count return is:\n" + returnString);    Scanner in=new Scanner(returnString);    in.nextLine();    assertEquals(3,in.nextLong());    assertEquals(3,in.nextLong());    assertEquals(fileLength + file2Length + newFileLength,in.nextLong());    out.reset();    val=-1;    try {      val=shell.run(new String[]{"-count","-x","-v",parent.toString()});    } catch (    Exception e) {      System.err.println("Exception raised from DFSShell.run " + e.getLocalizedMessage());    }    assertEquals(0,val);
private static int runCmd(FsShell shell,String... args) throws IOException {  StringBuilder cmdline=new StringBuilder("RUN:");  for (  String arg : args)   cmdline.append(" " + arg);
private static int runCmd(FsShell shell,String... args) throws IOException {  StringBuilder cmdline=new StringBuilder("RUN:");  for (  String arg : args)   cmdline.append(" " + arg);  LOG.info(cmdline.toString());  try {    int exitCode;    exitCode=shell.run(args);
  shell.setConf(conf);  try {    Path dir=new Path(chmodDir);    fs.delete(dir,true);    fs.mkdirs(dir);    confirmPermissionChange("u+rwx,g=rw,o-rwx","rwxrw----",fs,shell,dir);    Path file=new Path(chmodDir,"file");    TestDFSShell.writeFile(fs,file);    confirmPermissionChange("644","rw-r--r--",fs,shell,file);    runCmd(shell,"-chmod","-R","a+rwX",chmodDir);    assertEquals("rwxrwxrwx",fs.getFileStatus(dir).getPermission().toString());    assertEquals("rw-rw-rw-",fs.getFileStatus(file).getPermission().toString());    if (!Path.WINDOWS) {      Path dir2=new Path(dir,"stickybit");      fs.mkdirs(dir2);
private void confirmPermissionChange(String toApply,String expected,FileSystem fs,FsShell shell,Path dir2) throws IOException {
private void confirmPermissionChange(String toApply,String expected,FileSystem fs,FsShell shell,Path dir2) throws IOException {  LOG.info("Confirming permission change of " + toApply + " to "+ expected);  runCmd(shell,"-chmod",toApply,dir2.toString());  String result=fs.getFileStatus(dir2).getPermission().toString();
@Test public void testCloseDoesNotAllocateNewBuffer() throws Exception {  final int numBlocks=2;  DFSTestUtil.createStripedFile(cluster,filePath,null,numBlocks,stripesPerBlock,false,ecPolicy);  try (DFSInputStream in=fs.getClient().open(filePath.toString())){    assertTrue(in instanceof DFSStripedInputStream);    final DFSStripedInputStream stream=(DFSStripedInputStream)in;    final ElasticByteBufferPool ebbp=(ElasticByteBufferPool)stream.getBufferPool();
public void testMultipleDatanodeFailureRandomLength() throws Exception {  int lenIndex=RANDOM.nextInt(lengths.size());
  final HdfsConfiguration conf=newHdfsConfiguration();  final int[] fileLengths={cellSize * (dataBlocks * 2 - 2),(cellSize * dataBlocks) + 123};  int[] dnIndex=null;  if (parityBlocks > 1) {    dnIndex=new int[]{dataBlocks - 2,dataBlocks - 1};  } else {    dnIndex=new int[]{dataBlocks - 1};  }  for (  int length : fileLengths) {    final int[] killPos=getKillPositions(length,dnIndex.length);    try {      LOG.info("runTestWithMultipleFailure2: length==" + length + ", killPos="+ Arrays.toString(killPos)+ ", dnIndex="+ Arrays.toString(dnIndex));      setup(conf);      runTest(length,killPos,dnIndex,false);    } catch (    Throwable e) {      final String err="failed, killPos=" + Arrays.toString(killPos) + ", dnIndex="+ Arrays.toString(dnIndex)+ ", length="+ length;
void runTest(final int length){  final HdfsConfiguration conf=newHdfsConfiguration();  for (int dn=0; dn < dataBlocks + parityBlocks; dn++) {    try {
void runTestWithMultipleFailure(final int length) throws Exception {  final HdfsConfiguration conf=newHdfsConfiguration();  for (  int[] dnIndex : dnIndexSuite) {    int[] killPos=getKillPositions(length,dnIndex.length);    try {
static long getGenerationStamp(DFSStripedOutputStream out) throws IOException {  final long gs=out.getBlock().getGenerationStamp();
static DatanodeInfo killDatanode(MiniDFSCluster cluster,DFSStripedOutputStream out,final int dnIndex,final AtomicInteger pos){  final StripedDataStreamer s=out.getStripedDataStreamer(dnIndex);  final DatanodeInfo datanode=getDatanodes(s);
void checkNameNode(String[] baseDirs,long imageTxId) throws IOException {  for (  String baseDir : baseDirs) {
    conf=new HdfsConfiguration();    conf=UpgradeUtilities.initializeStorageStateConf(numDirs,conf);    String[] nameNodeDirs=conf.getStrings(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY);    String[] dataNodeDirs=conf.getStrings(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY);    conf.setBoolean(DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION,false);    log("Normal NameNode upgrade",numDirs);    UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");    cluster=createCluster();    try {      final DistributedFileSystem dfs=cluster.getFileSystem();      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);      dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);      fail();    } catch (    RemoteException re) {      assertEquals(InconsistentFSStateException.class.getName(),re.getClassName());
private void sendRecvData(String testDescription,boolean eofExpected) throws IOException {  Socket sock=null;  try {    if (testDescription != null) {
    sock.connect(dnAddr,HdfsConstants.READ_TIMEOUT);    sock.setSoTimeout(HdfsConstants.READ_TIMEOUT);    OutputStream out=sock.getOutputStream();    byte[] retBuf=new byte[recvBuf.size()];    DataInputStream in=new DataInputStream(sock.getInputStream());    out.write(sendBuf.toByteArray());    out.flush();    try {      in.readFully(retBuf);    } catch (    EOFException eof) {      if (eofExpected) {        LOG.info("Got EOF as expected.");        return;      }      throw eof;    }    String received=StringUtils.byteToHexString(retBuf);
    sock.setSoTimeout(HdfsConstants.READ_TIMEOUT);    OutputStream out=sock.getOutputStream();    byte[] retBuf=new byte[recvBuf.size()];    DataInputStream in=new DataInputStream(sock.getInputStream());    out.write(sendBuf.toByteArray());    out.flush();    try {      in.readFully(retBuf);    } catch (    EOFException eof) {      if (eofExpected) {        LOG.info("Got EOF as expected.");        return;      }      throw eof;    }    String received=StringUtils.byteToHexString(retBuf);    String expected=StringUtils.byteToHexString(recvBuf.toByteArray());
    doReturn(HdfsServerConstants.DATANODE_LAYOUT_VERSION).when(mockDnReg).getVersion();    doReturn("127.0.0.1").when(mockDnReg).getIpAddr();    doReturn(123).when(mockDnReg).getXferPort();    doReturn("fake-storage-id").when(mockDnReg).getDatanodeUuid();    doReturn(mockStorageInfo).when(mockDnReg).getStorageInfo();    doReturn("3.0.0").when(mockDnReg).getSoftwareVersion();    rpcServer.registerDatanode(mockDnReg);    doReturn("4.0.0").when(mockDnReg).getSoftwareVersion();    rpcServer.registerDatanode(mockDnReg);    doReturn("2.0.0").when(mockDnReg).getSoftwareVersion();    try {      rpcServer.registerDatanode(mockDnReg);      fail("Should not have been able to register DN with too-low version.");    } catch (    IncorrectVersionException ive) {      GenericTestUtils.assertExceptionContains("The reported DataNode version is too low",ive);
    doReturn(HdfsServerConstants.DATANODE_LAYOUT_VERSION).when(mockDnReg).getVersion();    doReturn("fake-storage-id").when(mockDnReg).getDatanodeUuid();    doReturn(mockStorageInfo).when(mockDnReg).getStorageInfo();    doReturn(VersionInfo.getVersion()).when(mockDnReg).getSoftwareVersion();    doReturn("127.0.0.1").when(mockDnReg).getIpAddr();    doReturn(123).when(mockDnReg).getXferPort();    rpcServer.registerDatanode(mockDnReg);    doReturn(nnCTime + 1).when(mockStorageInfo).getCTime();    rpcServer.registerDatanode(mockDnReg);    doReturn(VersionInfo.getVersion() + ".1").when(mockDnReg).getSoftwareVersion();    try {      rpcServer.registerDatanode(mockDnReg);      fail("Should not have been able to register DN with different software" + " versions and CTimes");    } catch (    IncorrectVersionException ive) {      GenericTestUtils.assertExceptionContains("does not match CTime of NN",ive);
@Test public void testDatanodeReport() throws Exception {  conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,500);  conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(NUM_OF_DATANODES).build();  try {    cluster.waitActive();    final String bpid=cluster.getNamesystem().getBlockPoolId();    final List<DataNode> datanodes=cluster.getDataNodes();    final DFSClient client=cluster.getFileSystem().dfs;    assertReports(NUM_OF_DATANODES,DatanodeReportType.ALL,client,datanodes,bpid);    assertReports(NUM_OF_DATANODES,DatanodeReportType.LIVE,client,datanodes,bpid);    assertReports(0,DatanodeReportType.DEAD,client,datanodes,bpid);    final DataNode last=datanodes.get(datanodes.size() - 1);
    final DatanodeManager datanodeManager=blockManager.getDatanodeManager();    BlockManagerTestUtil.recheckDecommissionState(datanodeManager);    DFSClient client=getDfsClient(0);    assertEquals("All datanodes must be alive",numDatanodes,client.datanodeReport(DatanodeReportType.LIVE).length);    final ExtendedBlock b=DFSTestUtil.getFirstBlock(fileSys,file1);    final String uuid=toDecomUuid;    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        BlockInfo info=blockManager.getStoredBlock(b.getLocalBlock());        int count=0;        StringBuilder sb=new StringBuilder("Replica locations: ");        for (int i=0; i < info.numNodes(); i++) {          DatanodeDescriptor dn=info.getDatanode(i);          sb.append(dn + ", ");          if (!dn.getDatanodeUuid().equals(uuid)) {
    final DatanodeManager datanodeManager=blockManager.getDatanodeManager();    BlockManagerTestUtil.recheckDecommissionState(datanodeManager);    DFSClient client=getDfsClient(0);    assertEquals("All datanodes must be alive",numDatanodes,client.datanodeReport(DatanodeReportType.LIVE).length);    final ExtendedBlock b=DFSTestUtil.getFirstBlock(fileSys,file1);    final String uuid=toDecomUuid;    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        BlockInfo info=blockManager.getStoredBlock(b.getLocalBlock());        int count=0;        StringBuilder sb=new StringBuilder("Replica locations: ");        for (int i=0; i < info.numNodes(); i++) {          DatanodeDescriptor dn=info.getDatanode(i);          sb.append(dn + ", ");          if (!dn.getDatanodeUuid().equals(uuid)) {
private boolean verifyOpenFilesListing(String message,HashSet<Path> closedFileSet,HashMap<Path,FSDataOutputStream> openFilesMap,ByteArrayOutputStream out,int expOpenFilesListSize){  final String outStr=scanIntoString(out);
  final CountDownLatch decomStarted=new CountDownLatch(0);  Thread decomTh=new Thread(){    public void run(){      try {        decomStarted.countDown();        decommissionNode(0,decommisionNodes,AdminStates.DECOMMISSIONED);      } catch (      Exception e) {        LOG.error("Exception while decommissioning",e);        Assert.fail("Shouldn't throw exception!");      }    }  };  int deadDecommissioned=fsn.getNumDecomDeadDataNodes();  int liveDecommissioned=fsn.getNumDecomLiveDataNodes();  decomTh.start();  decomStarted.await(5,TimeUnit.SECONDS);  Thread.sleep(3000);
    public void run(){      try {        decomStarted.countDown();        decommissionNode(0,decommisionNodes,AdminStates.DECOMMISSIONED);      } catch (      Exception e) {        LOG.error("Exception while decommissioning",e);        Assert.fail("Shouldn't throw exception!");      }    }  };  int deadDecommissioned=fsn.getNumDecomDeadDataNodes();  int liveDecommissioned=fsn.getNumDecomLiveDataNodes();  decomTh.start();  decomStarted.await(5,TimeUnit.SECONDS);  Thread.sleep(3000);  for (  DataNodeProperties dnp : stoppedDns) {    cluster.restartDataNode(dnp);
  int writeBytes=cellSize * dataBlocks;  writeStripedFile(dfs,ecFile,writeBytes);  Assert.assertEquals(0,bm.numOfUnderReplicatedBlocks());  FileChecksum fileChecksum1=dfs.getFileChecksum(ecFile,writeBytes);  final List<DatanodeInfo> decommisionNodes=new ArrayList<DatanodeInfo>();  LocatedBlock lb=dfs.getClient().getLocatedBlocks(ecFile.toString(),0).get(0);  DatanodeInfo[] dnLocs=lb.getLocations();  assertEquals(dataBlocks + parityBlocks,dnLocs.length);  int decommNodeIndex=1;  decommisionNodes.add(dnLocs[decommNodeIndex]);  decommissionNode(0,decommisionNodes,AdminStates.DECOMMISSIONED);  assertEquals(decommisionNodes.size(),fsn.getNumDecomLiveDataNodes());  assertNull(checkFile(dfs,ecFile,9,decommisionNodes,numDNs));  StripedFileTestUtil.checkData(dfs,ecFile,writeBytes,decommisionNodes,null,blockGroupSize);  FileChecksum fileChecksum2=dfs.getFileChecksum(ecFile,writeBytes);
  writeStripedFile(dfs,ecFile,writeBytes);  Assert.assertEquals(0,bm.numOfUnderReplicatedBlocks());  FileChecksum fileChecksum1=dfs.getFileChecksum(ecFile,writeBytes);  final List<DatanodeInfo> decommisionNodes=new ArrayList<DatanodeInfo>();  LocatedBlock lb=dfs.getClient().getLocatedBlocks(ecFile.toString(),0).get(0);  DatanodeInfo[] dnLocs=lb.getLocations();  assertEquals(dataBlocks + parityBlocks,dnLocs.length);  int decommNodeIndex=1;  decommisionNodes.add(dnLocs[decommNodeIndex]);  decommissionNode(0,decommisionNodes,AdminStates.DECOMMISSIONED);  assertEquals(decommisionNodes.size(),fsn.getNumDecomLiveDataNodes());  assertNull(checkFile(dfs,ecFile,9,decommisionNodes,numDNs));  StripedFileTestUtil.checkData(dfs,ecFile,writeBytes,decommisionNodes,null,blockGroupSize);  FileChecksum fileChecksum2=dfs.getFileChecksum(ecFile,writeBytes);  LOG.info("fileChecksum1:" + fileChecksum1);
private void waitNodeState(DatanodeInfo node,AdminStates state){  boolean done=state == node.getAdminState();  while (!done) {
private static String checkFile(FileSystem fileSys,Path name,int repl,List<DatanodeInfo> decommissionedNodes,int numDatanodes) throws IOException {  boolean isNodeDown=decommissionedNodes.size() > 0;  assertTrue("Not HDFS:" + fileSys.getUri(),fileSys instanceof DistributedFileSystem);  HdfsDataInputStream dis=(HdfsDataInputStream)fileSys.open(name);  Collection<LocatedBlock> dinfo=dis.getAllBlocks();  for (  LocatedBlock blk : dinfo) {    int hasdown=0;    DatanodeInfo[] nodes=blk.getLocations();    for (int j=0; j < nodes.length; j++) {
  final ExecutorService threadPool=HadoopExecutors.newFixedThreadPool(numThreads);  try {    final CountDownLatch allExecutorThreadsReady=new CountDownLatch(numThreads);    final CountDownLatch startBlocker=new CountDownLatch(1);    final CountDownLatch allDone=new CountDownLatch(numThreads);    final AtomicReference<Throwable> childError=new AtomicReference<>();    for (int i=0; i < numThreads; i++) {      threadPool.submit(new Runnable(){        @Override public void run(){          allExecutorThreadsReady.countDown();          try {            startBlocker.await();            final FileSystem fs=cluster.getFileSystem();            fs.mkdirs(new Path("/testStatisticsParallelChild"));          } catch (          Throwable t) {
            childError.compareAndSet(null,t);          } finally {            allDone.countDown();          }        }      });    }    final long oldMkdirOpCount=getOpStatistics(OpType.MKDIRS);    allExecutorThreadsReady.await();    startBlocker.countDown();    allDone.await();    assertNull("Child failed with exception " + childError.get(),childError.get());    checkStatistics(fs,0,numThreads,0);    checkOpStatistics(OpType.MKDIRS,numThreads + oldMkdirOpCount);    for (Iterator<LongStatistic> opCountIter=FileSystem.getGlobalStorageStatistics().get(DFSOpsCountStatistics.NAME).getLongStatistics(); opCountIter.hasNext(); ) {      final LongStatistic opCount=opCountIter.next();      if (OpType.MKDIRS.getSymbol().equals(opCount.getName())) {        assertEquals("Unexpected op count from iterator!",numThreads + oldMkdirOpCount,opCount.getValue());
@Test public void testSkewedRack1() throws Exception {  final int dataUnits=ecPolicy.getNumDataUnits();  final int parityUnits=ecPolicy.getNumParityUnits();  setupCluster(dataUnits + parityUnits,2,1);  final int filesize=ecPolicy.getNumDataUnits() * ecPolicy.getCellSize();  byte[] contents=new byte[filesize];  final Path path=new Path("/testfile");
@Test public void testSkewedRack2() throws Exception {  final int dataUnits=ecPolicy.getNumDataUnits();  final int parityUnits=ecPolicy.getNumParityUnits();  setupCluster(dataUnits + parityUnits * 2,dataUnits,dataUnits - 1);  final int filesize=ecPolicy.getNumDataUnits() * ecPolicy.getCellSize();  byte[] contents=new byte[filesize];  final Path path=new Path("/testfile");
@Test public void testSkewedRack3() throws Exception {  final int dataUnits=ecPolicy.getNumDataUnits();  final int parityUnits=ecPolicy.getNumParityUnits();  int numRacks=dataUnits - parityUnits + 2;  setupCluster(dataUnits + parityUnits * 4,numRacks,dataUnits - parityUnits);  final int filesize=ecPolicy.getNumDataUnits() * ecPolicy.getCellSize();  byte[] contents=new byte[filesize];  for (int i=0; i < 10; ++i) {    final Path path=new Path("/testfile" + i);
    create.close();    DFSTestUtil.waitReplication(fileSystem,f,(short)2);    LocatedBlocks lbs=fileSystem.dfs.getNamenode().getBlockLocations("/testAppend",0,Long.MAX_VALUE);    List<DataNode> dnsOfCluster=cluster.getDataNodes();    DatanodeInfo[] dnsWithLocations=lbs.getLastLocatedBlock().getLocations();    for (    DataNode dn : dnsOfCluster) {      for (      DatanodeInfo loc : dnsWithLocations) {        if (dn.getDatanodeId().equals(loc)) {          dn.shutdown();          DFSTestUtil.waitForDatanodeDeath(dn);        }      }    }    DFSTestUtil.waitReplication(fileSystem,f,(short)0);    try {      fileSystem.append(f);      fail("Append should fail because insufficient locations");    } catch (    IOException e) {
private void testStripedFileChecksum(int range1,int range2) throws Exception {  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,range1,false);  FileChecksum stripedFileChecksum2=getFileChecksum(stripedFile2,range1,false);  FileChecksum stripedFileChecksum3=getFileChecksum(stripedFile2,range2,false);
private void testStripedFileChecksum(int range1,int range2) throws Exception {  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,range1,false);  FileChecksum stripedFileChecksum2=getFileChecksum(stripedFile2,range1,false);  FileChecksum stripedFileChecksum3=getFileChecksum(stripedFile2,range2,false);  LOG.info("stripedFileChecksum1:" + stripedFileChecksum1);
private void testStripedFileChecksum(int range1,int range2) throws Exception {  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,range1,false);  FileChecksum stripedFileChecksum2=getFileChecksum(stripedFile2,range1,false);  FileChecksum stripedFileChecksum3=getFileChecksum(stripedFile2,range2,false);  LOG.info("stripedFileChecksum1:" + stripedFileChecksum1);  LOG.info("stripedFileChecksum2:" + stripedFileChecksum2);
@Test(timeout=90000) public void testStripedFileChecksumWithMissedDataBlocks1() throws Exception {  prepareTestFiles(fileSize,new String[]{stripedFile1});  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,fileSize,false);  FileChecksum stripedFileChecksumRecon=getFileChecksum(stripedFile1,fileSize,true);
@Test(timeout=90000) public void testStripedFileChecksumWithMissedDataBlocks1() throws Exception {  prepareTestFiles(fileSize,new String[]{stripedFile1});  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,fileSize,false);  FileChecksum stripedFileChecksumRecon=getFileChecksum(stripedFile1,fileSize,true);  LOG.info("stripedFileChecksum1:" + stripedFileChecksum1);
@Test(timeout=90000) public void testStripedFileChecksumWithMissedDataBlocks2() throws Exception {  prepareTestFiles(fileSize,new String[]{stripedFile1,stripedFile2});  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,-1,false);  FileChecksum stripedFileChecksum2=getFileChecksum(stripedFile2,-1,false);  FileChecksum stripedFileChecksum2Recon=getFileChecksum(stripedFile2,-1,true);
@Test(timeout=90000) public void testStripedFileChecksumWithMissedDataBlocks2() throws Exception {  prepareTestFiles(fileSize,new String[]{stripedFile1,stripedFile2});  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,-1,false);  FileChecksum stripedFileChecksum2=getFileChecksum(stripedFile2,-1,false);  FileChecksum stripedFileChecksum2Recon=getFileChecksum(stripedFile2,-1,true);  LOG.info("stripedFileChecksum1:" + stripedFileChecksum1);
@Test(timeout=90000) public void testStripedFileChecksumWithMissedDataBlocks2() throws Exception {  prepareTestFiles(fileSize,new String[]{stripedFile1,stripedFile2});  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile1,-1,false);  FileChecksum stripedFileChecksum2=getFileChecksum(stripedFile2,-1,false);  FileChecksum stripedFileChecksum2Recon=getFileChecksum(stripedFile2,-1,true);  LOG.info("stripedFileChecksum1:" + stripedFileChecksum1);  LOG.info("stripedFileChecksum2:" + stripedFileChecksum1);
private void testStripedFileChecksumWithMissedDataBlocksRangeQuery(String stripedFile,int requestedLen) throws Exception {
private void testStripedFileChecksumWithMissedDataBlocksRangeQuery(String stripedFile,int requestedLen) throws Exception {  LOG.info("Checksum file:{}, requested length:{}",stripedFile,requestedLen);  prepareTestFiles(fileSize,new String[]{stripedFile});  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile,requestedLen,false);  FileChecksum stripedFileChecksumRecon=getFileChecksum(stripedFile,requestedLen,true);
private void testStripedFileChecksumWithMissedDataBlocksRangeQuery(String stripedFile,int requestedLen) throws Exception {  LOG.info("Checksum file:{}, requested length:{}",stripedFile,requestedLen);  prepareTestFiles(fileSize,new String[]{stripedFile});  FileChecksum stripedFileChecksum1=getFileChecksum(stripedFile,requestedLen,false);  FileChecksum stripedFileChecksumRecon=getFileChecksum(stripedFile,requestedLen,true);  LOG.info("stripedFileChecksum1:" + stripedFileChecksum1);
    @Override public void run(){      try {        FSDataOutputStream outputStream=fileSystem.create(file);        if (syncType == SyncType.APPEND) {          outputStream.close();          outputStream=fileSystem.append(file);        }        try {          for (int i=0; !error.get() && i < numWrites; i++) {            final byte[] writeBuf=DFSTestUtil.generateSequentialBytes(i * writeSize,writeSize);            outputStream.write(writeBuf);            if (syncType == SyncType.SYNC) {              outputStream.hflush();            }            writerStarted.set(true);          }        } catch (        IOException e) {          error.set(true);
          outputStream=fileSystem.append(file);        }        try {          for (int i=0; !error.get() && i < numWrites; i++) {            final byte[] writeBuf=DFSTestUtil.generateSequentialBytes(i * writeSize,writeSize);            outputStream.write(writeBuf);            if (syncType == SyncType.SYNC) {              outputStream.hflush();            }            writerStarted.set(true);          }        } catch (        IOException e) {          error.set(true);          LOG.error("error writing to file",e);        } finally {          outputStream.close();        }        writerDone.set(true);      } catch (      Exception e) {
 catch (        IOException e) {          error.set(true);          LOG.error("error writing to file",e);        } finally {          outputStream.close();        }        writerDone.set(true);      } catch (      Exception e) {        LOG.error("error in writer",e);        throw new RuntimeException(e);      }    }  });  Thread tailer=new Thread(new Runnable(){    @Override public void run(){      try {        long startPos=0;        while (!writerDone.get() && !error.get()) {
        }        writerDone.set(true);      } catch (      Exception e) {        LOG.error("error in writer",e);        throw new RuntimeException(e);      }    }  });  Thread tailer=new Thread(new Runnable(){    @Override public void run(){      try {        long startPos=0;        while (!writerDone.get() && !error.get()) {          if (writerStarted.get()) {            try {              startPos=tailFile(file,startPos);            } catch (            IOException e) {              LOG.error(String.format("error tailing file %s",file),e);
private boolean validateSequentialBytes(byte[] buf,int startPos,int len){  for (int i=0; i < len; i++) {    int expected=(i + startPos) % 127;    if (buf[i] % 127 != expected) {
private long tailFile(Path file,long startPos) throws IOException {  long numRead=0;  FSDataInputStream inputStream=fileSystem.open(file);  inputStream.seek(startPos);  int len=4 * 1024;  byte[] buf=new byte[len];  int read;  while ((read=inputStream.read(buf)) > -1) {
private long tailFile(Path file,long startPos) throws IOException {  long numRead=0;  FSDataInputStream inputStream=fileSystem.open(file);  inputStream.seek(startPos);  int len=4 * 1024;  byte[] buf=new byte[len];  int read;  while ((read=inputStream.read(buf)) > -1) {    LOG.info(String.format("read %d bytes",read));    if (!validateSequentialBytes(buf,(int)(startPos + numRead),read)) {
@Test public void testFileCorruption() throws Exception {  MiniDFSCluster cluster=null;  DFSTestUtil util=new DFSTestUtil.Builder().setName("TestFileCorruption").setNumFiles(20).build();  try {    Configuration conf=new HdfsConfiguration();    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();    FileSystem fs=cluster.getFileSystem();    util.createFiles(fs,"/srcdat");    String bpid=cluster.getNamesystem().getBlockPoolId();    DataNode dn=cluster.getDataNodes().get(2);    Map<DatanodeStorage,BlockListAsLongs> blockReports=dn.getFSDataset().getBlockReports(bpid);    assertTrue("Blocks do not exist on data-dir",!blockReports.isEmpty());    for (    BlockListAsLongs report : blockReports.values()) {      for (      BlockReportReplica brr : report) {
@Test(timeout=300000) public void testSecondaryNodePorts() throws Exception {  NameNode nn=null;  try {    nn=startNameNode();    Configuration conf2=new HdfsConfiguration(config);    conf2.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,config.get(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY));
@Test(timeout=300000) public void testSecondaryNodePorts() throws Exception {  NameNode nn=null;  try {    nn=startNameNode();    Configuration conf2=new HdfsConfiguration(config);    conf2.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,config.get(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY));    LOG.info("= Starting 1 on: " + conf2.get(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY));    boolean started=canStartSecondaryNode(conf2);    assertFalse(started);    conf2.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,THIS_HOST);
@Test(timeout=300000) public void testBackupNodePorts() throws Exception {  NameNode nn=null;  try {    nn=startNameNode();    Configuration backup_config=new HdfsConfiguration(config);    backup_config.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_ADDRESS_KEY,THIS_HOST);    backup_config.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY,backup_config.get(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY));
@Test(timeout=300000) public void testBackupNodePorts() throws Exception {  NameNode nn=null;  try {    nn=startNameNode();    Configuration backup_config=new HdfsConfiguration(config);    backup_config.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_ADDRESS_KEY,THIS_HOST);    backup_config.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY,backup_config.get(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY));    LOG.info("= Starting 1 on: " + backup_config.get(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY));    assertFalse("Backup started on same port as Namenode",canStartBackupNode(backup_config));    backup_config.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_ADDRESS_KEY,THIS_HOST);    backup_config.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY,THIS_HOST);
private void waitForBlockReplication(String filename,ClientProtocol namenode,int expected,long maxWaitSec) throws IOException {  long start=Time.monotonicNow();
private void waitForBlockReplication(String filename,ClientProtocol namenode,int expected,long maxWaitSec) throws IOException {  long start=Time.monotonicNow();  LOG.info("Checking for block replication for " + filename);  LocatedBlocks blocks=namenode.getBlockLocations(filename,0,Long.MAX_VALUE);  assertEquals(numBlocks,blocks.locatedBlockCount());  for (int i=0; i < numBlocks; ++i) {    LOG.info("Checking for block:" + (i + 1));    while (true) {      blocks=namenode.getBlockLocations(filename,0,Long.MAX_VALUE);      assertEquals(numBlocks,blocks.locatedBlockCount());      LocatedBlock block=blocks.get(i);      int actual=block.getLocations().length;      if (actual == expected) {
    waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,20);    List<Map<DatanodeStorage,BlockListAsLongs>> blocksList=cluster.getAllBlockReports(bpid);    cluster.shutdown();    cluster=null;    LOG.info("Restarting minicluster");    conf=new HdfsConfiguration();    SimulatedFSDataset.setFactory(conf);    conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,"0.0f");    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes * 2).format(false).build();    cluster.waitActive();    Set<Block> uniqueBlocks=new HashSet<Block>();    for (    Map<DatanodeStorage,BlockListAsLongs> map : blocksList) {      for (      BlockListAsLongs blockList : map.values()) {        for (        Block b : blockList) {          uniqueBlocks.add(new Block(b));
static FSDataOutputStream createFile(FileSystem fileSys,Path name,int repl,final long blockSize) throws IOException {  FSDataOutputStream stm=fileSys.create(name,true,fileSys.getConf().getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY,4096),(short)repl,blockSize);
  long bytesToRead=fileSize;  byte[] compb=new byte[readSize];  if (verifyData) {    for (int j=0; j < readSize; j++) {      compb[j]=pattern[j % pattern.length];    }  }  FSDataInputStream stm=fs.open(name);  while (bytesToRead > 0) {    int thisread=(int)Math.min(readSize,bytesToRead);    stm.readFully(b,0,thisread);    if (verifyData) {      if (thisread == readSize) {        assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead),Arrays.equals(b,compb));      } else {        for (int k=0; k < thisread; k++) {          assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead),b[k] == compb[k]);
  byte[] compb=new byte[readSize];  if (verifyData) {    for (int j=0; j < readSize; j++) {      compb[j]=pattern[j % pattern.length];    }  }  FSDataInputStream stm=fs.open(name);  while (bytesToRead > 0) {    int thisread=(int)Math.min(readSize,bytesToRead);    stm.readFully(b,0,thisread);    if (verifyData) {      if (thisread == readSize) {        assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead),Arrays.equals(b,compb));      } else {        for (int k=0; k < thisread; k++) {          assertTrue("file is corrupted at or after byte " + (fileSize - bytesToRead),b[k] == compb[k]);        }      }    }    LOG.debug("Before update: to read: " + bytesToRead + "; read already: "+ thisread);
public void runTest(final long blockSize) throws IOException {  final long fileSize=blockSize + 1L;  Configuration conf=new Configuration();  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();  FileSystem fs=cluster.getFileSystem();  try {    Path file1=new Path("/tmp/TestLargeBlock",blockSize + ".dat");    FSDataOutputStream stm=createFile(fs,file1,1,blockSize);
public void runTest(final long blockSize) throws IOException {  final long fileSize=blockSize + 1L;  Configuration conf=new Configuration();  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();  FileSystem fs=cluster.getFileSystem();  try {    Path file1=new Path("/tmp/TestLargeBlock",blockSize + ".dat");    FSDataOutputStream stm=createFile(fs,file1,1,blockSize);    LOG.info("File " + file1 + " created with file size "+ fileSize+ " blocksize "+ blockSize);    assertTrue(file1 + " should be a file",fs.getFileStatus(file1).isFile());    writeFile(stm,fileSize);
public void runTest(final long blockSize) throws IOException {  final long fileSize=blockSize + 1L;  Configuration conf=new Configuration();  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();  FileSystem fs=cluster.getFileSystem();  try {    Path file1=new Path("/tmp/TestLargeBlock",blockSize + ".dat");    FSDataOutputStream stm=createFile(fs,file1,1,blockSize);    LOG.info("File " + file1 + " created with file size "+ fileSize+ " blocksize "+ blockSize);    assertTrue(file1 + " should be a file",fs.getFileStatus(file1).isFile());    writeFile(stm,fileSize);    LOG.info("File " + file1 + " written to.");    stm.close();
      dfs.renewLease();    } catch (    IOException e) {    }    try {      d_out.write(buf,0,1024);      LOG.info("Write worked beyond the soft limit as expected.");    } catch (    IOException e) {      Assert.fail("Write failed.");    }    long hardlimit=conf.getLong(DFSConfigKeys.DFS_LEASE_HARDLIMIT_KEY,DFSConfigKeys.DFS_LEASE_HARDLIMIT_DEFAULT) * 1000;    dfs.lastLeaseRenewal=Time.monotonicNow() - hardlimit - 1000;    dfs.renewLease();    try {      d_out.write(buf,0,1024);      d_out.close();      Assert.fail("Write did not fail even after the fatal lease renewal failure");    } catch (    IOException e) {
    }    long hardlimit=conf.getLong(DFSConfigKeys.DFS_LEASE_HARDLIMIT_KEY,DFSConfigKeys.DFS_LEASE_HARDLIMIT_DEFAULT) * 1000;    dfs.lastLeaseRenewal=Time.monotonicNow() - hardlimit - 1000;    dfs.renewLease();    try {      d_out.write(buf,0,1024);      d_out.close();      Assert.fail("Write did not fail even after the fatal lease renewal failure");    } catch (    IOException e) {      LOG.info("Write failed as expected. ",e);    }    Thread.sleep(1000);    Assert.assertTrue(originalRenewer.isEmpty());    doNothing().when(spyNN).renewLease(anyString());    try {      int num=c_in.read(buf,0,1);      if (num != 1) {
      Assert.fail("Write did not fail even after the fatal lease renewal failure");    } catch (    IOException e) {      LOG.info("Write failed as expected. ",e);    }    Thread.sleep(1000);    Assert.assertTrue(originalRenewer.isEmpty());    doNothing().when(spyNN).renewLease(anyString());    try {      int num=c_in.read(buf,0,1);      if (num != 1) {        Assert.fail("Failed to read 1 byte");      }      c_in.close();    } catch (    IOException e) {      LOG.error("Read failed with ",e);      Assert.fail("Read after lease renewal failure failed");    }    try {
  for (  DataNode dn : cluster.getDataNodes()) {    DataNodeTestUtils.setHeartbeatsDisabledForTests(dn,false);  }  cluster.waitActive();  cluster.setLeasePeriod(LONG_LEASE_PERIOD,SHORT_LEASE_PERIOD);  LocatedBlocks locatedBlocks;  do {    Thread.sleep(SHORT_LEASE_PERIOD);    locatedBlocks=dfs.dfs.getLocatedBlocks(fileStr,0L,size);  } while (locatedBlocks.isUnderConstruction());  assertEquals(size,locatedBlocks.getFileLength());  try {    stm.write('b');    stm.hflush();    fail("Should not be able to flush after we've lost the lease");  } catch (  IOException e) {
private void writePartialBlocks(int[] blockLengths) throws Exception {  final FSDataOutputStream out=dfs.create(p);  final DFSStripedOutputStream stripedOut=(DFSStripedOutputStream)out.getWrappedStream();  int[] posToKill=getPosToKill(blockLengths);  int checkingPos=nextCheckingPos(posToKill,0);  Set<Integer> stoppedStreamerIndexes=new HashSet<>();  try {    for (int pos=0; pos < testFileLength; pos++) {      out.write(StripedFileTestUtil.getByte(pos));      if (pos == checkingPos) {        for (        int index : getIndexToStop(posToKill,pos)) {          out.flush();          stripedOut.enqueueAllCurrentPackets();
          stripedOut.enqueueAllCurrentPackets();          LOG.info("Stopping block stream idx {} at file offset {} block " + "length {}",index,pos,blockLengths[index]);          StripedDataStreamer s=stripedOut.getStripedDataStreamer(index);          waitStreamerAllAcked(s);          waitByteSent(s,blockLengths[index]);          stopBlockStream(s);          stoppedStreamerIndexes.add(index);        }        checkingPos=nextCheckingPos(posToKill,pos);      }    }  }  finally {    out.flush();    stripedOut.enqueueAllCurrentPackets();    for (int i=0; i < blockLengths.length; i++) {      if (stoppedStreamerIndexes.contains(i)) {        continue;      }      StripedDataStreamer s=stripedOut.getStripedDataStreamer(i);
private void testFileBlockReplicationImpl(int maintenanceMinRepl,int numDataNodes,int numNewDataNodes,int fileBlockRepl) throws Exception {  setup();
private void testFileBlockReplicationImpl(int maintenanceMinRepl,int numDataNodes,int numNewDataNodes,int fileBlockRepl) throws Exception {  setup();  LOG.info("Starting testLargerMinMaintenanceReplication - maintMinRepl: " + maintenanceMinRepl + ", numDNs: "+ numDataNodes+ ", numNewDNs: "+ numNewDataNodes+ ", fileRepl: "+ fileBlockRepl);
private void testChangeReplicationFactor(int oldFactor,int newFactor,int expectedLiveReplicas) throws IOException {  setup();
static String checkFile(FSNamesystem ns,FileSystem fileSys,Path name,int repl,DatanodeInfo expectedExcludedNode,DatanodeInfo expectedMaintenanceNode) throws IOException {  assertTrue("Not HDFS:" + fileSys.getUri(),fileSys instanceof DistributedFileSystem);  HdfsDataInputStream dis=(HdfsDataInputStream)fileSys.open(name);  BlockManager bm=ns.getBlockManager();  Collection<LocatedBlock> dinfo=dis.getAllBlocks();  String output;  for (  LocatedBlock blk : dinfo) {    DatanodeInfo[] nodes=blk.getLocations();    for (int j=0; j < nodes.length; j++) {      if (expectedExcludedNode != null && nodes[j].equals(expectedExcludedNode)) {        output="For block " + blk.getBlock() + " replica on "+ nodes[j]+ " found in LocatedBlock.";
  assertTrue("Not HDFS:" + fileSys.getUri(),fileSys instanceof DistributedFileSystem);  HdfsDataInputStream dis=(HdfsDataInputStream)fileSys.open(name);  BlockManager bm=ns.getBlockManager();  Collection<LocatedBlock> dinfo=dis.getAllBlocks();  String output;  for (  LocatedBlock blk : dinfo) {    DatanodeInfo[] nodes=blk.getLocations();    for (int j=0; j < nodes.length; j++) {      if (expectedExcludedNode != null && nodes[j].equals(expectedExcludedNode)) {        output="For block " + blk.getBlock() + " replica on "+ nodes[j]+ " found in LocatedBlock.";        LOG.info(output);        return output;      } else {        if (nodes[j].isInMaintenance()) {          output="For block " + blk.getBlock() + " replica on "+ nodes[j]+ " which is in maintenance state.";
    if (repl != nodes.length) {      output="Wrong number of replicas for block " + blk.getBlock() + ": expected "+ repl+ ", got "+ nodes.length+ " ,";      for (int j=0; j < nodes.length; j++) {        output+=nodes[j] + ",";      }      output+="pending block # " + ns.getPendingReplicationBlocks() + " ,";      output+="under replicated # " + ns.getUnderReplicatedBlocks() + " ,";      if (expectedExcludedNode != null) {        output+="excluded node " + expectedExcludedNode;      }      LOG.info(output);      return output;    }    Iterator<DatanodeStorageInfo> storageInfoIter=bm.getStorages(blk.getBlock().getLocalBlock()).iterator();    List<DatanodeInfo> maintenanceNodes=new ArrayList<>();    while (storageInfoIter.hasNext()) {      DatanodeInfo node=storageInfoIter.next().getDatanodeDescriptor();      if (node.isMaintenance()) {
      output+="pending block # " + ns.getPendingReplicationBlocks() + " ,";      output+="under replicated # " + ns.getUnderReplicatedBlocks() + " ,";      if (expectedExcludedNode != null) {        output+="excluded node " + expectedExcludedNode;      }      LOG.info(output);      return output;    }    Iterator<DatanodeStorageInfo> storageInfoIter=bm.getStorages(blk.getBlock().getLocalBlock()).iterator();    List<DatanodeInfo> maintenanceNodes=new ArrayList<>();    while (storageInfoIter.hasNext()) {      DatanodeInfo node=storageInfoIter.next().getDatanodeDescriptor();      if (node.isMaintenance()) {        maintenanceNodes.add(node);      }    }    if (expectedMaintenanceNode != null) {      if (!maintenanceNodes.contains(expectedMaintenanceNode)) {        output="No maintenance replica on " + expectedMaintenanceNode;
public static void setupCluster(int replicationFactor,HdfsConfiguration conf) throws Exception {  util=new BlockReaderTestUtil(replicationFactor,conf);  dfsClient=util.getDFSClient();  long seed=Time.now();
@Test public void pipeline_01() throws IOException {  final String METHOD_NAME=GenericTestUtils.getMethodName();  if (LOG.isDebugEnabled()) {
static byte[] writeData(final FSDataOutputStream out,final int length) throws IOException {  int bytesToWrite=length;  byte[] ret=new byte[bytesToWrite];  byte[] toWrite=new byte[1024];  int written=0;  Random rb=new Random(rand.nextLong());  while (bytesToWrite > 0) {    rb.nextBytes(toWrite);    int bytesToWriteNext=(1024 < bytesToWrite) ? 1024 : bytesToWrite;    out.write(toWrite,0,bytesToWriteNext);    System.arraycopy(toWrite,0,ret,(ret.length - bytesToWrite),bytesToWriteNext);    written+=bytesToWriteNext;    if (LOG.isDebugEnabled()) {
  Path testFile=new Path(dir,"file");  FSDataOutputStream stream=dfs.create(testFile);  final LeaseRenewer leaseRenewer=dfs.getClient().getLeaseRenewer();  stream.write("whatever".getBytes());  try {    stream.hflush();    fail("flush should fail");  } catch (  DSQuotaExceededException expected) {  }  try {    stream.close();    fail("close should fail too");  } catch (  DSQuotaExceededException expected) {  }  GenericTestUtils.setLogLevel(LeaseRenewer.LOG,Level.TRACE);  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){
@Test public void testReportBadBlock() throws IOException {  final Path file=new Path("/corrupted");  final int length=10;  final byte[] bytes=StripedFileTestUtil.generateBytes(length);  DFSTestUtil.writeFile(dfs,file,bytes);  int dnIndex=ReadStripedFileWithDecodingHelper.findFirstDataNode(cluster,dfs,file,CELL_SIZE * NUM_DATA_UNITS);  Assert.assertNotEquals(-1,dnIndex);  LocatedStripedBlock slb=(LocatedStripedBlock)dfs.getClient().getLocatedBlocks(file.toString(),0,CELL_SIZE * NUM_DATA_UNITS).get(0);  final LocatedBlock[] blks=StripedBlockUtil.parseStripedBlockGroup(slb,CELL_SIZE,NUM_DATA_UNITS,NUM_PARITY_UNITS);  File storageDir=cluster.getInstanceStorageDir(dnIndex,0);  File blkFile=MiniDFSCluster.getBlockFile(storageDir,blks[0].getBlock());  Assert.assertTrue("Block file does not exist",blkFile.exists());
private void readFileWithMissingBlocks(Path srcPath,int fileLength,int missingDataNum,int missingParityNum,byte[] expected) throws Exception {
private void stopDataNodes(BlockLocation[] locs,int[] datanodes) throws IOException {  if (locs != null && locs.length > 0) {    for (    int failedDNIdx : datanodes) {      String name=(locs[0].getNames())[failedDNIdx];      for (      DataNode dn : cluster.getDataNodes()) {        int port=dn.getXferPort();        if (name.contains(Integer.toString(port))) {          dn.shutdown();          cluster.setDataNodeDead(dn.getDatanodeId());
private void assertFileBlocksReconstruction(String fileName,int fileLen,ReconstructionType type,int toRecoverBlockNum) throws Exception {  if (toRecoverBlockNum < 1 || toRecoverBlockNum > parityBlkNum) {    Assert.fail("toRecoverBlockNum should be between 1 ~ " + parityBlkNum);  }  assertTrue("File length must be positive.",fileLen > 0);  Path file=new Path(fileName);  writeFile(fs,fileName,fileLen);  LocatedBlocks locatedBlocks=StripedFileTestUtil.getLocatedBlocks(file,fs);  assertEquals(locatedBlocks.getFileLength(),fileLen);  LocatedStripedBlock lastBlock=(LocatedStripedBlock)locatedBlocks.getLastLocatedBlock();  DatanodeInfo[] storageInfos=lastBlock.getLocations();  byte[] indices=lastBlock.getBlockIndices();  BitSet bitset=new BitSet(dnNum);  for (  DatanodeInfo storageInfo : storageInfos) {    bitset.set(dnMap.get(storageInfo));  }  int[] dead=generateDeadDnIndices(type,toRecoverBlockNum,indices);
  int[] deadDnIndices=new int[toRecoverBlockNum];  ExtendedBlock[] blocks=new ExtendedBlock[toRecoverBlockNum];  File[] replicas=new File[toRecoverBlockNum];  long[] replicaLengths=new long[toRecoverBlockNum];  File[] metadatas=new File[toRecoverBlockNum];  byte[][] replicaContents=new byte[toRecoverBlockNum][];  Map<ExtendedBlock,DataNode> errorMap=new HashMap<>(dead.length);  for (int i=0; i < toRecoverBlockNum; i++) {    dataDNs[i]=storageInfos[dead[i]];    deadDnIndices[i]=dnMap.get(dataDNs[i]);    blocks[i]=StripedBlockUtil.constructInternalBlock(lastBlock.getBlock(),cellSize,dataBlkNum,indices[dead[i]]);    errorMap.put(blocks[i],cluster.getDataNodes().get(deadDnIndices[i]));    replicas[i]=cluster.getBlockFile(deadDnIndices[i],blocks[i]);    replicaLengths[i]=replicas[i].length();    metadatas[i]=cluster.getBlockMetadataFile(deadDnIndices[i],blocks[i]);
static void sleepSeconds(final int waittime) throws InterruptedException {
static void sleepSeconds(final int waittime) throws InterruptedException {
      for (int k=0; k < racks.length; k++) {        if (topologyPaths[j].startsWith(racks[k])) {          found=true;          break;        }      }      assertTrue(found);    }  }  boolean isOnSameRack=true, isNotOnSameRack=true;  for (  LocatedBlock blk : locations.getLocatedBlocks()) {    DatanodeInfo[] datanodes=blk.getLocations();    if (datanodes.length <= 1)     break;    if (datanodes.length == 2) {      isNotOnSameRack=!(datanodes[0].getNetworkLocation().equals(datanodes[1].getNetworkLocation()));      break;    }    isOnSameRack=false;    isNotOnSameRack=false;    for (int i=0; i < datanodes.length - 1; i++) {
private void waitForBlockReplication(String filename,ClientProtocol namenode,int expected,long maxWaitSec,boolean isUnderConstruction,boolean noOverReplication) throws IOException {  long start=Time.monotonicNow();
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();    cluster.waitActive();    DFSClient dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);    OutputStream out=cluster.getFileSystem().create(testPath);    out.write(buffer);    out.close();    waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,-1);    ExtendedBlock block=dfsClient.getNamenode().getBlockLocations(testFile,0,Long.MAX_VALUE).get(0).getBlock();    List<MaterializedReplica> replicas=new ArrayList<>();    for (int dnIndex=0; dnIndex < 3; dnIndex++) {      replicas.add(cluster.getMaterializedReplica(dnIndex,block));    }    assertEquals(3,replicas.size());    cluster.shutdown();    int fileCount=0;    for (    MaterializedReplica replica : replicas) {
    OutputStream out=cluster.getFileSystem().create(testPath);    out.write(buffer);    out.close();    waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,-1);    ExtendedBlock block=dfsClient.getNamenode().getBlockLocations(testFile,0,Long.MAX_VALUE).get(0).getBlock();    List<MaterializedReplica> replicas=new ArrayList<>();    for (int dnIndex=0; dnIndex < 3; dnIndex++) {      replicas.add(cluster.getMaterializedReplica(dnIndex,block));    }    assertEquals(3,replicas.size());    cluster.shutdown();    int fileCount=0;    for (    MaterializedReplica replica : replicas) {      if (fileCount == 0) {        LOG.info("Deleting block " + replica);        replica.deleteData();
@Test(timeout=30000) public void testRollingUpgradeWithQJM() throws Exception {  String nnDirPrefix=MiniDFSCluster.getBaseDirectory() + "/nn/";  final File nn1Dir=new File(nnDirPrefix + "image1");  final File nn2Dir=new File(nnDirPrefix + "image2");
@Test(timeout=30000) public void testRollingUpgradeWithQJM() throws Exception {  String nnDirPrefix=MiniDFSCluster.getBaseDirectory() + "/nn/";  final File nn1Dir=new File(nnDirPrefix + "image1");  final File nn2Dir=new File(nnDirPrefix + "image2");  LOG.info("nn1Dir=" + nn1Dir);
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).manageNameDfsDirs(false).checkExitOnShutdown(false).build();    cluster.shutdown();  }  MiniDFSCluster cluster2=null;  try {    FileUtil.fullyDelete(nn2Dir);    FileUtil.copy(nn1Dir,FileSystem.getLocal(conf).getRaw(),new Path(nn2Dir.getAbsolutePath()),false,conf);    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(false).manageNameDfsDirs(false).checkExitOnShutdown(false).build();    final Path foo=new Path("/foo");    final Path bar=new Path("/bar");    final Path baz=new Path("/baz");    final RollingUpgradeInfo info1;{      final DistributedFileSystem dfs=cluster.getFileSystem();      dfs.mkdirs(foo);      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);      info1=dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);
  assertEquals("Safe mode is ON. The reported blocks 0 needs additional " + "14 blocks to reach the threshold 0.9990 of total blocks 15." + NEWLINE + "The minimum number of live datanodes is not required. "+ "Safe mode will be turned off automatically once the thresholds have "+ "been reached.",status);  assertFalse("Mis-replicated block queues should not be initialized " + "until threshold is crossed",NameNodeAdapter.safeModeInitializedReplQueues(nn));  LOG.info("Restarting one DataNode");  cluster.restartDataNode(dnprops.remove(0));  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      return getLongCounter("StorageBlockReportNumOps",getMetrics(NN_METRICS)) == cluster.getStoragesPerDatanode();    }  },10,10000);  final long safe=NameNodeAdapter.getSafeModeSafeBlocks(nn);  assertTrue("Expected first block report to make some blocks safe.",safe > 0);  assertTrue("Did not expect first block report to make all blocks safe.",safe < 15);  assertTrue(NameNodeAdapter.safeModeInitializedReplQueues(nn));  BlockManagerTestUtil.updateState(nn.getNamesystem().getBlockManager());  long underReplicatedBlocks=nn.getNamesystem().getUnderReplicatedBlocks();  while (underReplicatedBlocks != (15 - safe)) {
@Test public void testCreateZoneAfterAuthTokenExpiry() throws Exception {  final UserGroupInformation ugi=UserGroupInformation.loginUserFromKeytabAndReturnUGI(hdfsPrincipal,keytab);
@Test public void testCreateZoneAfterAuthTokenExpiry() throws Exception {  final UserGroupInformation ugi=UserGroupInformation.loginUserFromKeytabAndReturnUGI(hdfsPrincipal,keytab);  LOG.info("Created ugi: {} ",ugi);  ugi.doAs((PrivilegedExceptionAction<Object>)() -> {    final Path zone=new Path("/expire1");    fsWrapper.mkdir(zone,FsPermission.getDirDefault(),true);    dfsAdmin.createEncryptionZone(zone,testKey,NO_TRASH);    final Path zone1=new Path("/expire2");    fsWrapper.mkdir(zone1,FsPermission.getDirDefault(),true);    final long sleepInterval=(AUTH_TOKEN_VALIDITY + 1) * 1000;
@Test public void testWriteReadSeq() throws IOException {  useFCOption=false;  positionReadOption=false;  String fname=filenameOption;  long rdBeginPos=0;  int stat=testWriteAndRead(fname,WR_NTIMES,WR_CHUNK_SIZE,rdBeginPos);
    currentPosition=in.getPos();  }  if (verboseOption)   LOG.info("reader begin: position: " + pos + " ; currentOffset = "+ currentPosition+ " ; bufferSize ="+ buffer.length+ " ; Filename = "+ fname);  try {    while (byteLeftToRead > 0 && currentPosition < visibleLen) {      byteToReadThisRound=(int)(byteLeftToRead >= buffer.length ? buffer.length : byteLeftToRead);      if (positionReadOption) {        byteRead=in.read(currentPosition,buffer,0,byteToReadThisRound);      } else {        byteRead=in.read(buffer,0,byteToReadThisRound);      }      if (byteRead <= 0)       break;      chunkNumber++;      totalByteRead+=byteRead;      currentPosition+=byteRead;      byteLeftToRead-=byteRead;      if (verboseOption) {
  FSDataOutputStream out=null;  byte[] outBuffer=new byte[BUFFER_SIZE];  byte[] inBuffer=new byte[BUFFER_SIZE];  for (int i=0; i < BUFFER_SIZE; i++) {    outBuffer[i]=(byte)(i & 0x00ff);  }  try {    Path path=getFullyQualifiedPath(fname);    long fileLengthBeforeOpen=0;    if (ifExists(path)) {      if (truncateOption) {        out=useFCOption ? mfc.create(path,EnumSet.of(CreateFlag.OVERWRITE)) : mfs.create(path,truncateOption);        LOG.info("File already exists. File open with Truncate mode: " + path);      } else {        out=useFCOption ? mfc.create(path,EnumSet.of(CreateFlag.APPEND)) : mfs.append(path);        fileLengthBeforeOpen=getFileLengthFromNN(path);
      }    } else {      out=useFCOption ? mfc.create(path,EnumSet.of(CreateFlag.CREATE)) : mfs.create(path);    }    long totalByteWritten=fileLengthBeforeOpen;    long totalByteVisible=fileLengthBeforeOpen;    long totalByteWrittenButNotVisible=0;    boolean toFlush;    for (int i=0; i < loopN; i++) {      toFlush=(i % 2) == 0;      writeData(out,outBuffer,chunkSize);      totalByteWritten+=chunkSize;      if (toFlush) {        out.hflush();        totalByteVisible+=chunkSize + totalByteWrittenButNotVisible;        totalByteWrittenButNotVisible=0;      } else {
      writeData(out,outBuffer,chunkSize);      totalByteWritten+=chunkSize;      if (toFlush) {        out.hflush();        totalByteVisible+=chunkSize + totalByteWrittenButNotVisible;        totalByteWrittenButNotVisible=0;      } else {        totalByteWrittenButNotVisible+=chunkSize;      }      if (verboseOption) {        LOG.info("TestReadWrite - Written " + chunkSize + ". Total written = "+ totalByteWritten+ ". TotalByteVisible = "+ totalByteVisible+ " to file "+ fname);      }      byteVisibleToRead=readData(fname,inBuffer,totalByteVisible,readBeginPosition);      String readmsg="Written=" + totalByteWritten + " ; Expected Visible="+ totalByteVisible+ " ; Got Visible="+ byteVisibleToRead+ " of file "+ fname;      if (byteVisibleToRead >= totalByteVisible && byteVisibleToRead <= totalByteWritten) {        readmsg="pass: reader sees expected number of visible byte. " + readmsg + " [pass]";      } else {
      byteVisibleToRead=readData(fname,inBuffer,totalByteVisible,readBeginPosition);      String readmsg="Written=" + totalByteWritten + " ; Expected Visible="+ totalByteVisible+ " ; Got Visible="+ byteVisibleToRead+ " of file "+ fname;      if (byteVisibleToRead >= totalByteVisible && byteVisibleToRead <= totalByteWritten) {        readmsg="pass: reader sees expected number of visible byte. " + readmsg + " [pass]";      } else {        countOfFailures++;        readmsg="fail: reader see different number of visible byte. " + readmsg + " [fail]";        if (abortTestOnFailure) {          throw new IOException(readmsg);        }      }      LOG.info(readmsg);    }    writeData(out,outBuffer,chunkSize);    totalByteWritten+=chunkSize;    totalByteVisible+=chunkSize + totalByteWrittenButNotVisible;    totalByteWrittenButNotVisible+=0;    out.close();
private void testOneFileUsingDFSStripedInputStream(String src,int fileLength,boolean withDataNodeFailure) throws Exception {  final byte[] expected=StripedFileTestUtil.generateBytes(fileLength);  Path srcPath=new Path(src);  DFSTestUtil.writeFile(fs,srcPath,new String(expected));  StripedFileTestUtil.waitBlockGroupsReported(fs,src);  StripedFileTestUtil.verifyLength(fs,srcPath,fileLength);  if (withDataNodeFailure) {    int dnIndex=1;
private void writeFileWithDNFailure(int fileLength,int dataDNFailureNum,int parityDNFailureNum) throws IOException {  String fileType=fileLength < (blockSize * dataBlocks) ? "smallFile" : "largeFile";  String src="/dnFailure_" + dataDNFailureNum + "_"+ parityDNFailureNum+ "_"+ fileType;
  Runnable readerRunnable=new Runnable(){    @Override public void run(){      try {        List<LocatedBlock> locatedBlocks=cluster.getNameNode().getRpcServer().getBlockLocations(TEST_FILE,0,TEST_FILE_LEN).getLocatedBlocks();        LocatedBlock lblock=locatedBlocks.get(0);        BlockReader blockReader=null;        try {          blockReader=BlockReaderTestUtil.getBlockReader(cluster.getFileSystem(),lblock,0,TEST_FILE_LEN);          Assert.fail("expected getBlockReader to fail the first time.");        } catch (        Throwable t) {          Assert.assertTrue("expected to see 'TCP reads were disabled " + "for testing' in exception " + t,t.getMessage().contains("TCP reads were disabled for testing"));        } finally {          if (blockReader != null)           blockReader.close();        }        gotFailureLatch.countDown();        shouldRetryLatch.await();
        BlockReader blockReader=null;        try {          blockReader=BlockReaderTestUtil.getBlockReader(cluster.getFileSystem(),lblock,0,TEST_FILE_LEN);          Assert.fail("expected getBlockReader to fail the first time.");        } catch (        Throwable t) {          Assert.assertTrue("expected to see 'TCP reads were disabled " + "for testing' in exception " + t,t.getMessage().contains("TCP reads were disabled for testing"));        } finally {          if (blockReader != null)           blockReader.close();        }        gotFailureLatch.countDown();        shouldRetryLatch.await();        try {          blockReader=BlockReaderTestUtil.getBlockReader(cluster.getFileSystem(),lblock,0,TEST_FILE_LEN);        } catch (        Throwable t) {          LOG.error("error trying to retrieve a block reader " + "the second time.",t);          throw t;
      try {        while (true) {          BlockReader blockReader=null;          try {            blockReader=BlockReaderTestUtil.getBlockReader(cluster.getFileSystem(),lblock,0,TEST_FILE_LEN);            sem.release();            try {              blockReader.readAll(buf,0,TEST_FILE_LEN);            }  finally {              sem.acquireUninterruptibly();            }          } catch (          ClosedByInterruptException e) {            LOG.info("got the expected ClosedByInterruptException",e);            sem.release();            break;          } finally {
  printMemUsage("before test1");  totalStart=System.nanoTime();  totalTrials=0;  for (int i=0; i < OP_NUM; i++) {    StorageType type=StorageType.values()[i % StorageType.values().length];    localStart=System.nanoTime();    do {      totalTrials+=1;      node=cluster.chooseRandom("",excluded);      assertNotNull(node);      if (isType(node,type)) {        break;      }      excluded.add(node);    } while (true);    excluded.clear();
    } while (true);    excluded.clear();    localEnd=System.nanoTime();    records[i]=localEnd - localStart;  }  totalEnd=System.nanoTime();  totalMs=(totalEnd - totalStart) / NS_TO_MS;  LOG.info("total time: {} avg time: {} avg trials: {}",totalMs,totalMs / OP_NUM,(float)totalTrials / OP_NUM);  Thread.sleep(1000);  printMemUsage("after test1 before test2");  totalStart=System.nanoTime();  for (int i=0; i < OP_NUM; i++) {    StorageType type=StorageType.values()[i % StorageType.values().length];    localStart=System.nanoTime();    node=dfscluster.chooseRandomWithStorageType("",excluded,type);    assertNotNull(node);
  totalStart=System.nanoTime();  totalTrials=0;  for (int i=0; i < OP_NUM; i++) {    localStart=System.nanoTime();    do {      totalTrials+=1;      node=cluster.chooseRandom("",excluded);      assertNotNull(node);      if (isType(node,StorageType.ARCHIVE)) {        break;      }      excluded.add(node);    } while (true);    excluded.clear();    localEnd=System.nanoTime();    records[i]=localEnd - localStart;
 while (true);    excluded.clear();    localEnd=System.nanoTime();    records[i]=localEnd - localStart;  }  totalEnd=System.nanoTime();  totalMs=(totalEnd - totalStart) / NS_TO_MS;  LOG.info("total time: {} avg time: {} avg trials: {}",totalMs,totalMs / OP_NUM,(float)totalTrials / OP_NUM);  Thread.sleep(1000);  printMemUsage("after test1 before test2");  totalStart=System.nanoTime();  for (int i=0; i < OP_NUM; i++) {    localStart=System.nanoTime();    node=dfscluster.chooseRandomWithStorageType("",excluded,StorageType.ARCHIVE);    assertNotNull(node);    assertTrue(isType(node,StorageType.ARCHIVE));
  Thread.sleep(1000);  printMemUsage("before test1");  totalStart=System.nanoTime();  totalTrials=0;  for (int i=0; i < OP_NUM; i++) {    do {      totalTrials+=1;      node=cluster.chooseRandom("",excluded);      assertNotNull(node);      if (isType(node,StorageType.DISK)) {        break;      }      excluded.add(node);    } while (true);    excluded.clear();  }  totalEnd=System.nanoTime();
        break;      }      excluded.add(node);    } while (true);    excluded.clear();  }  totalEnd=System.nanoTime();  totalMs=(totalEnd - totalStart) / NS_TO_MS;  LOG.info("total time: {} avg time: {} avg trials: {}",totalMs,totalMs / OP_NUM,(float)totalTrials / OP_NUM);  Thread.sleep(1000);  printMemUsage("after test1 before test2");  totalStart=System.nanoTime();  for (int i=0; i < OP_NUM; i++) {    node=dfscluster.chooseRandomWithStorageType("",excluded,StorageType.DISK);    assertNotNull(node);    assertTrue(isType(node,StorageType.DISK));  }  totalEnd=System.nanoTime();
  printMemUsage("before test1");  totalStart=System.nanoTime();  totalTrials=0;  for (int i=0; i < OP_NUM; i++) {    localStart=System.nanoTime();    do {      totalTrials+=1;      node=cluster.chooseRandom("",excluded);      assertNotNull(node);      if (isType(node,StorageType.ARCHIVE)) {        break;      }      excluded.add(node);    } while (true);    excluded.clear();    localEnd=System.nanoTime();
    } while (true);    excluded.clear();    localEnd=System.nanoTime();    records[i]=localEnd - localStart;  }  totalEnd=System.nanoTime();  totalMs=(totalEnd - totalStart) / NS_TO_MS;  LOG.info("total time: {} avg time: {} avg trials: {}",totalMs,totalMs / OP_NUM,(float)totalTrials / OP_NUM);  Thread.sleep(1000);  printMemUsage("after test1 before test2");  totalStart=System.nanoTime();  for (int i=0; i < OP_NUM; i++) {    localStart=System.nanoTime();    node=dfscluster.chooseRandomWithStorageType("",excluded,StorageType.ARCHIVE);    assertNotNull(node);    assertTrue(isType(node,StorageType.ARCHIVE));
  printMemUsage("before test1");  totalStart=System.nanoTime();  totalTrials=0;  for (int i=0; i < OP_NUM; i++) {    localStart=System.nanoTime();    totalTrials+=1;    node=cluster.chooseRandom("",excluded);    assertNotNull(node);    if (!isType(node,StorageType.ARCHIVE)) {      totalTrials+=1;      excluded.add(node);      node=dfscluster.chooseRandomWithStorageType("",excluded,StorageType.ARCHIVE);    }    assertTrue(isType(node,StorageType.ARCHIVE));    excluded.clear();    localEnd=System.nanoTime();
private void printMemUsage(String message) throws Exception {  Runtime runtime=Runtime.getRuntime();  NumberFormat format=NumberFormat.getInstance();  StringBuilder sb=new StringBuilder();  sb.append(message);  long maxMemory=runtime.maxMemory();  long allocatedMemory=runtime.totalMemory();  long freeMemory=runtime.freeMemory();  sb.append("\nfree memory: " + format.format(freeMemory / 1024));  sb.append("\nallocated memory: " + format.format(allocatedMemory / 1024));  sb.append("\nmax memory: " + format.format(maxMemory / 1024));  sb.append("\ntotal free memory: " + format.format((freeMemory + (maxMemory - allocatedMemory)) / 1024));
        qjm.createNewUniqueEpoch();        assertEquals(i + 1,qjm.getLoggerSetForTests().getEpoch());      }  finally {        qjm.close();      }    }    long prevEpoch=5;    for (int i=0; i < 20; i++) {      long newEpoch=-1;      while (true) {        qjm=new QuorumJournalManager(conf,uri,FAKE_NSINFO,new FaultyLoggerFactory());        try {          qjm.createNewUniqueEpoch();          newEpoch=qjm.getLoggerSetForTests().getEpoch();          break;        } catch (        IOException ioe) {        } finally {
  MiniJournalCluster cluster=new MiniJournalCluster.Builder(conf).build();  cluster.waitActive();  QuorumJournalManager qjm=null;  long ret;  try {    qjm=createInjectableQJM(cluster);    qjm.format(FAKE_NSINFO,false);    doWorkload(cluster,qjm);    SortedSet<Integer> ipcCounts=Sets.newTreeSet();    for (    AsyncLogger l : qjm.getLoggerSetForTests().getLoggersForTests()) {      InvocationCountingChannel ch=(InvocationCountingChannel)l;      ch.waitForAllPendingCalls();      ipcCounts.add(ch.getRpcCount());    }    assertEquals(1,ipcCounts.size());    ret=ipcCounts.first();
@Test public void testRecoverAfterDoubleFailures() throws Exception {  final long MAX_IPC_NUMBER=determineMaxIpcNumber();  for (int failA=1; failA <= MAX_IPC_NUMBER; failA++) {    for (int failB=1; failB <= MAX_IPC_NUMBER; failB++) {      String injectionStr="(" + failA + ", "+ failB+ ")";
  for (int failA=1; failA <= MAX_IPC_NUMBER; failA++) {    for (int failB=1; failB <= MAX_IPC_NUMBER; failB++) {      String injectionStr="(" + failA + ", "+ failB+ ")";      LOG.info("\n\n-------------------------------------------\n" + "Beginning test, failing at " + injectionStr + "\n"+ "-------------------------------------------\n\n");      MiniJournalCluster cluster=new MiniJournalCluster.Builder(conf).build();      cluster.waitActive();      QuorumJournalManager qjm=null;      try {        qjm=createInjectableQJM(cluster);        qjm.format(FAKE_NSINFO,false);        List<AsyncLogger> loggers=qjm.getLoggerSetForTests().getLoggersForTests();        failIpcNumber(loggers.get(0),failA);        failIpcNumber(loggers.get(1),failB);        int lastAckedTxn=doWorkload(cluster,qjm);        if (lastAckedTxn < 6) {
    seed=userSpecifiedSeed;    GenericTestUtils.setLogLevel(ProtobufRpcEngine2.LOG,Level.ALL);  } else {    seed=new Random().nextLong();  }  LOG.info("Random seed: " + seed);  Random r=new Random(seed);  MiniJournalCluster cluster=new MiniJournalCluster.Builder(conf).build();  cluster.waitActive();  QuorumJournalManager qjmForInitialFormat=createInjectableQJM(cluster);  qjmForInitialFormat.format(FAKE_NSINFO,false);  qjmForInitialFormat.close();  try {    long txid=0;    long lastAcked=0;    for (int i=0; i < NUM_WRITER_ITERS; i++) {
      QuorumJournalManager qjm=createRandomFaultyQJM(cluster,r);      try {        long recovered;        try {          recovered=QJMTestUtil.recoverAndReturnLastTxn(qjm);        } catch (        Throwable t) {          LOG.info("Failed recovery",t);          checkException(t);          continue;        }        assertTrue("Recovered only up to txnid " + recovered + " but had gotten an ack for "+ lastAcked,recovered >= lastAcked);        txid=recovered + 1;        if (txid > 100 && i % 10 == 1) {          qjm.purgeLogsOlderThan(txid - 100);        }        Holder<Throwable> thrown=new Holder<Throwable>(null);        for (int j=0; j < SEGMENTS_PER_WRITER; j++) {
@Test(timeout=300000) public void testRpcBindHostKey() throws IOException {  LOG.info("Testing without " + DFS_JOURNALNODE_RPC_BIND_HOST_KEY);  jCluster=new MiniJournalCluster.Builder(conf).format(true).numJournalNodes(NUM_JN).build();  jn=jCluster.getJournalNode(0);  String address=getRpcServerAddress(jn);  assertThat("Bind address not expected to be wildcard by default.",address,not("/" + WILDCARD_ADDRESS));
@Test(timeout=300000) public void testHttpBindHostKey() throws IOException {  LOG.info("Testing without " + DFS_JOURNALNODE_HTTP_BIND_HOST_KEY);  conf.set(DFS_JOURNALNODE_HTTP_ADDRESS_KEY,LOCALHOST_SERVER_ADDRESS);  jCluster=new MiniJournalCluster.Builder(conf).format(true).numJournalNodes(NUM_JN).build();  jn=jCluster.getJournalNode(0);  String address=jn.getHttpAddress().toString();  assertFalse("HTTP Bind address not expected to be wildcard by default.",address.startsWith(WILDCARD_ADDRESS));
@Test(timeout=300000) public void testHttpsBindHostKey() throws Exception {  LOG.info("Testing behavior without " + DFS_JOURNALNODE_HTTPS_BIND_HOST_KEY);  setupSsl();  conf.set(DFS_HTTP_POLICY_KEY,HttpConfig.Policy.HTTPS_ONLY.name());  conf.set(DFS_JOURNALNODE_HTTPS_ADDRESS_KEY,LOCALHOST_SERVER_ADDRESS);  jCluster=new MiniJournalCluster.Builder(conf).format(true).numJournalNodes(NUM_JN).build();  jn=jCluster.getJournalNode(0);  String address=jn.getHttpsAddress().toString();  assertFalse("HTTP Bind address not expected to be wildcard by default.",address.startsWith(WILDCARD_ADDRESS));
private static void configureSuperUserIPAddresses(Configuration conf,String superUserShortName) throws IOException {  ArrayList<String> ipList=new ArrayList<String>();  Enumeration<NetworkInterface> netInterfaceList=NetworkInterface.getNetworkInterfaces();  while (netInterfaceList.hasMoreElements()) {    NetworkInterface inf=netInterfaceList.nextElement();    Enumeration<InetAddress> addrList=inf.getInetAddresses();    while (addrList.hasMoreElements()) {      InetAddress addr=addrList.nextElement();      ipList.add(addr.getHostAddress());    }  }  StringBuilder builder=new StringBuilder();  for (  String ip : ipList) {    builder.append(ip);    builder.append(',');  }  builder.append("127.0.1.1,");  builder.append(InetAddress.getLocalHost().getCanonicalHostName());
private void doTest(Configuration conf,long[] capacities,String[] racks,long newCapacity,String newRack,NewNodeInfo nodes,boolean useTool,boolean useFile,boolean useNamesystemSpy,double clusterUtilization) throws Exception {
private void doTest(Configuration conf,long[] capacities,String[] racks,long newCapacity,String newRack,NewNodeInfo nodes,boolean useTool,boolean useFile,boolean useNamesystemSpy,double clusterUtilization) throws Exception {  LOG.info("capacities = " + long2String(capacities));
private void doTest(Configuration conf,long[] capacities,String[] racks,long newCapacity,String newRack,NewNodeInfo nodes,boolean useTool,boolean useFile,boolean useNamesystemSpy,double clusterUtilization) throws Exception {  LOG.info("capacities = " + long2String(capacities));  LOG.info("racks      = " + Arrays.asList(racks));
private void doTest(Configuration conf,long[] capacities,String[] racks,long newCapacity,String newRack,NewNodeInfo nodes,boolean useTool,boolean useFile,boolean useNamesystemSpy,double clusterUtilization) throws Exception {  LOG.info("capacities = " + long2String(capacities));  LOG.info("racks      = " + Arrays.asList(racks));  LOG.info("newCapacity= " + newCapacity);
private void doTest(Configuration conf,long[] capacities,String[] racks,long newCapacity,String newRack,NewNodeInfo nodes,boolean useTool,boolean useFile,boolean useNamesystemSpy,double clusterUtilization) throws Exception {  LOG.info("capacities = " + long2String(capacities));  LOG.info("racks      = " + Arrays.asList(racks));  LOG.info("newCapacity= " + newCapacity);  LOG.info("newRack    = " + newRack);
private static int runBalancer(Collection<URI> namenodes,final BalancerParameters p,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 + conf.getLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT) * 1000;
private static int runBalancer(Collection<URI> namenodes,final BalancerParameters p,Configuration conf) throws IOException, InterruptedException {  final long sleeptime=conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 + conf.getLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT) * 1000;  LOG.info("namenodes  = " + namenodes);
    cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_REPLICATION_KEY,1);    conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,1);    cluster.startDataNodes(conf,1,true,null,null,dnCapacities);    cluster.waitClusterUp();    cluster.waitActive();    final Path path=new Path("/testMaxIterationTime.dat");    DistributedFileSystem fs=cluster.getFileSystem();    DFSTestUtil.createFile(fs,path,4L * blockSize,rep,seed);    cluster.startDataNodes(conf,1,true,null,null,dnCapacities);    cluster.triggerHeartbeats();    List<NameNodeConnector> connectors=Collections.emptyList();    try {      BalancerParameters bParams=BalancerParameters.DEFAULT;      connectors=NameNodeConnector.newNameNodeConnectors(DFSUtil.getInternalNsRpcUris(conf),Balancer.class.getSimpleName(),Balancer.BALANCER_ID_PATH,conf,bParams.getMaxIdleIteration());      for (      NameNodeConnector nnc : connectors) {
@Test(timeout=100000) public void testManyBalancerSimultaneously() throws Exception {  final Configuration conf=new HdfsConfiguration();  initConf(conf);  long[] capacities=new long[]{4 * CAPACITY};  String[] racks=new String[]{RACK0};  long newCapacity=2 * CAPACITY;  String newRack=RACK0;
@Test(timeout=100000) public void testManyBalancerSimultaneously() throws Exception {  final Configuration conf=new HdfsConfiguration();  initConf(conf);  long[] capacities=new long[]{4 * CAPACITY};  String[] racks=new String[]{RACK0};  long newCapacity=2 * CAPACITY;  String newRack=RACK0;  LOG.info("capacities = " + long2String(capacities));
@Test(timeout=100000) public void testManyBalancerSimultaneously() throws Exception {  final Configuration conf=new HdfsConfiguration();  initConf(conf);  long[] capacities=new long[]{4 * CAPACITY};  String[] racks=new String[]{RACK0};  long newCapacity=2 * CAPACITY;  String newRack=RACK0;  LOG.info("capacities = " + long2String(capacities));  LOG.info("racks      = " + Arrays.asList(racks));
@Test(timeout=100000) public void testManyBalancerSimultaneously() throws Exception {  final Configuration conf=new HdfsConfiguration();  initConf(conf);  long[] capacities=new long[]{4 * CAPACITY};  String[] racks=new String[]{RACK0};  long newCapacity=2 * CAPACITY;  String newRack=RACK0;  LOG.info("capacities = " + long2String(capacities));  LOG.info("racks      = " + Arrays.asList(racks));  LOG.info("newCapacity= " + newCapacity);
  final long[] lengths={10,10,10,10};  final long[] capacities=new long[replication];  final long totalUsed=capacities.length * sum(lengths);  Arrays.fill(capacities,1000);  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).simulatedCapacities(capacities).build();  final DistributedFileSystem dfs=cluster.getFileSystem();  cluster.waitActive();  client=NameNodeProxies.createProxy(conf,dfs.getUri(),ClientProtocol.class).getProxy();  for (int i=0; i < lengths.length; i++) {    final long size=lengths[i];    final Path p=new Path("/file" + i + "_size"+ size);    try (OutputStream out=dfs.create(p)){      for (int j=0; j < size; j++) {        out.write(j);      }    }   }  cluster.startDataNodes(conf,capacities.length,true,null,null,capacities);
  final long[] capacities=new long[replication];  final long totalUsed=capacities.length * sum(lengths);  Arrays.fill(capacities,1000);  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).simulatedCapacities(capacities).build();  final DistributedFileSystem dfs=cluster.getFileSystem();  cluster.waitActive();  client=NameNodeProxies.createProxy(conf,dfs.getUri(),ClientProtocol.class).getProxy();  for (int i=0; i < lengths.length; i++) {    final long size=lengths[i];    final Path p=new Path("/file" + i + "_size"+ size);    try (OutputStream out=dfs.create(p)){      for (int j=0; j < size; j++) {        out.write(j);      }    }   }  cluster.startDataNodes(conf,capacities.length,true,null,null,capacities);  LOG.info("capacities    = " + Arrays.toString(capacities));
  final long totalUsed=capacities.length * sum(lengths);  Arrays.fill(capacities,1000);  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).simulatedCapacities(capacities).build();  final DistributedFileSystem dfs=cluster.getFileSystem();  cluster.waitActive();  client=NameNodeProxies.createProxy(conf,dfs.getUri(),ClientProtocol.class).getProxy();  for (int i=0; i < lengths.length; i++) {    final long size=lengths[i];    final Path p=new Path("/file" + i + "_size"+ size);    try (OutputStream out=dfs.create(p)){      for (int j=0; j < size; j++) {        out.write(j);      }    }   }  cluster.startDataNodes(conf,capacities.length,true,null,null,capacities);  LOG.info("capacities    = " + Arrays.toString(capacities));  LOG.info("totalUsedSpace= " + totalUsed);
void testBalancerRPCDelay(int getBlocksMaxQps) throws Exception {  final Configuration conf=new HdfsConfiguration();  initConf(conf);  conf.setInt(DFSConfigKeys.DFS_BALANCER_DISPATCHERTHREADS_KEY,30);  conf.setInt(DFSConfigKeys.DFS_NAMENODE_GETBLOCKS_MAX_QPS_KEY,getBlocksMaxQps);  int numDNs=20;  long[] capacities=new long[numDNs];  String[] racks=new String[numDNs];  for (int i=0; i < numDNs; i++) {    capacities[i]=CAPACITY;    racks[i]=(i < numDNs / 2 ? RACK0 : RACK1);  }  doTest(conf,capacities,racks,CAPACITY,RACK2,new PortNumberBasedNodes(1,0,0),false,false,true,0.5);  assertTrue("Number of getBlocks should be not less than " + getBlocksMaxQps,numGetBlocksCalls.get() >= getBlocksMaxQps);  long durationMs=1 + endGetBlocksTime.get() - startGetBlocksTime.get();  int durationSec=(int)Math.ceil(durationMs / 1000.0);
static void wait(final ClientProtocol[] clients,long expectedUsedSpace,long expectedTotalSpace) throws IOException {
static void runBalancer(Suite s,final long totalUsed,final long totalCapacity) throws Exception {  double avg=totalUsed * 100.0 / totalCapacity;
private static void compareTotalPoolUsage(DatanodeStorageReport[] preReports,DatanodeStorageReport[] postReports){  Assert.assertNotNull(preReports);  Assert.assertNotNull(postReports);  Assert.assertEquals(preReports.length,postReports.length);  for (  DatanodeStorageReport preReport : preReports) {    String dnUuid=preReport.getDatanodeInfo().getDatanodeUuid();    for (    DatanodeStorageReport postReport : postReports) {      if (postReport.getDatanodeInfo().getDatanodeUuid().equals(dnUuid)) {        Assert.assertEquals(getTotalPoolUsage(preReport),getTotalPoolUsage(postReport));
private void runTest(final int nNameNodes,String[] racks,String[] newRack,Configuration conf,int nNameNodestoBalance,BalancerParameters balancerParameters) throws Exception {  final int nDataNodes=racks.length;  final long[] capacities=new long[nDataNodes];  Arrays.fill(capacities,CAPACITY);
private void runTest(final int nNameNodes,String[] racks,String[] newRack,Configuration conf,int nNameNodestoBalance,BalancerParameters balancerParameters) throws Exception {  final int nDataNodes=racks.length;  final long[] capacities=new long[nDataNodes];  Arrays.fill(capacities,CAPACITY);  LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes="+ nDataNodes);  Assert.assertEquals(nDataNodes,racks.length);
private void verifyPlacementPolicy(final MiniDFSCluster cluster,final Path file,boolean isBlockPlacementSatisfied) throws IOException {  DistributedFileSystem dfs=cluster.getFileSystem();  BlockManager blockManager=cluster.getNamesystem().getBlockManager();  LocatedBlock lb=DFSTestUtil.getAllBlocks(dfs,file).get(0);  BlockInfo blockInfo=blockManager.getStoredBlock(lb.getBlock().getLocalBlock());  Iterator<DatanodeStorageInfo> itr=blockInfo.getStorageInfos();
private static void setFailure(AtomicReference<String> failure,String what){  failure.compareAndSet("",what);
@Test(timeout=180000) public void testRateLimitingDuringDataNodeStartup() throws Exception {  Configuration conf=new Configuration();  conf.setInt(DFS_NAMENODE_MAX_FULL_BLOCK_REPORT_LEASES,1);  conf.setLong(DFS_NAMENODE_FULL_BLOCK_REPORT_LEASE_LENGTH_MS,20L * 60L * 1000L);  final Semaphore fbrSem=new Semaphore(0);  final HashSet<DatanodeID> expectedFbrDns=new HashSet<>();  final HashSet<DatanodeID> fbrDns=new HashSet<>();  final AtomicReference<String> failure=new AtomicReference<String>("");  final BlockManagerFaultInjector injector=new BlockManagerFaultInjector(){    private int numLeases=0;    @Override public void incomingBlockReportRpc(    DatanodeID nodeID,    BlockReportContext context) throws IOException {
      LOG.info("Incoming full block report from " + nodeID + ".  Lease ID = 0x"+ Long.toHexString(context.getLeaseId()));      if (context.getLeaseId() == 0) {        setFailure(failure,"Got unexpected rate-limiting-" + "bypassing full block report RPC from " + nodeID);      }      fbrSem.acquireUninterruptibly();synchronized (this) {        fbrDns.add(nodeID);        if (!expectedFbrDns.remove(nodeID)) {          setFailure(failure,"Got unexpected full block report " + "RPC from " + nodeID + ".  expectedFbrDns = "+ Joiner.on(", ").join(expectedFbrDns));        }        LOG.info("Proceeding with full block report from " + nodeID + ".  Lease ID = 0x"+ Long.toHexString(context.getLeaseId()));      }    }    @Override public void requestBlockReportLease(    DatanodeDescriptor node,    long leaseId){      if (leaseId == 0) {        return;      }synchronized (this) {        numLeases++;        expectedFbrDns.add(node);
      fbrSem.acquireUninterruptibly();synchronized (this) {        fbrDns.add(nodeID);        if (!expectedFbrDns.remove(nodeID)) {          setFailure(failure,"Got unexpected full block report " + "RPC from " + nodeID + ".  expectedFbrDns = "+ Joiner.on(", ").join(expectedFbrDns));        }        LOG.info("Proceeding with full block report from " + nodeID + ".  Lease ID = 0x"+ Long.toHexString(context.getLeaseId()));      }    }    @Override public void requestBlockReportLease(    DatanodeDescriptor node,    long leaseId){      if (leaseId == 0) {        return;      }synchronized (this) {        numLeases++;        expectedFbrDns.add(node);        LOG.info("requestBlockReportLease(node=" + node + ", leaseId=0x"+ Long.toHexString(leaseId)+ ").  "+ "expectedFbrDns = "+ Joiner.on(", ").join(expectedFbrDns));        if (numLeases > 1) {          setFailure(failure,"More than 1 lease was issued at once.");
    String poolId=cluster.getNamesystem().getBlockPoolId();    DatanodeRegistration reg=InternalDataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().get(3),poolId);    cluster.stopDataNode(3);    DFSTestUtil.waitForDatanodeState(cluster,reg.getDatanodeUuid(),false,20000);    final FileSystem fs=cluster.getFileSystem();    DFSTestUtil.createFile(fs,filePath,1L,replicationFactor,1L);    ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);    DFSTestUtil.waitReplication(cluster.getFileSystem(),filePath,replicationFactor);    cluster.startDataNodes(conf,1,true,null,new String[]{"/rack2"});    cluster.waitActive();    try {      DFSTestUtil.waitForReplication(cluster,b,2,replicationFactor,0);      fail("NameNode should not have fixed the mis-replicated blocks" + " automatically.");    } catch (    TimeoutException e) {    }    String fsckOp=DFSTestUtil.runFsck(conf,0,true,filePath.toString(),"-replicate");
@Test public void testNumVersionsReportedCorrect() throws IOException {  FSNamesystem fsn=Mockito.mock(FSNamesystem.class);  Mockito.when(fsn.hasWriteLock()).thenReturn(true);  DatanodeManager dm=mockDatanodeManager(fsn,new Configuration());  Random rng=new Random();  int seed=rng.nextInt();  rng=new Random(seed);
  FSNamesystem fsn=Mockito.mock(FSNamesystem.class);  Mockito.when(fsn.hasWriteLock()).thenReturn(true);  DatanodeManager dm=mockDatanodeManager(fsn,new Configuration());  Random rng=new Random();  int seed=rng.nextInt();  rng=new Random(seed);  LOG.info("Using seed " + seed + " for testing");  HashMap<String,DatanodeRegistration> sIdToDnReg=new HashMap<String,DatanodeRegistration>();  for (int i=0; i < NUM_ITERATIONS; ++i) {    if (rng.nextBoolean() && i % 3 == 0 && sIdToDnReg.size() != 0) {      int randomIndex=rng.nextInt() % sIdToDnReg.size();      Iterator<Map.Entry<String,DatanodeRegistration>> it=sIdToDnReg.entrySet().iterator();      for (int j=0; j < randomIndex - 1; ++j) {        it.next();      }      DatanodeRegistration toRemove=it.next().getValue();
      dm.removeDatanode(toRemove);      it.remove();    } else {      String storageID="someStorageID" + rng.nextInt(5000);      DatanodeRegistration dr=Mockito.mock(DatanodeRegistration.class);      Mockito.when(dr.getDatanodeUuid()).thenReturn(storageID);      if (sIdToDnReg.containsKey(storageID)) {        dr=sIdToDnReg.get(storageID);        if (rng.nextBoolean()) {          dr.setIpAddr(dr.getIpAddr() + "newIP");        }      } else {        String ip="someIP" + storageID;        Mockito.when(dr.getIpAddr()).thenReturn(ip);        Mockito.when(dr.getXferAddr()).thenReturn(ip + ":9000");        Mockito.when(dr.getXferPort()).thenReturn(9000);
      if (dn.getDatanodeUuid().equals(datanodeUuid)) {        datanodeToRemoveStorageFrom=dn;        break;      }      datanodeToRemoveStorageFromIdx++;    }    StorageLocation volumeLocationToRemove=null;    try (FsVolumeReferences volumes=datanodeToRemoveStorageFrom.getFSDataset().getFsVolumeReferences()){      assertEquals(NUM_STORAGES_PER_DN,volumes.size());      for (      FsVolumeSpi volume : volumes) {        if (volume.getStorageID().equals(storageIdToRemove)) {          volumeLocationToRemove=volume.getStorageLocation();        }      }    }     ;    assertNotNull(volumeLocationToRemove);    datanodeToRemoveStorageFrom.shutdown();    FileUtil.fullyDelete(new File(volumeLocationToRemove.getUri()));    FileOutputStream fos=new FileOutputStream(new File(volumeLocationToRemove.getUri()));
      assertEquals(NUM_STORAGES_PER_DN,volumes.size());      for (      FsVolumeSpi volume : volumes) {        if (volume.getStorageID().equals(storageIdToRemove)) {          volumeLocationToRemove=volume.getStorageLocation();        }      }    }     ;    assertNotNull(volumeLocationToRemove);    datanodeToRemoveStorageFrom.shutdown();    FileUtil.fullyDelete(new File(volumeLocationToRemove.getUri()));    FileOutputStream fos=new FileOutputStream(new File(volumeLocationToRemove.getUri()));    try {      fos.write(1);    }  finally {      fos.close();    }    cluster.restartDataNode(datanodeToRemoveStorageFromIdx);    LOG.info("waiting for the datanode to remove " + storageIdToRemove);
    final String newStorageId=DatanodeStorage.generateUuid();    try {      File currentDir=new File(new File(volumeRefs.get(0).getStorageLocation().getUri()),"current");      File versionFile=new File(currentDir,"VERSION");      rewriteVersionFile(versionFile,newStorageId);    }  finally {      volumeRefs.close();    }    final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,TEST_PATH);    cluster.restartDataNodes();    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        cluster.getNamesystem().writeLock();        try {          Iterator<DatanodeStorageInfo> storageInfoIter=cluster.getNamesystem().getBlockManager().getStorages(block.getLocalBlock()).iterator();          if (!storageInfoIter.hasNext()) {
      rewriteVersionFile(versionFile,newStorageId);    }  finally {      volumeRefs.close();    }    final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,TEST_PATH);    cluster.restartDataNodes();    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        cluster.getNamesystem().writeLock();        try {          Iterator<DatanodeStorageInfo> storageInfoIter=cluster.getNamesystem().getBlockManager().getStorages(block.getLocalBlock()).iterator();          if (!storageInfoIter.hasNext()) {            LOG.info("Expected to find a storage for " + block.getBlockName() + ", but nothing was found.  "+ "Continuing to wait.");            return false;          }          DatanodeStorageInfo info=storageInfoIter.next();          if (!newStorageId.equals(info.getStorageID())) {
      volumeRefs.close();    }    final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,TEST_PATH);    cluster.restartDataNodes();    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        cluster.getNamesystem().writeLock();        try {          Iterator<DatanodeStorageInfo> storageInfoIter=cluster.getNamesystem().getBlockManager().getStorages(block.getLocalBlock()).iterator();          if (!storageInfoIter.hasNext()) {            LOG.info("Expected to find a storage for " + block.getBlockName() + ", but nothing was found.  "+ "Continuing to wait.");            return false;          }          DatanodeStorageInfo info=storageInfoIter.next();          if (!newStorageId.equals(info.getStorageID())) {            LOG.info("Expected " + block.getBlockName() + " to "+ "be in storage id "+ newStorageId+ ", but it "+ "was in "+ info.getStorageID()+ ".  Continuing "+ "to wait.");            return false;
    final FSNamesystem namesystem=cluster.getNamesystem();    FileSystem fs=cluster.getFileSystem();    Path testPath=new Path("/tmp/TestRBWBlockInvalidation","foo1");    out=fs.create(testPath,(short)2);    out.writeBytes("HDFS-3157: " + testPath);    out.hsync();    cluster.startDataNodes(conf,1,true,null,null,null);    ExtendedBlock blk=DFSTestUtil.getFirstBlock(fs,testPath);    MaterializedReplica replica=cluster.getMaterializedReplica(0,blk);    replica.deleteData();    replica.deleteMeta();    out.close();    int liveReplicas=0;    while (true) {      if ((liveReplicas=countReplicas(namesystem,blk).liveReplicas()) < 2) {
    cluster.startDataNodes(conf,1,true,null,null,null);    ExtendedBlock blk=DFSTestUtil.getFirstBlock(fs,testPath);    MaterializedReplica replica=cluster.getMaterializedReplica(0,blk);    replica.deleteData();    replica.deleteMeta();    out.close();    int liveReplicas=0;    while (true) {      if ((liveReplicas=countReplicas(namesystem,blk).liveReplicas()) < 2) {        LOG.info("Live Replicas after corruption: " + liveReplicas);        break;      }      Thread.sleep(100);    }    assertEquals("There should be less than 2 replicas in the " + "liveReplicasMap",1,liveReplicas);    while (true) {      if ((liveReplicas=countReplicas(namesystem,blk).liveReplicas()) > 1) {
@Test public void testReconstructForNotEnoughRacks() throws Exception {  LOG.info("cluster hosts: {}, racks: {}",Arrays.asList(hosts),Arrays.asList(racks));  cluster=new MiniDFSCluster.Builder(conf).racks(racks).hosts(hosts).numDataNodes(hosts.length).build();  cluster.waitActive();  fs=cluster.getFileSystem();  fs.enableErasureCodingPolicy(StripedFileTestUtil.getDefaultECPolicy().getName());  fs.setErasureCodingPolicy(new Path("/"),StripedFileTestUtil.getDefaultECPolicy().getName());  FSNamesystem fsn=cluster.getNamesystem();  BlockManager bm=fsn.getBlockManager();  MiniDFSCluster.DataNodeProperties lastHost=stopDataNode(hosts[hosts.length - 1]);  final Path file=new Path("/foo");  DFSTestUtil.createFile(fs,file,cellSize * dataBlocks * 2,(short)1,0L);  GenericTestUtils.waitFor(() -> bm.numOfUnderReplicatedBlocks() == 0,100,30000);
  BlockManager bm=fsn.getBlockManager();  MiniDFSCluster.DataNodeProperties lastHost=stopDataNode(hosts[hosts.length - 1]);  final Path file=new Path("/foo");  DFSTestUtil.createFile(fs,file,cellSize * dataBlocks * 2,(short)1,0L);  GenericTestUtils.waitFor(() -> bm.numOfUnderReplicatedBlocks() == 0,100,30000);  LOG.info("Created file {}",file);  final INodeFile fileNode=fsn.getFSDirectory().getINode4Write(file.toString()).asFile();  BlockInfoStriped blockInfo=(BlockInfoStriped)fileNode.getLastBlock();  Set<String> rackSet=new HashSet<>();  for (  DatanodeStorageInfo storage : blockInfo.storages) {    rackSet.add(storage.getDatanodeDescriptor().getNetworkLocation());  }  Assert.assertEquals("rackSet size is wrong: " + rackSet,dataBlocks - 1,rackSet.size());  cluster.restartDataNode(lastHost);  cluster.waitActive();  NetworkTopology topology=bm.getDatanodeManager().getNetworkTopology();
@Test(timeout=60000) public void testBlockGroupIdGeneration() throws IOException {  long blockGroupIdInitialValue=blockGrpIdGenerator.getCurrentValue();  Path path=new Path(ecDir,"testBlockGrpIdGeneration.dat");  DFSTestUtil.createFile(fs,path,cellSize,fileLen,blockSize,REPLICATION,SEED);  List<LocatedBlock> blocks=DFSTestUtil.getAllBlocks(fs,path);  assertThat("Wrong BlockGrps",blocks.size(),is(blockGrpCount));  blockGrpIdGenerator.setCurrentValue(blockGroupIdInitialValue);  for (int i=0; i < blocks.size(); ++i) {    blockGrpIdGenerator.skipTo((blockGrpIdGenerator.getCurrentValue() & ~BLOCK_GROUP_INDEX_MASK) + MAX_BLOCKS_IN_GROUP);    long nextBlockExpectedId=blockGrpIdGenerator.getCurrentValue();    long nextBlockGrpId=blocks.get(i).getBlock().getBlockId();
@Test public void testBlockIdGeneration() throws IOException {  Configuration conf=new HdfsConfiguration();  conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,1);  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();  try {    cluster.waitActive();    FileSystem fs=cluster.getFileSystem();    Path path=new Path("testBlockIdGeneration.dat");    DFSTestUtil.createFile(fs,path,IO_SIZE,BLOCK_SIZE * 10,BLOCK_SIZE,REPLICATION,SEED);    List<LocatedBlock> blocks=DFSTestUtil.getAllBlocks(fs,path);    LOG.info("Block0 id is " + blocks.get(0).getBlock().getBlockId());    long nextBlockExpectedId=blocks.get(0).getBlock().getBlockId() + 1;    for (int i=1; i < blocks.size(); ++i) {      long nextBlockId=blocks.get(i).getBlock().getBlockId();
private Set<ReportForJson> getAndDeserializeJson() throws IOException {  final String json=tracker.getJson();
private void assertDecommnNodePosition(int blkGrpWidth,HashMap<Integer,List<String>> decommissionedNodes,List<LocatedBlock> lbs){  for (int i=0; i < lbs.size(); i++) {    LocatedBlock blk=lbs.get(i);    DatanodeInfo[] nodes=blk.getLocations();    List<String> decommissionedNodeList=decommissionedNodes.get(i);    for (int j=0; j < nodes.length; j++) {      DatanodeInfo dnInfo=nodes[j];
@Test public void testRelativePathAsURI() throws IOException {  URI u=Util.stringAsURI(RELATIVE_FILE_PATH);
@Test public void testURI() throws IOException {  LOG.info("Testing correct Unix URI: " + URI_UNIX);  URI u=Util.stringAsURI(URI_UNIX);
@Test public void testURI() throws IOException {  LOG.info("Testing correct Unix URI: " + URI_UNIX);  URI u=Util.stringAsURI(URI_UNIX);  LOG.info("Uri: " + u);  assertNotNull("Uri should not be null at this point",u);  assertEquals(URI_FILE_SCHEMA,u.getScheme());  assertEquals(URI_PATH_UNIX,u.getPath());  LOG.info("Testing correct windows URI: " + URI_WINDOWS);  u=Util.stringAsURI(URI_WINDOWS);
  Mockito.when(request.getMethod()).thenReturn("GET");  Mockito.when(request.getRequestURI()).thenReturn(new StringBuffer(WebHdfsFileSystem.PATH_PREFIX + "/IAmARandomRequest/").toString());  Mockito.when(request.getQueryString()).thenReturn(null);  Mockito.when(request.getRemoteAddr()).thenReturn("192.168.1.2");  HttpServletResponse response=Mockito.mock(HttpServletResponse.class);  FilterChain chain=new FilterChain(){    @Override public void doFilter(    ServletRequest servletRequest,    ServletResponse servletResponse) throws IOException, ServletException {    }  };  Filter filter=new HostRestrictingAuthorizationFilter();  HashMap<String,String> configs=new HashMap<String,String>(){  };  configs.put(AuthenticationFilter.AUTH_TYPE,"simple");  FilterConfig fc=new DummyFilterConfig(configs);  filter.init(fc);  filter.doFilter(request,response,chain);
  boolean corruptedGs=false;  boolean corruptedLen=false;  int reportIndex=0;  for (  Map.Entry<DatanodeStorage,BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {    DatanodeStorage dnStorage=kvPair.getKey();    BlockListAsLongs blockList=kvPair.getValue();    BlockListAsLongs.Builder builder=BlockListAsLongs.builder();    for (    BlockReportReplica block : blockList) {      if (corruptOneBlockGs && !corruptedGs) {        long gsOld=block.getGenerationStamp();        long gsNew;        do {          gsNew=rand.nextInt();        } while (gsNew == gsOld);        block.setGenerationStamp(gsNew);
        long gsOld=block.getGenerationStamp();        long gsNew;        do {          gsNew=rand.nextInt();        } while (gsNew == gsOld);        block.setGenerationStamp(gsNew);        LOG.info("Corrupted the GS for block ID " + block);        corruptedGs=true;      } else       if (corruptOneBlockLen && !corruptedLen) {        long lenOld=block.getNumBytes();        long lenNew;        do {          lenNew=rand.nextInt((int)lenOld - 1);        } while (lenNew == lenOld);        block.setNumBytes(lenNew);
@Test(timeout=300000) public void blockReport_01() throws IOException {  final String METHOD_NAME=GenericTestUtils.getMethodName();  Path filePath=new Path("/" + METHOD_NAME + ".dat");  ArrayList<Block> blocks=prepareForRide(filePath,METHOD_NAME,FILE_SIZE);  if (LOG.isDebugEnabled()) {
  ArrayList<Block> blocks=prepareForRide(filePath,METHOD_NAME,FILE_SIZE);  if (LOG.isDebugEnabled()) {    LOG.debug("Number of blocks allocated " + blocks.size());  }  long[] oldLengths=new long[blocks.size()];  int tempLen;  for (int i=0; i < blocks.size(); i++) {    Block b=blocks.get(i);    if (LOG.isDebugEnabled()) {      LOG.debug("Block " + b.getBlockName() + " before\t"+ "Size "+ b.getNumBytes());    }    oldLengths[i]=b.getNumBytes();    if (LOG.isDebugEnabled()) {      LOG.debug("Setting new length");    }    tempLen=rand.nextInt(BLOCK_SIZE);    b.set(b.getBlockId(),tempLen,b.getGenerationStamp());    if (LOG.isDebugEnabled()) {
    if (LOG.isDebugEnabled()) {      LOG.debug("Block " + b.getBlockName() + " before\t"+ "Size "+ b.getNumBytes());    }    oldLengths[i]=b.getNumBytes();    if (LOG.isDebugEnabled()) {      LOG.debug("Setting new length");    }    tempLen=rand.nextInt(BLOCK_SIZE);    b.set(b.getBlockId(),tempLen,b.getGenerationStamp());    if (LOG.isDebugEnabled()) {      LOG.debug("Block " + b.getBlockName() + " after\t "+ "Size "+ b.getNumBytes());    }  }  DataNode dn=cluster.getDataNodes().get(DN_N0);  String poolId=cluster.getNamesystem().getBlockPoolId();  DatanodeRegistration dnR=dn.getDNRegistrationForBP(poolId);  StorageBlockReport[] reports=getBlockReports(dn,poolId,false,false);  sendBlockReports(dnR,poolId,reports);  List<LocatedBlock> blocksAfterReport=DFSTestUtil.getAllBlocks(fs.open(filePath));
@Test(timeout=300000) public void blockReport_02() throws IOException {  final String METHOD_NAME=GenericTestUtils.getMethodName();
  final String METHOD_NAME=GenericTestUtils.getMethodName();  LOG.info("Running test " + METHOD_NAME);  Path filePath=new Path("/" + METHOD_NAME + ".dat");  DFSTestUtil.createFile(fs,filePath,FILE_SIZE,REPL_FACTOR,rand.nextLong());  File dataDir=new File(cluster.getDataDirectory());  assertTrue(dataDir.isDirectory());  List<ExtendedBlock> blocks2Remove=new ArrayList<ExtendedBlock>();  List<Integer> removedIndex=new ArrayList<Integer>();  List<LocatedBlock> lBlocks=cluster.getNameNodeRpc().getBlockLocations(filePath.toString(),FILE_START,FILE_SIZE).getLocatedBlocks();  while (removedIndex.size() != 2) {    int newRemoveIndex=rand.nextInt(lBlocks.size());    if (!removedIndex.contains(newRemoveIndex))     removedIndex.add(newRemoveIndex);  }  for (  Integer aRemovedIndex : removedIndex) {    blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());  }  if (LOG.isDebugEnabled()) {
  File dataDir=new File(cluster.getDataDirectory());  assertTrue(dataDir.isDirectory());  List<ExtendedBlock> blocks2Remove=new ArrayList<ExtendedBlock>();  List<Integer> removedIndex=new ArrayList<Integer>();  List<LocatedBlock> lBlocks=cluster.getNameNodeRpc().getBlockLocations(filePath.toString(),FILE_START,FILE_SIZE).getLocatedBlocks();  while (removedIndex.size() != 2) {    int newRemoveIndex=rand.nextInt(lBlocks.size());    if (!removedIndex.contains(newRemoveIndex))     removedIndex.add(newRemoveIndex);  }  for (  Integer aRemovedIndex : removedIndex) {    blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());  }  if (LOG.isDebugEnabled()) {    LOG.debug("Number of blocks allocated " + lBlocks.size());  }  final DataNode dn0=cluster.getDataNodes().get(DN_N0);  for (  ExtendedBlock b : blocks2Remove) {    if (LOG.isDebugEnabled()) {
    int newRemoveIndex=rand.nextInt(lBlocks.size());    if (!removedIndex.contains(newRemoveIndex))     removedIndex.add(newRemoveIndex);  }  for (  Integer aRemovedIndex : removedIndex) {    blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());  }  if (LOG.isDebugEnabled()) {    LOG.debug("Number of blocks allocated " + lBlocks.size());  }  final DataNode dn0=cluster.getDataNodes().get(DN_N0);  for (  ExtendedBlock b : blocks2Remove) {    if (LOG.isDebugEnabled()) {      LOG.debug("Removing the block " + b.getBlockName());    }    for (    File f : findAllFiles(dataDir,new MyFileFilter(b.getBlockName(),true))) {      DataNodeTestUtils.getFSDataset(dn0).unfinalizeBlock(b);      if (!f.delete()) {        LOG.warn("Couldn't delete " + b.getBlockName());      } else {
@Test(timeout=300000) public void testInterleavedBlockReports() throws IOException, ExecutionException, InterruptedException {  int numConcurrentBlockReports=3;  DataNode dn=cluster.getDataNodes().get(DN_N0);  final String poolId=cluster.getNamesystem().getBlockPoolId();
private void waitForTempReplica(Block bl,int DN_N1) throws IOException {  final boolean tooLongWait=false;  final int TIMEOUT=40000;  if (LOG.isDebugEnabled()) {
  }  while (cluster.getDataNodes().size() <= DN_N1) {    waitTil(20);  }  if (LOG.isDebugEnabled()) {    LOG.debug("Total number of DNs " + cluster.getDataNodes().size());  }  cluster.waitActive();  final DataNode dn1=cluster.getDataNodes().get(DN_N1);  String bpid=cluster.getNamesystem().getBlockPoolId();  Replica r=DataNodeTestUtils.fetchReplicaInfo(dn1,bpid,bl.getBlockId());  long start=Time.monotonicNow();  int count=0;  while (r == null) {    waitTil(5);    r=DataNodeTestUtils.fetchReplicaInfo(dn1,bpid,bl.getBlockId());    long waiting_period=Time.monotonicNow() - start;    if (count++ % 100 == 0)     if (LOG.isDebugEnabled()) {
  Replica r=DataNodeTestUtils.fetchReplicaInfo(dn1,bpid,bl.getBlockId());  long start=Time.monotonicNow();  int count=0;  while (r == null) {    waitTil(5);    r=DataNodeTestUtils.fetchReplicaInfo(dn1,bpid,bl.getBlockId());    long waiting_period=Time.monotonicNow() - start;    if (count++ % 100 == 0)     if (LOG.isDebugEnabled()) {      LOG.debug("Has been waiting for " + waiting_period + " ms.");    }    if (waiting_period > TIMEOUT)     assertTrue("Was waiting too long to get ReplicaInfo from a datanode",tooLongWait);  }  HdfsServerConstants.ReplicaState state=r.getState();  if (LOG.isDebugEnabled()) {    LOG.debug("Replica state before the loop " + state.getValue());  }  start=Time.monotonicNow();  while (state != HdfsServerConstants.ReplicaState.TEMPORARY) {
  while (r == null) {    waitTil(5);    r=DataNodeTestUtils.fetchReplicaInfo(dn1,bpid,bl.getBlockId());    long waiting_period=Time.monotonicNow() - start;    if (count++ % 100 == 0)     if (LOG.isDebugEnabled()) {      LOG.debug("Has been waiting for " + waiting_period + " ms.");    }    if (waiting_period > TIMEOUT)     assertTrue("Was waiting too long to get ReplicaInfo from a datanode",tooLongWait);  }  HdfsServerConstants.ReplicaState state=r.getState();  if (LOG.isDebugEnabled()) {    LOG.debug("Replica state before the loop " + state.getValue());  }  start=Time.monotonicNow();  while (state != HdfsServerConstants.ReplicaState.TEMPORARY) {    waitTil(5);    state=r.getState();    if (LOG.isDebugEnabled()) {
private ArrayList<Block> prepareForRide(final Path filePath,final String METHOD_NAME,long fileSize) throws IOException {
private ArrayList<Block> locatedToBlocks(final List<LocatedBlock> locatedBlks,List<Integer> positionsToRemove){  ArrayList<Block> newList=new ArrayList<Block>();  for (int i=0; i < locatedBlks.size(); i++) {    if (positionsToRemove != null && positionsToRemove.contains(i)) {      if (LOG.isDebugEnabled()) {
@Test(timeout=15000) public void testRefreshLeaseId() throws Exception {  Mockito.when(mockNN1.sendHeartbeat(Mockito.any(DatanodeRegistration.class),Mockito.any(StorageReport[].class),Mockito.anyLong(),Mockito.anyLong(),Mockito.anyInt(),Mockito.anyInt(),Mockito.anyInt(),Mockito.any(VolumeFailureSummary.class),Mockito.anyBoolean(),Mockito.any(SlowPeerReports.class),Mockito.any(SlowDiskReports.class))).thenAnswer(new HeartbeatAnswer(0)).thenAnswer(new HeartbeatRegisterAnswer(0)).thenAnswer(new HeartbeatAnswer(0));  Mockito.when(mockNN1.blockReport(Mockito.any(DatanodeRegistration.class),Mockito.anyString(),Mockito.any(StorageBlockReport[].class),Mockito.any(BlockReportContext.class))).thenAnswer(new Answer(){    @Override public Object answer(    InvocationOnMock invocation) throws Throwable {      BlockReportContext context=(BlockReportContext)invocation.getArguments()[3];      long leaseId=context.getLeaseId();
        }      });    }    final ExecutorCompletionService<Boolean> verifyService=new ExecutorCompletionService<>(executor);    final AtomicLong verifyFileTime=new AtomicLong();    for (int i=0; i < NUM_FILES; i++) {      final Path file=createService.take().get();      verifyService.submit(new Callable<Boolean>(){        @Override public Boolean call() throws Exception {          final long start=Time.monotonicNow();          try {            return verifyFile(file,dfs);          }  finally {            verifyFileTime.addAndGet(Time.monotonicNow() - start);          }        }      });    }    for (int i=0; i < NUM_FILES; i++) {      Assert.assertTrue(verifyService.take().get());
      });    }    final ExecutorCompletionService<Boolean> verifyService=new ExecutorCompletionService<>(executor);    final AtomicLong verifyFileTime=new AtomicLong();    for (int i=0; i < NUM_FILES; i++) {      final Path file=createService.take().get();      verifyService.submit(new Callable<Boolean>(){        @Override public Boolean call() throws Exception {          final long start=Time.monotonicNow();          try {            return verifyFile(file,dfs);          }  finally {            verifyFileTime.addAndGet(Time.monotonicNow() - start);          }        }      });    }    for (int i=0; i < NUM_FILES; i++) {      Assert.assertTrue(verifyService.take().get());
static void logIbrCounts(List<DataNode> datanodes){  final String name="IncrementalBlockReportsNumOps";  for (  DataNode dn : datanodes) {    final MetricsRecordBuilder m=MetricsAsserts.getMetrics(dn.getMetrics().name());    final long ibr=MetricsAsserts.getLongCounter(name,m);
  final long seed;  final int numBlocks;{    final String name=f.getName();    final int i=name.indexOf('_');    seed=Long.parseLong(name.substring(0,i));    numBlocks=Integer.parseInt(name.substring(i + 1));  }  final byte[] computed=IO_BUF.get();  final byte[] expected=VERIFY_BUF.get();  try (FSDataInputStream in=dfs.open(f)){    for (int i=0; i < numBlocks; i++) {      in.read(computed);      nextBytes(i,seed,expected);      Assert.assertArrayEquals(expected,computed);    }    return true;  } catch (  Exception e) {
private void doLog(String string){synchronized (log) {
public void getTrashDirectoryForBlockFile(String fileName,int nestingLevel){  final String blockFileSubdir=makeRandomBlockFileSubdir(nestingLevel);  final String blockFileName=fileName;  String testFilePath=storage.getSingularStorageDir().getRoot() + File.separator + Storage.STORAGE_DIR_CURRENT+ blockFileSubdir+ blockFileName;  String expectedTrashPath=storage.getSingularStorageDir().getRoot() + File.separator + BlockPoolSliceStorage.TRASH_ROOT_DIR+ blockFileSubdir.substring(0,blockFileSubdir.length() - 1);
public void getTrashDirectoryForBlockFile(String fileName,int nestingLevel){  final String blockFileSubdir=makeRandomBlockFileSubdir(nestingLevel);  final String blockFileName=fileName;  String testFilePath=storage.getSingularStorageDir().getRoot() + File.separator + Storage.STORAGE_DIR_CURRENT+ blockFileSubdir+ blockFileName;  String expectedTrashPath=storage.getSingularStorageDir().getRoot() + File.separator + BlockPoolSliceStorage.TRASH_ROOT_DIR+ blockFileSubdir.substring(0,blockFileSubdir.length() - 1);  LOG.info("Got subdir {}",blockFileSubdir);
public void getRestoreDirectoryForBlockFile(String fileName,int nestingLevel){  BlockPoolSliceStorage storage=makeBlockPoolStorage();  final String blockFileSubdir=makeRandomBlockFileSubdir(nestingLevel);  final String blockFileName=fileName;  String deletedFilePath=storage.getSingularStorageDir().getRoot() + File.separator + BlockPoolSliceStorage.TRASH_ROOT_DIR+ blockFileSubdir+ blockFileName;  String expectedRestorePath=storage.getSingularStorageDir().getRoot() + File.separator + Storage.STORAGE_DIR_CURRENT+ blockFileSubdir.substring(0,blockFileSubdir.length() - 1);
  final AtomicReference<String> failure=new AtomicReference<String>(null);  Collection<RecoveringBlock> recoveringBlocks=initRecoveringBlocks();  final RecoveringBlock recoveringBlock=Iterators.get(recoveringBlocks.iterator(),0);  final ExtendedBlock block=recoveringBlock.getBlock();  Thread slowWriterThread=new Thread(new Runnable(){    @Override public void run(){      try {        LOG.debug("slowWriter creating rbw");        ReplicaHandler replicaHandler=spyDN.data.createRbw(StorageType.DISK,null,block,false);        replicaHandler.close();        LOG.debug("slowWriter created rbw");        progressParent.sem.release();        terminateSlowWriter.uninterruptiblyAcquire(60000);        LOG.debug("slowWriter exiting");      } catch (      Throwable t) {
        LOG.debug("slowWriter created rbw");        progressParent.sem.release();        terminateSlowWriter.uninterruptiblyAcquire(60000);        LOG.debug("slowWriter exiting");      } catch (      Throwable t) {        LOG.error("slowWriter got exception",t);        failure.compareAndSet(null,"slowWriter got exception " + t.getMessage());      }    }  });  slowWriterThread.start();  progressParent.uninterruptiblyAcquire(60000);  Thread stopWriterThread=new Thread(new Runnable(){    @Override public void run(){      try {        LOG.debug("initiating " + tswr.opName());        tswr.run(recoveringBlock);
        terminateSlowWriter.uninterruptiblyAcquire(60000);        LOG.debug("slowWriter exiting");      } catch (      Throwable t) {        LOG.error("slowWriter got exception",t);        failure.compareAndSet(null,"slowWriter got exception " + t.getMessage());      }    }  });  slowWriterThread.start();  progressParent.uninterruptiblyAcquire(60000);  Thread stopWriterThread=new Thread(new Runnable(){    @Override public void run(){      try {        LOG.debug("initiating " + tswr.opName());        tswr.run(recoveringBlock);        LOG.debug("finished " + tswr.opName());      } catch (      Throwable t) {
          isNewNode=false;          break;        }      }      if (isNewNode) {        newNode=node;        break;      }    }    assertTrue(newNode != null);    DatanodeInfo source=null;    ArrayList<DatanodeInfo> proxies=new ArrayList<DatanodeInfo>(2);    for (    DatanodeInfo node : datanodes) {      if (node != newNode) {        if (node.getNetworkLocation().equals(newNode.getNetworkLocation())) {          source=node;        } else {          proxies.add(node);        }      }    }    assertTrue(source != null && proxies.size() == 2);
        }      }      if (isNewNode) {        newNode=node;        break;      }    }    assertTrue(newNode != null);    DatanodeInfo source=null;    ArrayList<DatanodeInfo> proxies=new ArrayList<DatanodeInfo>(2);    for (    DatanodeInfo node : datanodes) {      if (node != newNode) {        if (node.getNetworkLocation().equals(newNode.getNetworkLocation())) {          source=node;        } else {          proxies.add(node);        }      }    }    assertTrue(source != null && proxies.size() == 2);    LOG.info("Testcase 1: Proxy " + newNode + " does not contain the block "+ b);    assertFalse(replaceBlock(b,source,newNode,proxies.get(0)));
      if (isNewNode) {        newNode=node;        break;      }    }    assertTrue(newNode != null);    DatanodeInfo source=null;    ArrayList<DatanodeInfo> proxies=new ArrayList<DatanodeInfo>(2);    for (    DatanodeInfo node : datanodes) {      if (node != newNode) {        if (node.getNetworkLocation().equals(newNode.getNetworkLocation())) {          source=node;        } else {          proxies.add(node);        }      }    }    assertTrue(source != null && proxies.size() == 2);    LOG.info("Testcase 1: Proxy " + newNode + " does not contain the block "+ b);    assertFalse(replaceBlock(b,source,newNode,proxies.get(0)));
      }    }    assertTrue(newNode != null);    DatanodeInfo source=null;    ArrayList<DatanodeInfo> proxies=new ArrayList<DatanodeInfo>(2);    for (    DatanodeInfo node : datanodes) {      if (node != newNode) {        if (node.getNetworkLocation().equals(newNode.getNetworkLocation())) {          source=node;        } else {          proxies.add(node);        }      }    }    assertTrue(source != null && proxies.size() == 2);    LOG.info("Testcase 1: Proxy " + newNode + " does not contain the block "+ b);    assertFalse(replaceBlock(b,source,newNode,proxies.get(0)));    LOG.info("Testcase 2: Destination " + proxies.get(1) + " contains the block "+ b);    assertFalse(replaceBlock(b,source,proxies.get(0),proxies.get(1)));    LOG.info("Testcase 3: Source=" + source + " Proxy="+ proxies.get(0)+ " Destination="+ newNode);
    LocatedBlock lb=dfs.getClient().getLocatedBlocks(fileName,0).get(0);    DatanodeInfo[] oldNodes=lb.getLocations();    assertEquals("Wrong block locations",oldNodes.length,1);    DatanodeInfo source=oldNodes[0];    ExtendedBlock b=lb.getBlock();    DatanodeInfo[] datanodes=dfs.getDataNodeStats();    DatanodeInfo destin=null;    for (    DatanodeInfo datanodeInfo : datanodes) {      if (!oldNodes[0].equals(datanodeInfo)) {        destin=datanodeInfo;        break;      }    }    assertNotNull("Failed to choose destination datanode!",destin);    assertFalse("Source and destin datanode should be different",source.equals(destin));    for (int i=0; i < cluster.getDataNodes().size(); i++) {      DataNode dn=cluster.getDataNodes().get(i);
  do {    try {      Thread.sleep(100);    } catch (    InterruptedException e) {    }    List<LocatedBlock> blocks=client.getNamenode().getBlockLocations(fileName,0,fileLen).getLocatedBlocks();    assertEquals(1,blocks.size());    DatanodeInfo[] nodes=blocks.get(0).getLocations();    notDone=(nodes.length != replFactor);    if (notDone) {      LOG.info("Expected replication factor is " + replFactor + " but the real replication factor is "+ nodes.length);    } else {      List<DatanodeInfo> nodeLocations=Arrays.asList(nodes);      for (      DatanodeInfo node : includeNodes) {        if (!nodeLocations.contains(node)) {          notDone=true;
    assertEquals(1,blocks.size());    DatanodeInfo[] nodes=blocks.get(0).getLocations();    notDone=(nodes.length != replFactor);    if (notDone) {      LOG.info("Expected replication factor is " + replFactor + " but the real replication factor is "+ nodes.length);    } else {      List<DatanodeInfo> nodeLocations=Arrays.asList(nodes);      for (      DatanodeInfo node : includeNodes) {        if (!nodeLocations.contains(node)) {          notDone=true;          LOG.info("Block is not located at " + node);          break;        }      }    }    if (Time.monotonicNow() > failtime) {      String expectedNodesList="";      String currentNodesList="";
    DatanodeInfo[] nodes=blocks.get(0).getLocations();    notDone=(nodes.length != replFactor);    if (notDone) {      LOG.info("Expected replication factor is " + replFactor + " but the real replication factor is "+ nodes.length);    } else {      List<DatanodeInfo> nodeLocations=Arrays.asList(nodes);      for (      DatanodeInfo node : includeNodes) {        if (!nodeLocations.contains(node)) {          notDone=true;          LOG.info("Block is not located at " + node);          break;        }      }    }    if (Time.monotonicNow() > failtime) {      String expectedNodesList="";      String currentNodesList="";      for (      DatanodeInfo dn : includeNodes)       expectedNodesList+=dn + ", ";
    Path fileName=new Path("/tmp.txt");    DFSTestUtil.createFile(fs,fileName,10L,(short)1,1234L);    DFSTestUtil.waitReplication(fs,fileName,(short)1);    client=new DFSClient(cluster.getFileSystem(0).getUri(),conf);    List<LocatedBlock> locatedBlocks=client.getNamenode().getBlockLocations("/tmp.txt",0,10L).getLocatedBlocks();    assertTrue(locatedBlocks.size() == 1);    assertTrue(locatedBlocks.get(0).getLocations().length == 1);    cluster.startDataNodes(conf,1,true,null,null,null,null);    assertEquals("Number of datanodes should be 2",2,cluster.getDataNodes().size());    DataNode dn0=cluster.getDataNodes().get(0);    DataNode dn1=cluster.getDataNodes().get(1);    String activeNNBPId=cluster.getNamesystem(0).getBlockPoolId();    DatanodeDescriptor sourceDnDesc=NameNodeAdapter.getDatanode(cluster.getNamesystem(0),dn0.getDNRegistrationForBP(activeNNBPId));    DatanodeDescriptor destDnDesc=NameNodeAdapter.getDatanode(cluster.getNamesystem(0),dn1.getDNRegistrationForBP(activeNNBPId));    ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);
  try {    List<BPOfferService> bpos=ctx.datanode.getAllBpOs();    assertEquals(1,bpos.size());    BlockIterator iter=volume.newBlockIterator(ctx.bpids[0],"test");    assertEquals(ctx.bpids[0],iter.getBlockPoolId());    iter.setMaxStalenessMs(maxStaleness);    while (true) {      HashSet<ExtendedBlock> blocks=new HashSet<ExtendedBlock>();      for (int blockIdx=0; blockIdx < numFiles; blockIdx++) {        blocks.add(ctx.getFileBlock(0,blockIdx));      }      while (true) {        ExtendedBlock block=iter.nextBlock();        if (block == null) {          break;        }        blocksProcessed++;
      }      while (true) {        ExtendedBlock block=iter.nextBlock();        if (block == null) {          break;        }        blocksProcessed++;        LOG.info("BlockIterator for {} found block {}, blocksProcessed = {}",volume,block,blocksProcessed);        if (testedSave && (savedBlock == null)) {          savedBlock=block;        }        if (testedLoad && (loadedBlock == null)) {          loadedBlock=block;          assertEquals(savedBlock,loadedBlock);        }        boolean blockRemoved=blocks.remove(block);        assertTrue("Found unknown block " + block,blockRemoved);        if (blocksProcessed > (numFiles / 3)) {          if (!testedSave) {
        blocksProcessed++;        LOG.info("BlockIterator for {} found block {}, blocksProcessed = {}",volume,block,blocksProcessed);        if (testedSave && (savedBlock == null)) {          savedBlock=block;        }        if (testedLoad && (loadedBlock == null)) {          loadedBlock=block;          assertEquals(savedBlock,loadedBlock);        }        boolean blockRemoved=blocks.remove(block);        assertTrue("Found unknown block " + block,blockRemoved);        if (blocksProcessed > (numFiles / 3)) {          if (!testedSave) {            LOG.info("Processed {} blocks out of {}.  Saving iterator.",blocksProcessed,numFiles);            iter.save();            testedSave=true;            savedBlocksProcessed=blocksProcessed;
        if (testedLoad && (loadedBlock == null)) {          loadedBlock=block;          assertEquals(savedBlock,loadedBlock);        }        boolean blockRemoved=blocks.remove(block);        assertTrue("Found unknown block " + block,blockRemoved);        if (blocksProcessed > (numFiles / 3)) {          if (!testedSave) {            LOG.info("Processed {} blocks out of {}.  Saving iterator.",blocksProcessed,numFiles);            iter.save();            testedSave=true;            savedBlocksProcessed=blocksProcessed;          }        }        if (blocksProcessed > (numFiles / 2)) {          if (!testedRewind) {            LOG.info("Processed {} blocks out of {}.  Rewinding iterator.",blocksProcessed,numFiles);            iter.rewind();
  }  TestScanResultHandler.Info info=TestScanResultHandler.getInfo(ctx.volumes.get(0));synchronized (info) {    info.shouldRun=true;    info.notify();  }  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      TestScanResultHandler.Info info=TestScanResultHandler.getInfo(ctx.volumes.get(0));      int numFoundBlocks=0;      StringBuilder foundBlocksBld=new StringBuilder();      String prefix="";synchronized (info) {        for (        ExtendedBlock block : info.goodBlocks) {          assertTrue(expectedBlocks.contains(block));          numFoundBlocks++;          foundBlocksBld.append(prefix).append(block);
    assertEquals(5,info.blocksScanned);    info.shouldRun=false;  }  ctx.datanode.shutdown();  URI vURI=ctx.volumes.get(0).getStorageLocation().getUri();  File cursorPath=new File(new File(new File(new File(vURI),"current"),ctx.bpids[0]),"scanner.cursor");  assertTrue("Failed to find cursor save file in " + cursorPath.getAbsolutePath(),cursorPath.exists());  Set<ExtendedBlock> prevGoodBlocks=new HashSet<ExtendedBlock>();synchronized (info) {    info.sem=new Semaphore(4);    prevGoodBlocks.addAll(info.goodBlocks);    info.goodBlocks.clear();  }  ctx.cluster.restartDataNode(0);synchronized (info) {    info.shouldRun=true;    info.notify();
  conf.setLong(DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,100L);  conf.set(INTERNAL_VOLUME_SCANNER_SCAN_RESULT_HANDLER,TestScanResultHandler.class.getName());  final TestContext ctx=new TestContext(conf,3);  final int BYTES_SCANNED_PER_FILE=5;  int TOTAL_FILES=16;  ctx.createFiles(0,TOTAL_FILES,1);  final TestScanResultHandler.Info info=TestScanResultHandler.getInfo(ctx.volumes.get(0));synchronized (info) {    info.shouldRun=true;    info.notify();  }  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){synchronized (info) {        Statistics stats=ctx.blockScanner.getVolumeStats(ctx.volumes.get(0).getStorageID());        if (stats.scansSinceRestart < 3) {
  conf.setLong(INTERNAL_DFS_BLOCK_SCANNER_CURSOR_SAVE_INTERVAL_MS,0L);  final TestContext ctx=new TestContext(conf,1);  final int NUM_EXPECTED_BLOCKS=10;  ctx.createFiles(0,NUM_EXPECTED_BLOCKS,1);  final TestScanResultHandler.Info info=TestScanResultHandler.getInfo(ctx.volumes.get(0));  String storageID=ctx.volumes.get(0).getStorageID();synchronized (info) {    info.sem=new Semaphore(4);    info.shouldRun=true;    info.notify();  }  LOG.info("Waiting for the first 4 blocks to be scanned.");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){synchronized (info) {        if (info.blocksScanned >= 4) {
  ctx.createFiles(0,NUM_EXPECTED_BLOCKS,1);  final TestScanResultHandler.Info info=TestScanResultHandler.getInfo(ctx.volumes.get(0));  String storageID=ctx.volumes.get(0).getStorageID();synchronized (info) {    info.sem=new Semaphore(4);    info.shouldRun=true;    info.notify();  }  LOG.info("Waiting for the first 4 blocks to be scanned.");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){synchronized (info) {        if (info.blocksScanned >= 4) {          LOG.info("info = {}.  blockScanned has now reached 4.",info);          return true;        } else {
          LOG.info("info = {}.  Waiting for blockScanned to reach 4.",info);          return false;        }      }    }  },50,30000);synchronized (info) {    assertEquals("Expected 4 good blocks.",4,info.goodBlocks.size());    info.goodBlocks.clear();    assertEquals("Expected 4 blocksScanned",4,info.blocksScanned);    assertEquals("Did not expect bad blocks.",0,info.badBlocks.size());    info.blocksScanned=0;  }  ExtendedBlock first=ctx.getFileBlock(0,0);  ctx.datanode.getBlockScanner().markSuspectBlock(storageID,first);  info.sem.release(2);  LOG.info("Waiting for 2 more blocks to be scanned.");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){
      }    }  },50,30000);synchronized (info) {    assertEquals("Expected 4 good blocks.",4,info.goodBlocks.size());    info.goodBlocks.clear();    assertEquals("Expected 4 blocksScanned",4,info.blocksScanned);    assertEquals("Did not expect bad blocks.",0,info.badBlocks.size());    info.blocksScanned=0;  }  ExtendedBlock first=ctx.getFileBlock(0,0);  ctx.datanode.getBlockScanner().markSuspectBlock(storageID,first);  info.sem.release(2);  LOG.info("Waiting for 2 more blocks to be scanned.");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){synchronized (info) {        if (info.blocksScanned >= 2) {
synchronized (info) {        if (info.blocksScanned >= 2) {          LOG.info("info = {}.  blockScanned has now reached 2.",info);          return true;        } else {          LOG.info("info = {}.  Waiting for blockScanned to reach 2.",info);          return false;        }      }    }  },50,30000);synchronized (info) {    assertTrue("Expected block " + first + " to have been scanned.",info.goodBlocks.contains(first));    assertEquals(2,info.goodBlocks.size());    info.goodBlocks.clear();    assertEquals("Did not expect bad blocks.",0,info.badBlocks.size());    assertEquals(2,info.blocksScanned);    info.blocksScanned=0;
          return true;        } else {          LOG.info("info = {}.  Waiting for blockScanned to reach 2.",info);          return false;        }      }    }  },50,30000);synchronized (info) {    assertTrue("Expected block " + first + " to have been scanned.",info.goodBlocks.contains(first));    assertEquals(2,info.goodBlocks.size());    info.goodBlocks.clear();    assertEquals("Did not expect bad blocks.",0,info.badBlocks.size());    assertEquals(2,info.blocksScanned);    info.blocksScanned=0;  }  ctx.datanode.getBlockScanner().markSuspectBlock(storageID,first);  info.sem.release(10);  LOG.info("Waiting for 5 more blocks to be scanned.");
  ctx.createFiles(0,NUM_FILES,5);  MaterializedReplica unreachableReplica=ctx.getMaterializedReplica(0,1);  ExtendedBlock unreachableBlock=ctx.getFileBlock(0,1);  unreachableReplica.makeUnreachable();  final TestScanResultHandler.Info info=TestScanResultHandler.getInfo(ctx.volumes.get(0));  String storageID=ctx.volumes.get(0).getStorageID();synchronized (info) {    info.sem=new Semaphore(NUM_FILES);    info.shouldRun=true;    info.notify();  }  LOG.info("Waiting for the blocks to be scanned.");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){synchronized (info) {        if (info.blocksScanned >= NUM_FILES - 1) {
  unreachableReplica.makeUnreachable();  final TestScanResultHandler.Info info=TestScanResultHandler.getInfo(ctx.volumes.get(0));  String storageID=ctx.volumes.get(0).getStorageID();synchronized (info) {    info.sem=new Semaphore(NUM_FILES);    info.shouldRun=true;    info.notify();  }  LOG.info("Waiting for the blocks to be scanned.");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){synchronized (info) {        if (info.blocksScanned >= NUM_FILES - 1) {          LOG.info("info = {}.  blockScanned has now reached " + info.blocksScanned,info);          return true;        } else {
private void waitForRescan(final TestScanResultHandler.Info info,final int numExpectedBlocks) throws TimeoutException, InterruptedException {  LOG.info("Waiting for the first 1 blocks to be scanned.");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){synchronized (info) {        if (info.blocksScanned >= numExpectedBlocks) {
private Scheduler makeMockScheduler(long now){
  FSDataInputStream fis=null;  long totalRead=0;  try {    fis=fs.open(p);    if (dropBehind != null) {      fis.setDropBehind(dropBehind);    }    byte buf[]=new byte[8196];    while (length > 0) {      int amt=(length > buf.length) ? buf.length : (int)length;      int ret=fis.read(buf,0,amt);      if (ret == -1) {        return totalRead;      }      totalRead+=ret;      length-=ret;    }  } catch (  IOException e) {
private void doTest(String fileName,int fileLen,int deadNodeIndex) throws Exception {  assertTrue(fileLen > 0);  assertTrue(deadNodeIndex >= 0 && deadNodeIndex < numDNs);  Path file=new Path(fileName);  final byte[] data=StripedFileTestUtil.generateBytes(fileLen);  DFSTestUtil.writeFile(fs,file,data);  StripedFileTestUtil.waitBlockGroupsReported(fs,fileName);  final LocatedBlocks locatedBlocks=StripedFileTestUtil.getLocatedBlocks(file,fs);  final LocatedStripedBlock lastBlock=(LocatedStripedBlock)locatedBlocks.getLastLocatedBlock();  assertTrue(lastBlock.getLocations().length > deadNodeIndex);  final DataNode toCorruptDn=cluster.getDataNode(lastBlock.getLocations()[deadNodeIndex].getIpcPort());
  dn.data=Mockito.spy(data);  final int newVolumeCount=40;  List<Thread> addVolumeDelayedThreads=Collections.synchronizedList(new ArrayList<>());  AtomicBoolean addVolumeError=new AtomicBoolean(false);  AtomicBoolean listStorageError=new AtomicBoolean(false);  CountDownLatch addVolumeCompletionLatch=new CountDownLatch(newVolumeCount);  final Thread listStorageThread=new Thread(new Runnable(){    @Override public void run(){      while (addVolumeCompletionLatch.getCount() != newVolumeCount) {        int i=0;        while (i++ < 1000) {          try {            dn.getStorage().listStorageDirectories();          } catch (          Exception e) {            listStorageError.set(true);
            LOG.error("Error listing storage: " + e);          }        }      }    }  });  listStorageThread.start();  doAnswer(new Answer<Object>(){    @Override public Object answer(    InvocationOnMock invocationOnMock) throws Throwable {      final Random r=new Random();      Thread addVolThread=new Thread(new Runnable(){        @Override public void run(){          try {            r.setSeed(Time.now());            if (r.nextInt(10) > 4) {              int s=r.nextInt(10) + 1;              Thread.sleep(s * 100);            }            invocationOnMock.callRealMethod();          } catch (          Throwable throwable) {
    DataNode dn=cluster.getDataNodes().get(0);    for (int i=0; i < 100; i++) {      DFSTestUtil.writeFile(cluster.getFileSystem(),new Path("/foo" + String.valueOf(i) + ".txt"),"test content");    }    DataNodeTestUtils.triggerBlockReport(dn);    MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();    ObjectName mxbeanName=new ObjectName("Hadoop:service=DataNode,name=DataNodeInfo");    String bpActorInfo=(String)mbs.getAttribute(mxbeanName,"BPServiceActorInfo");    Assert.assertEquals(dn.getBPServiceActorInfo(),bpActorInfo);    LOG.info("bpActorInfo is " + bpActorInfo);    TypeReference<ArrayList<Map<String,String>>> typeRef=new TypeReference<ArrayList<Map<String,String>>>(){    };    ArrayList<Map<String,String>> bpActorInfoList=new ObjectMapper().readValue(bpActorInfo,typeRef);    int maxDataLength=Integer.valueOf(bpActorInfoList.get(0).get("maxDataLength"));    int confMaxDataLength=dn.getConf().getInt(CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH,CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);    int maxBlockReportSize=Integer.valueOf(bpActorInfoList.get(0).get("maxBlockReportSize"));
    for (int i=0; i < 100; i++) {      DFSTestUtil.writeFile(cluster.getFileSystem(),new Path("/foo" + String.valueOf(i) + ".txt"),"test content");    }    DataNodeTestUtils.triggerBlockReport(dn);    MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();    ObjectName mxbeanName=new ObjectName("Hadoop:service=DataNode,name=DataNodeInfo");    String bpActorInfo=(String)mbs.getAttribute(mxbeanName,"BPServiceActorInfo");    Assert.assertEquals(dn.getBPServiceActorInfo(),bpActorInfo);    LOG.info("bpActorInfo is " + bpActorInfo);    TypeReference<ArrayList<Map<String,String>>> typeRef=new TypeReference<ArrayList<Map<String,String>>>(){    };    ArrayList<Map<String,String>> bpActorInfoList=new ObjectMapper().readValue(bpActorInfo,typeRef);    int maxDataLength=Integer.valueOf(bpActorInfoList.get(0).get("maxDataLength"));    int confMaxDataLength=dn.getConf().getInt(CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH,CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);    int maxBlockReportSize=Integer.valueOf(bpActorInfoList.get(0).get("maxBlockReportSize"));    LOG.info("maxDataLength is " + maxDataLength);
    assertEquals(datanodes.size(),1);    final DataNode datanode=datanodes.get(0);    MetricsRecordBuilder rb=getMetrics(datanode.getMetrics().name());    final long LONG_FILE_LEN=1024 * 1024 * 10;    final long startWriteValue=getLongCounter("TotalWriteTime",rb);    final long startReadValue=getLongCounter("TotalReadTime",rb);    final AtomicInteger x=new AtomicInteger(0);    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){        x.getAndIncrement();        try {          DFSTestUtil.createFile(fs,new Path("/time.txt." + x.get()),LONG_FILE_LEN,(short)1,Time.monotonicNow());          DFSTestUtil.readFile(fs,new Path("/time.txt." + x.get()));          fs.delete(new Path("/time.txt." + x.get()),true);        } catch (        IOException ioe) {
  try {    cluster.waitActive();    NameNode nn1=cluster.getNameNode(0);    NameNode nn2=cluster.getNameNode(1);    assertNotNull("cannot create nn1",nn1);    assertNotNull("cannot create nn2",nn2);    String bpid1=FSImageTestUtil.getFSImage(nn1).getBlockPoolID();    String bpid2=FSImageTestUtil.getFSImage(nn2).getBlockPoolID();    String cid1=FSImageTestUtil.getFSImage(nn1).getClusterID();    String cid2=FSImageTestUtil.getFSImage(nn2).getClusterID();    int lv1=FSImageTestUtil.getFSImage(nn1).getLayoutVersion();    int lv2=FSImageTestUtil.getFSImage(nn2).getLayoutVersion();    int ns1=FSImageTestUtil.getFSImage(nn1).getNamespaceID();    int ns2=FSImageTestUtil.getFSImage(nn2).getNamespaceID();    assertNotSame("namespace ids should be different",ns1,ns2);
    cluster.waitActive();    NameNode nn1=cluster.getNameNode(0);    NameNode nn2=cluster.getNameNode(1);    assertNotNull("cannot create nn1",nn1);    assertNotNull("cannot create nn2",nn2);    String bpid1=FSImageTestUtil.getFSImage(nn1).getBlockPoolID();    String bpid2=FSImageTestUtil.getFSImage(nn2).getBlockPoolID();    String cid1=FSImageTestUtil.getFSImage(nn1).getClusterID();    String cid2=FSImageTestUtil.getFSImage(nn2).getClusterID();    int lv1=FSImageTestUtil.getFSImage(nn1).getLayoutVersion();    int lv2=FSImageTestUtil.getFSImage(nn2).getLayoutVersion();    int ns1=FSImageTestUtil.getFSImage(nn1).getNamespaceID();    int ns2=FSImageTestUtil.getFSImage(nn2).getNamespaceID();    assertNotSame("namespace ids should be different",ns1,ns2);    LOG.info("nn1: lv=" + lv1 + ";cid="+ cid1+ ";bpid="+ bpid1+ ";uri="+ nn1.getNameNodeAddress());
    int lv1=FSImageTestUtil.getFSImage(nn1).getLayoutVersion();    int lv2=FSImageTestUtil.getFSImage(nn2).getLayoutVersion();    int ns1=FSImageTestUtil.getFSImage(nn1).getNamespaceID();    int ns2=FSImageTestUtil.getFSImage(nn2).getNamespaceID();    assertNotSame("namespace ids should be different",ns1,ns2);    LOG.info("nn1: lv=" + lv1 + ";cid="+ cid1+ ";bpid="+ bpid1+ ";uri="+ nn1.getNameNodeAddress());    LOG.info("nn2: lv=" + lv2 + ";cid="+ cid2+ ";bpid="+ bpid2+ ";uri="+ nn2.getNameNodeAddress());    DataNode dn=cluster.getDataNodes().get(0);    final Map<String,Object> volInfos=dn.data.getVolumeInfoMap();    Assert.assertTrue("No volumes in the fsdataset",volInfos.size() > 0);    int i=0;    for (    Map.Entry<String,Object> e : volInfos.entrySet()) {      LOG.info("vol " + i++ + ") "+ e.getKey()+ ": "+ e.getValue());    }    assertEquals("number of volumes is wrong",cluster.getFsDatasetTestUtils(0).getDefaultNumOfDataDirs(),volInfos.size());    for (    BPOfferService bpos : dn.getAllBpOs()) {
@Test public void testFedSingleNN() throws IOException {  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).nameNodePort(9927).build();  try {    NameNode nn1=cluster.getNameNode();    assertNotNull("cannot create nn1",nn1);    String bpid1=FSImageTestUtil.getFSImage(nn1).getBlockPoolID();    String cid1=FSImageTestUtil.getFSImage(nn1).getClusterID();    int lv1=FSImageTestUtil.getFSImage(nn1).getLayoutVersion();
  try {    NameNode nn1=cluster.getNameNode();    assertNotNull("cannot create nn1",nn1);    String bpid1=FSImageTestUtil.getFSImage(nn1).getBlockPoolID();    String cid1=FSImageTestUtil.getFSImage(nn1).getClusterID();    int lv1=FSImageTestUtil.getFSImage(nn1).getLayoutVersion();    LOG.info("nn1: lv=" + lv1 + ";cid="+ cid1+ ";bpid="+ bpid1+ ";uri="+ nn1.getNameNodeAddress());    DataNode dn=cluster.getDataNodes().get(0);    final Map<String,Object> volInfos=dn.data.getVolumeInfoMap();    Assert.assertTrue("No volumes in the fsdataset",volInfos.size() > 0);    int i=0;    for (    Map.Entry<String,Object> e : volInfos.entrySet()) {      LOG.info("vol " + i++ + ") "+ e.getKey()+ ": "+ e.getValue());    }    assertEquals("number of volumes is wrong",cluster.getFsDatasetTestUtils(0).getDefaultNumOfDataDirs(),volInfos.size());    for (    BPOfferService bpos : dn.getAllBpOs()) {
@Test public void testClusterIdMismatch() throws Exception {  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)).build();  try {    cluster.waitActive();    DataNode dn=cluster.getDataNodes().get(0);    List<BPOfferService> bposs=dn.getAllBpOs();
@Test public void testClusterIdMismatch() throws Exception {  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(2)).build();  try {    cluster.waitActive();    DataNode dn=cluster.getDataNodes().get(0);    List<BPOfferService> bposs=dn.getAllBpOs();    LOG.info("dn bpos len (should be 2):" + bposs.size());    Assert.assertEquals("should've registered with two namenodes",bposs.size(),2);    cluster.addNameNode(conf,9938);    Thread.sleep(500);    bposs=dn.getAllBpOs();
    DataNode dn=cluster.getDataNodes().get(0);    List<BPOfferService> bposs=dn.getAllBpOs();    LOG.info("dn bpos len (should be 2):" + bposs.size());    Assert.assertEquals("should've registered with two namenodes",bposs.size(),2);    cluster.addNameNode(conf,9938);    Thread.sleep(500);    bposs=dn.getAllBpOs();    LOG.info("dn bpos len (should be 3):" + bposs.size());    Assert.assertEquals("should've registered with three namenodes",bposs.size(),3);    StartupOption.FORMAT.setClusterId("DifferentCID");    cluster.addNameNode(conf,9948);    NameNode nn4=cluster.getNameNode(3);    assertNotNull("cannot create nn4",nn4);    Thread.sleep(500);    bposs=dn.getAllBpOs();
private void deleteAndEnsureInTrash(Path pathToDelete,File blockFile,File trashFile) throws Exception {  assertTrue(blockFile.exists());  assertFalse(trashFile.exists());
private void startNewDataNodeWithDiskFailure(File badDataDir,boolean tolerated) throws Exception {  final File data5=new File(dataDir,"data5");  final String newDirs=badDataDir.toString() + "," + data5.toString();  final Configuration newConf=new Configuration(conf);  newConf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY,newDirs);
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());
private void verifyDataNodeVolumeMetrics(final FileSystem fs,final MiniDFSCluster cluster,final Path fileName) throws IOException {  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());
  List<DataNode> datanodes=cluster.getDataNodes();  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());
  DataNode datanode=datanodes.get(0);  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());
  final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());
  final FsVolumeSpi volume=datanode.getFSDataset().getVolume(block);  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());
  DataNodeVolumeMetrics metrics=volume.getMetrics();  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());
  MetricsRecordBuilder rb=getMetrics(volume.getMetrics().name());  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());
  assertCounter("TotalDataFileIos",metrics.getTotalDataFileIos(),rb);  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());
  LOG.info("TotalMetadataOperations : " + metrics.getTotalMetadataOperations());  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());
  LOG.info("TotalDataFileIos : " + metrics.getTotalDataFileIos());  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());
  LOG.info("TotalFileIoErrors : " + metrics.getTotalFileIoErrors());  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());  LOG.info("readIoMean : " + metrics.getReadIoMean());
  LOG.info("MetadataOperationSampleCount : " + metrics.getMetadataOperationSampleCount());  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());  LOG.info("readIoMean : " + metrics.getReadIoMean());  LOG.info("readIoStdDev : " + metrics.getReadIoStdDev());
  LOG.info("MetadataOperationMean : " + metrics.getMetadataOperationMean());  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());  LOG.info("readIoMean : " + metrics.getReadIoMean());  LOG.info("readIoStdDev : " + metrics.getReadIoStdDev());  LOG.info("writeIoSampleCount : " + metrics.getWriteIoSampleCount());
  LOG.info("MetadataFileIoStdDev : " + metrics.getMetadataOperationStdDev());  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());  LOG.info("readIoMean : " + metrics.getReadIoMean());  LOG.info("readIoStdDev : " + metrics.getReadIoStdDev());  LOG.info("writeIoSampleCount : " + metrics.getWriteIoSampleCount());  LOG.info("writeIoMean : " + metrics.getWriteIoMean());
  LOG.info("DataFileIoSampleCount : " + metrics.getDataFileIoSampleCount());  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());  LOG.info("readIoMean : " + metrics.getReadIoMean());  LOG.info("readIoStdDev : " + metrics.getReadIoStdDev());  LOG.info("writeIoSampleCount : " + metrics.getWriteIoSampleCount());  LOG.info("writeIoMean : " + metrics.getWriteIoMean());  LOG.info("writeIoStdDev : " + metrics.getWriteIoStdDev());
  LOG.info("DataFileIoMean : " + metrics.getDataFileIoMean());  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());  LOG.info("readIoMean : " + metrics.getReadIoMean());  LOG.info("readIoStdDev : " + metrics.getReadIoStdDev());  LOG.info("writeIoSampleCount : " + metrics.getWriteIoSampleCount());  LOG.info("writeIoMean : " + metrics.getWriteIoMean());  LOG.info("writeIoStdDev : " + metrics.getWriteIoStdDev());  LOG.info("fileIoErrorSampleCount : " + metrics.getFileIoErrorSampleCount());
  LOG.info("DataFileIoStdDev : " + metrics.getDataFileIoStdDev());  LOG.info("flushIoSampleCount : " + metrics.getFlushIoSampleCount());  LOG.info("flushIoMean : " + metrics.getFlushIoMean());  LOG.info("flushIoStdDev : " + metrics.getFlushIoStdDev());  LOG.info("syncIoSampleCount : " + metrics.getSyncIoSampleCount());  LOG.info("syncIoMean : " + metrics.getSyncIoMean());  LOG.info("syncIoStdDev : " + metrics.getSyncIoStdDev());  LOG.info("readIoSampleCount : " + metrics.getReadIoMean());  LOG.info("readIoMean : " + metrics.getReadIoMean());  LOG.info("readIoStdDev : " + metrics.getReadIoStdDev());  LOG.info("writeIoSampleCount : " + metrics.getWriteIoSampleCount());  LOG.info("writeIoMean : " + metrics.getWriteIoMean());  LOG.info("writeIoStdDev : " + metrics.getWriteIoStdDev());  LOG.info("fileIoErrorSampleCount : " + metrics.getFileIoErrorSampleCount());  LOG.info("fileIoErrorMean : " + metrics.getFileIoErrorMean());
@Test(timeout=60000) public void testDatanodeRegistrationRetry() throws Exception {  final DatanodeProtocolClientSideTranslatorPB namenode=mock(DatanodeProtocolClientSideTranslatorPB.class);  Mockito.doAnswer(new Answer<DatanodeRegistration>(){    int i=0;    @Override public DatanodeRegistration answer(    InvocationOnMock invocation) throws Throwable {      i++;      if (i > 1 && i < 5) {
        LOG.info("mockito exception " + i);        throw new EOFException("TestDatanodeProtocolRetryPolicy");      } else {        DatanodeRegistration dr=(DatanodeRegistration)invocation.getArguments()[0];        datanodeRegistration=new DatanodeRegistration(dr.getDatanodeUuid(),dr);        LOG.info("mockito succeeded " + datanodeRegistration);        return datanodeRegistration;      }    }  }).when(namenode).registerDatanode(Mockito.any(DatanodeRegistration.class));  when(namenode.versionRequest()).thenReturn(new NamespaceInfo(1,CLUSTER_ID,POOL_ID,1L));  Mockito.doAnswer(new Answer<HeartbeatResponse>(){    int i=0;    @Override public HeartbeatResponse answer(    InvocationOnMock invocation) throws Throwable {      i++;      HeartbeatResponse heartbeatResponse;      if (i == 1) {
 else {        DatanodeRegistration dr=(DatanodeRegistration)invocation.getArguments()[0];        datanodeRegistration=new DatanodeRegistration(dr.getDatanodeUuid(),dr);        LOG.info("mockito succeeded " + datanodeRegistration);        return datanodeRegistration;      }    }  }).when(namenode).registerDatanode(Mockito.any(DatanodeRegistration.class));  when(namenode.versionRequest()).thenReturn(new NamespaceInfo(1,CLUSTER_ID,POOL_ID,1L));  Mockito.doAnswer(new Answer<HeartbeatResponse>(){    int i=0;    @Override public HeartbeatResponse answer(    InvocationOnMock invocation) throws Throwable {      i++;      HeartbeatResponse heartbeatResponse;      if (i == 1) {        LOG.info("mockito heartbeatResponse registration " + i);        heartbeatResponse=new HeartbeatResponse(new DatanodeCommand[]{RegisterCommand.REGISTER},new NNHAStatusHeartbeat(HAServiceState.ACTIVE,1),null,ThreadLocalRandom.current().nextLong() | 1L);
private long truncateBlockFile() throws IOException {  try (AutoCloseableLock lock=fds.acquireDatasetLock()){    for (    ReplicaInfo b : FsDatasetTestUtil.getReplicas(fds,bpid)) {      File f=new File(b.getBlockURI());      File mf=new File(b.getMetadataURI());      if (f.exists() && f.length() != 0 && mf.exists()) {        FileOutputStream s=null;        FileChannel channel=null;        try {          s=new FileOutputStream(f);          channel=s.getChannel();          channel.truncate(0);
private long deleteBlockFile(){  try (AutoCloseableLock lock=fds.acquireDatasetLock()){    for (    ReplicaInfo b : FsDatasetTestUtil.getReplicas(fds,bpid)) {      File f=new File(b.getBlockURI());      File mf=new File(b.getMetadataURI());      if (f.exists() && mf.exists() && f.delete()) {
private long deleteMetaFile(){  try (AutoCloseableLock lock=fds.acquireDatasetLock()){    for (    ReplicaInfo b : FsDatasetTestUtil.getReplicas(fds,bpid)) {      if (b.metadataExists() && b.deleteMetadata()) {
      for (      FsVolumeSpi v : volumes) {        if (v.getStorageID().equals(b.getVolume().getStorageID())) {          continue;        }        File sourceBlock=new File(b.getBlockURI());        File sourceMeta=new File(b.getMetadataURI());        URI sourceRoot=b.getVolume().getStorageLocation().getUri();        URI destRoot=v.getStorageLocation().getUri();        String relativeBlockPath=sourceRoot.relativize(sourceBlock.toURI()).getPath();        String relativeMetaPath=sourceRoot.relativize(sourceMeta.toURI()).getPath();        File destBlock=new File(new File(destRoot).toString(),relativeBlockPath);        File destMeta=new File(new File(destRoot).toString(),relativeMetaPath);        destBlock.getParentFile().mkdirs();        FileUtils.copyFile(sourceBlock,destBlock);        FileUtils.copyFile(sourceMeta,destMeta);        if (destBlock.exists() && destMeta.exists()) {
        if (v.getStorageID().equals(b.getVolume().getStorageID())) {          continue;        }        File sourceBlock=new File(b.getBlockURI());        File sourceMeta=new File(b.getMetadataURI());        URI sourceRoot=b.getVolume().getStorageLocation().getUri();        URI destRoot=v.getStorageLocation().getUri();        String relativeBlockPath=sourceRoot.relativize(sourceBlock.toURI()).getPath();        String relativeMetaPath=sourceRoot.relativize(sourceMeta.toURI()).getPath();        File destBlock=new File(new File(destRoot).toString(),relativeBlockPath);        File destMeta=new File(new File(destRoot).toString(),relativeMetaPath);        destBlock.getParentFile().mkdirs();        FileUtils.copyFile(sourceBlock,destBlock);        FileUtils.copyFile(sourceMeta,destMeta);        if (destBlock.exists() && destMeta.exists()) {          LOG.info("Copied " + sourceBlock + " ==> "+ destBlock);
private long createBlockFile() throws IOException {  long id=getFreeBlockId();  try (FsDatasetSpi.FsVolumeReferences volumes=fds.getFsVolumeReferences()){    int numVolumes=volumes.size();    int index=rand.nextInt(numVolumes - 1);    File finalizedDir=((FsVolumeImpl)volumes.get(index)).getFinalizedDir(bpid);    File file=new File(finalizedDir,getBlockFile(id));    if (file.createNewFile()) {
private long createMetaFile() throws IOException {  long id=getFreeBlockId();  try (FsDatasetSpi.FsVolumeReferences refs=fds.getFsVolumeReferences()){    int numVolumes=refs.size();    int index=rand.nextInt(numVolumes - 1);    File finalizedDir=((FsVolumeImpl)refs.get(index)).getFinalizedDir(bpid);    File file=new File(finalizedDir,getMetaFile(id));    if (file.createNewFile()) {
private long createBlockMetaFile() throws IOException {  long id=getFreeBlockId();  try (FsDatasetSpi.FsVolumeReferences refs=fds.getFsVolumeReferences()){    int numVolumes=refs.size();    int index=rand.nextInt(numVolumes - 1);    File finalizedDir=((FsVolumeImpl)refs.get(index)).getFinalizedDir(bpid);    File file=new File(finalizedDir,getBlockFile(id));    if (file.createNewFile()) {
private long createBlockMetaFile() throws IOException {  long id=getFreeBlockId();  try (FsDatasetSpi.FsVolumeReferences refs=fds.getFsVolumeReferences()){    int numVolumes=refs.size();    int index=rand.nextInt(numVolumes - 1);    File finalizedDir=((FsVolumeImpl)refs.get(index)).getFinalizedDir(bpid);    File file=new File(finalizedDir,getBlockFile(id));    if (file.createNewFile()) {      LOG.info("Created block file " + file.getName());      String name1=file.getAbsolutePath() + ".l";      String name2=file.getAbsolutePath() + ".n";      file=new File(name1);      if (file.createNewFile()) {
  long id=getFreeBlockId();  try (FsDatasetSpi.FsVolumeReferences refs=fds.getFsVolumeReferences()){    int numVolumes=refs.size();    int index=rand.nextInt(numVolumes - 1);    File finalizedDir=((FsVolumeImpl)refs.get(index)).getFinalizedDir(bpid);    File file=new File(finalizedDir,getBlockFile(id));    if (file.createNewFile()) {      LOG.info("Created block file " + file.getName());      String name1=file.getAbsolutePath() + ".l";      String name2=file.getAbsolutePath() + ".n";      file=new File(name1);      if (file.createNewFile()) {        LOG.info("Created extraneous file " + name1);      }      file=new File(name2);      if (file.createNewFile()) {
    int index=rand.nextInt(numVolumes - 1);    File finalizedDir=((FsVolumeImpl)refs.get(index)).getFinalizedDir(bpid);    File file=new File(finalizedDir,getBlockFile(id));    if (file.createNewFile()) {      LOG.info("Created block file " + file.getName());      String name1=file.getAbsolutePath() + ".l";      String name2=file.getAbsolutePath() + ".n";      file=new File(name1);      if (file.createNewFile()) {        LOG.info("Created extraneous file " + name1);      }      file=new File(name2);      if (file.createNewFile()) {        LOG.info("Created extraneous file " + name2);      }      file=new File(finalizedDir,getMetaFile(id));      if (file.createNewFile()) {
    int retries=maxRetries;    while ((retries > 0) && ((ratio < 7f) || (ratio > 10f))) {      scanner=new DirectoryScanner(fds,conf);      ratio=runThrottleTest(blocks);      retries-=1;    }    LOG.info("RATIO: " + ratio);    assertTrue("Throttle is too restrictive",ratio <= 10f);    assertTrue("Throttle is too permissive",ratio >= 7f);    conf.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY,200);    ratio=0.0f;    retries=maxRetries;    while ((retries > 0) && ((ratio < 2.75f) || (ratio > 4.5f))) {      scanner=new DirectoryScanner(fds,conf);      ratio=runThrottleTest(blocks);      retries-=1;
    retries=maxRetries;    while ((retries > 0) && ((ratio < 2.75f) || (ratio > 4.5f))) {      scanner=new DirectoryScanner(fds,conf);      ratio=runThrottleTest(blocks);      retries-=1;    }    LOG.info("RATIO: " + ratio);    assertTrue("Throttle is too restrictive",ratio <= 4.5f);    assertTrue("Throttle is too permissive",ratio >= 2.75f);    conf.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THREADS_KEY,3);    conf.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY,100);    ratio=0.0f;    retries=maxRetries;    while ((retries > 0) && ((ratio < 7f) || (ratio > 10f))) {      scanner=new DirectoryScanner(fds,conf);      ratio=runThrottleTest(blocks);
        scanner=new DirectoryScanner(fds,conf);        scanner.setRetainDiffs(true);        final AtomicLong nowMs=new AtomicLong();        interruptor.schedule(new Runnable(){          @Override public void run(){            nowMs.set(Time.monotonicNow());            scanner.shutdown();          }        },2L,TimeUnit.SECONDS);        scanner.reconcile();        assertFalse(scanner.getRunStatus());        long finalMs=nowMs.get();        if (finalMs > 0) {          LOG.info("Scanner took " + (Time.monotonicNow() - finalMs) + "ms to shutdown");          assertTrue("Scanner took too long to shutdown",Time.monotonicNow() - finalMs < 1000L);        }        ratio=(float)scanner.timeWaitingMs.get() / scanner.timeRunningMs.get();
  Configuration conf=getDefaultConf();  conf.setLong(DFSConfigKeys.DFS_DATANODE_CACHE_REVOCATION_TIMEOUT_MS,250L);  conf.setLong(DFSConfigKeys.DFS_DATANODE_CACHE_REVOCATION_POLLING_MS,2L);  MiniDFSCluster cluster=null;  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();  cluster.waitActive();  DistributedFileSystem dfs=cluster.getFileSystem();  final String testFile="/test_file2";  DFSTestUtil.createFile(dfs,new Path(testFile),BLOCK_SIZE,(short)1,0xcafe);  dfs.addCachePool(new CachePoolInfo("pool"));  long cacheDirectiveId=dfs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("pool").setPath(new Path(testFile)).setReplication((short)1).build());  FsDatasetSpi<?> fsd=cluster.getDataNodes().get(0).getFSDataset();  DFSTestUtil.verifyExpectedCacheUsage(BLOCK_SIZE,1,fsd);  FSDataInputStream in=dfs.open(new Path(testFile));  ByteBuffer buf=in.read(null,BLOCK_SIZE,EnumSet.noneOf(ReadOption.class));
  conf.setLong(DFSConfigKeys.DFS_DATANODE_CACHE_REVOCATION_POLLING_MS,2L);  MiniDFSCluster cluster=null;  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();  cluster.waitActive();  DistributedFileSystem dfs=cluster.getFileSystem();  final String testFile="/test_file2";  DFSTestUtil.createFile(dfs,new Path(testFile),BLOCK_SIZE,(short)1,0xcafe);  dfs.addCachePool(new CachePoolInfo("pool"));  long cacheDirectiveId=dfs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("pool").setPath(new Path(testFile)).setReplication((short)1).build());  FsDatasetSpi<?> fsd=cluster.getDataNodes().get(0).getFSDataset();  DFSTestUtil.verifyExpectedCacheUsage(BLOCK_SIZE,1,fsd);  FSDataInputStream in=dfs.open(new Path(testFile));  ByteBuffer buf=in.read(null,BLOCK_SIZE,EnumSet.noneOf(ReadOption.class));  LOG.info("removing cache directive {}",cacheDirectiveId);  dfs.removeCacheDirective(cacheDirectiveId);
@Override protected void sendBlockReports(DatanodeRegistration dnR,String poolId,StorageBlockReport[] reports) throws IOException {
private static void createProvidedReplicas(Configuration conf){  long numReplicas=(long)Math.ceil((double)FILE_LEN / BLK_LEN);  File providedFile=new File(BASE_DIR,FILE_NAME);  replicas=new ArrayList<ProvidedReplica>();
private static LocalReplicaInPipeline getReplica(final DataNode datanode,final String bpid,final ReplicaState expectedState) throws InterruptedException {  final Collection<ReplicaInfo> replicas=FsDatasetTestUtil.getReplicas(datanode.getFSDataset(),bpid);  for (int i=0; i < 5 && replicas.size() == 0; i++) {
@Test public void testTransferRbw() throws Exception {  final HdfsConfiguration conf=new HdfsConfiguration();  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION).build();  try {    cluster.waitActive();    final DistributedFileSystem fs=cluster.getFileSystem();    final Path p=new Path("/foo");    final int size=(1 << 16) + RAN.nextInt(1 << 16);
    final FSDataOutputStream out=fs.create(p,REPLICATION);    final byte[] bytes=new byte[1024];    for (int remaining=size; remaining > 0; ) {      RAN.nextBytes(bytes);      final int len=bytes.length < remaining ? bytes.length : remaining;      out.write(bytes,0,len);      out.hflush();      remaining-=len;    }    final ReplicaBeingWritten oldrbw;    final DataNode newnode;    final DatanodeInfo newnodeinfo;    final String bpid=cluster.getNamesystem().getBlockPoolId();{      final DataNode oldnode=cluster.getDataNodes().get(0);      Assert.assertNull(oldnode.xserver.getWriteThrottler());      oldrbw=getRbw(oldnode,bpid);
@Test(timeout=10000) public void testCheckAllVolumes() throws Exception {  LOG.info("Executing {}",testName.getMethodName());  final List<FsVolumeSpi> volumes=makeVolumes(NUM_VOLUMES,expectedVolumeHealth);  final FsDatasetSpi<FsVolumeSpi> dataset=makeDataset(volumes);  final DatasetVolumeChecker checker=new DatasetVolumeChecker(new HdfsConfiguration(),new FakeTimer());  checker.setDelegateChecker(new DummyChecker());  Set<FsVolumeSpi> failedVolumes=checker.checkAllVolumes(dataset);
protected final LocatedBlocks ensureFileReplicasOnStorageType(Path path,StorageType storageType) throws IOException, TimeoutException, InterruptedException {
public static void initCacheManipulator(){  NativeIO.POSIX.setCacheManipulator(new NativeIO.POSIX.CacheManipulator(){    @Override public void mlock(    String identifier,    ByteBuffer mmap,    long length) throws IOException {
  BlockReaderTestUtil.enableHdfsCachingTracing();  Assert.assertEquals(0,CACHE_CAPACITY % BLOCK_SIZE);  assertEquals(CACHE_CAPACITY,cacheManager.getCacheCapacity());  assertEquals(0L,cacheManager.getMemCacheCapacity());  final Path testFile=new Path("/testFile");  final long testFileLen=maxCacheBlocksNum * BLOCK_SIZE;  DFSTestUtil.createFile(fs,testFile,testFileLen,(short)1,0xbeef);  List<ExtendedBlockId> blockKeys=getExtendedBlockId(testFile,testFileLen);  fs.addCachePool(new CachePoolInfo("testPool"));  final long cacheDirectiveId=fs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("testPool").setPath(testFile).setReplication((short)1).build());  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());      long blocksCached=MetricsAsserts.getLongCounter("BlocksCached",dnMetrics);      if (blocksCached != maxCacheBlocksNum) {
    } else {      fail("The cache path is not the expected one: " + cachePath);    }  }  final Path smallTestFile=new Path("/smallTestFile");  final long smallTestFileLen=BLOCK_SIZE;  DFSTestUtil.createFile(fs,smallTestFile,smallTestFileLen,(short)1,0xbeef);  final long smallFileCacheDirectiveId=fs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("testPool").setPath(smallTestFile).setReplication((short)1).build());  Thread.sleep(10000);  MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());  long blocksCached=MetricsAsserts.getLongCounter("BlocksCached",dnMetrics);  assertTrue(blocksCached == maxCacheBlocksNum);  assertEquals(blockKeyToVolume.size(),maxCacheBlocksNum);  assertTrue(blockKeyToVolume.keySet().containsAll(blockKeys));  fs.removeCacheDirective(smallFileCacheDirectiveId);  fs.removeCacheDirective(cacheDirectiveId);  GenericTestUtils.waitFor(new Supplier<Boolean>(){
    }  }  final Path smallTestFile=new Path("/smallTestFile");  final long smallTestFileLen=BLOCK_SIZE;  DFSTestUtil.createFile(fs,smallTestFile,smallTestFileLen,(short)1,0xbeef);  final long smallFileCacheDirectiveId=fs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("testPool").setPath(smallTestFile).setReplication((short)1).build());  Thread.sleep(10000);  MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());  long blocksCached=MetricsAsserts.getLongCounter("BlocksCached",dnMetrics);  assertTrue(blocksCached == maxCacheBlocksNum);  assertEquals(blockKeyToVolume.size(),maxCacheBlocksNum);  assertTrue(blockKeyToVolume.keySet().containsAll(blockKeys));  fs.removeCacheDirective(smallFileCacheDirectiveId);  fs.removeCacheDirective(cacheDirectiveId);  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());
@Test(timeout=600000) public void testCacheAndUncacheBlockWithRetries() throws Exception {  NativeIO.POSIX.setCacheManipulator(new NoMlockCacheManipulator(){    private final Set<String> seenIdentifiers=new HashSet<String>();    @Override public void mlock(    String identifier,    ByteBuffer mmap,    long length) throws IOException {      if (seenIdentifiers.contains(identifier)) {
  final int NUM_BLOCKS=5;  DFSTestUtil.verifyExpectedCacheUsage(0,0,fsd);  final Path testFile=new Path("/testCacheBlock");  final long testFileLen=BLOCK_SIZE * NUM_BLOCKS;  DFSTestUtil.createFile(fs,testFile,testFileLen,(short)1,0xABBAl);  HdfsBlockLocation[] locs=(HdfsBlockLocation[])fs.getFileBlockLocations(testFile,0,testFileLen);  assertEquals("Unexpected number of blocks",NUM_BLOCKS,locs.length);  final long[] blockSizes=getBlockSizes(locs);  final long cacheCapacity=fsd.getCacheCapacity();  long cacheUsed=fsd.getCacheUsed();  long current=0;  assertEquals("Unexpected cache capacity",CACHE_CAPACITY,cacheCapacity);  assertEquals("Unexpected amount of cache used",current,cacheUsed);  NativeIO.POSIX.setCacheManipulator(new NoMlockCacheManipulator(){    @Override public void mlock(    String identifier,    ByteBuffer mmap,    long length) throws IOException {
  final int TOTAL_BLOCKS_PER_CACHE=Ints.checkedCast(CACHE_CAPACITY / BLOCK_SIZE);  BlockReaderTestUtil.enableHdfsCachingTracing();  Assert.assertEquals(0,CACHE_CAPACITY % BLOCK_SIZE);  final Path SMALL_FILE=new Path("/smallFile");  DFSTestUtil.createFile(fs,SMALL_FILE,BLOCK_SIZE,(short)1,0xcafe);  final Path BIG_FILE=new Path("/bigFile");  DFSTestUtil.createFile(fs,BIG_FILE,TOTAL_BLOCKS_PER_CACHE * BLOCK_SIZE,(short)1,0xbeef);  final DistributedFileSystem dfs=cluster.getFileSystem();  dfs.addCachePool(new CachePoolInfo("pool"));  final long bigCacheDirectiveId=dfs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("pool").setPath(BIG_FILE).setReplication((short)1).build());  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());      long blocksCached=MetricsAsserts.getLongCounter("BlocksCached",dnMetrics);      if (blocksCached != TOTAL_BLOCKS_PER_CACHE) {
    }  },1000,30000);  final long shortCacheDirectiveId=dfs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("pool").setPath(SMALL_FILE).setReplication((short)1).build());  Thread.sleep(10000);  MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());  Assert.assertEquals(TOTAL_BLOCKS_PER_CACHE,MetricsAsserts.getLongCounter("BlocksCached",dnMetrics));  dfs.removeCacheDirective(bigCacheDirectiveId);  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      RemoteIterator<CacheDirectiveEntry> iter;      try {        iter=dfs.listCacheDirectives(new CacheDirectiveInfo.Builder().build());        CacheDirectiveEntry entry;        do {          entry=iter.next();        } while (entry.getInfo().getId() != shortCacheDirectiveId);
  final long shortCacheDirectiveId=dfs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("pool").setPath(SMALL_FILE).setReplication((short)1).build());  Thread.sleep(10000);  MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());  Assert.assertEquals(TOTAL_BLOCKS_PER_CACHE,MetricsAsserts.getLongCounter("BlocksCached",dnMetrics));  dfs.removeCacheDirective(bigCacheDirectiveId);  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      RemoteIterator<CacheDirectiveEntry> iter;      try {        iter=dfs.listCacheDirectives(new CacheDirectiveInfo.Builder().build());        CacheDirectiveEntry entry;        do {          entry=iter.next();        } while (entry.getInfo().getId() != shortCacheDirectiveId);        if (entry.getStats().getFilesCached() != 1) {
  final int numExistingVolumes=getNumVolumes();  final int totalVolumes=numNewVolumes + numExistingVolumes;  Set<String> expectedVolumes=new HashSet<String>();  List<NamespaceInfo> nsInfos=Lists.newArrayList();  for (  String bpid : BLOCK_POOL_IDS) {    nsInfos.add(new NamespaceInfo(0,CLUSTER_ID,bpid,1));  }  for (int i=0; i < numNewVolumes; i++) {    String path=BASE_DIR + "/newData" + i;    String pathUri=new Path(path).toUri().toString();    expectedVolumes.add(new File(pathUri).getAbsolutePath());    StorageLocation loc=StorageLocation.parse(pathUri);    Storage.StorageDirectory sd=createStorageDirectory(new File(path),conf);    DataStorage.VolumeBuilder builder=new DataStorage.VolumeBuilder(storage,sd);    when(storage.prepareVolume(eq(datanode),eq(loc),anyList())).thenReturn(builder);    dataset.addVolume(loc,nsInfos);
    String path=BASE_DIR + "/newData" + i;    String pathUri=new Path(path).toUri().toString();    expectedVolumes.add(new File(pathUri).getAbsolutePath());    StorageLocation loc=StorageLocation.parse(pathUri);    Storage.StorageDirectory sd=createStorageDirectory(new File(path),conf);    DataStorage.VolumeBuilder builder=new DataStorage.VolumeBuilder(storage,sd);    when(storage.prepareVolume(eq(datanode),eq(loc),anyList())).thenReturn(builder);    dataset.addVolume(loc,nsInfos);    LOG.info("expectedVolumes " + i + " is "+ new File(pathUri).getAbsolutePath());  }  assertEquals(totalVolumes,getNumVolumes());  assertEquals(totalVolumes,dataset.storageMap.size());  Set<String> actualVolumes=new HashSet<String>();  try (FsDatasetSpi.FsVolumeReferences volumes=dataset.getFsVolumeReferences()){    for (int i=0; i < numNewVolumes; i++) {      String volumeName=volumes.get(numExistingVolumes + i).toString();
        volRemoveStartedLatch.await();      } catch (      Exception e) {        LOG.info("Unexpected exception when waiting for vol removal:",e);      }      LOG.info("Getting block report");      dataset.getBlockReports(eb.getBlockPoolId());      LOG.info("Successfully received block report");      blockReportReceivedLatch.countDown();    }  }class ResponderThread extends Thread {    public void run(){      try (ReplicaHandler replica=dataset.createRbw(StorageType.DEFAULT,null,eb,false)){        LOG.info("CreateRbw finished");        startFinalizeLatch.countDown();        try {          Thread.sleep(1000);        } catch (        InterruptedException ie) {
    public void run(){      try (ReplicaHandler replica=dataset.createRbw(StorageType.DEFAULT,null,eb,false)){        LOG.info("CreateRbw finished");        startFinalizeLatch.countDown();        try {          Thread.sleep(1000);        } catch (        InterruptedException ie) {          LOG.info("Ignoring ",ie);        }        blockReportReceivedLatch.await();        dataset.finalizeBlock(eb,false);        LOG.info("FinalizeBlock finished");      } catch (      Exception e) {        LOG.warn("Exception caught. This should not affect the test",e);      }    }  }class VolRemoveThread extends Thread {    public void run(){
        startFinalizeLatch.countDown();        try {          Thread.sleep(1000);        } catch (        InterruptedException ie) {          LOG.info("Ignoring ",ie);        }        blockReportReceivedLatch.await();        dataset.finalizeBlock(eb,false);        LOG.info("FinalizeBlock finished");      } catch (      Exception e) {        LOG.warn("Exception caught. This should not affect the test",e);      }    }  }class VolRemoveThread extends Thread {    public void run(){      Set<StorageLocation> volumesToRemove=new HashSet<>();      try {        volumesToRemove.add(dataset.getVolume(eb).getStorageLocation());
          Thread.sleep(1000);        } catch (        InterruptedException ie) {          LOG.info("Ignoring ",ie);        }        blockReportReceivedLatch.await();        dataset.finalizeBlock(eb,false);        LOG.info("FinalizeBlock finished");      } catch (      Exception e) {        LOG.warn("Exception caught. This should not affect the test",e);      }    }  }class VolRemoveThread extends Thread {    public void run(){      Set<StorageLocation> volumesToRemove=new HashSet<>();      try {        volumesToRemove.add(dataset.getVolume(eb).getStorageLocation());      } catch (      Exception e) {        LOG.info("Problem preparing volumes to remove: ",e);
@Test(timeout=30000) public void testMoveBlockFailure(){  MiniDFSCluster cluster=null;  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storageTypes(new StorageType[]{StorageType.DISK,StorageType.DISK}).storagesPerDatanode(2).build();    FileSystem fs=cluster.getFileSystem();    DataNode dataNode=cluster.getDataNodes().get(0);    Path filePath=new Path("testData");    DFSTestUtil.createFile(fs,filePath,100,(short)1,0);    ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,filePath);    FsDatasetImpl fsDataSetImpl=(FsDatasetImpl)dataNode.getFSDataset();    ReplicaInfo newReplicaInfo=createNewReplicaObj(block,fsDataSetImpl);    FSDataOutputStream out=fs.append(filePath,(short)1);    out.write(100);    out.hflush();
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storageTypes(new StorageType[]{StorageType.DISK,StorageType.DISK}).storagesPerDatanode(2).build();    FileSystem fs=cluster.getFileSystem();    DataNode dataNode=cluster.getDataNodes().get(0);    Path filePath=new Path("testData");    DFSTestUtil.createFile(fs,filePath,100,(short)1,0);    ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,filePath);    FsDatasetImpl fsDataSetImpl=(FsDatasetImpl)dataNode.getFSDataset();    ReplicaInfo newReplicaInfo=createNewReplicaObj(block,fsDataSetImpl);    FSDataOutputStream out=fs.append(filePath,(short)1);    out.write(100);    out.hflush();    LOG.info("GenerationStamp of old replica: {}",block.getGenerationStamp());    LOG.info("GenerationStamp of new replica: {}",fsDataSetImpl.getReplicaInfo(block.getBlockPoolId(),newReplicaInfo.getBlockId()).getGenerationStamp());    LambdaTestUtils.intercept(IOException.class,"Generation Stamp " + "should be monotonically increased.",() -> fsDataSetImpl.finalizeNewReplica(newReplicaInfo,block));  } catch (  Exception ex) {
@Test public void testConcurrentRead() throws Exception {  getClusterBuilder().setRamDiskReplicaCapacity(2).build();  final String METHOD_NAME=GenericTestUtils.getMethodName();  final Path path1=new Path("/" + METHOD_NAME + ".dat");  final int SEED=0xFADED;  final int NUM_TASKS=5;  makeRandomTestFile(path1,BLOCK_SIZE,true,SEED);  ensureFileReplicasOnStorageType(path1,RAM_DISK);  final CountDownLatch latch=new CountDownLatch(NUM_TASKS);  final AtomicBoolean testFailed=new AtomicBoolean(false);  Runnable readerRunnable=new Runnable(){    @Override public void run(){      try {        Assert.assertTrue(verifyReadRandomFile(path1,BLOCK_SIZE,SEED));      } catch (      Throwable e) {
private void waitForLockedBytesUsed(final FsDatasetSpi<?> fsd,final long expectedLockedBytes) throws TimeoutException, InterruptedException {  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      long cacheUsed=fsd.getCacheUsed();
@Test(timeout=60000) public void testCacheRecovery() throws Exception {  final int cacheBlocksNum=Ints.checkedCast(CACHE_AMOUNT / BLOCK_SIZE);  BlockReaderTestUtil.enableHdfsCachingTracing();  Assert.assertEquals(0,CACHE_AMOUNT % BLOCK_SIZE);  final Path testFile=new Path("/testFile");  final long testFileLen=cacheBlocksNum * BLOCK_SIZE;  DFSTestUtil.createFile(fs,testFile,testFileLen,(short)1,0xbeef);  List<ExtendedBlockId> blockKeys=getExtendedBlockId(testFile,testFileLen);  fs.addCachePool(new CachePoolInfo("testPool"));  final long cacheDirectiveId=fs.addCacheDirective(new CacheDirectiveInfo.Builder().setPool("testPool").setPath(testFile).setReplication((short)1).build());  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      MetricsRecordBuilder dnMetrics=getMetrics(dn.getMetrics().name());      long blocksCached=MetricsAsserts.getLongCounter("BlocksCached",dnMetrics);      if (blocksCached != cacheBlocksNum) {
@Test public void testProvidedVolumeImpl() throws IOException {  assertEquals(NUM_LOCAL_INIT_VOLUMES + NUM_PROVIDED_INIT_VOLUMES,getNumVolumes());  assertEquals(NUM_PROVIDED_INIT_VOLUMES,providedVolumes.size());  assertEquals(0,dataset.getNumFailedVolumes());  for (int i=0; i < providedVolumes.size(); i++) {    assertEquals(DFSConfigKeys.DFS_PROVIDER_STORAGEUUID_DEFAULT,providedVolumes.get(i).getStorageID());    assertEquals(StorageType.PROVIDED,providedVolumes.get(i).getStorageType());    long space=providedVolumes.get(i).getBlockPoolUsed(BLOCK_POOL_IDS[CHOSEN_BP_ID]);    assertEquals(spaceUsed,space);    assertEquals(NUM_PROVIDED_BLKS,providedVolumes.get(i).getNumBlocks());    providedVolumes.get(i).shutdownBlockPool(BLOCK_POOL_IDS[1 - CHOSEN_BP_ID],null);    try {      assertEquals(0,providedVolumes.get(i).getBlockPoolUsed(BLOCK_POOL_IDS[1 - CHOSEN_BP_ID]));      assertTrue(false);    } catch (    IOException e) {
  try (FSDataOutputStream fout=fs.create(file)){    fout.write(data);  }   PathHandle pathHandle=fs.getPathHandle(fs.getFileStatus(file),Options.HandleOpt.changed(true),Options.HandleOpt.moved(true));  FinalizedProvidedReplica replica=new FinalizedProvidedReplica(0,file.toUri(),0,chunkSize,0,pathHandle,null,conf,fs);  byte[] content=new byte[chunkSize];  IOUtils.readFully(replica.getDataInputStream(0),content,0,chunkSize);  assertArrayEquals(data,content);  fs.rename(file,new Path("/testfile.1"));  IOUtils.readFully(replica.getDataInputStream(0),content,0,chunkSize);  assertArrayEquals(data,content);  replica.setPathHandle(null);  try {    replica.getDataInputStream(0);    fail("Expected an exception");  } catch (  IOException e) {
  DataNodeFaultInjector.set(new DataNodeFaultInjector(){    private int tries=0;    @Override public void failMirrorConnection() throws IOException {      if (tries++ == 0) {        throw new IOException("Failing Mirror for space reservation");      }    }  });  FSDataOutputStream os=fs.create(file,replication);  os.write(new byte[1]);  os.close();  cluster.triggerBlockReports();  for (  final DataNode dn : cluster.getDataNodes()) {    try (FsDatasetSpi.FsVolumeReferences volumes=dn.getFSDataset().getFsVolumeReferences()){      final FsVolumeImpl volume=(FsVolumeImpl)volumes.get(0);      GenericTestUtils.waitFor(new Supplier<Boolean>(){        @Override public Boolean get(){
private void checkReservedSpace(final long expectedReserved) throws TimeoutException, InterruptedException, IOException {  for (  final DataNode dn : cluster.getDataNodes()) {    try (FsDatasetSpi.FsVolumeReferences volumes=dn.getFSDataset().getFsVolumeReferences()){      final FsVolumeImpl volume=(FsVolumeImpl)volumes.get(0);      GenericTestUtils.waitFor(new Supplier<Boolean>(){        @Override public Boolean get(){
  final String methodName=GenericTestUtils.getMethodName();  final Path file=new Path("/" + methodName + ".01.dat");  FSDataOutputStream os=fs.create(file,replication);  os.write(new byte[8192]);  os.hflush();  os.close();  HdfsBlockLocation blockLocation=(HdfsBlockLocation)fs.getClient().getBlockLocations(file.toString(),0,BLOCK_SIZE)[0];  LocatedBlock lastBlock=blockLocation.getLocatedBlock();  cluster.stopDataNode(lastBlock.getLocations()[2].getName());  try {    os=fs.append(file);    DFSTestUtil.setPipeline((DFSOutputStream)os.getWrappedStream(),lastBlock);    os.writeBytes("hi");    os.hsync();  } catch (  IOException e) {
public void injectFastNodesSamples(DataNodePeerMetrics peerMetrics){  for (int nodeIndex=0; nodeIndex < MIN_OUTLIER_DETECTION_PEERS; ++nodeIndex) {    final String nodeName="FastNode-" + nodeIndex;
@Test public void testOutliersFromTestMatrix(){  for (  Map.Entry<Map<String,Double>,Set<String>> entry : outlierTestMatrix.entrySet()) {
  final int dataNodeIndex=0;  final long cap=blockSize * 2L * blockCount;  conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);  conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,blockSize);  final MiniDFSCluster cluster=new ClusterBuilder().setBlockCount(blockCount).setBlockSize(blockSize).setDiskCount(diskCount).setNumDatanodes(dataNodeCount).setConf(conf).setCapacities(new long[]{cap,cap}).build();  try {    DataNode node=cluster.getDataNodes().get(dataNodeIndex);    final FsDatasetSpi<?> fsDatasetSpy=Mockito.spy(node.getFSDataset());    DiskBalancerWorkItem item=Mockito.spy(new DiskBalancerWorkItem());    Mockito.doReturn((long)10).when(item).getBandwidth();    doAnswer(new Answer<Object>(){      public Object answer(      InvocationOnMock invocation){        try {          node.getFSDataset().moveBlockAcrossVolumes((ExtendedBlock)invocation.getArguments()[0],(FsVolumeSpi)invocation.getArguments()[1]);        } catch (        Exception e) {
    Path dir=new Path("/testMoverWithPinnedBlocks");    dfs.mkdirs(dir);    dfs.setStoragePolicy(dir,"HOT");    final FSDataOutputStream out=dfs.create(new Path(file));    byte[] fileData=StripedFileTestUtil.generateBytes(DEFAULT_BLOCK_SIZE * 3);    out.write(fileData);    out.close();    LocatedBlock lb=dfs.getClient().getLocatedBlocks(file,0).get(0);    StorageType[] storageTypes=lb.getStorageTypes();    for (    StorageType storageType : storageTypes) {      Assert.assertTrue(StorageType.DISK == storageType);    }    StorageType[][] newtypes=new StorageType[][]{{StorageType.SSD}};    startAdditionalDNs(conf,1,newtypes,cluster);    for (int i=0; i < cluster.getDataNodes().size(); i++) {      DataNode dn=cluster.getDataNodes().get(i);
  InetSocketAddress[] favoredNodes=new InetSocketAddress[2];  int j=0;  for (int i=dataNodes.size() - 1; i >= 2; i--) {    favoredNodes[j++]=dataNodes.get(i).getXferAddress();  }  final String file="/parent/testMoverFailedRetryWithPinnedBlocks2";  final FSDataOutputStream out=dfs.create(new Path(file),FsPermission.getDefault(),true,DEFAULT_BLOCK_SIZE,(short)2,DEFAULT_BLOCK_SIZE,null,favoredNodes);  byte[] fileData=StripedFileTestUtil.generateBytes(DEFAULT_BLOCK_SIZE * 2);  out.write(fileData);  out.close();  LocatedBlocks locatedBlocks=dfs.getClient().getLocatedBlocks(file,0);  Assert.assertEquals("Wrong block count",2,locatedBlocks.locatedBlockCount());  LocatedBlock lb=locatedBlocks.get(0);  DatanodeInfo datanodeInfo=lb.getLocations()[0];  for (  DataNode dn : cluster.getDataNodes()) {    if (dn.getDatanodeId().getDatanodeUuid().equals(datanodeInfo.getDatanodeUuid())) {
static void banner(String string){
  NamespaceScheme nsScheme=new NamespaceScheme(Arrays.asList(fooDir),null,BLOCK_SIZE,null,policyMap);  ClusterScheme clusterScheme=new ClusterScheme(DEFAULT_CONF,NUM_DATANODES,REPL,genStorageTypes(NUM_DATANODES),null);  MigrationTest test=new MigrationTest(clusterScheme,nsScheme);  test.setupCluster();  banner("writing to file /foo/bar");  final Path barFile=new Path(fooDir,"bar");  DFSTestUtil.createFile(test.dfs,barFile,BLOCK_SIZE,(short)1,0L);  FSDataOutputStream out=test.dfs.append(barFile);  out.writeBytes("hello, ");  ((DFSOutputStream)out.getWrappedStream()).hsync();  try {    banner("start data migration");    test.setStoragePolicy();    test.migrate(ExitStatus.SUCCESS);    LocatedBlocks lbs=test.dfs.getClient().getLocatedBlocks(barFile.toString(),BLOCK_SIZE);
  ((DFSOutputStream)out.getWrappedStream()).hsync();  try {    banner("start data migration");    test.setStoragePolicy();    test.migrate(ExitStatus.SUCCESS);    LocatedBlocks lbs=test.dfs.getClient().getLocatedBlocks(barFile.toString(),BLOCK_SIZE);    LOG.info("Locations: " + lbs);    List<LocatedBlock> blks=lbs.getLocatedBlocks();    Assert.assertEquals(1,blks.size());    Assert.assertEquals(1,blks.get(0).getLocations().length);    banner("finish the migration, continue writing");    out.writeBytes("world!");    ((DFSOutputStream)out.getWrappedStream()).hsync();    IOUtils.cleanupWithLogger(LOG,out);    lbs=test.dfs.getClient().getLocatedBlocks(barFile.toString(),BLOCK_SIZE);
private void waitForAllReplicas(int expectedReplicaNum,Path file,DistributedFileSystem dfs,int retryCount) throws Exception {
private void setVolumeFull(DataNode dn,StorageType type){  try (FsDatasetSpi.FsVolumeReferences refs=dn.getFSDataset().getFsVolumeReferences()){    for (    FsVolumeSpi fvs : refs) {      FsVolumeImpl volume=(FsVolumeImpl)fvs;      if (volume.getStorageType() == type) {
public static void assertNNHasCheckpoints(MiniDFSCluster cluster,int nnIdx,List<Integer> txids){  for (  File nameDir : getNameNodeCurrentDirs(cluster,nnIdx)) {    LOG.info("examining name dir with files: " + Joiner.on(",").join(nameDir.listFiles()));
public static void logStorageContents(Logger log,NNStorage storage){  log.info("current storages and corresponding sizes:");  for (  StorageDirectory sd : storage.dirIterable(null)) {    File curDir=sd.getCurrentDir();
public static void logStorageContents(Logger log,NNStorage storage){  log.info("current storages and corresponding sizes:");  for (  StorageDirectory sd : storage.dirIterable(null)) {    File curDir=sd.getCurrentDir();    log.info("In directory " + curDir);    File[] files=curDir.listFiles();    Arrays.sort(files);    for (    File f : files) {
static void setNameNodeLoggingLevel(Level logLevel){
@Test public void testRetryAddBlockWhileInChooseTarget() throws Exception {  final String src="/testRetryAddBlockWhileInChooseTarget";  final FSNamesystem ns=cluster.getNamesystem();  final NamenodeProtocols nn=cluster.getNameNodeRpc();  nn.create(src,FsPermission.getFileDefault(),"clientName",new EnumSetWritable<CreateFlag>(EnumSet.of(CreateFlag.CREATE)),true,(short)3,1024,null,null,null);
  final FSNamesystem ns=cluster.getNamesystem();  final NamenodeProtocols nn=cluster.getNameNodeRpc();  nn.create(src,FsPermission.getFileDefault(),"clientName",new EnumSetWritable<CreateFlag>(EnumSet.of(CreateFlag.CREATE)),true,(short)3,1024,null,null,null);  LOG.info("Starting first addBlock for " + src);  LocatedBlock[] onRetryBlock=new LocatedBlock[1];  ns.readLock();  FSDirWriteFileOp.ValidateAddBlockResult r;  FSPermissionChecker pc=Mockito.mock(FSPermissionChecker.class);  try {    r=FSDirWriteFileOp.validateAddBlock(ns,pc,src,HdfsConstants.GRANDFATHER_INODE_ID,"clientName",null,onRetryBlock);  }  finally {    ns.readUnlock();    ;  }  DatanodeStorageInfo targets[]=FSDirWriteFileOp.chooseTargetForNewBlock(ns.getBlockManager(),src,null,null,null,r);  assertNotNull("Targets must be generated",targets);
@Test public void testAddBlockRetryShouldReturnBlockWithLocations() throws Exception {  final String src="/testAddBlockRetryShouldReturnBlockWithLocations";  NamenodeProtocols nameNodeRpc=cluster.getNameNodeRpc();  nameNodeRpc.create(src,FsPermission.getFileDefault(),"clientName",new EnumSetWritable<CreateFlag>(EnumSet.of(CreateFlag.CREATE)),true,(short)3,1024,null,null,null);
  config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY,true);  cluster=new MiniDFSCluster.Builder(config).manageDataDfsDirs(false).manageNameDfsDirs(false).build();  cluster.waitActive();  assertNotNull(cluster);  nn=cluster.getNameNode();  assertNotNull(nn);  LOG.info("Mini cluster created OK");  LOG.info("Verifying format will fail with allowformat false");  config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY,false);  try {    cluster.shutdown();    NameNode.format(config);    fail("Format succeeded, when it should have failed");  } catch (  IOException e) {    assertTrue("Exception was not about formatting Namenode",e.getMessage().startsWith("The option " + DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY));
@Test public void testAuditLoggerWithCallContext() throws IOException {  Configuration conf=new HdfsConfiguration();  conf.setBoolean(HADOOP_CALLER_CONTEXT_ENABLED_KEY,true);  conf.setInt(HADOOP_CALLER_CONTEXT_MAX_SIZE_KEY,128);  conf.setInt(HADOOP_CALLER_CONTEXT_SIGNATURE_MAX_SIZE_KEY,40);  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();  LogCapturer auditlog=LogCapturer.captureLogs(FSNamesystem.auditLog);  try {    cluster.waitClusterUp();    final FileSystem fs=cluster.getFileSystem();    final long time=System.currentTimeMillis();    final Path p=new Path("/");    assertNull(CallerContext.getCurrent());    CallerContext context=new CallerContext.Builder("setTimes").build();    CallerContext.setCurrent(context);
  LogCapturer auditlog=LogCapturer.captureLogs(FSNamesystem.auditLog);  try {    cluster.waitClusterUp();    final FileSystem fs=cluster.getFileSystem();    final long time=System.currentTimeMillis();    final Path p=new Path("/");    assertNull(CallerContext.getCurrent());    CallerContext context=new CallerContext.Builder("setTimes").build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.setTimes(p,time,time);    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setTimes%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("setTimes").setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();    CallerContext.setCurrent(context);
    CallerContext context=new CallerContext.Builder("setTimes").build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.setTimes(p,time,time);    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setTimes%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("setTimes").setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.setTimes(p,time,time);    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setTimes:L%n")));    auditlog.clearOutput();    final String longContext=StringUtils.repeat("foo",100);    context=new CallerContext.Builder(longContext).setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();    CallerContext.setCurrent(context);
    final String longContext=StringUtils.repeat("foo",100);    context=new CallerContext.Builder(longContext).setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.setTimes(p,time,time);    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=%s:L%n",longContext.substring(0,128))));    auditlog.clearOutput();    context=new CallerContext.Builder("").setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();    CallerContext.setCurrent(context);    LOG.info("Set empty caller context");    fs.setTimes(p,time,time);    assertFalse(auditlog.getOutput().contains("callerContext="));    auditlog.clearOutput();    context=new CallerContext.Builder("setTimes").setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();    CallerContext.setCurrent(context);
    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    Thread child=new Thread(new Runnable(){      @Override public void run(){        try {          fs.setTimes(p,time,time);        } catch (        IOException e) {          fail("Unexpected exception found." + e);        }      }    });    child.start();    try {      child.join();    } catch (    InterruptedException ignored) {    }    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setTimes:L%n")));    auditlog.clearOutput();    final CallerContext childContext=new CallerContext.Builder("setPermission").setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();
    }    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setTimes:L%n")));    auditlog.clearOutput();    final CallerContext childContext=new CallerContext.Builder("setPermission").setSignature("L".getBytes(CallerContext.SIGNATURE_ENCODING)).build();    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    child=new Thread(new Runnable(){      @Override public void run(){        try {          CallerContext.setCurrent(childContext);          fs.setPermission(p,new FsPermission((short)777));        } catch (        IOException e) {          fail("Unexpected exception found." + e);        }      }    });    child.start();    try {      child.join();
      @Override public void run(){        try {          CallerContext.setCurrent(childContext);          fs.setPermission(p,new FsPermission((short)777));        } catch (        IOException e) {          fail("Unexpected exception found." + e);        }      }    });    child.start();    try {      child.join();    } catch (    InterruptedException ignored) {    }    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setPermission:L%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("mkdirs").setSignature(CallerContext.getCurrent().getSignature()).build();    CallerContext.setCurrent(context);
          fail("Unexpected exception found." + e);        }      }    });    child.start();    try {      child.join();    } catch (    InterruptedException ignored) {    }    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setPermission:L%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("mkdirs").setSignature(CallerContext.getCurrent().getSignature()).build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.mkdirs(new Path("/reuse-context-signature"));    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=mkdirs:L%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("setTimes").setSignature(new byte[41]).build();
    try {      child.join();    } catch (    InterruptedException ignored) {    }    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setPermission:L%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("mkdirs").setSignature(CallerContext.getCurrent().getSignature()).build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.mkdirs(new Path("/reuse-context-signature"));    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=mkdirs:L%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("setTimes").setSignature(new byte[41]).build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.setTimes(p,time,time);
    auditlog.clearOutput();    context=new CallerContext.Builder("mkdirs").setSignature(CallerContext.getCurrent().getSignature()).build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.mkdirs(new Path("/reuse-context-signature"));    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=mkdirs:L%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("setTimes").setSignature(new byte[41]).build();    CallerContext.setCurrent(context);    LOG.info("Set current caller context as {}",CallerContext.getCurrent());    fs.setTimes(p,time,time);    assertTrue(auditlog.getOutput().endsWith(String.format("callerContext=setTimes%n")));    auditlog.clearOutput();    context=new CallerContext.Builder("setTimes").setSignature(null).build();    CallerContext.setCurrent(context);
void waitCheckpointDone(MiniDFSCluster cluster,long txid){  long thisCheckpointTxId;  do {    try {
  try {    Configuration nnconf=new HdfsConfiguration(c);    DFSTestUtil.formatNameNode(nnconf);    nn=NameNode.createNameNode(new String[]{},nnconf);  } catch (  IOException e) {    LOG.info("IOException is thrown creating name node");    throw e;  }  c.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,"kerberos");  c.set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY,"");  BackupNode bn=null;  try {    bn=(BackupNode)NameNode.createNameNode(new String[]{startupOpt.getName()},c);    assertTrue("Namesystem in BackupNode should be null",bn.getNamesystem() == null);    fail("Incorrect authentication setting should throw IOException");  } catch (  IOException e) {
private void testBNInSync(MiniDFSCluster cluster,final BackupNode backup,int testIdx) throws Exception {  final NameNode nn=cluster.getNameNode();  final FileSystem fs=cluster.getFileSystem();  for (int i=0; i < 10; i++) {    final String src="/test_" + testIdx + "_"+ i;
private void testBNInSync(MiniDFSCluster cluster,final BackupNode backup,int testIdx) throws Exception {  final NameNode nn=cluster.getNameNode();  final FileSystem fs=cluster.getFileSystem();  for (int i=0; i < 10; i++) {    final String src="/test_" + testIdx + "_"+ i;    LOG.info("Creating " + src + " on NN");    Path p=new Path(src);    assertTrue(fs.mkdirs(p));    GenericTestUtils.waitFor(new Supplier<Boolean>(){      @Override public Boolean get(){
  conf.setInt(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,-1);  conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,1);  MiniDFSCluster cluster=null;  FileSystem fileSys=null;  BackupNode backup=null;  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();    fileSys=cluster.getFileSystem();    assertTrue(!fileSys.exists(file1));    assertTrue(!fileSys.exists(file2));    assertTrue(fileSys.mkdirs(file1));    long txid=cluster.getNameNodeRpc().getTransactionID();    backup=startBackupNode(conf,op,1);    waitCheckpointDone(cluster,txid);  } catch (  IOException e) {
    fileSys.mkdirs(file2);    long txid=cluster.getNameNodeRpc().getTransactionID();    backup=startBackupNode(conf,op,1);    waitCheckpointDone(cluster,txid);    for (int i=0; i < 10; i++) {      fileSys.mkdirs(new Path("file_" + i));    }    txid=cluster.getNameNodeRpc().getTransactionID();    backup.doCheckpoint();    waitCheckpointDone(cluster,txid);    txid=cluster.getNameNodeRpc().getTransactionID();    backup.doCheckpoint();    waitCheckpointDone(cluster,txid);    InetSocketAddress add=backup.getNameNodeAddress();    FileSystem bnFS=FileSystem.get(new Path("hdfs://" + NetUtils.getHostPortString(add)).toUri(),conf);    boolean canWrite=true;
    txid=cluster.getNameNodeRpc().getTransactionID();    backup.doCheckpoint();    waitCheckpointDone(cluster,txid);    txid=cluster.getNameNodeRpc().getTransactionID();    backup.doCheckpoint();    waitCheckpointDone(cluster,txid);    InetSocketAddress add=backup.getNameNodeAddress();    FileSystem bnFS=FileSystem.get(new Path("hdfs://" + NetUtils.getHostPortString(add)).toUri(),conf);    boolean canWrite=true;    try {      DFSTestUtil.createFile(bnFS,file3,fileSize,fileSize,blockSize,replication,seed);    } catch (    IOException eio) {      LOG.info("Write to " + backup.getRole() + " failed as expected: ",eio);      canWrite=false;    }    assertFalse("Write to BackupNode must be prohibited.",canWrite);
    } catch (    IOException eio) {      LOG.info("Read from " + backup.getRole() + " failed: ",eio);      canRead=false;    }    assertEquals("Reads to BackupNode are allowed, but not CheckpointNode.",canRead,backup.isRole(NamenodeRole.BACKUP));    DFSTestUtil.createFile(fileSys,file3,fileSize,fileSize,blockSize,replication,seed);    TestCheckpoint.checkFile(fileSys,file3,replication);    assertTrue("file3 does not exist on BackupNode",op != StartupOption.BACKUP || backup.getNamesystem().getFileInfo(file3.toUri().getPath(),false,false,false) != null);  } catch (  IOException e) {    LOG.error("Error in TestBackupNode:",e);    throw new AssertionError(e);  } finally {    if (backup != null)     backup.stop();    if (fileSys != null)     fileSys.close();    if (cluster != null)     cluster.shutdown();  }  FSImageTestUtil.assertParallelFilesAreIdentical(ImmutableList.of(bnCurDir,nnCurDir),ImmutableSet.<String>of("VERSION"));
    String nnAddr=cluster.getNameNode().getNameNodeAddressHostPortString();    conf.get(DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY);    String bnAddr=backup.getNameNodeAddressHostPortString();    conf.set(DFSConfigKeys.DFS_NAMESERVICES,"bnCluster");    conf.set(DFSConfigKeys.DFS_NAMESERVICE_ID,"bnCluster");    conf.set(DFSConfigKeys.DFS_HA_NAMENODES_KEY_PREFIX + ".bnCluster","nnActive, nnBackup");    conf.set(rpcAddrKeyPreffix + ".nnActive",nnAddr);    conf.set(rpcAddrKeyPreffix + ".nnBackup",bnAddr);    cluster.startDataNodes(conf,3,true,StartupOption.REGULAR,null);    DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,(short)3,seed);    FileSystem bnFS=FileSystem.get(new Path("hdfs://" + bnAddr).toUri(),conf);    String nnData=DFSTestUtil.readFile(fileSys,file1);    String bnData=DFSTestUtil.readFile(bnFS,file1);    assertEquals("Data read from BackupNode and NameNode is not the same.",nnData,bnData);  } catch (  IOException e) {
private static void waitForCachedBlocks(NameNode nn,final int expectedCachedBlocks,final int expectedCachedReplicas,final String logString) throws Exception {  final FSNamesystem namesystem=nn.getNamesystem();  final CacheManager cacheManager=namesystem.getCacheManager();
private static void waitForCacheDirectiveStats(final DistributedFileSystem dfs,final long targetBytesNeeded,final long targetBytesCached,final long targetFilesNeeded,final long targetFilesCached,final CacheDirectiveInfo filter,final String infoString) throws Exception {
  LOG.info("Polling listCacheDirectives " + ((filter == null) ? "ALL" : filter.toString()) + " for "+ targetBytesNeeded+ " targetBytesNeeded, "+ targetBytesCached+ " targetBytesCached, "+ targetFilesNeeded+ " targetFilesNeeded, "+ targetFilesCached+ " targetFilesCached");  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      RemoteIterator<CacheDirectiveEntry> iter=null;      CacheDirectiveEntry entry=null;      try {        iter=dfs.listCacheDirectives(filter);        entry=iter.next();      } catch (      IOException e) {        fail("got IOException while calling " + "listCacheDirectives: " + e.getMessage());      }      Assert.assertNotNull(entry);      CacheDirectiveStats stats=entry.getStats();      if ((targetBytesNeeded == stats.getBytesNeeded()) && (targetBytesCached == stats.getBytesCached()) && (targetFilesNeeded == stats.getFilesNeeded())&& (targetFilesCached == stats.getFilesCached())) {        return true;      } else {
private static void waitForCachePoolStats(final DistributedFileSystem dfs,final long targetBytesNeeded,final long targetBytesCached,final long targetFilesNeeded,final long targetFilesCached,final CachePoolInfo pool,final String infoString) throws Exception {
      }      while (true) {        CachePoolEntry entry=null;        try {          if (!iter.hasNext()) {            break;          }          entry=iter.next();        } catch (        IOException e) {          fail("got IOException while iterating through " + "listCachePools: " + e.getMessage());        }        if (entry == null) {          break;        }        if (!entry.getInfo().getPoolName().equals(pool.getPoolName())) {          continue;        }        CachePoolStats stats=entry.getStats();        if ((targetBytesNeeded == stats.getBytesNeeded()) && (targetBytesCached == stats.getBytesCached()) && (targetFilesNeeded == stats.getFilesNeeded())&& (targetFilesCached == stats.getFilesCached())) {          return true;
private static void checkNumCachedReplicas(final DistributedFileSystem dfs,final List<Path> paths,final int expectedBlocks,final int expectedReplicas) throws Exception {  int numCachedBlocks=0;  int numCachedReplicas=0;  for (  Path p : paths) {    final FileStatus f=dfs.getFileStatus(p);    final long len=f.getLen();    final long blockSize=f.getBlockSize();    final long numBlocks=(len + blockSize - 1) / blockSize;    BlockLocation[] locs=dfs.getFileBlockLocations(p,0,len);    assertEquals("Unexpected number of block locations for path " + p,numBlocks,locs.length);    for (    BlockLocation l : locs) {      if (l.getCachedHosts().length > 0) {        numCachedBlocks++;      }      numCachedReplicas+=l.getCachedHosts().length;    }  }  LOG.info("Found " + numCachedBlocks + " of "+ expectedBlocks+ " blocks");
  Configuration conf=new HdfsConfiguration();  MiniDFSCluster cluster=null;  SecondaryNameNode secondary=null;  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();    StorageDirectory savedSd=null;    secondary=startSecondaryNameNode(conf);    NNStorage storage=secondary.getFSImage().getStorage();    for (    StorageDirectory sd : storage.dirIterable(null)) {      assertLockFails(sd);      savedSd=sd;    }    LOG.info("===> Shutting down first 2NN");    secondary.shutdown();    secondary=null;    LOG.info("===> Locking a dir, starting second 2NN");
  Configuration conf=new HdfsConfiguration();  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();    secondary=startSecondaryNameNode(conf);    Mockito.doThrow(new IOException("Injecting failure after rolling edit logs")).when(faultInjector).afterSecondaryCallsRollEditLog();    try {      secondary.doCheckpoint();      fail("Should have failed upload");    } catch (    IOException ioe) {      LOG.info("Got expected failure",ioe);      assertTrue(ioe.toString().contains("Injecting failure"));    }    try {      secondary.doCheckpoint();      fail("Should have failed upload");    } catch (    IOException ioe) {
    Configuration snnConf=new Configuration(conf);    File checkpointDir=new File(MiniDFSCluster.getBaseDirectory(),"namesecondary");    snnConf.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY,checkpointDir.getAbsolutePath());    secondary=startSecondaryNameNode(snnConf);    secondary.doCheckpoint();    cluster.shutdown();    cluster=null;    try {      Thread.sleep(100);    } catch (    InterruptedException ie) {    }    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).nameNodePort(origPort).nameNodeHttpPort(origHttpPort).format(true).build();    try {      secondary.doCheckpoint();      fail("Should have failed checkpoint against a different namespace");    } catch (    IOException ioe) {
private void testDeleteAndCommitBlockSynchronizationRace(boolean hasSnapshot) throws Exception {
  testList.add(new AbstractMap.SimpleImmutableEntry<String,Boolean>("/testdir/testdir1/test-file1",true));  final Path rootPath=new Path("/");  final Configuration conf=new Configuration();  conf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY,false);  conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,BLOCK_SIZE);  FSDataOutputStream stm=null;  Map<DataNode,DatanodeProtocolClientSideTranslatorPB> dnMap=new HashMap<DataNode,DatanodeProtocolClientSideTranslatorPB>();  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();    cluster.waitActive();    DistributedFileSystem fs=cluster.getFileSystem();    int stId=0;    for (    AbstractMap.SimpleImmutableEntry<String,Boolean> stest : testList) {      String testPath=stest.getKey();      Boolean mkSameDir=stest.getValue();
  Map<DataNode,DatanodeProtocolClientSideTranslatorPB> dnMap=new HashMap<DataNode,DatanodeProtocolClientSideTranslatorPB>();  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();    cluster.waitActive();    DistributedFileSystem fs=cluster.getFileSystem();    int stId=0;    for (    AbstractMap.SimpleImmutableEntry<String,Boolean> stest : testList) {      String testPath=stest.getKey();      Boolean mkSameDir=stest.getValue();      LOG.info("test on " + testPath + " mkSameDir: "+ mkSameDir+ " snapshot: "+ hasSnapshot);      Path fPath=new Path(testPath);      Path grandestNonRootParent=fPath;      while (!grandestNonRootParent.getParent().equals(rootPath)) {        grandestNonRootParent=grandestNonRootParent.getParent();      }      stm=fs.create(fPath);
      LOG.info("test on " + testPath + " mkSameDir: "+ mkSameDir+ " snapshot: "+ hasSnapshot);      Path fPath=new Path(testPath);      Path grandestNonRootParent=fPath;      while (!grandestNonRootParent.getParent().equals(rootPath)) {        grandestNonRootParent=grandestNonRootParent.getParent();      }      stm=fs.create(fPath);      LOG.info("test on " + testPath + " created "+ fPath);      AppendTestUtil.write(stm,0,BLOCK_SIZE / 2);      stm.hflush();      if (hasSnapshot) {        SnapshotTestHelper.createSnapshot(fs,rootPath,"st" + String.valueOf(stId));        ++stId;      }      NameNode nn=cluster.getNameNode();      ExtendedBlock blk=DFSTestUtil.getFirstBlock(fs,fPath);      DatanodeDescriptor expectedPrimary=DFSTestUtil.getExpectedPrimaryNode(nn,blk);
      NameNode nn=cluster.getNameNode();      ExtendedBlock blk=DFSTestUtil.getFirstBlock(fs,fPath);      DatanodeDescriptor expectedPrimary=DFSTestUtil.getExpectedPrimaryNode(nn,blk);      LOG.info("Expecting block recovery to be triggered on DN " + expectedPrimary);      DataNode primaryDN=cluster.getDataNode(expectedPrimary.getIpcPort());      DatanodeProtocolClientSideTranslatorPB nnSpy=dnMap.get(primaryDN);      if (nnSpy == null) {        nnSpy=InternalDataNodeTestUtils.spyOnBposToNN(primaryDN,nn);        dnMap.put(primaryDN,nnSpy);      }      DelayAnswer delayer=new DelayAnswer(LOG);      Mockito.doAnswer(delayer).when(nnSpy).commitBlockSynchronization(Mockito.eq(blk),Mockito.anyLong(),Mockito.anyLong(),Mockito.eq(true),Mockito.eq(false),Mockito.any(),Mockito.any());      fs.recoverLease(fPath);      LOG.info("Waiting for commitBlockSynchronization call from primary");      delayer.waitForCall();      LOG.info("Deleting recursively " + grandestNonRootParent);
@Test public void testDeleteAndLeaseRecoveryHardLimitSnapshot() throws Exception {  final Path rootPath=new Path("/");  final Configuration config=new Configuration();  config.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY,false);  config.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,BLOCK_SIZE);  FSDataOutputStream stm=null;  try {    cluster=new MiniDFSCluster.Builder(config).numDataNodes(3).build();    cluster.waitActive();    final DistributedFileSystem fs=cluster.getFileSystem();    final Path testPath=new Path("/testfile");    stm=fs.create(testPath);
  fileSys=cluster.getFileSystem();  final FSNamesystem namesystem=cluster.getNamesystem();  FSImage fsimage=namesystem.getFSImage();  final FSEditLog editLog=fsimage.getEditLog();  fileSys.mkdirs(new Path("/tmp"));  Iterator<StorageDirectory> iter=fsimage.getStorage().dirIterator(NameNodeDirType.EDITS);  LinkedList<StorageDirectory> sds=new LinkedList<StorageDirectory>();  while (iter.hasNext()) {    sds.add(iter.next());  }  editLog.close();  cluster.shutdown();  for (  StorageDirectory sd : sds) {    File editFile=NNStorage.getFinalizedEditsFile(sd,1,3);    assertTrue(editFile.exists());    long fileLen=editFile.length();
      stream.create(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);      if (!inBothDirs) {        break;      }      NNStorage storage=new NNStorage(conf,Collections.<URI>emptyList(),Lists.newArrayList(uri));      if (updateTransactionIdFile) {        storage.writeTransactionIdFileToStorage(3);      }      storage.close();    }  finally {      stream.close();    }  }  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATA_NODES).format(false).build();    if (!shouldSucceed) {      fail("Should not have succeeded in startin cluster");    }  } catch (  IOException ioe) {    if (shouldSucceed) {
@Test public void testAlternatingJournalFailure() throws IOException {  File f1=new File(TEST_DIR + "/alternatingjournaltest0");  File f2=new File(TEST_DIR + "/alternatingjournaltest1");  List<URI> editUris=ImmutableList.of(f1.toURI(),f2.toURI());  NNStorage storage=setupEdits(editUris,10,new AbortSpec(1,0),new AbortSpec(2,1),new AbortSpec(3,0),new AbortSpec(4,1),new AbortSpec(5,0),new AbortSpec(6,1),new AbortSpec(7,0),new AbortSpec(8,1),new AbortSpec(9,0),new AbortSpec(10,1));  long totaltxnread=0;  FSEditLog editlog=getFSEditLog(storage);  editlog.initJournalsForWrite();  long startTxId=1;  Iterable<EditLogInputStream> editStreams=editlog.selectInputStreams(startTxId,TXNS_PER_ROLL * 11);  for (  EditLogInputStream edits : editStreams) {    FSEditLogLoader.EditLogValidation val=FSEditLogLoader.scanEditLog(edits,Long.MAX_VALUE);    long read=(val.getEndTxId() - edits.getFirstTxId()) + 1;
    @Override public boolean accept(    File dir,    String name){      if (name.startsWith(NNStorage.getFinalizedEditsFileName(startErrorTxId,endErrorTxId))) {        return true;      }      return false;    }  });  assertEquals(1,files.length);  assertTrue(files[0].delete());  FSEditLog editlog=getFSEditLog(storage);  editlog.initJournalsForWrite();  long startTxId=1;  Collection<EditLogInputStream> streams=null;  try {    streams=editlog.selectInputStreams(startTxId,4 * TXNS_PER_ROLL);    readAllEdits(streams,startTxId);  } catch (  IOException e) {
@Test public void testEditLogFailOverFromCorrupt() throws IOException {  File f1=new File(TEST_DIR + "/failover0");  File f2=new File(TEST_DIR + "/failover1");  List<URI> editUris=ImmutableList.of(f1.toURI(),f2.toURI());  NNStorage storage=setupEdits(editUris,3);  final long startErrorTxId=1 * TXNS_PER_ROLL + 1;  final long endErrorTxId=2 * TXNS_PER_ROLL;  File[] files=new File(f1,"current").listFiles(new FilenameFilter(){    @Override public boolean accept(    File dir,    String name){      if (name.startsWith(NNStorage.getFinalizedEditsFileName(startErrorTxId,endErrorTxId))) {        return true;      }      return false;    }  });  assertEquals(1,files.length);  long fileLen=files[0].length();
      editLog.rollEditLog(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION);      assertExistsInStorageDirs(cluster,NameNodeDirType.EDITS,NNStorage.getFinalizedEditsFileName((i * 3) + 1,(i * 3) + 3));    }    editLog.close();  }  finally {    if (fileSys != null)     fileSys.close();    if (cluster != null)     cluster.shutdown();  }  long startTime=Time.now();  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATA_NODES).build();    cluster.waitActive();  }  finally {    if (cluster != null) {      cluster.shutdown();    }  }  long endTime=Time.now();  double delta=((float)(endTime - startTime)) / 1000.0;
  int retryCount=0;  while (true) {    try {      int basePort=10060 + random.nextInt(100) * 2;      MiniDFSNNTopology topology=new MiniDFSNNTopology().addNameservice(new MiniDFSNNTopology.NSConf("ns1").addNN(new MiniDFSNNTopology.NNConf("nn1").setHttpPort(basePort)).addNN(new MiniDFSNNTopology.NNConf("nn2").setHttpPort(basePort + 1)));      cluster=new MiniDFSCluster.Builder(conf).nnTopology(topology).numDataNodes(0).build();      cluster.waitActive();      nn0=cluster.getNameNode(0);      fs=HATestUtil.configureFailoverFs(cluster,conf);      cluster.transitionToActive(0);      fs=cluster.getFileSystem(0);      editLog=nn0.getNamesystem().getEditLog();      ++retryCount;      break;    } catch (    BindException e) {
@Test(timeout=60000) public void testScanCorruptEditLog() throws Exception {  Configuration conf=new Configuration();  File editLog=new File(GenericTestUtils.getTempPath("testCorruptEditLog"));
  PermissionStatus perms=PermissionStatus.createImmutable("myuser","mygroup",FsPermission.createImmutable((short)0777));  mkdirOp.setPermissionStatus(perms);  elos.write(mkdirOp);  mkdirOp.reset();  mkdirOp.setRpcCallId(456);  mkdirOp.setTransactionId(2);  mkdirOp.setInodeId(123L);  mkdirOp.setPath("/mydir2");  perms=PermissionStatus.createImmutable("myuser","mygroup",FsPermission.createImmutable((short)0666));  mkdirOp.setPermissionStatus(perms);  elos.write(mkdirOp);  elos.setReadyToFlush();  elos.flushAndSync(false);  elos.close();  long fileLen=editLog.length();
  elos.write(mkdirOp);  elos.setReadyToFlush();  elos.flushAndSync(false);  elos.close();  long fileLen=editLog.length();  LOG.debug("Corrupting last 4 bytes of edit log file " + editLog + ", whose length is "+ fileLen);  RandomAccessFile rwf=new RandomAccessFile(editLog,"rw");  rwf.seek(fileLen - 4);  int b=rwf.readInt();  rwf.seek(fileLen - 4);  rwf.writeInt(b + 1);  rwf.close();  EditLogFileInputStream elis=new EditLogFileInputStream(editLog);  Assert.assertEquals(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION,elis.getVersion(true));  Assert.assertEquals(1,elis.scanNextOp());
  LOG.debug("Corrupting last 4 bytes of edit log file " + editLog + ", whose length is "+ fileLen);  RandomAccessFile rwf=new RandomAccessFile(editLog,"rw");  rwf.seek(fileLen - 4);  int b=rwf.readInt();  rwf.seek(fileLen - 4);  rwf.writeInt(b + 1);  rwf.close();  EditLogFileInputStream elis=new EditLogFileInputStream(editLog);  Assert.assertEquals(NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION,elis.getVersion(true));  Assert.assertEquals(1,elis.scanNextOp());  LOG.debug("Read transaction 1 from " + editLog);  try {    elis.scanNextOp();    Assert.fail("Expected scanNextOp to fail when op checksum was corrupt.");  } catch (  IOException e) {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATA_NODES).build();    cluster.waitActive();    fileSys=cluster.getFileSystem();    final FSNamesystem namesystem=cluster.getNamesystem();    FSImage fsimage=namesystem.getFSImage();    FSEditLog editLog=fsimage.getEditLog();    startTransactionWorkers(cluster,caughtErr);    for (int i=0; i < NUM_SAVE_IMAGE && caughtErr.get() == null; i++) {      try {        Thread.sleep(20);      } catch (      InterruptedException ignored) {      }      LOG.info("Save " + i + ": entering safe mode");      namesystem.enterSafeMode(false);      long logStartTxId=fsimage.getStorage().getMostRecentCheckpointTxId() + 1;      verifyEditLogs(namesystem,fsimage,NNStorage.getInProgressEditsFileName(logStartTxId),logStartTxId);
    fileSys=cluster.getFileSystem();    final FSNamesystem namesystem=cluster.getNamesystem();    FSImage fsimage=namesystem.getFSImage();    FSEditLog editLog=fsimage.getEditLog();    startTransactionWorkers(cluster,caughtErr);    for (int i=0; i < NUM_SAVE_IMAGE && caughtErr.get() == null; i++) {      try {        Thread.sleep(20);      } catch (      InterruptedException ignored) {      }      LOG.info("Save " + i + ": entering safe mode");      namesystem.enterSafeMode(false);      long logStartTxId=fsimage.getStorage().getMostRecentCheckpointTxId() + 1;      verifyEditLogs(namesystem,fsimage,NNStorage.getInProgressEditsFileName(logStartTxId),logStartTxId);      LOG.info("Save " + i + ": saving namespace");      namesystem.saveNamespace(0,0);
    for (int i=0; i < NUM_SAVE_IMAGE && caughtErr.get() == null; i++) {      try {        Thread.sleep(20);      } catch (      InterruptedException ignored) {      }      LOG.info("Save " + i + ": entering safe mode");      namesystem.enterSafeMode(false);      long logStartTxId=fsimage.getStorage().getMostRecentCheckpointTxId() + 1;      verifyEditLogs(namesystem,fsimage,NNStorage.getInProgressEditsFileName(logStartTxId),logStartTxId);      LOG.info("Save " + i + ": saving namespace");      namesystem.saveNamespace(0,0);      LOG.info("Save " + i + ": leaving safemode");      long savedImageTxId=fsimage.getStorage().getMostRecentCheckpointTxId();      verifyEditLogs(namesystem,fsimage,NNStorage.getFinalizedEditsFileName(logStartTxId,savedImageTxId),logStartTxId);      assertEquals(fsimage.getStorage().getMostRecentCheckpointTxId(),editLog.getLastWrittenTxId() - 1);      namesystem.leaveSafeMode(false);
  try {    FSImage fsimage=namesystem.getFSImage();    FSEditLog editLog=fsimage.getEditLog();    JournalAndStream jas=editLog.getJournals().get(0);    EditLogFileOutputStream spyElos=spy((EditLogFileOutputStream)jas.getCurrentStream());    jas.setCurrentStreamForTests(spyElos);    final AtomicReference<Throwable> deferredException=new AtomicReference<Throwable>();    final CountDownLatch waitToEnterFlush=new CountDownLatch(1);    final Thread doAnEditThread=new Thread(){      @Override public void run(){        try {          LOG.info("Starting mkdirs");          namesystem.mkdirs("/test",new PermissionStatus("test","test",new FsPermission((short)00755)),true);          LOG.info("mkdirs complete");        } catch (        Throwable ioe) {
      @Override public Void answer(      InvocationOnMock invocation) throws Throwable {        LOG.info("Flush called");        if (useAsyncEditLog || Thread.currentThread() == doAnEditThread) {          LOG.info("edit thread: Telling main thread we made it to flush section...");          waitToEnterFlush.countDown();          LOG.info("edit thread: sleeping for " + BLOCK_TIME + "secs");          Thread.sleep(BLOCK_TIME * 1000);          LOG.info("Going through to flush. This will allow the main thread to continue.");        }        invocation.callRealMethod();        LOG.info("Flush complete");        return null;      }    };    doAnswer(blockingFlush).when(spyElos).flush();    doAnEditThread.start();    LOG.info("Main thread: waiting to enter flush...");
    final Thread doAnEditThread=new Thread(){      @Override public void run(){        try {          LOG.info("Starting setOwner");          namesystem.writeLock();          try {            editLog.logSetOwner("/","test","test");          }  finally {            namesystem.writeUnlock();          }          sleepingBeforeSync.countDown();          LOG.info("edit thread: sleeping for " + BLOCK_TIME + "secs");          Thread.sleep(BLOCK_TIME * 1000);          editLog.logSync();          LOG.info("edit thread: logSync complete");        } catch (        Throwable ioe) {
    final FSEditLog editLog=namesystem.getEditLog();    FSEditLogOp.OpInstanceCache cache=editLog.cache.get();    final FSEditLogOp op=FSEditLogOp.SetOwnerOp.getInstance(cache).setSource("/").setUser("u").setGroup("g");    final FSEditLogOp reuseOp=Mockito.spy(op);    Mockito.doNothing().when(reuseOp).reset();    Future[] logSpammers=new Future[16];    for (int i=0; i < logSpammers.length; i++) {      final int ii=i;      logSpammers[i]=executor.submit(new Callable(){        @Override public Void call() throws Exception {          Thread.currentThread().setName("Log spammer " + ii);          startSpamLatch.await();          for (int i=0; !done.get() && i < 1000000; i++) {            editLog.logEdit(reuseOp);            if (i % 2048 == 0) {
@Test(timeout=300000) public void testXAttrMultiSetRemove() throws Exception {  List<XAttr> existingXAttrs=Lists.newArrayListWithCapacity(0);  final Random rand=new Random(0xFEEDA);  int numExpectedXAttrs=0;  while (numExpectedXAttrs < numGeneratedXAttrs) {
@Test(timeout=300000) public void testXAttrMultiSetRemove() throws Exception {  List<XAttr> existingXAttrs=Lists.newArrayListWithCapacity(0);  final Random rand=new Random(0xFEEDA);  int numExpectedXAttrs=0;  while (numExpectedXAttrs < numGeneratedXAttrs) {    LOG.info("Currently have " + numExpectedXAttrs + " xattrs");    final int numToAdd=rand.nextInt(5) + 1;    List<XAttr> toAdd=Lists.newArrayListWithCapacity(numToAdd);    for (int i=0; i < numToAdd; i++) {      if (numExpectedXAttrs >= numGeneratedXAttrs) {        break;      }      toAdd.add(generatedXAttrs.get(numExpectedXAttrs));      numExpectedXAttrs++;    }    LOG.info("Attempting to add " + toAdd.size() + " XAttrs");    for (int i=0; i < toAdd.size(); i++) {
    LOG.info("Currently have " + numExpectedXAttrs + " xattrs");    final int numToAdd=rand.nextInt(5) + 1;    List<XAttr> toAdd=Lists.newArrayListWithCapacity(numToAdd);    for (int i=0; i < numToAdd; i++) {      if (numExpectedXAttrs >= numGeneratedXAttrs) {        break;      }      toAdd.add(generatedXAttrs.get(numExpectedXAttrs));      numExpectedXAttrs++;    }    LOG.info("Attempting to add " + toAdd.size() + " XAttrs");    for (int i=0; i < toAdd.size(); i++) {      LOG.info("Will add XAttr " + toAdd.get(i));    }    List<XAttr> newXAttrs=FSDirXAttrOp.setINodeXAttrs(fsdir,existingXAttrs,toAdd,EnumSet.of(XAttrSetFlag.CREATE));    verifyXAttrsPresent(newXAttrs,numExpectedXAttrs);    existingXAttrs=newXAttrs;  }  while (numExpectedXAttrs > 0) {
@Test public void testBasicTruncate() throws IOException {  int startingFileSize=3 * BLOCK_SIZE;  fs.mkdirs(parent);  fs.setQuota(parent,100,1000);  byte[] contents=AppendTestUtil.initBuffer(startingFileSize);  for (int fileLength=startingFileSize; fileLength > 0; fileLength-=BLOCK_SIZE - 1) {    for (int toTruncate=0; toTruncate <= fileLength; toTruncate++) {      final Path p=new Path(parent,"testBasicTruncate" + fileLength);      writeContents(contents,fileLength,p);      int newLength=fileLength - toTruncate;      boolean isReady=fs.truncate(p,newLength);
@Test public void testMultipleTruncate() throws IOException {  Path dir=new Path("/testMultipleTruncate");  fs.mkdirs(dir);  final Path p=new Path(dir,"file");  final byte[] data=new byte[100 * BLOCK_SIZE];  ThreadLocalRandom.current().nextBytes(data);  writeContents(data,data.length,p);  for (int n=data.length; n > 0; ) {    final int newLength=ThreadLocalRandom.current().nextInt(n);    final boolean isReady=fs.truncate(p,newLength);
static void runTestToCommaSeparatedNumber(long n){  final String s=FsImageValidation.Util.toCommaSeparatedNumber(n);
public static String runFsck(Configuration conf,int expectedErrCode,boolean checkErrorCode,String... path) throws Exception {  ByteArrayOutputStream bStream=new ByteArrayOutputStream();  PrintStream out=new PrintStream(bStream,true);  GenericTestUtils.setLogLevel(FSPermissionChecker.LOG,org.slf4j.event.Level.TRACE);  int errCode=ToolRunner.run(new DFSck(conf,out),path);
      if (m.matches()) {        numMissing=m.group(1);      }      m=NUM_CORRUPT_BLOCKS_PATTERN.matcher(line);      if (m.matches()) {        numCorrupt=m.group(1);      }      if (numMissing != null && numCorrupt != null) {        break;      }    }    if (numMissing == null || numCorrupt == null) {      throw new IOException("failed to find number of missing or corrupt" + " blocks in fsck output.");    }    if (numMissing.equals(Integer.toString(totalMissingBlocks))) {      assertTrue(numCorrupt.equals(Integer.toString(0)));      assertTrue(outStr.contains(NamenodeFsck.CORRUPT_STATUS));      break;    }    try {      Thread.sleep(100);
  final int dfsBlockSize=512 * 1024;  final int numDatanodes=1;  final int replication=1;  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,dfsBlockSize);  conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);  conf.setInt(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY,1);  conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,replication);  File builderBaseDir=new File(GenericTestUtils.getRandomizedTempPath());  cluster=new MiniDFSCluster.Builder(conf,builderBaseDir).build();  DistributedFileSystem dfs=cluster.getFileSystem();  cluster.waitActive();  final String srcDir="/srcdat";  final DFSTestUtil util=new DFSTestUtil.Builder().setName("TestFsck").setMinSize(dfsBlockSize * 2).setMaxSize(dfsBlockSize * 3).setNumFiles(1).build();  util.createFiles(dfs,srcDir,(short)replication);  final String[] fileNames=util.getFileNames(srcDir);
    @Override public Boolean get(){      try {        final String str=runFsck(conf,1,false,"/");        String numCorrupt=null;        for (        String line : str.split(LINE_SEPARATOR)) {          Matcher m=NUM_CORRUPT_BLOCKS_PATTERN.matcher(line);          if (m.matches()) {            numCorrupt=m.group(1);            break;          }        }        if (numCorrupt == null) {          Assert.fail("Cannot find corrupt blocks count in fsck output.");        }        if (Integer.parseInt(numCorrupt) == ctf.getTotalMissingBlocks()) {          assertTrue(str.contains(NamenodeFsck.CORRUPT_STATUS));          return true;        }      } catch (      Exception e) {
          Assert.fail("Cannot find corrupt blocks count in fsck output.");        }        if (Integer.parseInt(numCorrupt) == ctf.getTotalMissingBlocks()) {          assertTrue(str.contains(NamenodeFsck.CORRUPT_STATUS));          return true;        }      } catch (      Exception e) {        LOG.error("Exception caught",e);        Assert.fail("Caught unexpected exception.");      }      return false;    }  },1000,60000);  runFsck(conf,1,true,"/","-files","-blocks","-racks");  LOG.info("Moving blocks to lost+found");  runFsck(conf,1,false,"/","-move");  final List<LocatedFileStatus> retVal=new ArrayList<>();  final RemoteIterator<LocatedFileStatus> iter=dfs.listFiles(new Path("/lost+found"),true);  while (iter.hasNext()) {
private void runTest(final int nNameNodes,final int nDataNodes,Configuration conf) throws Exception {
  LOG.info("nNameNodes=" + nNameNodes + ", nDataNodes="+ nDataNodes);  LOG.info("RUN_TEST -1");  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)).numDataNodes(nDataNodes).build();  LOG.info("RUN_TEST 0");  DFSTestUtil.setFederatedConfiguration(cluster,conf);  try {    cluster.waitActive();    LOG.info("RUN_TEST 1");    final Suite s=new Suite(cluster,nNameNodes,nDataNodes);    for (int i=0; i < nNameNodes; i++) {      s.createFile(i,1024);    }    LOG.info("RUN_TEST 2");    final String[] urls=new String[nNameNodes];    for (int i=0; i < urls.length; i++) {      urls[i]=cluster.getFileSystem(i).getUri() + FILE_NAME;
  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(nNameNodes)).numDataNodes(nDataNodes).build();  LOG.info("RUN_TEST 0");  DFSTestUtil.setFederatedConfiguration(cluster,conf);  try {    cluster.waitActive();    LOG.info("RUN_TEST 1");    final Suite s=new Suite(cluster,nNameNodes,nDataNodes);    for (int i=0; i < nNameNodes; i++) {      s.createFile(i,1024);    }    LOG.info("RUN_TEST 2");    final String[] urls=new String[nNameNodes];    for (int i=0; i < urls.length; i++) {      urls[i]=cluster.getFileSystem(i).getUri() + FILE_NAME;      LOG.info("urls[" + i + "]="+ urls[i]);      final String result=TestFsck.runFsck(conf,0,false,urls[i]);
    }    LOG.info("RUN_TEST 2");    final String[] urls=new String[nNameNodes];    for (int i=0; i < urls.length; i++) {      urls[i]=cluster.getFileSystem(i).getUri() + FILE_NAME;      LOG.info("urls[" + i + "]="+ urls[i]);      final String result=TestFsck.runFsck(conf,0,false,urls[i]);      LOG.info("result=" + result);      Assert.assertTrue(result.contains("Status: HEALTHY"));    }    LOG.info("RUN_TEST 3");    final String[] vurls=new String[nNameNodes];    for (int i=0; i < vurls.length; i++) {      String link="/mount/nn_" + i + FILE_NAME;      ConfigUtil.addLink(conf,link,new URI(urls[i]));      vurls[i]="viewfs:" + link;    }    for (int i=0; i < vurls.length; i++) {
    final String[] urls=new String[nNameNodes];    for (int i=0; i < urls.length; i++) {      urls[i]=cluster.getFileSystem(i).getUri() + FILE_NAME;      LOG.info("urls[" + i + "]="+ urls[i]);      final String result=TestFsck.runFsck(conf,0,false,urls[i]);      LOG.info("result=" + result);      Assert.assertTrue(result.contains("Status: HEALTHY"));    }    LOG.info("RUN_TEST 3");    final String[] vurls=new String[nNameNodes];    for (int i=0; i < vurls.length; i++) {      String link="/mount/nn_" + i + FILE_NAME;      ConfigUtil.addLink(conf,link,new URI(urls[i]));      vurls[i]="viewfs:" + link;    }    for (int i=0; i < vurls.length; i++) {      LOG.info("vurls[" + i + "]="+ vurls[i]);
  Configuration conf=getConf();  short REPLICATION_FACTOR=2;  final Path filePath=new Path("/testFile");  HostsFileWriter hostsFileWriter=new HostsFileWriter();  hostsFileWriter.initialize(conf,"temp/decommission");  String racks[]={"/rack1","/rack1","/rack2","/rack2"};  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(racks.length).racks(racks).build();  final FSNamesystem ns=cluster.getNameNode().getNamesystem();  try {    final FileSystem fs=cluster.getFileSystem();    DFSTestUtil.createFile(fs,filePath,1L,REPLICATION_FACTOR,1L);    ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);    DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);    BlockLocation locs[]=fs.getFileBlockLocations(fs.getFileStatus(filePath),0,Long.MAX_VALUE);    String name=locs[0].getNames()[0];
private void verifyFileStatus(UserGroupInformation ugi) throws IOException {  FileSystem fs=FileSystem.get(miniDFS.getConfiguration(0));  FileStatus status=fs.getFileStatus(new Path("/"));
  INodeFile inodeFile;  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,new Short((short)3),StripedFileTestUtil.getDefaultECPolicy().getId(),preferredBlockSize,HdfsConstants.WARM_STORAGE_POLICY_ID,CONTIGUOUS);    fail("INodeFile construction should fail when both replication and " + "ECPolicy requested!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,null,null,preferredBlockSize,HdfsConstants.WARM_STORAGE_POLICY_ID,CONTIGUOUS);    fail("INodeFile construction should fail when replication param not " + "provided for contiguous layout!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,Short.MAX_VALUE,null,preferredBlockSize,HdfsConstants.WARM_STORAGE_POLICY_ID,CONTIGUOUS);    fail("INodeFile construction should fail when replication param is " + "beyond the range supported!");  } catch (  IllegalArgumentException iae) {
    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,null,null,preferredBlockSize,HdfsConstants.WARM_STORAGE_POLICY_ID,CONTIGUOUS);    fail("INodeFile construction should fail when replication param not " + "provided for contiguous layout!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,Short.MAX_VALUE,null,preferredBlockSize,HdfsConstants.WARM_STORAGE_POLICY_ID,CONTIGUOUS);    fail("INodeFile construction should fail when replication param is " + "beyond the range supported!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  final Short replication=new Short((short)3);  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,replication,null,preferredBlockSize,HdfsConstants.WARM_STORAGE_POLICY_ID,STRIPED);    fail("INodeFile construction should fail when replication param is " + "provided for striped layout!");
private Path getInodePath(long inodeId,String remainingPath){  StringBuilder b=new StringBuilder();  b.append(Path.SEPARATOR).append(FSDirectory.DOT_RESERVED_STRING).append(Path.SEPARATOR).append(FSDirectory.DOT_INODES_STRING).append(Path.SEPARATOR).append(inodeId).append(Path.SEPARATOR).append(remainingPath);  Path p=new Path(b.toString());
 catch (        IOException ex) {          LOG.info("createFile exception ",ex);          break;        }      }    }  };  threads[1]=new TestThread(){    @Override protected void execute() throws Throwable {      while (live) {        try {          int blockcount=getBlockCount();          if (blockcount < TOTAL_BLOCKS && blockcount > 0) {            mc.getNamesystem().writeLock();            try {              lockOps++;            }  finally {              mc.getNamesystem().writeUnlock();
          int blockcount=getBlockCount();          if (blockcount < TOTAL_BLOCKS && blockcount > 0) {            mc.getNamesystem().writeLock();            try {              lockOps++;            }  finally {              mc.getNamesystem().writeUnlock();            }            Thread.sleep(1);          }        } catch (        InterruptedException ex) {          LOG.info("lockOperation exception ",ex);          break;        }      }    }  };  threads[0].start();  threads[1].start();  final long start=Time.now();
    assertCorruptFilesCount(cluster,badFiles.size());    String bpid=cluster.getNamesystem().getBlockPoolId();    File storageDir=cluster.getInstanceStorageDir(0,1);    File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);    assertTrue("data directory does not exist",data_dir.exists());    List<File> metaFiles=MiniDFSCluster.getAllBlockFiles(data_dir);    assertTrue("Data directory does not contain any blocks or there was an " + "IO error",metaFiles != null && !metaFiles.isEmpty());    File metaFile=metaFiles.get(0);    RandomAccessFile file=new RandomAccessFile(metaFile,"rw");    FileChannel channel=file.getChannel();    long position=channel.size() - corruptionLength;    byte[] buffer=new byte[corruptionLength];    new Random(13L).nextBytes(buffer);    channel.write(ByteBuffer.wrap(buffer),position);    file.close();
    RandomAccessFile file=new RandomAccessFile(metaFile,"rw");    FileChannel channel=file.getChannel();    long position=channel.size() - corruptionLength;    byte[] buffer=new byte[corruptionLength];    new Random(13L).nextBytes(buffer);    channel.write(ByteBuffer.wrap(buffer),position);    file.close();    LOG.info("Deliberately corrupting file " + metaFile.getName() + " at offset "+ position+ " length "+ corruptionLength);    try {      util.checkFiles(fs,"/srcdat10");    } catch (    BlockMissingException e) {      System.out.println("Received BlockMissingException as expected.");    }catch (    IOException e) {      assertTrue("Corrupted replicas not handled properly. Expecting BlockMissingException " + " but received IOException " + e,false);    }    badFiles=namenode.getNamesystem().listCorruptFileBlocks("/",null);
    assertEquals("Namenode has " + badFiles.size() + " corrupt files. Expecting None.",0,badFiles.size());    assertCorruptFilesCount(cluster,badFiles.size());    File storageDir=cluster.getInstanceStorageDir(0,0);    File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,cluster.getNamesystem().getBlockPoolId());    assertTrue("data directory does not exist",data_dir.exists());    List<File> metaFiles=MiniDFSCluster.getAllBlockFiles(data_dir);    assertTrue("Data directory does not contain any blocks or there was an " + "IO error",metaFiles != null && !metaFiles.isEmpty());    File metaFile=metaFiles.get(0);    RandomAccessFile file=new RandomAccessFile(metaFile,"rw");    FileChannel channel=file.getChannel();    long position=channel.size() - corruptionLength;    byte[] buffer=new byte[corruptionLength];    new Random(13L).nextBytes(buffer);    channel.write(ByteBuffer.wrap(buffer),position);    file.close();
    RandomAccessFile file=new RandomAccessFile(metaFile,"rw");    FileChannel channel=file.getChannel();    long position=channel.size() - corruptionLength;    byte[] buffer=new byte[corruptionLength];    new Random(13L).nextBytes(buffer);    channel.write(ByteBuffer.wrap(buffer),position);    file.close();    LOG.info("Deliberately corrupting file " + metaFile.getName() + " at offset "+ position+ " length "+ corruptionLength);    try {      util.checkFiles(fs,"/srcdat10");    } catch (    BlockMissingException e) {      System.out.println("Received BlockMissingException as expected.");    }catch (    IOException e) {      assertTrue("Corrupted replicas not handled properly. " + "Expecting BlockMissingException " + " but received IOException "+ e,false);    }    badFiles=cluster.getNameNode().getNamesystem().listCorruptFileBlocks("/",null);
    badFiles=cluster.getNameNode().getNamesystem().listCorruptFileBlocks("/",null);    LOG.info("Namenode has bad files. " + badFiles.size());    assertEquals("Namenode has " + badFiles.size() + " bad files. "+ "Expecting 1.",1,badFiles.size());    assertCorruptFilesCount(cluster,badFiles.size());    cluster.restartNameNode(0);    fs=cluster.getFileSystem();    while (!cluster.getNameNode().namesystem.getBlockManager().isPopulatingReplQueues()) {      try {        LOG.info("waiting for replication queues");        Thread.sleep(1000);      } catch (      InterruptedException ignore) {      }    }    try {      util.checkFiles(fs,"/srcdat10");    } catch (    BlockMissingException e) {      System.out.println("Received BlockMissingException as expected.");
      try {        LOG.info("waiting for replication queues");        Thread.sleep(1000);      } catch (      InterruptedException ignore) {      }    }    try {      util.checkFiles(fs,"/srcdat10");    } catch (    BlockMissingException e) {      System.out.println("Received BlockMissingException as expected.");    }catch (    IOException e) {      assertTrue("Corrupted replicas not handled properly. " + "Expecting BlockMissingException " + " but received IOException "+ e,false);    }    badFiles=cluster.getNameNode().getNamesystem().listCorruptFileBlocks("/",null);    LOG.info("Namenode has bad files. " + badFiles.size());    assertEquals("Namenode has " + badFiles.size() + " bad files. "+ "Expecting 1.",1,badFiles.size());    assertCorruptFilesCount(cluster,badFiles.size());    assertTrue("Namenode is not in safe mode",cluster.getNameNode().isInSafeMode());
    util.createFiles(fs,"/corruptData");    final NameNode namenode=cluster.getNameNode();    Collection<FSNamesystem.CorruptFileBlockInfo> corruptFileBlocks=namenode.getNamesystem().listCorruptFileBlocks("/corruptData",null);    int numCorrupt=corruptFileBlocks.size();    assertEquals(0,numCorrupt);    assertCorruptFilesCount(cluster,numCorrupt);    String bpid=cluster.getNamesystem().getBlockPoolId();    for (int i=0; i < 4; i++) {      for (int j=0; j <= 1; j++) {        File storageDir=cluster.getInstanceStorageDir(i,j);        File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);        List<File> metadataFiles=MiniDFSCluster.getAllBlockMetadataFiles(data_dir);        if (metadataFiles == null)         continue;        for (        File metadataFile : metadataFiles) {          File blockFile=Block.metaToBlockFile(metadataFile);
    Collection<FSNamesystem.CorruptFileBlockInfo> corruptFileBlocks=namenode.getNamesystem().listCorruptFileBlocks("/corruptData",null);    int numCorrupt=corruptFileBlocks.size();    assertEquals(0,numCorrupt);    assertCorruptFilesCount(cluster,numCorrupt);    String bpid=cluster.getNamesystem().getBlockPoolId();    for (int i=0; i < 4; i++) {      for (int j=0; j <= 1; j++) {        File storageDir=cluster.getInstanceStorageDir(i,j);        File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);        List<File> metadataFiles=MiniDFSCluster.getAllBlockMetadataFiles(data_dir);        if (metadataFiles == null)         continue;        for (        File metadataFile : metadataFiles) {          File blockFile=Block.metaToBlockFile(metadataFile);          LOG.info("Deliberately removing file " + blockFile.getName());          assertTrue("Cannot remove file.",blockFile.delete());
    DistributedFileSystem dfs=(DistributedFileSystem)fs;    DFSTestUtil util=new DFSTestUtil.Builder().setName("testGetCorruptFiles").setNumFiles(3).setMaxLevels(1).setMaxSize(1024).build();    util.createFiles(fs,"/corruptData");    RemoteIterator<Path> corruptFileBlocks=dfs.listCorruptFileBlocks(new Path("/corruptData"));    int numCorrupt=countPaths(corruptFileBlocks);    assertEquals(0,numCorrupt);    assertCorruptFilesCount(cluster,numCorrupt);    String bpid=cluster.getNamesystem().getBlockPoolId();    for (int i=0; i < 2; i++) {      File storageDir=cluster.getInstanceStorageDir(0,i);      File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);      List<File> metadataFiles=MiniDFSCluster.getAllBlockMetadataFiles(data_dir);      if (metadataFiles == null)       continue;      for (      File metadataFile : metadataFiles) {        File blockFile=Block.metaToBlockFile(metadataFile);
    util.createFiles(fs,"/corruptData");    RemoteIterator<Path> corruptFileBlocks=dfs.listCorruptFileBlocks(new Path("/corruptData"));    int numCorrupt=countPaths(corruptFileBlocks);    assertEquals(0,numCorrupt);    assertCorruptFilesCount(cluster,numCorrupt);    String bpid=cluster.getNamesystem().getBlockPoolId();    for (int i=0; i < 2; i++) {      File storageDir=cluster.getInstanceStorageDir(0,i);      File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);      List<File> metadataFiles=MiniDFSCluster.getAllBlockMetadataFiles(data_dir);      if (metadataFiles == null)       continue;      for (      File metadataFile : metadataFiles) {        File blockFile=Block.metaToBlockFile(metadataFile);        LOG.info("Deliberately removing file " + blockFile.getName());        assertTrue("Cannot remove file.",blockFile.delete());
    cluster=new MiniDFSCluster.Builder(conf).build();    FileSystem fs=cluster.getFileSystem();    final int maxCorruptFileBlocks=conf.getInt(DFSConfigKeys.DFS_NAMENODE_MAX_CORRUPT_FILE_BLOCKS_RETURNED_KEY,100);    DFSTestUtil util=new DFSTestUtil.Builder().setName("testMaxCorruptFiles").setNumFiles(maxCorruptFileBlocks * 3).setMaxLevels(1).setMaxSize(512).build();    util.createFiles(fs,"/srcdat2",(short)1);    util.waitReplication(fs,"/srcdat2",(short)1);    final NameNode namenode=cluster.getNameNode();    Collection<FSNamesystem.CorruptFileBlockInfo> badFiles=namenode.getNamesystem().listCorruptFileBlocks("/srcdat2",null);    assertEquals("Namenode has " + badFiles.size() + " corrupt files. Expecting none.",0,badFiles.size());    assertCorruptFilesCount(cluster,badFiles.size());    final String bpid=cluster.getNamesystem().getBlockPoolId();    for (int i=0; i < 4; i++) {      for (int j=0; j <= 1; j++) {        File storageDir=cluster.getInstanceStorageDir(i,j);        File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);
        File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);        LOG.info("Removing files from " + data_dir);        List<File> metadataFiles=MiniDFSCluster.getAllBlockMetadataFiles(data_dir);        if (metadataFiles == null)         continue;        for (        File metadataFile : metadataFiles) {          File blockFile=Block.metaToBlockFile(metadataFile);          assertTrue("Cannot remove file.",blockFile.delete());          assertTrue("Cannot remove file.",metadataFile.delete());        }      }    }    DataNode dn=cluster.getDataNodes().get(0);    DataNodeTestUtils.runDirectoryScanner(dn);    LOG.info("Restarting Datanode to trigger BlockPoolSliceScanner");    cluster.restartDataNodes();    cluster.waitActive();    badFiles=namenode.getNamesystem().listCorruptFileBlocks("/srcdat2",null);    while (badFiles.size() < maxCorruptFileBlocks) {
        for (        File metadataFile : metadataFiles) {          File blockFile=Block.metaToBlockFile(metadataFile);          assertTrue("Cannot remove file.",blockFile.delete());          assertTrue("Cannot remove file.",metadataFile.delete());        }      }    }    DataNode dn=cluster.getDataNodes().get(0);    DataNodeTestUtils.runDirectoryScanner(dn);    LOG.info("Restarting Datanode to trigger BlockPoolSliceScanner");    cluster.restartDataNodes();    cluster.waitActive();    badFiles=namenode.getNamesystem().listCorruptFileBlocks("/srcdat2",null);    while (badFiles.size() < maxCorruptFileBlocks) {      LOG.info("# of corrupt files is: " + badFiles.size());      Thread.sleep(10000);      badFiles=namenode.getNamesystem().listCorruptFileBlocks("/srcdat2",null);    }    badFiles=namenode.getNamesystem().listCorruptFileBlocks("/srcdat2",null);
    fs.setWorkingDirectory(baseDir);    DFSTestUtil util=new DFSTestUtil.Builder().setName("testGetCorruptFilesOnRelativePath").setNumFiles(3).setMaxLevels(1).setMaxSize(1024).build();    util.createFiles(fs,"corruptData");    RemoteIterator<Path> corruptFileBlocks=dfs.listCorruptFileBlocks(new Path("corruptData"));    int numCorrupt=countPaths(corruptFileBlocks);    assertEquals(0,numCorrupt);    assertCorruptFilesCount(cluster,numCorrupt);    String bpid=cluster.getNamesystem().getBlockPoolId();    for (int i=0; i < 2; i++) {      File storageDir=cluster.getInstanceStorageDir(0,i);      File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);      List<File> metadataFiles=MiniDFSCluster.getAllBlockMetadataFiles(data_dir);      if (metadataFiles == null)       continue;      for (      File metadataFile : metadataFiles) {        File blockFile=Block.metaToBlockFile(metadataFile);
    util.createFiles(fs,"corruptData");    RemoteIterator<Path> corruptFileBlocks=dfs.listCorruptFileBlocks(new Path("corruptData"));    int numCorrupt=countPaths(corruptFileBlocks);    assertEquals(0,numCorrupt);    assertCorruptFilesCount(cluster,numCorrupt);    String bpid=cluster.getNamesystem().getBlockPoolId();    for (int i=0; i < 2; i++) {      File storageDir=cluster.getInstanceStorageDir(0,i);      File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);      List<File> metadataFiles=MiniDFSCluster.getAllBlockMetadataFiles(data_dir);      if (metadataFiles == null)       continue;      for (      File metadataFile : metadataFiles) {        File blockFile=Block.metaToBlockFile(metadataFile);        LOG.info("Deliberately removing file " + blockFile.getName());        assertTrue("Cannot remove file.",blockFile.delete());
    ThreadLocalRandom.current().nextBytes(data);    DFSTestUtil.createOpenFiles(fileSystem,"ha-open-file",((BATCH_SIZE * 4) + (BATCH_SIZE / 2)));    final DFSAdmin dfsAdmin=new DFSAdmin(haConf);    final AtomicBoolean failoverCompleted=new AtomicBoolean(false);    final AtomicBoolean listOpenFilesError=new AtomicBoolean(false);    final int listingIntervalMsec=250;    Thread clientThread=new Thread(new Runnable(){      @Override public void run(){        while (!failoverCompleted.get()) {          try {            assertEquals(0,ToolRunner.run(dfsAdmin,new String[]{"-listOpenFiles"}));            assertEquals(0,ToolRunner.run(dfsAdmin,new String[]{"-listOpenFiles","-blockingDecommission"}));            Thread.sleep(listingIntervalMsec);          } catch (          Exception e) {            listOpenFilesError.set(true);
  MiniDFSCluster cluster=null;  HostsFileWriter hostsFileWriter=new HostsFileWriter();  hostsFileWriter.initialize(conf,"temp/TestNameNodeMXBean");  try {    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();    cluster.waitActive();    FSNamesystem fsn=cluster.getNameNode().namesystem;    MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();    ObjectName mxbeanName=new ObjectName("Hadoop:service=NameNode,name=NameNodeInfo");    List<String> hosts=new ArrayList<>();    for (    DataNode dn : cluster.getDataNodes()) {      hosts.add(dn.getDisplayName());    }    hostsFileWriter.initIncludeHosts(hosts.toArray(new String[hosts.size()]));    fsn.getBlockManager().getDatanodeManager().refreshNodes(conf);    String liveNodesInfo=(String)(mbs.getAttribute(mxbeanName,"LiveNodes"));
    for (    Map<String,Object> liveNode : liveNodes.values()) {      assertTrue(liveNode.containsKey("lastContact"));      assertTrue(liveNode.containsKey("xferaddr"));    }    Map<String,Long> maintenanceNodes=new HashMap<>();    maintenanceNodes.put(cluster.getDataNodes().get(0).getDisplayName(),Time.now() + expirationInMs);    hostsFileWriter.initOutOfServiceHosts(null,maintenanceNodes);    fsn.getBlockManager().getDatanodeManager().refreshNodes(conf);    boolean recheck=true;    while (recheck) {      String enteringMaintenanceNodesInfo=(String)(mbs.getAttribute(mxbeanName,"EnteringMaintenanceNodes"));      Map<String,Map<String,Object>> enteringMaintenanceNodes=(Map<String,Map<String,Object>>)JSON.parse(enteringMaintenanceNodesInfo);      if (enteringMaintenanceNodes.size() <= 0) {        LOG.info("Waiting for a node to Enter Maintenance state!");        Uninterruptibles.sleepUninterruptibly(1,TimeUnit.SECONDS);        continue;
    if (!finalize) {      FSEditLog spyLog=spy(cluster.getNameNode().getFSImage().getEditLog());      doNothing().when(spyLog).endCurrentLogSegment(true);      DFSTestUtil.setEditLogForTesting(cluster.getNamesystem(),spyLog);    }    fileSys=cluster.getFileSystem();    final FSNamesystem namesystem=cluster.getNamesystem();    FSImage fsimage=namesystem.getFSImage();    fileSys.mkdirs(new Path(TEST_PATH));    fileSys.mkdirs(new Path(TEST_PATH2));    sd=fsimage.getStorage().dirIterator(NameNodeDirType.EDITS).next();  }  finally {    if (cluster != null) {      cluster.shutdown();    }  }  File editFile=FSImageTestUtil.findLatestEditsLog(sd).getFile();  assertTrue("Should exist: " + editFile,editFile.exists());
  finally {    if (cluster != null) {      cluster.shutdown();    }  }  File editFile=FSImageTestUtil.findLatestEditsLog(sd).getFile();  assertTrue("Should exist: " + editFile,editFile.exists());  LOG.info("corrupting edit log file '" + editFile + "'");  corruptor.corrupt(editFile);  cluster=null;  try {    LOG.debug("trying to start normally (this should fail)...");    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).enableManagedDfsDirsRedundancy(false).format(false).build();    cluster.waitActive();    cluster.shutdown();    if (needRecovery) {      fail("expected the corrupted edit log to prevent normal startup");
  } catch (  IOException e) {    if (!needRecovery) {      LOG.error("Got unexpected failure with " + corruptor.getName() + corruptor,e);      fail("got unexpected exception " + e.getMessage());    }  } finally {    if (cluster != null) {      cluster.shutdown();    }  }  cluster=null;  try {    LOG.debug("running recovery...");    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).enableManagedDfsDirsRedundancy(false).format(false).startupOption(recoverStartOpt).build();  } catch (  IOException e) {    fail("caught IOException while trying to recover. " + "message was " + e.getMessage() + "\nstack trace\n"+ StringUtils.stringifyException(e));  } finally {    if (cluster != null) {
    final List<DatanodeDescriptor> live=new ArrayList<DatanodeDescriptor>();    final List<DatanodeDescriptor> dead=new ArrayList<DatanodeDescriptor>();    dm.fetchDatanodes(live,dead,false);    assertTrue(live.size() == 1);    long used, remaining, configCapacity, nonDFSUsed, bpUsed;    float percentUsed, percentRemaining, percentBpUsed;    for (    final DatanodeDescriptor datanode : live) {      used=datanode.getDfsUsed();      remaining=datanode.getRemaining();      nonDFSUsed=datanode.getNonDfsUsed();      configCapacity=datanode.getCapacity();      percentUsed=datanode.getDfsUsedPercent();      percentRemaining=datanode.getRemainingPercent();      bpUsed=datanode.getBlockPoolUsed();      percentBpUsed=datanode.getBlockPoolUsedPercent();
      assertTrue(percentUsed == DFSUtilClient.getPercentUsed(used,configCapacity));      assertTrue(percentRemaining == DFSUtilClient.getPercentRemaining(remaining,configCapacity));      assertTrue(percentBpUsed == DFSUtilClient.getPercentUsed(bpUsed,configCapacity));    }    final FsDatasetTestUtils utils=cluster.getFsDatasetTestUtils(0);    int numOfDataDirs=utils.getDefaultNumOfDataDirs();    long diskCapacity=numOfDataDirs * utils.getRawCapacity();    reserved*=numOfDataDirs;    configCapacity=namesystem.getCapacityTotal();    used=namesystem.getCapacityUsed();    nonDFSUsed=namesystem.getNonDfsUsedSpace();    remaining=namesystem.getCapacityRemaining();    percentUsed=namesystem.getPercentUsed();    percentRemaining=namesystem.getPercentRemaining();    bpUsed=namesystem.getBlockPoolUsedSpace();    percentBpUsed=namesystem.getPercentBlockPoolUsed();
      assertTrue(percentRemaining == DFSUtilClient.getPercentRemaining(remaining,configCapacity));      assertTrue(percentBpUsed == DFSUtilClient.getPercentUsed(bpUsed,configCapacity));    }    final FsDatasetTestUtils utils=cluster.getFsDatasetTestUtils(0);    int numOfDataDirs=utils.getDefaultNumOfDataDirs();    long diskCapacity=numOfDataDirs * utils.getRawCapacity();    reserved*=numOfDataDirs;    configCapacity=namesystem.getCapacityTotal();    used=namesystem.getCapacityUsed();    nonDFSUsed=namesystem.getNonDfsUsedSpace();    remaining=namesystem.getCapacityRemaining();    percentUsed=namesystem.getPercentUsed();    percentRemaining=namesystem.getPercentRemaining();    bpUsed=namesystem.getBlockPoolUsedSpace();    percentBpUsed=namesystem.getPercentBlockPoolUsed();    LOG.info("Data node directory " + cluster.getDataDirectory());
@Test public void testDelete() throws Throwable {  for (  TestMatrixEntry testMatrixEntry : createTestMatrix()) {    Configuration conf=new HdfsConfiguration();    MiniDFSCluster cluster=setupTestCase(conf,testMatrixEntry.getProtectedPaths(),testMatrixEntry.getUnprotectedPaths());    try {
@Test public void testMoveToTrash() throws Throwable {  for (  TestMatrixEntry testMatrixEntry : createTestMatrix()) {    Configuration conf=new HdfsConfiguration();    conf.setInt(DFSConfigKeys.FS_TRASH_INTERVAL_KEY,3600);    MiniDFSCluster cluster=setupTestCase(conf,testMatrixEntry.getProtectedPaths(),testMatrixEntry.getUnprotectedPaths());    try {
@Test public void testRename() throws Throwable {  for (  TestMatrixEntry testMatrixEntry : createTestMatrix()) {    Configuration conf=new HdfsConfiguration();    MiniDFSCluster cluster=setupTestCase(conf,testMatrixEntry.getProtectedPaths(),testMatrixEntry.getUnprotectedPaths());    try {
@Test public void testRenameProtectSubDirs() throws Throwable {  for (  TestMatrixEntry testMatrixEntry : createTestMatrixForProtectSubDirs()) {    Configuration conf=new HdfsConfiguration();    conf.setBoolean(DFS_PROTECTED_SUBDIRECTORIES_ENABLE,true);    MiniDFSCluster cluster=setupTestCase(conf,testMatrixEntry.getProtectedPaths(),testMatrixEntry.getUnprotectedPaths());    try {
@Test public void testMoveProtectedSubDirsToTrash() throws Throwable {  for (  TestMatrixEntry testMatrixEntry : createTestMatrixForProtectSubDirs()) {    Configuration conf=new HdfsConfiguration();    conf.setBoolean(DFS_PROTECTED_SUBDIRECTORIES_ENABLE,true);    conf.setInt(DFSConfigKeys.FS_TRASH_INTERVAL_KEY,3600);    MiniDFSCluster cluster=setupTestCase(conf,testMatrixEntry.getProtectedPaths(),testMatrixEntry.getUnprotectedPaths());    try {
@Test public void testDeleteProtectSubDirs() throws Throwable {  for (  TestMatrixEntry testMatrixEntry : createTestMatrixForProtectSubDirs()) {    Configuration conf=new HdfsConfiguration();    conf.setBoolean(DFS_PROTECTED_SUBDIRECTORIES_ENABLE,true);    MiniDFSCluster cluster=setupTestCase(conf,testMatrixEntry.getProtectedPaths(),testMatrixEntry.getUnprotectedPaths());    try {
  dfs.setStoragePolicy(bar,HdfsConstants.ONESSD_STORAGE_POLICY_NAME);  dfs.setQuotaByStorageType(foo,StorageType.SSD,BLOCKSIZE * 4);  dfs.setQuotaByStorageType(bar,StorageType.SSD,BLOCKSIZE * 2);  INode fnode=fsdir.getINode4Write(foo.toString());  assertTrue(fnode.isDirectory());  assertTrue(fnode.isQuotaSet());  long file1Len=BLOCKSIZE * 3;  int bufLen=BLOCKSIZE / 16;  DFSTestUtil.createFile(dfs,createdFile1foo,bufLen,file1Len,BLOCKSIZE,REPLICATION,seed);  long ssdConsumed=fnode.asDirectory().getDirectoryWithQuotaFeature().getSpaceConsumed().getTypeSpaces().get(StorageType.SSD);  assertEquals(file1Len,ssdConsumed);  try {    dfs.rename(createdFile1foo,createdFile1bar);    fail("Should have failed with QuotaByStorageTypeExceededException ");  } catch (  Throwable t) {
  int bufLen=BLOCKSIZE / 16;  DFSTestUtil.createFile(dfs,createdFile1,bufLen,file1Len,BLOCKSIZE,REPLICATION,seed);  long currentSSDConsumed=fnode.asDirectory().getDirectoryWithQuotaFeature().getSpaceConsumed().getTypeSpaces().get(StorageType.SSD);  assertEquals(file1Len,currentSSDConsumed);  Path createdFile2=new Path(foo,"created_file2.data");  long file2Len=BLOCKSIZE + BLOCKSIZE / 2;  DFSTestUtil.createFile(dfs,createdFile2,bufLen,file2Len,BLOCKSIZE,REPLICATION,seed);  currentSSDConsumed=fnode.asDirectory().getDirectoryWithQuotaFeature().getSpaceConsumed().getTypeSpaces().get(StorageType.SSD);  assertEquals(file1Len + file2Len,currentSSDConsumed);  Path createdFile3=new Path(foo,"created_file3.data");  long file3Len=BLOCKSIZE;  try {    DFSTestUtil.createFile(dfs,createdFile3,bufLen,file3Len,BLOCKSIZE,REPLICATION,seed);    fail("Should have failed with QuotaByStorageTypeExceededException ");  } catch (  Throwable t) {
  Path createdFile1=new Path(child,"created_file1.data");  long file1Len=BLOCKSIZE * 2 + BLOCKSIZE / 2;  int bufLen=BLOCKSIZE / 16;  DFSTestUtil.createFile(dfs,createdFile1,bufLen,file1Len,BLOCKSIZE,replication,seed);  INode fnode=fsdir.getINode4Write(parent.toString());  assertTrue(fnode.isDirectory());  assertTrue(fnode.isQuotaSet());  long currentSSDConsumed=fnode.asDirectory().getDirectoryWithQuotaFeature().getSpaceConsumed().getTypeSpaces().get(StorageType.SSD);  assertEquals(file1Len,currentSSDConsumed);  Path createdFile2=new Path(child,"created_file2.data");  long file2Len=BLOCKSIZE;  try {    DFSTestUtil.createFile(dfs,createdFile2,bufLen,file2Len,BLOCKSIZE,replication,seed);    fail("Should have failed with QuotaByStorageTypeExceededException ");  } catch (  Throwable t) {
@Test(timeout=60000) public void testQuotaByStorageTypeParentOnChildOn() throws Exception {  final Path parent=new Path(dir,"parent");  final Path child=new Path(parent,"child");  dfs.mkdirs(parent);  dfs.mkdirs(child);  dfs.setStoragePolicy(parent,HdfsConstants.ONESSD_STORAGE_POLICY_NAME);  dfs.setQuotaByStorageType(parent,StorageType.SSD,2 * BLOCKSIZE);  dfs.setQuotaByStorageType(child,StorageType.SSD,3 * BLOCKSIZE);  Path createdFile1=new Path(child,"created_file1.data");  long file1Len=BLOCKSIZE * 2 + BLOCKSIZE / 2;  int bufLen=BLOCKSIZE / 16;  try {    DFSTestUtil.createFile(dfs,createdFile1,bufLen,file1Len,BLOCKSIZE,REPLICATION,seed);    fail("Should have failed with QuotaByStorageTypeExceededException ");  } catch (  Throwable t) {
  dfs.mkdirs(testDir);  dfs.setStoragePolicy(testDir,HdfsConstants.ONESSD_STORAGE_POLICY_NAME);  final long ssdQuota=BLOCKSIZE * ssdQuotaInBlocks;  final long storageSpaceQuota=BLOCKSIZE * storageSpaceQuotaInBlocks;  dfs.setQuota(testDir,Long.MAX_VALUE - 1,storageSpaceQuota);  dfs.setQuotaByStorageType(testDir,StorageType.SSD,ssdQuota);  INode testDirNode=fsdir.getINode4Write(testDir.toString());  assertTrue(testDirNode.isDirectory());  assertTrue(testDirNode.isQuotaSet());  Path createdFile=new Path(testDir,"created_file.data");  long fileLen=testFileLenInBlocks * BLOCKSIZE;  try {    DFSTestUtil.createFile(dfs,createdFile,BLOCKSIZE / 16,fileLen,BLOCKSIZE,replication,seed);    fail("Should have failed with DSQuotaExceededException or " + "QuotaByStorageTypeExceededException ");  } catch (  Throwable t) {
  final long storageTypeSpaceQuota=BLOCKSIZE * 1;  dfs.setQuota(testDir,HdfsConstants.QUOTA_DONT_SET,storageSpaceQuota);  Path createdFile;  final long fileLen=BLOCKSIZE;  createdFile=new Path(testDir,"file1.data");  DFSTestUtil.createFile(dfs,createdFile,BLOCKSIZE / 16,fileLen,BLOCKSIZE,REPLICATION,seed);  assertTrue(dfs.exists(createdFile));  assertTrue(dfs.isFile(createdFile));  dfs.setQuotaByStorageType(testDir,StorageType.DISK,storageTypeSpaceQuota);  dfs.setStoragePolicy(testDir,HdfsConstants.WARM_STORAGE_POLICY_NAME);  try {    createdFile=new Path(testDir,"file2.data");    DFSTestUtil.createFile(dfs,createdFile,BLOCKSIZE / 16,fileLen,BLOCKSIZE,REPLICATION,seed);    fail("should fail on QuotaByStorageTypeExceededException");  } catch (  QuotaByStorageTypeExceededException e) {
  assertNotEquals(fei0.getEzKeyVersionName(),zs.getEzKeyVersionName());  assertEquals(fei1.getEzKeyVersionName(),zs.getEzKeyVersionName());  assertEquals(10,zs.getFilesReencrypted());  dfsAdmin.reencryptEncryptionZone(zone,ReencryptAction.START);  waitForReencryptedZones(3);  assertKeyVersionEquals(encFile1,fei1);  try {    dfsAdmin.reencryptEncryptionZone(subdir,ReencryptAction.START);    fail("Re-encrypting non-EZ should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertExceptionContains("not the root of an encryption zone",expected);  }  try {    dfsAdmin.reencryptEncryptionZone(new Path(zone,"notexist"),ReencryptAction.START);    fail("Re-encrypting non-existing dir should fail");
  try {    dfsAdmin.reencryptEncryptionZone(subdir,ReencryptAction.START);    fail("Re-encrypting non-EZ should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertExceptionContains("not the root of an encryption zone",expected);  }  try {    dfsAdmin.reencryptEncryptionZone(new Path(zone,"notexist"),ReencryptAction.START);    fail("Re-encrypting non-existing dir should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertTrue(expected.unwrapRemoteException() instanceof FileNotFoundException);  }  try {    dfsAdmin.reencryptEncryptionZone(encFile1,ReencryptAction.START);    fail("Re-encrypting on a file should fail");
  try {    dfsAdmin.reencryptEncryptionZone(new Path(zone,"notexist"),ReencryptAction.START);    fail("Re-encrypting non-existing dir should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertTrue(expected.unwrapRemoteException() instanceof FileNotFoundException);  }  try {    dfsAdmin.reencryptEncryptionZone(encFile1,ReencryptAction.START);    fail("Re-encrypting on a file should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertExceptionContains("not the root of an encryption zone",expected);  }  getEzManager().pauseReencryptForTesting();  dfsAdmin.reencryptEncryptionZone(zone,ReencryptAction.START);  waitForQueuedZones(1);
    DFSTestUtil.createFile(fs,new Path(zone,Integer.toString(i)),len,(short)1,0xFEED);  }  final Path subdir=new Path("/dir");  fsWrapper.mkdir(subdir,FsPermission.getDirDefault(),true);  DFSTestUtil.createFile(fs,new Path(subdir,"f"),len,(short)1,0xFEED);  final Path zoneSnap=fs.createSnapshot(zone);  fsWrapper.rename(new Path(zone,"5"),new Path(zone,"5new"));  fsWrapper.rename(new Path(zone,"6"),new Path(zone,"6new"));  fsWrapper.delete(new Path(zone,"6new"),true);  final Path encFile1=new Path(zone,"0");  final FileEncryptionInfo fei0=getFileEncryptionInfo(encFile1);  rollKey(TEST_KEY);  try {    dfsAdmin.reencryptEncryptionZone(zoneSnap,ReencryptAction.START);    fail("Reencrypt command on snapshot path should fail.");  } catch (  RemoteException expected) {
private void waitForReencryptedZones(final int expected) throws TimeoutException, InterruptedException {
private void waitForQueuedZones(final int expected) throws TimeoutException, InterruptedException {
private void waitForTotalZones(final int expected) throws TimeoutException, InterruptedException {
private void waitForZoneCompletes(final String zone) throws TimeoutException, InterruptedException {
private void waitForReencryptedFiles(final String zone,final int expected) throws TimeoutException, InterruptedException {
 catch (  RemoteException expected) {    assertExceptionContains("not under re-encryption",expected);  }  rollKey(TEST_KEY);  getEzManager().pauseForTestingAfterNthSubmission(1);  getEzManager().resumeReencryptForTesting();  dfsAdmin.reencryptEncryptionZone(zone,ReencryptAction.START);  waitForReencryptedFiles(zone.toString(),5);  dfsAdmin.reencryptEncryptionZone(zone,ReencryptAction.CANCEL);  getEzManager().resumeReencryptForTesting();  waitForZoneCompletes(zone.toString());  assertEquals(5,getZoneStatus(zone.toString()).getFilesReencrypted());  assertNull(getEzManager().getZoneStatus(zone.toString()).getLastCheckpointFile());  assertNull(getReencryptionStatus().getNextUnprocessedZone());  try {    dfsAdmin.reencryptEncryptionZone(subdir,ReencryptAction.CANCEL);
  dfsAdmin.reencryptEncryptionZone(zone,ReencryptAction.START);  waitForReencryptedFiles(zone.toString(),5);  dfsAdmin.reencryptEncryptionZone(zone,ReencryptAction.CANCEL);  getEzManager().resumeReencryptForTesting();  waitForZoneCompletes(zone.toString());  assertEquals(5,getZoneStatus(zone.toString()).getFilesReencrypted());  assertNull(getEzManager().getZoneStatus(zone.toString()).getLastCheckpointFile());  assertNull(getReencryptionStatus().getNextUnprocessedZone());  try {    dfsAdmin.reencryptEncryptionZone(subdir,ReencryptAction.CANCEL);    fail("Re-encrypting non-EZ should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertExceptionContains("not the root of an encryption zone",expected);  }  try {
  assertNull(getReencryptionStatus().getNextUnprocessedZone());  try {    dfsAdmin.reencryptEncryptionZone(subdir,ReencryptAction.CANCEL);    fail("Re-encrypting non-EZ should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertExceptionContains("not the root of an encryption zone",expected);  }  try {    dfsAdmin.reencryptEncryptionZone(new Path(zone,"notexist"),ReencryptAction.CANCEL);    fail("Re-encrypting non-existing dir should fail");  } catch (  RemoteException expected) {    LOG.info("Expected exception caught.",expected);    assertTrue(expected.unwrapRemoteException() instanceof FileNotFoundException);  }  final Path encFile=new Path(zone,"0");  try {
shouldFail=false;break;default:fail("Unknown fail type");break;}try {doAnEdit(fsn,1);fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);try {fsn.saveNamespace(0,0);if (shouldFail) {fail("Did not fail!");}} catch (Exception e) {if (!shouldFail) {throw e;} else {
    originalImage.close();    fsn.close();    fsn=null;    LOG.info("Loading new FSmage from disk.");    fsn=FSNamesystem.loadFromDisk(conf);    LOG.info("Checking reloaded image.");    checkEditExists(fsn,1);    LOG.info("Reloaded image is good.");  }  finally {    if (rootDir.exists()) {      fs.setPermission(rootPath,permissionAll);    }    if (fsn != null) {      try {        fsn.close();      } catch (      Throwable t) {
  NNStorage storage=originalImage.getStorage();  storage.close();  NNStorage spyStorage=spy(storage);  originalImage.storage=spyStorage;  FSImage spyImage=spy(originalImage);  Whitebox.setInternalState(fsn,"fsImage",spyImage);  spyImage.storage.setStorageDirectories(FSNamesystem.getNamespaceDirs(conf),FSNamesystem.getNamespaceEditsDirs(conf));  doThrow(new IOException("Injected fault: saveFSImage")).when(spyImage).saveFSImage(any(),any(),any());  try {    doAnEdit(fsn,1);    fsn.setSafeMode(SafeModeAction.SAFEMODE_ENTER);    try {      fsn.saveNamespace(0,0);      fail("saveNamespace did not fail even when all directories failed!");    } catch (    IOException ioe) {
public void createCheckPoint(int count) throws IOException {  LOG.info("--starting mini cluster");  MiniDFSCluster cluster=null;  SecondaryNameNode sn=null;  try {    cluster=new MiniDFSCluster.Builder(config).manageDataDfsDirs(false).manageNameDfsDirs(false).build();    cluster.waitActive();    LOG.info("--starting Secondary Node");    sn=new SecondaryNameNode(config);    assertNotNull(sn);    for (int i=0; i < count; i++) {      FileSystem fileSys=cluster.getFileSystem();      Path p=new Path("t" + i);      DFSTestUtil.createFile(fileSys,p,fileSize,fileSize,blockSize,(short)1,seed);
public void invalidateStorage(FSImage fi,Set<File> filesToInvalidate) throws IOException {  ArrayList<StorageDirectory> al=new ArrayList<StorageDirectory>(2);  Iterator<StorageDirectory> it=fi.getStorage().dirIterator();  while (it.hasNext()) {    StorageDirectory sd=it.next();    if (filesToInvalidate.contains(sd.getRoot())) {
@Test public void testDfsAdminCmd() throws Exception {  cluster=new MiniDFSCluster.Builder(config).numDataNodes(2).manageNameDfsDirs(false).build();  cluster.waitActive();  try {    FSImage fsi=cluster.getNameNode().getFSImage();    boolean restore=fsi.getStorage().getRestoreFailedStorage();
  INodeFile inodeFile;  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,new Short((short)3),StripedFileTestUtil.getDefaultECPolicy().getId(),1024L,HdfsConstants.WARM_STORAGE_POLICY_ID,STRIPED);    fail("INodeFile construction should fail when both replication and " + "ECPolicy requested!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,null,null,1024L,HdfsConstants.WARM_STORAGE_POLICY_ID,STRIPED);    fail("INodeFile construction should fail when EC Policy param not " + "provided for striped layout!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,null,Byte.MAX_VALUE,1024L,HdfsConstants.WARM_STORAGE_POLICY_ID,STRIPED);    fail("INodeFile construction should fail when EC Policy is " + "not in the supported list!");  } catch (  IllegalArgumentException iae) {
    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,null,null,1024L,HdfsConstants.WARM_STORAGE_POLICY_ID,STRIPED);    fail("INodeFile construction should fail when EC Policy param not " + "provided for striped layout!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,null,Byte.MAX_VALUE,1024L,HdfsConstants.WARM_STORAGE_POLICY_ID,STRIPED);    fail("INodeFile construction should fail when EC Policy is " + "not in the supported list!");  } catch (  IllegalArgumentException iae) {    LOG.info("Expected exception: ",iae);  }  final Byte ecPolicyID=StripedFileTestUtil.getDefaultECPolicy().getId();  try {    new INodeFile(HdfsConstants.GRANDFATHER_INODE_ID,null,perm,0L,0L,null,null,ecPolicyID,1024L,HdfsConstants.WARM_STORAGE_POLICY_ID,CONTIGUOUS);    fail("INodeFile construction should fail when replication param is " + "provided for striped layout!");
@Test public void testDownloadingLaterCheckpoint() throws Exception {  nn0.getRpcServer().rollEditLog();  nn0.getRpcServer().rollEditLog();  NameNodeAdapter.enterSafeMode(nn0,false);  NameNodeAdapter.saveNamespace(nn0);  NameNodeAdapter.leaveSafeMode(nn0);  long expectedCheckpointTxId=NameNodeAdapter.getNamesystem(nn0).getFSImage().getMostRecentCheckpointTxId();  assertEquals(6,expectedCheckpointTxId);  cluster.getFileSystem(0).create(new Path("/test_txid"),(short)1).close();  URI editsUri=cluster.getSharedEditsDir(0,maxNNCount - 1);  long seen_txid_shared=FSImageTestUtil.getStorageTxId(nn0,editsUri);  for (int i=1; i < maxNNCount; i++) {    assertEquals(0,forceBootstrap(i));
private void removeStandbyNameDirs(){  for (int i=1; i < maxNNCount; i++) {    for (    URI u : cluster.getNameDirs(i)) {      assertTrue(u.getScheme().equals("file"));      File dir=new File(u.getPath());
@Test public void testStartingWithUpgradeInProgressSucceeds() throws Exception {  MiniDFSCluster cluster=null;  try {    cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(0).build();    for (int i=0; i < 2; i++) {      for (      URI uri : cluster.getNameDirs(i)) {        File prevTmp=new File(new File(uri),Storage.STORAGE_TMP_PREVIOUS);
    AppendTestUtil.write(out,10,10);    out.hflush();    cluster.triggerBlockReports();    numQueued+=numDN * 2;  }  finally {    IOUtils.closeStream(out);    cluster.triggerHeartbeats();    numQueued+=numDN;  }  assertEquals(numQueued,cluster.getNameNode(1).getNamesystem().getPendingDataNodeMessageCount());  try {    out=fs.append(TEST_FILE_PATH);    AppendTestUtil.write(out,20,10);  }  finally {    IOUtils.closeStream(out);    numQueued+=numDN;
private void banner(String string){
  identifier.readFields(new DataInputStream(new ByteArrayInputStream(tokenId)));  LOG.info("A valid token should have non-null password, " + "and should be renewed successfully");  assertTrue(null != dtSecretManager.retrievePassword(identifier));  dtSecretManager.renewToken(token,"JobTracker");  cluster.transitionToStandby(0);  try {    cluster.getNameNodeRpc(0).renewDelegationToken(token);    fail("StandbyException is expected since nn0 is in standby state");  } catch (  StandbyException e) {    GenericTestUtils.assertExceptionContains(HAServiceState.STANDBY.toString(),e);  }  new Thread(){    @Override public void run(){      try {        cluster.transitionToActive(1);      } catch (      Exception e) {
  } catch (  StandbyException e) {    GenericTestUtils.assertExceptionContains(HAServiceState.STANDBY.toString(),e);  }  new Thread(){    @Override public void run(){      try {        cluster.transitionToActive(1);      } catch (      Exception e) {        LOG.error("Transition nn1 to active failed",e);      }    }  }.start();  Thread.sleep(1000);  try {    nn1.getNamesystem().verifyToken(token.decodeIdentifier(),token.getPassword());    fail("RetriableException/StandbyException is expected since nn1 is in transition");  } catch (  IOException e) {    assertTrue(e instanceof StandbyException || e instanceof RetriableException);
private void testFailoverFinalizesAndReadsInProgress(boolean partialTxAtEnd) throws Exception {  Configuration conf=new Configuration();  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(0).build();  try {    URI sharedUri=cluster.getSharedEditsDir(0,1);    File sharedDir=new File(sharedUri.getPath(),"current");    FSNamesystem fsn=cluster.getNamesystem(0);    FSImageTestUtil.createAbortedLogWithMkdirs(sharedDir,NUM_DIRS_IN_LOG,1,fsn.getFSDirectory().getLastInodeId() + 1);    assertEditFiles(Collections.singletonList(sharedUri),NNStorage.getInProgressEditsFileName(1));    if (partialTxAtEnd) {      FileOutputStream outs=null;      try {        File editLogFile=new File(sharedDir,NNStorage.getInProgressEditsFileName(1));        outs=new FileOutputStream(editLogFile,true);        outs.write(new byte[]{0x18,0x00,0x00,0x00});
private void assertEditFiles(Iterable<URI> dirs,String... files) throws IOException {  for (  URI u : dirs) {    File editDirRoot=new File(u.getPath());    File editDir=new File(editDirRoot,"current");    GenericTestUtils.assertExists(editDir);    if (files.length == 0) {
@Before public void setUpCluster() throws Exception {  conf=new Configuration();  conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_CHECK_PERIOD_KEY,1);  conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,1);  conf.setInt(DFSConfigKeys.DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED_KEY,10);  conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY,1);  conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_EDITS_ASYNC_LOGGING,useAsyncEditLogging);  HAUtil.setAllowStandbyReads(conf,true);  if (clusterType == TestType.SHARED_DIR_HA) {    int basePort=10000;    int retryCount=0;    while (true) {      try {        basePort=10000 + RANDOM.nextInt(1000) * 4;
  if (clusterType == TestType.SHARED_DIR_HA) {    int basePort=10000;    int retryCount=0;    while (true) {      try {        basePort=10000 + RANDOM.nextInt(1000) * 4;        LOG.info("Set SHARED_DIR_HA cluster's basePort to " + basePort);        MiniDFSNNTopology topology=MiniQJMHACluster.createDefaultTopology(basePort);        cluster=new MiniDFSCluster.Builder(conf).nnTopology(topology).numDataNodes(0).checkExitOnShutdown(false).build();        break;      } catch (      BindException e) {        if (cluster != null) {          cluster.shutdown(true);          cluster=null;        }        ++retryCount;
@Test(timeout=300000) public void testClientRetrySafeMode() throws Exception {  final Map<Path,Boolean> results=Collections.synchronizedMap(new HashMap<Path,Boolean>());  final Path test=new Path("/test");  cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY,3);  NameNodeAdapter.enterSafeMode(nn0,false);  Whitebox.setInternalState(nn0.getNamesystem(),"manualSafeMode",false);  BlockManagerTestUtil.setStartupSafeModeForTest(nn0.getNamesystem().getBlockManager());  assertTrue(nn0.getNamesystem().isInStartupSafeMode());  LOG.info("enter safemode");  new Thread(){    @Override public void run(){      try {        boolean mkdir=fs.mkdirs(test);
  cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY,3);  NameNodeAdapter.enterSafeMode(nn0,false);  Whitebox.setInternalState(nn0.getNamesystem(),"manualSafeMode",false);  BlockManagerTestUtil.setStartupSafeModeForTest(nn0.getNamesystem().getBlockManager());  assertTrue(nn0.getNamesystem().isInStartupSafeMode());  LOG.info("enter safemode");  new Thread(){    @Override public void run(){      try {        boolean mkdir=fs.mkdirs(test);        LOG.info("mkdir finished, result is " + mkdir);synchronized (TestHASafeMode.this) {          results.put(test,mkdir);          TestHASafeMode.this.notifyAll();        }      } catch (      Exception e) {
static void banner(String string){
private void testManualFailoverFailback(MiniDFSCluster cluster,Configuration conf,int nsIndex) throws Exception {  int nn0=2 * nsIndex, nn1=2 * nsIndex + 1;  cluster.transitionToActive(nn0);
private void testManualFailoverFailback(MiniDFSCluster cluster,Configuration conf,int nsIndex) throws Exception {  int nn0=2 * nsIndex, nn1=2 * nsIndex + 1;  cluster.transitionToActive(nn0);  LOG.info("Starting with NN 0 active in namespace " + nsIndex);  FileSystem fs=HATestUtil.configureFailoverFs(cluster,conf);  fs.mkdirs(TEST_DIR);
private void testManualFailoverFailback(MiniDFSCluster cluster,Configuration conf,int nsIndex) throws Exception {  int nn0=2 * nsIndex, nn1=2 * nsIndex + 1;  cluster.transitionToActive(nn0);  LOG.info("Starting with NN 0 active in namespace " + nsIndex);  FileSystem fs=HATestUtil.configureFailoverFs(cluster,conf);  fs.mkdirs(TEST_DIR);  LOG.info("Failing over to NN 1 in namespace " + nsIndex);  cluster.transitionToStandby(nn0);  cluster.transitionToActive(nn1);  assertTrue(fs.exists(TEST_DIR));  DFSTestUtil.writeFile(fs,TEST_FILE_PATH,TEST_FILE_DATA);
  FileSystem fs=HATestUtil.configureFailoverFs(cluster,conf);  fs.mkdirs(TEST_DIR);  LOG.info("Failing over to NN 1 in namespace " + nsIndex);  cluster.transitionToStandby(nn0);  cluster.transitionToActive(nn1);  assertTrue(fs.exists(TEST_DIR));  DFSTestUtil.writeFile(fs,TEST_FILE_PATH,TEST_FILE_DATA);  LOG.info("Failing over to NN 0 in namespace " + nsIndex);  cluster.transitionToStandby(nn1);  cluster.transitionToActive(nn0);  assertTrue(fs.exists(TEST_DIR));  assertEquals(TEST_FILE_DATA,DFSTestUtil.readFile(fs,TEST_FILE_PATH));  LOG.info("Removing test file");  fs.delete(TEST_DIR,true);  assertFalse(fs.exists(TEST_DIR));
static void banner(String string){
@Test public void testFsckWithObserver() throws Exception {  setObserverRead(true);  dfs.create(testPath,(short)1).close();  assertSentTo(0);  final String result=TestFsck.runFsck(conf,0,true,"/");
@Test public void testFsckDelete() throws Exception {  setObserverRead(true);  DFSTestUtil.createFile(dfs,testPath,512,(short)1,0);  DFSTestUtil.waitForReplication(dfs,testPath,(short)1,5000);  ExtendedBlock block=DFSTestUtil.getFirstBlock(dfs,testPath);  int dnToCorrupt=DFSTestUtil.firstDnWithBlock(dfsCluster,block);  FSNamesystem ns=dfsCluster.getNameNode(0).getNamesystem();  dfsCluster.corruptReplica(dnToCorrupt,block);  dfsCluster.restartDataNode(dnToCorrupt);  DFSTestUtil.waitCorruptReplicas(dfs,ns,testPath,block,1);  final String result=TestFsck.runFsck(conf,1,true,"/","-delete");
  MiniDFSCluster cluster=newMiniCluster(conf,5);  try {    cluster.waitActive();    cluster.transitionToActive(0);    Thread.sleep(500);    LOG.info("Starting with NN 0 active");    FileSystem fs=HATestUtil.configureFailoverFs(cluster,conf);    stm=fs.create(TEST_PATH);    AppendTestUtil.write(stm,0,BLOCK_AND_A_HALF);    stm.hflush();    int nextActive=failover(cluster,scenario);    assertTrue(fs.exists(TEST_PATH));    cluster.stopDataNode(0);    AppendTestUtil.write(stm,BLOCK_AND_A_HALF,BLOCK_AND_A_HALF);    stm.hflush();
  conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,BLOCK_SIZE);  FSDataOutputStream stm=null;  final MiniDFSCluster cluster=newMiniCluster(conf,3);  try {    cluster.waitActive();    cluster.transitionToActive(0);    Thread.sleep(500);    LOG.info("Starting with NN 0 active");    FileSystem fs=HATestUtil.configureFailoverFs(cluster,conf);    stm=fs.create(TEST_PATH);    AppendTestUtil.write(stm,0,BLOCK_SIZE / 2);    stm.hflush();    NameNode nn0=cluster.getNameNode(0);    ExtendedBlock blk=DFSTestUtil.getFirstBlock(fs,TEST_PATH);    DatanodeDescriptor expectedPrimary=DFSTestUtil.getExpectedPrimaryNode(nn0,blk);
@Test(timeout=STRESS_RUNTIME * 3) public void testPipelineRecoveryStress() throws Exception {  LOG.info("HDFS-6694 Debug Data BEGIN");  String[][] scmds=new String[][]{{"/bin/sh","-c","ulimit -a"},{"hostname"},{"ifconfig","-a"}};  for (  String[] scmd : scmds) {    String scmd_str=StringUtils.join(" ",scmd);    try {      ShellCommandExecutor sce=new ShellCommandExecutor(scmd);      sce.execute();
public void testClientRetryWithFailover(final AtMostOnceOp op) throws Exception {  final Map<String,Object> results=new ConcurrentHashMap<>();  op.prepare();  DummyRetryInvocationHandler.block.set(true);  new Thread(){    @Override public void run(){      try {        op.invoke();        Object result=op.getResult();
        op.invoke();        Object result=op.getResult();        LOG.info("Operation " + op.name + " finished");        results.put(op.name,result == null ? "SUCCESS" : result);      } catch (      Exception e) {        LOG.info("Got Exception while calling " + op.name,e);      } finally {        IOUtils.cleanup(null,op.client);      }    }  }.start();  assertTrue("After waiting the operation " + op.name + " still has not taken effect on NN yet",op.checkNamenodeBeforeReturn());  cluster.transitionToStandby(0);  cluster.transitionToActive(1);  LOG.info("Setting block to false");  DummyRetryInvocationHandler.block.set(false);  GenericTestUtils.waitFor(() -> results.containsKey(op.name),5,10000);
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED_KEY,1);  conf.setInt(DFSConfigKeys.DFS_NAMENODE_NUM_EXTRA_EDITS_RETAINED_KEY,0);  int retryCount=0;  while (true) {    try {      int basePort=10060 + random.nextInt(100) * 2;      MiniDFSNNTopology topology=new MiniDFSNNTopology().addNameservice(new MiniDFSNNTopology.NSConf("ns1").addNN(new MiniDFSNNTopology.NNConf("nn1").setHttpPort(basePort)).addNN(new MiniDFSNNTopology.NNConf("nn2").setHttpPort(basePort + 1)).addNN(new MiniDFSNNTopology.NNConf("nn3").setHttpPort(basePort + 2)));      cluster=new MiniDFSCluster.Builder(conf).nnTopology(topology).numDataNodes(1).build();      cluster.waitActive();      setNNs();      fs=HATestUtil.configureFailoverFs(cluster,conf);      cluster.transitionToActive(0);      ++retryCount;      break;    } catch (    BindException e) {
private static void assertEditFiles(Iterable<URI> dirs,String... files) throws IOException {  for (  URI u : dirs) {    File editDirRoot=new File(u.getPath());    File editDir=new File(editDirRoot,"current");    GenericTestUtils.assertExists(editDir);    if (files.length == 0) {
public static Path createSnapshot(DistributedFileSystem hdfs,Path snapshotRoot,String snapshotName) throws Exception {
@Test(timeout=600000) public void testOpenFileDeletionAndNNRestart() throws Exception {  final Path snapRootDir=new Path("/level_0_A/test");  final String hbaseFileName="hbase.log";  final String snap1Name="snap_1";  final Path hbaseFile=new Path(snapRootDir,hbaseFileName);  createFile(hbaseFile);  FSDataOutputStream hbaseOutputStream=fs.append(hbaseFile);  int newWriteLength=(int)(BLOCKSIZE * 1.5);  byte[] buf=new byte[newWriteLength];  Random random=new Random();  random.nextBytes(buf);  writeToStream(hbaseOutputStream,buf);  final Path snap1Dir=SnapshotTestHelper.createSnapshot(fs,snapRootDir,snap1Name);
          flumeOutputStream.write(bytes);          if (hbaseOutputStream != null) {            hbaseOutputStream.write(bytes);          }          if (i == 50000) {            startLatch.countDown();          } else           if (i == 100000) {            deleteLatch.countDown();          } else           if (i == 150000) {            hbaseOutputStream.hsync();            fs.delete(hbaseFile,true);            try {              hbaseOutputStream.close();            } catch (            Exception e) {            }            hbaseOutputStream=null;          } else           if (i % 5000 == 0) {
 else           if (i == 150000) {            hbaseOutputStream.hsync();            fs.delete(hbaseFile,true);            try {              hbaseOutputStream.close();            } catch (            Exception e) {            }            hbaseOutputStream=null;          } else           if (i % 5000 == 0) {            LOG.info("Write pos: " + flumeOutputStream.getPos() + ", size: "+ fs.getFileStatus(flumeFile).getLen()+ ", loop: "+ (i + 1));          }        }      } catch (      Exception e) {        LOG.warn("Writer error: " + e);        writerError.set(true);      }    }  });  t.start();  startLatch.await();
            fs.delete(hbaseFile,true);            try {              hbaseOutputStream.close();            } catch (            Exception e) {            }            hbaseOutputStream=null;          } else           if (i % 5000 == 0) {            LOG.info("Write pos: " + flumeOutputStream.getPos() + ", size: "+ fs.getFileStatus(flumeFile).getLen()+ ", loop: "+ (i + 1));          }        }      } catch (      Exception e) {        LOG.warn("Writer error: " + e);        writerError.set(true);      }    }  });  t.start();  startLatch.await();  final Path snap1Dir=SnapshotTestHelper.createSnapshot(fs,snapRootDir,snap1Name);  final Path flumeS1Path=new Path(snap1Dir,flumeFileName);
            try {              hbaseOutputStream.close();            } catch (            Exception e) {            }            hbaseOutputStream=null;          } else           if (i % 5000 == 0) {            LOG.info("Write pos: " + flumeOutputStream.getPos() + ", size: "+ fs.getFileStatus(flumeFile).getLen()+ ", loop: "+ (i + 1));          }        }      } catch (      Exception e) {        LOG.warn("Writer error: " + e);        writerError.set(true);      }    }  });  t.start();  startLatch.await();  final Path snap1Dir=SnapshotTestHelper.createSnapshot(fs,snapRootDir,snap1Name);  final Path flumeS1Path=new Path(snap1Dir,flumeFileName);  LOG.info("Snap1 file status: " + fs.getFileStatus(flumeS1Path));
              hbaseOutputStream.close();            } catch (            Exception e) {            }            hbaseOutputStream=null;          } else           if (i % 5000 == 0) {            LOG.info("Write pos: " + flumeOutputStream.getPos() + ", size: "+ fs.getFileStatus(flumeFile).getLen()+ ", loop: "+ (i + 1));          }        }      } catch (      Exception e) {        LOG.warn("Writer error: " + e);        writerError.set(true);      }    }  });  t.start();  startLatch.await();  final Path snap1Dir=SnapshotTestHelper.createSnapshot(fs,snapRootDir,snap1Name);  final Path flumeS1Path=new Path(snap1Dir,flumeFileName);  LOG.info("Snap1 file status: " + fs.getFileStatus(flumeS1Path));  LOG.info("Current file status: " + fs.getFileStatus(flumeFile));
@Test(timeout=900000) public void testRandomOperationsWithSnapshots() throws IOException, InterruptedException, TimeoutException {  long seed=System.currentTimeMillis();
@Test(timeout=900000) public void testRandomOperationsWithSnapshots() throws IOException, InterruptedException, TimeoutException {  long seed=System.currentTimeMillis();  LOG.info("testRandomOperationsWithSnapshots, seed to be used: " + seed);  generator=new Random(seed);  int fileLen=generator.nextInt(MAX_NUM_FILE_LENGTH);  createFiles(TESTDIRSTRING,fileLen);  SnapshottableDirectoryStatus[] snapshottableDirectoryStatus=hdfs.getSnapshottableDirListing();  for (  SnapshottableDirectoryStatus ssds : snapshottableDirectoryStatus) {    snapshottableDirectories.add(ssds.getFullPath());  }  if (snapshottableDirectories.size() == 0) {    hdfs.allowSnapshot(hdfs.getHomeDirectory());    snapshottableDirectories.add(hdfs.getHomeDirectory());  }  int numberOfIterations=generator.nextInt(MAX_NUM_ITERATIONS);  LOG.info("Number of iterations: " + numberOfIterations);  int numberFileSystemOperations=generator.nextInt(MAX_NUM_FILESYSTEM_OPERATIONS - MIN_NUM_OPERATIONS + 1) + MIN_NUM_OPERATIONS;
  LOG.info("testRandomOperationsWithSnapshots, seed to be used: " + seed);  generator=new Random(seed);  int fileLen=generator.nextInt(MAX_NUM_FILE_LENGTH);  createFiles(TESTDIRSTRING,fileLen);  SnapshottableDirectoryStatus[] snapshottableDirectoryStatus=hdfs.getSnapshottableDirListing();  for (  SnapshottableDirectoryStatus ssds : snapshottableDirectoryStatus) {    snapshottableDirectories.add(ssds.getFullPath());  }  if (snapshottableDirectories.size() == 0) {    hdfs.allowSnapshot(hdfs.getHomeDirectory());    snapshottableDirectories.add(hdfs.getHomeDirectory());  }  int numberOfIterations=generator.nextInt(MAX_NUM_ITERATIONS);  LOG.info("Number of iterations: " + numberOfIterations);  int numberFileSystemOperations=generator.nextInt(MAX_NUM_FILESYSTEM_OPERATIONS - MIN_NUM_OPERATIONS + 1) + MIN_NUM_OPERATIONS;  LOG.info("Number of FileSystem operations: " + numberFileSystemOperations);  int numberSnapshotOperations=generator.nextInt(MAX_NUM_SNAPSHOT_OPERATIONS - MIN_NUM_OPERATIONS) + MIN_NUM_OPERATIONS;
public void randomOperationsWithSnapshots(int numberOfIterations,int numberFileSystemOperations,int numberSnapshotOperations) throws IOException, InterruptedException, TimeoutException {  for (int i=0; i < numberOfIterations; i++) {    for (int j=0; j < numberFileSystemOperations; j++) {      Operations fsOperation=Operations.getRandomOperation(OperationType.FileSystem);
      break;case FileSystem_DeleteDir:    deleteTestDir();  break;case FileSystem_RenameDir:renameTestDir();break;case FileSystem_CreateFile:createTestFile();break;case FileSystem_DeleteFile:deleteTestFile();break;case FileSystem_RenameFile:renameTestFile();break;default:assertNull("Invalid FileSystem operation",fsOperation);break;}}for (int k=0; k < numberSnapshotOperations; k++) {Operations snapshotOperation=Operations.getRandomOperation(OperationType.Snapshot);
private void createSnapshot() throws IOException {  if (snapshottableDirectories.size() > 0) {    int index=generator.nextInt(snapshottableDirectories.size());    Path randomDir=snapshottableDirectories.get(index);    String snapshotName=Integer.toString(generator.nextInt()) + ".ss";    hdfs.createSnapshot(randomDir,snapshotName);
private void deleteSnapshot() throws IOException {  if (!pathToSnapshotsMap.isEmpty()) {    int index=generator.nextInt(pathToSnapshotsMap.size());    Object[] snapshotPaths=pathToSnapshotsMap.keySet().toArray();    Path snapshotPath=(Path)snapshotPaths[index];    ArrayList<String> snapshotNameList=pathToSnapshotsMap.get(snapshotPath);    String snapshotNameToBeDeleted=snapshotNameList.get(generator.nextInt(snapshotNameList.size()));    hdfs.deleteSnapshot(snapshotPath,snapshotNameToBeDeleted);
private void renameSnapshot() throws IOException {  if (!pathToSnapshotsMap.isEmpty()) {    int index=generator.nextInt(pathToSnapshotsMap.size());    Object[] snapshotPaths=pathToSnapshotsMap.keySet().toArray();    Path snapshotPath=(Path)snapshotPaths[index];    ArrayList<String> snapshotNameList=pathToSnapshotsMap.get(snapshotPath);    String snapshotOldName=snapshotNameList.get(generator.nextInt(snapshotNameList.size()));    String snapshotOldNameNoExt=snapshotOldName.substring(0,snapshotOldName.lastIndexOf('.') - 1);    String snapshotNewName=snapshotOldNameNoExt + "_rename.ss";    hdfs.renameSnapshot(snapshotPath,snapshotOldName,snapshotNewName);
  if (snapshottableDirectories.size() > 0) {    int index=generator.nextInt(snapshottableDirectories.size());    Path randomDir=snapshottableDirectories.get(index);    FileStatus[] fileStatusList=hdfs.listStatus(randomDir);    for (    FileStatus fsEntry : fileStatusList) {      if (fsEntry.isFile()) {        Path oldFile=fsEntry.getPath();        Path newFile=oldFile.suffix("_renameFile");        for (        OperationDirectories dir : OperationDirectories.values()) {          if (dir == OperationDirectories.WitnessDir) {            oldFile=new Path(getNewPathString(oldFile.toString(),TESTDIRSTRING,WITNESSDIRSTRING));            newFile=new Path(getNewPathString(newFile.toString(),TESTDIRSTRING,WITNESSDIRSTRING));          }          hdfs.rename(oldFile,newFile,Options.Rename.OVERWRITE);          assertTrue("Target file exists",hdfs.exists(newFile));          assertFalse("Source file does not exist",hdfs.exists(oldFile));
private void createFiles(String rootDir,int fileLength) throws IOException {  if (!rootDir.endsWith("/")) {    rootDir+="/";  }  for (int i=0; i < TOTAL_FILECOUNT; i++) {    String filename=rootDir;    int dirs=generator.nextInt(MAX_NUM_SUB_DIRECTORIES_LEVEL);    for (int j=i; j >= (i - dirs); j--) {      filename+=j + "/";    }    filename+="file" + i;    createFile(filename,fileLength,true);    assertTrue("Test file created",hdfs.exists(new Path(filename)));    LOG.info("createFiles, file: " + filename + "was created");    String witnessFile=filename.replaceAll(TESTDIRSTRING,WITNESSDIRSTRING);    createFile(witnessFile,fileLength,false);    assertTrue("Witness file exists",hdfs.exists(new Path(witnessFile)));
private String getNewPathString(String originalString,String targetString,String replacementString){  String str=originalString.replaceAll(targetString,replacementString);
private String getNewPathString(String originalString,String targetString,String replacementString){  String str=originalString.replaceAll(targetString,replacementString);  LOG.info("Original string: " + originalString);
@Test(timeout=60000) public void testRenameTwiceInSnapshot() throws Exception {  hdfs.mkdirs(sub1);  hdfs.allowSnapshot(sub1);  DFSTestUtil.createFile(hdfs,file1,BLOCKSIZE,REPL,SEED);  hdfs.createSnapshot(sub1,snap1);  hdfs.rename(file1,file2);  hdfs.createSnapshot(sub1,snap2);  hdfs.rename(file2,file3);  SnapshotDiffReport diffReport;  diffReport=hdfs.getSnapshotDiffReport(sub1,snap1,snap2);
  hdfs.mkdirs(sub1);  hdfs.allowSnapshot(sub1);  DFSTestUtil.createFile(hdfs,file1,BLOCKSIZE,REPL,SEED);  hdfs.createSnapshot(sub1,snap1);  hdfs.rename(file1,file2);  hdfs.createSnapshot(sub1,snap2);  hdfs.rename(file2,file3);  SnapshotDiffReport diffReport;  diffReport=hdfs.getSnapshotDiffReport(sub1,snap1,snap2);  LOG.info("DiffList is " + diffReport.toString());  List<DiffReportEntry> entries=diffReport.getDiffList();  assertTrue(entries.size() == 2);  assertTrue(existsInDiffReport(entries,DiffType.MODIFY,"",null));  assertTrue(existsInDiffReport(entries,DiffType.RENAME,file1.getName(),file2.getName()));  diffReport=hdfs.getSnapshotDiffReport(sub1,snap2,"");
  hdfs.rename(file2,file3);  SnapshotDiffReport diffReport;  diffReport=hdfs.getSnapshotDiffReport(sub1,snap1,snap2);  LOG.info("DiffList is " + diffReport.toString());  List<DiffReportEntry> entries=diffReport.getDiffList();  assertTrue(entries.size() == 2);  assertTrue(existsInDiffReport(entries,DiffType.MODIFY,"",null));  assertTrue(existsInDiffReport(entries,DiffType.RENAME,file1.getName(),file2.getName()));  diffReport=hdfs.getSnapshotDiffReport(sub1,snap2,"");  LOG.info("DiffList is " + diffReport.toString());  entries=diffReport.getDiffList();  assertTrue(entries.size() == 2);  assertTrue(existsInDiffReport(entries,DiffType.MODIFY,"",null));  assertTrue(existsInDiffReport(entries,DiffType.RENAME,file2.getName(),file3.getName()));  diffReport=hdfs.getSnapshotDiffReport(sub1,snap1,"");
  hdfs.mkdirs(st);  hdfs.allowSnapshot(st);  Path[] files=new Path[3];  for (int i=0; i < 3; i++) {    files[i]=new Path(st,i + ".txt");  }  Path dest=new Path(st,"dest.txt");  hdfs.createNewFile(dest);  hdfs.createSnapshot(st,"ss");  for (int i=0; i < 3; i++) {    for (int j=0; j < 3; j++) {      FileSystem fs=cluster.getFileSystem();      DFSTestUtil.createFile(fs,files[j],false,1024,1024,512,(short)1,RandomUtils.nextLong(1,512),true);    }    hdfs.concat(dest,files);    hdfs.createSnapshot(st,"s" + i);    SnapshotDiffReport sdr=hdfs.getSnapshotDiffReport(st,"s" + i,"ss");
  Path subsub1=new Path(sub1,"subsub1");  Path subsubsub1=new Path(subsub1,"subsubsub1");  hdfs.mkdirs(subsubsub1);  modifyAndCreateSnapshot(sub1,new Path[]{sub1,subsubsub1});  modifyAndCreateSnapshot(subsubsub1,new Path[]{sub1,subsubsub1});  final String invalidName="invalid";  try {    hdfs.getSnapshotDiffReport(sub1,invalidName,invalidName);    fail("Expect exception when providing invalid snapshot name " + "for diff report");  } catch (  IOException e) {    GenericTestUtils.assertExceptionContains("Cannot find the snapshot of directory " + sub1 + " with name "+ invalidName,e);  }  SnapshotDiffReport report=hdfs.getSnapshotDiffReport(sub1,"s0","s0");  LOG.info(report.toString());  assertEquals(0,report.getDiffList().size());  report=hdfs.getSnapshotDiffReport(sub1,"","");
    fail("Expect exception when providing invalid snapshot name " + "for diff report");  } catch (  IOException e) {    GenericTestUtils.assertExceptionContains("Cannot find the snapshot of directory " + sub1 + " with name "+ invalidName,e);  }  SnapshotDiffReport report=hdfs.getSnapshotDiffReport(sub1,"s0","s0");  LOG.info(report.toString());  assertEquals(0,report.getDiffList().size());  report=hdfs.getSnapshotDiffReport(sub1,"","");  LOG.info(report.toString());  assertEquals(0,report.getDiffList().size());  try {    report=hdfs.getSnapshotDiffReport(subsubsub1,null,"s2");    fail("Expect exception when providing null fromSnapshot ");  } catch (  IllegalArgumentException e) {    GenericTestUtils.assertExceptionContains("null fromSnapshot",e);  }  report=hdfs.getSnapshotDiffReport(subsubsub1,"s0","s2");
    GenericTestUtils.assertExceptionContains("Cannot find the snapshot of directory " + sub1 + " with name "+ invalidName,e);  }  SnapshotDiffReport report=hdfs.getSnapshotDiffReport(sub1,"s0","s0");  LOG.info(report.toString());  assertEquals(0,report.getDiffList().size());  report=hdfs.getSnapshotDiffReport(sub1,"","");  LOG.info(report.toString());  assertEquals(0,report.getDiffList().size());  try {    report=hdfs.getSnapshotDiffReport(subsubsub1,null,"s2");    fail("Expect exception when providing null fromSnapshot ");  } catch (  IllegalArgumentException e) {    GenericTestUtils.assertExceptionContains("null fromSnapshot",e);  }  report=hdfs.getSnapshotDiffReport(subsubsub1,"s0","s2");  LOG.info(report.toString());  assertEquals(0,report.getDiffList().size());
@Test public void testSnapshotDiffInfo() throws Exception {  Path snapshotRootDirPath=dir;  Path snapshotDirDescendantPath=new Path(snapshotRootDirPath,"desc");  Path snapshotDirNonDescendantPath=new Path("/dummy/non/snap/desc");  hdfs.mkdirs(snapshotDirDescendantPath);  hdfs.mkdirs(snapshotDirNonDescendantPath);  hdfs.allowSnapshot(snapshotRootDirPath);  hdfs.createSnapshot(snapshotRootDirPath,"s0");  hdfs.createSnapshot(snapshotRootDirPath,"s1");  INodeDirectory snapshotRootDir=cluster.getNameNode().getNamesystem().getFSDirectory().getINode(snapshotRootDirPath.toUri().getPath()).asDirectory();  INodeDirectory snapshotRootDescendantDir=cluster.getNameNode().getNamesystem().getFSDirectory().getINode(snapshotDirDescendantPath.toUri().getPath()).asDirectory();  INodeDirectory snapshotRootNonDescendantDir=cluster.getNameNode().getNamesystem().getFSDirectory().getINode(snapshotDirNonDescendantPath.toUri().getPath()).asDirectory();  try {    SnapshotDiffInfo sdi=new SnapshotDiffInfo(snapshotRootDir,snapshotRootDescendantDir,new Snapshot(0,"s0",snapshotRootDescendantDir),new Snapshot(0,"s1",snapshotRootDescendantDir));
  hdfs.mkdirs(snapshotDirDescendantPath);  hdfs.mkdirs(snapshotDirNonDescendantPath);  hdfs.allowSnapshot(snapshotRootDirPath);  hdfs.createSnapshot(snapshotRootDirPath,"s0");  hdfs.createSnapshot(snapshotRootDirPath,"s1");  INodeDirectory snapshotRootDir=cluster.getNameNode().getNamesystem().getFSDirectory().getINode(snapshotRootDirPath.toUri().getPath()).asDirectory();  INodeDirectory snapshotRootDescendantDir=cluster.getNameNode().getNamesystem().getFSDirectory().getINode(snapshotDirDescendantPath.toUri().getPath()).asDirectory();  INodeDirectory snapshotRootNonDescendantDir=cluster.getNameNode().getNamesystem().getFSDirectory().getINode(snapshotDirNonDescendantPath.toUri().getPath()).asDirectory();  try {    SnapshotDiffInfo sdi=new SnapshotDiffInfo(snapshotRootDir,snapshotRootDescendantDir,new Snapshot(0,"s0",snapshotRootDescendantDir),new Snapshot(0,"s1",snapshotRootDescendantDir));    LOG.info("SnapshotDiffInfo: " + sdi.getFrom() + " - "+ sdi.getTo());  } catch (  IllegalArgumentException iae) {    fail("Unexpected exception when constructing SnapshotDiffInfo: " + iae);  }  try {    SnapshotDiffInfo sdi=new SnapshotDiffInfo(snapshotRootDir,snapshotRootNonDescendantDir,new Snapshot(0,"s0",snapshotRootNonDescendantDir),new Snapshot(0,"s1",snapshotRootNonDescendantDir));
private void printAtime(Path path,Path ssRoot,String ssName) throws IOException {  Path ssPath=getSSpath(path,ssRoot,ssName);
private void verifyDiffReportForGivenReport(Path dirPath,String from,String to,SnapshotDiffReport report,DiffReportEntry... entries) throws IOException {  SnapshotDiffReport inverseReport=hdfs.getSnapshotDiffReport(dirPath,to,from);
private void verifyDiffReportForGivenReport(Path dirPath,String from,String to,SnapshotDiffReport report,DiffReportEntry... entries) throws IOException {  SnapshotDiffReport inverseReport=hdfs.getSnapshotDiffReport(dirPath,to,from);  LOG.info(report.toString());
private void waitExpectedStorageType(MiniDFSCluster cluster,final String fileName,long fileLen,final StorageType expectedStorageType,int expectedStorageCount,int expectedBlkLocationCount,int timeout) throws Exception {  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      int actualStorageCount=0;      try {        LocatedBlocks locatedBlocks=cluster.getFileSystem().getClient().getLocatedBlocks(fileName,0,fileLen);        for (        LocatedBlock lb : locatedBlocks.getLocatedBlocks()) {
    @Override public Boolean get(){      int actualStorageCount=0;      try {        LocatedBlocks locatedBlocks=cluster.getFileSystem().getClient().getLocatedBlocks(fileName,0,fileLen);        for (        LocatedBlock lb : locatedBlocks.getLocatedBlocks()) {          LOG.info("LocatedBlocks => Size {}, locs {}",lb.getLocations().length,lb);          if (lb.getLocations().length > expectedBlkLocationCount) {            return false;          }          for (          StorageType storageType : lb.getStorageTypes()) {            if (expectedStorageType == storageType) {              actualStorageCount++;            } else {              LOG.info("Expected storage type {} and actual {}",expectedStorageType,storageType);            }          }        }        LOG.info(expectedStorageType + " replica count, expected={} and actual={}",expectedStorageCount,actualStorageCount);      } catch (      IOException e) {
private void waitForAttemptedItems(long expectedBlkMovAttemptedCount,int timeout) throws TimeoutException, InterruptedException {  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){
@Test public void testDataLocality() throws Exception {  final Configuration conf=WebHdfsTestUtil.createConf();  final String[] racks={RACK0,RACK0,RACK1,RACK1,RACK2,RACK2};  final int nDataNodes=racks.length;
@Test public void testDataLocality() throws Exception {  final Configuration conf=WebHdfsTestUtil.createConf();  final String[] racks={RACK0,RACK0,RACK1,RACK1,RACK2,RACK2};  final int nDataNodes=racks.length;  LOG.info("nDataNodes=" + nDataNodes + ", racks="+ Arrays.asList(racks));  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(nDataNodes).racks(racks).build();  try {    cluster.waitActive();    final DistributedFileSystem dfs=cluster.getFileSystem();    final NameNode namenode=cluster.getNameNode();    final DatanodeManager dm=namenode.getNamesystem().getBlockManager().getDatanodeManager();
@Test public void testExcludeDataNodes() throws Exception {  final Configuration conf=WebHdfsTestUtil.createConf();  final String[] racks={RACK0,RACK0,RACK1,RACK1,RACK2,RACK2};  final String[] hosts={"DataNode1","DataNode2","DataNode3","DataNode4","DataNode5","DataNode6"};  final int nDataNodes=hosts.length;
@Test public void testExcludeDataNodes() throws Exception {  final Configuration conf=WebHdfsTestUtil.createConf();  final String[] racks={RACK0,RACK0,RACK1,RACK1,RACK2,RACK2};  final String[] hosts={"DataNode1","DataNode2","DataNode3","DataNode4","DataNode5","DataNode6"};  final int nDataNodes=hosts.length;  LOG.info("nDataNodes=" + nDataNodes + ", racks="+ Arrays.asList(racks)+ ", hosts="+ Arrays.asList(hosts));  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).hosts(hosts).numDataNodes(nDataNodes).racks(racks).build();  try {    cluster.waitActive();    final DistributedFileSystem dfs=cluster.getFileSystem();    final NameNode namenode=cluster.getNameNode();    final DatanodeManager dm=namenode.getNamesystem().getBlockManager().getDatanodeManager();
  final String file1="/testMoveWithBlockPinning";  InetSocketAddress[] favoredNodes=new InetSocketAddress[favoredNodesCount];  for (int i=0; i < favoredNodesCount; i++) {    favoredNodes[i]=dns.get(i).getXferAddress();  }  DFSTestUtil.createFile(dfs,new Path(file1),false,1024,100,DEFAULT_BLOCK_SIZE,(short)3,0,false,favoredNodes);  LocatedBlocks locatedBlocks=dfs.getClient().getLocatedBlocks(file1,0);  Assert.assertEquals("Wrong block count",1,locatedBlocks.locatedBlockCount());  LocatedBlock lb=locatedBlocks.get(0);  StorageType[] storageTypes=lb.getStorageTypes();  for (  StorageType storageType : storageTypes) {    Assert.assertTrue(StorageType.DISK == storageType);  }  DatanodeInfo[] locations=lb.getLocations();  Assert.assertEquals(3,locations.length);  Assert.assertTrue(favoredNodesCount < locations.length);  for (  DatanodeInfo dnInfo : locations) {
public void waitForAttemptedItems(long expectedBlkMovAttemptedCount,int timeout) throws TimeoutException, InterruptedException {  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){
public void waitForBlocksMovementAttemptReport(long expectedMovementFinishedBlocksCount,int timeout) throws TimeoutException, InterruptedException {  GenericTestUtils.waitFor(new Supplier<Boolean>(){    @Override public Boolean get(){      int actualCount=externalSps.getAttemptedItemsMonitor().getAttemptedItemsCount();
    resetStream();    assertEquals(0,ToolRunner.run(dfsAdmin,new String[]{"-report"}));    verifyNodesAndCorruptBlocks(numDn,numDn,0,0,client,0L,0L);    final short replFactor=1;    final long fileLength=512L;    final DistributedFileSystem fs=miniCluster.getFileSystem();    final Path file=new Path(baseDir,"/corrupted");    fs.enableErasureCodingPolicy(ecPolicy.getName());    DFSTestUtil.createFile(fs,file,fileLength,replFactor,12345L);    DFSTestUtil.waitReplication(fs,file,replFactor);    final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,file);    LocatedBlocks lbs=miniCluster.getFileSystem().getClient().getNamenode().getBlockLocations(file.toString(),0,fileLength);    assertTrue("Unexpected block type: " + lbs.get(0),lbs.get(0) instanceof LocatedBlock);    LocatedBlock locatedBlock=lbs.get(0);    DatanodeInfo locatedDataNode=locatedBlock.getLocations()[0];
    int cellSize=ecPolicy.getCellSize();    int blockSize=stripesPerBlock * cellSize;    int blockGroupSize=ecPolicy.getNumDataUnits() * blockSize;    int totalBlockGroups=1;    DFSTestUtil.createStripedFile(miniCluster,ecFile,ecDir,totalBlockGroups,stripesPerBlock,false,ecPolicy);    resetStream();    assertEquals(0,ToolRunner.run(dfsAdmin,new String[]{"-report"}));    verifyNodesAndCorruptBlocks(numDn,numDn,0,0,client,0L,0L);    final List<DataNode> datanodes=miniCluster.getDataNodes();    DataNode dataNodeToShutdown=null;    for (    DataNode dn : datanodes) {      if (!dn.getDatanodeId().getDatanodeUuid().equals(locatedDataNode.getDatanodeUuid())) {        dataNodeToShutdown=dn;        break;      }    }    assertTrue("Unable to choose a DataNode to shutdown!",dataNodeToShutdown != null);
private void verifyOpenFilesListing(HashSet<Path> closedFileSet,HashMap<Path,FSDataOutputStream> openFilesMap){  final String outStr=scanIntoString(out);
private Object runTool(String... args) throws Exception {  errOutBytes.reset();  outBytes.reset();  LOG.info("Running: DFSHAAdmin " + Joiner.on(" ").join(args));  int ret=tool.run(args);  errOutput=new String(errOutBytes.toByteArray(),Charsets.UTF_8);  output=new String(outBytes.toByteArray(),Charsets.UTF_8);
private int runTool(String... args) throws Exception {  errOutBytes.reset();  LOG.info("Running: DFSHAAdmin " + Joiner.on(" ").join(args));  int ret=tool.run(args);  errOutput=new String(errOutBytes.toByteArray(),Charsets.UTF_8);
  try {    cluster.waitActive();    DistributedFileSystem fs=cluster.getFileSystem();    LocalFileSystem localFileSystem=FileSystem.getLocal(conf);    Path p=new Path(f.getRoot().getAbsolutePath(),tokenFile);    p=localFileSystem.makeQualified(p);    DelegationTokenFetcher.saveDelegationToken(conf,fs,null,p);    Credentials creds=Credentials.readTokenStorageFile(p,conf);    Iterator<Token<?>> itr=creds.getAllTokens().iterator();    assertTrue("token not exist error",itr.hasNext());    final Token token=itr.next();    assertNotNull("Token should be there without renewer",token);    String expectedNonVerbose="Token (HDFS_DELEGATION_TOKEN token 1 for " + System.getProperty("user.name") + " with renewer ) for";    String resNonVerbose=DelegationTokenFetcher.printTokensToString(conf,p,false);    assertTrue("The non verbose output is expected to start with \"" + expectedNonVerbose + "\"",resNonVerbose.startsWith(expectedNonVerbose));
    cluster.waitActive();    DistributedFileSystem fs=cluster.getFileSystem();    LocalFileSystem localFileSystem=FileSystem.getLocal(conf);    Path p=new Path(f.getRoot().getAbsolutePath(),tokenFile);    p=localFileSystem.makeQualified(p);    DelegationTokenFetcher.saveDelegationToken(conf,fs,null,p);    Credentials creds=Credentials.readTokenStorageFile(p,conf);    Iterator<Token<?>> itr=creds.getAllTokens().iterator();    assertTrue("token not exist error",itr.hasNext());    final Token token=itr.next();    assertNotNull("Token should be there without renewer",token);    String expectedNonVerbose="Token (HDFS_DELEGATION_TOKEN token 1 for " + System.getProperty("user.name") + " with renewer ) for";    String resNonVerbose=DelegationTokenFetcher.printTokensToString(conf,p,false);    assertTrue("The non verbose output is expected to start with \"" + expectedNonVerbose + "\"",resNonVerbose.startsWith(expectedNonVerbose));    LOG.info(resNonVerbose);
@Test public void testGenerated() throws IOException {  String edits=nnHelper.generateEdits();
@Test public void testGenerated() throws IOException {  String edits=nnHelper.generateEdits();  LOG.info("Generated edits=" + edits);  String editsParsedXml=folder.newFile("editsParsed.xml").getAbsolutePath();  String editsReparsed=folder.newFile("editsParsed").getAbsolutePath();  String editsParsedXML_caseInSensitive=folder.newFile("editsRecoveredParsed.XML").getAbsolutePath();  assertEquals(0,runOev(edits,editsParsedXml,"xml",false));  assertEquals(0,runOev(edits,editsParsedXML_caseInSensitive,"xml",false));  assertEquals(0,runOev(editsParsedXml,editsReparsed,"binary",false));  assertEquals(0,runOev(editsParsedXML_caseInSensitive,editsReparsed,"binary",false));  assertTrue("Edits " + edits + " should have all op codes",hasAllOpCodes(edits));
private int runOev(String inFilename,String outFilename,String processor,boolean recovery) throws IOException {
private boolean hasAllOpCodes(String inFilename) throws IOException {  String outFilename=inFilename + ".stats";  FileOutputStream fout=new FileOutputStream(outFilename);  StatisticsEditsVisitor visitor=new StatisticsEditsVisitor(fout);  OfflineEditsViewer oev=new OfflineEditsViewer();  if (oev.go(inFilename,outFilename,"stats",new Flags(),visitor) != 0)   return false;
private boolean hasAllOpCodes(String inFilename) throws IOException {  String outFilename=inFilename + ".stats";  FileOutputStream fout=new FileOutputStream(outFilename);  StatisticsEditsVisitor visitor=new StatisticsEditsVisitor(fout);  OfflineEditsViewer oev=new OfflineEditsViewer();  if (oev.go(inFilename,outFilename,"stats",new Flags(),visitor) != 0)   return false;  LOG.info("Statistics for " + inFilename + "\n"+ visitor.getStatisticsString());  boolean hasAllOpCodes=true;  for (  FSEditLogOpCodes opCode : FSEditLogOpCodes.values()) {    if (skippedOps.contains(opCode))     continue;    Long count=visitor.getStatistics().get(opCode);    if ((count == null) || (count == 0)) {      hasAllOpCodes=false;
@Test public void testProcessorWithSameTypeFormatFile() throws IOException {  String edits=nnHelper.generateEdits();
    hdfs.create(emptyECFile).close();    writtenFiles.put(emptyECFile.toString(),pathToFileEntry(hdfs,emptyECFile.toString()));    filesECCount++;    Path smallECFile=new Path(ecDir,"SmallECFile.txt");    FSDataOutputStream out=hdfs.create(smallECFile);    Random r=new Random();    byte[] bytes=new byte[1024 * 10];    r.nextBytes(bytes);    out.write(bytes);    writtenFiles.put(smallECFile.toString(),pathToFileEntry(hdfs,smallECFile.toString()));    filesECCount++;    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER,false);    hdfs.saveNamespace();    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE,false);    originalFsimage=FSImageTestUtil.findLatestImageFile(FSImageTestUtil.getFSImage(cluster.getNameNode()).getStorage().getStorageDir(0));
@Test public void testReverseXmlRoundTrip() throws Throwable {  GenericTestUtils.setLogLevel(OfflineImageReconstructor.LOG,Level.TRACE);  File reverseImageXml=new File(tempDir,"reverseImage.xml");  File reverseImage=new File(tempDir,"reverseImage");  File reverseImage2Xml=new File(tempDir,"reverseImage2.xml");
    }     try (FSDataOutputStream o=hdfs.create(new Path(parentDir,"file4"))){      o.write("123".getBytes());    }     Path link1=new Path("/link1");    Path link2=new Path("/dirForLinks/linkfordir1");    hdfs.createSymlink(new Path("/parentDir/file4"),link1,true);    summaryFromDFS=hdfs.getContentSummary(parentDir);    emptyDirSummaryFromDFS=hdfs.getContentSummary(childDir2);    fileSummaryFromDFS=hdfs.getContentSummary(file1OnParentDir);    symLinkSummaryFromDFS=hdfs.getContentSummary(link1);    hdfs.createSymlink(childDir1,link2,true);    symLinkSummaryForDirContainsFromDFS=hdfs.getContentSummary(new Path("/dirForLinks"));    hdfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER,false);    hdfs.saveNamespace();    originalFsimage=FSImageTestUtil.findLatestImageFile(FSImageTestUtil.getFSImage(cluster.getNameNode()).getStorage().getStorageDir(0));    if (originalFsimage == null) {
    hdfs.mkdirs(dir);    hdfs.setStoragePolicy(dir,HdfsConstants.ALLSSD_STORAGE_POLICY_NAME);    dir=new Path("/dir_w_sp_allssd/sub_dir_wo_sp");    hdfs.mkdirs(dir);    file=new Path("/dir_w_sp_allssd/file_wo_sp");    try (FSDataOutputStream o=hdfs.create(file)){      o.write(123);      o.close();    }     dir=new Path("/dir_w_sp_allssd/sub_dir_w_sp_hot");    hdfs.mkdirs(dir);    hdfs.setStoragePolicy(dir,HdfsConstants.HOT_STORAGE_POLICY_NAME);    hdfs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER,false);    hdfs.saveNamespace();    originalFsimage=FSImageTestUtil.findLatestImageFile(FSImageTestUtil.getFSImage(cluster.getNameNode()).getStorage().getStorageDir(0));    if (originalFsimage == null) {
static void verifySeek(FileSystem fs,Path p,long offset,long length,byte[] buf,byte[] expected) throws IOException {  long remaining=length - offset;  long checked=0;
static void verifyPread(FileSystem fs,Path p,long offset,long length,byte[] buf,byte[] expected) throws IOException {  long remaining=length - offset;  long checked=0;
private void checkResponseContainsLocation(URL url,String TYPE) throws JSONException, IOException {  HttpURLConnection conn=(HttpURLConnection)url.openConnection();  conn.setRequestMethod(TYPE);  conn.setInstanceFollowRedirects(false);  String response=IOUtils.toString(conn.getInputStream());
@Test public void testWebHdfsNoRedirect() throws Exception {  final Configuration conf=WebHdfsTestUtil.createConf();  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();  LOG.info("Started cluster");  InetSocketAddress addr=cluster.getNameNode().getHttpAddress();  URL url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirectCreate" + "?op=CREATE"+ Param.toSortedString("&",new NoRedirectParam(true)));
@Test public void testWebHdfsNoRedirect() throws Exception {  final Configuration conf=WebHdfsTestUtil.createConf();  cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();  LOG.info("Started cluster");  InetSocketAddress addr=cluster.getNameNode().getHttpAddress();  URL url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirectCreate" + "?op=CREATE"+ Param.toSortedString("&",new NoRedirectParam(true)));  LOG.info("Sending create request " + url);  checkResponseContainsLocation(url,"PUT");  final WebHdfsFileSystem fs=WebHdfsTestUtil.getWebHdfsFileSystem(conf,WebHdfsConstants.WEBHDFS_SCHEME);  final String PATH="/testWebHdfsNoRedirect";  byte[] CONTENTS=new byte[1024];  RANDOM.nextBytes(CONTENTS);  try (OutputStream os=fs.create(new Path(PATH))){    os.write(CONTENTS);  }   url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=OPEN"+ Param.toSortedString("&",new NoRedirectParam(true)));
  LOG.info("Started cluster");  InetSocketAddress addr=cluster.getNameNode().getHttpAddress();  URL url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirectCreate" + "?op=CREATE"+ Param.toSortedString("&",new NoRedirectParam(true)));  LOG.info("Sending create request " + url);  checkResponseContainsLocation(url,"PUT");  final WebHdfsFileSystem fs=WebHdfsTestUtil.getWebHdfsFileSystem(conf,WebHdfsConstants.WEBHDFS_SCHEME);  final String PATH="/testWebHdfsNoRedirect";  byte[] CONTENTS=new byte[1024];  RANDOM.nextBytes(CONTENTS);  try (OutputStream os=fs.create(new Path(PATH))){    os.write(CONTENTS);  }   url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=OPEN"+ Param.toSortedString("&",new NoRedirectParam(true)));  LOG.info("Sending open request " + url);  checkResponseContainsLocation(url,"GET");  url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=GETFILECHECKSUM"+ Param.toSortedString("&",new NoRedirectParam(true)));
  LOG.info("Sending create request " + url);  checkResponseContainsLocation(url,"PUT");  final WebHdfsFileSystem fs=WebHdfsTestUtil.getWebHdfsFileSystem(conf,WebHdfsConstants.WEBHDFS_SCHEME);  final String PATH="/testWebHdfsNoRedirect";  byte[] CONTENTS=new byte[1024];  RANDOM.nextBytes(CONTENTS);  try (OutputStream os=fs.create(new Path(PATH))){    os.write(CONTENTS);  }   url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=OPEN"+ Param.toSortedString("&",new NoRedirectParam(true)));  LOG.info("Sending open request " + url);  checkResponseContainsLocation(url,"GET");  url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=GETFILECHECKSUM"+ Param.toSortedString("&",new NoRedirectParam(true)));  LOG.info("Sending getfilechecksum request " + url);  checkResponseContainsLocation(url,"GET");  url=new URL("http",addr.getHostString(),addr.getPort(),WebHdfsFileSystem.PATH_PREFIX + "/testWebHdfsNoRedirect" + "?op=APPEND"+ Param.toSortedString("&",new NoRedirectParam(true)));
      try {        clientSocket=serverSocket.accept();        fs.connectionFactory=connectionFactory;        if (consumeConnectionBacklog) {          consumeConnectionBacklog();        }        in=clientSocket.getInputStream();        isr=new InputStreamReader(in);        br=new BufferedReader(isr);        for (; ; ) {          String line=br.readLine();          if (line == null || line.isEmpty()) {            break;          }        }        out=clientSocket.getOutputStream();        out.write(temporaryRedirect().getBytes("UTF-8"));      } catch (      IOException e) {
private static void setupCluster(final int nNameNodes,final int nDataNodes) throws Exception {
private static String[] createStrings(String prefix,String name){  final String[] strings=new String[webhdfs.length];  for (int i=0; i < webhdfs.length; i++) {    strings[i]=createString(prefix,i);
@Test public void testPermissionParam(){  final PermissionParam p=new PermissionParam(PermissionParam.DEFAULT);  Assert.assertEquals(new FsPermission((short)0755),p.getDirFsPermission());  Assert.assertEquals(new FsPermission((short)0644),p.getFileFsPermission());  new PermissionParam("0");  try {    new PermissionParam("-1");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  new PermissionParam("1777");  try {    new PermissionParam("2000");    Assert.fail();  } catch (  IllegalArgumentException e) {
  try {    new PermissionParam("-1");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  new PermissionParam("1777");  try {    new PermissionParam("2000");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new PermissionParam("8");    Assert.fail();  } catch (  IllegalArgumentException e) {
    LOG.info("EXPECTED: " + e);  }  new PermissionParam("1777");  try {    new PermissionParam("2000");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new PermissionParam("8");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new PermissionParam("abc");    Assert.fail();
@Test public void testAclPermissionParam(){  final AclPermissionParam p=new AclPermissionParam("user::rwx,group::r--,other::rwx,user:user1:rwx");  List<AclEntry> setAclList=AclEntry.parseAclSpec("user::rwx,group::r--,other::rwx,user:user1:rwx",true);  Assert.assertEquals(setAclList.toString(),p.getAclPermission(true).toString());  new AclPermissionParam("user::rw-,group::rwx,other::rw-,user:user1:rwx");  try {    new AclPermissionParam("user::rw--,group::rwx-,other::rw-");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  new AclPermissionParam("user::rw-,group::rwx,other::rw-,user:user1:rwx,group:group1:rwx,other::rwx,mask::rwx,default:user:user1:rwx");  try {    new AclPermissionParam("user:r-,group:rwx,other:rw-");    Assert.fail();  } catch (  IllegalArgumentException e) {
  try {    new AclPermissionParam("user::rw--,group::rwx-,other::rw-");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  new AclPermissionParam("user::rw-,group::rwx,other::rw-,user:user1:rwx,group:group1:rwx,other::rwx,mask::rwx,default:user:user1:rwx");  try {    new AclPermissionParam("user:r-,group:rwx,other:rw-");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new AclPermissionParam("default:::r-,default:group::rwx,other::rw-");    Assert.fail();  } catch (  IllegalArgumentException e) {
    LOG.info("EXPECTED: " + e);  }  new AclPermissionParam("user::rw-,group::rwx,other::rw-,user:user1:rwx,group:group1:rwx,other::rwx,mask::rwx,default:user:user1:rwx");  try {    new AclPermissionParam("user:r-,group:rwx,other:rw-");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new AclPermissionParam("default:::r-,default:group::rwx,other::rw-");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new AclPermissionParam("user:r-,group::rwx,other:rw-,mask:rw-,temp::rwx");    Assert.fail();
  new FsActionParam("r-x");  new FsActionParam("-wx");  new FsActionParam("r--");  new FsActionParam("-w-");  new FsActionParam("--x");  new FsActionParam("---");  try {    new FsActionParam("rw");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("qwx");    Assert.fail();  } catch (  IllegalArgumentException e) {
  new FsActionParam("---");  try {    new FsActionParam("rw");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("qwx");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("qrwx");    Assert.fail();  } catch (  IllegalArgumentException e) {
 catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("qwx");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("qrwx");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("rwxx");    Assert.fail();
    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("qrwx");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("rwxx");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("xwr");
  try {    new FsActionParam("qrwx");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("rwxx");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);  }  try {    new FsActionParam("xwr");    Assert.fail();  } catch (  IllegalArgumentException e) {    LOG.info("EXPECTED: " + e);
    String hosts[]={"foo1.example.com","foo2.example.com"};    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).racks(racks).hosts(hosts).build();    cluster.waitActive();    NamenodeProtocols nn=cluster.getNameNodeRpc();    Assert.assertNotNull(nn);    DatanodeInfo[] info;    while (true) {      info=nn.getDatanodeReport(DatanodeReportType.LIVE);      Assert.assertFalse(info.length == 2);      if (info.length == 1) {        break;      }      Thread.sleep(1000);    }    int validIdx=info[0].getHostName().equals(hosts[0]) ? 0 : 1;    int invalidIdx=validIdx == 1 ? 0 : 1;    StaticMapping.addNodeToRack(hosts[invalidIdx],racks[validIdx]);
private void verifyResults(int upperbound,Set<Node> excludedNodes,Map<Node,Integer> frequency){
private void verifyResults(int upperbound,Set<Node> excludedNodes,Map<Node,Integer> frequency){  LOG.info("Excluded nodes are: {}",excludedNodes);  for (int i=0; i < upperbound; ++i) {    final Node n=dataNodes[i];
static FsPermission checkPermission(FileSystem fs,String path,FsPermission expected) throws IOException {  FileStatus s=fs.getFileStatus(new Path(path));
    out.write(123);    out.close();    checkPermission(fs,"/b1",inheritPerm);    checkPermission(fs,"/b1/b2",inheritPerm);    checkPermission(fs,"/b1/b2/b3.txt",filePerm);    conf.set(FsPermission.UMASK_LABEL,"022");    permission=FsPermission.createImmutable((short)0666);    FileSystem.mkdirs(fs,new Path("/c1"),new FsPermission(permission));    FileSystem.create(fs,new Path("/c1/c2.txt"),new FsPermission(permission));    checkPermission(fs,"/c1",permission);    checkPermission(fs,"/c1/c2.txt",permission);  }  finally {    try {      if (fs != null)       fs.close();    } catch (    Exception e) {
    checkPermission(fs,"/b1/b2/b3.txt",filePerm);    conf.set(FsPermission.UMASK_LABEL,"022");    permission=FsPermission.createImmutable((short)0666);    FileSystem.mkdirs(fs,new Path("/c1"),new FsPermission(permission));    FileSystem.create(fs,new Path("/c1/c2.txt"),new FsPermission(permission));    checkPermission(fs,"/c1",permission);    checkPermission(fs,"/c1/c2.txt",permission);  }  finally {    try {      if (fs != null)       fs.close();    } catch (    Exception e) {      LOG.error(StringUtils.stringifyException(e));    }    try {      if (cluster != null)       cluster.shutdown();    } catch (    Exception e) {
@Test public void testGroupMappingRefresh() throws Exception {  DFSAdmin admin=new DFSAdmin(config);  String[] args=new String[]{"-refreshUserToGroupsMappings"};  Groups groups=Groups.getUserToGroupsMappingService(config);  String user=UserGroupInformation.getCurrentUser().getUserName();  LOG.debug("First attempt:");  List<String> g1=groups.getGroups(user);
@Test public void testGroupMappingRefresh() throws Exception {  DFSAdmin admin=new DFSAdmin(config);  String[] args=new String[]{"-refreshUserToGroupsMappings"};  Groups groups=Groups.getUserToGroupsMappingService(config);  String user=UserGroupInformation.getCurrentUser().getUserName();  LOG.debug("First attempt:");  List<String> g1=groups.getGroups(user);  LOG.debug(g1.toString());  LOG.debug("Second attempt, should be the same:");  List<String> g2=groups.getGroups(user);
  DFSAdmin admin=new DFSAdmin(config);  String[] args=new String[]{"-refreshUserToGroupsMappings"};  Groups groups=Groups.getUserToGroupsMappingService(config);  String user=UserGroupInformation.getCurrentUser().getUserName();  LOG.debug("First attempt:");  List<String> g1=groups.getGroups(user);  LOG.debug(g1.toString());  LOG.debug("Second attempt, should be the same:");  List<String> g2=groups.getGroups(user);  LOG.debug(g2.toString());  for (int i=0; i < g2.size(); i++) {    assertEquals("Should be same group ",g1.get(i),g2.get(i));  }  admin.run(args);  LOG.debug("Third attempt(after refresh command), should be different:");  List<String> g3=groups.getGroups(user);
public void start() throws IOException, FileNotFoundException {  dfs=new MiniDFSCluster.Builder(conf).nameNodePort(nameNodePort).nameNodeHttpPort(nameNodeHttpPort).numDataNodes(numDataNodes).startupOption(dfsOpts).format(format).build();  dfs.waitActive();
private boolean parseArguments(String[] args){  Options options=makeOptions();  CommandLine cli;  try {    CommandLineParser parser=new GnuParser();    cli=parser.parse(options,args);  } catch (  ParseException e) {    LOG.warn("options parsing failed",e);    new HelpFormatter().printHelp("...",options);    return false;  }  if (cli.hasOption("help")) {    new HelpFormatter().printHelp("...",options);    return false;  }  if (cli.getArgs().length > 0) {    for (    String arg : cli.getArgs()) {
public void serviceStart() throws Exception {  taskRunner=HadoopExecutors.newSingleThreadExecutor(new ThreadFactoryBuilder().setDaemon(true).setNameFormat("uber-SubtaskRunner").build());  eventHandler=new Thread(new EventHandler(),"uber-EventHandler");  if (jobClassLoader != null) {
@VisibleForTesting protected static MapOutputFile renameMapOutputForReduce(JobConf conf,TaskAttemptId mapId,MapOutputFile subMapOutputFile) throws IOException {  FileSystem localFs=FileSystem.getLocal(conf);  Path mapOut=subMapOutputFile.getOutputFile();  FileStatus mStatus=localFs.getFileStatus(mapOut);  Path reduceIn=subMapOutputFile.getInputFileForWrite(TypeConverter.fromYarn(mapId).getTaskID(),mStatus.getLen());  Path mapOutIndex=subMapOutputFile.getOutputIndexFile();  Path reduceInIndex=new Path(reduceIn.toString() + ".index");  if (LOG.isDebugEnabled()) {
@Override public boolean canCommit(TaskAttemptID taskAttemptID) throws IOException {
@Override public void commitPending(TaskAttemptID taskAttemptID,TaskStatus taskStatsu) throws IOException, InterruptedException {
@Override public void preempted(TaskAttemptID taskAttemptID,TaskStatus taskStatus) throws IOException, InterruptedException {
@Override public void done(TaskAttemptID taskAttemptID) throws IOException {
@Override public void fatalError(TaskAttemptID taskAttemptID,String msg,boolean fastFail) throws IOException {
@Override public void fsError(TaskAttemptID taskAttemptID,String message) throws IOException {
@Override public MapTaskCompletionEventsUpdate getMapCompletionEvents(JobID jobIdentifier,int startIndex,int maxEvents,TaskAttemptID taskAttemptID) throws IOException {
@Override public void reportDiagnosticInfo(TaskAttemptID taskAttemptID,String diagnosticInfo) throws IOException {  diagnosticInfo=StringInterner.weakIntern(diagnosticInfo);
@Override public AMFeedback statusUpdate(TaskAttemptID taskAttemptID,TaskStatus taskStatus) throws IOException, InterruptedException {  org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId yarnAttemptID=TypeConverter.toYarn(taskAttemptID);  AMFeedback feedback=new AMFeedback();  feedback.setTaskFound(true);  AtomicReference<TaskAttemptStatus> lastStatusRef=attemptIdToStatus.get(yarnAttemptID);  if (lastStatusRef == null) {    if (!taskHeartbeatHandler.hasRecentlyUnregistered(yarnAttemptID)) {
@Override public AMFeedback statusUpdate(TaskAttemptID taskAttemptID,TaskStatus taskStatus) throws IOException, InterruptedException {  org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId yarnAttemptID=TypeConverter.toYarn(taskAttemptID);  AMFeedback feedback=new AMFeedback();  feedback.setTaskFound(true);  AtomicReference<TaskAttemptStatus> lastStatusRef=attemptIdToStatus.get(yarnAttemptID);  if (lastStatusRef == null) {    if (!taskHeartbeatHandler.hasRecentlyUnregistered(yarnAttemptID)) {      LOG.error("Status update was called with illegal TaskAttemptId: " + yarnAttemptID);      feedback.setTaskFound(false);    }    return feedback;  }  if (getConfig().getBoolean(MRJobConfig.TASK_PREEMPTION,false) && preemptionPolicy.isPreempted(yarnAttemptID)) {    feedback.setPreemption(true);    LOG.info("Setting preemption bit for task: " + yarnAttemptID + " of type "+ yarnAttemptID.getTaskId().getTaskType());  }  if (taskStatus == null) {    if (LOG.isDebugEnabled()) {
@Override public JvmTask getTask(JvmContext context) throws IOException {  JVMId jvmId=context.jvmId;
@Override public JvmTask getTask(JvmContext context) throws IOException {  JVMId jvmId=context.jvmId;  LOG.info("JVM with ID : " + jvmId + " asked for a task");  JvmTask jvmTask=null;  WrappedJvmID wJvmID=new WrappedJvmID(jvmId.getJobId(),jvmId.isMap,jvmId.getId());  if (!jvmIDToActiveAttemptMap.containsKey(wJvmID)) {
@Override public JvmTask getTask(JvmContext context) throws IOException {  JVMId jvmId=context.jvmId;  LOG.info("JVM with ID : " + jvmId + " asked for a task");  JvmTask jvmTask=null;  WrappedJvmID wJvmID=new WrappedJvmID(jvmId.getJobId(),jvmId.isMap,jvmId.getId());  if (!jvmIDToActiveAttemptMap.containsKey(wJvmID)) {    LOG.info("JVM with ID: " + jvmId + " is invalid and will be killed.");    jvmTask=TASK_FOR_INVALID_JVM;  } else {    if (!launchedJVMs.contains(wJvmID)) {      jvmTask=null;      LOG.info("JVM with ID: " + jvmId + " asking for task before AM launch registered. Given null task");    } else {      org.apache.hadoop.mapred.Task task=jvmIDToActiveAttemptMap.remove(wJvmID);      launchedJVMs.remove(wJvmID);
private void coalesceStatusUpdate(TaskAttemptId yarnAttemptID,TaskAttemptStatus taskAttemptStatus,AtomicReference<TaskAttemptStatus> lastStatusRef){  List<TaskAttemptId> fetchFailedMaps=taskAttemptStatus.fetchFailedMaps;  TaskAttemptStatus lastStatus=null;  boolean done=false;  while (!done) {    lastStatus=lastStatusRef.get();    if (lastStatus != null && lastStatus.fetchFailedMaps != null) {      if (taskAttemptStatus.fetchFailedMaps == null) {        taskAttemptStatus.fetchFailedMaps=lastStatus.fetchFailedMaps;      } else {        taskAttemptStatus.fetchFailedMaps=new ArrayList<>(lastStatus.fetchFailedMaps.size() + fetchFailedMaps.size());        taskAttemptStatus.fetchFailedMaps.addAll(lastStatus.fetchFailedMaps);        taskAttemptStatus.fetchFailedMaps.addAll(fetchFailedMaps);      }    }    done=lastStatusRef.compareAndSet(lastStatus,taskAttemptStatus);    if (!done) {
  Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());  LOG.debug("Child starting");  final JobConf job=new JobConf(MRJobConfig.JOB_CONF_FILE);  Limits.init(job);  UserGroupInformation.setConfiguration(job);  SecurityUtil.setConfiguration(job);  String host=args[0];  int port=Integer.parseInt(args[1]);  final InetSocketAddress address=NetUtils.createSocketAddrForHost(host,port);  final TaskAttemptID firstTaskid=TaskAttemptID.forName(args[2]);  long jvmIdLong=Long.parseLong(args[3]);  JVMId jvmId=new JVMId(firstTaskid.getJobID(),firstTaskid.getTaskType() == TaskType.MAP,jvmIdLong);  CallerContext.setCurrent(new CallerContext.Builder("mr_" + firstTaskid.toString()).build());  DefaultMetricsSystem.initialize(StringUtils.camelize(firstTaskid.getTaskType().name()) + "Task");  Credentials credentials=UserGroupInformation.getCurrentUser().getCredentials();
  taskOwner.addToken(jt);  final TaskUmbilicalProtocol umbilical=taskOwner.doAs(new PrivilegedExceptionAction<TaskUmbilicalProtocol>(){    @Override public TaskUmbilicalProtocol run() throws Exception {      return (TaskUmbilicalProtocol)RPC.getProxy(TaskUmbilicalProtocol.class,TaskUmbilicalProtocol.versionID,address,job);    }  });  JvmContext context=new JvmContext(jvmId,"-1000");  LOG.debug("PID: " + System.getenv().get("JVM_PID"));  Task task=null;  UserGroupInformation childUGI=null;  ScheduledExecutorService logSyncer=null;  try {    int idleLoopCount=0;    JvmTask myTask=null;    for (int idle=0; null == myTask; ++idle) {      long sleepTimeMilliSecs=Math.min(idle * 500,1500);
        return null;      }    });  } catch (  FSError e) {    LOG.error("FSError from child",e);    if (!ShutdownHookManager.get().isShutdownInProgress()) {      umbilical.fsError(taskid,e.getMessage());    }  }catch (  Exception exception) {    LOG.warn("Exception running child : " + StringUtils.stringifyException(exception));    try {      if (task != null) {        if (childUGI == null) {          task.taskCleanup(umbilical);        } else {          final Task taskFinal=task;          childUGI.doAs(new PrivilegedExceptionAction<Object>(){
private static void configureLocalDirs(Task task,JobConf job) throws IOException {  String[] localSysDirs=StringUtils.getTrimmedStrings(System.getenv(Environment.LOCAL_DIRS.name()));  job.setStrings(MRConfig.LOCAL_DIR,localSysDirs);
private static void configureTask(JobConf job,Task task,Credentials credentials,Token<JobTokenIdentifier> jt) throws IOException {  job.setCredentials(credentials);  ApplicationAttemptId appAttemptId=ContainerId.fromString(System.getenv(Environment.CONTAINER_ID.name())).getApplicationAttemptId();
public static FSDataInputStream getPreviousJobHistoryFileStream(Configuration conf,ApplicationAttemptId applicationAttemptId) throws IOException {  FSDataInputStream in=null;  Path historyFile=null;  String jobId=TypeConverter.fromYarn(applicationAttemptId.getApplicationId()).toString();  String jobhistoryDir=JobHistoryUtils.getConfiguredHistoryStagingDirPrefix(conf,jobId);  Path histDirPath=FileContext.getFileContext(conf).makeQualified(new Path(jobhistoryDir));  FileContext fc=FileContext.getFileContext(histDirPath.toUri(),conf);  historyFile=fc.makeQualified(JobHistoryUtils.getStagingJobHistoryFile(histDirPath,jobId,(applicationAttemptId.getAttemptId() - 1)));
  String stagingDirStr=null;  String doneDirStr=null;  String userDoneDirStr=null;  try {    stagingDirStr=JobHistoryUtils.getConfiguredHistoryStagingDirPrefix(conf,jobId);    doneDirStr=JobHistoryUtils.getConfiguredHistoryIntermediateDoneDirPrefix(conf);    userDoneDirStr=JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);  } catch (  IOException e) {    LOG.error("Failed while getting the configured log directories",e);    throw new YarnRuntimeException(e);  }  try {    stagingDirPath=FileContext.getFileContext(conf).makeQualified(new Path(stagingDirStr));    stagingDirFS=FileSystem.get(stagingDirPath.toUri(),conf);    mkdir(stagingDirFS,stagingDirPath,new FsPermission(JobHistoryUtils.HISTORY_STAGING_DIR_PERMISSIONS));  } catch (  IOException e) {
 catch (  IOException e) {    LOG.error("Failed while getting the configured log directories",e);    throw new YarnRuntimeException(e);  }  try {    stagingDirPath=FileContext.getFileContext(conf).makeQualified(new Path(stagingDirStr));    stagingDirFS=FileSystem.get(stagingDirPath.toUri(),conf);    mkdir(stagingDirFS,stagingDirPath,new FsPermission(JobHistoryUtils.HISTORY_STAGING_DIR_PERMISSIONS));  } catch (  IOException e) {    LOG.error("Failed while checking for/creating  history staging path: [" + stagingDirPath + "]",e);    throw new YarnRuntimeException(e);  }  Path doneDirPath=null;  try {    doneDirPath=FileContext.getFileContext(conf).makeQualified(new Path(doneDirStr));    doneDirFS=FileSystem.get(doneDirPath.toUri(),conf);    if (!doneDirFS.exists(doneDirPath)) {
    mkdir(stagingDirFS,stagingDirPath,new FsPermission(JobHistoryUtils.HISTORY_STAGING_DIR_PERMISSIONS));  } catch (  IOException e) {    LOG.error("Failed while checking for/creating  history staging path: [" + stagingDirPath + "]",e);    throw new YarnRuntimeException(e);  }  Path doneDirPath=null;  try {    doneDirPath=FileContext.getFileContext(conf).makeQualified(new Path(doneDirStr));    doneDirFS=FileSystem.get(doneDirPath.toUri(),conf);    if (!doneDirFS.exists(doneDirPath)) {      if (JobHistoryUtils.shouldCreateNonUserDirectory(conf)) {        LOG.info("Creating intermediate history logDir: [" + doneDirPath + "] + based on conf. Should ideally be created by the JobHistoryServer: "+ MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR);        mkdir(doneDirFS,doneDirPath,new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS.toShort()));      } else {        String message="Not creating intermediate history logDir: [" + doneDirPath + "] based on conf: "+ MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR+ ". Either set to true or pre-create this directory with"+ " appropriate permissions";        LOG.error(message);
  Path doneDirPath=null;  try {    doneDirPath=FileContext.getFileContext(conf).makeQualified(new Path(doneDirStr));    doneDirFS=FileSystem.get(doneDirPath.toUri(),conf);    if (!doneDirFS.exists(doneDirPath)) {      if (JobHistoryUtils.shouldCreateNonUserDirectory(conf)) {        LOG.info("Creating intermediate history logDir: [" + doneDirPath + "] + based on conf. Should ideally be created by the JobHistoryServer: "+ MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR);        mkdir(doneDirFS,doneDirPath,new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS.toShort()));      } else {        String message="Not creating intermediate history logDir: [" + doneDirPath + "] based on conf: "+ MRJobConfig.MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR+ ". Either set to true or pre-create this directory with"+ " appropriate permissions";        LOG.error(message);        throw new YarnRuntimeException(message);      }    }  } catch (  IOException e) {    LOG.error("Failed checking for the existance of history intermediate " + "done directory: [" + doneDirPath + "]");    throw new YarnRuntimeException(e);
    LOG.error("Failed checking for the existance of history intermediate " + "done directory: [" + doneDirPath + "]");    throw new YarnRuntimeException(e);  }  try {    doneDirPrefixPath=FileContext.getFileContext(conf).makeQualified(new Path(userDoneDirStr));    mkdir(doneDirFS,doneDirPrefixPath,JobHistoryUtils.getConfiguredHistoryIntermediateUserDoneDirPermissions(conf));  } catch (  IOException e) {    LOG.error("Error creating user intermediate history done directory: [ " + doneDirPrefixPath + "]",e);    throw new YarnRuntimeException(e);  }  maxUnflushedCompletionEvents=conf.getInt(MRJobConfig.MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS,MRJobConfig.DEFAULT_MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS);  postJobCompletionMultiplier=conf.getInt(MRJobConfig.MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER,MRJobConfig.DEFAULT_MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER);  flushTimeout=conf.getLong(MRJobConfig.MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS,MRJobConfig.DEFAULT_MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS);  minQueueSizeForBatchingFlushes=conf.getInt(MRJobConfig.MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD,MRJobConfig.DEFAULT_MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD);  if (conf.getBoolean(MRJobConfig.MAPREDUCE_JOB_EMIT_TIMELINE_DATA,MRJobConfig.DEFAULT_MAPREDUCE_JOB_EMIT_TIMELINE_DATA)) {    LOG.info("Emitting job history data to the timeline service is enabled");    if (YarnConfiguration.timelineServiceEnabled(conf)) {
private void mkdir(FileSystem fs,Path path,FsPermission fsp) throws IOException {  if (!fs.exists(path)) {    try {      fs.mkdirs(path,fsp);      FileStatus fsStatus=fs.getFileStatus(path);      LOG.info("Perms after creating " + fsStatus.getPermission().toShort() + ", Expected: "+ fsp.toShort());      if (fsStatus.getPermission().toShort() != fsp.toShort()) {
synchronized (lock) {    if (eventHandlingThread != null) {      LOG.debug("Interrupting Event Handling thread");      eventHandlingThread.interrupt();    } else {      LOG.debug("Null event handling thread");    }  }  try {    if (eventHandlingThread != null) {      LOG.debug("Waiting for Event Handling thread to complete");      eventHandlingThread.join();    }  } catch (  InterruptedException ie) {    LOG.info("Interrupted Exception while stopping",ie);  }  for (  MetaInfo mi : fileMap.values()) {    try {      if (LOG.isDebugEnabled()) {
      eventHandlingThread.interrupt();    } else {      LOG.debug("Null event handling thread");    }  }  try {    if (eventHandlingThread != null) {      LOG.debug("Waiting for Event Handling thread to complete");      eventHandlingThread.join();    }  } catch (  InterruptedException ie) {    LOG.info("Interrupted Exception while stopping",ie);  }  for (  MetaInfo mi : fileMap.values()) {    try {      if (LOG.isDebugEnabled()) {        LOG.debug("Shutting down timer for " + mi);      }      mi.shutDownTimer();    } catch (    IOException e) {
 catch (    IOException e) {      LOG.info("Exception while canceling delayed flush timer. " + "Likely caused by a failed flush " + e.getMessage());    }  }  Iterator<JobHistoryEvent> it=eventQueue.iterator();  while (it.hasNext()) {    JobHistoryEvent ev=it.next();    LOG.info("In stop, writing event " + ev.getType());    handleEvent(ev);  }  if (forceJobCompletion) {    for (    Map.Entry<JobId,MetaInfo> jobIt : fileMap.entrySet()) {      JobId toClose=jobIt.getKey();      MetaInfo mi=jobIt.getValue();      if (mi != null && mi.isWriterActive()) {        LOG.warn("Found jobId " + toClose + " to have not been closed. Will close");        final Job job=context.getJob(toClose);        int successfulMaps=job.getCompletedMaps() - job.getFailedMaps() - job.getKilledMaps();
  if (stagingDirPath == null) {    LOG.error("Log Directory is null, returning");    throw new IOException("Missing Log Directory for History");  }  MetaInfo oldFi=fileMap.get(jobId);  Configuration conf=getConfig();  Path historyFile=JobHistoryUtils.getStagingJobHistoryFile(stagingDirPath,jobId,startCount);  String user=UserGroupInformation.getCurrentUser().getShortUserName();  if (user == null) {    throw new IOException("User is null while setting up jobhistory eventwriter");  }  String jobName=context.getJob(jobId).getName();  EventWriter writer=(oldFi == null) ? null : oldFi.writer;  Path logDirConfPath=JobHistoryUtils.getStagingConfFile(stagingDirPath,jobId,startCount);  if (writer == null) {    try {      writer=createEventWriter(historyFile);
    throw new IOException("Missing Log Directory for History");  }  MetaInfo oldFi=fileMap.get(jobId);  Configuration conf=getConfig();  Path historyFile=JobHistoryUtils.getStagingJobHistoryFile(stagingDirPath,jobId,startCount);  String user=UserGroupInformation.getCurrentUser().getShortUserName();  if (user == null) {    throw new IOException("User is null while setting up jobhistory eventwriter");  }  String jobName=context.getJob(jobId).getName();  EventWriter writer=(oldFi == null) ? null : oldFi.writer;  Path logDirConfPath=JobHistoryUtils.getStagingConfFile(stagingDirPath,jobId,startCount);  if (writer == null) {    try {      writer=createEventWriter(historyFile);      LOG.info("Event Writer setup for JobId: " + jobId + ", File: "+ historyFile);    } catch (    IOException ioe) {
      try {        AMStartedEvent amStartedEvent=(AMStartedEvent)event.getHistoryEvent();        setupEventWriter(event.getJobID(),amStartedEvent);      } catch (      IOException ioe) {        LOG.error("Error JobHistoryEventHandler in handleEvent: " + event,ioe);        throw new YarnRuntimeException(ioe);      }    }    MetaInfo mi=fileMap.get(event.getJobID());    try {      HistoryEvent historyEvent=event.getHistoryEvent();      if (!(historyEvent instanceof NormalizedResourceEvent)) {        mi.writeEvent(historyEvent);      }      processEventForJobSummary(event.getHistoryEvent(),mi.getJobSummary(),event.getJobID());      if (LOG.isDebugEnabled()) {        LOG.debug("In HistoryEventHandler " + event.getHistoryEvent().getEventType());      }    } catch (    IOException e) {
tEvent.addEventInfo("START_TIME",ase.getStartTime());tEvent.addEventInfo("SUBMIT_TIME",ase.getSubmitTime());tEntity.addEvent(tEvent);tEntity.setEntityId(jobId.toString());tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);break;default:break;}try {TimelinePutResponse response=timelineClient.putEntities(tEntity);List<TimelinePutResponse.TimelinePutError> errors=response.getErrors();if (errors.size() == 0) {if (LOG.isDebugEnabled()) {LOG.debug("Timeline entities are successfully put in event " + event.getEventType());}} else {for (TimelinePutResponse.TimelinePutError error : errors) {
tEntity.addEvent(tEvent);tEntity.setEntityId(jobId.toString());tEntity.setEntityType(MAPREDUCE_JOB_ENTITY_TYPE);break;default:break;}try {TimelinePutResponse response=timelineClient.putEntities(tEntity);List<TimelinePutResponse.TimelinePutError> errors=response.getErrors();if (errors.size() == 0) {if (LOG.isDebugEnabled()) {LOG.debug("Timeline entities are successfully put in event " + event.getEventType());}} else {for (TimelinePutResponse.TimelinePutError error : errors) {LOG.error("Error when publishing entity [" + error.getEntityType() + ","+ error.getEntityId()+ "], server side error code: "+ error.getErrorCode());}}} catch (YarnException|IOException|ClientHandlerException ex) {
      int size=entry.getKey().length() + entry.getValue().length();      configSize+=size;      if (configSize > JobHistoryEventUtils.ATS_CONFIG_PUBLISH_SIZE_BYTES) {        if (jobEntityForConfigs.getConfigs().size() > 0) {          timelineV2Client.putEntities(jobEntityForConfigs);          timelineV2Client.putEntities(appEntityForConfigs);          jobEntityForConfigs=createJobEntity(jobId);          appEntityForConfigs=new ApplicationEntity();          appEntityForConfigs.setId(appId);        }        configSize=size;      }      jobEntityForConfigs.addConfig(entry.getKey(),entry.getValue());      appEntityForConfigs.addConfig(entry.getKey(),entry.getValue());    }    if (configSize > 0) {      timelineV2Client.putEntities(jobEntityForConfigs);      timelineV2Client.putEntities(appEntityForConfigs);
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity appEntityWithJobMetrics=null;if (taskId == null) {tEntity=createJobEntity(event,timestamp,jobId,MAPREDUCE_JOB_ENTITY_TYPE,setCreatedTime);if (event.getEventType() == EventType.JOB_FINISHED && event.getTimelineMetrics() != null) {appEntityWithJobMetrics=createAppEntityWithJobMetrics(event,jobId);}} else {if (taskAttemptId == null) {tEntity=createTaskEntity(event,timestamp,taskId,MAPREDUCE_TASK_ENTITY_TYPE,MAPREDUCE_JOB_ENTITY_TYPE,jobId,setCreatedTime,taskIdPrefix);} else {tEntity=createTaskAttemptEntity(event,timestamp,taskAttemptId,MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE,MAPREDUCE_TASK_ENTITY_TYPE,taskId,setCreatedTime,taskAttemptIdPrefix);}}try {if (appEntityWithJobMetrics == null) {timelineV2Client.putEntitiesAsync(tEntity);} else {timelineV2Client.putEntities(tEntity,appEntityWithJobMetrics);
    throw new IOException("No MetaInfo found for JobId: [" + jobId + "]");  }  if (mi.getHistoryFile() == null) {    LOG.warn("No file for job-history with " + jobId + " found in cache!");  }  if (mi.getConfFile() == null) {    LOG.warn("No file for jobconf with " + jobId + " found in cache!");  }  Path qualifiedSummaryDoneFile=null;  FSDataOutputStream summaryFileOut=null;  try {    String doneSummaryFileName=getTempFileName(JobHistoryUtils.getIntermediateSummaryFileName(jobId));    qualifiedSummaryDoneFile=doneDirFS.makeQualified(new Path(doneDirPrefixPath,doneSummaryFileName));    summaryFileOut=doneDirFS.create(qualifiedSummaryDoneFile,true);    summaryFileOut.writeUTF(mi.getJobSummary().getJobSummaryString());    summaryFileOut.close();    doneDirFS.setPermission(qualifiedSummaryDoneFile,new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));  } catch (  IOException e) {
      Path historyFile=mi.getHistoryFile();      Path qualifiedLogFile=stagingDirFS.makeQualified(historyFile);      int jobNameLimit=getConfig().getInt(JHAdminConfig.MR_HS_JOBNAME_LIMIT,JHAdminConfig.DEFAULT_MR_HS_JOBNAME_LIMIT);      String doneJobHistoryFileName=getTempFileName(FileNameIndexUtils.getDoneFileName(mi.getJobIndexInfo(),jobNameLimit));      qualifiedDoneFile=doneDirFS.makeQualified(new Path(doneDirPrefixPath,doneJobHistoryFileName));      if (moveToDoneNow(qualifiedLogFile,qualifiedDoneFile)) {        String historyUrl=MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(getConfig(),context.getApplicationID());        context.setHistoryUrl(historyUrl);        LOG.info("Set historyUrl to " + historyUrl);      }    }    Path qualifiedConfDoneFile=null;    if (mi.getConfFile() != null) {      Path confFile=mi.getConfFile();      Path qualifiedConfFile=stagingDirFS.makeQualified(confFile);      String doneConfFileName=getTempFileName(JobHistoryUtils.getIntermediateConfFileName(jobId));      qualifiedConfDoneFile=doneDirFS.makeQualified(new Path(doneDirPrefixPath,doneConfFileName));
protected void moveTmpToDone(Path tmpPath) throws IOException {  if (tmpPath != null) {    String tmpFileName=tmpPath.getName();    String fileName=getFileNameFromTmpFN(tmpFileName);    Path path=new Path(tmpPath.getParent(),fileName);    doneDirFS.rename(tmpPath,path);
protected boolean moveToDoneNow(Path fromPath,Path toPath) throws IOException {  boolean success=false;  if (stagingDirFS.exists(fromPath)) {
protected boolean moveToDoneNow(Path fromPath,Path toPath) throws IOException {  boolean success=false;  if (stagingDirFS.exists(fromPath)) {    LOG.info("Copying " + fromPath.toString() + " to "+ toPath.toString());    doneDirFS.delete(toPath,true);    boolean copied=FileUtil.copy(stagingDirFS,fromPath,doneDirFS,toPath,false,getConfig());    doneDirFS.setPermission(toPath,new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_FILE_PERMISSIONS));    if (copied) {
public void setForcejobCompletion(boolean forceJobCompletion){  this.forceJobCompletion=forceJobCompletion;
  }  boolean copyHistory=false;  committer=createOutputCommitter(conf);  try {    String user=UserGroupInformation.getCurrentUser().getShortUserName();    Path stagingDir=MRApps.getStagingAreaDir(conf,user);    FileSystem fs=getFileSystem(conf);    boolean stagingExists=fs.exists(stagingDir);    Path startCommitFile=MRApps.getStartJobCommitFile(conf,user,jobId);    boolean commitStarted=fs.exists(startCommitFile);    Path endCommitSuccessFile=MRApps.getEndJobCommitSuccessFile(conf,user,jobId);    boolean commitSuccess=fs.exists(endCommitSuccessFile);    Path endCommitFailureFile=MRApps.getEndJobCommitFailureFile(conf,user,jobId);    boolean commitFailure=fs.exists(endCommitFailureFile);    if (!stagingExists) {      isLastAMRetry=true;
    Path startCommitFile=MRApps.getStartJobCommitFile(conf,user,jobId);    boolean commitStarted=fs.exists(startCommitFile);    Path endCommitSuccessFile=MRApps.getEndJobCommitSuccessFile(conf,user,jobId);    boolean commitSuccess=fs.exists(endCommitSuccessFile);    Path endCommitFailureFile=MRApps.getEndJobCommitFailureFile(conf,user,jobId);    boolean commitFailure=fs.exists(endCommitFailureFile);    if (!stagingExists) {      isLastAMRetry=true;      LOG.info("Attempt num: " + appAttemptID.getAttemptId() + " is last retry: "+ isLastAMRetry+ " because the staging dir doesn't exist.");      errorHappenedShutDown=true;      forcedState=JobStateInternal.ERROR;      shutDownMessage="Staging dir does not exist " + stagingDir;      LOG.error(shutDownMessage);    } else     if (commitStarted) {      errorHappenedShutDown=true;
private OutputCommitter createOutputCommitter(Configuration conf){  return callWithJobClassLoader(conf,new Action<OutputCommitter>(){    public OutputCommitter call(    Configuration conf){      OutputCommitter committer=null;
  return callWithJobClassLoader(conf,new Action<Speculator>(){    public Speculator call(    Configuration conf){      Class<? extends Speculator> speculatorClass;      try {        speculatorClass=conf.getClass(MRJobConfig.MR_AM_JOB_SPECULATOR,DefaultSpeculator.class,Speculator.class);        Constructor<? extends Speculator> speculatorConstructor=speculatorClass.getConstructor(Configuration.class,AppContext.class);        Speculator result=speculatorConstructor.newInstance(conf,context);        return result;      } catch (      InstantiationException ex) {        LOG.error("Can't make a speculator -- check " + MRJobConfig.MR_AM_JOB_SPECULATOR,ex);        throw new YarnRuntimeException(ex);      }catch (      IllegalAccessException ex) {        LOG.error("Can't make a speculator -- check " + MRJobConfig.MR_AM_JOB_SPECULATOR,ex);        throw new YarnRuntimeException(ex);      }catch (      InvocationTargetException ex) {
      try {        speculatorClass=conf.getClass(MRJobConfig.MR_AM_JOB_SPECULATOR,DefaultSpeculator.class,Speculator.class);        Constructor<? extends Speculator> speculatorConstructor=speculatorClass.getConstructor(Configuration.class,AppContext.class);        Speculator result=speculatorConstructor.newInstance(conf,context);        return result;      } catch (      InstantiationException ex) {        LOG.error("Can't make a speculator -- check " + MRJobConfig.MR_AM_JOB_SPECULATOR,ex);        throw new YarnRuntimeException(ex);      }catch (      IllegalAccessException ex) {        LOG.error("Can't make a speculator -- check " + MRJobConfig.MR_AM_JOB_SPECULATOR,ex);        throw new YarnRuntimeException(ex);      }catch (      InvocationTargetException ex) {        LOG.error("Can't make a speculator -- check " + MRJobConfig.MR_AM_JOB_SPECULATOR,ex);        throw new YarnRuntimeException(ex);      }catch (      NoSuchMethodException ex) {
  cleanUpPreviousJobOutput();  AMInfo amInfo=MRBuilderUtils.newAMInfo(appAttemptID,startTime,containerID,nmHost,nmPort,nmHttpPort);  job=createJob(getConfig(),forcedState,shutDownMessage);  for (  AMInfo info : amInfos) {    dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(),new AMStartedEvent(info.getAppAttemptId(),info.getStartTime(),info.getContainerId(),info.getNodeManagerHost(),info.getNodeManagerPort(),info.getNodeManagerHttpPort(),appSubmitTime)));  }  dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(),new AMStartedEvent(amInfo.getAppAttemptId(),amInfo.getStartTime(),amInfo.getContainerId(),amInfo.getNodeManagerHost(),amInfo.getNodeManagerPort(),amInfo.getNodeManagerHttpPort(),this.forcedState == null ? null : this.forcedState.toString(),appSubmitTime)));  amInfos.add(amInfo);  DefaultMetricsSystem.initialize("MRAppMaster");  boolean initFailed=false;  if (!errorHappenedShutDown) {    JobEvent initJobEvent=new JobEvent(job.getID(),JobEventType.JOB_INIT);    jobEventDispatcher.handle(initJobEvent);    initFailed=(((JobImpl)job).getInternalState() != JobStateInternal.INITED);    if (job.isUber()) {      speculatorEventDispatcher.disableSpeculation();
  for (  AMInfo info : amInfos) {    dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(),new AMStartedEvent(info.getAppAttemptId(),info.getStartTime(),info.getContainerId(),info.getNodeManagerHost(),info.getNodeManagerPort(),info.getNodeManagerHttpPort(),appSubmitTime)));  }  dispatcher.getEventHandler().handle(new JobHistoryEvent(job.getID(),new AMStartedEvent(amInfo.getAppAttemptId(),amInfo.getStartTime(),amInfo.getContainerId(),amInfo.getNodeManagerHost(),amInfo.getNodeManagerPort(),amInfo.getNodeManagerHttpPort(),this.forcedState == null ? null : this.forcedState.toString(),appSubmitTime)));  amInfos.add(amInfo);  DefaultMetricsSystem.initialize("MRAppMaster");  boolean initFailed=false;  if (!errorHappenedShutDown) {    JobEvent initJobEvent=new JobEvent(job.getID(),JobEventType.JOB_INIT);    jobEventDispatcher.handle(initJobEvent);    initFailed=(((JobImpl)job).getInternalState() != JobStateInternal.INITED);    if (job.isUber()) {      speculatorEventDispatcher.disableSpeculation();      LOG.info("MRAppMaster uberizing job " + job.getID() + " in local container (\"uber-AM\") on node "+ nmHost+ ":"+ nmPort+ ".");    } else {      dispatcher.getEventHandler().handle(new SpeculatorEvent(job.getID(),clock.getTime()));
private static FSDataInputStream getPreviousJobHistoryStream(Configuration conf,ApplicationAttemptId appAttemptId) throws IOException {  Path historyFile=JobHistoryUtils.getPreviousJobHistoryPath(conf,appAttemptId);
private void parsePreviousJobHistory() throws IOException {  FSDataInputStream in=getPreviousJobHistoryStream(getConfig(),appAttemptID);  JobHistoryParser parser=new JobHistoryParser(in);  JobInfo jobInfo=parser.parse();  Exception parseException=parser.getParseException();  if (parseException != null) {
  FSDataInputStream in=getPreviousJobHistoryStream(getConfig(),appAttemptID);  JobHistoryParser parser=new JobHistoryParser(in);  JobInfo jobInfo=parser.parse();  Exception parseException=parser.getParseException();  if (parseException != null) {    LOG.info("Got an error parsing job-history file" + ", ignoring incomplete events.",parseException);  }  Map<org.apache.hadoop.mapreduce.TaskID,TaskInfo> taskInfos=jobInfo.getAllTasks();  for (  TaskInfo taskInfo : taskInfos.values()) {    if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {      Iterator<Entry<TaskAttemptID,TaskAttemptInfo>> taskAttemptIterator=taskInfo.getAllTaskAttempts().entrySet().iterator();      while (taskAttemptIterator.hasNext()) {        Map.Entry<TaskAttemptID,TaskAttemptInfo> currentEntry=taskAttemptIterator.next();        if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {          taskAttemptIterator.remove();        }      }      completedTasksFromPreviousRun.put(TypeConverter.toYarn(taskInfo.getTaskId()),taskInfo);
private static void validateInputParam(String value,String param) throws IOException {  if (value == null) {    String msg=param + " is null";
    validateInputParam(nodePortString,Environment.NM_PORT.name());    validateInputParam(nodeHttpPortString,Environment.NM_HTTP_PORT.name());    validateInputParam(appSubmitTimeStr,ApplicationConstants.APP_SUBMIT_TIME_ENV);    ContainerId containerId=ContainerId.fromString(containerIdStr);    ApplicationAttemptId applicationAttemptId=containerId.getApplicationAttemptId();    if (applicationAttemptId != null) {      CallerContext.setCurrent(new CallerContext.Builder("mr_appmaster_" + applicationAttemptId.toString()).build());    }    long appSubmitTime=Long.parseLong(appSubmitTimeStr);    MRAppMaster appMaster=new MRAppMaster(applicationAttemptId,containerId,nodeHostString,Integer.parseInt(nodePortString),Integer.parseInt(nodeHttpPortString),appSubmitTime);    ShutdownHookManager.get().addShutdownHook(new MRAppMasterShutdownHook(appMaster),SHUTDOWN_HOOK_PRIORITY);    JobConf conf=new JobConf(new YarnConfiguration());    conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));    MRWebAppUtil.initialize(conf);    String systemPropsToLog=MRApps.getSystemPropertiesToLog(conf);    if (systemPropsToLog != null) {
    if (applicationAttemptId != null) {      CallerContext.setCurrent(new CallerContext.Builder("mr_appmaster_" + applicationAttemptId.toString()).build());    }    long appSubmitTime=Long.parseLong(appSubmitTimeStr);    MRAppMaster appMaster=new MRAppMaster(applicationAttemptId,containerId,nodeHostString,Integer.parseInt(nodePortString),Integer.parseInt(nodeHttpPortString),appSubmitTime);    ShutdownHookManager.get().addShutdownHook(new MRAppMasterShutdownHook(appMaster),SHUTDOWN_HOOK_PRIORITY);    JobConf conf=new JobConf(new YarnConfiguration());    conf.addResource(new Path(MRJobConfig.JOB_CONF_FILE));    MRWebAppUtil.initialize(conf);    String systemPropsToLog=MRApps.getSystemPropertiesToLog(conf);    if (systemPropsToLog != null) {      LOG.info(systemPropsToLog);    }    String jobUserName=System.getenv(ApplicationConstants.Environment.USER.name());    conf.set(MRJobConfig.USER_NAME,jobUserName);    initAndStartAppMaster(appMaster,conf,jobUserName);  } catch (  Throwable t) {
public void notifyIsLastAMRetry(boolean isLastAMRetry){  if (containerAllocator instanceof ContainerAllocatorRouter) {
protected static void initAndStartAppMaster(final MRAppMaster appMaster,final JobConf conf,String jobUserName) throws IOException, InterruptedException {  UserGroupInformation.setConfiguration(conf);  SecurityUtil.setConfiguration(conf);  Credentials credentials=UserGroupInformation.getCurrentUser().getCredentials();
@Override protected void serviceInit(Configuration conf) throws Exception {  super.serviceInit(conf);  taskTimeOut=conf.getLong(MRJobConfig.TASK_TIMEOUT,MRJobConfig.DEFAULT_TASK_TIMEOUT_MILLIS);  unregisterTimeOut=conf.getLong(MRJobConfig.TASK_EXIT_TIMEOUT,MRJobConfig.TASK_EXIT_TIMEOUT_DEFAULT);  taskStuckTimeOut=conf.getLong(MRJobConfig.TASK_STUCK_TIMEOUT_MS,MRJobConfig.DEFAULT_TASK_STUCK_TIMEOUT_MS);  long taskProgressReportIntervalMillis=MRJobConfUtil.getTaskProgressReportInterval(conf);  long minimumTaskTimeoutAllowed=taskProgressReportIntervalMillis * 2;  if (taskTimeOut < minimumTaskTimeoutAllowed) {    taskTimeOut=minimumTaskTimeoutAllowed;
protected void serviceStart() throws Exception {  Configuration conf=getConfig();  YarnRPC rpc=YarnRPC.create(conf);  InetSocketAddress address=new InetSocketAddress(0);  server=rpc.getServer(MRClientProtocol.class,protocolHandler,address,conf,appContext.getClientToAMTokenSecretManager(),conf.getInt(MRJobConfig.MR_AM_JOB_CLIENT_THREAD_COUNT,MRJobConfig.DEFAULT_MR_AM_JOB_CLIENT_THREAD_COUNT),MRJobConfig.MR_AM_JOB_CLIENT_PORT_RANGE);  if (conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,false)) {    refreshServiceAcls(conf,new MRAMPolicyProvider());  }  server.start();  this.bindAddress=NetUtils.createSocketAddrForHost(appContext.getNMHostname(),server.getListenerAddress().getPort());  LOG.info("Instantiated MRClientService at " + this.bindAddress);  try {    HttpConfig.Policy httpPolicy=conf.getBoolean(MRJobConfig.MR_AM_WEBAPP_HTTPS_ENABLED,MRJobConfig.DEFAULT_MR_AM_WEBAPP_HTTPS_ENABLED) ? Policy.HTTPS_ONLY : Policy.HTTP_ONLY;    boolean needsClientAuth=conf.getBoolean(MRJobConfig.MR_AM_WEBAPP_HTTPS_CLIENT_AUTH,MRJobConfig.DEFAULT_MR_AM_WEBAPP_HTTPS_CLIENT_AUTH);    webApp=WebApps.$for("mapreduce",AppContext.class,appContext,"ws").withHttpPolicy(conf,httpPolicy).withPortRange(conf,MRJobConfig.MR_AM_WEBAPP_PORT_RANGE).needsClientAuth(needsClientAuth).start(new AMWebApp());  } catch (  Exception e) {
        Thread thread=new Thread(r);        thread.setContextClassLoader(jobClassLoader);        return thread;      }    };    tfBuilder.setThreadFactory(backingTf);  }  ThreadFactory tf=tfBuilder.build();  launcherPool=new HadoopThreadPoolExecutor(5,5,1,TimeUnit.HOURS,new LinkedBlockingQueue<Runnable>(),tf);  eventHandlingThread=new Thread(new Runnable(){    @Override public void run(){      CommitterEvent event=null;      while (!stopped.get() && !Thread.currentThread().isInterrupted()) {        try {          event=eventQueue.take();        } catch (        InterruptedException e) {          if (!stopped.get()) {
@Override public void handle(JobEvent event){  if (LOG.isDebugEnabled()) {
  boolean smallInput=(dataInputLength <= sysMaxBytes);  long requiredMapMB=conf.getLong(MRJobConfig.MAP_MEMORY_MB,0);  long requiredReduceMB=conf.getLong(MRJobConfig.REDUCE_MEMORY_MB,0);  long requiredMB=Math.max(requiredMapMB,requiredReduceMB);  int requiredMapCores=conf.getInt(MRJobConfig.MAP_CPU_VCORES,MRJobConfig.DEFAULT_MAP_CPU_VCORES);  int requiredReduceCores=conf.getInt(MRJobConfig.REDUCE_CPU_VCORES,MRJobConfig.DEFAULT_REDUCE_CPU_VCORES);  int requiredCores=Math.max(requiredMapCores,requiredReduceCores);  if (numReduceTasks == 0) {    requiredMB=requiredMapMB;    requiredCores=requiredMapCores;  }  boolean smallMemory=(requiredMB <= sysMemSizeForUberSlot) || (sysMemSizeForUberSlot == JobConf.DISABLED_MEMORY_LIMIT);  boolean smallCpu=requiredCores <= sysCPUSizeForUberSlot;  boolean notChainJob=!isChainJob(conf);  isUber=uberEnabled && smallNumMapTasks && smallNumReduceTasks&& smallInput&& smallMemory&& smallCpu&& notChainJob;  if (isUber) {
    conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART,1.0f);    conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS,1);    conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS,1);    conf.setBoolean(MRJobConfig.MAP_SPECULATIVE,false);    conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE,false);  } else {    StringBuilder msg=new StringBuilder();    msg.append("Not uberizing ").append(jobId).append(" because:");    if (!uberEnabled)     msg.append(" not enabled;");    if (!smallNumMapTasks)     msg.append(" too many maps;");    if (!smallNumReduceTasks)     msg.append(" too many reduces;");    if (!smallInput)     msg.append(" too much input;");    if (!smallCpu)     msg.append(" too much CPU;");    if (!smallMemory)     msg.append(" too much RAM;");    if (!smallCpu)     msg.append(" too much CPU;");
private void actOnUnusableNode(NodeId nodeId,NodeState nodeState){  if (getInternalState() == JobStateInternal.RUNNING && !allReducersComplete()) {    List<TaskAttemptId> taskAttemptIdList=nodesToSucceededTaskAttempts.get(nodeId);    if (taskAttemptIdList != null) {      String mesg="TaskAttempt killed because it ran on unusable node " + nodeId;      for (      TaskAttemptId id : taskAttemptIdList) {        if (TaskType.MAP == id.getTaskId().getTaskType()) {
private static ByteBuffer configureTokens(Token<JobTokenIdentifier> jobToken,Credentials credentials,Map<String,ByteBuffer> serviceData) throws IOException {
private static ByteBuffer configureTokens(Token<JobTokenIdentifier> jobToken,Credentials credentials,Map<String,ByteBuffer> serviceData) throws IOException {  LOG.info("Adding #" + credentials.numberOfTokens() + " tokens and #"+ credentials.numberOfSecretKeys()+ " secret keys for NM use for launching container");  Credentials taskCredentials=new Credentials(credentials);  TokenCache.setJobToken(jobToken,taskCredentials);  DataOutputBuffer containerTokens_dob=new DataOutputBuffer();
@SuppressWarnings("unchecked") @Override public void handle(TaskAttemptEvent event){  if (LOG.isDebugEnabled()) {
@SuppressWarnings("unchecked") @Override public void handle(TaskAttemptEvent event){  if (LOG.isDebugEnabled()) {    LOG.debug("Processing " + event.getTaskAttemptID() + " of type "+ event.getType());  }  writeLock.lock();  try {    final TaskAttemptStateInternal oldState=getInternalState();    try {      stateMachine.doTransition(event.getType(),event);    } catch (    InvalidStateTransitionException e) {      LOG.error("Can't handle this event at current state for " + this.attemptId,e);      eventHandler.handle(new JobDiagnosticsUpdateEvent(this.attemptId.getTaskId().getJobId(),"Invalid event " + event.getType() + " on TaskAttempt "+ this.attemptId));      eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),JobEventType.INTERNAL_ERROR));    }    if (oldState != getInternalState()) {      if (getInternalState() == TaskAttemptStateInternal.FAILED) {        String nodeId=null == this.container ? "Not-assigned" : this.container.getNodeId().toString();
    LOG.debug("Processing " + event.getTaskAttemptID() + " of type "+ event.getType());  }  writeLock.lock();  try {    final TaskAttemptStateInternal oldState=getInternalState();    try {      stateMachine.doTransition(event.getType(),event);    } catch (    InvalidStateTransitionException e) {      LOG.error("Can't handle this event at current state for " + this.attemptId,e);      eventHandler.handle(new JobDiagnosticsUpdateEvent(this.attemptId.getTaskId().getJobId(),"Invalid event " + event.getType() + " on TaskAttempt "+ this.attemptId));      eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(),JobEventType.INTERNAL_ERROR));    }    if (oldState != getInternalState()) {      if (getInternalState() == TaskAttemptStateInternal.FAILED) {        String nodeId=null == this.container ? "Not-assigned" : this.container.getNodeId().toString();        LOG.info(attemptId + " transitioned from state " + oldState+ " to "+ getInternalState()+ ", event type is "+ event.getType()+ " and nodeId="+ nodeId);      } else {
  reportedStatus.id=attemptId;  reportedStatus.progress=1.0f;  reportedStatus.counters=taInfo.getCounters();  reportedStatus.stateString=taInfo.getState();  reportedStatus.phase=Phase.CLEANUP;  reportedStatus.mapFinishTime=taInfo.getMapFinishTime();  reportedStatus.shuffleFinishTime=taInfo.getShuffleFinishTime();  reportedStatus.sortFinishTime=taInfo.getSortFinishTime();  addDiagnosticInfo(taInfo.getError());  boolean needToClean=false;  String recoveredState=taInfo.getTaskStatus();  if (recoverOutput && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {    TaskAttemptContext tac=new TaskAttemptContextImpl(conf,TypeConverter.fromYarn(attemptId));    try {      committer.recoverTask(tac);
  reportedStatus.counters=taInfo.getCounters();  reportedStatus.stateString=taInfo.getState();  reportedStatus.phase=Phase.CLEANUP;  reportedStatus.mapFinishTime=taInfo.getMapFinishTime();  reportedStatus.shuffleFinishTime=taInfo.getShuffleFinishTime();  reportedStatus.sortFinishTime=taInfo.getSortFinishTime();  addDiagnosticInfo(taInfo.getError());  boolean needToClean=false;  String recoveredState=taInfo.getTaskStatus();  if (recoverOutput && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {    TaskAttemptContext tac=new TaskAttemptContextImpl(conf,TypeConverter.fromYarn(attemptId));    try {      committer.recoverTask(tac);      LOG.info("Recovered output from task attempt " + attemptId);    } catch (    Exception e) {
  reportedStatus.stateString=taInfo.getState();  reportedStatus.phase=Phase.CLEANUP;  reportedStatus.mapFinishTime=taInfo.getMapFinishTime();  reportedStatus.shuffleFinishTime=taInfo.getShuffleFinishTime();  reportedStatus.sortFinishTime=taInfo.getSortFinishTime();  addDiagnosticInfo(taInfo.getError());  boolean needToClean=false;  String recoveredState=taInfo.getTaskStatus();  if (recoverOutput && TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {    TaskAttemptContext tac=new TaskAttemptContextImpl(conf,TypeConverter.fromYarn(attemptId));    try {      committer.recoverTask(tac);      LOG.info("Recovered output from task attempt " + attemptId);    } catch (    Exception e) {      LOG.error("Unable to recover task attempt " + attemptId,e);
      needToClean=true;    }  }  TaskAttemptStateInternal attemptState;  if (TaskAttemptState.SUCCEEDED.toString().equals(recoveredState)) {    attemptState=TaskAttemptStateInternal.SUCCEEDED;    reportedStatus.taskState=TaskAttemptState.SUCCEEDED;    eventHandler.handle(createJobCounterUpdateEventTASucceeded(this));    logAttemptFinishedEvent(attemptState);  } else   if (TaskAttemptState.FAILED.toString().equals(recoveredState)) {    attemptState=TaskAttemptStateInternal.FAILED;    reportedStatus.taskState=TaskAttemptState.FAILED;    eventHandler.handle(createJobCounterUpdateEventTAFailed(this,false));    TaskAttemptUnsuccessfulCompletionEvent tauce=createTaskAttemptUnsuccessfulCompletionEvent(this,TaskAttemptStateInternal.FAILED);    eventHandler.handle(new JobHistoryEvent(attemptId.getTaskId().getJobId(),tauce));  } else {    if (!TaskAttemptState.KILLED.toString().equals(recoveredState)) {
@SuppressWarnings("unchecked") private void sendLaunchedEvents(){  JobCounterUpdateEvent jce=new JobCounterUpdateEvent(attemptId.getTaskId().getJobId());  jce.addCounterUpdate(attemptId.getTaskId().getTaskType() == TaskType.MAP ? JobCounter.TOTAL_LAUNCHED_MAPS : JobCounter.TOTAL_LAUNCHED_REDUCES,1);  eventHandler.handle(jce);
@Override public boolean canCommit(TaskAttemptId taskAttemptID){  readLock.lock();  boolean canCommit=false;  try {    if (commitAttempt != null) {      canCommit=taskAttemptID.equals(commitAttempt);
private TaskAttemptImpl addAttempt(Avataar avataar){  TaskAttemptImpl attempt=createAttempt();  attempt.setAvataar(avataar);  if (LOG.isDebugEnabled()) {
@Override public void handle(TaskEvent event){  if (LOG.isDebugEnabled()) {
protected void internalError(TaskEventType type){
private TaskStateInternal recover(TaskInfo taskInfo,OutputCommitter committer,boolean recoverTaskOutput){
      Set<String> allNodes=new HashSet<String>();      while (!stopped.get() && !Thread.currentThread().isInterrupted()) {        try {          event=eventQueue.take();        } catch (        InterruptedException e) {          if (!stopped.get()) {            LOG.error("Returning, interrupted : " + e);          }          return;        }        allNodes.add(event.getContainerMgrAddress());        int poolSize=launcherPool.getCorePoolSize();        if (poolSize != limitOnPoolSize) {          int numNodes=allNodes.size();          int idealPoolSize=Math.min(limitOnPoolSize,numNodes);          if (poolSize < idealPoolSize) {            int newPoolSize=Math.min(limitOnPoolSize,idealPoolSize + initialPoolSize);
@SuppressWarnings("unchecked") void sendContainerLaunchFailedMsg(TaskAttemptId taskAttemptID,String message){
  AllocateRequest allocateRequest=AllocateRequest.newInstance(this.lastResponseID,super.getApplicationProgress(),new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>(),null);  AllocateResponse allocateResponse=null;  try {    allocateResponse=scheduler.allocate(allocateRequest);    retrystartTime=System.currentTimeMillis();  } catch (  ApplicationAttemptNotFoundException e) {    LOG.info("Event from RM: shutting down Application Master");    eventHandler.handle(new JobEvent(this.getJob().getID(),JobEventType.JOB_AM_REBOOT));    throw new YarnRuntimeException("Resource Manager doesn't recognize AttemptId: " + this.getContext().getApplicationID(),e);  }catch (  ApplicationMasterNotRegisteredException e) {    LOG.info("ApplicationMaster is out of sync with ResourceManager," + " hence resync and send outstanding requests.");    this.lastResponseID=0;    register();  }catch (  Exception e) {    if (System.currentTimeMillis() - retrystartTime >= retryInterval) {
@SuppressWarnings("unchecked") @Override public void handle(ContainerAllocatorEvent event){  if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {
  if (clientService != null) {    serviceAddr=clientService.getBindAddress();  }  try {    RegisterApplicationMasterRequest request=recordFactory.newRecordInstance(RegisterApplicationMasterRequest.class);    if (serviceAddr != null) {      request.setHost(serviceAddr.getHostName());      request.setRpcPort(serviceAddr.getPort());      request.setTrackingUrl(MRWebAppUtil.getAMWebappScheme(getConfig()) + serviceAddr.getHostName() + ":"+ clientService.getHttpPort());    }    RegisterApplicationMasterResponse response=scheduler.registerApplicationMaster(request);    isApplicationMasterRegistered=true;    maxContainerCapability=response.getMaximumResourceCapability();    this.context.getClusterInfo().setMaxContainerCapability(maxContainerCapability);    if (UserGroupInformation.isSecurityEnabled()) {      setClientToAMToken(response.getClientToAMTokenMasterKey());    }    this.applicationACLs=response.getApplicationACLs();
  }  try {    RegisterApplicationMasterRequest request=recordFactory.newRecordInstance(RegisterApplicationMasterRequest.class);    if (serviceAddr != null) {      request.setHost(serviceAddr.getHostName());      request.setRpcPort(serviceAddr.getPort());      request.setTrackingUrl(MRWebAppUtil.getAMWebappScheme(getConfig()) + serviceAddr.getHostName() + ":"+ clientService.getHttpPort());    }    RegisterApplicationMasterResponse response=scheduler.registerApplicationMaster(request);    isApplicationMasterRegistered=true;    maxContainerCapability=response.getMaximumResourceCapability();    this.context.getClusterInfo().setMaxContainerCapability(maxContainerCapability);    if (UserGroupInformation.isSecurityEnabled()) {      setClientToAMToken(response.getClientToAMTokenMasterKey());    }    this.applicationACLs=response.getApplicationACLs();    LOG.info("maxContainerCapability: " + maxContainerCapability);    String queue=response.getQueue();
public void setShouldUnregister(boolean shouldUnregister){  this.shouldUnregister=shouldUnregister;
public void setSignalled(boolean isSignalled){  this.isSignalled=isSignalled;
@Override public void handle(ContainerAllocatorEvent event){  int qSize=eventQueue.size();  if (qSize != 0 && qSize % 1000 == 0) {
    } else {      handleReduceContainerRequest(reqEvent);    }  } else   if (event.getType() == ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {    LOG.info("Processing the event " + event.toString());    TaskAttemptId aId=event.getAttemptID();    boolean removed=scheduledRequests.remove(aId);    if (!removed) {      ContainerId containerId=assignedRequests.get(aId);      if (containerId != null) {        removed=true;        assignedRequests.remove(aId);        containersReleased++;        pendingRelease.add(containerId);        release(containerId);      }    }    if (!removed) {
@SuppressWarnings({"unchecked"}) private void handleReduceContainerRequest(ContainerRequestEvent reqEvent){  assert(reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.REDUCE));  Resource supportedMaxContainerCapability=getMaxContainerCapability();  JobId jobId=getJob().getID();  if (reduceResourceRequest.equals(Resources.none())) {    reduceResourceRequest=reqEvent.getCapability();    eventHandler.handle(new JobHistoryEvent(jobId,new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.REDUCE,reduceResourceRequest.getMemorySize())));
    reduceResourceRequest=reqEvent.getCapability();    eventHandler.handle(new JobHistoryEvent(jobId,new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.REDUCE,reduceResourceRequest.getMemorySize())));    LOG.info("reduceResourceRequest:" + reduceResourceRequest);  }  boolean reduceContainerRequestAccepted=true;  if (reduceResourceRequest.getMemorySize() > supportedMaxContainerCapability.getMemorySize() || reduceResourceRequest.getVirtualCores() > supportedMaxContainerCapability.getVirtualCores()) {    reduceContainerRequestAccepted=false;  }  if (reduceContainerRequestAccepted) {    reqEvent.getCapability().setVirtualCores(reduceResourceRequest.getVirtualCores());    reqEvent.getCapability().setMemorySize(reduceResourceRequest.getMemorySize());    if (reqEvent.getEarlierAttemptFailed()) {      pendingReduces.addFirst(new ContainerRequest(reqEvent,PRIORITY_REDUCE,reduceNodeLabelExpression));    } else {      pendingReduces.add(new ContainerRequest(reqEvent,PRIORITY_REDUCE,reduceNodeLabelExpression));    }  } else {    String diagMsg="REDUCE capability required is more than the " + "supported max container capability in the cluster. Killing" + " the Job. reduceResourceRequest: " + reduceResourceRequest + " maxContainerCapability:"+ supportedMaxContainerCapability;
@SuppressWarnings({"unchecked"}) private void handleMapContainerRequest(ContainerRequestEvent reqEvent){  assert(reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP));  Resource supportedMaxContainerCapability=getMaxContainerCapability();  JobId jobId=getJob().getID();  if (mapResourceRequest.equals(Resources.none())) {    mapResourceRequest=reqEvent.getCapability();    eventHandler.handle(new JobHistoryEvent(jobId,new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,mapResourceRequest.getMemorySize())));
  Resource supportedMaxContainerCapability=getMaxContainerCapability();  JobId jobId=getJob().getID();  if (mapResourceRequest.equals(Resources.none())) {    mapResourceRequest=reqEvent.getCapability();    eventHandler.handle(new JobHistoryEvent(jobId,new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,mapResourceRequest.getMemorySize())));    LOG.info("mapResourceRequest:" + mapResourceRequest);  }  boolean mapContainerRequestAccepted=true;  if (mapResourceRequest.getMemorySize() > supportedMaxContainerCapability.getMemorySize() || mapResourceRequest.getVirtualCores() > supportedMaxContainerCapability.getVirtualCores()) {    mapContainerRequestAccepted=false;  }  if (mapContainerRequestAccepted) {    reqEvent.getCapability().setMemorySize(mapResourceRequest.getMemorySize());    reqEvent.getCapability().setVirtualCores(mapResourceRequest.getVirtualCores());    scheduledRequests.addMap(reqEvent);  } else {    String diagMsg="The required MAP capability is more than the " + "supported max container capability in the cluster. Killing" + " the Job. mapResourceRequest: " + mapResourceRequest + " maxContainerCapability:"+ supportedMaxContainerCapability;
private void preemptReducer(int hangingMapRequests){  clearAllPendingReduceRequests();  int preemptionReduceNumForOneMap=ResourceCalculatorUtils.divideAndCeilContainers(mapResourceRequest,reduceResourceRequest,getSchedulerResourceTypes());  int preemptionReduceNumForPreemptionLimit=ResourceCalculatorUtils.divideAndCeilContainers(Resources.multiply(getResourceLimit(),maxReducePreemptionLimit),reduceResourceRequest,getSchedulerResourceTypes());  int preemptionReduceNumForAllMaps=ResourceCalculatorUtils.divideAndCeilContainers(Resources.multiply(mapResourceRequest,hangingMapRequests),reduceResourceRequest,getSchedulerResourceTypes());  int toPreempt=Math.min(Math.max(preemptionReduceNumForOneMap,preemptionReduceNumForPreemptionLimit),preemptionReduceNumForAllMaps);
  float completedMapPercent=0f;  if (totalMaps != 0) {    completedMapPercent=(float)completedMaps / totalMaps;  } else {    completedMapPercent=1;  }  Resource netScheduledMapResource=Resources.multiply(mapResourceReqt,(scheduledMaps + assignedMaps));  Resource netScheduledReduceResource=Resources.multiply(reduceResourceReqt,(scheduledReduces + assignedReduces));  Resource finalMapResourceLimit;  Resource finalReduceResourceLimit;  Resource totalResourceLimit=getResourceLimit();  Resource idealReduceResourceLimit=Resources.multiply(totalResourceLimit,Math.min(completedMapPercent,maxReduceRampupLimit));  Resource ideaMapResourceLimit=Resources.subtract(totalResourceLimit,idealReduceResourceLimit);  if (ResourceCalculatorUtils.computeAvailableContainers(ideaMapResourceLimit,mapResourceReqt,getSchedulerResourceTypes()) >= (scheduledMaps + assignedMaps)) {    Resource unusedMapResourceLimit=Resources.subtract(ideaMapResourceLimit,netScheduledMapResource);    finalReduceResourceLimit=Resources.add(idealReduceResourceLimit,unusedMapResourceLimit);
  AllocateResponse response;  try {    response=makeRemoteRequest();    retrystartTime=System.currentTimeMillis();  } catch (  ApplicationAttemptNotFoundException e) {    eventHandler.handle(new JobEvent(this.getJob().getID(),JobEventType.JOB_AM_REBOOT));    throw new RMContainerAllocationException("Resource Manager doesn't recognize AttemptId: " + this.getContext().getApplicationAttemptId(),e);  }catch (  ApplicationMasterNotRegisteredException e) {    LOG.info("ApplicationMaster is out of sync with ResourceManager," + " hence resync and send outstanding requests.");    lastResponseID=0;    register();    addOutstandingRequestOnResync();    return null;  }catch (  InvalidLabelResourceRequestException e) {    String diagMsg="Requested node-label-expression is invalid: " + StringUtils.stringifyException(e);
    if (System.currentTimeMillis() - retrystartTime >= retryInterval) {      LOG.error("Could not contact RM after " + retryInterval + " milliseconds.");      eventHandler.handle(new JobEvent(this.getJob().getID(),JobEventType.JOB_AM_REBOOT));      throw new RMContainerAllocationException("Could not contact RM after " + retryInterval + " milliseconds.");    }    throw e;  }  Resource newHeadRoom=getAvailableResources();  List<Container> newContainers=response.getAllocatedContainers();  if (response.getNMTokens() != null) {    for (    NMToken nmToken : response.getNMTokens()) {      NMTokenCache.setNMToken(nmToken.getNodeId().toString(),nmToken.getToken());    }  }  if (response.getAMRMToken() != null) {    updateAMRMToken(response.getAMRMToken());  }  List<ContainerStatus> finishedContainers=response.getCompletedContainersStatuses();  final PreemptionMessage preemptReq=response.getPreemptionMessage();  if (preemptReq != null) {
@SuppressWarnings("unchecked") @VisibleForTesting void processFinishedContainer(ContainerStatus container){
@SuppressWarnings("unchecked") @VisibleForTesting void processFinishedContainer(ContainerStatus container){  LOG.info("Received completed container " + container.getContainerId());  TaskAttemptId attemptID=assignedRequests.get(container.getContainerId());  if (attemptID == null) {
@SuppressWarnings("unchecked") private void handleUpdatedNodes(AllocateResponse response){  List<NodeReport> updatedNodes=response.getUpdatedNodes();  if (!updatedNodes.isEmpty()) {    eventHandler.handle(new JobUpdatedNodesEvent(getJob().getID(),updatedNodes));    HashSet<NodeId> unusableNodes=new HashSet<NodeId>();    for (    NodeReport nr : updatedNodes) {      NodeState nodeState=nr.getNodeState();      if (nodeState.isUnusable()) {        unusableNodes.add(nr.getNodeId());      }    }    for (int i=0; i < 2; ++i) {      HashMap<TaskAttemptId,Container> taskSet=i == 0 ? assignedRequests.maps : assignedRequests.reduces;      for (      Map.Entry<TaskAttemptId,Container> entry : taskSet.entrySet()) {        TaskAttemptId tid=entry.getKey();        NodeId taskAttemptNodeId=entry.getValue().getNodeId();        if (unusableNodes.contains(taskAttemptNodeId)) {
@Override protected void serviceInit(Configuration conf) throws Exception {  super.serviceInit(conf);  nodeBlacklistingEnabled=conf.getBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE,true);
@Override protected void serviceInit(Configuration conf) throws Exception {  super.serviceInit(conf);  nodeBlacklistingEnabled=conf.getBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE,true);  LOG.info("nodeBlacklistingEnabled:" + nodeBlacklistingEnabled);  maxTaskFailuresPerNode=conf.getInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER,3);  blacklistDisablePercent=conf.getInt(MRJobConfig.MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT,MRJobConfig.DEFAULT_MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERCENT);
protected AllocateResponse makeRemoteRequest() throws YarnException, IOException {  applyRequestLimits();  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<String>(blacklistAdditions),new ArrayList<String>(blacklistRemovals));  AllocateRequest allocateRequest=AllocateRequest.newInstance(lastResponseID,super.getApplicationProgress(),new ArrayList<ResourceRequest>(ask),new ArrayList<ContainerId>(release),blacklistRequest);  AllocateResponse allocateResponse=scheduler.allocate(allocateRequest);  lastResponseID=allocateResponse.getResponseId();  availableResources=allocateResponse.getAvailableResources();  lastClusterNmCount=clusterNmCount;  clusterNmCount=allocateResponse.getNumClusterNodes();  int numCompletedContainers=allocateResponse.getCompletedContainersStatuses().size();  if (ask.size() > 0 || release.size() > 0) {
  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<String>(blacklistAdditions),new ArrayList<String>(blacklistRemovals));  AllocateRequest allocateRequest=AllocateRequest.newInstance(lastResponseID,super.getApplicationProgress(),new ArrayList<ResourceRequest>(ask),new ArrayList<ContainerId>(release),blacklistRequest);  AllocateResponse allocateResponse=scheduler.allocate(allocateRequest);  lastResponseID=allocateResponse.getResponseId();  availableResources=allocateResponse.getAvailableResources();  lastClusterNmCount=clusterNmCount;  clusterNmCount=allocateResponse.getNumClusterNodes();  int numCompletedContainers=allocateResponse.getCompletedContainersStatuses().size();  if (ask.size() > 0 || release.size() > 0) {    LOG.info("getResources() for " + applicationId + ":"+ " ask="+ ask.size()+ " release= "+ release.size()+ " newContainers="+ allocateResponse.getAllocatedContainers().size()+ " finishedContainers="+ numCompletedContainers+ " resourcelimit="+ availableResources+ " knownNMs="+ clusterNmCount);  }  ask.clear();  release.clear();  if (numCompletedContainers > 0) {    requestLimitsToUpdate.addAll(requestLimits.keySet());  }  if (blacklistAdditions.size() > 0 || blacklistRemovals.size() > 0) {
  if (!nodeBlacklistingEnabled) {    return;  }  if (blacklistDisablePercent != -1 && (blacklistedNodeCount != blacklistedNodes.size() || clusterNmCount != lastClusterNmCount)) {    blacklistedNodeCount=blacklistedNodes.size();    if (clusterNmCount == 0) {      LOG.info("KnownNode Count at 0. Not computing ignoreBlacklisting");      return;    }    int val=(int)((float)blacklistedNodes.size() / clusterNmCount * 100);    if (val >= blacklistDisablePercent) {      if (ignoreBlacklisting.compareAndSet(false,true)) {        LOG.info("Ignore blacklisting set to true. Known: " + clusterNmCount + ", Blacklisted: "+ blacklistedNodeCount+ ", "+ val+ "%");        blacklistAdditions.clear();        blacklistRemovals.addAll(blacklistedNodes);      }    } else {      if (ignoreBlacklisting.compareAndSet(true,false)) {
private void addResourceRequest(Priority priority,String resourceName,Resource capability,String nodeLabelExpression,ExecutionType executionType){  Map<String,Map<Resource,ResourceRequest>> remoteRequests=this.remoteRequestsTable.get(priority);  if (remoteRequests == null) {    remoteRequests=new HashMap<String,Map<Resource,ResourceRequest>>();    this.remoteRequestsTable.put(priority,remoteRequests);    if (LOG.isDebugEnabled()) {
  Map<Resource,ResourceRequest> reqMap=remoteRequests.get(resourceName);  if (reqMap == null) {    reqMap=new HashMap<Resource,ResourceRequest>();    remoteRequests.put(resourceName,reqMap);  }  ResourceRequest remoteRequest=reqMap.get(capability);  if (remoteRequest == null) {    remoteRequest=recordFactory.newRecordInstance(ResourceRequest.class);    remoteRequest.setPriority(priority);    remoteRequest.setResourceName(resourceName);    remoteRequest.setCapability(capability);    remoteRequest.setNumContainers(0);    remoteRequest.setNodeLabelExpression(nodeLabelExpression);    remoteRequest.setExecutionTypeRequest(ExecutionTypeRequest.newInstance(executionType,true));    reqMap.put(capability,remoteRequest);  }  remoteRequest.setNumContainers(remoteRequest.getNumContainers() + 1);
private void decResourceRequest(Priority priority,String resourceName,Resource capability){  Map<String,Map<Resource,ResourceRequest>> remoteRequests=this.remoteRequestsTable.get(priority);  Map<Resource,ResourceRequest> reqMap=remoteRequests.get(resourceName);  if (reqMap == null) {    if (LOG.isDebugEnabled()) {
      LOG.debug("Not decrementing resource as " + resourceName + " is not present in request table");    }    return;  }  ResourceRequest remoteRequest=reqMap.get(capability);  if (LOG.isDebugEnabled()) {    LOG.debug("BEFORE decResourceRequest:" + " applicationId=" + applicationId.getId() + " priority="+ priority.getPriority()+ " resourceName="+ resourceName+ " numContainers="+ remoteRequest.getNumContainers()+ " #asks="+ ask.size());  }  if (remoteRequest.getNumContainers() > 0) {    remoteRequest.setNumContainers(remoteRequest.getNumContainers() - 1);  }  if (remoteRequest.getNumContainers() == 0) {    reqMap.remove(capability);    if (reqMap.size() == 0) {      remoteRequests.remove(resourceName);    }    if (remoteRequests.size() == 0) {      remoteRequestsTable.remove(priority);    }  }  addResourceRequestToAsk(remoteRequest);  if (LOG.isDebugEnabled()) {
@Override public void preempt(Context ctxt,PreemptionMessage preemptionRequests){  if (preemptionRequests != null) {    StrictPreemptionContract cStrict=preemptionRequests.getStrictContract();    if (cStrict != null && cStrict.getContainers() != null && cStrict.getContainers().size() > 0) {      LOG.info("strict preemption :" + preemptionRequests.getStrictContract().getContainers().size() + " containers to kill");      for (      PreemptionContainer c : preemptionRequests.getStrictContract().getContainers()) {        ContainerId reqCont=c.getId();        TaskAttemptId reqTask=ctxt.getTaskAttempt(reqCont);        if (reqTask != null) {          if (org.apache.hadoop.mapreduce.v2.api.records.TaskType.REDUCE.equals(reqTask.getTaskId().getTaskType())) {            toBePreempted.add(reqTask);
 else {            LOG.info("NOT preempting " + reqCont + " running task:"+ reqTask);          }        }      }    }    PreemptionContract cNegot=preemptionRequests.getContract();    if (cNegot != null && cNegot.getResourceRequest() != null && cNegot.getResourceRequest().size() > 0 && cNegot.getContainers() != null && cNegot.getContainers().size() > 0) {      LOG.info("negotiable preemption :" + preemptionRequests.getContract().getResourceRequest().size() + " resourceReq, "+ preemptionRequests.getContract().getContainers().size()+ " containers");      List<PreemptionResourceRequest> reqResources=preemptionRequests.getContract().getResourceRequest();      int pendingPreemptionRam=0;      int pendingPreemptionCores=0;      for (      Resource r : pendingFlexiblePreemptions.values()) {        pendingPreemptionRam+=r.getMemorySize();        pendingPreemptionCores+=r.getVirtualCores();      }      for (      PreemptionResourceRequest rr : reqResources) {        ResourceRequest reqRsrc=rr.getResourceRequest();        if (!ResourceRequest.ANY.equals(reqRsrc.getResourceName())) {          continue;
          totalMemoryToRelease-=pendingPreemptionRam;          pendingPreemptionRam-=totalMemoryToRelease;        }        if (pendingPreemptionCores > 0) {          totalCoresToRelease-=pendingPreemptionCores;          pendingPreemptionCores-=totalCoresToRelease;        }        List<Container> listOfCont=ctxt.getContainers(TaskType.REDUCE);        Collections.sort(listOfCont,new Comparator<Container>(){          @Override public int compare(          final Container o1,          final Container o2){            return o2.getId().compareTo(o1.getId());          }        });        for (        Container cont : listOfCont) {          if (totalMemoryToRelease <= 0 && totalCoresToRelease <= 0) {            break;          }          TaskAttemptId reduceId=ctxt.getTaskAttempt(cont.getId());          int cMem=(int)cont.getResource().getMemorySize();
@Override public void handleCompletedContainer(TaskAttemptId attemptID){
@SuppressWarnings("unchecked") private void killContainer(Context ctxt,PreemptionContainer c){  ContainerId reqCont=c.getId();  TaskAttemptId reqTask=ctxt.getTaskAttempt(reqCont);
  try {    Class<? extends TaskRuntimeEstimator> estimatorClass=conf.getClass(MRJobConfig.MR_AM_TASK_ESTIMATOR,LegacyTaskRuntimeEstimator.class,TaskRuntimeEstimator.class);    Constructor<? extends TaskRuntimeEstimator> estimatorConstructor=estimatorClass.getConstructor();    estimator=estimatorConstructor.newInstance();    estimator.contextualize(conf,context);  } catch (  InstantiationException ex) {    LOG.error("Can't make a speculation runtime estimator",ex);    throw new YarnRuntimeException(ex);  }catch (  IllegalAccessException ex) {    LOG.error("Can't make a speculation runtime estimator",ex);    throw new YarnRuntimeException(ex);  }catch (  InvocationTargetException ex) {    LOG.error("Can't make a speculation runtime estimator",ex);    throw new YarnRuntimeException(ex);  }catch (  NoSuchMethodException ex) {
@Override protected void serviceStart() throws Exception {  Runnable speculationBackgroundCore=new Runnable(){    @Override public void run(){      while (!stopped && !Thread.currentThread().isInterrupted()) {        long backgroundRunStartTime=clock.getTime();        try {          int speculations=computeSpeculations();          long mininumRecomp=speculations > 0 ? soonestRetryAfterSpeculate : soonestRetryAfterNoSpeculate;          long wait=Math.max(mininumRecomp,clock.getTime() - backgroundRunStartTime);          if (speculations > 0) {
    requireJob();  } catch (  Exception e) {    renderText(e.getMessage());    return;  }  if (app.getJob() != null) {    try {      String taskType=$(TASK_TYPE);      if (taskType.isEmpty()) {        throw new RuntimeException("missing task-type.");      }      String attemptState=$(ATTEMPT_STATE);      if (attemptState.isEmpty()) {        throw new RuntimeException("missing attempt-state.");      }      setTitle(join(attemptState," ",MRApps.taskType(taskType).toString()," attempts in ",$(JOB_ID)));      render(attemptsPage());    } catch (    Exception e) {
  Path confPath=job.getConfFile();  try {    ConfInfo info=new ConfInfo(job);    html.div().a("/jobhistory/downloadconf/" + jid,confPath.toString()).__();    TBODY<TABLE<Hamlet>> tbody=html.table("#conf").thead().tr().th(_TH,"key").th(_TH,"value").th(_TH,"source chain").__().__().tbody();    for (    ConfEntryInfo entry : info.getProperties()) {      StringBuffer buffer=new StringBuffer();      String[] sources=entry.getSource();      boolean first=true;      for (int i=(sources.length - 2); i >= 0; i--) {        if (!first) {          buffer.append(" <- ");        }        first=false;        buffer.append(sources[i]);      }      tbody.tr().td(entry.getName()).td(entry.getValue()).td(buffer.toString()).__();
@SuppressWarnings("rawtypes") @Test(timeout=10000) public void testKillJob() throws Exception {  JobConf conf=new JobConf();  AppContext context=mock(AppContext.class);  final CountDownLatch isDone=new CountDownLatch(1);  EventHandler<Event> handler=new EventHandler<Event>(){    @Override public void handle(    Event event){
@Test public void testEscapeJobSummary(){  summary.setJobName("aa\rbb\ncc\r\ndd");  String out=summary.getJobSummaryString();
public void verifyCompleted(){  for (  Job job : getContext().getAllJobs().values()) {    JobReport jobReport=job.getReport();
public void verifyCompleted(){  for (  Job job : getContext().getAllJobs().values()) {    JobReport jobReport=job.getReport();    LOG.info("Job start time :{}",jobReport.getStartTime());
public void verifyCompleted(){  for (  Job job : getContext().getAllJobs().values()) {    JobReport jobReport=job.getReport();    LOG.info("Job start time :{}",jobReport.getStartTime());    LOG.info("Job finish time :",jobReport.getFinishTime());    Assert.assertTrue("Job start time is not less than finish time",jobReport.getStartTime() <= jobReport.getFinishTime());    Assert.assertTrue("Job finish time is in future",jobReport.getFinishTime() <= System.currentTimeMillis());    for (    Task task : job.getTasks().values()) {      TaskReport taskReport=task.getReport();
public void verifyCompleted(){  for (  Job job : getContext().getAllJobs().values()) {    JobReport jobReport=job.getReport();    LOG.info("Job start time :{}",jobReport.getStartTime());    LOG.info("Job finish time :",jobReport.getFinishTime());    Assert.assertTrue("Job start time is not less than finish time",jobReport.getStartTime() <= jobReport.getFinishTime());    Assert.assertTrue("Job finish time is in future",jobReport.getFinishTime() <= System.currentTimeMillis());    for (    Task task : job.getTasks().values()) {      TaskReport taskReport=task.getReport();      LOG.info("Task {} start time : {}",task.getID(),taskReport.getStartTime());
  String userName="TestAppMasterUser";  JobConf conf=new JobConf();  conf.set(MRJobConfig.MR_AM_STAGING_DIR,stagingDir);  conf.setInt(org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,1);  ApplicationAttemptId applicationAttemptId=ApplicationAttemptId.fromString(applicationAttemptIdStr);  JobId jobId=TypeConverter.toYarn(TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));  Path start=MRApps.getStartJobCommitFile(conf,userName,jobId);  FileSystem fs=FileSystem.get(conf);  fs.create(start).close();  ContainerId containerId=ContainerId.fromString(containerIdStr);  MRAppMaster appMaster=new MRAppMasterTest(applicationAttemptId,containerId,"host",-1,-1,System.currentTimeMillis(),false,false);  boolean caught=false;  try {    MRAppMaster.initAndStartAppMaster(appMaster,conf,userName);  } catch (  IOException e) {
  JobConf conf=new JobConf();  conf.set(MRJobConfig.MR_AM_STAGING_DIR,stagingDir);  ApplicationAttemptId applicationAttemptId=ApplicationAttemptId.fromString(applicationAttemptIdStr);  JobId jobId=TypeConverter.toYarn(TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));  Path start=MRApps.getStartJobCommitFile(conf,userName,jobId);  Path end=MRApps.getEndJobCommitSuccessFile(conf,userName,jobId);  FileSystem fs=FileSystem.get(conf);  fs.create(start).close();  fs.create(end).close();  ContainerId containerId=ContainerId.fromString(containerIdStr);  MRAppMaster appMaster=new MRAppMasterTest(applicationAttemptId,containerId,"host",-1,-1,System.currentTimeMillis(),false,false);  boolean caught=false;  try {    MRAppMaster.initAndStartAppMaster(appMaster,conf,userName);  } catch (  IOException e) {
  JobConf conf=new JobConf();  conf.set(MRJobConfig.MR_AM_STAGING_DIR,stagingDir);  ApplicationAttemptId applicationAttemptId=ApplicationAttemptId.fromString(applicationAttemptIdStr);  JobId jobId=TypeConverter.toYarn(TypeConverter.fromYarn(applicationAttemptId.getApplicationId()));  Path start=MRApps.getStartJobCommitFile(conf,userName,jobId);  Path end=MRApps.getEndJobCommitFailureFile(conf,userName,jobId);  FileSystem fs=FileSystem.get(conf);  fs.create(start).close();  fs.create(end).close();  ContainerId containerId=ContainerId.fromString(containerIdStr);  MRAppMaster appMaster=new MRAppMasterTest(applicationAttemptId,containerId,"host",-1,-1,System.currentTimeMillis(),false,false);  boolean caught=false;  try {    MRAppMaster.initAndStartAppMaster(appMaster,conf,userName);  } catch (  IOException e) {
  String applicationAttemptIdStr="appattempt_1317529182569_0004_000002";  String containerIdStr="container_1317529182569_0004_000002_1";  String userName="TestAppMasterUser";  JobConf conf=new JobConf();  conf.set(MRJobConfig.MR_AM_STAGING_DIR,stagingDir);  ApplicationAttemptId applicationAttemptId=ApplicationAttemptId.fromString(applicationAttemptIdStr);  File dir=new File(stagingDir);  if (dir.exists()) {    FileUtils.deleteDirectory(dir);  }  ContainerId containerId=ContainerId.fromString(containerIdStr);  MRAppMaster appMaster=new MRAppMasterTest(applicationAttemptId,containerId,"host",-1,-1,System.currentTimeMillis(),false,false);  boolean caught=false;  try {    MRAppMaster.initAndStartAppMaster(appMaster,conf,userName);  } catch (  IOException e) {
  Task mapTask2=it.next();  Task reduceTask=it.next();  app.waitForState(mapTask1,TaskState.RUNNING);  app.waitForState(mapTask2,TaskState.RUNNING);  app.getContext().getEventHandler().handle(new TaskEvent(mapTask1.getID(),TaskEventType.T_ADD_SPEC_ATTEMPT));  int timeOut=0;  while (mapTask1.getAttempts().size() != 2 && timeOut++ < 10) {    Thread.sleep(1000);    LOG.info("Waiting for next attempt to start");  }  Iterator<TaskAttempt> t1it=mapTask1.getAttempts().values().iterator();  TaskAttempt task1Attempt1=t1it.next();  TaskAttempt task1Attempt2=t1it.next();  TaskAttempt task2Attempt=mapTask2.getAttempts().values().iterator().next();  waitForContainerAssignment(task1Attempt2);  ContainerId t1a2contId=task1Attempt2.getAssignedContainerID();
  assertThat(containerLauncher.initialPoolSize).isEqualTo(MRJobConfig.DEFAULT_MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE);  Assert.assertEquals(0,threadPool.getPoolSize());  Assert.assertEquals(containerLauncher.initialPoolSize,threadPool.getCorePoolSize());  Assert.assertNull(containerLauncher.foundErrors);  containerLauncher.expectedCorePoolSize=containerLauncher.initialPoolSize;  for (int i=0; i < 10; i++) {    ContainerId containerId=ContainerId.newContainerId(appAttemptId,i);    TaskAttemptId taskAttemptId=MRBuilderUtils.newTaskAttemptId(taskId,i);    containerLauncher.handle(new ContainerLauncherEvent(taskAttemptId,containerId,"host" + i + ":1234",null,ContainerLauncher.EventType.CONTAINER_REMOTE_LAUNCH));  }  waitForEvents(containerLauncher,10);  Assert.assertEquals(10,threadPool.getPoolSize());  Assert.assertNull(containerLauncher.foundErrors);  containerLauncher.finishEventHandling=true;  int timeOut=0;  while (containerLauncher.numEventsProcessed.get() < 10 && timeOut++ < 200) {
private void waitForEvents(CustomContainerLauncher containerLauncher,int expectedNumEvents) throws InterruptedException {  int timeOut=0;  while (containerLauncher.numEventsProcessing.get() < expectedNumEvents && timeOut++ < 20) {
  server.start();  MRApp app=new MRAppWithSlowNM(tokenSecretManager);  try {    Job job=app.submit(conf);    app.waitForState(job,JobState.RUNNING);    Map<TaskId,Task> tasks=job.getTasks();    Assert.assertEquals("Num tasks is not correct",1,tasks.size());    Task task=tasks.values().iterator().next();    app.waitForState(task,TaskState.SCHEDULED);    Map<TaskAttemptId,TaskAttempt> attempts=tasks.values().iterator().next().getAttempts();    Assert.assertEquals("Num attempts is not correct",maxAttempts,attempts.size());    TaskAttempt attempt=attempts.values().iterator().next();    app.waitForInternalState((TaskAttemptImpl)attempt,TaskAttemptStateInternal.ASSIGNED);    app.waitForState(job,JobState.FAILED);    String diagnostics=attempt.getDiagnostics().toString();
  assertBlacklistAdditionsAndRemovals(0,0,rm);  Assert.assertEquals("No of assignments must be 0",0,assigned.size());  LOG.info("RM Heartbeat (To process the re-scheduled containers)");  assigned=allocator.schedule();  rm.drainEvents();  assertBlacklistAdditionsAndRemovals(0,0,rm);  Assert.assertEquals("No of assignments must be 0",0,assigned.size());  LOG.info("h3 Heartbeat (To re-schedule the containers)");  nodeManager3.nodeHeartbeat(true);  rm.drainEvents();  LOG.info("RM Heartbeat (To process the re-scheduled containers for H3)");  assigned=allocator.schedule();  assertBlacklistAdditionsAndRemovals(0,0,rm);  rm.drainEvents();  for (  TaskAttemptContainerAssignedEvent assig : assigned) {
      } catch (      InterruptedException e) {        throw new IOException(e);      }catch (      ExecutionException e) {        throw new IOException(e);      }      String pathString=path.toUri().toString();      String link=entry.getKey();      String target=new File(path.toUri()).getPath();      symlink(workDir,target,link);      if (resource.getType() == LocalResourceType.ARCHIVE) {        localArchives.add(pathString);      } else       if (resource.getType() == LocalResourceType.FILE) {        localFiles.add(pathString);      } else       if (resource.getType() == LocalResourceType.PATTERN) {        throw new IllegalArgumentException("Resource type PATTERN is not " + "implemented yet. " + resource.getResource());      }      Path resourcePath;
private void symlink(File workDir,String target,String link) throws IOException {  if (link != null) {    link=workDir.toString() + Path.SEPARATOR + link;    File flink=new File(link);    if (!flink.exists()) {
protected MRClientProtocol instantiateHistoryProxy(final Configuration conf,final InetSocketAddress hsAddress) throws IOException {  if (LOG.isDebugEnabled()) {
public static void setClassLoader(ClassLoader classLoader,Configuration conf){  if (classLoader != null) {
static String readOutput(Path outDir,Configuration conf) throws IOException {  FileSystem fs=outDir.getFileSystem(conf);  StringBuffer result=new StringBuffer();  Path[] fileList=FileUtil.stat2Paths(fs.listStatus(outDir,new Utils.OutputFileUtils.OutputFilesFilter()));  for (  Path outputFile : fileList) {
    assert(readSegmentIndex != 0);    assert(currentKVOffset != 0);    readSegmentIndex--;  }  int i=0;  Iterator<Segment<K,V>> itr=segmentList.iterator();  while (itr.hasNext()) {    Segment<K,V> s=itr.next();    if (i == readSegmentIndex) {      break;    }    s.close();    itr.remove();    i++;    LOG.debug("Dropping a segment");  }  firstSegmentOffset=currentKVOffset;  readSegmentIndex=0;
    }  }  inReset=true;  for (int i=0; i < segmentList.size(); i++) {    Segment<K,V> s=segmentList.get(i);    if (s.inMemory()) {      int offset=(i == 0) ? firstSegmentOffset : 0;      s.getReader().reset(offset);    } else {      s.closeReader();      if (i == 0) {        s.reinitReader(firstSegmentOffset);        s.getReader().disableChecksumValidation();      }    }  }  currentKVOffset=firstSegmentOffset;  nextKVOffset=-1;  readSegmentIndex=0;  hasMore=false;
protected static boolean deletePath(PathDeletionContext context) throws IOException {  context.enablePathForCleanup();  if (LOG.isDebugEnabled()) {
public void log(Logger log){  log.info("Counters: " + size());  for (  Group group : this) {
public void log(Logger log){  log.info("Counters: " + size());  for (  Group group : this) {    log.info("  " + group.getDisplayName());    for (    Counter counter : group) {
  int numThreads=job.getInt(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS,org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);  StopWatch sw=new StopWatch().start();  if (numThreads == 1) {    List<FileStatus> locatedFiles=singleThreadedListStatus(job,dirs,inputFilter,recursive);    result=locatedFiles.toArray(new FileStatus[locatedFiles.size()]);  } else {    Iterable<FileStatus> locatedFiles=null;    try {      LocatedFileStatusFetcher locatedFileStatusFetcher=new LocatedFileStatusFetcher(job,dirs,recursive,inputFilter,false);      locatedFiles=locatedFileStatusFetcher.getFileStatuses();    } catch (    InterruptedException e) {      throw (IOException)new InterruptedIOException("Interrupted while getting file statuses").initCause(e);    }    result=Iterables.toArray(locatedFiles,FileStatus.class);  }  sw.stop();  if (LOG.isDebugEnabled()) {
      BlockLocation[] blkLocations;      if (file instanceof LocatedFileStatus) {        blkLocations=((LocatedFileStatus)file).getBlockLocations();      } else {        blkLocations=fs.getFileBlockLocations(file,0,length);      }      if (isSplitable(fs,path)) {        long blockSize=file.getBlockSize();        long splitSize=computeSplitSize(goalSize,minSize,blockSize);        long bytesRemaining=length;        while (((double)bytesRemaining) / splitSize > SPLIT_SLOP) {          String[][] splitHosts=getSplitHostsAndCachedHosts(blkLocations,length - bytesRemaining,splitSize,clusterMap);          splits.add(makeSplit(path,length - bytesRemaining,splitSize,splitHosts[0],splitHosts[1]));          bytesRemaining-=splitSize;        }        if (bytesRemaining != 0) {          String[][] splitHosts=getSplitHostsAndCachedHosts(blkLocations,length - bytesRemaining,bytesRemaining,clusterMap);
      if (isSplitable(fs,path)) {        long blockSize=file.getBlockSize();        long splitSize=computeSplitSize(goalSize,minSize,blockSize);        long bytesRemaining=length;        while (((double)bytesRemaining) / splitSize > SPLIT_SLOP) {          String[][] splitHosts=getSplitHostsAndCachedHosts(blkLocations,length - bytesRemaining,splitSize,clusterMap);          splits.add(makeSplit(path,length - bytesRemaining,splitSize,splitHosts[0],splitHosts[1]));          bytesRemaining-=splitSize;        }        if (bytesRemaining != 0) {          String[][] splitHosts=getSplitHostsAndCachedHosts(blkLocations,length - bytesRemaining,bytesRemaining,clusterMap);          splits.add(makeSplit(path,length - bytesRemaining,bytesRemaining,splitHosts[0],splitHosts[1]));        }      } else {        if (LOG.isDebugEnabled()) {          if (length > Math.min(file.getBlockSize(),minSize)) {            LOG.debug("File is not splittable so no parallelization " + "is possible: " + file.getPath());
public boolean checkAccess(UserGroupInformation callerUGI,JobACL jobOperation,String jobOwner,AccessControlList jobACL){  if (LOG.isDebugEnabled()) {
private int getMemoryRequiredHelper(String configName,int defaultValue,int heapSize,float heapRatio){  int memory=getInt(configName,-1);  if (memory <= 0) {    if (heapSize > 0) {      memory=(int)Math.ceil(heapSize / heapRatio);
  if (notification != null) {    do {      try {        int code=httpNotification(notification.getUri(),notification.getTimeout());        if (code != 200) {          throw new IOException("Invalid response status code: " + code);        } else {          break;        }      } catch (      IOException ioex) {        LOG.error("Notification error [" + notification.getUri() + "]",ioex);      }catch (      Exception ex) {        LOG.error("Notification error [" + notification.getUri() + "]",ex);      }      try {        Thread.sleep(notification.getRetryInterval());      } catch (      InterruptedException iex) {
public Iterable<FileStatus> getFileStatuses() throws InterruptedException, IOException {  runningTasks.incrementAndGet();  for (  Path p : inputDirs) {
private void registerError(Throwable t){
@SuppressWarnings("unchecked") private <INKEY,INVALUE,OUTKEY,OUTVALUE>void runOldMapper(final JobConf job,final TaskSplitIndex splitIndex,final TaskUmbilicalProtocol umbilical,TaskReporter reporter) throws IOException, InterruptedException, ClassNotFoundException {  InputSplit inputSplit=getSplitDetails(new Path(splitIndex.getSplitLocation()),splitIndex.getStartOffset());  updateJobWithSplit(job,inputSplit);  reporter.setInputSplit(inputSplit);  RecordReader<INKEY,INVALUE> in=isSkipping() ? new SkippingRecordReader<INKEY,INVALUE>(umbilical,reporter,job) : new TrackedRecordReader<INKEY,INVALUE>(reporter,job);  job.setBoolean(JobContext.SKIP_RECORDS,isSkipping());  int numReduceTasks=conf.getNumReduceTasks();
@SuppressWarnings("unchecked") private <INKEY,INVALUE,OUTKEY,OUTVALUE>void runNewMapper(final JobConf job,final TaskSplitIndex splitIndex,final TaskUmbilicalProtocol umbilical,TaskReporter reporter) throws IOException, ClassNotFoundException, InterruptedException {  org.apache.hadoop.mapreduce.TaskAttemptContext taskContext=new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,getTaskID(),reporter);  org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper=(org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)ReflectionUtils.newInstance(taskContext.getMapperClass(),job);  org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat=(org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)ReflectionUtils.newInstance(taskContext.getInputFormatClass(),job);  org.apache.hadoop.mapreduce.InputSplit split=null;  split=getSplitDetails(new Path(splitIndex.getSplitLocation()),splitIndex.getStartOffset());
JobQueueInfo getJobQueueInfo(){  JobQueueInfo queueInfo=new JobQueueInfo();  queueInfo.setQueueName(name);
    return false;  }  if (!(name.equals(newState.getName()))) {    LOG.info(" current name " + name + " not equal to "+ newState.getName());    return false;  }  if (children == null || children.size() == 0) {    if (newState.getChildren() != null && newState.getChildren().size() > 0) {      LOG.info(newState + " has added children in refresh ");      return false;    }  } else   if (children.size() > 0) {    if (newState.getChildren() == null) {      LOG.error("In the current state, queue " + getName() + " has "+ children.size()+ " but the new state has none!");      return false;    }    int childrenSize=children.size();    int newChildrenSize=newState.getChildren().size();    if (childrenSize != newChildrenSize) {
      LOG.info(newState + " has added children in refresh ");      return false;    }  } else   if (children.size() > 0) {    if (newState.getChildren() == null) {      LOG.error("In the current state, queue " + getName() + " has "+ children.size()+ " but the new state has none!");      return false;    }    int childrenSize=children.size();    int newChildrenSize=newState.getChildren().size();    if (childrenSize != newChildrenSize) {      LOG.error("Number of children for queue " + newState.getName() + " in newState is "+ newChildrenSize+ " which is not equal to "+ childrenSize+ " in the current state.");      return false;    }    Iterator<Queue> itr1=children.iterator();    Iterator<Queue> itr2=newState.getChildren().iterator();    while (itr1.hasNext()) {      Queue q=itr1.next();
private void initialize(QueueConfigurationParser cp){  this.root=cp.getRoot();  leafQueues.clear();  allQueues.clear();  leafQueues=getRoot().getLeafQueues();  allQueues.putAll(getRoot().getInnerQueues());  allQueues.putAll(leafQueues);
public synchronized boolean hasAccess(String queueName,QueueACL qACL,UserGroupInformation ugi){  Queue q=leafQueues.get(queueName);  if (q == null) {
  if (jobCleanup) {    runJobCleanupTask(umbilical,reporter);    return;  }  if (jobSetup) {    runJobSetupTask(umbilical,reporter);    return;  }  if (taskCleanup) {    runTaskCleanupTask(umbilical,reporter);    return;  }  codec=initCodec();  RawKeyValueIterator rIter=null;  ShuffleConsumerPlugin shuffleConsumerPlugin=null;  Class combinerClass=conf.getCombinerClass();  CombineOutputCollector combineCollector=(null != combinerClass) ? new CombineOutputCollector(reduceCombineOutputCounter,reporter,conf) : null;  Class<? extends ShuffleConsumerPlugin> clazz=job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN,Shuffle.class,ShuffleConsumerPlugin.class);
    return;  }  long startIndex=range.getStartIndex();  long endIndex=range.getEndIndex();  SortedSet<Range> headSet=ranges.headSet(range);  if (headSet.size() > 0) {    Range previousRange=headSet.last();    LOG.debug("previousRange " + previousRange);    if (startIndex < previousRange.getEndIndex()) {      if (ranges.remove(previousRange)) {        indicesCount-=previousRange.getLength();      }      startIndex=previousRange.getStartIndex();      endIndex=endIndex >= previousRange.getEndIndex() ? endIndex : previousRange.getEndIndex();    }  }  Iterator<Range> tailSetIt=ranges.tailSet(range).iterator();  while (tailSetIt.hasNext()) {    Range nextRange=tailSetIt.next();
  long startIndex=range.getStartIndex();  long endIndex=range.getEndIndex();  SortedSet<Range> headSet=ranges.headSet(range);  if (headSet.size() > 0) {    Range previousRange=headSet.last();    LOG.debug("previousRange " + previousRange);    if (startIndex < previousRange.getEndIndex()) {      if (ranges.remove(previousRange)) {        indicesCount-=previousRange.getLength();        LOG.debug("removed previousRange " + previousRange);      }      add(previousRange.getStartIndex(),startIndex);      if (endIndex <= previousRange.getEndIndex()) {        add(endIndex,previousRange.getEndIndex());      }    }  }  Iterator<Range> tailSetIt=ranges.tailSet(range).iterator();  while (tailSetIt.hasNext()) {
private void add(long start,long end){  if (end > start) {    Range recRange=new Range(start,end - start);    ranges.add(recRange);    indicesCount+=recRange.getLength();
protected void reportFatalError(TaskAttemptID id,Throwable throwable,String logMsg,boolean fastFail){
protected void reportNextRecordRange(final TaskUmbilicalProtocol umbilical,long nextRecIndex) throws IOException {  long len=nextRecIndex - currentRecStartIndex + 1;  SortedRanges.Range range=new SortedRanges.Range(currentRecStartIndex,len);  taskStatus.setNextRecordRange(range);  if (LOG.isDebugEnabled()) {
    setState(TaskStatus.State.COMMIT_PENDING);    while (true) {      try {        umbilical.commitPending(taskId,taskStatus);        break;      } catch (      InterruptedException ie) {      }catch (      IOException ie) {        LOG.warn("Failure sending commit pending: " + StringUtils.stringifyException(ie));        if (--retries == 0) {          System.exit(67);        }      }    }    commit(umbilical,reporter,committer);  }  taskDone.set(true);  reporter.stopCommunicationThread();  updateCounters();  sendLastUpdate(umbilical);
  int retries=MAX_RETRIES;  while (true) {    try {      while (!umbilical.canCommit(taskId)) {        try {          Thread.sleep(1000);        } catch (        InterruptedException ie) {        }        reporter.setProgressFlag();      }      break;    } catch (    IOException ie) {      LOG.warn("Failure asking whether task can commit: " + StringUtils.stringifyException(ie));      if (--retries == 0) {        discardOutput(taskContext);        System.exit(68);      }    }  }  try {
protected void runJobCleanupTask(TaskUmbilicalProtocol umbilical,TaskReporter reporter) throws IOException, InterruptedException {  setPhase(TaskStatus.Phase.CLEANUP);  getProgress().setStatus("cleanup");  statusUpdate(umbilical);  LOG.info("Cleaning up job");  if (jobRunStateForCleanup == JobStatus.State.FAILED || jobRunStateForCleanup == JobStatus.State.KILLED) {
public void setDiagnosticInfo(String info){  if (diagnosticInfo != null && diagnosticInfo.length() == getMaxStringSize()) {
@SuppressWarnings("unchecked") public void configure(JobConf jobConf){  int numberOfThreads=jobConf.getInt(MultithreadedMapper.NUM_THREADS,10);  if (LOG.isDebugEnabled()) {
public void run(RecordReader<K1,V1> input,OutputCollector<K2,V2> output,Reporter reporter) throws IOException {  try {    K1 key=input.createKey();    V1 value=input.createValue();    while (input.next(key,value)) {      executorService.execute(new MapperInvokeRunable(key,value,output,reporter));      checkForExceptionsFromProcessingThreads();      key=input.createKey();      value=input.createValue();    }    if (LOG.isDebugEnabled()) {      LOG.debug("Finished dispatching all Mappper.map calls, job " + job.getJobName());    }    executorService.shutdown();    try {      while (!executorService.awaitTermination(100,TimeUnit.MILLISECONDS)) {        if (LOG.isDebugEnabled()) {
void abort(Throwable t) throws IOException {
public void authenticate(String digest,String challenge) throws IOException {
private void initialize(InetSocketAddress jobTrackAddr,Configuration conf) throws IOException {  initProviderList();  final IOException initEx=new IOException("Cannot initialize Cluster. Please check your configuration for " + MRConfig.FRAMEWORK_NAME + " and the correspond server addresses.");  if (jobTrackAddr != null) {
    ClientProtocol clientProtocol=null;    try {      if (jobTrackAddr == null) {        clientProtocol=provider.create(conf);      } else {        clientProtocol=provider.create(jobTrackAddr,conf);      }      if (clientProtocol != null) {        clientProtocolProvider=provider;        client=clientProtocol;        LOG.debug("Picked " + provider.getClass().getName() + " as the ClientProtocolProvider");        break;      } else {        LOG.debug("Cannot pick " + provider.getClass().getName() + " as the ClientProtocolProvider - returned null protocol");      }    } catch (    Exception e) {      final String errMsg="Failed to use " + provider.getClass().getName() + " due to error: ";
public static FSDataOutputStream wrapIfNecessary(Configuration conf,FSDataOutputStream out,boolean closeOutputStream) throws IOException {  if (isEncryptedSpillEnabled(conf)) {    out.write(ByteBuffer.allocate(8).putLong(out.getPos()).array());    byte[] iv=createIV(conf);    out.write(iv);    if (LOG.isDebugEnabled()) {
public static FSDataInputStream wrapIfNecessary(Configuration conf,FSDataInputStream in) throws IOException {  if (isEncryptedSpillEnabled(conf)) {    CryptoCodec cryptoCodec=CryptoCodec.getInstance(conf);    int bufferSize=getBufferSize(conf);    IOUtils.readFully(in,new byte[8],0,8);    byte[] iv=new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];    IOUtils.readFully(in,iv,0,cryptoCodec.getCipherSuite().getAlgorithmBlockSize());    if (LOG.isDebugEnabled()) {
private static Map<String,Boolean> getSharedCacheUploadPolicies(Configuration conf,boolean areFiles){  String confParam=areFiles ? MRJobConfig.CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES : MRJobConfig.CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES;  Collection<String> policies=conf.getStringCollection(confParam);  String[] policy;  Map<String,Boolean> policyMap=new LinkedHashMap<String,Boolean>();  for (  String s : policies) {    policy=s.split(DELIM);    if (policy.length != 2) {
public boolean monitorAndPrintJob() throws IOException, InterruptedException {  String lastReport=null;  Job.TaskStatusFilter filter;  Configuration clientConf=getConfiguration();  filter=Job.getTaskOutputFilter(clientConf);  JobID jobId=getJobID();
  int progMonitorPollIntervalMillis=Job.getProgressPollInterval(clientConf);  boolean reportedAfterCompletion=false;  boolean reportedUberMode=false;  while (!isComplete() || !reportedAfterCompletion) {    if (isComplete()) {      reportedAfterCompletion=true;    } else {      Thread.sleep(progMonitorPollIntervalMillis);    }    if (status.getState() == JobStatus.State.PREP) {      continue;    }    if (!reportedUberMode) {      reportedUberMode=true;      LOG.info("Job " + jobId + " running in uber mode : "+ isUber());    }    String report=(" map " + StringUtils.formatPercent(mapProgress(),0) + " reduce "+ StringUtils.formatPercent(reduceProgress(),0));    if (!report.equals(lastReport)) {
 else {      Thread.sleep(progMonitorPollIntervalMillis);    }    if (status.getState() == JobStatus.State.PREP) {      continue;    }    if (!reportedUberMode) {      reportedUberMode=true;      LOG.info("Job " + jobId + " running in uber mode : "+ isUber());    }    String report=(" map " + StringUtils.formatPercent(mapProgress(),0) + " reduce "+ StringUtils.formatPercent(reduceProgress(),0));    if (!report.equals(lastReport)) {      LOG.info(report);      lastReport=report;    }    TaskCompletionEvent[] events=getTaskCompletionEvents(eventCounter,10);    eventCounter+=events.length;    printTaskEvents(events,filter,profiling,mapRanges,reduceRanges);  }  boolean success=isSuccessful();
    }    if (status.getState() == JobStatus.State.PREP) {      continue;    }    if (!reportedUberMode) {      reportedUberMode=true;      LOG.info("Job " + jobId + " running in uber mode : "+ isUber());    }    String report=(" map " + StringUtils.formatPercent(mapProgress(),0) + " reduce "+ StringUtils.formatPercent(reduceProgress(),0));    if (!report.equals(lastReport)) {      LOG.info(report);      lastReport=report;    }    TaskCompletionEvent[] events=getTaskCompletionEvents(eventCounter,10);    eventCounter+=events.length;    printTaskEvents(events,filter,profiling,mapRanges,reduceRanges);  }  boolean success=isSuccessful();  if (success) {    LOG.info("Job " + jobId + " completed successfully");
private void printTaskEvents(TaskCompletionEvent[] events,Job.TaskStatusFilter filter,boolean profiling,IntegerRanges mapRanges,IntegerRanges reduceRanges) throws IOException, InterruptedException {  for (  TaskCompletionEvent event : events) {switch (filter) {case NONE:      break;case SUCCEEDED:    if (event.getStatus() == TaskCompletionEvent.Status.SUCCEEDED) {
  for (  TaskCompletionEvent event : events) {switch (filter) {case NONE:      break;case SUCCEEDED:    if (event.getStatus() == TaskCompletionEvent.Status.SUCCEEDED) {      LOG.info(event.toString());    }  break;case FAILED:if (event.getStatus() == TaskCompletionEvent.Status.FAILED) {  LOG.info(event.toString());  TaskAttemptID taskId=event.getTaskAttemptId();  String[] taskDiagnostics=getTaskDiagnostics(taskId);  if (taskDiagnostics != null) {    for (    String diagnostics : taskDiagnostics) {      System.err.println(diagnostics);    }  }}break;case KILLED:if (event.getStatus() == TaskCompletionEvent.Status.KILLED) {
private void disableErasureCodingForPath(Path path) throws IOException {  try {    if (jtFs instanceof DistributedFileSystem) {
  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,conf);  InetAddress ip=InetAddress.getLocalHost();  if (ip != null) {    submitHostAddress=ip.getHostAddress();    submitHostName=ip.getHostName();    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);  }  JobID jobId=submitClient.getNewJobID();  job.setJobID(jobId);  Path submitJobDir=new Path(jobStagingArea,jobId.toString());  JobStatus status=null;  try {    conf.set(MRJobConfig.USER_NAME,UserGroupInformation.getCurrentUser().getShortUserName());    conf.set("hadoop.http.filter.initializers","org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR,submitJobDir.toString());
    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);    populateTokenCache(conf,job.getCredentials());    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {      KeyGenerator keyGen;      try {        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);        keyGen.init(SHUFFLE_KEY_LENGTH);      } catch (      NoSuchAlgorithmException e) {        throw new IOException("Error generating shuffle secret key",e);      }      SecretKey shuffleKey=keyGen.generateKey();      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());    }    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");    }    copyAndConfigureFiles(job,submitJobDir);
      KeyGenerator keyGen;      try {        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);        keyGen.init(SHUFFLE_KEY_LENGTH);      } catch (      NoSuchAlgorithmException e) {        throw new IOException("Error generating shuffle secret key",e);      }      SecretKey shuffleKey=keyGen.generateKey();      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());    }    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");    }    copyAndConfigureFiles(job,submitJobDir);    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));    int maps=writeSplits(job,submitJobDir);
    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);    AccessControlList acl=submitClient.getQueueAdmins(queue);    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());    TokenCache.cleanUpTokenReferral(conf);    if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {      ArrayList<String> trackingIds=new ArrayList<String>();      for (      Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {        trackingIds.add(t.decodeIdentifier().getTrackingId());      }      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,trackingIds.toArray(new String[trackingIds.size()]));    }    ReservationId reservationId=job.getReservationId();    if (reservationId != null) {      conf.set(MRJobConfig.RESERVATION_ID,reservationId.toString());    }    writeConf(conf,submitJobFile);    printTokens(jobId,job.getCredentials());    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
private void printTokens(JobID jobId,Credentials credentials) throws IOException {
private void printTokens(JobID jobId,Credentials credentials) throws IOException {  LOG.info("Submitting tokens for job: " + jobId);
private void populateTokenCache(Configuration conf,Credentials credentials) throws IOException {  readTokensFromFiles(conf,credentials);  String[] nameNodes=conf.getStrings(MRJobConfig.JOB_NAMENODES);
    DBSplitter splitter=getSplitter(sqlDataType);    if (null == splitter) {      throw new IOException("Unknown SQL data type: " + sqlDataType);    }    return splitter.split(job.getConfiguration(),results,getDBConf().getInputOrderBy());  } catch (  SQLException e) {    throw new IOException(e.getMessage());  } finally {    try {      if (null != results) {        results.close();      }    } catch (    SQLException se) {      LOG.debug("SQLException closing resultset: " + se.toString());    }    try {      if (null != statement) {        statement.close();
  } catch (  SQLException e) {    throw new IOException(e.getMessage());  } finally {    try {      if (null != results) {        results.close();      }    } catch (    SQLException se) {      LOG.debug("SQLException closing resultset: " + se.toString());    }    try {      if (null != statement) {        statement.close();      }    } catch (    SQLException se) {      LOG.debug("SQLException closing statement: " + se.toString());    }    try {      connection.commit();
protected RecordReader<LongWritable,T> createDBRecordReader(DBInputSplit split,Configuration conf) throws IOException {  DBConfiguration dbConf=getDBConf();  @SuppressWarnings("unchecked") Class<T> inputClass=(Class<T>)(dbConf.getInputClass());  String dbProductName=getDBProductName();
    query.append("SELECT ");    for (int i=0; i < fieldNames.length; i++) {      query.append(fieldNames[i]);      if (i != fieldNames.length - 1) {        query.append(", ");      }    }    query.append(" FROM ").append(tableName);    if (!dbProductName.startsWith("ORACLE")) {      query.append(" AS ").append(tableName);    }    query.append(" WHERE ");    if (conditions != null && conditions.length() > 0) {      query.append("( ").append(conditions).append(" ) AND ");    }    query.append(conditionClauses.toString());  } else {    String inputQuery=dbConf.getInputQuery();    if (inputQuery.indexOf(DataDrivenDBInputFormat.SUBSTITUTE_TOKEN) == -1) {
    method=conn.getClass().getMethod("setSessionTimeZone",new Class[]{String.class});  } catch (  Exception ex) {    LOG.error("Could not find method setSessionTimeZone in " + conn.getClass().getName(),ex);    throw new SQLException(ex);  }  String clientTimeZone=conf.get(SESSION_TIMEZONE_KEY,"GMT");  try {    method.setAccessible(true);    method.invoke(conn,clientTimeZone);    LOG.info("Time zone has been set to " + clientTimeZone);  } catch (  Exception ex) {    LOG.warn("Time zone " + clientTimeZone + " could not be set on Oracle database.");    LOG.warn("Setting default time zone: GMT");    try {      method.invoke(conn,"GMT");    } catch (    Exception ex2) {
public void setup(Context context) throws IOException, InterruptedException {  Configuration conf=context.getConfiguration();  this.fieldSeparator=conf.get(FieldSelectionHelper.DATA_FIELD_SEPARATOR,"\t");  this.reduceOutputKeyValueSpec=conf.get(FieldSelectionHelper.REDUCE_OUTPUT_KEY_VALUE_SPEC,"0-:");  allReduceValueFieldsFrom=FieldSelectionHelper.parseOutputKeyValueSpec(reduceOutputKeyValueSpec,reduceOutputKeyFieldList,reduceOutputValueFieldList);
  List<FileStatus> result=null;  int numThreads=job.getConfiguration().getInt(LIST_STATUS_NUM_THREADS,DEFAULT_LIST_STATUS_NUM_THREADS);  StopWatch sw=new StopWatch().start();  if (numThreads == 1) {    result=singleThreadedListStatus(job,dirs,inputFilter,recursive);  } else {    Iterable<FileStatus> locatedFiles=null;    try {      LocatedFileStatusFetcher locatedFileStatusFetcher=new LocatedFileStatusFetcher(job.getConfiguration(),dirs,recursive,inputFilter,true);      locatedFiles=locatedFileStatusFetcher.getFileStatuses();    } catch (    InterruptedException e) {      throw (IOException)new InterruptedIOException("Interrupted while getting file statuses").initCause(e);    }    result=Lists.newArrayList(locatedFiles);  }  sw.stop();  if (LOG.isDebugEnabled()) {
 else {        FileSystem fs=path.getFileSystem(job.getConfiguration());        blkLocations=fs.getFileBlockLocations(file,0,length);      }      if (isSplitable(job,path)) {        long blockSize=file.getBlockSize();        long splitSize=computeSplitSize(blockSize,minSize,maxSize);        long bytesRemaining=length;        while (((double)bytesRemaining) / splitSize > SPLIT_SLOP) {          int blkIndex=getBlockIndex(blkLocations,length - bytesRemaining);          splits.add(makeSplit(path,length - bytesRemaining,splitSize,blkLocations[blkIndex].getHosts(),blkLocations[blkIndex].getCachedHosts()));          bytesRemaining-=splitSize;        }        if (bytesRemaining != 0) {          int blkIndex=getBlockIndex(blkLocations,length - bytesRemaining);          splits.add(makeSplit(path,length - bytesRemaining,bytesRemaining,blkLocations[blkIndex].getHosts(),blkLocations[blkIndex].getCachedHosts()));        }      } else {
        long bytesRemaining=length;        while (((double)bytesRemaining) / splitSize > SPLIT_SLOP) {          int blkIndex=getBlockIndex(blkLocations,length - bytesRemaining);          splits.add(makeSplit(path,length - bytesRemaining,splitSize,blkLocations[blkIndex].getHosts(),blkLocations[blkIndex].getCachedHosts()));          bytesRemaining-=splitSize;        }        if (bytesRemaining != 0) {          int blkIndex=getBlockIndex(blkLocations,length - bytesRemaining);          splits.add(makeSplit(path,length - bytesRemaining,bytesRemaining,blkLocations[blkIndex].getHosts(),blkLocations[blkIndex].getCachedHosts()));        }      } else {        if (LOG.isDebugEnabled()) {          if (length > Math.min(file.getBlockSize(),minSize)) {            LOG.debug("File is not splittable so no parallelization " + "is possible: " + file.getPath());          }        }        splits.add(makeSplit(path,0,length,blkLocations[0].getHosts(),blkLocations[0].getCachedHosts()));      }    } else {      splits.add(makeSplit(path,0,length,new String[0]));
    isCompressedInput=true;    decompressor=CodecPool.getDecompressor(codec);    CompressionInputStream cIn=codec.createInputStream(fileIn,decompressor);    filePosition=cIn;    inputStream=cIn;    numRecordsRemainingInSplit=Long.MAX_VALUE;    LOG.info("Compressed input; cannot compute number of records in the split");  } else {    fileIn.seek(start);    filePosition=fileIn;    inputStream=fileIn;    long splitSize=end - start - numBytesToSkip;    numRecordsRemainingInSplit=(splitSize + recordLength - 1) / recordLength;    if (numRecordsRemainingInSplit < 0) {      numRecordsRemainingInSplit=0;
  }  for (  ControlledJob n : jobList) {    if (!hasInComingEdge(n,jobList,processedMap)) {      SourceSet.add(n);    }  }  while (!SourceSet.isEmpty()) {    ControlledJob controlledJob=SourceSet.iterator().next();    SourceSet.remove(controlledJob);    if (controlledJob.getDependentJobs() != null) {      for (int i=0; i < controlledJob.getDependentJobs().size(); i++) {        ControlledJob depenControlledJob=controlledJob.getDependentJobs().get(i);        processedMap.get(controlledJob).add(depenControlledJob);        if (!hasInComingEdge(controlledJob,jobList,processedMap)) {          SourceSet.add(depenControlledJob);        }      }    }  }  for (  ControlledJob controlledJob : jobList) {    if (controlledJob.getDependentJobs() != null && controlledJob.getDependentJobs().size() != processedMap.get(controlledJob).size()) {      cyclePresent=true;
@Override public void run(Context context) throws IOException, InterruptedException {  outer=context;  int numberOfThreads=getNumberOfThreads(context);  mapClass=getMapperClass(context);  if (LOG.isDebugEnabled()) {
public void setupJob(JobContext context) throws IOException {  if (hasOutputPath()) {    Path jobAttemptPath=getJobAttemptPath(context);    FileSystem fs=jobAttemptPath.getFileSystem(context.getConfiguration());    if (!fs.mkdirs(jobAttemptPath)) {
private void mergePaths(FileSystem fs,final FileStatus from,final Path to,JobContext context) throws IOException {  if (LOG.isDebugEnabled()) {
    FileStatus taskAttemptDirStatus;    try {      taskAttemptDirStatus=fs.getFileStatus(taskAttemptPath);    } catch (    FileNotFoundException e) {      taskAttemptDirStatus=null;    }    if (taskAttemptDirStatus != null) {      if (algorithmVersion == 1) {        Path committedTaskPath=getCommittedTaskPath(context);        if (fs.exists(committedTaskPath)) {          if (!fs.delete(committedTaskPath,true)) {            throw new IOException("Could not delete " + committedTaskPath);          }        }        if (!fs.rename(taskAttemptPath,committedTaskPath)) {          throw new IOException("Could not rename " + taskAttemptPath + " to "+ committedTaskPath);        }        LOG.info("Saved output of task '" + attemptId + "' to "+ committedTaskPath);      } else {
      taskAttemptDirStatus=fs.getFileStatus(taskAttemptPath);    } catch (    FileNotFoundException e) {      taskAttemptDirStatus=null;    }    if (taskAttemptDirStatus != null) {      if (algorithmVersion == 1) {        Path committedTaskPath=getCommittedTaskPath(context);        if (fs.exists(committedTaskPath)) {          if (!fs.delete(committedTaskPath,true)) {            throw new IOException("Could not delete " + committedTaskPath);          }        }        if (!fs.rename(taskAttemptPath,committedTaskPath)) {          throw new IOException("Could not rename " + taskAttemptPath + " to "+ committedTaskPath);        }        LOG.info("Saved output of task '" + attemptId + "' to "+ committedTaskPath);      } else {        mergePaths(fs,taskAttemptDirStatus,outputPath,context);        LOG.info("Saved output of task '" + attemptId + "' to "+ outputPath);
    if (LOG.isDebugEnabled()) {      LOG.debug("Trying to recover task from " + previousCommittedTaskPath);    }    if (algorithmVersion == 1) {      if (fs.exists(previousCommittedTaskPath)) {        Path committedTaskPath=getCommittedTaskPath(context);        if (!fs.delete(committedTaskPath,true) && fs.exists(committedTaskPath)) {          throw new IOException("Could not delete " + committedTaskPath);        }        Path committedParent=committedTaskPath.getParent();        fs.mkdirs(committedParent);        if (!fs.rename(previousCommittedTaskPath,committedTaskPath)) {          throw new IOException("Could not rename " + previousCommittedTaskPath + " to "+ committedTaskPath);        }      } else {        LOG.warn(attemptId + " had no output to recover.");      }    } else {      try {
      if (fs.exists(previousCommittedTaskPath)) {        Path committedTaskPath=getCommittedTaskPath(context);        if (!fs.delete(committedTaskPath,true) && fs.exists(committedTaskPath)) {          throw new IOException("Could not delete " + committedTaskPath);        }        Path committedParent=committedTaskPath.getParent();        fs.mkdirs(committedParent);        if (!fs.rename(previousCommittedTaskPath,committedTaskPath)) {          throw new IOException("Could not rename " + previousCommittedTaskPath + " to "+ committedTaskPath);        }      } else {        LOG.warn(attemptId + " had no output to recover.");      }    } else {      try {        FileStatus from=fs.getFileStatus(previousCommittedTaskPath);        LOG.info("Recovering task for upgrading scenario, moving files from " + previousCommittedTaskPath + " to "+ outputPath);        mergePaths(fs,from,outputPath,context);
public static Path getWorkOutputPath(TaskInputOutputContext<?,?,?,?> context) throws IOException, InterruptedException {  PathOutputCommitter committer=(PathOutputCommitter)context.getOutputCommitter();  Path workPath=committer.getWorkPath();
public Path getDefaultWorkFile(TaskAttemptContext context,String extension) throws IOException {  OutputCommitter c=getOutputCommitter(context);  Preconditions.checkState(c instanceof PathOutputCommitter,"Committer %s is not a PathOutputCommitter",c);  Path workPath=((PathOutputCommitter)c).getWorkPath();  Preconditions.checkNotNull(workPath,"Null workPath returned by committer %s",c);  Path workFile=new Path(workPath,getUniqueFile(context,getOutputName(context),extension));
@SuppressWarnings("JavaReflectionMemberAccess") @Override public PathOutputCommitter createOutputCommitter(Path outputPath,TaskAttemptContext context) throws IOException {  Class<? extends PathOutputCommitter> clazz=loadCommitterClass(context);
protected final PathOutputCommitter createFileOutputCommitter(Path outputPath,TaskAttemptContext context) throws IOException {
public static PathOutputCommitterFactory getCommitterFactory(Path outputPath,Configuration conf){
public static PathOutputCommitterFactory getCommitterFactory(Path outputPath,Configuration conf){  LOG.debug("Looking for committer factory for path {}",outputPath);  String key=COMMITTER_FACTORY_CLASS;  if (StringUtils.isEmpty(conf.getTrimmed(key)) && outputPath != null) {    String scheme=outputPath.toUri().getScheme();    String schemeKey=String.format(COMMITTER_FACTORY_SCHEME_PATTERN,scheme);    if (StringUtils.isNotEmpty(conf.getTrimmed(schemeKey))) {
  if (StringUtils.isEmpty(conf.getTrimmed(key)) && outputPath != null) {    String scheme=outputPath.toUri().getScheme();    String schemeKey=String.format(COMMITTER_FACTORY_SCHEME_PATTERN,scheme);    if (StringUtils.isNotEmpty(conf.getTrimmed(schemeKey))) {      LOG.debug("Using schema-specific factory for {}",outputPath);      key=schemeKey;    } else {      LOG.debug("No scheme-specific factory defined in {}",schemeKey);    }  }  Class<? extends PathOutputCommitterFactory> factory;  String trimmedValue=conf.getTrimmed(key,"");  if (StringUtils.isEmpty(trimmedValue)) {    LOG.debug("No output committer factory defined," + " defaulting to FileOutputCommitterFactory");    factory=FileOutputCommitterFactory.class;  } else {    factory=conf.getClass(key,FileOutputCommitterFactory.class,PathOutputCommitterFactory.class);
@SuppressWarnings("unchecked") public static <K,V>void writePartitionFile(Job job,Sampler<K,V> sampler) throws IOException, ClassNotFoundException, InterruptedException {  Configuration conf=job.getConfiguration();  final InputFormat inf=ReflectionUtils.newInstance(job.getInputFormatClass(),conf);  int numPartitions=job.getNumReduceTasks();  K[] samples=(K[])sampler.getSample(inf,job);
@InterfaceAudience.Private @Deprecated public static Credentials loadTokens(String jobTokenFile,JobConf conf) throws IOException {  Path localJobTokenFile=new Path("file:///" + jobTokenFile);  Credentials ts=Credentials.readTokenStorageFile(localJobTokenFile,conf);  if (LOG.isDebugEnabled()) {
@Override public void run(){  int failures=0;  LOG.info(reduce + " Thread started: " + getName());  try {    while (!stopped && !Thread.currentThread().isInterrupted()) {      try {        int numNewMaps=getMapCompletionEvents();        failures=0;        if (numNewMaps > 0) {
  LOG.info(reduce + " Thread started: " + getName());  try {    while (!stopped && !Thread.currentThread().isInterrupted()) {      try {        int numNewMaps=getMapCompletionEvents();        failures=0;        if (numNewMaps > 0) {          LOG.info(reduce + ": " + "Got "+ numNewMaps+ " new map-outputs");        }        LOG.debug("GetMapEventsThread about to sleep for " + SLEEP_TIME);        if (!Thread.currentThread().isInterrupted()) {          Thread.sleep(SLEEP_TIME);        }      } catch (      InterruptedException e) {        LOG.info("EventFetcher is interrupted.. Returning");        return;      }catch (      IOException ie) {
  if (rc == TOO_MANY_REQ_STATUS_CODE) {    long backoff=connection.getHeaderFieldLong(FETCH_RETRY_AFTER_HEADER,FETCH_RETRY_DELAY_DEFAULT);    if (backoff < 0) {      backoff=FETCH_RETRY_DELAY_DEFAULT;      LOG.warn("Get a negative backoff value from ShuffleHandler. Setting" + " it to the default value " + FETCH_RETRY_DELAY_DEFAULT);    }    throw new TryAgainLaterException(backoff,url.getHost());  }  if (rc != HttpURLConnection.HTTP_OK) {    throw new IOException("Got invalid response code " + rc + " from "+ url+ ": "+ connection.getResponseMessage());  }  if (!ShuffleHeader.DEFAULT_HTTP_HEADER_NAME.equals(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_NAME)) || !ShuffleHeader.DEFAULT_HTTP_HEADER_VERSION.equals(connection.getHeaderField(ShuffleHeader.HTTP_HEADER_VERSION))) {    throw new IOException("Incompatible shuffle response version");  }  String replyHash=connection.getHeaderField(SecureShuffleUtils.HTTP_HEADER_REPLY_URL_HASH);  if (replyHash == null) {    throw new IOException("security validation of TT Map output failed");  }  LOG.debug("url=" + msgToEncode + ";encHash="+ encHash+ ";replyHash="+ replyHash);  SecureShuffleUtils.verifyReply(replyHash,encHash,shuffleSecretKey);
      mapId=TaskAttemptID.forName(header.mapId);      compressedLength=header.compressedLength;      decompressedLength=header.uncompressedLength;      forReduce=header.forReduce;    } catch (    IllegalArgumentException e) {      badIdErrs.increment(1);      LOG.warn("Invalid map id ",e);      return remaining.toArray(new TaskAttemptID[remaining.size()]);    }    InputStream is=input;    is=CryptoUtils.wrapIfNecessary(jobConf,is,compressedLength);    compressedLength-=CryptoUtils.cryptoPadding(jobConf);    decompressedLength-=CryptoUtils.cryptoPadding(jobConf);    if (!verifySanity(compressedLength,decompressedLength,forReduce,remaining,mapId)) {      return new TaskAttemptID[]{mapId};    }    if (LOG.isDebugEnabled()) {
      return remaining.toArray(new TaskAttemptID[remaining.size()]);    }    InputStream is=input;    is=CryptoUtils.wrapIfNecessary(jobConf,is,compressedLength);    compressedLength-=CryptoUtils.cryptoPadding(jobConf);    decompressedLength-=CryptoUtils.cryptoPadding(jobConf);    if (!verifySanity(compressedLength,decompressedLength,forReduce,remaining,mapId)) {      return new TaskAttemptID[]{mapId};    }    if (LOG.isDebugEnabled()) {      LOG.debug("header: " + mapId + ", len: "+ compressedLength+ ", decomp len: "+ decompressedLength);    }    try {      mapOutput=merger.reserve(mapId,decompressedLength,id);    } catch (    IOException ioe) {      ioErrs.increment(1);      scheduler.reportLocalError(ioe);      return EMPTY_ATTEMPT_ID_ARRAY;
    is=CryptoUtils.wrapIfNecessary(jobConf,is,compressedLength);    compressedLength-=CryptoUtils.cryptoPadding(jobConf);    decompressedLength-=CryptoUtils.cryptoPadding(jobConf);    if (!verifySanity(compressedLength,decompressedLength,forReduce,remaining,mapId)) {      return new TaskAttemptID[]{mapId};    }    if (LOG.isDebugEnabled()) {      LOG.debug("header: " + mapId + ", len: "+ compressedLength+ ", decomp len: "+ decompressedLength);    }    try {      mapOutput=merger.reserve(mapId,decompressedLength,id);    } catch (    IOException ioe) {      ioErrs.increment(1);      scheduler.reportLocalError(ioe);      return EMPTY_ATTEMPT_ID_ARRAY;    }    if (mapOutput == null) {      LOG.info("fetcher#" + id + " - MergeManager returned status WAIT ...");
  }  long startTime=Time.monotonicNow();  long lastTime=startTime;  int attempts=0;  connection.setConnectTimeout(unit);  while (true) {    try {      attempts++;      connection.connect();      break;    } catch (    IOException ioe) {      long currentTime=Time.monotonicNow();      long retryTime=currentTime - startTime;      long leftTime=connectionTimeout - retryTime;      long timeSinceLastIteration=currentTime - lastTime;      if (leftTime <= 0) {
private void doCopy(Set<TaskAttemptID> maps) throws IOException {  Iterator<TaskAttemptID> iter=maps.iterator();  while (iter.hasNext()) {    TaskAttemptID map=iter.next();
private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {  Path mapOutputFileName=localMapFiles.get(mapTaskId).getOutputFile();  Path indexFileName=mapOutputFileName.suffix(".index");  SpillRecord sr=new SpillRecord(indexFileName,job);  IndexRecord ir=sr.getIndex(reduce);  long compressedLength=ir.partLength;  long decompressedLength=ir.rawLength;  compressedLength-=CryptoUtils.cryptoPadding(job);  decompressedLength-=CryptoUtils.cryptoPadding(job);  MapOutput<K,V> mapOutput=merger.reserve(mapTaskId,decompressedLength,id);  if (mapOutput == null) {
@Override public synchronized MapOutput<K,V> reserve(TaskAttemptID mapId,long requestedSize,int fetcher) throws IOException {  if (requestedSize > maxSingleShuffleLimit) {
public synchronized void closeInMemoryFile(InMemoryMapOutput<K,V> mapOutput){  inMemoryMapOutputs.add(mapOutput);
public synchronized void closeInMemoryFile(InMemoryMapOutput<K,V> mapOutput){  inMemoryMapOutputs.add(mapOutput);  LOG.info("closeInMemoryFile -> map-output of size: " + mapOutput.getSize() + ", inMemoryMapOutputs.size() -> "+ inMemoryMapOutputs.size()+ ", commitMemory -> "+ commitMemory+ ", usedMemory ->"+ usedMemory);  commitMemory+=mapOutput.getSize();  if (commitMemory >= mergeThreshold) {
public synchronized void closeInMemoryMergedFile(InMemoryMapOutput<K,V> mapOutput){  inMemoryMergedMapOutputs.add(mapOutput);
private RawKeyValueIterator finalMerge(JobConf job,FileSystem fs,List<InMemoryMapOutput<K,V>> inMemoryMapOutputs,List<CompressAwarePath> onDiskMapOutputs) throws IOException {
        onDiskMapOutputs.add(new CompressAwarePath(outputPath,writer.getRawLength(),writer.getCompressedLength()));        writer=null;      } catch (      IOException e) {        if (null != outputPath) {          try {            fs.delete(outputPath,true);          } catch (          IOException ie) {          }        }        throw e;      } finally {        if (null != writer) {          writer.close();        }      }      LOG.info("Merged " + numMemDiskSegments + " segments, "+ inMemToDiskBytes+ " bytes to disk to satisfy "+ "reduce memory limit");      inMemToDiskBytes=0;      memDiskSegments.clear();    } else     if (inMemToDiskBytes != 0) {
          }        }        throw e;      } finally {        if (null != writer) {          writer.close();        }      }      LOG.info("Merged " + numMemDiskSegments + " segments, "+ inMemToDiskBytes+ " bytes to disk to satisfy "+ "reduce memory limit");      inMemToDiskBytes=0;      memDiskSegments.clear();    } else     if (inMemToDiskBytes != 0) {      LOG.info("Keeping " + numMemDiskSegments + " segments, "+ inMemToDiskBytes+ " bytes in memory for "+ "intermediate, on-disk merge");    }  }  List<Segment<K,V>> diskSegments=new ArrayList<Segment<K,V>>();  long onDiskBytes=inMemToDiskBytes;  long rawBytes=inMemToDiskBytes;  CompressAwarePath[] onDisk=onDiskMapOutputs.toArray(new CompressAwarePath[onDiskMapOutputs.size()]);  for (  CompressAwarePath file : onDisk) {    long fileLength=fs.getFileStatus(file).getLen();
      } finally {        if (null != writer) {          writer.close();        }      }      LOG.info("Merged " + numMemDiskSegments + " segments, "+ inMemToDiskBytes+ " bytes to disk to satisfy "+ "reduce memory limit");      inMemToDiskBytes=0;      memDiskSegments.clear();    } else     if (inMemToDiskBytes != 0) {      LOG.info("Keeping " + numMemDiskSegments + " segments, "+ inMemToDiskBytes+ " bytes in memory for "+ "intermediate, on-disk merge");    }  }  List<Segment<K,V>> diskSegments=new ArrayList<Segment<K,V>>();  long onDiskBytes=inMemToDiskBytes;  long rawBytes=inMemToDiskBytes;  CompressAwarePath[] onDisk=onDiskMapOutputs.toArray(new CompressAwarePath[onDiskMapOutputs.size()]);  for (  CompressAwarePath file : onDisk) {    long fileLength=fs.getFileStatus(file).getLen();    onDiskBytes+=fileLength;
@Override public void resolve(TaskCompletionEvent event){switch (event.getTaskStatus()) {case SUCCEEDED:    URI u=getBaseURI(reduceId,event.getTaskTrackerHttp());  addKnownMapOutput(u.getHost() + ":" + u.getPort(),u.toString(),event.getTaskAttemptId());maxMapRuntime=Math.max(maxMapRuntime,event.getTaskRunTime());break;case FAILED:case KILLED:case OBSOLETE:obsoleteMapOutput(event.getTaskAttemptId());
@Override public void resolve(TaskCompletionEvent event){switch (event.getTaskStatus()) {case SUCCEEDED:    URI u=getBaseURI(reduceId,event.getTaskTrackerHttp());  addKnownMapOutput(u.getHost() + ":" + u.getPort(),u.toString(),event.getTaskAttemptId());maxMapRuntime=Math.max(maxMapRuntime,event.getTaskRunTime());break;case FAILED:case KILLED:case OBSOLETE:obsoleteMapOutput(event.getTaskAttemptId());LOG.info("Ignoring obsolete output of " + event.getTaskStatus() + " map-task: '"+ event.getTaskAttemptId()+ "'");break;case TIPFAILED:tipFailed(event.getTaskAttemptId().getTaskID());
    output.commit();    finishedMaps[mapIndex]=true;    shuffledMapsCounter.increment(1);    if (--remainingMaps == 0) {      notifyAll();    }    long copyMillis=(endMillis - startMillis);    if (copyMillis == 0)     copyMillis=1;    float bytesPerMillis=(float)bytes / copyMillis;    float transferRate=bytesPerMillis * BYTES_PER_MILLIS_TO_MBS;    String individualProgress="copy task(" + mapId + " succeeded"+ " at "+ mbpsFormat.format(transferRate)+ " MB/s)";    copyTimeTracker.add(startMillis,endMillis);    totalBytesShuffledTillNow+=bytes;    updateStatus(individualProgress);    reduceShuffleBytes.increment(bytes);    lastProgressTime=Time.monotonicNow();
private void checkAndInformMRAppMaster(int failures,TaskAttemptID mapId,boolean readError,boolean connectExcpt,boolean hostFailed){  if (connectExcpt || (reportReadErrorImmediately && readError) || ((failures % maxFetchFailuresBeforeReporting) == 0)|| hostFailed) {
  Iterator<TaskAttemptID> itr=list.iterator();  List<TaskAttemptID> result=new ArrayList<TaskAttemptID>();  int includedMaps=0;  int totalSize=list.size();  while (itr.hasNext()) {    TaskAttemptID id=itr.next();    if (!obsoleteMaps.contains(id) && !finishedMaps[id.getTaskID().getId()]) {      result.add(id);      if (++includedMaps >= MAX_MAPS_AT_ONCE) {        break;      }    }  }  while (itr.hasNext()) {    TaskAttemptID id=itr.next();    if (!obsoleteMaps.contains(id) && !finishedMaps[id.getTaskID().getId()]) {      host.addKnownMap(id);    }  }  if (LOG.isDebugEnabled()) {
private String run(HistoryViewerPrinter printer) throws Exception {  ByteArrayOutputStream boas=new ByteArrayOutputStream();  PrintStream out=new PrintStream(boas,true);  printer.print(out);  out.close();  String outStr=boas.toString("UTF-8");
private Job loadJob(JobId jobId) throws RuntimeException, IOException {  if (LOG.isDebugEnabled()) {
  report.setUser(jobInfo.getUsername());  report.setDiagnostics(jobInfo.getErrorInfo());  if (getTotalMaps() == 0) {    report.setMapProgress(1.0f);  } else {    report.setMapProgress((float)getCompletedMaps() / getTotalMaps());  }  if (getTotalReduces() == 0) {    report.setReduceProgress(1.0f);  } else {    report.setReduceProgress((float)getCompletedReduces() / getTotalReduces());  }  report.setJobFile(getConfFile().toString());  String historyUrl="N/A";  try {    historyUrl=MRWebAppUtil.getApplicationWebURLOnJHSWithScheme(conf,jobId.getAppId());  } catch (  UnknownHostException e) {
protected synchronized void loadFullHistoryData(boolean loadTasks,Path historyFileAbsolute) throws IOException {
public static void logSuccess(String user,String operation,String target){  if (LOG.isInfoEnabled()) {
private void mkdir(FileContext fc,Path path,FsPermission fsp) throws IOException {  if (!fc.util().exists(path)) {    try {      fc.mkdir(path,fsp,true);      FileStatus fsStatus=fc.getFileStatus(path);      LOG.info("Perms after creating " + fsStatus.getPermission().toShort() + ", Expected: "+ fsp.toShort());      if (fsStatus.getPermission().toShort() != fsp.toShort()) {
@SuppressWarnings("unchecked") void initExisting() throws IOException {  LOG.info("Initializing Existing Jobs...");  List<FileStatus> timestampedDirList=findTimestampedDirectories();  Collections.sort(timestampedDirList);
  List<FileStatus> timestampedDirList=findTimestampedDirectories();  Collections.sort(timestampedDirList);  LOG.info("Found " + timestampedDirList.size() + " directories to load");  for (  FileStatus fs : timestampedDirList) {    addDirectoryToSerialNumberIndex(fs.getPath());  }  final double maxCacheSize=(double)jobListCache.maxSize;  int prevCacheSize=jobListCache.size();  for (int i=timestampedDirList.size() - 1; i >= 0 && !jobListCache.isFull(); i--) {    FileStatus fs=timestampedDirList.get(i);    addDirectoryToJobListCache(fs.getPath());    int currCacheSize=jobListCache.size();    if ((currCacheSize - prevCacheSize) / maxCacheSize >= 0.05) {      LOG.info(currCacheSize * 100.0 / maxCacheSize + "% of cache is loaded.");    }    prevCacheSize=currCacheSize;  }  final double loadedPercent=maxCacheSize == 0.0 ? 100 : prevCacheSize * 100.0 / maxCacheSize;
private void addDirectoryToSerialNumberIndex(Path serialDirPath){  if (LOG.isDebugEnabled()) {
private void addDirectoryToJobListCache(Path path) throws IOException {  if (LOG.isDebugEnabled()) {
private void scanIntermediateDirectory(final Path absPath) throws IOException {  if (LOG.isDebugEnabled()) {
    if (LOG.isDebugEnabled()) {      LOG.debug("scanning file: " + fs.getPath());    }    JobIndexInfo jobIndexInfo=FileNameIndexUtils.getIndexInfo(fs.getPath().getName());    String confFileName=JobHistoryUtils.getIntermediateConfFileName(jobIndexInfo.getJobId());    String summaryFileName=JobHistoryUtils.getIntermediateSummaryFileName(jobIndexInfo.getJobId());    HistoryFileInfo fileInfo=createHistoryFileInfo(fs.getPath(),new Path(fs.getPath().getParent(),confFileName),new Path(fs.getPath().getParent(),summaryFileName),jobIndexInfo,false);    final HistoryFileInfo old=jobListCache.addIfAbsent(fileInfo);    if (old == null || old.didMoveFail()) {      final HistoryFileInfo found=(old == null) ? fileInfo : old;      long cutoff=System.currentTimeMillis() - maxHistoryAge;      if (found.getJobIndexInfo().getFinishTime() <= cutoff) {        try {          found.delete();        } catch (        IOException e) {          LOG.warn("Error cleaning up a HistoryFile that is out of date.",e);
    HistoryFileInfo fileInfo=createHistoryFileInfo(fs.getPath(),new Path(fs.getPath().getParent(),confFileName),new Path(fs.getPath().getParent(),summaryFileName),jobIndexInfo,false);    final HistoryFileInfo old=jobListCache.addIfAbsent(fileInfo);    if (old == null || old.didMoveFail()) {      final HistoryFileInfo found=(old == null) ? fileInfo : old;      long cutoff=System.currentTimeMillis() - maxHistoryAge;      if (found.getJobIndexInfo().getFinishTime() <= cutoff) {        try {          found.delete();        } catch (        IOException e) {          LOG.warn("Error cleaning up a HistoryFile that is out of date.",e);        }      } else {        if (LOG.isDebugEnabled()) {          LOG.debug("Scheduling move to done of " + found);        }        moveToDoneExecutor.execute(new Runnable(){          @Override public void run(){
private void moveToDoneNow(final Path src,final Path target) throws IOException {
@Override public void storeToken(MRDelegationTokenIdentifier tokenId,Long renewDate) throws IOException {  if (LOG.isDebugEnabled()) {
@Override public void updateToken(MRDelegationTokenIdentifier tokenId,Long renewDate) throws IOException {  if (LOG.isDebugEnabled()) {
@Override public void removeToken(MRDelegationTokenIdentifier tokenId) throws IOException {  if (LOG.isDebugEnabled()) {
@Override public void storeTokenMasterKey(DelegationKey key) throws IOException {  if (LOG.isDebugEnabled()) {
@Override public void removeTokenMasterKey(DelegationKey key) throws IOException {  if (LOG.isDebugEnabled()) {
private void loadTokenState(HistoryServerState state) throws IOException {  int numKeys=loadKeys(state);  int numTokens=loadTokens(state);
@Override protected void startStorage() throws IOException {  Path storeRoot=createStorageDir(getConfig());  Options options=new Options();  options.createIfMissing(false);
@Override public HistoryServerState loadState() throws IOException {  HistoryServerState state=new HistoryServerState();  int numKeys=loadTokenMasterKeys(state);
@Override public HistoryServerState loadState() throws IOException {  HistoryServerState state=new HistoryServerState();  int numKeys=loadTokenMasterKeys(state);  LOG.info("Recovered " + numKeys + " token master keys");  int numTokens=loadTokens(state);
@Override public void storeToken(MRDelegationTokenIdentifier tokenId,Long renewDate) throws IOException {  if (LOG.isDebugEnabled()) {
@Override public void storeTokenMasterKey(DelegationKey masterKey) throws IOException {  if (LOG.isDebugEnabled()) {
@Override public void removeTokenMasterKey(DelegationKey masterKey) throws IOException {  if (LOG.isDebugEnabled()) {
private void checkVersion() throws IOException {  Version loadedVersion=loadVersion();
@Override protected void storeNewMasterKey(DelegationKey key) throws IOException {  if (LOG.isDebugEnabled()) {
@Override protected void removeStoredMasterKey(DelegationKey key){  if (LOG.isDebugEnabled()) {
@Override protected void storeNewToken(MRDelegationTokenIdentifier tokenId,long renewDate){  if (LOG.isDebugEnabled()) {
@Override protected void removeStoredToken(MRDelegationTokenIdentifier tokenId) throws IOException {  if (LOG.isDebugEnabled()) {
@Override protected void updateStoredToken(MRDelegationTokenIdentifier tokenId,long renewDate){  if (LOG.isDebugEnabled()) {
@Override public Map<JobId,Job> getAllJobs(ApplicationId appID){  if (LOG.isDebugEnabled()) {
  long amStartTimeEst=System.currentTimeMillis();  conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,MyResolver.class,DNSToSwitchMapping.class);  RackResolver.init(conf);  MRApp app=new MRAppWithHistory(numMaps,numReduces,true,this.getClass().getName(),true);  app.submit(conf);  Job job=app.getContext().getAllJobs().values().iterator().next();  JobId jobId=job.getID();  LOG.info("JOBID is " + TypeConverter.fromYarn(jobId).toString());  app.waitForState(job,JobState.SUCCEEDED);  app.waitForState(Service.STATE.STOPPED);  String jobhistoryDir=JobHistoryUtils.getHistoryIntermediateDoneDirForUser(conf);  FileContext fc=null;  try {    fc=FileContext.getFileContext(conf);  } catch (  IOException ioe) {
    Assert.assertTrue("finishTime should not be 0",Long.parseLong(jobSummaryElements.get("finishTime")) != 0);    Assert.assertEquals("Mismatch in num map slots",numSuccessfulMaps,Integer.parseInt(jobSummaryElements.get("numMaps")));    Assert.assertEquals("Mismatch in num reduce slots",numReduces,Integer.parseInt(jobSummaryElements.get("numReduces")));    Assert.assertEquals("User does not match",System.getProperty("user.name"),jobSummaryElements.get("user"));    Assert.assertEquals("Queue does not match","default",jobSummaryElements.get("queue"));    Assert.assertEquals("Status does not match","SUCCEEDED",jobSummaryElements.get("status"));  }  JobHistory jobHistory=new JobHistory();  jobHistory.init(conf);  HistoryFileInfo fileInfo=jobHistory.getJobFileInfo(jobId);  JobInfo jobInfo;  long numFinishedMaps;synchronized (fileInfo) {    Path historyFilePath=fileInfo.getHistoryFile();    FSDataInputStream in=null;    LOG.info("JobHistoryFile is: " + historyFilePath);
    app.waitForState(job,JobState.SUCCEEDED);    app.waitForState(Service.STATE.STOPPED);    JobHistory jobHistory=new JobHistory();    jobHistory.init(conf);    HistoryFileInfo fileInfo=jobHistory.getJobFileInfo(jobId);    JobHistoryParser parser;    JobInfo jobInfo;synchronized (fileInfo) {      Path historyFilePath=fileInfo.getHistoryFile();      FSDataInputStream in=null;      FileContext fc=null;      try {        fc=FileContext.getFileContext(conf);        in=fc.open(fc.makeQualified(historyFilePath));      } catch (      IOException ioe) {
    app.waitForState(job,JobState.SUCCEEDED);    app.waitForState(Service.STATE.STOPPED);    jobHistory=new JobHistory();    jobHistory.init(conf);    HistoryFileInfo fileInfo=jobHistory.getJobFileInfo(jobId);    JobHistoryParser parser;    JobInfo jobInfo;synchronized (fileInfo) {      Path historyFilePath=fileInfo.getHistoryFile();      FSDataInputStream in=null;      FileContext fc=null;      try {        fc=FileContext.getFileContext(conf);        in=fc.open(fc.makeQualified(historyFilePath));      } catch (      IOException ioe) {
    app.waitForState(job,JobState.FAILED);    app.waitForState(Service.STATE.STOPPED);    JobHistory jobHistory=new JobHistory();    jobHistory.init(conf);    HistoryFileInfo fileInfo=jobHistory.getJobFileInfo(jobId);    JobHistoryParser parser;    JobInfo jobInfo;synchronized (fileInfo) {      Path historyFilePath=fileInfo.getHistoryFile();      FSDataInputStream in=null;      FileContext fc=null;      try {        fc=FileContext.getFileContext(conf);        in=fc.open(fc.makeQualified(historyFilePath));      } catch (      IOException ioe) {
    app.waitForState(job,JobState.KILLED);    app.waitForState(Service.STATE.STOPPED);    JobHistory jobHistory=new JobHistory();    jobHistory.init(conf);    HistoryFileInfo fileInfo=jobHistory.getJobFileInfo(jobId);    JobHistoryParser parser;    JobInfo jobInfo;synchronized (fileInfo) {      Path historyFilePath=fileInfo.getHistoryFile();      FSDataInputStream in=null;      FileContext fc=null;      try {        fc=FileContext.getFileContext(conf);        in=fc.open(fc.makeQualified(historyFilePath));      } catch (      IOException ioe) {
@Test public void testTaskAttemptUnsuccessfulCompletionWithoutCounters203() throws IOException {  Path histPath=new Path(getClass().getClassLoader().getResource("job_2.0.3-alpha-FAILED.jhist").getFile());  JobHistoryParser parser=new JobHistoryParser(FileSystem.getLocal(new Configuration()),histPath);  JobInfo jobInfo=parser.parse();
@Test public void testTaskAttemptUnsuccessfulCompletionWithoutCounters240() throws IOException {  Path histPath=new Path(getClass().getClassLoader().getResource("job_2.4.0-FAILED.jhist").getFile());  JobHistoryParser parser=new JobHistoryParser(FileSystem.getLocal(new Configuration()),histPath);  JobInfo jobInfo=parser.parse();
@Test public void testTaskAttemptUnsuccessfulCompletionWithoutCounters0239() throws IOException {  Path histPath=new Path(getClass().getClassLoader().getResource("job_0.23.9-FAILED.jhist").getFile());  JobHistoryParser parser=new JobHistoryParser(FileSystem.getLocal(new Configuration()),histPath);  JobInfo jobInfo=parser.parse();
    application=rm.getApplicationReport(appId);  } catch (  ApplicationNotFoundException e) {    application=null;  }catch (  YarnException e2) {    throw new IOException(e2);  }  if (application != null) {    trackingUrl=application.getTrackingUrl();  }  InetSocketAddress serviceAddr=null;  while (application == null || YarnApplicationState.RUNNING == application.getYarnApplicationState()) {    if (application == null) {      LOG.info("Could not get Job info from RM for job " + jobId + ". Redirecting to job history server.");      return checkAndGetHSProxy(null,JobState.NEW);    }    try {      if (application.getHost() == null || "".equals(application.getHost())) {        LOG.debug("AM not assigned to Job. Waiting to get the AM ...");
catch (  YarnException e2) {    throw new IOException(e2);  }  if (application != null) {    trackingUrl=application.getTrackingUrl();  }  InetSocketAddress serviceAddr=null;  while (application == null || YarnApplicationState.RUNNING == application.getYarnApplicationState()) {    if (application == null) {      LOG.info("Could not get Job info from RM for job " + jobId + ". Redirecting to job history server.");      return checkAndGetHSProxy(null,JobState.NEW);    }    try {      if (application.getHost() == null || "".equals(application.getHost())) {        LOG.debug("AM not assigned to Job. Waiting to get the AM ...");        Thread.sleep(2000);        LOG.debug("Application state is " + application.getYarnApplicationState());        application=rm.getApplicationReport(appId);
    }    try {      if (application.getHost() == null || "".equals(application.getHost())) {        LOG.debug("AM not assigned to Job. Waiting to get the AM ...");        Thread.sleep(2000);        LOG.debug("Application state is " + application.getYarnApplicationState());        application=rm.getApplicationReport(appId);        continue;      } else       if (UNAVAILABLE.equals(application.getHost())) {        if (!amAclDisabledStatusLogged) {          LOG.info("Job " + jobId + " is running, but the host is unknown."+ " Verify user has VIEW_JOB access.");          amAclDisabledStatusLogged=true;        }        return getNotRunningJob(application,JobState.RUNNING);      }      if (!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED,false)) {        UserGroupInformation newUgi=UserGroupInformation.createRemoteUser(UserGroupInformation.getCurrentUser().getUserName());        serviceAddr=NetUtils.createSocketAddrForHost(application.getHost(),application.getRpcPort());
        continue;      } else       if (UNAVAILABLE.equals(application.getHost())) {        if (!amAclDisabledStatusLogged) {          LOG.info("Job " + jobId + " is running, but the host is unknown."+ " Verify user has VIEW_JOB access.");          amAclDisabledStatusLogged=true;        }        return getNotRunningJob(application,JobState.RUNNING);      }      if (!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED,false)) {        UserGroupInformation newUgi=UserGroupInformation.createRemoteUser(UserGroupInformation.getCurrentUser().getUserName());        serviceAddr=NetUtils.createSocketAddrForHost(application.getHost(),application.getRpcPort());        if (UserGroupInformation.isSecurityEnabled()) {          org.apache.hadoop.yarn.api.records.Token clientToAMToken=application.getClientToAMToken();          Token<ClientToAMTokenIdentifier> token=ConverterUtils.convertFromYarn(clientToAMToken,serviceAddr);          newUgi.addToken(token);        }        LOG.debug("Connecting to " + serviceAddr);        final InetSocketAddress finalServiceAddr=serviceAddr;
          amAclDisabledStatusLogged=true;        }        return getNotRunningJob(application,JobState.RUNNING);      }      if (!conf.getBoolean(MRJobConfig.JOB_AM_ACCESS_DISABLED,false)) {        UserGroupInformation newUgi=UserGroupInformation.createRemoteUser(UserGroupInformation.getCurrentUser().getUserName());        serviceAddr=NetUtils.createSocketAddrForHost(application.getHost(),application.getRpcPort());        if (UserGroupInformation.isSecurityEnabled()) {          org.apache.hadoop.yarn.api.records.Token clientToAMToken=application.getClientToAMToken();          Token<ClientToAMTokenIdentifier> token=ConverterUtils.convertFromYarn(clientToAMToken,serviceAddr);          newUgi.addToken(token);        }        LOG.debug("Connecting to " + serviceAddr);        final InetSocketAddress finalServiceAddr=serviceAddr;        realProxy=newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>(){          @Override public MRClientProtocol run() throws IOException {            return instantiateAMProxy(finalServiceAddr);          }        });
        }        LOG.debug("Connecting to " + serviceAddr);        final InetSocketAddress finalServiceAddr=serviceAddr;        realProxy=newUgi.doAs(new PrivilegedExceptionAction<MRClientProtocol>(){          @Override public MRClientProtocol run() throws IOException {            return instantiateAMProxy(finalServiceAddr);          }        });      } else {        if (!amAclDisabledStatusLogged) {          LOG.info("Network ACL closed to AM for job " + jobId + ". Not going to try to reach the AM.");          amAclDisabledStatusLogged=true;        }        return getNotRunningJob(null,JobState.RUNNING);      }      return realProxy;    } catch (    IOException e) {      LOG.info("Could not connect to " + serviceAddr + ". Waiting for getting the latest AM address...");      try {
  Method methodOb=null;  try {    methodOb=MRClientProtocol.class.getMethod(method,argClass);  } catch (  SecurityException e) {    throw new YarnRuntimeException(e);  }catch (  NoSuchMethodException e) {    throw new YarnRuntimeException("Method name mismatch",e);  }  maxClientRetry=this.conf.getInt(MRJobConfig.MR_CLIENT_MAX_RETRIES,MRJobConfig.DEFAULT_MR_CLIENT_MAX_RETRIES);  IOException lastException=null;  while (maxClientRetry > 0) {    MRClientProtocol MRClientProxy=null;    try {      MRClientProxy=getProxy();      return methodOb.invoke(MRClientProxy,args);    } catch (    InvocationTargetException e) {
    try {      MRClientProxy=getProxy();      return methodOb.invoke(MRClientProxy,args);    } catch (    InvocationTargetException e) {      LOG.debug("Failed to contact AM/History for job " + jobId + " retrying..",e.getTargetException());      realProxy=null;      if (e.getCause() instanceof AuthorizationException) {        throw new IOException(e.getTargetException());      }      if (!usingAMProxy.get()) {        maxClientRetry--;      }      usingAMProxy.set(false);      lastException=new IOException(e.getTargetException());      try {        Thread.sleep(100);      } catch (      InterruptedException ie) {
public String getStagingAreaDir() throws IOException, InterruptedException {  String user=UserGroupInformation.getCurrentUser().getShortUserName();  Path path=MRApps.getStagingAreaDir(conf,user);
private Map<String,LocalResource> setupLocalResources(Configuration jobConf,String jobSubmitDir) throws IOException {  Map<String,LocalResource> localResources=new HashMap<>();  Path jobConfPath=new Path(jobSubmitDir,MRJobConfig.JOB_CONF_FILE);  URL yarnUrlForJobSubmitDir=URL.fromPath(defaultFileContext.getDefaultFileSystem().resolvePath(defaultFileContext.makeQualified(new Path(jobSubmitDir))));
  if (regex != null && !regex.isEmpty()) {    setTokenRenewerConf(amContainer,conf,regex);  }  Collection<String> tagsFromConf=jobConf.getTrimmedStringCollection(MRJobConfig.JOB_TAGS);  ApplicationSubmissionContext appContext=recordFactory.newRecordInstance(ApplicationSubmissionContext.class);  appContext.setApplicationId(applicationId);  appContext.setQueue(jobConf.get(JobContext.QUEUE_NAME,YarnConfiguration.DEFAULT_QUEUE_NAME));  ReservationId reservationID=null;  try {    reservationID=ReservationId.parseReservationId(jobConf.get(JobContext.RESERVATION_ID));  } catch (  NumberFormatException e) {    String errMsg="Invalid reservationId: " + jobConf.get(JobContext.RESERVATION_ID) + " specified for the app: "+ applicationId;    LOG.warn(errMsg);    throw new IOException(errMsg);  }  if (reservationID != null) {    appContext.setReservationID(reservationID);
        LOG.warn("Configuration " + MR_AM_RESOURCE_PREFIX + resourceName+ "="+ resourceReq.getValue()+ resourceReq.getUnits()+ " is overriding the "+ MRJobConfig.MR_AM_VMEM_MB+ "="+ conf.get(MRJobConfig.MR_AM_VMEM_MB)+ " configuration");      }    } else     if (MRJobConfig.RESOURCE_TYPE_NAME_VCORE.equals(resourceName)) {      capability.setVirtualCores((int)UnitsConversionUtil.convert(resourceReq.getUnits(),"",resourceReq.getValue()));      cpuVcoresSet=true;      if (conf.get(MRJobConfig.MR_AM_CPU_VCORES) != null) {        LOG.warn("Configuration " + MR_AM_RESOURCE_PREFIX + resourceName+ "="+ resourceReq.getValue()+ resourceReq.getUnits()+ " is overriding the "+ MRJobConfig.MR_AM_CPU_VCORES+ "="+ conf.get(MRJobConfig.MR_AM_CPU_VCORES)+ " configuration");      }    } else     if (!MRJobConfig.MR_AM_VMEM_MB.equals(MR_AM_RESOURCE_PREFIX + resourceName) && !MRJobConfig.MR_AM_CPU_VCORES.equals(MR_AM_RESOURCE_PREFIX + resourceName)) {      ResourceInformation resourceInformation=capability.getResourceInformation(resourceName);      resourceInformation.setUnits(resourceReq.getUnits());      resourceInformation.setValue(resourceReq.getValue());      capability.setResourceInformation(resourceName,resourceInformation);    }  }  if (!memorySet) {    capability.setMemorySize(conf.getInt(MRJobConfig.MR_AM_VMEM_MB,MRJobConfig.DEFAULT_MR_AM_VMEM_MB));  }  if (!cpuVcoresSet) {    capability.setVirtualCores(conf.getInt(MRJobConfig.MR_AM_CPU_VCORES,MRJobConfig.DEFAULT_MR_AM_CPU_VCORES));
  Configuration copy=new Configuration(false);  copy.clear();  int count=0;  for (  Map.Entry<String,String> map : conf) {    String key=map.getKey();    String val=map.getValue();    if (key.matches(regex)) {      copy.set(key,val);      count++;    }  }  copy.write(dob);  ByteBuffer appConf=ByteBuffer.wrap(dob.getData(),0,dob.getLength());  LOG.info("Send configurations that match regex expression: " + regex + " , total number of configs: "+ count+ ", total size : "+ dob.getLength()+ " bytes.");  if (LOG.isDebugEnabled()) {    for (Iterator<Map.Entry<String,String>> itor=copy.iterator(); itor.hasNext(); ) {      Map.Entry<String,String> entry=itor.next();
protected static float getProbability(final String klass){  String newProbName=FPROB_NAME + klass;  String newValue=System.getProperty(newProbName,conf.get(ALL_PROBABILITIES));  if (newValue != null && !newValue.equals(conf.get(newProbName)))   conf.set(newProbName,newValue);  float ret=conf.getFloat(newProbName,conf.getFloat(ALL_PROBABILITIES,DEFAULT_PROB));
private static void createControlFile(FileSystem fs,int fileSize,int nrFiles) throws IOException {
      testType=TEST_TYPE_READ;    } else     if (args[i].startsWith("-w")) {      testType=TEST_TYPE_WRITE;    } else     if (args[i].startsWith("-clean")) {      testType=TEST_TYPE_CLEANUP;    } else     if (args[i].startsWith("-seq")) {      isSequential=true;    } else     if (args[i].equals("-nrFiles")) {      nrFiles=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-fileSize")) {      fileSize=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-bufferSize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-resFile")) {      resFileName=args[++i];
      testType=TEST_TYPE_READ;    } else     if (args[i].startsWith("-w")) {      testType=TEST_TYPE_WRITE;    } else     if (args[i].startsWith("-clean")) {      testType=TEST_TYPE_CLEANUP;    } else     if (args[i].startsWith("-seq")) {      isSequential=true;    } else     if (args[i].equals("-nrFiles")) {      nrFiles=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-fileSize")) {      fileSize=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-bufferSize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-resFile")) {      resFileName=args[++i];
    } else     if (args[i].startsWith("-w")) {      testType=TEST_TYPE_WRITE;    } else     if (args[i].startsWith("-clean")) {      testType=TEST_TYPE_CLEANUP;    } else     if (args[i].startsWith("-seq")) {      isSequential=true;    } else     if (args[i].equals("-nrFiles")) {      nrFiles=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-fileSize")) {      fileSize=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-bufferSize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-resFile")) {      resFileName=args[++i];    }  }  LOG.info("nrFiles = " + nrFiles);
  float sqrate=0;  String line;  while ((line=lines.readLine()) != null) {    StringTokenizer tokens=new StringTokenizer(line," \t\n\r\f%");    String attr=tokens.nextToken();    if (attr.endsWith(":tasks"))     tasks=Long.parseLong(tokens.nextToken()); else     if (attr.endsWith(":size"))     size=Long.parseLong(tokens.nextToken()); else     if (attr.endsWith(":time"))     time=Long.parseLong(tokens.nextToken()); else     if (attr.endsWith(":rate"))     rate=Float.parseFloat(tokens.nextToken()); else     if (attr.endsWith(":sqrate"))     sqrate=Float.parseFloat(tokens.nextToken());  }  double med=rate / 1000 / tasks;  double stdDev=Math.sqrt(Math.abs(sqrate / 1000 / tasks - med * med));  String resultLines[]={"----- DFSCIOTest ----- : " + ((testType == TEST_TYPE_WRITE) ? "write" : (testType == TEST_TYPE_READ) ? "read" : "unknown"),"           Date & time: " + new Date(System.currentTimeMillis()),"       Number of files: " + tasks,"Total MBytes processed: " + size / MEGA,"     Throughput mb/sec: " + size * 1000.0 / (time * MEGA),"Average IO rate mb/sec: " + med," Std IO rate deviation: " + stdDev,"    Test exec time sec: " + (float)execTime / 1000,""};  PrintStream res=new PrintStream(new FileOutputStream(new File(resFileName),true));  for (int i=0; i < resultLines.length; i++) {
  if (args.length == 1 && args[0].startsWith("-h")) {    System.err.println(usage);    System.exit(-1);  }  for (int i=0; i < args.length; i++) {    if (args[i].equals("-root")) {      rootName=args[++i];    } else     if (args[i].startsWith("-clean")) {      testType=TEST_TYPE_CLEANUP;    } else     if (args[i].equals("-bufferSize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-resFile")) {      resFileName=args[++i];    } else     if (args[i].startsWith("-stat")) {      viewStats=true;    }  }  LOG.info("root = " + rootName);
  try {    for (int i=start; i < end; i++) {      String name=getFileName(i);      Path controlFile=new Path(INPUT_DIR,"in_file_" + name);      SequenceFile.Writer writer=null;      try {        writer=SequenceFile.createWriter(fs,fs.getConf(),controlFile,Text.class,LongWritable.class,CompressionType.NONE);        String logFile=jhLogFiles[i].getPath().toString();        writer.append(new Text(logFile),new LongWritable(0));      } catch (      Exception e) {        throw new IOException(e);      } finally {        if (writer != null)         writer.close();        writer=null;      }    }  } catch (  IOException ex) {
    int start=0;    int step=jhLogFiles.length / NUM_CREATE_THREADS + ((jhLogFiles.length % NUM_CREATE_THREADS) > 0 ? 1 : 0);    FileCreateDaemon[] daemons=new FileCreateDaemon[NUM_CREATE_THREADS];    numRunningThreads=0;    for (int tIdx=0; tIdx < NUM_CREATE_THREADS && start < jhLogFiles.length; tIdx++) {      int end=Math.min(start + step,jhLogFiles.length);      daemons[tIdx]=new FileCreateDaemon(fs,start,end);      start+=step;      numRunningThreads++;    }    for (int tIdx=0; tIdx < numRunningThreads; tIdx++) {      daemons[tIdx].start();    }  }  finally {    int prevValue=0;    while (numFinishedThreads < numRunningThreads) {      if (prevValue < numFinishedThreads) {
private static void createControlFile(FileSystem fs,Path jhLogDir) throws IOException {
private static void createControlFile(FileSystem fs,Path jhLogDir) throws IOException {  LOG.info("creating control file: JH log dir = " + jhLogDir);  FileCreateDaemon.createControlFile(fs,jhLogDir);
@SuppressWarnings("deprecation") private void createControlFile(FileSystem fs,long nrBytes,int nrFiles) throws IOException {
@Override public int run(String[] args) throws IOException {  TestType testType=null;  int bufferSize=DEFAULT_BUFFER_SIZE;  long nrBytes=1 * MEGA;  String erasureCodePolicyName=null;  int nrFiles=1;  long skipSize=0;  String resFileName=DEFAULT_RES_FILE_NAME;  String compressionClass=null;  String storagePolicy=null;  boolean isSequential=false;  String version=TestDFSIO.class.getSimpleName() + ".1.8";
      compressionClass=args[++i];    } else     if (args[i].equalsIgnoreCase("-nrfiles")) {      nrFiles=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-filesize") || args[i].equalsIgnoreCase("-size")) {      nrBytes=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-skipsize")) {      skipSize=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-buffersize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-resfile")) {      resFileName=args[++i];    } else     if (args[i].equalsIgnoreCase("-storagePolicy")) {      storagePolicy=args[++i];    } else     if (args[i].equalsIgnoreCase("-erasureCodePolicy")) {      erasureCodePolicyName=args[++i];
      compressionClass=args[++i];    } else     if (args[i].equalsIgnoreCase("-nrfiles")) {      nrFiles=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-filesize") || args[i].equalsIgnoreCase("-size")) {      nrBytes=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-skipsize")) {      skipSize=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-buffersize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-resfile")) {      resFileName=args[++i];    } else     if (args[i].equalsIgnoreCase("-storagePolicy")) {      storagePolicy=args[++i];    } else     if (args[i].equalsIgnoreCase("-erasureCodePolicy")) {      erasureCodePolicyName=args[++i];
    } else     if (args[i].equalsIgnoreCase("-nrfiles")) {      nrFiles=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-filesize") || args[i].equalsIgnoreCase("-size")) {      nrBytes=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-skipsize")) {      skipSize=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-buffersize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-resfile")) {      resFileName=args[++i];    } else     if (args[i].equalsIgnoreCase("-storagePolicy")) {      storagePolicy=args[++i];    } else     if (args[i].equalsIgnoreCase("-erasureCodePolicy")) {      erasureCodePolicyName=args[++i];    } else {
      nrFiles=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-filesize") || args[i].equalsIgnoreCase("-size")) {      nrBytes=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-skipsize")) {      skipSize=parseSize(args[++i]);    } else     if (args[i].equalsIgnoreCase("-buffersize")) {      bufferSize=Integer.parseInt(args[++i]);    } else     if (args[i].equalsIgnoreCase("-resfile")) {      resFileName=args[++i];    } else     if (args[i].equalsIgnoreCase("-storagePolicy")) {      storagePolicy=args[++i];    } else     if (args[i].equalsIgnoreCase("-erasureCodePolicy")) {      erasureCodePolicyName=args[++i];    } else {      System.err.println("Illegal argument: " + args[i]);
  boolean isValid=false;  for (  ErasureCodingPolicyInfo ec : list) {    if (erasureCodePolicyName.equals(ec.getPolicy().getName())) {      isValid=true;      break;    }  }  if (!isValid) {    System.out.println("Invalid erasure code policy: " + erasureCodePolicyName);    System.out.println("Current supported erasure code policy list: ");    for (    ErasureCodingPolicyInfo ec : list) {      System.out.println(ec.getPolicy().getName());    }    return false;  }  if (testType == TestType.TEST_TYPE_APPEND || testType == TestType.TEST_TYPE_TRUNCATE) {    System.out.println("So far append or truncate operation" + " does not support erasureCodePolicy");    return false;  }  config.set(ERASURE_CODE_POLICY_NAME_KEY,erasureCodePolicyName);
  Collection<BlockStoragePolicy> storagePolicies=((DistributedFileSystem)fs).getAllStoragePolicies();  try {    for (    BlockStoragePolicy policy : storagePolicies) {      if (policy.getName().equals(storagePolicy)) {        isValid=true;        break;      }    }  } catch (  Exception e) {    throw new IOException("Get block storage policies error: ",e);  }  if (!isValid) {    System.out.println("Invalid block storage policy: " + storagePolicy);    System.out.println("Current supported storage policy list: ");    for (    BlockStoragePolicy policy : storagePolicies) {      System.out.println(policy.getName());    }    return false;  }  config.set(STORAGE_POLICY_NAME_KEY,storagePolicy);
void createAndEnableECOnPath(FileSystem fs,Path path) throws IOException {  String erasureCodePolicyName=getConf().get(ERASURE_CODE_POLICY_NAME_KEY,null);  fs.mkdirs(path);  Collection<ErasureCodingPolicyInfo> list=((DistributedFileSystem)fs).getAllErasureCodingPolicies();  for (  ErasureCodingPolicyInfo info : list) {    final ErasureCodingPolicy ec=info.getPolicy();    if (erasureCodePolicyName.equals(ec.getName())) {      ((DistributedFileSystem)fs).setErasureCodingPolicy(path,ec.getName());
 else       if (attr.endsWith(":size"))       size=Long.parseLong(tokens.nextToken()); else       if (attr.endsWith(":time"))       time=Long.parseLong(tokens.nextToken()); else       if (attr.endsWith(":rate"))       rate=Float.parseFloat(tokens.nextToken()); else       if (attr.endsWith(":sqrate"))       sqrate=Float.parseFloat(tokens.nextToken());    }  }  finally {    if (in != null)     in.close();    if (lines != null)     lines.close();  }  double med=rate / 1000 / tasks;  double stdDev=Math.sqrt(Math.abs(sqrate / 1000 / tasks - med * med));  DecimalFormat df=new DecimalFormat("#.##");  String resultLines[]={"----- TestDFSIO ----- : " + testType,"            Date & time: " + new Date(System.currentTimeMillis()),"        Number of files: " + tasks," Total MBytes processed: " + df.format(toMB(size)),"      Throughput mb/sec: " + df.format(toMB(size) / msToSecs(time))," Average IO rate mb/sec: " + df.format(med),"  IO rate std deviation: " + df.format(stdDev),"     Test exec time sec: " + df.format(msToSecs(execTime)),""};  PrintStream res=null;  try {    res=new PrintStream(new FileOutputStream(new File(resFileName),true));    for (int i=0; i < resultLines.length; i++) {
public static void testFs(long megaBytes,int numFiles,long seed) throws Exception {  FileSystem fs=FileSystem.get(conf);  if (seed == 0)   seed=new Random().nextLong();
public static void createControlFile(FileSystem fs,long megaBytes,int numFiles,long seed) throws Exception {
    System.exit(-1);  }  for (int i=0; i < args.length; i++) {    if (args[i].equals("-files")) {      files=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-megaBytes")) {      megaBytes=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-noread")) {      noRead=true;    } else     if (args[i].equals("-nowrite")) {      noWrite=true;    } else     if (args[i].equals("-noseek")) {      noSeek=true;    } else     if (args[i].equals("-fastcheck")) {      fastCheck=true;    }  }  LOG.info("seed = " + seed);
  }  for (int i=0; i < args.length; i++) {    if (args[i].equals("-files")) {      files=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-megaBytes")) {      megaBytes=Integer.parseInt(args[++i]);    } else     if (args[i].equals("-noread")) {      noRead=true;    } else     if (args[i].equals("-nowrite")) {      noWrite=true;    } else     if (args[i].equals("-noseek")) {      noSeek=true;    } else     if (args[i].equals("-fastcheck")) {      fastCheck=true;    }  }  LOG.info("seed = " + seed);  LOG.info("files = " + files);
static void runTestCache(int port) throws Exception {  Configuration conf=new Configuration();  MiniDFSCluster cluster=null;  try {    cluster=new MiniDFSCluster.Builder(conf).nameNodePort(port).numDataNodes(2).build();    URI uri=cluster.getFileSystem().getUri();
    long timeTaken=0, bytesAppended=0;    DataWriter writer=new DataWriter(getRandom());    LOG.info("Attempting to append to file at " + fn + " of size "+ Helper.toByteInfo(appendSize));{      long startTime=Timer.now();      os=fs.append(fn);      timeTaken+=Timer.elapsed(startTime);      GenerateOutput stats=writer.writeSegment(appendSize,os);      timeTaken+=stats.getTimeTaken();      bytesAppended+=stats.getBytesWritten();      startTime=Timer.now();      os.close();      os=null;      timeTaken+=Timer.elapsed(startTime);    }    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.BYTES_WRITTEN,bytesAppended));    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.OK_TIME_TAKEN,timeTaken));
  if (cfg == null) {    return;  }  LOG.info("Base directory = " + cfg.getBaseDirectory());  LOG.info("Data directory = " + cfg.getDataPath());  LOG.info("Output directory = " + cfg.getOutputPath());  LOG.info("Result file = " + cfg.getResultFile());  LOG.info("Grid queue = " + cfg.getQueueName());  LOG.info("Should exit on first error = " + cfg.shouldExitOnFirstError());{    String duration="Duration = ";    if (cfg.getDurationMilliseconds() == Integer.MAX_VALUE) {      duration+="unlimited";    } else {      duration+=cfg.getDurationMilliseconds() + " milliseconds";    }    LOG.info(duration);  }  LOG.info("Map amount = " + cfg.getMapAmount());
    return;  }  LOG.info("Base directory = " + cfg.getBaseDirectory());  LOG.info("Data directory = " + cfg.getDataPath());  LOG.info("Output directory = " + cfg.getOutputPath());  LOG.info("Result file = " + cfg.getResultFile());  LOG.info("Grid queue = " + cfg.getQueueName());  LOG.info("Should exit on first error = " + cfg.shouldExitOnFirstError());{    String duration="Duration = ";    if (cfg.getDurationMilliseconds() == Integer.MAX_VALUE) {      duration+="unlimited";    } else {      duration+=cfg.getDurationMilliseconds() + " milliseconds";    }    LOG.info(duration);  }  LOG.info("Map amount = " + cfg.getMapAmount());  LOG.info("Reducer amount = " + cfg.getReducerAmount());
  }  LOG.info("Base directory = " + cfg.getBaseDirectory());  LOG.info("Data directory = " + cfg.getDataPath());  LOG.info("Output directory = " + cfg.getOutputPath());  LOG.info("Result file = " + cfg.getResultFile());  LOG.info("Grid queue = " + cfg.getQueueName());  LOG.info("Should exit on first error = " + cfg.shouldExitOnFirstError());{    String duration="Duration = ";    if (cfg.getDurationMilliseconds() == Integer.MAX_VALUE) {      duration+="unlimited";    } else {      duration+=cfg.getDurationMilliseconds() + " milliseconds";    }    LOG.info(duration);  }  LOG.info("Map amount = " + cfg.getMapAmount());  LOG.info("Reducer amount = " + cfg.getReducerAmount());  LOG.info("Operation amount = " + cfg.getOpCount());
  LOG.info("Base directory = " + cfg.getBaseDirectory());  LOG.info("Data directory = " + cfg.getDataPath());  LOG.info("Output directory = " + cfg.getOutputPath());  LOG.info("Result file = " + cfg.getResultFile());  LOG.info("Grid queue = " + cfg.getQueueName());  LOG.info("Should exit on first error = " + cfg.shouldExitOnFirstError());{    String duration="Duration = ";    if (cfg.getDurationMilliseconds() == Integer.MAX_VALUE) {      duration+="unlimited";    } else {      duration+=cfg.getDurationMilliseconds() + " milliseconds";    }    LOG.info(duration);  }  LOG.info("Map amount = " + cfg.getMapAmount());  LOG.info("Reducer amount = " + cfg.getReducerAmount());  LOG.info("Operation amount = " + cfg.getOpCount());
    String duration="Duration = ";    if (cfg.getDurationMilliseconds() == Integer.MAX_VALUE) {      duration+="unlimited";    } else {      duration+=cfg.getDurationMilliseconds() + " milliseconds";    }    LOG.info(duration);  }  LOG.info("Map amount = " + cfg.getMapAmount());  LOG.info("Reducer amount = " + cfg.getReducerAmount());  LOG.info("Operation amount = " + cfg.getOpCount());  LOG.info("Total file limit = " + cfg.getTotalFiles());  LOG.info("Total dir file limit = " + cfg.getDirSize());{    String read="Read size = ";    if (cfg.shouldReadFullFile()) {      read+="entire file";    } else {
    }    LOG.info(duration);  }  LOG.info("Map amount = " + cfg.getMapAmount());  LOG.info("Reducer amount = " + cfg.getReducerAmount());  LOG.info("Operation amount = " + cfg.getOpCount());  LOG.info("Total file limit = " + cfg.getTotalFiles());  LOG.info("Total dir file limit = " + cfg.getDirSize());{    String read="Read size = ";    if (cfg.shouldReadFullFile()) {      read+="entire file";    } else {      read+=cfg.getReadSize() + " bytes";    }    LOG.info(read);  }{    String write="Write size = ";    if (cfg.shouldWriteUseBlockSize()) {      write+="blocksize";
  LOG.info("Total file limit = " + cfg.getTotalFiles());  LOG.info("Total dir file limit = " + cfg.getDirSize());{    String read="Read size = ";    if (cfg.shouldReadFullFile()) {      read+="entire file";    } else {      read+=cfg.getReadSize() + " bytes";    }    LOG.info(read);  }{    String write="Write size = ";    if (cfg.shouldWriteUseBlockSize()) {      write+="blocksize";    } else {      write+=cfg.getWriteSize() + " bytes";    }    LOG.info(write);  }{    String append="Append size = ";
    if (cfg.shouldReadFullFile()) {      read+="entire file";    } else {      read+=cfg.getReadSize() + " bytes";    }    LOG.info(read);  }{    String write="Write size = ";    if (cfg.shouldWriteUseBlockSize()) {      write+="blocksize";    } else {      write+=cfg.getWriteSize() + " bytes";    }    LOG.info(write);  }{    String append="Append size = ";    if (cfg.shouldAppendUseBlockSize()) {      append+="blocksize";    } else {
    } else {      read+=cfg.getReadSize() + " bytes";    }    LOG.info(read);  }{    String write="Write size = ";    if (cfg.shouldWriteUseBlockSize()) {      write+="blocksize";    } else {      write+=cfg.getWriteSize() + " bytes";    }    LOG.info(write);  }{    String append="Append size = ";    if (cfg.shouldAppendUseBlockSize()) {      append+="blocksize";    } else {      append+=cfg.getAppendSize() + " bytes";    }    LOG.info(append);
      read+=cfg.getReadSize() + " bytes";    }    LOG.info(read);  }{    String write="Write size = ";    if (cfg.shouldWriteUseBlockSize()) {      write+="blocksize";    } else {      write+=cfg.getWriteSize() + " bytes";    }    LOG.info(write);  }{    String append="Append size = ";    if (cfg.shouldAppendUseBlockSize()) {      append+="blocksize";    } else {      append+=cfg.getAppendSize() + " bytes";    }    LOG.info(append);  }{    String bsize="Block size = ";
  FSDataOutputStream os=null;  try {    Path fn=getCreateFile();    Range<Long> writeSizeRange=getConfig().getWriteSize();    long writeSize=0;    long blockSize=determineBlockSize();    short replicationAmount=determineReplication();    if (getConfig().shouldWriteUseBlockSize()) {      writeSizeRange=getConfig().getBlockSize();    }    writeSize=Range.betweenPositive(getRandom(),writeSizeRange);    long bytesWritten=0;    long timeTaken=0;    int bufSize=getBufferSize();    boolean overWrite=false;    DataWriter writer=new DataWriter(getRandom());
    long timeTaken=0;    int bufSize=getBufferSize();    boolean overWrite=false;    DataWriter writer=new DataWriter(getRandom());    LOG.info("Attempting to create file at " + fn + " of size "+ Helper.toByteInfo(writeSize)+ " using blocksize "+ Helper.toByteInfo(blockSize)+ " and replication amount "+ replicationAmount);{      long startTime=Timer.now();      os=fs.create(fn,overWrite,bufSize,replicationAmount,blockSize);      timeTaken+=Timer.elapsed(startTime);      GenerateOutput stats=writer.writeSegment(writeSize,os);      bytesWritten+=stats.getBytesWritten();      timeTaken+=stats.getTimeTaken();      startTime=Timer.now();      os.close();      os=null;      timeTaken+=Timer.elapsed(startTime);
@Override List<OperationOutput> run(FileSystem fs){  List<OperationOutput> out=super.run(fs);  try {    Path fn=getDeleteFile();    long timeTaken=0;    boolean deleteStatus=false;{      long startTime=Timer.now();      deleteStatus=fs.delete(fn,false);      timeTaken=Timer.elapsed(startTime);    }    if (!deleteStatus) {      out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.FAILURES,1L));      LOG.info("Could not delete " + fn);    } else {      out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.OK_TIME_TAKEN,timeTaken));      out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.SUCCESSES,1L));
    Range<Long> readSizeRange=getConfig().getReadSize();    long readSize=0;    String readStrAm="";    if (getConfig().shouldReadFullFile()) {      readSize=Long.MAX_VALUE;      readStrAm="full file";    } else {      readSize=Range.betweenPositive(getRandom(),readSizeRange);      readStrAm=Helper.toByteInfo(readSize);    }    long timeTaken=0;    long chunkSame=0;    long chunkDiff=0;    long bytesRead=0;    long startTime=0;    DataVerifier vf=new DataVerifier();
{      startTime=Timer.now();      is=fs.open(fn);      timeTaken+=Timer.elapsed(startTime);      VerifyOutput vo=vf.verifyFile(readSize,is);      timeTaken+=vo.getReadTime();      chunkSame+=vo.getChunksSame();      chunkDiff+=vo.getChunksDifferent();      bytesRead+=vo.getBytesRead();      startTime=Timer.now();      is.close();      is=null;      timeTaken+=Timer.elapsed(startTime);    }    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.OK_TIME_TAKEN,timeTaken));    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.BYTES_READ,bytesRead));    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.SUCCESSES,1L));
private void writeMessage(String msg,PrintWriter os){
private void logAndSetStatus(Reporter r,String msg){  r.setStatus(msg);
private void logAndSetStatus(Reporter r,String msg){  r.setStatus(msg);
    parsedOpts=argHolder.parse();    if (parsedOpts.shouldOutputHelp()) {      parsedOpts.outputHelp();      return 1;    }  } catch (  Exception e) {    LOG.error("Unable to parse arguments due to error: ",e);    return 1;  }  LOG.info("Running with option list " + Helper.stringifyArray(args," "));  ConfigExtractor config=null;  try {    ConfigMerger cfgMerger=new ConfigMerger();    Configuration cfg=cfgMerger.getMerged(parsedOpts,new Configuration(base));    if (cfg != null) {      config=new ConfigExtractor(cfg);    }  } catch (  Exception e) {
  }  LOG.info("Running with option list " + Helper.stringifyArray(args," "));  ConfigExtractor config=null;  try {    ConfigMerger cfgMerger=new ConfigMerger();    Configuration cfg=cfgMerger.getMerged(parsedOpts,new Configuration(base));    if (cfg != null) {      config=new ConfigExtractor(cfg);    }  } catch (  Exception e) {    LOG.error("Unable to merge config due to error: ",e);    return 1;  }  if (config == null) {    LOG.error("Unable to merge config & options!");    return 1;  }  try {    LOG.info("Options are:");
    }  } catch (  Exception e) {    LOG.error("Unable to merge config due to error: ",e);    return 1;  }  if (config == null) {    LOG.error("Unable to merge config & options!");    return 1;  }  try {    LOG.info("Options are:");    ConfigExtractor.dumpOptions(config);  } catch (  Exception e) {    LOG.error("Unable to dump options due to error: ",e);    return 1;  }  boolean jobOk=false;  try {    LOG.info("Running job:");
  if (config == null) {    LOG.error("Unable to merge config & options!");    return 1;  }  try {    LOG.info("Options are:");    ConfigExtractor.dumpOptions(config);  } catch (  Exception e) {    LOG.error("Unable to dump options due to error: ",e);    return 1;  }  boolean jobOk=false;  try {    LOG.info("Running job:");    runJob(config);    jobOk=true;  } catch (  Exception e) {
  } catch (  Exception e) {    LOG.error("Unable to dump options due to error: ",e);    return 1;  }  boolean jobOk=false;  try {    LOG.info("Running job:");    runJob(config);    jobOk=true;  } catch (  Exception e) {    LOG.error("Unable to run job due to error: ",e);  }  if (jobOk) {    try {      LOG.info("Reporting on job:");      writeReport(config);    } catch (    Exception e) {
private void writeReport(ConfigExtractor cfg) throws Exception {  Path dn=cfg.getOutputPath();
            List<OperationOutput> opList=splitTypes.get(op);            if (opList == null) {              opList=new ArrayList<OperationOutput>();            }            opList.add(data);            splitTypes.put(op,opList);          } else {            noOperations.add(data);          }        } else {          throw new IOException("Unparseable line " + line);        }      }      fileReader.close();      fileReader=null;    }    File resFile=null;    if (cfg.getResultFile() != null) {      resFile=new File(cfg.getResultFile());    }    if (resFile != null) {
private void cleanup(ConfigExtractor cfg) throws IOException {  Path base=cfg.getBaseDirectory();  if (base != null) {
private void rDelete(File place) throws Exception {  if (place.isFile()) {
@Test public void testDataWriting() throws Exception {  long byteAm=100;  File fn=getTestFile();  DataWriter writer=new DataWriter(rnd);  FileOutputStream fs=new FileOutputStream(fn);  GenerateOutput ostat=writer.writeSegment(byteAm,fs);
@Test public void testDataWriting() throws Exception {  long byteAm=100;  File fn=getTestFile();  DataWriter writer=new DataWriter(rnd);  FileOutputStream fs=new FileOutputStream(fn);  GenerateOutput ostat=writer.writeSegment(byteAm,fs);  LOG.info(ostat.toString());  fs.close();  assertTrue(ostat.getBytesWritten() == byteAm);  DataVerifier vf=new DataVerifier();  FileInputStream fin=new FileInputStream(fn);  VerifyOutput vfout=vf.verifyFile(byteAm,new DataInputStream(fin));
    boolean waitOnTruncate=getConfig().shouldWaitOnTruncate();    long currentSize=fs.getFileStatus(fn).getLen();    Range<Long> truncateSizeRange=getConfig().getTruncateSize();    if (getConfig().shouldTruncateUseBlockSize()) {      truncateSizeRange=getConfig().getBlockSize();    }    long truncateSize=Math.max(0L,currentSize - Range.betweenPositive(getRandom(),truncateSizeRange));    long timeTaken=0;    LOG.info("Attempting to truncate file at " + fn + " to size "+ Helper.toByteInfo(truncateSize));{      long startTime=Timer.now();      boolean completed=fs.truncate(fn,truncateSize);      if (!completed && waitOnTruncate)       waitForRecovery(fs,fn,truncateSize);      timeTaken+=Timer.elapsed(startTime);    }    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.BYTES_WRITTEN,0));    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.OK_TIME_TAKEN,timeTaken));    out.add(new OperationOutput(OutputType.LONG,getType(),ReportWriter.SUCCESSES,1L));
private void waitForRecovery(FileSystem fs,Path fn,long newLength) throws IOException {
private void configureOperations(ConfigExtractor cfg){  operations=new TreeMap<OperationType,OperationInfo>();  Map<OperationType,OperationData> opinfo=cfg.getOperations();  int totalAm=cfg.getOpCount();  int opsLeft=totalAm;  NumberFormat formatter=Formatter.getPercentFormatter();  for (  final OperationType type : opinfo.keySet()) {    OperationData opData=opinfo.get(type);    OperationInfo info=new OperationInfo();    info.distribution=opData.getDistribution();    int amLeft=determineHowMany(totalAm,opData,type);    opsLeft-=amLeft;
    opsLeft-=amLeft;    LOG.info(type.name() + " has " + amLeft+ " initial operations out of "+ totalAm+ " for its ratio "+ formatter.format(opData.getPercent()));    info.amountLeft=amLeft;    Operation op=factory.getOperation(type);    if (op != null) {      Observer fn=new Observer(){        public void notifyFinished(        Operation op){          OperationInfo opInfo=operations.get(type);          if (opInfo != null) {            --opInfo.amountLeft;          }        }        public void notifyStarting(        Operation op){        }      };      info.operation=new ObserveableOp(op,fn);      operations.put(type,info);    }  }  if (opsLeft > 0) {
 else     if (args[i].equals("-numberOfFiles")) {      checkArgs(i + 1,args.length);      numberOfFiles=Long.parseLong(args[++i]);    } else     if (args[i].equals("-replicationFactorPerFile")) {      checkArgs(i + 1,args.length);      replicationFactorPerFile=Short.parseShort(args[++i]);    } else     if (args[i].equals("-baseDir")) {      checkArgs(i + 1,args.length);      baseDir=args[++i];    } else     if (args[i].equals("-readFileAfterOpen")) {      checkArgs(i + 1,args.length);      readFileAfterOpen=Boolean.parseBoolean(args[++i]);    } else     if (args[i].equals("-help")) {      displayUsage();      isHelpMessage=true;
      checkArgs(i + 1,args.length);      numberOfFiles=Long.parseLong(args[++i]);    } else     if (args[i].equals("-replicationFactorPerFile")) {      checkArgs(i + 1,args.length);      replicationFactorPerFile=Short.parseShort(args[++i]);    } else     if (args[i].equals("-baseDir")) {      checkArgs(i + 1,args.length);      baseDir=args[++i];    } else     if (args[i].equals("-readFileAfterOpen")) {      checkArgs(i + 1,args.length);      readFileAfterOpen=Boolean.parseBoolean(args[++i]);    } else     if (args[i].equals("-help")) {      displayUsage();      isHelpMessage=true;    }  }  LOG.info("Test Inputs: ");
      numberOfFiles=Long.parseLong(args[++i]);    } else     if (args[i].equals("-replicationFactorPerFile")) {      checkArgs(i + 1,args.length);      replicationFactorPerFile=Short.parseShort(args[++i]);    } else     if (args[i].equals("-baseDir")) {      checkArgs(i + 1,args.length);      baseDir=args[++i];    } else     if (args[i].equals("-readFileAfterOpen")) {      checkArgs(i + 1,args.length);      readFileAfterOpen=Boolean.parseBoolean(args[++i]);    } else     if (args[i].equals("-help")) {      displayUsage();      isHelpMessage=true;    }  }  LOG.info("Test Inputs: ");  LOG.info("           Test Operation: " + operation);
    } else     if (args[i].equals("-replicationFactorPerFile")) {      checkArgs(i + 1,args.length);      replicationFactorPerFile=Short.parseShort(args[++i]);    } else     if (args[i].equals("-baseDir")) {      checkArgs(i + 1,args.length);      baseDir=args[++i];    } else     if (args[i].equals("-readFileAfterOpen")) {      checkArgs(i + 1,args.length);      readFileAfterOpen=Boolean.parseBoolean(args[++i]);    } else     if (args[i].equals("-help")) {      displayUsage();      isHelpMessage=true;    }  }  LOG.info("Test Inputs: ");  LOG.info("           Test Operation: " + operation);  LOG.info("               Start time: " + sdf.format(new Date(startTime)));
  Path tempDir=new Path(dir,"tmp");  fs.delete(dir,true);  FileInputFormat.setInputPaths(job,dir);  fs.mkdirs(tempDir);  LongWritable tkey=new LongWritable();  Text tval=new Text();  SequenceFile.Writer writer=SequenceFile.createWriter(fs,job,file,LongWritable.class,Text.class,compressionType,new DefaultCodec());  try {    for (int i=0; i < RECORDS; ++i) {      tkey.set(1234);      tval.set("valuevaluevaluevaluevaluevaluevaluevaluevaluevaluevalue");      writer.append(tkey,tval);    }  }  finally {    writer.close();  }  long fileLength=fs.getFileStatus(file).getLen();
private static void createBigMapInputFile(Configuration conf,FileSystem fs,Path dir,long fileSizeInMB) throws IOException {  if (fs.exists(dir)) {    FileStatus[] list=fs.listStatus(dir);    if (list.length > 0) {      throw new IOException("Input path: " + dir + " already exists... ");    }  }  Path file=new Path(dir,"part-0");  SequenceFile.Writer writer=SequenceFile.createWriter(fs,conf,file,BytesWritable.class,BytesWritable.class,CompressionType.NONE);  long numBytesToWrite=fileSizeInMB * 1024 * 1024;  int minKeySize=conf.getInt(MIN_KEY,10);  ;  int keySizeRange=conf.getInt(MAX_KEY,1000) - minKeySize;  int minValueSize=conf.getInt(MIN_VALUE,0);  int valueSizeRange=conf.getInt(MAX_VALUE,20000) - minValueSize;  BytesWritable randomKey=new BytesWritable();  BytesWritable randomValue=new BytesWritable();
public void generateTextFile(FileSystem fs,Path inputFile,long numLines,Order sortOrder) throws IOException {
private ArrayList<Long> runJobInSequence(JobConf masterJobConf,int numRuns) throws IOException {  Random rand=new Random();  ArrayList<Long> execTimes=new ArrayList<Long>();  for (int i=0; i < numRuns; i++) {    JobConf jobConf=new JobConf(masterJobConf);    jobConf.setJar(masterJobConf.getJar());    FileOutputFormat.setOutputPath(jobConf,new Path(OUTPUT_DIR,"output_" + rand.nextInt()));
private void checkJobExitStatus(int status,String jobName){  if (status != 0) {
private void runTest(final JobClient jc,final Configuration conf,final String jobClass,final String[] args,KillTaskThread killTaskThread,KillTrackerThread killTrackerThread) throws Exception {  Thread t=new Thread("Job Test"){    public void run(){      try {        Class<?> jobClassObj=conf.getClassByName(jobClass);        int status=ToolRunner.run(conf,(Tool)(jobClassObj.newInstance()),args);        checkJobExitStatus(status,jobClass);      } catch (      Exception e) {        LOG.error("JOB " + jobClass + " failed to run");        System.exit(-1);      }    }  };  t.setDaemon(true);  t.start();  JobStatus[] jobs;  while ((jobs=jc.jobsToComplete()).length == 0) {
        int status=ToolRunner.run(conf,(Tool)(jobClassObj.newInstance()),args);        checkJobExitStatus(status,jobClass);      } catch (      Exception e) {        LOG.error("JOB " + jobClass + " failed to run");        System.exit(-1);      }    }  };  t.setDaemon(true);  t.start();  JobStatus[] jobs;  while ((jobs=jc.jobsToComplete()).length == 0) {    LOG.info("Waiting for the job " + jobClass + " to start");    Thread.sleep(1000);  }  JobID jobId=jobs[jobs.length - 1].getJobID();  RunningJob rJob=jc.getJob(jobId);  if (rJob.isComplete()) {
 catch (      Exception e) {        LOG.error("JOB " + jobClass + " failed to run");        System.exit(-1);      }    }  };  t.setDaemon(true);  t.start();  JobStatus[] jobs;  while ((jobs=jc.jobsToComplete()).length == 0) {    LOG.info("Waiting for the job " + jobClass + " to start");    Thread.sleep(1000);  }  JobID jobId=jobs[jobs.length - 1].getJobID();  RunningJob rJob=jc.getJob(jobId);  if (rJob.isComplete()) {    LOG.error("The last job returned by the querying " + "JobTracker is complete :" + rJob.getJobID() + " .Exiting the test");    System.exit(-1);
  int mapRecs=input.size() - mapperBadRecords.size();  assertEquals(counters.findCounter(TaskCounter.MAP_INPUT_RECORDS).getCounter(),mapRecs);  assertEquals(counters.findCounter(TaskCounter.MAP_OUTPUT_RECORDS).getCounter(),mapRecs);  int redRecs=mapRecs - redBadRecords.size();  assertEquals(counters.findCounter(TaskCounter.REDUCE_SKIPPED_RECORDS).getCounter(),redBadRecords.size());  assertEquals(counters.findCounter(TaskCounter.REDUCE_SKIPPED_GROUPS).getCounter(),redBadRecords.size());  assertEquals(counters.findCounter(TaskCounter.REDUCE_INPUT_GROUPS).getCounter(),redRecs);  assertEquals(counters.findCounter(TaskCounter.REDUCE_INPUT_RECORDS).getCounter(),redRecs);  assertEquals(counters.findCounter(TaskCounter.REDUCE_OUTPUT_RECORDS).getCounter(),redRecs);  Path skipDir=SkipBadRecords.getSkipOutputPath(conf);  assertNotNull(skipDir);  Path[] skips=FileUtil.stat2Paths(getFileSystem().listStatus(skipDir));  List<String> mapSkipped=new ArrayList<String>();  List<String> redSkipped=new ArrayList<String>();  for (  Path skipPath : skips) {
  assertEquals(counters.findCounter(TaskCounter.REDUCE_INPUT_RECORDS).getCounter(),redRecs);  assertEquals(counters.findCounter(TaskCounter.REDUCE_OUTPUT_RECORDS).getCounter(),redRecs);  Path skipDir=SkipBadRecords.getSkipOutputPath(conf);  assertNotNull(skipDir);  Path[] skips=FileUtil.stat2Paths(getFileSystem().listStatus(skipDir));  List<String> mapSkipped=new ArrayList<String>();  List<String> redSkipped=new ArrayList<String>();  for (  Path skipPath : skips) {    LOG.info("skipPath: " + skipPath);    SequenceFile.Reader reader=new SequenceFile.Reader(getFileSystem(),skipPath,conf);    Object key=ReflectionUtils.newInstance(reader.getKeyClass(),conf);    Object value=ReflectionUtils.newInstance(reader.getValueClass(),conf);    key=reader.next(key);    while (key != null) {      value=reader.getCurrentValue(value);
    Object value=ReflectionUtils.newInstance(reader.getValueClass(),conf);    key=reader.next(key);    while (key != null) {      value=reader.getCurrentValue(value);      LOG.debug("key:" + key + " value:"+ value.toString());      if (skipPath.getName().contains("_r_")) {        redSkipped.add(value.toString());      } else {        mapSkipped.add(value.toString());      }      key=reader.next(key);    }    reader.close();  }  assertTrue(mapSkipped.containsAll(mapperBadRecords));  assertTrue(redSkipped.containsAll(redBadRecords));  Path[] outputFiles=FileUtil.stat2Paths(getFileSystem().listStatus(getOutputDir(),new Utils.OutputFileUtils.OutputFilesFilter()));  List<String> mapperOutput=getProcessed(input,mapperBadRecords);
    while (key != null) {      value=reader.getCurrentValue(value);      LOG.debug("key:" + key + " value:"+ value.toString());      if (skipPath.getName().contains("_r_")) {        redSkipped.add(value.toString());      } else {        mapSkipped.add(value.toString());      }      key=reader.next(key);    }    reader.close();  }  assertTrue(mapSkipped.containsAll(mapperBadRecords));  assertTrue(redSkipped.containsAll(redBadRecords));  Path[] outputFiles=FileUtil.stat2Paths(getFileSystem().listStatus(getOutputDir(),new Utils.OutputFileUtils.OutputFilesFilter()));  List<String> mapperOutput=getProcessed(input,mapperBadRecords);  LOG.debug("mapperOutput " + mapperOutput.size());  List<String> reducerOutput=getProcessed(mapperOutput,redBadRecords);
private void validateCounters(org.apache.hadoop.mapreduce.Counters counters){  Iterator<org.apache.hadoop.mapreduce.CounterGroup> it=counters.iterator();  while (it.hasNext()) {    org.apache.hadoop.mapreduce.CounterGroup group=it.next();
@Test(timeout=10000) public void testFormat() throws Exception {  JobConf job=new JobConf(conf);  Reporter reporter=Reporter.NULL;  Random random=new Random();  long seed=random.nextLong();
  Reporter reporter=Reporter.NULL;  Random random=new Random();  long seed=random.nextLong();  LOG.info("seed = " + seed);  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  InputFormat<IntWritable,BytesWritable> format=new CombineSequenceFileInputFormat<IntWritable,BytesWritable>();  IntWritable key=new IntWritable();  BytesWritable value=new BytesWritable();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / (SequenceFile.SYNC_INTERVAL / 20)) + 1;
  long seed=random.nextLong();  LOG.info("seed = " + seed);  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  InputFormat<IntWritable,BytesWritable> format=new CombineSequenceFileInputFormat<IntWritable,BytesWritable>();  IntWritable key=new IntWritable();  BytesWritable value=new BytesWritable();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / (SequenceFile.SYNC_INTERVAL / 20)) + 1;    LOG.info("splitting: requesting = " + numSplits);    InputSplit[] splits=format.getSplits(job,numSplits);
@Test(timeout=10000) public void testFormat() throws Exception {  JobConf job=new JobConf(defaultConf);  Random random=new Random();  long seed=random.nextLong();
  JobConf job=new JobConf(defaultConf);  Random random=new Random();  long seed=random.nextLong();  LOG.info("seed = " + seed);  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  CombineTextInputFormat format=new CombineTextInputFormat();  LongWritable key=new LongWritable();  Text value=new Text();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / 20) + 1;
  long seed=random.nextLong();  LOG.info("seed = " + seed);  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  CombineTextInputFormat format=new CombineTextInputFormat();  LongWritable key=new LongWritable();  Text value=new Text();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / 20) + 1;    LOG.info("splitting: requesting = " + numSplits);    InputSplit[] splits=format.getSplits(job,numSplits);
  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  CombineTextInputFormat format=new CombineTextInputFormat();  LongWritable key=new LongWritable();  Text value=new Text();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / 20) + 1;    LOG.info("splitting: requesting = " + numSplits);    InputSplit[] splits=format.getSplits(job,numSplits);    LOG.info("splitting: got =        " + splits.length);    assertEquals("We got more than one splits!",1,splits.length);    InputSplit split=splits[0];    assertEquals("It should be CombineFileSplit",CombineFileSplit.class,split.getClass());    BitSet bits=new BitSet(length);
  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / 20) + 1;    LOG.info("splitting: requesting = " + numSplits);    InputSplit[] splits=format.getSplits(job,numSplits);    LOG.info("splitting: got =        " + splits.length);    assertEquals("We got more than one splits!",1,splits.length);    InputSplit split=splits[0];    assertEquals("It should be CombineFileSplit",CombineFileSplit.class,split.getClass());    BitSet bits=new BitSet(length);    LOG.debug("split= " + split);    RecordReader<LongWritable,Text> reader=format.getRecordReader(split,job,voidReporter);    try {      int count=0;      while (reader.next(key,value)) {        int v=Integer.parseInt(value.toString());
@Test(timeout=5000) public void testNoRecordLength() throws IOException {  localFs.delete(workDir,true);  Path file=new Path(workDir,new String("testFormat.txt"));  createFile(file,null,10,10);  JobConf job=new JobConf(defaultConf);  FileInputFormat.setInputPaths(job,workDir);  FixedLengthInputFormat format=new FixedLengthInputFormat();  format.configure(job);  InputSplit splits[]=format.getSplits(job,1);  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      RecordReader<LongWritable,BytesWritable> reader=format.getRecordReader(split,job,voidReporter);    } catch (    IOException ioe) {      exceptionThrown=true;
  localFs.delete(workDir,true);  Path file=new Path(workDir,new String("testFormat.txt"));  createFile(file,null,10,10);  JobConf job=new JobConf(defaultConf);  FileInputFormat.setInputPaths(job,workDir);  FixedLengthInputFormat format=new FixedLengthInputFormat();  format.setRecordLength(job,0);  format.configure(job);  InputSplit splits[]=format.getSplits(job,1);  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      RecordReader<LongWritable,BytesWritable> reader=format.getRecordReader(split,job,voidReporter);    } catch (    IOException ioe) {      exceptionThrown=true;
  localFs.delete(workDir,true);  Path file=new Path(workDir,new String("testFormat.txt"));  createFile(file,null,10,10);  JobConf job=new JobConf(defaultConf);  FileInputFormat.setInputPaths(job,workDir);  FixedLengthInputFormat format=new FixedLengthInputFormat();  format.setRecordLength(job,-10);  format.configure(job);  InputSplit splits[]=format.getSplits(job,1);  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      RecordReader<LongWritable,BytesWritable> reader=format.getRecordReader(split,job,voidReporter);    } catch (    IOException ioe) {      exceptionThrown=true;
  Path file=new Path(workDir,fileName.toString());  int seed=new Random().nextInt();  LOG.info("Seed = " + seed);  Random random=new Random(seed);  int MAX_TESTS=20;  LongWritable key=new LongWritable();  BytesWritable value=new BytesWritable();  for (int i=0; i < MAX_TESTS; i++) {    LOG.info("----------------------------------------------------------");    int totalRecords=random.nextInt(999) + 1;    if (i == 8) {      totalRecords=0;    }    int recordLength=random.nextInt(1024 * 100) + 1;    if (i == 10) {      recordLength=1;
    int fileSize=(totalRecords * recordLength);    LOG.info("totalRecords=" + totalRecords + " recordLength="+ recordLength);    JobConf job=new JobConf(defaultConf);    if (codec != null) {      ReflectionUtils.setConf(codec,job);    }    ArrayList<String> recordList=createFile(file,codec,recordLength,totalRecords);    assertTrue(localFs.exists(file));    FixedLengthInputFormat.setRecordLength(job,recordLength);    int numSplits=1;    if (i > 0) {      if (i == (MAX_TESTS - 1)) {        numSplits=(int)(fileSize / Math.max(1,Math.floor(recordLength / 2)));      } else {        if (MAX_TESTS % i == 0) {          numSplits=fileSize / (fileSize - random.nextInt(fileSize));
  JobConf job=new JobConf(defaultConf);  format.setRecordLength(job,5);  FileInputFormat.setInputPaths(job,workDir);  if (codec != null) {    ReflectionUtils.setConf(codec,job);  }  format.configure(job);  writeFile(localFs,new Path(workDir,fileName.toString()),codec,"one  two  threefour five six  seveneightnine ten");  InputSplit[] splits=format.getSplits(job,100);  if (codec != null) {    assertEquals("compressed splits == 1",1,splits.length);  }  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      List<String> results=readSplit(format,split,job);    } catch (    IOException ioe) {
private void testSuccessfulJob(String filename,Class<? extends OutputCommitter> committer,String[] exclude) throws IOException {  JobConf jc=mr.createJobConf();  Path outDir=getNewOutputDir();  configureJob(jc,"job with cleanup()",1,0,outDir);  jc.setOutputCommitter(committer);  JobClient jobClient=new JobClient(jc);  RunningJob job=jobClient.submitJob(jc);  JobID id=job.getID();  job.waitForCompletion();
@Test public void testFormat() throws Exception {  JobConf job=new JobConf();  Path file=new Path(workDir,"test.txt");  Reporter reporter=Reporter.NULL;  int seed=new Random().nextInt();
@Test public void testFormat() throws Exception {  JobConf job=new JobConf();  Path file=new Path(workDir,"test.txt");  Reporter reporter=Reporter.NULL;  int seed=new Random().nextInt();  LOG.info("seed = " + seed);  Random random=new Random(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {    LOG.debug("creating; entries = " + length);    Writer writer=new OutputStreamWriter(localFs.create(file));    try {      for (int i=0; i < length; i++) {        writer.write(Integer.toString(i * 2));        writer.write("\t");        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    KeyValueTextInputFormat format=new KeyValueTextInputFormat();    format.configure(job);    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;
    Writer writer=new OutputStreamWriter(localFs.create(file));    try {      for (int i=0; i < length; i++) {        writer.write(Integer.toString(i * 2));        writer.write("\t");        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    KeyValueTextInputFormat format=new KeyValueTextInputFormat();    format.configure(job);    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;      LOG.debug("splitting: requesting = " + numSplits);      InputSplit[] splits=format.getSplits(job,numSplits);
        writer.write(Integer.toString(i * 2));        writer.write("\t");        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    KeyValueTextInputFormat format=new KeyValueTextInputFormat();    format.configure(job);    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;      LOG.debug("splitting: requesting = " + numSplits);      InputSplit[] splits=format.getSplits(job,numSplits);      LOG.debug("splitting: got =        " + splits.length);      BitSet bits=new BitSet(length);      for (int j=0; j < splits.length; j++) {
        assertEquals("reader class is KeyValueLineRecordReader.",KeyValueLineRecordReader.class,readerClass);        Text key=reader.createKey();        Class keyClass=key.getClass();        Text value=reader.createValue();        Class valueClass=value.getClass();        assertEquals("Key class is Text.",Text.class,keyClass);        assertEquals("Value class is Text.",Text.class,valueClass);        try {          int count=0;          while (reader.next(key,value)) {            int v=Integer.parseInt(value.toString());            LOG.debug("read " + v);            if (bits.get(v)) {              LOG.warn("conflict with " + v + " in split "+ j+ " at position "+ reader.getPos());            }            assertFalse("Key in multiple partitions.",bits.get(v));
private void verifyEntity(File entityFile,String eventId,boolean chkMetrics,boolean chkCfg,Set<String> cfgsToVerify,boolean checkIdPrefix) throws IOException {  BufferedReader reader=null;  String strLine;  try {    reader=new BufferedReader(new FileReader(entityFile));    long idPrefix=-1;    while ((strLine=reader.readLine()) != null) {      if (strLine.trim().length() > 0) {        org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity=FileSystemTimelineReaderImpl.getTimelineRecordFromJSON(strLine.trim(),org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity.class);
@BeforeClass public static void setup() throws IOException {  dfs=new MiniDFSCluster.Builder(conf).build();  fileSys=dfs.getFileSystem();  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
private Path initFiles(FileSystem fs,int numFiles,int numBytes) throws IOException {  Path dir=new Path(System.getProperty("test.build.data",".") + "/mapred");  Path multiFileDir=new Path(dir,"test.multifile");  fs.delete(multiFileDir,true);  fs.mkdirs(multiFileDir);
private Path initFiles(FileSystem fs,int numFiles,int numBytes) throws IOException {  Path dir=new Path(System.getProperty("test.build.data",".") + "/mapred");  Path multiFileDir=new Path(dir,"test.multifile");  fs.delete(multiFileDir,true);  fs.mkdirs(multiFileDir);  LOG.info("Creating " + numFiles + " file(s) in "+ multiFileDir);  for (int i=0; i < numFiles; i++) {    Path path=new Path(multiFileDir,"file_" + i);    FSDataOutputStream out=fs.create(path);    if (numBytes == -1) {      numBytes=rand.nextInt(MAX_BYTES);    }    for (int j=0; j < numBytes; j++) {      out.write(rand.nextInt());    }    out.close();    if (LOG.isDebugEnabled()) {
@Test public void testFormat() throws IOException {  LOG.info("Test started");  LOG.info("Max split count           = " + MAX_SPLIT_COUNT);  LOG.info("Split count increment     = " + SPLIT_COUNT_INCR);  LOG.info("Max bytes per file        = " + MAX_BYTES);  LOG.info("Max number of files       = " + MAX_NUM_FILES);  LOG.info("Number of files increment = " + NUM_FILES_INCR);  MultiFileInputFormat<Text,Text> format=new DummyMultiFileInputFormat();  FileSystem fs=FileSystem.getLocal(job);  for (int numFiles=1; numFiles < MAX_NUM_FILES; numFiles+=(NUM_FILES_INCR / 2) + rand.nextInt(NUM_FILES_INCR / 2)) {    Path dir=initFiles(fs,numFiles,-1);    BitSet bits=new BitSet(numFiles);    for (int i=1; i < MAX_SPLIT_COUNT; i+=rand.nextInt(SPLIT_COUNT_INCR) + 1) {
@Test public void testRegexFilter() throws Exception {  LOG.info("Testing Regex Filter with patter: \\A10*");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.RegexFilter.class);  SequenceFileInputFilter.RegexFilter.setPattern(job,"\\A10*");  fs.delete(inDir,true);  for (int length=1; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
@Test public void testPercentFilter() throws Exception {  LOG.info("Testing Percent Filter with frequency: 1000");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.PercentFilter.class);  SequenceFileInputFilter.PercentFilter.setFrequency(job,1000);  fs.delete(inDir,true);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
@Test public void testPercentFilter() throws Exception {  LOG.info("Testing Percent Filter with frequency: 1000");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.PercentFilter.class);  SequenceFileInputFilter.PercentFilter.setFrequency(job,1000);  fs.delete(inDir,true);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {    LOG.info("******Number of records: " + length);    createSequenceFile(length);    int count=countRecords(1);
@Test public void testMD5Filter() throws Exception {  LOG.info("Testing MD5 Filter with frequency: 1000");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.MD5Filter.class);  SequenceFileInputFilter.MD5Filter.setFrequency(job,1000);  fs.delete(inDir,true);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
  assertEquals(9,sr.getIndicesCount());  sr.add(new SortedRanges.Range(3,5));  assertEquals(9,sr.getIndicesCount());  sr.add(new SortedRanges.Range(7,1));  assertEquals(9,sr.getIndicesCount());  sr.add(new Range(1,12));  assertEquals(12,sr.getIndicesCount());  sr.add(new Range(7,9));  assertEquals(15,sr.getIndicesCount());  sr.add(new Range(31,10));  sr.add(new Range(51,10));  sr.add(new Range(66,10));  assertEquals(45,sr.getIndicesCount());  sr.add(new Range(21,50));  assertEquals(70,sr.getIndicesCount());
  assertEquals(19,sr.getIndicesCount());  sr.remove(new SortedRanges.Range(15,8));  assertEquals(13,sr.getIndicesCount());  sr.remove(new SortedRanges.Range(6,5));  assertEquals(8,sr.getIndicesCount());  sr.remove(new SortedRanges.Range(8,4));  assertEquals(7,sr.getIndicesCount());  sr.add(new Range(18,5));  assertEquals(12,sr.getIndicesCount());  sr.add(new Range(25,1));  assertEquals(13,sr.getIndicesCount());  sr.remove(new SortedRanges.Range(7,24));  assertEquals(4,sr.getIndicesCount());  sr.remove(new SortedRanges.Range(5,1));  assertEquals(3,sr.getIndicesCount());
  conf.setOutputKeyClass(LongWritable.class);  conf.setOutputValueClass(Text.class);  conf.setMapperClass(IdentityMapper.class);  conf.setReducerClass(IdentityReducer.class);  FileInputFormat.setInputPaths(conf,inDir);  FileOutputFormat.setOutputPath(conf,outDir);  conf.setNumMapTasks(numMaps);  conf.setNumReduceTasks(numReduces);  RunningJob runningJob=JobClient.runJob(conf);  try {    assertTrue(runningJob.isComplete());    assertTrue(runningJob.isSuccessful());    assertTrue("Output folder not found!",fs.exists(new Path("/testing/output/" + OUTPUT_FILENAME)));  } catch (  NullPointerException npe) {    fail("A NPE should not have happened.");
@Test(timeout=500000) public void testFormat() throws Exception {  JobConf job=new JobConf(defaultConf);  Path file=new Path(workDir,"test.txt");  Reporter reporter=Reporter.NULL;  int seed=new Random().nextInt();
@Test(timeout=500000) public void testFormat() throws Exception {  JobConf job=new JobConf(defaultConf);  Path file=new Path(workDir,"test.txt");  Reporter reporter=Reporter.NULL;  int seed=new Random().nextInt();  LOG.info("seed = " + seed);  Random random=new Random(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {    LOG.debug("creating; entries = " + length);    Writer writer=new OutputStreamWriter(localFs.create(file));    try {      for (int i=0; i < length; i++) {        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    TextInputFormat format=new TextInputFormat();    format.configure(job);    LongWritable key=new LongWritable();    Text value=new Text();    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;
    Writer writer=new OutputStreamWriter(localFs.create(file));    try {      for (int i=0; i < length; i++) {        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    TextInputFormat format=new TextInputFormat();    format.configure(job);    LongWritable key=new LongWritable();    Text value=new Text();    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;      LOG.debug("splitting: requesting = " + numSplits);      InputSplit[] splits=format.getSplits(job,numSplits);
      if (length == 0) {        assertEquals("Files of length 0 are not returned from FileInputFormat.getSplits().",1,splits.length);        assertEquals("Empty file length == 0",0,splits[0].getLength());      }      BitSet bits=new BitSet(length);      for (int j=0; j < splits.length; j++) {        LOG.debug("split[" + j + "]= "+ splits[j]);        RecordReader<LongWritable,Text> reader=format.getRecordReader(splits[j],job,reporter);        try {          int count=0;          while (reader.next(key,value)) {            int v=Integer.parseInt(value.toString());            LOG.debug("read " + v);            if (bits.get(v)) {              LOG.warn("conflict with " + v + " in split "+ j+ " at position "+ reader.getPos());            }            assertFalse("Key in multiple partitions.",bits.get(v));
  }  Path file=new Path(workDir,"test" + codec.getDefaultExtension());  FileSystem localFs=FileSystem.getLocal(conf);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(conf,workDir);  int length=250000;  LOG.info("creating; entries = " + length);  Writer writer=new OutputStreamWriter(codec.createOutputStream(localFs.create(file)));  try {    for (int i=0; i < length; i++) {      writer.write(Integer.toString(i));      writer.write("\n");    }  }  finally {    writer.close();  }  for (long splitpos=203418; splitpos < 203430; ++splitpos) {    TextInputFormat format=new TextInputFormat();
    conf.setLong("mapreduce.input.fileinputformat.split.minsize",splitpos);    LongWritable key=new LongWritable();    Text value=new Text();    InputSplit[] splits=format.getSplits(conf,2);    LOG.info("splitting: got =        " + splits.length);    BitSet bits=new BitSet(length);    for (int j=0; j < splits.length; j++) {      LOG.debug("split[" + j + "]= "+ splits[j]);      RecordReader<LongWritable,Text> reader=format.getRecordReader(splits[j],conf,Reporter.NULL);      try {        int counter=0;        while (reader.next(key,value)) {          int v=Integer.parseInt(value.toString());          LOG.debug("read " + v);          if (bits.get(v)) {
    Text value=new Text();    InputSplit[] splits=format.getSplits(conf,2);    LOG.info("splitting: got =        " + splits.length);    BitSet bits=new BitSet(length);    for (int j=0; j < splits.length; j++) {      LOG.debug("split[" + j + "]= "+ splits[j]);      RecordReader<LongWritable,Text> reader=format.getRecordReader(splits[j],conf,Reporter.NULL);      try {        int counter=0;        while (reader.next(key,value)) {          int v=Integer.parseInt(value.toString());          LOG.debug("read " + v);          if (bits.get(v)) {            LOG.warn("conflict with " + v + " in split "+ j+ " at position "+ reader.getPos());          }          assertFalse("Key in multiple partitions.",bits.get(v));
private void verifyPartitions(int length,int numSplits,Path file,CompressionCodec codec,JobConf conf) throws IOException {
private void verifyPartitions(int length,int numSplits,Path file,CompressionCodec codec,JobConf conf) throws IOException {  LOG.info("creating; entries = " + length);  Writer writer=new OutputStreamWriter(codec.createOutputStream(localFs.create(file)));  try {    for (int i=0; i < length; i++) {      writer.write(Integer.toString(i));      writer.write("\n");    }  }  finally {    writer.close();  }  TextInputFormat format=new TextInputFormat();  format.configure(conf);  LongWritable key=new LongWritable();  Text value=new Text();  LOG.info("splitting: requesting = " + numSplits);  InputSplit[] splits=format.getSplits(conf,numSplits);
  try {    for (int i=0; i < length; i++) {      writer.write(Integer.toString(i));      writer.write("\n");    }  }  finally {    writer.close();  }  TextInputFormat format=new TextInputFormat();  format.configure(conf);  LongWritable key=new LongWritable();  Text value=new Text();  LOG.info("splitting: requesting = " + numSplits);  InputSplit[] splits=format.getSplits(conf,numSplits);  LOG.info("splitting: got =        " + splits.length);  BitSet bits=new BitSet(length);  for (int j=0; j < splits.length; j++) {
  LOG.info("splitting: requesting = " + numSplits);  InputSplit[] splits=format.getSplits(conf,numSplits);  LOG.info("splitting: got =        " + splits.length);  BitSet bits=new BitSet(length);  for (int j=0; j < splits.length; j++) {    LOG.debug("split[" + j + "]= "+ splits[j]);    RecordReader<LongWritable,Text> reader=format.getRecordReader(splits[j],conf,Reporter.NULL);    try {      int counter=0;      while (reader.next(key,value)) {        int v=Integer.parseInt(value.toString());        LOG.debug("read " + v);        if (bits.get(v)) {          LOG.warn("conflict with " + v + " in split "+ j+ " at position "+ reader.getPos());        }        assertFalse("Key in multiple partitions.",bits.get(v));
  LOG.info("splitting: got =        " + splits.length);  BitSet bits=new BitSet(length);  for (int j=0; j < splits.length; j++) {    LOG.debug("split[" + j + "]= "+ splits[j]);    RecordReader<LongWritable,Text> reader=format.getRecordReader(splits[j],conf,Reporter.NULL);    try {      int counter=0;      while (reader.next(key,value)) {        int v=Integer.parseInt(value.toString());        LOG.debug("read " + v);        if (bits.get(v)) {          LOG.warn("conflict with " + v + " in split "+ j+ " at position "+ reader.getPos());        }        assertFalse("Key in multiple partitions.",bits.get(v));        bits.set(v);        counter++;
@Test(timeout=20000) public void testJobSubmissionFailure() throws Exception {  when(resourceMgrDelegate.submitApplication(any(ApplicationSubmissionContext.class))).thenReturn(appId);  ApplicationReport report=mock(ApplicationReport.class);  when(report.getApplicationId()).thenReturn(appId);  when(report.getDiagnostics()).thenReturn(failString);  when(report.getYarnApplicationState()).thenReturn(YarnApplicationState.FAILED);  when(resourceMgrDelegate.getApplicationReport(appId)).thenReturn(report);  Credentials credentials=new Credentials();  File jobxml=new File(testWorkDir,"job.xml");  OutputStream out=new FileOutputStream(jobxml);  conf.writeXml(out);  out.close();  try {    yarnRunner.submitJob(jobId,testWorkDir.getAbsolutePath().toString(),credentials);  } catch (  IOException io) {
  job.setOutputFormat(SequenceFileOutputFormat.class);  job.setMapperClass(Map.class);  job.setReducerClass(IdentityReducer.class);  job.setOutputKeyClass(BytesWritable.class);  job.setOutputValueClass(BytesWritable.class);  JobClient client=new JobClient(job);  ClusterStatus cluster=client.getClusterStatus();  long totalDataSize=dataSizePerMap * numMapsPerHost * cluster.getTaskTrackers();  job.set("test.tmb.bytes_per_map",String.valueOf(dataSizePerMap * 1024 * 1024));  job.setNumReduceTasks(0);  job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());  FileOutputFormat.setOutputPath(job,INPUT_DIR);  FileSystem fs=FileSystem.get(job);  fs.delete(BASE_DIR,true);  LOG.info("Generating random input for the benchmark");
  job.setMapperClass(Map.class);  job.setReducerClass(IdentityReducer.class);  job.setOutputKeyClass(BytesWritable.class);  job.setOutputValueClass(BytesWritable.class);  JobClient client=new JobClient(job);  ClusterStatus cluster=client.getClusterStatus();  long totalDataSize=dataSizePerMap * numMapsPerHost * cluster.getTaskTrackers();  job.set("test.tmb.bytes_per_map",String.valueOf(dataSizePerMap * 1024 * 1024));  job.setNumReduceTasks(0);  job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());  FileOutputFormat.setOutputPath(job,INPUT_DIR);  FileSystem fs=FileSystem.get(job);  fs.delete(BASE_DIR,true);  LOG.info("Generating random input for the benchmark");  LOG.info("Total data : " + totalDataSize + " mb");
  job.setOutputKeyClass(BytesWritable.class);  job.setOutputValueClass(BytesWritable.class);  JobClient client=new JobClient(job);  ClusterStatus cluster=client.getClusterStatus();  long totalDataSize=dataSizePerMap * numMapsPerHost * cluster.getTaskTrackers();  job.set("test.tmb.bytes_per_map",String.valueOf(dataSizePerMap * 1024 * 1024));  job.setNumReduceTasks(0);  job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());  FileOutputFormat.setOutputPath(job,INPUT_DIR);  FileSystem fs=FileSystem.get(job);  fs.delete(BASE_DIR,true);  LOG.info("Generating random input for the benchmark");  LOG.info("Total data : " + totalDataSize + " mb");  LOG.info("Data per map: " + dataSizePerMap + " mb");  LOG.info("Number of spills : " + numSpillsPerMap);
  job.setOutputValueClass(BytesWritable.class);  JobClient client=new JobClient(job);  ClusterStatus cluster=client.getClusterStatus();  long totalDataSize=dataSizePerMap * numMapsPerHost * cluster.getTaskTrackers();  job.set("test.tmb.bytes_per_map",String.valueOf(dataSizePerMap * 1024 * 1024));  job.setNumReduceTasks(0);  job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());  FileOutputFormat.setOutputPath(job,INPUT_DIR);  FileSystem fs=FileSystem.get(job);  fs.delete(BASE_DIR,true);  LOG.info("Generating random input for the benchmark");  LOG.info("Total data : " + totalDataSize + " mb");  LOG.info("Data per map: " + dataSizePerMap + " mb");  LOG.info("Number of spills : " + numSpillsPerMap);  LOG.info("Number of maps per host : " + numMapsPerHost);
    ClusterStatus cluster=client.getClusterStatus();    job.setNumMapTasks(numMapsPerHost * cluster.getTaskTrackers());    job.setNumReduceTasks(1);    int ioSortMb=(int)Math.ceil(FACTOR * dataSizePerMap);    job.set(JobContext.IO_SORT_MB,String.valueOf(ioSortMb));    fs=FileSystem.get(job);    LOG.info("Running sort with 1 spill per map");    long startTime=System.currentTimeMillis();    JobClient.runJob(job);    long endTime=System.currentTimeMillis();    LOG.info("Total time taken : " + String.valueOf(endTime - startTime) + " millisec");    fs.delete(OUTPUT_DIR,true);    JobConf spilledJob=new JobConf(job,ThreadedMapBenchmark.class);    ioSortMb=(int)Math.ceil(FACTOR * Math.ceil((double)dataSizePerMap / numSpillsPerMap));    spilledJob.set(JobContext.IO_SORT_MB,String.valueOf(ioSortMb));
public JobInfo parseHistoryFile(Path path) throws IOException {
public Configuration parseConfiguration(Path path) throws IOException {
    LOG.info(context.getTaskAttemptID().getTaskID() + " will process no jobs");  } else {    LOG.info(context.getTaskAttemptID().getTaskID() + " will process " + jobs.size()+ " jobs");  }  for (  JobFiles job : jobs) {    String jobIdStr=job.getJobId();    LOG.info("processing " + jobIdStr + "...");    JobId jobId=TypeConverter.toYarn(JobID.forName(jobIdStr));    ApplicationId appId=jobId.getAppId();    try {      Path historyFilePath=job.getJobHistoryFilePath();      Path confFilePath=job.getJobConfFilePath();      if ((historyFilePath == null) || (confFilePath == null)) {        continue;      }      JobInfo jobInfo=parser.parseHistoryFile(historyFilePath);      Configuration jobConf=parser.parseConfiguration(confFilePath);
    LOG.info(context.getTaskAttemptID().getTaskID() + " will process " + jobs.size()+ " jobs");  }  for (  JobFiles job : jobs) {    String jobIdStr=job.getJobId();    LOG.info("processing " + jobIdStr + "...");    JobId jobId=TypeConverter.toYarn(JobID.forName(jobIdStr));    ApplicationId appId=jobId.getAppId();    try {      Path historyFilePath=job.getJobHistoryFilePath();      Path confFilePath=job.getJobConfFilePath();      if ((historyFilePath == null) || (confFilePath == null)) {        continue;      }      JobInfo jobInfo=parser.parseHistoryFile(historyFilePath);      Configuration jobConf=parser.parseConfiguration(confFilePath);      LOG.info("parsed the job history file and the configuration file for job " + jobIdStr);      long totalTime=0;
      long totalTime=0;      Set<TimelineEntity> entitySet=converter.createTimelineEntities(jobInfo,jobConf);      LOG.info("converted them into timeline entities for job " + jobIdStr);      UserGroupInformation ugi=UserGroupInformation.getCurrentUser();      long startWrite=System.nanoTime();      try {switch (replayMode) {case JobHistoryFileReplayHelper.WRITE_ALL_AT_ONCE:          writeAllEntities(tlc,entitySet,ugi);        break;case JobHistoryFileReplayHelper.WRITE_PER_ENTITY:      writePerEntity(tlc,entitySet,ugi);    break;default:  break;}} catch (Exception e) {context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).increment(1);LOG.error("writing to the timeline service failed",e);
private void writePerEntity(TimelineClient tlc,Set<TimelineEntity> entitySet,UserGroupInformation ugi) throws IOException, YarnException {  for (  TimelineEntity entity : entitySet) {    tlc.putEntities(entity);
  } else {    LOG.info(context.getTaskAttemptID().getTaskID() + " will process " + jobs.size()+ " jobs");  }  for (  JobFiles job : jobs) {    String jobIdStr=job.getJobId();    if (job.getJobConfFilePath() == null || job.getJobHistoryFilePath() == null) {      LOG.info(jobIdStr + " missing either the job history file or the " + "configuration file. Skipping.");      continue;    }    LOG.info("processing " + jobIdStr + "...");    JobId jobId=TypeConverter.toYarn(JobID.forName(jobIdStr));    ApplicationId appId=jobId.getAppId();    AppLevelTimelineCollector collector=new AppLevelTimelineCollector(appId);    manager.putIfAbsent(appId,collector);    try {      JobInfo jobInfo=parser.parseHistoryFile(job.getJobHistoryFilePath());      Configuration jobConf=parser.parseConfiguration(job.getJobConfFilePath());
      LOG.info(jobIdStr + " missing either the job history file or the " + "configuration file. Skipping.");      continue;    }    LOG.info("processing " + jobIdStr + "...");    JobId jobId=TypeConverter.toYarn(JobID.forName(jobIdStr));    ApplicationId appId=jobId.getAppId();    AppLevelTimelineCollector collector=new AppLevelTimelineCollector(appId);    manager.putIfAbsent(appId,collector);    try {      JobInfo jobInfo=parser.parseHistoryFile(job.getJobHistoryFilePath());      Configuration jobConf=parser.parseConfiguration(job.getJobConfFilePath());      LOG.info("parsed the job history file and the configuration file " + "for job " + jobIdStr);      TimelineCollectorContext tlContext=collector.getTimelineEntityContext();      tlContext.setFlowName(jobInfo.getJobname());      tlContext.setFlowRunId(jobInfo.getSubmitTime());      tlContext.setUserId(jobInfo.getUsername());
      long totalTime=0;      List<TimelineEntity> entitySet=converter.createTimelineEntities(jobInfo,jobConf);      LOG.info("converted them into timeline entities for job " + jobIdStr);      UserGroupInformation ugi=UserGroupInformation.getCurrentUser();      long startWrite=System.nanoTime();      try {switch (replayMode) {case JobHistoryFileReplayHelper.WRITE_ALL_AT_ONCE:          writeAllEntities(collector,entitySet,ugi);        break;case JobHistoryFileReplayHelper.WRITE_PER_ENTITY:      writePerEntity(collector,entitySet,ugi);    break;default:  break;}} catch (Exception e) {context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).increment(1);LOG.error("writing to the timeline service failed",e);
private void writePerEntity(AppLevelTimelineCollector collector,List<TimelineEntity> entitySet,UserGroupInformation ugi) throws IOException {  for (  TimelineEntity entity : entitySet) {    TimelineEntities entities=new TimelineEntities();    entities.addEntity(entity);    collector.putEntities(entities,ugi);
public static String readOutput(Path outDir,Configuration conf) throws IOException {  FileSystem fs=outDir.getFileSystem(conf);  StringBuffer result=new StringBuffer();  Path[] fileList=FileUtil.stat2Paths(fs.listStatus(outDir,new Utils.OutputFileUtils.OutputFilesFilter()));  for (  Path outputFile : fileList) {
    }    String entId=taskAttemptId + "_" + Integer.toString(i);    final TimelineEntity entity=new TimelineEntity();    entity.setEntityId(entId);    entity.setEntityType("FOO_ATTEMPT");    entity.addOtherInfo("PERF_TEST",payLoad);    TimelineEvent event=new TimelineEvent();    event.setTimestamp(System.currentTimeMillis());    event.setEventType("foo_event");    entity.addEvent(event);    UserGroupInformation ugi=UserGroupInformation.getCurrentUser();    long startWrite=System.nanoTime();    try {      tlc.putEntities(entity);    } catch (    Exception e) {      context.getCounter(PerfCounters.TIMELINE_SERVICE_WRITE_FAILURES).increment(1);
      event.setTimestamp(System.currentTimeMillis());      event.addInfo("foo_event","test");      entity.addEvent(event);      TimelineMetric metric=new TimelineMetric();      metric.setId("foo_metric");      metric.addValue(System.currentTimeMillis(),123456789L);      entity.addMetric(metric);      entity.addConfig("foo","bar");      TimelineEntities entities=new TimelineEntities();      entities.addEntity(entity);      UserGroupInformation ugi=UserGroupInformation.getCurrentUser();      long startWrite=System.nanoTime();      try {        collector.putEntities(entities,ugi);      } catch (      Exception e) {
  final Thread toInterrupt=Thread.currentThread();  Thread interrupter=new Thread(){    public void run(){      try {        Thread.sleep(120 * 1000);        toInterrupt.interrupt();      } catch (      InterruptedException ie) {      }    }  };  LOG.info("Submitting job...");  job.submit();  LOG.info("Starting thread to interrupt main thread in 2 minutes");  interrupter.start();  LOG.info("Waiting for job to complete...");  try {    job.waitForCompletion(true);
  Path outputDir=getOutputPath();  Configuration conf=new Configuration();  FileSystem fs=FileSystem.getLocal(conf);  FileStatus[] stats=fs.listStatus(outputDir);  int valueSum=0;  for (  FileStatus f : stats) {    FSDataInputStream istream=fs.open(f.getPath());    BufferedReader r=new BufferedReader(new InputStreamReader(istream));    String line=null;    while ((line=r.readLine()) != null) {      valueSum+=Integer.valueOf(line.trim());    }    r.close();  }  int maxVal=NUMBER_FILE_VAL - 1;  int expectedPerMapper=maxVal * (maxVal + 1) / 2;  int expectedSum=expectedPerMapper * numMaps;
public static int runTool(Configuration conf,Tool tool,String[] args,OutputStream out) throws Exception {
private void testListBlackList(Configuration conf) throws Exception {  CLI jc=createJobClient();  ByteArrayOutputStream out=new ByteArrayOutputStream();  int exitCode=runTool(conf,jc,new String[]{"-list-blacklisted-trackers","second in"},out);  assertEquals("Exit code",-1,exitCode);  exitCode=runTool(conf,jc,new String[]{"-list-blacklisted-trackers"},out);  assertEquals("Exit code",0,exitCode);  String line;  BufferedReader br=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));  int counter=0;  while ((line=br.readLine()) != null) {
private void testListAttemptIds(String jobId,Configuration conf) throws Exception {  CLI jc=createJobClient();  ByteArrayOutputStream out=new ByteArrayOutputStream();  int exitCode=runTool(conf,jc,new String[]{"-list-attempt-ids"},out);  assertEquals("Exit code",-1,exitCode);  exitCode=runTool(conf,jc,new String[]{"-list-attempt-ids",jobId,"MAP","completed"},out);  assertEquals("Exit code",0,exitCode);  String line;  BufferedReader br=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));  int counter=0;  while ((line=br.readLine()) != null) {
private void testListTrackers(Configuration conf) throws Exception {  CLI jc=createJobClient();  ByteArrayOutputStream out=new ByteArrayOutputStream();  int exitCode=runTool(conf,jc,new String[]{"-list-active-trackers","second parameter"},out);  assertEquals("Exit code",-1,exitCode);  exitCode=runTool(conf,jc,new String[]{"-list-active-trackers"},out);  assertEquals("Exit code",0,exitCode);  String line;  BufferedReader br=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));  int counter=0;  while ((line=br.readLine()) != null) {
private void testJobEvents(String jobId,Configuration conf) throws Exception {  CLI jc=createJobClient();  ByteArrayOutputStream out=new ByteArrayOutputStream();  int exitCode=runTool(conf,jc,new String[]{"-events"},out);  assertEquals("Exit code",-1,exitCode);  exitCode=runTool(conf,jc,new String[]{"-events",jobId,"0","100"},out);  assertEquals("Exit code",0,exitCode);  String line;  BufferedReader br=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));  int counter=0;  String attemptId=("attempt" + jobId.substring(3));  while ((line=br.readLine()) != null) {
private void testJobStatus(String jobId,Configuration conf) throws Exception {  CLI jc=createJobClient();  ByteArrayOutputStream out=new ByteArrayOutputStream();  int exitCode=runTool(conf,jc,new String[]{"-status"},out);  assertEquals("Exit code",-1,exitCode);  exitCode=runTool(conf,jc,new String[]{"-status",jobId},out);  assertEquals("Exit code",0,exitCode);  String line;  BufferedReader br=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));  while ((line=br.readLine()) != null) {
protected void testAllJobList(String jobId,Configuration conf) throws Exception {  ByteArrayOutputStream out=new ByteArrayOutputStream();  int exitCode=runTool(conf,createJobClient(),new String[]{"-list","alldata"},out);  assertEquals("Exit code",-1,exitCode);  exitCode=runTool(conf,createJobClient(),new String[]{"-list","all"},out);  assertEquals("Exit code",0,exitCode);  BufferedReader br=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));  String line;  int counter=0;  while ((line=br.readLine()) != null) {
protected void testSubmittedJobList(Configuration conf) throws Exception {  Job job=runJobInBackGround(conf);  ByteArrayOutputStream out=new ByteArrayOutputStream();  String line;  int counter=0;  int exitCode=runTool(conf,createJobClient(),new String[]{"-list"},out);  assertEquals("Exit code",0,exitCode);  BufferedReader br=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(out.toByteArray())));  counter=0;  while ((line=br.readLine()) != null) {
protected void verifyJobPriority(String jobId,String priority,Configuration conf,CLI jc) throws Exception {  PipedInputStream pis=new PipedInputStream();  PipedOutputStream pos=new PipedOutputStream(pis);  int exitCode=runTool(conf,jc,new String[]{"-list","all"},pos);  assertEquals("Exit code",0,exitCode);  BufferedReader br=new BufferedReader(new InputStreamReader(pis));  String line;  while ((line=br.readLine()) != null) {
protected void verifyJobName(String jobId,String name,Configuration conf,CLI jc) throws Exception {  PipedInputStream pis=new PipedInputStream();  PipedOutputStream pos=new PipedOutputStream(pis);  int exitCode=runTool(conf,jc,new String[]{"-list","all"},pos);  assertEquals("Exit code",0,exitCode);  BufferedReader br=new BufferedReader(new InputStreamReader(pis));  String line=null;  while ((line=br.readLine()) != null) {
private static void runTest(String name,Job job) throws Exception {  job.setNumReduceTasks(1);  job.getConfiguration().set(MRConfig.FRAMEWORK_NAME,MRConfig.LOCAL_FRAMEWORK_NAME);  job.getConfiguration().setInt(MRJobConfig.IO_SORT_FACTOR,1000);  job.getConfiguration().set("fs.defaultFS","file:///");  job.getConfiguration().setInt("test.mapcollection.num.maps",1);  job.setInputFormatClass(FakeIF.class);  job.setOutputFormatClass(NullOutputFormat.class);  job.setMapperClass(Mapper.class);  job.setReducerClass(SpillReducer.class);  job.setMapOutputKeyClass(KeyWritable.class);  job.setMapOutputValueClass(ValWritable.class);  job.setSortComparatorClass(VariableComparator.class);
@Test public void testRandom() throws Exception {  Configuration conf=new Configuration();  conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY,100);  Job job=Job.getInstance(conf);  conf=job.getConfiguration();  conf.setInt(MRJobConfig.IO_SORT_MB,1);  conf.setClass("test.mapcollection.class",RandomFactory.class,RecordFactory.class);  final Random r=new Random();  final long seed=r.nextLong();
@Test public void testRandomCompress() throws Exception {  Configuration conf=new Configuration();  conf.setInt(Job.COMPLETION_POLL_INTERVAL_KEY,100);  Job job=Job.getInstance(conf);  conf=job.getConfiguration();  conf.setInt(MRJobConfig.IO_SORT_MB,1);  conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,true);  conf.setClass("test.mapcollection.class",RandomFactory.class,RecordFactory.class);  final Random r=new Random();  final long seed=r.nextLong();
private static int test0(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  int errors=0;  IntWritable i;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();
private static int test0(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  int errors=0;  IntWritable i;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  LOG.info("Executing TEST:0 for Key:" + key.toString());  values.mark();  LOG.info("TEST:0. Marking");  while (values.hasNext()) {    i=values.next();    expectedValues.add(i);
  int errors=0;  IntWritable i;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  LOG.info("Executing TEST:0 for Key:" + key.toString());  values.mark();  LOG.info("TEST:0. Marking");  while (values.hasNext()) {    i=values.next();    expectedValues.add(i);    LOG.info(key + ":" + i);  }  values.reset();  LOG.info("TEST:0. Reset");  int count=0;  while (values.hasNext()) {    i=values.next();
  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  LOG.info("Executing TEST:0 for Key:" + key.toString());  values.mark();  LOG.info("TEST:0. Marking");  while (values.hasNext()) {    i=values.next();    expectedValues.add(i);    LOG.info(key + ":" + i);  }  values.reset();  LOG.info("TEST:0. Reset");  int count=0;  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (i != expectedValues.get(count)) {
private static int test1(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  IntWritable i;  int errors=0;  int count=0;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  ArrayList<IntWritable> expectedValues1=new ArrayList<IntWritable>();
private static int test1(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  IntWritable i;  int errors=0;  int count=0;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  ArrayList<IntWritable> expectedValues1=new ArrayList<IntWritable>();  LOG.info("Executing TEST:1 for Key:" + key);  values.mark();  LOG.info("TEST:1. Marking");  while (values.hasNext()) {    i=values.next();
  LOG.info("Executing TEST:1 for Key:" + key);  values.mark();  LOG.info("TEST:1. Marking");  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    expectedValues.add(i);    if (count == 2) {      break;    }    count++;  }  values.reset();  LOG.info("TEST:1. Reset");  count=0;  while (values.hasNext()) {    i=values.next();
    i=values.next();    LOG.info(key + ":" + i);    expectedValues.add(i);    if (count == 2) {      break;    }    count++;  }  values.reset();  LOG.info("TEST:1. Reset");  count=0;  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (count < expectedValues.size()) {      if (i != expectedValues.get(count)) {        errors++;
    }    if (count == 3) {      values.mark();      LOG.info("TEST:1. Marking -- " + key + ": "+ i);    }    if (count >= 3) {      expectedValues1.add(i);    }    if (count == 5) {      break;    }    count++;  }  if (count < expectedValues.size()) {    LOG.info(("TEST:1 Check:2. Iterator returned lesser values"));    errors++;    return errors;  }  values.reset();  count=0;  LOG.info("TEST:1. Reset");
    }    if (count >= 3) {      expectedValues1.add(i);    }    if (count == 5) {      break;    }    count++;  }  if (count < expectedValues.size()) {    LOG.info(("TEST:1 Check:2. Iterator returned lesser values"));    errors++;    return errors;  }  values.reset();  count=0;  LOG.info("TEST:1. Reset");  expectedValues.clear();  while (values.hasNext()) {    i=values.next();
  expectedValues.clear();  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (count < expectedValues1.size()) {      if (i != expectedValues1.get(count)) {        errors++;        LOG.info("TEST:1. Check:3 Expected: " + expectedValues1.get(count) + ", Got: "+ i);        return errors;      }    }    if (count == 25) {      values.mark();      LOG.info("TEST:1. Marking -- " + key + ":"+ i);    }    if (count >= 25) {      expectedValues.add(i);    }    count++;
    LOG.info(key + ":" + i);    if (count < expectedValues1.size()) {      if (i != expectedValues1.get(count)) {        errors++;        LOG.info("TEST:1. Check:3 Expected: " + expectedValues1.get(count) + ", Got: "+ i);        return errors;      }    }    if (count == 25) {      values.mark();      LOG.info("TEST:1. Marking -- " + key + ":"+ i);    }    if (count >= 25) {      expectedValues.add(i);    }    count++;  }  if (count < expectedValues1.size()) {    LOG.info(("TEST:1 Check:4. Iterator returned fewer values"));    errors++;
private static int test2(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  IntWritable i;  int errors=0;  int count=0;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  ArrayList<IntWritable> expectedValues1=new ArrayList<IntWritable>();
private static int test2(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  IntWritable i;  int errors=0;  int count=0;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  ArrayList<IntWritable> expectedValues1=new ArrayList<IntWritable>();  LOG.info("Executing TEST:2 for Key:" + key);  values.mark();  LOG.info("TEST:2 Marking");  while (values.hasNext()) {    i=values.next();
  LOG.info("Executing TEST:2 for Key:" + key);  values.mark();  LOG.info("TEST:2 Marking");  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    expectedValues.add(i);    if (count == 8) {      break;    }    count++;  }  values.reset();  count=0;  LOG.info("TEST:2 reset");  while (values.hasNext()) {    i=values.next();
    i=values.next();    LOG.info(key + ":" + i);    expectedValues.add(i);    if (count == 8) {      break;    }    count++;  }  values.reset();  count=0;  LOG.info("TEST:2 reset");  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (count < expectedValues.size()) {      if (i != expectedValues.get(count)) {        errors++;
    if (count < expectedValues.size()) {      if (i != expectedValues.get(count)) {        errors++;        LOG.info("TEST:2. Check:1 Expected: " + expectedValues.get(count) + ", Got: "+ i);        return errors;      }    }    if (count == 3) {      values.mark();      LOG.info("TEST:2. Marking -- " + key + ":"+ i);    }    if (count >= 3) {      expectedValues1.add(i);    }    count++;  }  values.reset();  LOG.info("TEST:2. Reset");  expectedValues.clear();  count=0;
        return errors;      }    }    if (count == 3) {      values.mark();      LOG.info("TEST:2. Marking -- " + key + ":"+ i);    }    if (count >= 3) {      expectedValues1.add(i);    }    count++;  }  values.reset();  LOG.info("TEST:2. Reset");  expectedValues.clear();  count=0;  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (count < expectedValues1.size()) {
  LOG.info("TEST:2. Reset");  expectedValues.clear();  count=0;  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (count < expectedValues1.size()) {      if (i != expectedValues1.get(count)) {        errors++;        LOG.info("TEST:2. Check:2 Expected: " + expectedValues1.get(count) + ", Got: "+ i);        return errors;      }    }    if (count == 20) {      values.mark();      LOG.info("TEST:2. Marking -- " + key + ":"+ i);    }    if (count >= 20) {
  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (count < expectedValues1.size()) {      if (i != expectedValues1.get(count)) {        errors++;        LOG.info("TEST:2. Check:2 Expected: " + expectedValues1.get(count) + ", Got: "+ i);        return errors;      }    }    if (count == 20) {      values.mark();      LOG.info("TEST:2. Marking -- " + key + ":"+ i);    }    if (count >= 20) {      expectedValues.add(i);    }    count++;  }  values.reset();
private static int test3(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  int errors=0;  IntWritable i;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();
private static int test3(IntWritable key,MarkableIterator<IntWritable> values) throws IOException {  int errors=0;  IntWritable i;  ArrayList<IntWritable> expectedValues=new ArrayList<IntWritable>();  LOG.info("Executing TEST:3 for Key:" + key);  values.mark();  LOG.info("TEST:3. Marking");  int count=0;  while (values.hasNext()) {    i=values.next();    ;
    }    if (count == 8) {      LOG.info("TEST:3. Marking -- " + key + ":"+ i);      values.mark();    }    if (count >= 8) {      expectedValues.add(i);    }    count++;  }  values.reset();  LOG.info("TEST:3. After reset");  if (!values.hasNext()) {    errors++;    LOG.info("TEST:3, Check:1. HasNext returned false");    return errors;  }  count=0;  while (values.hasNext()) {    i=values.next();
    }    if (count >= 8) {      expectedValues.add(i);    }    count++;  }  values.reset();  LOG.info("TEST:3. After reset");  if (!values.hasNext()) {    errors++;    LOG.info("TEST:3, Check:1. HasNext returned false");    return errors;  }  count=0;  while (values.hasNext()) {    i=values.next();    LOG.info(key + ":" + i);    if (count < expectedValues.size()) {      if (i != expectedValues.get(count)) {
private void validateOutput() throws IOException {  Path[] outputFiles=FileUtil.stat2Paths(localFs.listStatus(new Path(TEST_ROOT_DIR + "/out"),new Utils.OutputFileUtils.OutputFilesFilter()));  if (outputFiles.length > 0) {    InputStream is=localFs.open(outputFiles[0]);    BufferedReader reader=new BufferedReader(new InputStreamReader(is));    String line=reader.readLine();    while (line != null) {      StringTokenizer tokeniz=new StringTokenizer(line,"\t");      String key=tokeniz.nextToken();      String value=tokeniz.nextToken();
  job.setEntityType(JOB);  job.setEntityId(jobInfo.getJobId().toString());  job.setStartTime(jobInfo.getSubmitTime());  job.addPrimaryFilter("JOBNAME",jobInfo.getJobname());  job.addPrimaryFilter("USERNAME",jobInfo.getUsername());  job.addOtherInfo("JOB_QUEUE_NAME",jobInfo.getJobQueueName());  job.addOtherInfo("SUBMIT_TIME",jobInfo.getSubmitTime());  job.addOtherInfo("LAUNCH_TIME",jobInfo.getLaunchTime());  job.addOtherInfo("FINISH_TIME",jobInfo.getFinishTime());  job.addOtherInfo("JOB_STATUS",jobInfo.getJobStatus());  job.addOtherInfo("PRIORITY",jobInfo.getPriority());  job.addOtherInfo("TOTAL_MAPS",jobInfo.getTotalMaps());  job.addOtherInfo("TOTAL_REDUCES",jobInfo.getTotalReduces());  job.addOtherInfo("UBERIZED",jobInfo.getUberized());  job.addOtherInfo("ERROR_INFO",jobInfo.getErrorInfo());
private Set<TimelineEntity> createTaskAndTaskAttemptEntities(JobInfo jobInfo){  Set<TimelineEntity> entities=new HashSet<>();  Map<TaskID,TaskInfo> taskInfoMap=jobInfo.getAllTasks();
private TimelineEntity createTaskEntity(TaskInfo taskInfo){  TimelineEntity task=new TimelineEntity();  task.setEntityType(TASK);  task.setEntityId(taskInfo.getTaskId().toString());  task.setStartTime(taskInfo.getStartTime());  task.addOtherInfo("START_TIME",taskInfo.getStartTime());  task.addOtherInfo("FINISH_TIME",taskInfo.getFinishTime());  task.addOtherInfo("TASK_TYPE",taskInfo.getTaskType());  task.addOtherInfo("TASK_STATUS",taskInfo.getTaskStatus());  task.addOtherInfo("ERROR_INFO",taskInfo.getError());
private Set<TimelineEntity> createTaskAttemptEntities(TaskInfo taskInfo){  Set<TimelineEntity> taskAttempts=new HashSet<TimelineEntity>();  Map<TaskAttemptID,TaskAttemptInfo> taskAttemptInfoMap=taskInfo.getAllTaskAttempts();
private TimelineEntity createTaskAttemptEntity(TaskAttemptInfo taskAttemptInfo){  TimelineEntity taskAttempt=new TimelineEntity();  taskAttempt.setEntityType(TASK_ATTEMPT);  taskAttempt.setEntityId(taskAttemptInfo.getAttemptId().toString());  taskAttempt.setStartTime(taskAttemptInfo.getStartTime());  taskAttempt.addOtherInfo("START_TIME",taskAttemptInfo.getStartTime());  taskAttempt.addOtherInfo("FINISH_TIME",taskAttemptInfo.getFinishTime());  taskAttempt.addOtherInfo("MAP_FINISH_TIME",taskAttemptInfo.getMapFinishTime());  taskAttempt.addOtherInfo("SHUFFLE_FINISH_TIME",taskAttemptInfo.getShuffleFinishTime());  taskAttempt.addOtherInfo("SORT_FINISH_TIME",taskAttemptInfo.getSortFinishTime());  taskAttempt.addOtherInfo("TASK_STATUS",taskAttemptInfo.getTaskStatus());  taskAttempt.addOtherInfo("STATE",taskAttemptInfo.getState());  taskAttempt.addOtherInfo("ERROR",taskAttemptInfo.getError());  taskAttempt.addOtherInfo("CONTAINER_ID",taskAttemptInfo.getContainerId().toString());
  job.addInfo("USERNAME",jobInfo.getUsername());  job.addInfo("JOB_QUEUE_NAME",jobInfo.getJobQueueName());  job.addInfo("SUBMIT_TIME",jobInfo.getSubmitTime());  job.addInfo("LAUNCH_TIME",jobInfo.getLaunchTime());  job.addInfo("FINISH_TIME",jobInfo.getFinishTime());  job.addInfo("JOB_STATUS",jobInfo.getJobStatus());  job.addInfo("PRIORITY",jobInfo.getPriority());  job.addInfo("TOTAL_MAPS",jobInfo.getTotalMaps());  job.addInfo("TOTAL_REDUCES",jobInfo.getTotalReduces());  job.addInfo("UBERIZED",jobInfo.getUberized());  job.addInfo("ERROR_INFO",jobInfo.getErrorInfo());  Counters totalCounters=jobInfo.getTotalCounters();  if (totalCounters != null) {    addMetrics(job,totalCounters);  }  addConfiguration(job,conf);
private List<TimelineEntity> createTaskAndTaskAttemptEntities(JobInfo jobInfo){  List<TimelineEntity> entities=new ArrayList<>();  Map<TaskID,TaskInfo> taskInfoMap=jobInfo.getAllTasks();
private Set<TimelineEntity> createTaskAttemptEntities(TaskInfo taskInfo){  Set<TimelineEntity> taskAttempts=new HashSet<TimelineEntity>();  Map<TaskAttemptID,TaskAttemptInfo> taskAttemptInfoMap=taskInfo.getAllTaskAttempts();
@Test(timeout=10000) public void testFormat() throws IOException, InterruptedException {  Job job=Job.getInstance(conf);  Random random=new Random();  long seed=random.nextLong();  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random,job);  TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());  InputFormat<IntWritable,BytesWritable> format=new CombineSequenceFileInputFormat<IntWritable,BytesWritable>();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / (SequenceFile.SYNC_INTERVAL / 20)) + 1;
  Job job=Job.getInstance(conf);  Random random=new Random();  long seed=random.nextLong();  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random,job);  TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());  InputFormat<IntWritable,BytesWritable> format=new CombineSequenceFileInputFormat<IntWritable,BytesWritable>();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / (SequenceFile.SYNC_INTERVAL / 20)) + 1;    LOG.info("splitting: requesting = " + numSplits);    List<InputSplit> splits=format.getSplits(job);
    LOG.info("splitting: got =        " + splits.size());    assertEquals("We got more than one splits!",1,splits.size());    InputSplit split=splits.get(0);    assertEquals("It should be CombineFileSplit",CombineFileSplit.class,split.getClass());    BitSet bits=new BitSet(length);    RecordReader<IntWritable,BytesWritable> reader=format.createRecordReader(split,context);    MapContext<IntWritable,BytesWritable,IntWritable,BytesWritable> mcontext=new MapContextImpl<IntWritable,BytesWritable,IntWritable,BytesWritable>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),split);    reader.initialize(split,mcontext);    assertEquals("reader class is CombineFileRecordReader.",CombineFileRecordReader.class,reader.getClass());    try {      while (reader.nextKeyValue()) {        IntWritable key=reader.getCurrentKey();        BytesWritable value=reader.getCurrentValue();        assertNotNull("Value should not be null.",value);        final int k=key.get();
@Test(timeout=10000) public void testFormat() throws Exception {  Job job=Job.getInstance(new Configuration(defaultConf));  Random random=new Random();  long seed=random.nextLong();
@Test(timeout=10000) public void testFormat() throws Exception {  Job job=Job.getInstance(new Configuration(defaultConf));  Random random=new Random();  long seed=random.nextLong();  LOG.info("seed = " + seed);  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  CombineTextInputFormat format=new CombineTextInputFormat();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / 20) + 1;
  Job job=Job.getInstance(new Configuration(defaultConf));  Random random=new Random();  long seed=random.nextLong();  LOG.info("seed = " + seed);  random.setSeed(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  CombineTextInputFormat format=new CombineTextInputFormat();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / 20) + 1;    LOG.info("splitting: requesting = " + numSplits);    List<InputSplit> splits=format.getSplits(job);
  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int length=10000;  final int numFiles=10;  createFiles(length,numFiles,random);  CombineTextInputFormat format=new CombineTextInputFormat();  for (int i=0; i < 3; i++) {    int numSplits=random.nextInt(length / 20) + 1;    LOG.info("splitting: requesting = " + numSplits);    List<InputSplit> splits=format.getSplits(job);    LOG.info("splitting: got =        " + splits.size());    assertEquals("We got more than one splits!",1,splits.size());    InputSplit split=splits.get(0);    assertEquals("It should be CombineFileSplit",CombineFileSplit.class,split.getClass());    BitSet bits=new BitSet(length);
    assertEquals("It should be CombineFileSplit",CombineFileSplit.class,split.getClass());    BitSet bits=new BitSet(length);    LOG.debug("split= " + split);    TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());    RecordReader<LongWritable,Text> reader=format.createRecordReader(split,context);    assertEquals("reader class is CombineFileRecordReader.",CombineFileRecordReader.class,reader.getClass());    MapContext<LongWritable,Text,LongWritable,Text> mcontext=new MapContextImpl<LongWritable,Text,LongWritable,Text>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),split);    reader.initialize(split,mcontext);    try {      int count=0;      while (reader.nextKeyValue()) {        LongWritable key=reader.getCurrentKey();        assertNotNull("Key should not be null.",key);        Text value=reader.getCurrentValue();        final int v=Integer.parseInt(value.toString());
  Path file=new Path(workDir,new String("testFormat.txt"));  createFile(file,null,10,10);  Job job=Job.getInstance(defaultConf);  FileInputFormat.setInputPaths(job,workDir);  FixedLengthInputFormat format=new FixedLengthInputFormat();  List<InputSplit> splits=format.getSplits(job);  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());      RecordReader<LongWritable,BytesWritable> reader=format.createRecordReader(split,context);      MapContext<LongWritable,BytesWritable,LongWritable,BytesWritable> mcontext=new MapContextImpl<LongWritable,BytesWritable,LongWritable,BytesWritable>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),split);      reader.initialize(split,mcontext);    } catch (    IOException ioe) {      exceptionThrown=true;
  createFile(file,null,10,10);  Job job=Job.getInstance(defaultConf);  FixedLengthInputFormat format=new FixedLengthInputFormat();  format.setRecordLength(job.getConfiguration(),0);  FileInputFormat.setInputPaths(job,workDir);  List<InputSplit> splits=format.getSplits(job);  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());      RecordReader<LongWritable,BytesWritable> reader=format.createRecordReader(split,context);      MapContext<LongWritable,BytesWritable,LongWritable,BytesWritable> mcontext=new MapContextImpl<LongWritable,BytesWritable,LongWritable,BytesWritable>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),split);      reader.initialize(split,mcontext);    } catch (    IOException ioe) {      exceptionThrown=true;
  createFile(file,null,10,10);  Job job=Job.getInstance(defaultConf);  FixedLengthInputFormat format=new FixedLengthInputFormat();  format.setRecordLength(job.getConfiguration(),-10);  FileInputFormat.setInputPaths(job,workDir);  List<InputSplit> splits=format.getSplits(job);  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());      RecordReader<LongWritable,BytesWritable> reader=format.createRecordReader(split,context);      MapContext<LongWritable,BytesWritable,LongWritable,BytesWritable> mcontext=new MapContextImpl<LongWritable,BytesWritable,LongWritable,BytesWritable>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),split);      reader.initialize(split,mcontext);    } catch (    IOException ioe) {      exceptionThrown=true;
  Path file=new Path(workDir,fileName.toString());  int seed=new Random().nextInt();  LOG.info("Seed = " + seed);  Random random=new Random(seed);  int MAX_TESTS=20;  LongWritable key;  BytesWritable value;  for (int i=0; i < MAX_TESTS; i++) {    LOG.info("----------------------------------------------------------");    int totalRecords=random.nextInt(999) + 1;    if (i == 8) {      totalRecords=0;    }    int recordLength=random.nextInt(1024 * 100) + 1;    if (i == 10) {      recordLength=1;
    int fileSize=(totalRecords * recordLength);    LOG.info("totalRecords=" + totalRecords + " recordLength="+ recordLength);    Job job=Job.getInstance(defaultConf);    if (codec != null) {      ReflectionUtils.setConf(codec,job.getConfiguration());    }    ArrayList<String> recordList=createFile(file,codec,recordLength,totalRecords);    assertTrue(localFs.exists(file));    FixedLengthInputFormat.setRecordLength(job.getConfiguration(),recordLength);    int numSplits=1;    if (i > 0) {      if (i == (MAX_TESTS - 1)) {        numSplits=(int)(fileSize / Math.floor(recordLength / 2));      } else {        if (MAX_TESTS % i == 0) {          numSplits=fileSize / (fileSize - random.nextInt(fileSize));
    fileName.append(".gz");    ReflectionUtils.setConf(codec,job.getConfiguration());  }  writeFile(localFs,new Path(workDir,fileName.toString()),codec,"one  two  threefour five six  seveneightnine ten");  FixedLengthInputFormat format=new FixedLengthInputFormat();  format.setRecordLength(job.getConfiguration(),5);  FileInputFormat.setInputPaths(job,workDir);  List<InputSplit> splits=format.getSplits(job);  if (codec != null) {    assertEquals("compressed splits == 1",1,splits.size());  }  boolean exceptionThrown=false;  for (  InputSplit split : splits) {    try {      List<String> results=readSplit(format,split,job);    } catch (    IOException ioe) {      exceptionThrown=true;
@Test public void testFormat() throws Exception {  Job job=Job.getInstance(new Configuration(defaultConf));  Path file=new Path(workDir,"test.txt");  int seed=new Random().nextInt();
@Test public void testFormat() throws Exception {  Job job=Job.getInstance(new Configuration(defaultConf));  Path file=new Path(workDir,"test.txt");  int seed=new Random().nextInt();  LOG.info("seed = " + seed);  Random random=new Random(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int MAX_LENGTH=10000;  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
  final int MAX_LENGTH=10000;  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {    LOG.debug("creating; entries = " + length);    Writer writer=new OutputStreamWriter(localFs.create(file));    try {      for (int i=0; i < length; i++) {        writer.write(Integer.toString(i * 2));        writer.write("\t");        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    KeyValueTextInputFormat format=new KeyValueTextInputFormat();    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;
    LOG.debug("creating; entries = " + length);    Writer writer=new OutputStreamWriter(localFs.create(file));    try {      for (int i=0; i < length; i++) {        writer.write(Integer.toString(i * 2));        writer.write("\t");        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    KeyValueTextInputFormat format=new KeyValueTextInputFormat();    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;      LOG.debug("splitting: requesting = " + numSplits);      List<InputSplit> splits=format.getSplits(job);
      for (int i=0; i < length; i++) {        writer.write(Integer.toString(i * 2));        writer.write("\t");        writer.write(Integer.toString(i));        writer.write("\n");      }    }  finally {      writer.close();    }    KeyValueTextInputFormat format=new KeyValueTextInputFormat();    for (int i=0; i < 3; i++) {      int numSplits=random.nextInt(MAX_LENGTH / 20) + 1;      LOG.debug("splitting: requesting = " + numSplits);      List<InputSplit> splits=format.getSplits(job);      LOG.debug("splitting: got =        " + splits.size());      BitSet bits=new BitSet(length);      for (int j=0; j < splits.size(); j++) {
        try {          int count=0;          while (reader.nextKeyValue()) {            key=reader.getCurrentKey();            clazz=key.getClass();            assertEquals("Key class is Text.",Text.class,clazz);            value=reader.getCurrentValue();            clazz=value.getClass();            assertEquals("Value class is Text.",Text.class,clazz);            final int k=Integer.parseInt(key.toString());            final int v=Integer.parseInt(value.toString());            assertEquals("Bad key",0,k % 2);            assertEquals("Mismatched key/value",k / 2,v);            LOG.debug("read " + v);            assertFalse("Key in multiple partitions.",bits.get(v));
  final Configuration conf=job.getConfiguration();  CompressionCodec codec=null;  try {    codec=(CompressionCodec)ReflectionUtils.newInstance(conf.getClassByName("org.apache.hadoop.io.compress.BZip2Codec"),conf);  } catch (  ClassNotFoundException cnfe) {    throw new IOException("Illegal codec!");  }  Path file=new Path(workDir,"test" + codec.getDefaultExtension());  int seed=new Random().nextInt();  LOG.info("seed = " + seed);  Random random=new Random(seed);  localFs.delete(workDir,true);  FileInputFormat.setInputPaths(job,workDir);  final int MAX_LENGTH=500000;  FileInputFormat.setMaxInputSplitSize(job,MAX_LENGTH / 20);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 4) + 1) {
        RecordReader<Text,Text> reader=format.createRecordReader(splits.get(j),context);        Class<?> clazz=reader.getClass();        MapContext<Text,Text,Text,Text> mcontext=new MapContextImpl<Text,Text,Text,Text>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),splits.get(j));        reader.initialize(splits.get(j),mcontext);        Text key=null;        Text value=null;        try {          int count=0;          while (reader.nextKeyValue()) {            key=reader.getCurrentKey();            value=reader.getCurrentValue();            final int k=Integer.parseInt(key.toString());            final int v=Integer.parseInt(value.toString());            assertEquals("Bad key",0,k % 2);            assertEquals("Mismatched key/value",k / 2,v);
        MapContext<Text,Text,Text,Text> mcontext=new MapContextImpl<Text,Text,Text,Text>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),splits.get(j));        reader.initialize(splits.get(j),mcontext);        Text key=null;        Text value=null;        try {          int count=0;          while (reader.nextKeyValue()) {            key=reader.getCurrentKey();            value=reader.getCurrentValue();            final int k=Integer.parseInt(key.toString());            final int v=Integer.parseInt(value.toString());            assertEquals("Bad key",0,k % 2);            assertEquals("Mismatched key/value",k / 2,v);            LOG.debug("read " + k + ","+ v);            assertFalse(k + "," + v+ " in multiple partitions.",bits.get(v));
@Test public void testRegexFilter() throws Exception {  LOG.info("Testing Regex Filter with patter: \\A10*");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.RegexFilter.class);  SequenceFileInputFilter.RegexFilter.setPattern(job.getConfiguration(),"\\A10*");  fs.delete(inDir,true);  for (int length=1; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
@Test public void testPercentFilter() throws Exception {  LOG.info("Testing Percent Filter with frequency: 1000");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.PercentFilter.class);  SequenceFileInputFilter.PercentFilter.setFrequency(job.getConfiguration(),1000);  fs.delete(inDir,true);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
@Test public void testPercentFilter() throws Exception {  LOG.info("Testing Percent Filter with frequency: 1000");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.PercentFilter.class);  SequenceFileInputFilter.PercentFilter.setFrequency(job.getConfiguration(),1000);  fs.delete(inDir,true);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {    LOG.info("******Number of records: " + length);    createSequenceFile(length);    int count=countRecords(1);
@Test public void testMD5Filter() throws Exception {  LOG.info("Testing MD5 Filter with frequency: 1000");  SequenceFileInputFilter.setFilterClass(job,SequenceFileInputFilter.MD5Filter.class);  SequenceFileInputFilter.MD5Filter.setFrequency(job.getConfiguration(),1000);  fs.delete(inDir,true);  for (int length=0; length < MAX_LENGTH; length+=random.nextInt(MAX_LENGTH / 10) + 1) {
private void testKeySpecs(String input,String expectedOutput,KeyFieldHelper helper,int s1,int e1){
private void testKeySpecs(String input,String expectedOutput,KeyFieldHelper helper,int s1,int e1){  LOG.info("input : " + input);  String keySpecs=helper.keySpecs().get(0).toString();
private void testKeySpecs(String input,String expectedOutput,KeyFieldHelper helper,int s1,int e1){  LOG.info("input : " + input);  String keySpecs=helper.keySpecs().get(0).toString();  LOG.info("keyspecs : " + keySpecs);  byte[] inputBytes=input.getBytes();  if (e1 == -1) {    e1=inputBytes.length;  }  LOG.info("length : " + e1);  int[] indices=helper.getWordLengths(inputBytes,s1,e1);  int start=helper.getStartOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));  LOG.info("start : " + start);  if (expectedOutput == null) {    assertEquals("Expected -1 when the start index is invalid",-1,start);    return;  }  int end=helper.getEndOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));
  LOG.info("keyspecs : " + keySpecs);  byte[] inputBytes=input.getBytes();  if (e1 == -1) {    e1=inputBytes.length;  }  LOG.info("length : " + e1);  int[] indices=helper.getWordLengths(inputBytes,s1,e1);  int start=helper.getStartOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));  LOG.info("start : " + start);  if (expectedOutput == null) {    assertEquals("Expected -1 when the start index is invalid",-1,start);    return;  }  int end=helper.getEndOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));  LOG.info("end : " + end);  end=(end >= inputBytes.length) ? inputBytes.length - 1 : end;  int length=end + 1 - start;
  }  LOG.info("length : " + e1);  int[] indices=helper.getWordLengths(inputBytes,s1,e1);  int start=helper.getStartOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));  LOG.info("start : " + start);  if (expectedOutput == null) {    assertEquals("Expected -1 when the start index is invalid",-1,start);    return;  }  int end=helper.getEndOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));  LOG.info("end : " + end);  end=(end >= inputBytes.length) ? inputBytes.length - 1 : end;  int length=end + 1 - start;  LOG.info("length : " + length);  byte[] outputBytes=new byte[length];  System.arraycopy(inputBytes,start,outputBytes,0,length);  String output=new String(outputBytes);
  LOG.info("length : " + e1);  int[] indices=helper.getWordLengths(inputBytes,s1,e1);  int start=helper.getStartOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));  LOG.info("start : " + start);  if (expectedOutput == null) {    assertEquals("Expected -1 when the start index is invalid",-1,start);    return;  }  int end=helper.getEndOffset(inputBytes,s1,e1,indices,helper.keySpecs().get(0));  LOG.info("end : " + end);  end=(end >= inputBytes.length) ? inputBytes.length - 1 : end;  int length=end + 1 - start;  LOG.info("length : " + length);  byte[] outputBytes=new byte[length];  System.arraycopy(inputBytes,start,outputBytes,0,length);  String output=new String(outputBytes);
  long tokenFetchTime;  try {    jobHistoryServer=new JobHistoryServer(){      protected void doSecureLogin(      Configuration conf) throws IOException {      }      @Override protected JHSDelegationTokenSecretManager createJHSSecretManager(      Configuration conf,      HistoryServerStateStoreService store){        return new JHSDelegationTokenSecretManager(initialInterval,maxLifetime,renewInterval,3600000,store);      }    };    jobHistoryServer.init(conf);    jobHistoryServer.start();    final MRClientProtocol hsService=jobHistoryServer.getClientService().getClientHandler();    UserGroupInformation loggedInUser=UserGroupInformation.createRemoteUser("testrenewer@APACHE.ORG");    Assert.assertEquals("testrenewer",loggedInUser.getShortUserName());    loggedInUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);    Token token=getDelegationToken(loggedInUser,hsService,loggedInUser.getShortUserName());    tokenFetchTime=System.currentTimeMillis();
      clientUsingDT.getJobReport(jobReportRequest);    } catch (    IOException e) {      Assert.assertEquals("Unknown job job_123456_0001",e.getMessage());    }    while (System.currentTimeMillis() < tokenFetchTime + initialInterval / 2) {      Thread.sleep(500l);    }    long nextExpTime=renewDelegationToken(loggedInUser,hsService,token);    long renewalTime=System.currentTimeMillis();    LOG.info("Renewed token at: " + renewalTime + ", NextExpiryTime: "+ nextExpTime);    while (System.currentTimeMillis() > tokenFetchTime + initialInterval && System.currentTimeMillis() < nextExpTime) {      Thread.sleep(500l);    }    Thread.sleep(50l);    try {      clientUsingDT.getJobReport(jobReportRequest);    } catch (    IOException e) {      Assert.assertEquals("Unknown job job_123456_0001",e.getMessage());
    LOG.info("Renewed token at: " + renewalTime + ", NextExpiryTime: "+ nextExpTime);    while (System.currentTimeMillis() > tokenFetchTime + initialInterval && System.currentTimeMillis() < nextExpTime) {      Thread.sleep(500l);    }    Thread.sleep(50l);    try {      clientUsingDT.getJobReport(jobReportRequest);    } catch (    IOException e) {      Assert.assertEquals("Unknown job job_123456_0001",e.getMessage());    }    while (System.currentTimeMillis() < renewalTime + renewInterval) {      Thread.sleep(500l);    }    Thread.sleep(50l);    LOG.info("At time: " + System.currentTimeMillis() + ", token should be invalid");    try {      clientUsingDT.getJobReport(jobReportRequest);      fail("Should not have succeeded with an expired token");
      Thread.sleep(500l);    }    Thread.sleep(50l);    LOG.info("At time: " + System.currentTimeMillis() + ", token should be invalid");    try {      clientUsingDT.getJobReport(jobReportRequest);      fail("Should not have succeeded with an expired token");    } catch (    IOException e) {      assertTrue(e.getCause().getMessage().contains("is expired"));    }    if (clientUsingDT != null) {      clientUsingDT=null;    }    token=getDelegationToken(loggedInUser,hsService,loggedInUser.getShortUserName());    tokenFetchTime=System.currentTimeMillis();    LOG.info("Got delegation token at: " + tokenFetchTime);    clientUsingDT=getMRClientProtocol(token,jobHistoryServer.getClientService().getBindAddress(),"loginuser2",conf);    try {
@Test public void testJobTokenRpc() throws Exception {  TaskUmbilicalProtocol mockTT=mock(TaskUmbilicalProtocol.class);  doReturn(TaskUmbilicalProtocol.versionID).when(mockTT).getProtocolVersion(anyString(),anyLong());  doReturn(ProtocolSignature.getProtocolSignature(mockTT,TaskUmbilicalProtocol.class.getName(),TaskUmbilicalProtocol.versionID,0)).when(mockTT).getProtocolSignature(anyString(),anyLong(),anyInt());  JobTokenSecretManager sm=new JobTokenSecretManager();  final Server server=new RPC.Builder(conf).setProtocol(TaskUmbilicalProtocol.class).setInstance(mockTT).setBindAddress(ADDRESS).setPort(0).setNumHandlers(5).setVerbose(true).setSecretManager(sm).build();  server.start();  final UserGroupInformation current=UserGroupInformation.getCurrentUser();  final InetSocketAddress addr=NetUtils.getConnectAddress(server);  String jobId=current.getUserName();  JobTokenIdentifier tokenId=new JobTokenIdentifier(new Text(jobId));  Token<JobTokenIdentifier> token=new Token<JobTokenIdentifier>(tokenId,sm);  sm.addTokenForJob(jobId,token);  SecurityUtil.setTokenService(token,addr);
private String relativeToWorking(String pathname){  String cwd=System.getProperty("user.dir","/");  pathname=(new Path(pathname)).toUri().getPath();  cwd=(new Path(cwd)).toUri().getPath();  String[] cwdParts=cwd.split(Path.SEPARATOR);  String[] pathParts=pathname.split(Path.SEPARATOR);  if (cwd.equals(pathname)) {
    if (cwdParts[i].equals(pathParts[i])) {      common++;    } else {      break;    }  }  StringBuilder sb=new StringBuilder();  int parentDirsRequired=cwdParts.length - common;  for (int i=0; i < parentDirsRequired; i++) {    sb.append("..");    sb.append(Path.SEPARATOR);  }  for (int i=common; i < pathParts.length; i++) {    sb.append(pathParts[i]);    sb.append(Path.SEPARATOR);  }  String s=sb.toString();  if (s.endsWith(Path.SEPARATOR)) {    s=s.substring(0,s.length() - 1);
  }  conf.set(MRConfig.FRAMEWORK_NAME,MRConfig.YARN_FRAMEWORK_NAME);  String stagingDir=conf.get(MRJobConfig.MR_AM_STAGING_DIR);  if (stagingDir == null || stagingDir.equals(MRJobConfig.DEFAULT_MR_AM_STAGING_DIR)) {    conf.set(MRJobConfig.MR_AM_STAGING_DIR,new File(getTestWorkDir(),"apps_staging_dir/").getAbsolutePath());  }  if (!conf.getBoolean(MRConfig.MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING,MRConfig.DEFAULT_MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING)) {    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED,false);    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED,false);  }  conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,"000");  try {    Path stagingPath=FileContext.getFileContext(conf).makeQualified(new Path(conf.get(MRJobConfig.MR_AM_STAGING_DIR)));    if (Path.WINDOWS) {      if (LocalFileSystem.class.isInstance(stagingPath.getFileSystem(conf))) {        conf.set(MRJobConfig.MR_AM_STAGING_DIR,new File(conf.get(MRJobConfig.MR_AM_STAGING_DIR)).getAbsolutePath());      }    }    FileContext fc=FileContext.getFileContext(stagingPath.toUri(),conf);    if (fc.util().exists(stagingPath)) {
  if (stagingDir == null || stagingDir.equals(MRJobConfig.DEFAULT_MR_AM_STAGING_DIR)) {    conf.set(MRJobConfig.MR_AM_STAGING_DIR,new File(getTestWorkDir(),"apps_staging_dir/").getAbsolutePath());  }  if (!conf.getBoolean(MRConfig.MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING,MRConfig.DEFAULT_MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING)) {    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED,false);    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED,false);  }  conf.set(CommonConfigurationKeys.FS_PERMISSIONS_UMASK_KEY,"000");  try {    Path stagingPath=FileContext.getFileContext(conf).makeQualified(new Path(conf.get(MRJobConfig.MR_AM_STAGING_DIR)));    if (Path.WINDOWS) {      if (LocalFileSystem.class.isInstance(stagingPath.getFileSystem(conf))) {        conf.set(MRJobConfig.MR_AM_STAGING_DIR,new File(conf.get(MRJobConfig.MR_AM_STAGING_DIR)).getAbsolutePath());      }    }    FileContext fc=FileContext.getFileContext(stagingPath.toUri(),conf);    if (fc.util().exists(stagingPath)) {      LOG.info(stagingPath + " exists! deleting...");      fc.delete(stagingPath,true);
@Before public void setup() throws Exception {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test public void testJobWithNonNormalizedCapabilities() throws Exception {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@After public void tearDown(){  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@BeforeClass public static void setup() throws IOException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
private void testSleepJobInternal(Configuration sleepConf,boolean useRemoteJar,boolean jobSubmissionShouldSucceed,ResourceViolation violation) throws Exception {
@Test(timeout=3000000) public void testJobWithChangePriority() throws Exception {  Configuration sleepConf=new Configuration(mrCluster.getConfig());  Assume.assumeFalse(sleepConf.get(YarnConfiguration.RM_SCHEDULER).equals(FairScheduler.class.getCanonicalName()));  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test(timeout=300000) public void testJobWithWorkflowPriority() throws Exception {  Configuration sleepConf=new Configuration(mrCluster.getConfig());  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
private void testJobClassloader(boolean useCustomClasses) throws IOException, InterruptedException, ClassNotFoundException {
private void testJobClassloader(boolean useCustomClasses) throws IOException, InterruptedException, ClassNotFoundException {  LOG.info("\n\n\nStarting testJobClassloader()" + " useCustomClasses=" + useCustomClasses);  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test(timeout=60000) public void testRandomWriter() throws IOException, InterruptedException, ClassNotFoundException {  LOG.info("\n\n\nStarting testRandomWriter().");  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test(timeout=60000) public void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException {  LOG.info("\n\n\nStarting testFailingMapper().");  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test(timeout=120000) public void testContainerRollingLog() throws IOException, InterruptedException, ClassNotFoundException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
private void testDistributedCache(String jobJarPath,boolean withWildcard) throws Exception {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test(timeout=120000) public void testThreadDumpOnTaskTimeout() throws IOException, InterruptedException, ClassNotFoundException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test public void testSharedCache() throws Exception {  Path localJobJarPath=makeJobJarWithLib(TEST_ROOT_DIR.toUri().toString());  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Before public void setup() throws InterruptedException, IOException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@After public void tearDown(){  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test(timeout=90000) public void testJobHistoryData() throws IOException, InterruptedException, AvroRemoteException, ClassNotFoundException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
  job.waitForCompletion(true);  Counters counterMR=job.getCounters();  JobId jobId=TypeConverter.toYarn(job.getJobID());  ApplicationId appID=jobId.getAppId();  int pollElapsed=0;  while (true) {    Thread.sleep(1000);    pollElapsed+=1000;    if (TERMINAL_RM_APP_STATES.contains(mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState())) {      break;    }    if (pollElapsed >= 60000) {      LOG.warn("application did not reach terminal state within 60 seconds");      break;    }  }  Assert.assertEquals(RMAppState.FINISHED,mrCluster.getResourceManager().getRMContext().getRMApps().get(appID).getState());  Counters counterHS=job.getCounters();
@BeforeClass public static void setup() throws InterruptedException, IOException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@AfterClass public static void tearDown(){  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
private void testProfilerInternal(boolean useDefault) throws Exception {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@BeforeClass public static void setup() throws IOException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test public void testJobSucceed() throws IOException, InterruptedException, ClassNotFoundException {  LOG.info("\n\n\nStarting testJobSucceed().");  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test public void testJobFail() throws IOException, InterruptedException, ClassNotFoundException {  LOG.info("\n\n\nStarting testJobFail().");  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@BeforeClass public static void setup() throws IOException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test public void testRMNMInfo() throws Exception {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Before public void setup() throws IOException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test public void testExecNonSpeculative() throws Exception {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@BeforeClass public static void setup() throws IOException {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Test public void testSpeculativeExecution() throws Exception {  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
@Override @Test public void testFailingMapper() throws IOException, InterruptedException, ClassNotFoundException {  LOG.info("\n\n\nStarting uberized testFailingMapper().");  if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
public static INativeHandler create(String nativeHandlerName,Configuration conf,DataChannel channel) throws IOException {  final int bufferSize=conf.getInt(Constants.NATIVE_PROCESSOR_BUFFER_KB,1024) * 1024;
@SuppressWarnings("unchecked") @Override public void init(Context context) throws IOException, ClassNotFoundException {  this.context=context;  this.job=context.getJobConf();  Platforms.init(job);  if (job.getNumReduceTasks() == 0) {    String message="There is no reducer, no need to use native output collector";
@SuppressWarnings("unchecked") @Override public void init(Context context) throws IOException, ClassNotFoundException {  this.context=context;  this.job=context.getJobConf();  Platforms.init(job);  if (job.getNumReduceTasks() == 0) {    String message="There is no reducer, no need to use native output collector";    LOG.error(message);    throw new InvalidJobConfException(message);  }  Class<?> comparatorClass=job.getClass(MRJobConfig.KEY_COMPARATOR,null,RawComparator.class);  if (comparatorClass != null && !Platforms.define(comparatorClass)) {    String message="Native output collector doesn't support customized java comparator " + job.get(MRJobConfig.KEY_COMPARATOR);    LOG.error(message);    throw new InvalidJobConfException(message);  }  if (!QuickSort.class.getName().equals(job.get(Constants.MAP_SORT_CLASS))) {    String message="Native-Task doesn't support sort class " + job.get(Constants.MAP_SORT_CLASS);
  if (job.getNumReduceTasks() == 0) {    String message="There is no reducer, no need to use native output collector";    LOG.error(message);    throw new InvalidJobConfException(message);  }  Class<?> comparatorClass=job.getClass(MRJobConfig.KEY_COMPARATOR,null,RawComparator.class);  if (comparatorClass != null && !Platforms.define(comparatorClass)) {    String message="Native output collector doesn't support customized java comparator " + job.get(MRJobConfig.KEY_COMPARATOR);    LOG.error(message);    throw new InvalidJobConfException(message);  }  if (!QuickSort.class.getName().equals(job.get(Constants.MAP_SORT_CLASS))) {    String message="Native-Task doesn't support sort class " + job.get(Constants.MAP_SORT_CLASS);    LOG.error(message);    throw new InvalidJobConfException(message);  }  if (job.getBoolean(MRConfig.SHUFFLE_SSL_ENABLED_KEY,false) == true) {    String message="Native-Task doesn't support secure shuffle";
    String message="Native output collector doesn't support customized java comparator " + job.get(MRJobConfig.KEY_COMPARATOR);    LOG.error(message);    throw new InvalidJobConfException(message);  }  if (!QuickSort.class.getName().equals(job.get(Constants.MAP_SORT_CLASS))) {    String message="Native-Task doesn't support sort class " + job.get(Constants.MAP_SORT_CLASS);    LOG.error(message);    throw new InvalidJobConfException(message);  }  if (job.getBoolean(MRConfig.SHUFFLE_SSL_ENABLED_KEY,false) == true) {    String message="Native-Task doesn't support secure shuffle";    LOG.error(message);    throw new InvalidJobConfException(message);  }  final Class<?> keyCls=job.getMapOutputKeyClass();  try {    @SuppressWarnings("rawtypes") final INativeSerializer serializer=NativeSerialization.getInstance().getSerializer(keyCls);    if (null == serializer) {
  if (!QuickSort.class.getName().equals(job.get(Constants.MAP_SORT_CLASS))) {    String message="Native-Task doesn't support sort class " + job.get(Constants.MAP_SORT_CLASS);    LOG.error(message);    throw new InvalidJobConfException(message);  }  if (job.getBoolean(MRConfig.SHUFFLE_SSL_ENABLED_KEY,false) == true) {    String message="Native-Task doesn't support secure shuffle";    LOG.error(message);    throw new InvalidJobConfException(message);  }  final Class<?> keyCls=job.getMapOutputKeyClass();  try {    @SuppressWarnings("rawtypes") final INativeSerializer serializer=NativeSerialization.getInstance().getSerializer(keyCls);    if (null == serializer) {      String message="Key type not supported. Cannot find serializer for " + keyCls.getName();      LOG.error(message);      throw new InvalidJobConfException(message);
  }  if (job.getBoolean(MRConfig.SHUFFLE_SSL_ENABLED_KEY,false) == true) {    String message="Native-Task doesn't support secure shuffle";    LOG.error(message);    throw new InvalidJobConfException(message);  }  final Class<?> keyCls=job.getMapOutputKeyClass();  try {    @SuppressWarnings("rawtypes") final INativeSerializer serializer=NativeSerialization.getInstance().getSerializer(keyCls);    if (null == serializer) {      String message="Key type not supported. Cannot find serializer for " + keyCls.getName();      LOG.error(message);      throw new InvalidJobConfException(message);    } else     if (!Platforms.support(keyCls.getName(),serializer,job)) {      String message="Native output collector doesn't support this key, " + "this key is not comparable in native: " + keyCls.getName();      LOG.error(message);      throw new InvalidJobConfException(message);
    @SuppressWarnings("rawtypes") final INativeSerializer serializer=NativeSerialization.getInstance().getSerializer(keyCls);    if (null == serializer) {      String message="Key type not supported. Cannot find serializer for " + keyCls.getName();      LOG.error(message);      throw new InvalidJobConfException(message);    } else     if (!Platforms.support(keyCls.getName(),serializer,job)) {      String message="Native output collector doesn't support this key, " + "this key is not comparable in native: " + keyCls.getName();      LOG.error(message);      throw new InvalidJobConfException(message);    }  } catch (  final IOException e) {    String message="Cannot find serializer for " + keyCls.getName();    LOG.error(message);    throw new IOException(message);  }  final boolean ret=NativeRuntime.isNativeLibraryLoaded();  if (ret) {
      LOG.error(message);      throw new InvalidJobConfException(message);    }  } catch (  final IOException e) {    String message="Cannot find serializer for " + keyCls.getName();    LOG.error(message);    throw new IOException(message);  }  final boolean ret=NativeRuntime.isNativeLibraryLoaded();  if (ret) {    if (job.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,false)) {      String codec=job.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC);      if (!NativeRuntime.supportsCompressionCodec(codec.getBytes(Charsets.UTF_8))) {        String message="Native output collector doesn't support compression codec " + codec;        LOG.error(message);        throw new InvalidJobConfException(message);      }    }    NativeRuntime.configure(job);
    if (job.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,false)) {      String codec=job.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC);      if (!NativeRuntime.supportsCompressionCodec(codec.getBytes(Charsets.UTF_8))) {        String message="Native output collector doesn't support compression codec " + codec;        LOG.error(message);        throw new InvalidJobConfException(message);      }    }    NativeRuntime.configure(job);    final long updateInterval=job.getLong(Constants.NATIVE_STATUS_UPDATE_INTERVAL,Constants.NATIVE_STATUS_UPDATE_INTERVAL_DEFVAL);    updater=new StatusReportChecker(context.getReporter(),updateInterval);    updater.start();  } else {    String message="NativeRuntime cannot be loaded, please check that " + "libnativetask.so is in hadoop library dir";    LOG.error(message);    throw new InvalidJobConfException(message);  }  this.handler=null;
public static boolean support(String keyClassName,INativeSerializer<?> serializer,JobConf job){synchronized (platforms) {    for (    Platform platform : platforms) {      if (platform.support(keyClassName,serializer,job)) {
public static boolean define(Class<?> keyComparator){synchronized (platforms) {    for (    Platform platform : platforms) {      if (platform.define(keyComparator)) {
public static NativeTaskOutput createNativeTaskOutput(Configuration conf,String id){  Class<?> clazz=conf.getClass(OutputUtil.NATIVE_TASK_OUTPUT_MANAGER,NativeTaskOutputFiles.class);
@Test public void testLargeValueCombiner() throws Exception {  final Configuration normalConf=ScenarioConfiguration.getNormalConfiguration();  final Configuration nativeConf=ScenarioConfiguration.getNativeConfiguration();  normalConf.addResource(TestConstants.COMBINER_CONF_PATH);  nativeConf.addResource(TestConstants.COMBINER_CONF_PATH);  final int deafult_KVSize_Maximum=1 << 22;  final int KVSize_Maximum=normalConf.getInt(TestConstants.NATIVETASK_KVSIZE_MAX_LARGEKV_TEST,deafult_KVSize_Maximum);  final String inputPath=TestConstants.NATIVETASK_COMBINER_TEST_INPUTDIR + "/largeKV";  final String nativeOutputPath=TestConstants.NATIVETASK_COMBINER_TEST_NATIVE_OUTPUTDIR + "/nativeLargeKV";  final String hadoopOutputPath=TestConstants.NATIVETASK_COMBINER_TEST_NORMAL_OUTPUTDIR + "/normalLargeKV";  final FileSystem fs=FileSystem.get(normalConf);  for (int i=65536; i <= KVSize_Maximum; i*=4) {    int max=i;    int min=Math.max(i / 4,max - 10);
@Parameters(name="key:{0}\nvalue:{1}") public static Iterable<Class<?>[]> data() throws Exception {  final String valueClassesStr=nativekvtestconf.get(TestConstants.NATIVETASK_KVTEST_VALUECLASSES);
@Parameters(name="key:{0}\nvalue:{1}") public static Iterable<Class<?>[]> data() throws Exception {  final String valueClassesStr=nativekvtestconf.get(TestConstants.NATIVETASK_KVTEST_VALUECLASSES);  LOG.info("Parameterizing with value classes: " + valueClassesStr);  List<Class<?>> valueClasses=parseClassNames(valueClassesStr);  final String keyClassesStr=nativekvtestconf.get(TestConstants.NATIVETASK_KVTEST_KEYCLASSES);
public void createSequenceTestFile(String filepath,int base,byte start) throws Exception {
  ServerBootstrap bootstrap=new ServerBootstrap(selector);  timer=new HashedWheelTimer();  try {    pipelineFact=new HttpPipelineFactory(conf,timer);  } catch (  Exception ex) {    throw new RuntimeException(ex);  }  bootstrap.setOption("backlog",conf.getInt(SHUFFLE_LISTEN_QUEUE_SIZE,DEFAULT_SHUFFLE_LISTEN_QUEUE_SIZE));  bootstrap.setOption("child.keepAlive",true);  bootstrap.setPipelineFactory(pipelineFact);  port=conf.getInt(SHUFFLE_PORT_CONFIG_KEY,DEFAULT_SHUFFLE_PORT);  Channel ch=bootstrap.bind(new InetSocketAddress(port));  accepted.add(ch);  port=((InetSocketAddress)ch.getLocalAddress()).getPort();  conf.set(SHUFFLE_PORT_CONFIG_KEY,Integer.toString(port));  pipelineFact.SHUFFLE.setPort(port);
private void startStore(Path recoveryRoot) throws IOException {  Options options=new Options();  options.createIfMissing(false);  Path dbPath=new Path(recoveryRoot,STATE_DB_NAME);
private void checkVersion() throws IOException {  Version loadedVersion=loadVersion();
private void addJobToken(JobID jobId,String user,Token<JobTokenIdentifier> jobToken){  userRsrc.put(jobId.toString(),user);  secretManager.addTokenForJob(jobId.toString(),jobToken);
@Test(timeout=100000) public void testMapFileAccess() throws IOException {  assumeTrue(NativeIO.isAvailable());  Configuration conf=new Configuration();  conf.setInt(ShuffleHandler.SHUFFLE_PORT_CONFIG_KEY,0);  conf.setInt(ShuffleHandler.MAX_SHUFFLE_CONNECTIONS,3);  conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,"kerberos");  UserGroupInformation.setConfiguration(conf);  conf.set(YarnConfiguration.NM_LOCAL_DIRS,ABS_LOG_DIR.getAbsolutePath());  ApplicationId appId=ApplicationId.newInstance(12345,1);
@VisibleForTesting void collectPackages() throws UploaderException {  parseLists();  String[] list=StringUtils.split(input,File.pathSeparatorChar);  for (  String item : list) {
@VisibleForTesting void collectPackages() throws UploaderException {  parseLists();  String[] list=StringUtils.split(input,File.pathSeparatorChar);  for (  String item : list) {    LOG.info("Original source " + item);    String expanded=expandEnvironmentVariables(item,System.getenv());
  parseLists();  String[] list=StringUtils.split(input,File.pathSeparatorChar);  for (  String item : list) {    LOG.info("Original source " + item);    String expanded=expandEnvironmentVariables(item,System.getenv());    LOG.info("Expanded source " + expanded);    if (expanded.endsWith("*")) {      File path=new File(expanded.substring(0,expanded.length() - 1));      if (path.isDirectory()) {        File[] files=path.listFiles();        if (files != null) {          for (          File jar : files) {            if (!jar.isDirectory()) {              addJar(jar);            } else {
@VisibleForTesting void beginUpload() throws IOException, UploaderException {  if (targetStream == null) {    int lastIndex=target.indexOf('#');    targetPath=new Path(target.substring(0,lastIndex == -1 ? target.length() : lastIndex));    alias=lastIndex != -1 ? target.substring(lastIndex + 1) : targetPath.getName();
@VisibleForTesting void beginUpload() throws IOException, UploaderException {  if (targetStream == null) {    int lastIndex=target.indexOf('#');    targetPath=new Path(target.substring(0,lastIndex == -1 ? target.length() : lastIndex));    alias=lastIndex != -1 ? target.substring(lastIndex + 1) : targetPath.getName();    LOG.info("Target " + targetPath);    FileSystem fileSystem=targetPath.getFileSystem(conf);    targetStream=null;    if (fileSystem instanceof DistributedFileSystem) {
@VisibleForTesting void beginUpload() throws IOException, UploaderException {  if (targetStream == null) {    int lastIndex=target.indexOf('#');    targetPath=new Path(target.substring(0,lastIndex == -1 ? target.length() : lastIndex));    alias=lastIndex != -1 ? target.substring(lastIndex + 1) : targetPath.getName();    LOG.info("Target " + targetPath);    FileSystem fileSystem=targetPath.getFileSystem(conf);    targetStream=null;    if (fileSystem instanceof DistributedFileSystem) {      LOG.info("Set replication to " + initialReplication + " for path: "+ targetPath);
    FileSystem fileSystem=targetPath.getFileSystem(conf);    targetStream=null;    if (fileSystem instanceof DistributedFileSystem) {      LOG.info("Set replication to " + initialReplication + " for path: "+ targetPath);      LOG.info("Disabling Erasure Coding for path: " + targetPath);      DistributedFileSystem dfs=(DistributedFileSystem)fileSystem;      DistributedFileSystem.HdfsDataOutputStreamBuilder builder=dfs.createFile(targetPath).overwrite(true).ecPolicyName(SystemErasureCodingPolicies.getReplicationPolicy().getName());      if (initialReplication > 0) {        builder.replication(initialReplication);      }      targetStream=builder.build();    } else {      LOG.warn("Cannot set replication to " + initialReplication + " for path: "+ targetPath+ " on a non-distributed fileystem "+ fileSystem.getClass().getName());    }    if (targetStream == null) {      targetStream=fileSystem.create(targetPath,true);    }    if (!FRAMEWORK_PERMISSION.equals(FRAMEWORK_PERMISSION.applyUMask(FsPermission.getUMask(conf)))) {
private void endUpload() throws IOException, InterruptedException {  FileSystem fileSystem=targetPath.getFileSystem(conf);  if (fileSystem instanceof DistributedFileSystem) {    fileSystem.setReplication(targetPath,finalReplication);
  FileSystem fileSystem=targetPath.getFileSystem(conf);  if (fileSystem instanceof DistributedFileSystem) {    fileSystem.setReplication(targetPath,finalReplication);    LOG.info("Set replication to " + finalReplication + " for path: "+ targetPath);    if (timeout == 0) {      LOG.info("Timeout is set to 0. Skipping replication check.");    } else {      long startTime=System.currentTimeMillis();      long endTime=startTime;      long currentReplication=0;      while (endTime - startTime < timeout * 1000 && currentReplication < acceptableReplication) {        Thread.sleep(1000);        endTime=System.currentTimeMillis();        currentReplication=getSmallestReplicatedBlockCount();      }      if (endTime - startTime >= timeout * 1000) {
    fileSystem.setReplication(targetPath,finalReplication);    LOG.info("Set replication to " + finalReplication + " for path: "+ targetPath);    if (timeout == 0) {      LOG.info("Timeout is set to 0. Skipping replication check.");    } else {      long startTime=System.currentTimeMillis();      long endTime=startTime;      long currentReplication=0;      while (endTime - startTime < timeout * 1000 && currentReplication < acceptableReplication) {        Thread.sleep(1000);        endTime=System.currentTimeMillis();        currentReplication=getSmallestReplicatedBlockCount();      }      if (endTime - startTime >= timeout * 1000) {        LOG.error(String.format("Timed out after %d seconds while waiting for acceptable" + " replication of %d (current replication is %d)",timeout,acceptableReplication,currentReplication));      }    }  } else {
@VisibleForTesting void buildPackage() throws IOException, UploaderException, InterruptedException {  beginUpload();  LOG.info("Compressing tarball");  try (TarArchiveOutputStream out=new TarArchiveOutputStream(targetStream)){    for (    String fullPath : filteredInputFiles) {
private void parseLists() throws UploaderException {  Map<String,String> env=System.getenv();  for (  Map.Entry<String,String> item : env.entrySet()) {
private void addJar(File jar) throws UploaderException {  boolean found=false;  if (!jar.getName().endsWith(".jar")) {
  for (  Pattern pattern : whitelistedFiles) {    Matcher matcher=pattern.matcher(jar.getAbsolutePath());    if (matcher.matches()) {      LOG.info("Whitelisted " + jar.getAbsolutePath());      found=true;      break;    }  }  boolean excluded=false;  for (  Pattern pattern : blacklistedFiles) {    Matcher matcher=pattern.matcher(jar.getAbsolutePath());    if (matcher.matches()) {      LOG.info("Blacklisted " + jar.getAbsolutePath());      excluded=true;      break;    }  }  if (ignoreSymlink && !excluded) {    excluded=checkSymlink(jar);
      found=true;      break;    }  }  boolean excluded=false;  for (  Pattern pattern : blacklistedFiles) {    Matcher matcher=pattern.matcher(jar.getAbsolutePath());    if (matcher.matches()) {      LOG.info("Blacklisted " + jar.getAbsolutePath());      excluded=true;      break;    }  }  if (ignoreSymlink && !excluded) {    excluded=checkSymlink(jar);  }  if (found && !excluded) {    LOG.info("Whitelisted " + jar.getAbsolutePath());    if (!filteredInputFiles.add(jar.getAbsolutePath())) {      throw new UploaderException("Duplicate jar" + jar.getAbsolutePath());
    }  }  boolean excluded=false;  for (  Pattern pattern : blacklistedFiles) {    Matcher matcher=pattern.matcher(jar.getAbsolutePath());    if (matcher.matches()) {      LOG.info("Blacklisted " + jar.getAbsolutePath());      excluded=true;      break;    }  }  if (ignoreSymlink && !excluded) {    excluded=checkSymlink(jar);  }  if (found && !excluded) {    LOG.info("Whitelisted " + jar.getAbsolutePath());    if (!filteredInputFiles.add(jar.getAbsolutePath())) {      throw new UploaderException("Duplicate jar" + jar.getAbsolutePath());    }  }  if (!found) {    LOG.info("Ignored " + jar.getAbsolutePath() + " because it is missing "+ "from the whitelist");
@VisibleForTesting boolean checkSymlink(File jar){  if (Files.isSymbolicLink(jar.toPath())) {    try {      java.nio.file.Path link=Files.readSymbolicLink(jar.toPath());      java.nio.file.Path jarPath=Paths.get(jar.getAbsolutePath());      String linkString=link.toString();      java.nio.file.Path jarParent=jarPath.getParent();      java.nio.file.Path linkPath=jarParent == null ? null : jarParent.resolve(linkString);      java.nio.file.Path linkPathParent=linkPath == null ? null : linkPath.getParent();      java.nio.file.Path normalizedLinkPath=linkPathParent == null ? null : linkPathParent.normalize();      if (normalizedLinkPath != null && jarParent.normalize().equals(normalizedLinkPath)) {
  initialReplication=Short.parseShort(parser.getCommandLine().getOptionValue("initialReplication","3"));  finalReplication=Short.parseShort(parser.getCommandLine().getOptionValue("finalReplication","10"));  acceptableReplication=Short.parseShort(parser.getCommandLine().getOptionValue("acceptableReplication","9"));  timeout=Integer.parseInt(parser.getCommandLine().getOptionValue("timeout","10"));  if (parser.getCommandLine().hasOption("nosymlink")) {    ignoreSymlink=true;  }  String fs=parser.getCommandLine().getOptionValue("fs",null);  String path=parser.getCommandLine().getOptionValue("target","/usr/lib/mr-framework.tar.gz#mr-framework");  boolean isFullPath=path.startsWith("hdfs://") || path.startsWith("file://");  if (fs == null) {    fs=conf.getTrimmed(FS_DEFAULT_NAME_KEY);    if (fs == null && !isFullPath) {      LOG.error("No filesystem specified in either fs or target.");      printHelp(opts);      return false;
private boolean verify() throws SQLException {  String countAccessQuery="SELECT COUNT(*) FROM HAccess";  String sumPageviewQuery="SELECT SUM(pageview) FROM Pageview";  Statement st=null;  ResultSet rs=null;  try {    st=connection.createStatement();    rs=st.executeQuery(countAccessQuery);    rs.next();    long totalPageview=rs.getLong(1);    rs=st.executeQuery(sumPageviewQuery);    rs.next();    long sumPageview=rs.getLong(1);
private boolean verify() throws SQLException {  String countAccessQuery="SELECT COUNT(*) FROM HAccess";  String sumPageviewQuery="SELECT SUM(pageview) FROM Pageview";  Statement st=null;  ResultSet rs=null;  try {    st=connection.createStatement();    rs=st.executeQuery(countAccessQuery);    rs.next();    long totalPageview=rs.getLong(1);    rs=st.executeQuery(sumPageviewQuery);    rs.next();    long sumPageview=rs.getLong(1);    LOG.info("totalPageview=" + totalPageview);
void pickBestSplits(Host host){  int tasksToPick=Math.min(slotsPerHost,(int)Math.ceil((double)remainingSplits / hosts.size()));  Split[] best=new Split[tasksToPick];  for (  Split cur : host.splits) {
private boolean rejectRootDirectoryDelete(boolean isEmptyDir,boolean recursive) throws IOException {
private void createFakeDirectoryIfNecessary(Path f) throws IOException {  String key=pathToKey(f);  if (StringUtils.isNotEmpty(key) && !exists(f)) {
@Override public FileStatus[] listStatus(Path path) throws IOException {  String key=pathToKey(path);  if (LOG.isDebugEnabled()) {
@Override public FileStatus[] listStatus(Path path) throws IOException {  String key=pathToKey(path);  if (LOG.isDebugEnabled()) {    LOG.debug("List status for path: " + path);  }  final List<FileStatus> result=new ArrayList<FileStatus>();  final FileStatus fileStatus=getFileStatus(path);  if (fileStatus.isDirectory()) {    if (LOG.isDebugEnabled()) {      LOG.debug("listStatus: doing listObjects for directory " + key);    }    ObjectListing objects=store.listObjects(key,maxKeys,null,false);    while (true) {      for (      OSSObjectSummary objectSummary : objects.getObjectSummaries()) {        String objKey=objectSummary.getKey();        if (objKey.equals(key + "/")) {          if (LOG.isDebugEnabled()) {
  final List<FileStatus> result=new ArrayList<FileStatus>();  final FileStatus fileStatus=getFileStatus(path);  if (fileStatus.isDirectory()) {    if (LOG.isDebugEnabled()) {      LOG.debug("listStatus: doing listObjects for directory " + key);    }    ObjectListing objects=store.listObjects(key,maxKeys,null,false);    while (true) {      for (      OSSObjectSummary objectSummary : objects.getObjectSummaries()) {        String objKey=objectSummary.getKey();        if (objKey.equals(key + "/")) {          if (LOG.isDebugEnabled()) {            LOG.debug("Ignoring: " + objKey);          }          continue;        } else {          Path keyPath=keyToPath(objectSummary.getKey()).makeQualified(uri,workingDir);
    }    ObjectListing objects=store.listObjects(key,maxKeys,null,false);    while (true) {      for (      OSSObjectSummary objectSummary : objects.getObjectSummaries()) {        String objKey=objectSummary.getKey();        if (objKey.equals(key + "/")) {          if (LOG.isDebugEnabled()) {            LOG.debug("Ignoring: " + objKey);          }          continue;        } else {          Path keyPath=keyToPath(objectSummary.getKey()).makeQualified(uri,workingDir);          if (LOG.isDebugEnabled()) {            LOG.debug("Adding: fi: " + keyPath);          }          result.add(new OSSFileStatus(objectSummary.getSize(),false,1,getDefaultBlockSize(keyPath),objectSummary.getLastModified().getTime(),keyPath,username));        }      }      for (      String prefix : objects.getCommonPrefixes()) {        if (prefix.equals(key + "/")) {
        if (objKey.equals(key + "/")) {          if (LOG.isDebugEnabled()) {            LOG.debug("Ignoring: " + objKey);          }          continue;        } else {          Path keyPath=keyToPath(objectSummary.getKey()).makeQualified(uri,workingDir);          if (LOG.isDebugEnabled()) {            LOG.debug("Adding: fi: " + keyPath);          }          result.add(new OSSFileStatus(objectSummary.getSize(),false,1,getDefaultBlockSize(keyPath),objectSummary.getLastModified().getTime(),keyPath,username));        }      }      for (      String prefix : objects.getCommonPrefixes()) {        if (prefix.equals(key + "/")) {          if (LOG.isDebugEnabled()) {            LOG.debug("Ignoring: " + prefix);          }          continue;        } else {
private RemoteIterator<LocatedFileStatus> innerList(final Path f,final FileStatus status,final PathFilter filter,final FileStatusAcceptor acceptor,final boolean recursive) throws IOException {  Path qualifiedPath=f.makeQualified(uri,workingDir);  String key=pathToKey(qualifiedPath);  if (status.isFile()) {
  if (StringUtils.isNotEmpty(proxyHost)) {    clientConf.setProxyHost(proxyHost);    if (proxyPort >= 0) {      clientConf.setProxyPort(proxyPort);    } else {      if (secureConnections) {        LOG.warn("Proxy host set without port. Using HTTPS default 443");        clientConf.setProxyPort(443);      } else {        LOG.warn("Proxy host set without port. Using HTTP default 80");        clientConf.setProxyPort(80);      }    }    String proxyUsername=conf.getTrimmed(PROXY_USERNAME_KEY);    String proxyPassword=conf.getTrimmed(PROXY_PASSWORD_KEY);    if ((proxyUsername == null) != (proxyPassword == null)) {      String msg="Proxy error: " + PROXY_USERNAME_KEY + " or "+ PROXY_PASSWORD_KEY+ " set without the other.";
        clientConf.setProxyPort(443);      } else {        LOG.warn("Proxy host set without port. Using HTTP default 80");        clientConf.setProxyPort(80);      }    }    String proxyUsername=conf.getTrimmed(PROXY_USERNAME_KEY);    String proxyPassword=conf.getTrimmed(PROXY_PASSWORD_KEY);    if ((proxyUsername == null) != (proxyPassword == null)) {      String msg="Proxy error: " + PROXY_USERNAME_KEY + " or "+ PROXY_PASSWORD_KEY+ " set without the other.";      LOG.error(msg);      throw new IllegalArgumentException(msg);    }    clientConf.setProxyUsername(proxyUsername);    clientConf.setProxyPassword(proxyPassword);    clientConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN_KEY));    clientConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION_KEY));  } else   if (proxyPort >= 0) {
private boolean singleCopy(String srcKey,String dstKey){  CopyObjectResult copyResult=ossClient.copyObject(bucketName,srcKey,bucketName,dstKey);  statistics.incrementWriteOps(1);
  int tries=3;  while (tries > 0) {    try {      instream=new FileInputStream(file);      UploadPartRequest uploadRequest=new UploadPartRequest();      uploadRequest.setBucketName(bucketName);      uploadRequest.setKey(key);      uploadRequest.setUploadId(uploadId);      uploadRequest.setInputStream(instream);      uploadRequest.setPartSize(file.length());      uploadRequest.setPartNumber(idx);      UploadPartResult uploadResult=ossClient.uploadPart(uploadRequest);      statistics.incrementWriteOps(1);      return uploadResult.getPartETag();    } catch (    Exception e) {
static int intOption(Configuration conf,String key,int defVal,int min){  int v=conf.getInt(key,defVal);  Preconditions.checkArgument(v >= min,String.format("Value of %s: %d is below the minimum value %d",key,v,min));
static long longOption(Configuration conf,String key,long defVal,long min){  long v=conf.getLong(key,defVal);  Preconditions.checkArgument(v >= min,String.format("Value of %s: %d is below the minimum value %d",key,v,min));
@Test public void testSeekFile() throws Exception {  Path smallSeekFile=setPath("/test/smallSeekFile.txt");  long size=5 * 1024 * 1024;  ContractTestUtils.generateTestFile(this.fs,smallSeekFile,size,256,255);  LOG.info("5MB file created: smallSeekFile.txt");  FSDataInputStream instream=this.fs.open(smallSeekFile);  int seekTimes=5;  LOG.info("multiple fold position seeking test...:");  for (int i=0; i < seekTimes; i++) {    long pos=size / (seekTimes - i) - 1;
@Test public void testSeekFile() throws Exception {  Path smallSeekFile=setPath("/test/smallSeekFile.txt");  long size=5 * 1024 * 1024;  ContractTestUtils.generateTestFile(this.fs,smallSeekFile,size,256,255);  LOG.info("5MB file created: smallSeekFile.txt");  FSDataInputStream instream=this.fs.open(smallSeekFile);  int seekTimes=5;  LOG.info("multiple fold position seeking test...:");  for (int i=0; i < seekTimes; i++) {    long pos=size / (seekTimes - i) - 1;    LOG.info("begin seeking for pos: " + pos);    instream.seek(pos);    assertTrue("expected position at:" + pos + ", but got:"+ instream.getPos(),instream.getPos() == pos);
  ContractTestUtils.generateTestFile(this.fs,smallSeekFile,size,256,255);  LOG.info("5MB file created: smallSeekFile.txt");  FSDataInputStream instream=this.fs.open(smallSeekFile);  int seekTimes=5;  LOG.info("multiple fold position seeking test...:");  for (int i=0; i < seekTimes; i++) {    long pos=size / (seekTimes - i) - 1;    LOG.info("begin seeking for pos: " + pos);    instream.seek(pos);    assertTrue("expected position at:" + pos + ", but got:"+ instream.getPos(),instream.getPos() == pos);    LOG.info("completed seeking at pos: " + instream.getPos());  }  LOG.info("random position seeking test...:");  Random rand=new Random();  for (int i=0; i < seekTimes; i++) {    long pos=Math.abs(rand.nextLong()) % size;
  int seekTimes=5;  LOG.info("multiple fold position seeking test...:");  for (int i=0; i < seekTimes; i++) {    long pos=size / (seekTimes - i) - 1;    LOG.info("begin seeking for pos: " + pos);    instream.seek(pos);    assertTrue("expected position at:" + pos + ", but got:"+ instream.getPos(),instream.getPos() == pos);    LOG.info("completed seeking at pos: " + instream.getPos());  }  LOG.info("random position seeking test...:");  Random rand=new Random();  for (int i=0; i < seekTimes; i++) {    long pos=Math.abs(rand.nextLong()) % size;    LOG.info("begin seeking for pos: " + pos);    instream.seek(pos);    assertTrue("expected position at:" + pos + ", but got:"+ instream.getPos(),instream.getPos() == pos);
@Test public void testReadFile() throws Exception {  final int bufLen=256;  final int sizeFlag=5;  String filename="readTestFile_" + sizeFlag + ".txt";  Path readTestFile=setPath("/test/" + filename);  long size=sizeFlag * 1024 * 1024;  ContractTestUtils.generateTestFile(this.fs,readTestFile,size,256,255);
@Override public void testListEmptyRootDirectory() throws IOException {  for (int attempt=1, maxAttempts=10; attempt <= maxAttempts; ++attempt) {    try {      super.testListEmptyRootDirectory();      break;    } catch (    AssertionError|FileNotFoundException e) {      if (attempt < maxAttempts) {        LOG.info("Attempt {} of {} for empty root directory test failed.  " + "Attempting retry.",attempt,maxAttempts);        try {          Thread.sleep(1000);        } catch (        InterruptedException e2) {          Thread.currentThread().interrupt();          fail("Test interrupted.");          break;        }      } else {
  List<LogAggregationFileController> fileControllers=factory.getConfiguredLogAggregationFileControllerList();  if (fileControllers == null || fileControllers.isEmpty()) {    LOG.info("Can not find any valid fileControllers.");    if (verbose) {      LOG.info("The configurated fileControllers:" + YarnConfiguration.LOG_AGGREGATION_FILE_FORMATS);    }    return 0;  }  try {    fs=FileSystem.get(conf);    int previousTotal=0;    for (    LogAggregationFileController fileController : fileControllers) {      Path remoteRootLogDir=fileController.getRemoteRootLogDir();      String suffix=fileController.getRemoteRootLogDirSuffix();      Path workingDir=new Path(remoteRootLogDir,"archive-logs-work");      if (verbose) {        LOG.info("LogAggregationFileController:" + fileController.getClass().getName());
  if (fileControllers == null || fileControllers.isEmpty()) {    LOG.info("Can not find any valid fileControllers.");    if (verbose) {      LOG.info("The configurated fileControllers:" + YarnConfiguration.LOG_AGGREGATION_FILE_FORMATS);    }    return 0;  }  try {    fs=FileSystem.get(conf);    int previousTotal=0;    for (    LogAggregationFileController fileController : fileControllers) {      Path remoteRootLogDir=fileController.getRemoteRootLogDir();      String suffix=fileController.getRemoteRootLogDirSuffix();      Path workingDir=new Path(remoteRootLogDir,"archive-logs-work");      if (verbose) {        LOG.info("LogAggregationFileController:" + fileController.getClass().getName());        LOG.info("Remote Log Dir Root: " + remoteRootLogDir);
    LOG.info("Can not find any valid fileControllers.");    if (verbose) {      LOG.info("The configurated fileControllers:" + YarnConfiguration.LOG_AGGREGATION_FILE_FORMATS);    }    return 0;  }  try {    fs=FileSystem.get(conf);    int previousTotal=0;    for (    LogAggregationFileController fileController : fileControllers) {      Path remoteRootLogDir=fileController.getRemoteRootLogDir();      String suffix=fileController.getRemoteRootLogDirSuffix();      Path workingDir=new Path(remoteRootLogDir,"archive-logs-work");      if (verbose) {        LOG.info("LogAggregationFileController:" + fileController.getClass().getName());        LOG.info("Remote Log Dir Root: " + remoteRootLogDir);        LOG.info("Log Suffix: " + suffix);
      Path workingDir=new Path(remoteRootLogDir,"archive-logs-work");      if (verbose) {        LOG.info("LogAggregationFileController:" + fileController.getClass().getName());        LOG.info("Remote Log Dir Root: " + remoteRootLogDir);        LOG.info("Log Suffix: " + suffix);        LOG.info("Working Dir: " + workingDir);      }      checkFilesAndSeedApps(fs,remoteRootLogDir,suffix,workingDir);      filterAppsByAggregatedStatus();      if (eligibleApplications.size() > previousTotal) {        workingDirs.add(workingDir);        previousTotal=eligibleApplications.size();      }    }    checkMaxEligible();    if (workingDirs.isEmpty() || eligibleApplications.isEmpty()) {      LOG.info("No eligible applications to process");      return 0;
      }      checkFilesAndSeedApps(fs,remoteRootLogDir,suffix,workingDir);      filterAppsByAggregatedStatus();      if (eligibleApplications.size() > previousTotal) {        workingDirs.add(workingDir);        previousTotal=eligibleApplications.size();      }    }    checkMaxEligible();    if (workingDirs.isEmpty() || eligibleApplications.isEmpty()) {      LOG.info("No eligible applications to process");      return 0;    }    for (    Path workingDir : workingDirs) {      if (!prepareWorkingDir(fs,workingDir)) {        LOG.error("Failed to create the workingDir:" + workingDir.toString());        return 1;      }    }    StringBuilder sb=new StringBuilder("Will process the following applications:");    for (    AppInfo app : eligibleApplications) {
  opts.addOption(maxTotalLogsSizeOpt);  opts.addOption(memoryOpt);  opts.addOption(verboseOpt);  opts.addOption(forceOpt);  opts.addOption(noProxyOpt);  try {    CommandLineParser parser=new GnuParser();    CommandLine commandLine=parser.parse(opts,args);    if (commandLine.hasOption(HELP_OPTION)) {      HelpFormatter formatter=new HelpFormatter();      formatter.printHelp("mapred archive-logs",opts);      System.exit(0);    }    if (commandLine.hasOption(MAX_ELIGIBLE_APPS_OPTION)) {      maxEligible=Integer.parseInt(commandLine.getOptionValue(MAX_ELIGIBLE_APPS_OPTION));      if (maxEligible == 0) {
@VisibleForTesting void filterAppsByAggregatedStatus() throws IOException, YarnException {  YarnClient client=YarnClient.createYarnClient();  try {    client.init(getConf());    client.start();    for (Iterator<AppInfo> it=eligibleApplications.iterator(); it.hasNext(); ) {      AppInfo app=it.next();      try {        ApplicationReport report=client.getApplicationReport(ApplicationId.fromString(app.getAppId()));        LogAggregationStatus aggStatus=report.getLogAggregationStatus();        if (aggStatus.equals(LogAggregationStatus.RUNNING) || aggStatus.equals(LogAggregationStatus.RUNNING_WITH_FAILURE) || aggStatus.equals(LogAggregationStatus.NOT_START)|| aggStatus.equals(LogAggregationStatus.DISABLED)|| aggStatus.equals(LogAggregationStatus.FAILED)) {          if (verbose) {
  YarnClient client=YarnClient.createYarnClient();  try {    client.init(getConf());    client.start();    for (Iterator<AppInfo> it=eligibleApplications.iterator(); it.hasNext(); ) {      AppInfo app=it.next();      try {        ApplicationReport report=client.getApplicationReport(ApplicationId.fromString(app.getAppId()));        LogAggregationStatus aggStatus=report.getLogAggregationStatus();        if (aggStatus.equals(LogAggregationStatus.RUNNING) || aggStatus.equals(LogAggregationStatus.RUNNING_WITH_FAILURE) || aggStatus.equals(LogAggregationStatus.NOT_START)|| aggStatus.equals(LogAggregationStatus.DISABLED)|| aggStatus.equals(LogAggregationStatus.FAILED)) {          if (verbose) {            LOG.info("Skipping " + app.getAppId() + " due to aggregation status being "+ aggStatus);          }          it.remove();        } else {          if (verbose) {
    for (Iterator<AppInfo> it=eligibleApplications.iterator(); it.hasNext(); ) {      AppInfo app=it.next();      try {        ApplicationReport report=client.getApplicationReport(ApplicationId.fromString(app.getAppId()));        LogAggregationStatus aggStatus=report.getLogAggregationStatus();        if (aggStatus.equals(LogAggregationStatus.RUNNING) || aggStatus.equals(LogAggregationStatus.RUNNING_WITH_FAILURE) || aggStatus.equals(LogAggregationStatus.NOT_START)|| aggStatus.equals(LogAggregationStatus.DISABLED)|| aggStatus.equals(LogAggregationStatus.FAILED)) {          if (verbose) {            LOG.info("Skipping " + app.getAppId() + " due to aggregation status being "+ aggStatus);          }          it.remove();        } else {          if (verbose) {            LOG.info(app.getAppId() + " has aggregation status " + aggStatus);          }          app.setFinishTime(report.getFinishTime());        }      } catch (      ApplicationNotFoundException e) {        if (verbose) {
@VisibleForTesting void checkFilesAndSeedApps(FileSystem fs,Path remoteRootLogDir,String suffix,Path workingDir) throws IOException {  for (RemoteIterator<FileStatus> userIt=fs.listStatusIterator(remoteRootLogDir); userIt.hasNext(); ) {    Path userLogPath=userIt.next().getPath();    try {      for (RemoteIterator<FileStatus> appIt=fs.listStatusIterator(new Path(userLogPath,suffix)); appIt.hasNext(); ) {        Path appLogPath=appIt.next().getPath();        try {          FileStatus[] files=fs.listStatus(appLogPath);          if (files.length >= minNumLogFiles) {            boolean eligible=true;            long totalFileSize=0L;            for (            FileStatus file : files) {              if (file.getPath().getName().equals(appLogPath.getName() + ".har")) {                eligible=false;                if (verbose) {
        try {          FileStatus[] files=fs.listStatus(appLogPath);          if (files.length >= minNumLogFiles) {            boolean eligible=true;            long totalFileSize=0L;            for (            FileStatus file : files) {              if (file.getPath().getName().equals(appLogPath.getName() + ".har")) {                eligible=false;                if (verbose) {                  LOG.info("Skipping " + appLogPath.getName() + " due to existing .har file");                }                break;              }              totalFileSize+=file.getLen();              if (totalFileSize > maxTotalLogsSize) {                eligible=false;                if (verbose) {
            long totalFileSize=0L;            for (            FileStatus file : files) {              if (file.getPath().getName().equals(appLogPath.getName() + ".har")) {                eligible=false;                if (verbose) {                  LOG.info("Skipping " + appLogPath.getName() + " due to existing .har file");                }                break;              }              totalFileSize+=file.getLen();              if (totalFileSize > maxTotalLogsSize) {                eligible=false;                if (verbose) {                  LOG.info("Skipping " + appLogPath.getName() + " due to "+ "total file size being too large ("+ totalFileSize+ " > "+ maxTotalLogsSize+ ")");                }                break;              }            }            if (eligible) {              if (verbose) {
              if (totalFileSize > maxTotalLogsSize) {                eligible=false;                if (verbose) {                  LOG.info("Skipping " + appLogPath.getName() + " due to "+ "total file size being too large ("+ totalFileSize+ " > "+ maxTotalLogsSize+ ")");                }                break;              }            }            if (eligible) {              if (verbose) {                LOG.info("Adding " + appLogPath.getName() + " for user "+ userLogPath.getName());              }              AppInfo context=new AppInfo();              context.setAppId(appLogPath.getName());              context.setUser(userLogPath.getName());              context.setSuffix(suffix);              context.setRemoteRootLogDir(remoteRootLogDir);              context.setWorkingDir(workingDir);              eligibleApplications.add(context);
                  LOG.info("Skipping " + appLogPath.getName() + " due to "+ "total file size being too large ("+ totalFileSize+ " > "+ maxTotalLogsSize+ ")");                }                break;              }            }            if (eligible) {              if (verbose) {                LOG.info("Adding " + appLogPath.getName() + " for user "+ userLogPath.getName());              }              AppInfo context=new AppInfo();              context.setAppId(appLogPath.getName());              context.setUser(userLogPath.getName());              context.setSuffix(suffix);              context.setRemoteRootLogDir(remoteRootLogDir);              context.setWorkingDir(workingDir);              eligibleApplications.add(context);            }          } else {            if (verbose) {              LOG.info("Skipping " + appLogPath.getName() + " due to not "+ "having enough log files ("+ files.length+ " < "+ minNumLogFiles+ ")");
              }            }            if (eligible) {              if (verbose) {                LOG.info("Adding " + appLogPath.getName() + " for user "+ userLogPath.getName());              }              AppInfo context=new AppInfo();              context.setAppId(appLogPath.getName());              context.setUser(userLogPath.getName());              context.setSuffix(suffix);              context.setRemoteRootLogDir(remoteRootLogDir);              context.setWorkingDir(workingDir);              eligibleApplications.add(context);            }          } else {            if (verbose) {              LOG.info("Skipping " + appLogPath.getName() + " due to not "+ "having enough log files ("+ files.length+ " < "+ minNumLogFiles+ ")");            }          }        } catch (        IOException ioe) {          if (verbose) {
@VisibleForTesting void checkMaxEligible(){  if (maxEligible > 0 && eligibleApplications.size() > maxEligible) {    if (verbose) {
@VisibleForTesting void generateScript(File localScript) throws IOException {  if (verbose) {
private boolean runDistributedShell(File localScript) throws Exception {  String[] dsArgs={"--appname","ArchiveLogs","--jar",ApplicationMaster.class.getProtectionDomain().getCodeSource().getLocation().getPath(),"--num_containers",Integer.toString(eligibleApplications.size()),"--container_memory",Long.toString(memory),"--shell_script",localScript.getAbsolutePath()};  if (verbose) {
@Override public int run(String[] args) throws Exception {  handleOpts(args);  Integer exitCode=1;  UserGroupInformation loginUser=UserGroupInformation.getLoginUser();  if (!proxy || loginUser.getShortUserName().equals(user)) {
  checkNotEmpty();  if (reuseLastProvider && lastProvider != null) {    return lastProvider.getCredentials();  }  AmazonClientException lastException=null;  for (  AWSCredentialsProvider provider : providers) {    try {      AWSCredentials credentials=provider.getCredentials();      Preconditions.checkNotNull(credentials,"Null credentials returned by %s",provider);      if ((credentials.getAWSAccessKeyId() != null && credentials.getAWSSecretKey() != null) || (credentials instanceof AnonymousAWSCredentials)) {        lastProvider=provider;        LOG.debug("Using credentials from {}",provider);        return credentials;      }    } catch (    NoAwsCredentialsException e) {      if (lastException == null) {        lastException=e;
    return lastProvider.getCredentials();  }  AmazonClientException lastException=null;  for (  AWSCredentialsProvider provider : providers) {    try {      AWSCredentials credentials=provider.getCredentials();      Preconditions.checkNotNull(credentials,"Null credentials returned by %s",provider);      if ((credentials.getAWSAccessKeyId() != null && credentials.getAWSSecretKey() != null) || (credentials instanceof AnonymousAWSCredentials)) {        lastProvider=provider;        LOG.debug("Using credentials from {}",provider);        return credentials;      }    } catch (    NoAwsCredentialsException e) {      if (lastException == null) {        lastException=e;      }      LOG.debug("No credentials from {}: {}",provider,e.toString());    }catch (    AmazonClientException e) {
public boolean shouldDelay(String key){  float p=getDelayKeyProbability();  boolean delay=key.contains(getDelayKeySubstring());  delay=delay && trueWithProbability(p);
@Override public void deleteObject(DeleteObjectRequest deleteObjectRequest) throws AmazonClientException, AmazonServiceException {  String key=deleteObjectRequest.getKey();
@Override public PutObjectResult putObject(PutObjectRequest putObjectRequest) throws AmazonClientException, AmazonServiceException {
private ObjectListing innerlistObjects(ListObjectsRequest listObjectsRequest) throws AmazonClientException, AmazonServiceException {
private ListObjectsV2Result innerListObjectsV2(ListObjectsV2Request request){
private void addSummaryIfNotPresent(List<S3ObjectSummary> list,S3ObjectSummary item){  String key=item.getKey();  if (list.stream().noneMatch((member) -> member.getKey().equals(key))) {
private void addPrefixIfNotPresent(List<String> prefixes,String ancestor,String child){  Path prefixCandidate=new Path(child).getParent();  Path ancestorPath=new Path(ancestor);  Preconditions.checkArgument(child.startsWith(ancestor),"%s does not " + "start with %s",child,ancestor);  while (!prefixCandidate.isRoot()) {    Path nextParent=prefixCandidate.getParent();    if (nextParent.equals(ancestorPath)) {      String prefix=prefixCandidate.toString();      if (!prefixes.contains(prefix)) {
private boolean isKeyDelayed(Long enqueueTime,String key){  if (enqueueTime == null) {
private void enqueueDelayedPut(String key){
@Override public S3Object getObject(GetObjectRequest var1) throws SdkClientException, AmazonServiceException {  maybeFail("file not found",404);  S3Object o=super.getObject(var1);
@Override public S3Object getObject(String bucketName,String key) throws SdkClientException, AmazonServiceException {  S3Object o=super.getObject(bucketName,key);
@Retries.RetryRaw public <T>T retryUntranslated(String text,boolean idempotent,Retried retrying,Operation<T> operation) throws IOException {  Preconditions.checkArgument(retrying != null,"null retrying argument");  int retryCount=0;  Exception caught;  RetryPolicy.RetryAction retryAction;  boolean shouldRetry;  do {    try {      if (retryCount > 0) {
public RemoteIterator<S3ALocatedFileStatus> getListFilesAssumingDir(Path path,boolean recursive,Listing.FileStatusAcceptor acceptor,boolean collectTombstones,boolean forceNonAuthoritativeMS) throws IOException {  String key=maybeAddTrailingSlash(pathToKey(path));  String delimiter=recursive ? null : "/";  if (recursive) {
public long uploadCompleted(){  long delta=upload.getProgress().getBytesTransferred() - lastBytesTransferred;  if (delta > 0) {
private synchronized S3ADataBlocks.DataBlock createBlockIfNeeded() throws IOException {  if (activeBlock == null) {    blockCount++;    if (blockCount >= Constants.MAX_MULTIPART_COUNT) {
    if (multiPartUpload == null) {      if (hasBlock) {        bytes=putObject();        bytesSubmitted=bytes;      }    } else {      if (hasBlock && (block.hasData() || multiPartUpload.getPartsSubmitted() == 0)) {        uploadCurrentBlock();      }      final List<PartETag> partETags=multiPartUpload.waitForAllPartUploads();      bytes=bytesSubmitted;      if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),partETags,bytes)) {        multiPartUpload.complete(partETags);      } else {        LOG.info("File {} will be visible when the job is committed",key);      }    }    if (!putTracker.outputImmediatelyVisible()) {      statistics.commitUploaded(bytes);
      if (hasBlock && (block.hasData() || multiPartUpload.getPartsSubmitted() == 0)) {        uploadCurrentBlock();      }      final List<PartETag> partETags=multiPartUpload.waitForAllPartUploads();      bytes=bytesSubmitted;      if (putTracker.aboutToComplete(multiPartUpload.getUploadId(),partETags,bytes)) {        multiPartUpload.complete(partETags);      } else {        LOG.info("File {} will be visible when the job is committed",key);      }    }    if (!putTracker.outputImmediatelyVisible()) {      statistics.commitUploaded(bytes);    }    LOG.debug("Upload complete to {} by {}",key,writeOperationHelper);  } catch (  IOException ioe) {    if (multiPartUpload != null) {      multiPartUpload.abort();    }    writeOperationHelper.writeFailed(ioe);
public void initialize(URI name,Configuration originalConf) throws IOException {  bucket=name.getHost();  try {
    initCannedAcls(conf);    doBucketProbing();    inputPolicy=S3AInputPolicy.getPolicy(conf.getTrimmed(INPUT_FADVISE,INPUT_FADV_NORMAL));    LOG.debug("Input fadvise policy = {}",inputPolicy);    changeDetectionPolicy=ChangeDetectionPolicy.getPolicy(conf);    LOG.debug("Change detection policy = {}",changeDetectionPolicy);    boolean magicCommitterEnabled=conf.getBoolean(CommitConstants.MAGIC_COMMITTER_ENABLED,CommitConstants.DEFAULT_MAGIC_COMMITTER_ENABLED);    LOG.debug("Filesystem support for magic committers {} enabled",magicCommitterEnabled ? "is" : "is not");    committerIntegration=new MagicCommitIntegration(this,magicCommitterEnabled);    selectBinding=new SelectBinding(writeHelper);    boolean blockUploadEnabled=conf.getBoolean(FAST_UPLOAD,true);    if (!blockUploadEnabled) {      LOG.warn("The \"slow\" output stream is no longer supported");    }    blockOutputBuffer=conf.getTrimmed(FAST_UPLOAD_BUFFER,DEFAULT_FAST_UPLOAD_BUFFER);    partSize=ensureOutputParameterInRange(MULTIPART_SIZE,partSize);
    LOG.debug("Filesystem support for magic committers {} enabled",magicCommitterEnabled ? "is" : "is not");    committerIntegration=new MagicCommitIntegration(this,magicCommitterEnabled);    selectBinding=new SelectBinding(writeHelper);    boolean blockUploadEnabled=conf.getBoolean(FAST_UPLOAD,true);    if (!blockUploadEnabled) {      LOG.warn("The \"slow\" output stream is no longer supported");    }    blockOutputBuffer=conf.getTrimmed(FAST_UPLOAD_BUFFER,DEFAULT_FAST_UPLOAD_BUFFER);    partSize=ensureOutputParameterInRange(MULTIPART_SIZE,partSize);    blockFactory=S3ADataBlocks.createFactory(this,blockOutputBuffer);    blockOutputActiveBlocks=intOption(conf,FAST_UPLOAD_ACTIVE_BLOCKS,DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS,1);    LOG.debug("Using S3ABlockOutputStream with buffer = {}; block={};" + " queue limit={}",blockOutputBuffer,partSize,blockOutputActiveBlocks);    long authDirTtl=conf.getTimeDuration(METADATASTORE_METADATA_TTL,DEFAULT_METADATASTORE_METADATA_TTL,TimeUnit.MILLISECONDS);    ttlTimeProvider=new S3Guard.TtlTimeProvider(authDirTtl);    setMetadataStore(S3Guard.getMetadataStore(this,ttlTimeProvider));    allowAuthoritativeMetadataStore=conf.getBoolean(METADATASTORE_AUTHORITATIVE,DEFAULT_METADATASTORE_AUTHORITATIVE);
@Retries.RetryTranslated public void abortOutstandingMultipartUploads(long seconds) throws IOException {  Preconditions.checkArgument(seconds >= 0);  Date purgeBefore=new Date(new Date().getTime() - seconds * 1000);
protected void setAmazonS3Client(AmazonS3 client){  Preconditions.checkNotNull(client,"client");
@InterfaceStability.Unstable public void setInputPolicy(S3AInputPolicy inputPolicy){  Objects.requireNonNull(inputPolicy,"Null inputStrategy");
@Override public Path makeQualified(final Path path){  Path q=super.makeQualified(path);  if (!q.isRoot()) {    String urlString=q.toUri().toString();    if (urlString.endsWith(Path.SEPARATOR)) {
@Retries.RetryTranslated public boolean rename(Path src,Path dst) throws IOException {  try (DurationInfo ignored=new DurationInfo(LOG,false,"rename(%s, %s",src,dst)){    long bytesCopied=innerRename(src,dst);
  }  S3AFileStatus srcStatus=innerGetFileStatus(src,true,StatusProbeEnum.ALL);  if (srcKey.equals(dstKey)) {    LOG.debug("rename: src and dest refer to the same file or directory: {}",dst);    throw new RenameFailedException(src,dst,"source and dest refer to the same file or directory").withExitCode(srcStatus.isFile());  }  S3AFileStatus dstStatus=null;  try {    dstStatus=innerGetFileStatus(dst,true,StatusProbeEnum.ALL);    if (srcStatus.isDirectory()) {      if (dstStatus.isFile()) {        throw new RenameFailedException(src,dst,"source is a directory and dest is a file").withExitCode(srcStatus.isFile());      } else       if (dstStatus.isEmptyDirectory() != Tristate.TRUE) {        throw new RenameFailedException(src,dst,"Destination is a non-empty directory").withExitCode(false);      }    } else {      if (dstStatus.isFile()) {        throw new RenameFailedException(src,dst,"Cannot rename onto an existing file").withExitCode(false);
@Retries.RetryMixed private long innerRename(Path source,Path dest) throws RenameFailedException, FileNotFoundException, IOException, AmazonClientException {  Path src=qualify(source);  Path dst=qualify(dest);
@Retries.RetryTranslated public Map<String,Object> getObjectHeaders(Path path) throws IOException {
@Retries.RetryRaw protected ObjectMetadata getObjectMetadata(String key,ChangeTracker changeTracker,Invoker changeInvoker,String operation) throws IOException {  GetObjectMetadataRequest request=new GetObjectMetadataRequest(bucket,key);  generateSSECustomerKey().ifPresent(request::setSSECustomerKey);  ObjectMetadata meta=changeInvoker.retryUntranslated("GET " + key,true,() -> {    incrementStatistic(OBJECT_METADATA_REQUESTS);
@Retries.RetryRaw protected S3ListResult listObjects(S3ListRequest request) throws IOException {  incrementReadOperations();  incrementStatistic(OBJECT_LIST_REQUESTS);
@Retries.OnceRaw public UploadInfo putObject(PutObjectRequest putObjectRequest){  long len=getPutRequestLength(putObjectRequest);
@VisibleForTesting @Retries.OnceRaw("For PUT; post-PUT actions are RetryTranslated") PutObjectResult putObjectDirect(PutObjectRequest putObjectRequest) throws AmazonClientException, MetadataPersistenceException {  long len=getPutRequestLength(putObjectRequest);
public void incrementPutStartStatistics(long bytes){
public void incrementPutCompletedStatistics(boolean success,long bytes){
@Retries.RetryRaw private DeleteObjectsResult removeKeysS3(List<DeleteObjectsRequest.KeyVersion> keysToDelete,boolean deleteFakeDir,boolean quiet) throws MultiObjectDeleteException, AmazonClientException, IOException {  if (LOG.isDebugEnabled()) {
@Retries.RetryRaw private DeleteObjectsResult removeKeysS3(List<DeleteObjectsRequest.KeyVersion> keysToDelete,boolean deleteFakeDir,boolean quiet) throws MultiObjectDeleteException, AmazonClientException, IOException {  if (LOG.isDebugEnabled()) {    LOG.debug("Initiating delete operation for {} objects",keysToDelete.size());    for (    DeleteObjectsRequest.KeyVersion key : keysToDelete) {
@Retries.RetryTranslated private void createFakeDirectoryIfNecessary(Path f) throws IOException, AmazonClientException {  String key=pathToKey(f);  if (!key.isEmpty() && !s3Exists(f,StatusProbeEnum.DIRECTORIES)) {
private S3AFileStatus[] innerListStatus(Path f) throws FileNotFoundException, IOException, AmazonClientException {  Path path=qualify(f);  String key=pathToKey(path);
private S3AFileStatus[] innerListStatus(Path f) throws FileNotFoundException, IOException, AmazonClientException {  Path path=qualify(f);  String key=pathToKey(path);  LOG.debug("List status for path: {}",path);  entryPoint(INVOCATION_LIST_STATUS);  List<S3AFileStatus> result;  final S3AFileStatus fileStatus=innerGetFileStatus(path,false,StatusProbeEnum.ALL);  if (fileStatus.isDirectory()) {    if (!key.isEmpty()) {      key=key + '/';    }    boolean allowAuthoritative=allowAuthoritative(f);    DirListingMetadata dirMeta=S3Guard.listChildrenWithTtl(metadataStore,path,ttlTimeProvider,allowAuthoritative);    if (allowAuthoritative && dirMeta != null && dirMeta.isAuthoritative()) {      return S3Guard.dirMetaToStatuses(dirMeta);    }    S3ListRequest request=createListObjectsRequest(key,"/");
  if (fileStatus.isDirectory()) {    if (!key.isEmpty()) {      key=key + '/';    }    boolean allowAuthoritative=allowAuthoritative(f);    DirListingMetadata dirMeta=S3Guard.listChildrenWithTtl(metadataStore,path,ttlTimeProvider,allowAuthoritative);    if (allowAuthoritative && dirMeta != null && dirMeta.isAuthoritative()) {      return S3Guard.dirMetaToStatuses(dirMeta);    }    S3ListRequest request=createListObjectsRequest(key,"/");    LOG.debug("listStatus: doing listObjects for directory {}",key);    Listing.FileStatusListingIterator files=listing.createFileStatusListingIterator(path,request,ACCEPT_ALL,new Listing.AcceptAllButSelfAndS3nDirs(path));    result=new ArrayList<>(files.getBatchSize());    while (files.hasNext()) {      result.add(files.next());    }    return S3Guard.dirListingUnion(metadataStore,path,result,dirMeta,allowAuthoritative,ttlTimeProvider);  } else {
private boolean innerMkdirs(Path p,FsPermission permission) throws IOException, FileAlreadyExistsException, AmazonClientException {  Path f=qualify(p);
@VisibleForTesting @Retries.RetryTranslated S3AFileStatus innerGetFileStatus(final Path f,final boolean needEmptyDirectoryFlag,final Set<StatusProbeEnum> probes) throws IOException {  final Path path=qualify(f);  String key=pathToKey(path);
@VisibleForTesting @Retries.RetryTranslated S3AFileStatus innerGetFileStatus(final Path f,final boolean needEmptyDirectoryFlag,final Set<StatusProbeEnum> probes) throws IOException {  final Path path=qualify(f);  String key=pathToKey(path);  LOG.debug("Getting path status for {}  ({}); needEmptyDirectory={}",path,key,needEmptyDirectoryFlag);  boolean allowAuthoritative=allowAuthoritative(path);  PathMetadata pm=null;  if (hasMetadataStore()) {    pm=S3Guard.getWithTtl(metadataStore,path,ttlTimeProvider,needEmptyDirectoryFlag,allowAuthoritative);  }  Set<Path> tombstones=Collections.emptySet();  if (pm != null) {    S3AFileStatus msStatus=pm.getFileStatus();    if (pm.isDeleted()) {      OffsetDateTime deletedAt=OffsetDateTime.ofInstant(Instant.ofEpochMilli(msStatus.getModificationTime()),ZoneOffset.UTC);      throw new FileNotFoundException("Path " + path + " is recorded as "+ "deleted by S3Guard at "+ deletedAt);    }    if (!msStatus.isDirectory() && !allowAuthoritative && probes.contains(StatusProbeEnum.Head)) {
  boolean allowAuthoritative=allowAuthoritative(path);  PathMetadata pm=null;  if (hasMetadataStore()) {    pm=S3Guard.getWithTtl(metadataStore,path,ttlTimeProvider,needEmptyDirectoryFlag,allowAuthoritative);  }  Set<Path> tombstones=Collections.emptySet();  if (pm != null) {    S3AFileStatus msStatus=pm.getFileStatus();    if (pm.isDeleted()) {      OffsetDateTime deletedAt=OffsetDateTime.ofInstant(Instant.ofEpochMilli(msStatus.getModificationTime()),ZoneOffset.UTC);      throw new FileNotFoundException("Path " + path + " is recorded as "+ "deleted by S3Guard at "+ deletedAt);    }    if (!msStatus.isDirectory() && !allowAuthoritative && probes.contains(StatusProbeEnum.Head)) {      LOG.debug("Metadata for {} found in the non-auth metastore.",path);      long validTime=ttlTimeProvider.getNow() - ttlTimeProvider.getMetadataTtl();      final long msModTime=msStatus.getModificationTime();      if (msModTime < validTime) {
  Set<Path> tombstones=Collections.emptySet();  if (pm != null) {    S3AFileStatus msStatus=pm.getFileStatus();    if (pm.isDeleted()) {      OffsetDateTime deletedAt=OffsetDateTime.ofInstant(Instant.ofEpochMilli(msStatus.getModificationTime()),ZoneOffset.UTC);      throw new FileNotFoundException("Path " + path + " is recorded as "+ "deleted by S3Guard at "+ deletedAt);    }    if (!msStatus.isDirectory() && !allowAuthoritative && probes.contains(StatusProbeEnum.Head)) {      LOG.debug("Metadata for {} found in the non-auth metastore.",path);      long validTime=ttlTimeProvider.getNow() - ttlTimeProvider.getMetadataTtl();      final long msModTime=msStatus.getModificationTime();      if (msModTime < validTime) {        LOG.debug("Metastore entry of {} is out of date, probing S3",path);        try {          S3AFileStatus s3AFileStatus=s3GetFileStatus(path,key,probes,tombstones,needEmptyDirectoryFlag);          final long s3ModTime=s3AFileStatus.getModificationTime();
          final long s3ModTime=s3AFileStatus.getModificationTime();          if (s3ModTime > msModTime) {            LOG.debug("S3Guard metadata for {} is outdated;" + " s3modtime={}; msModTime={} updating metastore",path,s3ModTime,msModTime);            S3Guard.putAndReturn(metadataStore,s3AFileStatus,ttlTimeProvider);          } else {            S3Guard.refreshEntry(metadataStore,pm,s3AFileStatus,ttlTimeProvider);          }          return s3AFileStatus;        } catch (        FileNotFoundException fne) {          LOG.warn("Failed to find file {}. Either it is not yet visible, or " + "it has been deleted.",path);        }      }    }    if (needEmptyDirectoryFlag && msStatus.isDirectory()) {      if (pm.isEmptyDirectory() != Tristate.UNKNOWN) {        return msStatus;      } else {        DirListingMetadata children=S3Guard.listChildrenWithTtl(metadataStore,path,ttlTimeProvider,allowAuthoritative);        if (children != null) {
@VisibleForTesting @Retries.RetryTranslated S3AFileStatus s3GetFileStatus(final Path path,final String key,final Set<StatusProbeEnum> probes,@Nullable final Set<Path> tombstones,final boolean needEmptyDirectoryFlag) throws IOException {
@VisibleForTesting @Retries.RetryTranslated S3AFileStatus s3GetFileStatus(final Path path,final String key,final Set<StatusProbeEnum> probes,@Nullable final Set<Path> tombstones,final boolean needEmptyDirectoryFlag) throws IOException {  LOG.debug("S3GetFileStatus {}",path);  Preconditions.checkArgument(!needEmptyDirectoryFlag || probes.contains(StatusProbeEnum.List),"s3GetFileStatus(%s) wants to know if a directory is empty but" + " does not request a list probe",path);  if (!key.isEmpty() && !key.endsWith("/") && probes.contains(StatusProbeEnum.Head)) {    try {      ObjectMetadata meta=getObjectMetadata(key);
      } else {        listSize=Math.min(2 + tombstones.size(),Math.max(2,maxKeys));      }      S3ListRequest request=createListObjectsRequest(dirKey,"/",listSize);      S3ListResult listResult=listObjects(request);      if (listResult.hasPrefixesOrObjects(contextAccessors,tombstones)) {        if (LOG.isDebugEnabled()) {          LOG.debug("Found path as directory (with /)");          listResult.logAtDebug(LOG);        }        if (needEmptyDirectoryFlag && listResult.representsEmptyDirectory(contextAccessors,dirKey,tombstones)) {          return new S3AFileStatus(Tristate.TRUE,path,username);        }        return new S3AFileStatus(Tristate.FALSE,path,username);      } else       if (key.isEmpty()) {        LOG.debug("Found root directory");        return new S3AFileStatus(Tristate.TRUE,path,username);      }    } catch (    AmazonServiceException e) {
@Override public void copyFromLocalFile(boolean delSrc,boolean overwrite,Path src,Path dst) throws IOException {  entryPoint(INVOCATION_COPY_FROM_LOCAL_FILE);
@Retries.RetryTranslated private void innerCopyFromLocalFile(boolean delSrc,boolean overwrite,Path src,Path dst) throws IOException, FileAlreadyExistsException, AmazonClientException {  entryPoint(INVOCATION_COPY_FROM_LOCAL_FILE);
@Retries.RetryTranslated private CopyResult copyFile(String srcKey,String dstKey,long size,S3ObjectAttributes srcAttributes,S3AReadOpContext readContext) throws IOException, InterruptedIOException {
@Retries.RetryTranslated private CopyResult copyFile(String srcKey,String dstKey,long size,S3ObjectAttributes srcAttributes,S3AReadOpContext readContext) throws IOException, InterruptedIOException {  LOG.debug("copyFile {} -> {} ",srcKey,dstKey);  ProgressListener progressListener=progressEvent -> {switch (progressEvent.getEventType()) {case TRANSFER_PART_COMPLETED_EVENT:      incrementWriteOperations();    break;default:  break;}};ChangeTracker changeTracker=new ChangeTracker(keyToQualifiedPath(srcKey).toString(),changeDetectionPolicy,readContext.instrumentation.newInputStreamStatistics().getVersionMismatchCounter(),srcAttributes);String action="copyFile(" + srcKey + ", "+ dstKey+ ")";Invoker readInvoker=readContext.getReadInvoker();ObjectMetadata srcom;try {srcom=once(action,srcKey,() -> getObjectMetadata(srcKey,changeTracker,readInvoker,"copy"));} catch (FileNotFoundException e) {
private void setOptionalCopyObjectRequestParameters(ObjectMetadata srcom,CopyObjectRequest copyObjectRequest){  String sourceKMSId=srcom.getSSEAwsKmsKeyId();  if (isNotEmpty(sourceKMSId)) {
@Retries.OnceRaw InitiateMultipartUploadResult initiateMultipartUpload(InitiateMultipartUploadRequest request) throws IOException {
@InterfaceAudience.Private @Retries.RetryTranslated("Except if failOnMetadataWriteError=false, in which" + " case RetryExceptionsSwallowed") void finishedWrite(String key,long length,String eTag,String versionId,@Nullable final BulkOperationState operationState) throws MetadataPersistenceException {
      if (activeState == null) {        stateToClose=S3Guard.initiateBulkWrite(metadataStore,isDir ? BulkOperationState.OperationType.Mkdir : BulkOperationState.OperationType.Put,keyToPath(key));        activeState=stateToClose;      }      S3Guard.addAncestors(metadataStore,p,ttlTimeProvider,activeState);      S3AFileStatus status=createUploadFileStatus(p,isDir,length,getDefaultBlockSize(p),username,eTag,versionId);      boolean authoritative=false;      if (isDir) {        status.setIsEmptyDirectory(Tristate.TRUE);        authoritative=allowAuthoritative(p);      }      if (!authoritative) {        S3Guard.putAndReturn(metadataStore,status,ttlTimeProvider,activeState);      } else {        S3Guard.putAuthDirectoryMarker(metadataStore,status,ttlTimeProvider,activeState);      }    }    waitForCompletionIgnoringExceptions(deletion);  } catch (  IOException e) {
@Override @Retries.RetryTranslated public EtagChecksum getFileChecksum(Path f,final long length) throws IOException {  Preconditions.checkArgument(length >= 0);  entryPoint(INVOCATION_GET_FILE_CHECKSUM);  if (getConf().getBoolean(ETAG_CHECKSUM_ENABLED,ETAG_CHECKSUM_ENABLED_DEFAULT)) {    Path path=qualify(f);
@Retries.RetryTranslated private RemoteIterator<S3ALocatedFileStatus> innerListFiles(final Path f,final boolean recursive,final Listing.FileStatusAcceptor acceptor,final S3AFileStatus status,final boolean collectTombstones,final boolean forceNonAuthoritativeMS) throws IOException {  entryPoint(INVOCATION_LIST_FILES);  Path path=qualify(f);
@Retries.RetryTranslated private RemoteIterator<S3ALocatedFileStatus> innerListFiles(final Path f,final boolean recursive,final Listing.FileStatusAcceptor acceptor,final S3AFileStatus status,final boolean collectTombstones,final boolean forceNonAuthoritativeMS) throws IOException {  entryPoint(INVOCATION_LIST_FILES);  Path path=qualify(f);  LOG.debug("listFiles({}, {})",path,recursive);  try {    if (status != null && status.isFile()) {
@Override @Retries.OnceTranslated("s3guard not retrying") public RemoteIterator<LocatedFileStatus> listLocatedStatus(final Path f,final PathFilter filter) throws FileNotFoundException, IOException {  entryPoint(INVOCATION_LIST_LOCATED_STATUS);  Path path=qualify(f);
@Retries.OnceRaw void abortMultipartUpload(String destKey,String uploadId){
@Retries.OnceRaw void abortMultipartUpload(MultipartUpload upload){  String destKey;  String uploadId;  destKey=upload.getKey();  uploadId=upload.getUploadId();  if (LOG.isInfoEnabled()) {    DateFormat df=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
public AWSCredentialProviderList shareCredentials(final String purpose){
@Override @Retries.RetryTranslated public CompletableFuture<FSDataInputStream> openFileWithOptions(final Path rawPath,final OpenFileParameters parameters) throws IOException {  final Path path=qualify(rawPath);  Configuration options=parameters.getOptions();  Set<String> mandatoryKeys=parameters.getMandatoryKeys();  String sql=options.get(SelectConstants.SELECT_SQL,null);  boolean isSelect=sql != null;  if (isSelect) {    rejectUnknownMandatoryKeys(mandatoryKeys,InternalSelectConstants.SELECT_OPTIONS,"for " + path + " in S3 Select operation");  } else {    rejectUnknownMandatoryKeys(mandatoryKeys,InternalConstants.STANDARD_OPENFILE_KEYS,"for " + path + " in non-select file I/O");  }  FileStatus providedStatus=parameters.getStatus();  S3AFileStatus fileStatus;  if (providedStatus != null) {    Preconditions.checkArgument(path.equals(providedStatus.getPath()),"FileStatus parameter is not for the path %s: %s",path,providedStatus);    if (providedStatus instanceof S3AFileStatus) {
  Set<String> mandatoryKeys=parameters.getMandatoryKeys();  String sql=options.get(SelectConstants.SELECT_SQL,null);  boolean isSelect=sql != null;  if (isSelect) {    rejectUnknownMandatoryKeys(mandatoryKeys,InternalSelectConstants.SELECT_OPTIONS,"for " + path + " in S3 Select operation");  } else {    rejectUnknownMandatoryKeys(mandatoryKeys,InternalConstants.STANDARD_OPENFILE_KEYS,"for " + path + " in non-select file I/O");  }  FileStatus providedStatus=parameters.getStatus();  S3AFileStatus fileStatus;  if (providedStatus != null) {    Preconditions.checkArgument(path.equals(providedStatus.getPath()),"FileStatus parameter is not for the path %s: %s",path,providedStatus);    if (providedStatus instanceof S3AFileStatus) {      LOG.debug("File was opened with a supplied S3AFileStatus;" + " skipping getFileStatus call in open() operation: {}",providedStatus);      fileStatus=(S3AFileStatus)providedStatus;    } else     if (providedStatus instanceof S3ALocatedFileStatus) {
  if (isSelect) {    rejectUnknownMandatoryKeys(mandatoryKeys,InternalSelectConstants.SELECT_OPTIONS,"for " + path + " in S3 Select operation");  } else {    rejectUnknownMandatoryKeys(mandatoryKeys,InternalConstants.STANDARD_OPENFILE_KEYS,"for " + path + " in non-select file I/O");  }  FileStatus providedStatus=parameters.getStatus();  S3AFileStatus fileStatus;  if (providedStatus != null) {    Preconditions.checkArgument(path.equals(providedStatus.getPath()),"FileStatus parameter is not for the path %s: %s",path,providedStatus);    if (providedStatus instanceof S3AFileStatus) {      LOG.debug("File was opened with a supplied S3AFileStatus;" + " skipping getFileStatus call in open() operation: {}",providedStatus);      fileStatus=(S3AFileStatus)providedStatus;    } else     if (providedStatus instanceof S3ALocatedFileStatus) {      LOG.debug("File was opened with a supplied S3ALocatedFileStatus;" + " skipping getFileStatus call in open() operation: {}",providedStatus);      fileStatus=((S3ALocatedFileStatus)providedStatus).toS3AFileStatus();    } else {
  }  long diff=targetPos - pos;  if (diff > 0) {    int available=wrappedStream.available();    long forwardSeekRange=Math.max(readahead,available);    long remainingInCurrentRequest=remainingInCurrentRequest();    long forwardSeekLimit=Math.min(remainingInCurrentRequest,forwardSeekRange);    boolean skipForward=remainingInCurrentRequest > 0 && diff < forwardSeekLimit;    if (skipForward) {      LOG.debug("Forward seek on {}, of {} bytes",uri,diff);      streamStatistics.seekForwards(diff);      long skipped=wrappedStream.skip(diff);      if (skipped > 0) {        pos+=skipped;        incrementBytesRead(diff);      }      if (pos == targetPos) {
@Retries.OnceTranslated private void onReadFailure(IOException ioe,int length,boolean forceAbort) throws IOException {
@Retries.OnceRaw private void closeStream(String reason,long length,boolean forceAbort){  if (isObjectStreamOpen()) {    long remaining=remainingInCurrentRequest();
public MutableGaugeLong lookupGauge(String name){  MutableMetric metric=lookupMetric(name);  if (metric == null) {
public MutableQuantiles lookupQuantiles(String name){  MutableMetric metric=lookupMetric(name);  if (metric == null) {
public long incrementCounter(Statistic op,long count){  long updated=opsCount.get(op).addAndGet(count);
public static AWSCredentialProviderList createAWSCredentialProviderSet(@Nullable URI binding,Configuration conf) throws IOException {  S3xLoginHelper.rejectSecretsInURIs(binding);  AWSCredentialProviderList credentials=buildAWSProviderList(binding,conf,AWS_CREDENTIALS_PROVIDER,STANDARD_AWS_PROVIDERS,new HashSet<>());
public static int intOption(Configuration conf,String key,int defVal,int min){  int v=conf.getInt(key,defVal);  Preconditions.checkArgument(v >= min,String.format("Value of %s: %d is below the minimum value %d",key,v,min));
public static long longOption(Configuration conf,String key,long defVal,long min){  long v=conf.getLong(key,defVal);  Preconditions.checkArgument(v >= min,String.format("Value of %s: %d is below the minimum value %d",key,v,min));
public static long longBytesOption(Configuration conf,String key,long defVal,long min){  long v=conf.getLongBytes(key,defVal);  Preconditions.checkArgument(v >= min,String.format("Value of %s: %d is below the minimum value %d",key,v,min));
public static Configuration propagateBucketOptions(Configuration source,String bucket){  Preconditions.checkArgument(StringUtils.isNotEmpty(bucket),"bucket");  final String bucketPrefix=FS_S3A_BUCKET_PREFIX + bucket + '.';
  Preconditions.checkArgument(StringUtils.isNotEmpty(bucket),"bucket");  final String bucketPrefix=FS_S3A_BUCKET_PREFIX + bucket + '.';  LOG.debug("Propagating entries under {}",bucketPrefix);  final Configuration dest=new Configuration(source);  for (  Map.Entry<String,String> entry : source) {    final String key=entry.getKey();    final String value=entry.getValue();    if (!key.startsWith(bucketPrefix) || bucketPrefix.equals(key)) {      continue;    }    final String stripped=key.substring(bucketPrefix.length());    if (stripped.startsWith("bucket.") || "impl".equals(stripped)) {      LOG.debug("Ignoring bucket option {}",key);    } else {      String origin="[" + StringUtils.join(source.getPropertySources(key),", ") + "]";      final String generic=FS_S3A_PREFIX + stripped;
  initProxySupport(conf,bucket,awsConf);  initUserAgent(conf,awsConf);  if (StringUtils.isNotEmpty(awsServiceIdentifier)) {    String configKey=null;switch (awsServiceIdentifier) {case AWS_SERVICE_IDENTIFIER_S3:      configKey=SIGNING_ALGORITHM_S3;    break;case AWS_SERVICE_IDENTIFIER_DDB:  configKey=SIGNING_ALGORITHM_DDB;break;case AWS_SERVICE_IDENTIFIER_STS:configKey=SIGNING_ALGORITHM_STS;break;default:}if (configKey != null) {String signerOverride=conf.getTrimmed(configKey,"");if (!signerOverride.isEmpty()) {
public static void initConnectionSettings(Configuration conf,ClientConfiguration awsConf) throws IOException {  awsConf.setMaxConnections(intOption(conf,MAXIMUM_CONNECTIONS,DEFAULT_MAXIMUM_CONNECTIONS,1));  initProtocolSettings(conf,awsConf);  awsConf.setMaxErrorRetry(intOption(conf,MAX_ERROR_RETRIES,DEFAULT_MAX_ERROR_RETRIES,0));  awsConf.setConnectionTimeout(intOption(conf,ESTABLISH_TIMEOUT,DEFAULT_ESTABLISH_TIMEOUT,0));  awsConf.setSocketTimeout(intOption(conf,SOCKET_TIMEOUT,DEFAULT_SOCKET_TIMEOUT,0));  int sockSendBuffer=intOption(conf,SOCKET_SEND_BUFFER,DEFAULT_SOCKET_SEND_BUFFER,2048);  int sockRecvBuffer=intOption(conf,SOCKET_RECV_BUFFER,DEFAULT_SOCKET_RECV_BUFFER,2048);  long requestTimeoutMillis=conf.getTimeDuration(REQUEST_TIMEOUT,DEFAULT_REQUEST_TIMEOUT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);  if (requestTimeoutMillis > Integer.MAX_VALUE) {
  awsConf.setMaxConnections(intOption(conf,MAXIMUM_CONNECTIONS,DEFAULT_MAXIMUM_CONNECTIONS,1));  initProtocolSettings(conf,awsConf);  awsConf.setMaxErrorRetry(intOption(conf,MAX_ERROR_RETRIES,DEFAULT_MAX_ERROR_RETRIES,0));  awsConf.setConnectionTimeout(intOption(conf,ESTABLISH_TIMEOUT,DEFAULT_ESTABLISH_TIMEOUT,0));  awsConf.setSocketTimeout(intOption(conf,SOCKET_TIMEOUT,DEFAULT_SOCKET_TIMEOUT,0));  int sockSendBuffer=intOption(conf,SOCKET_SEND_BUFFER,DEFAULT_SOCKET_SEND_BUFFER,2048);  int sockRecvBuffer=intOption(conf,SOCKET_RECV_BUFFER,DEFAULT_SOCKET_RECV_BUFFER,2048);  long requestTimeoutMillis=conf.getTimeDuration(REQUEST_TIMEOUT,DEFAULT_REQUEST_TIMEOUT,TimeUnit.SECONDS,TimeUnit.MILLISECONDS);  if (requestTimeoutMillis > Integer.MAX_VALUE) {    LOG.debug("Request timeout is too high({} ms). Setting to {} ms instead",requestTimeoutMillis,Integer.MAX_VALUE);    requestTimeoutMillis=Integer.MAX_VALUE;  }  awsConf.setRequestTimeout((int)requestTimeoutMillis);  awsConf.setSocketBufferSizeHints(sockSendBuffer,sockRecvBuffer);  String signerOverride=conf.getTrimmed(SIGNING_ALGORITHM,"");  if (!signerOverride.isEmpty()) {
  if (!proxyHost.isEmpty()) {    awsConf.setProxyHost(proxyHost);    if (proxyPort >= 0) {      awsConf.setProxyPort(proxyPort);    } else {      if (conf.getBoolean(SECURE_CONNECTIONS,DEFAULT_SECURE_CONNECTIONS)) {        LOG.warn("Proxy host set without port. Using HTTPS default 443");        awsConf.setProxyPort(443);      } else {        LOG.warn("Proxy host set without port. Using HTTP default 80");        awsConf.setProxyPort(80);      }    }    final String proxyUsername=lookupPassword(bucket,conf,PROXY_USERNAME,null,null);    final String proxyPassword=lookupPassword(bucket,conf,PROXY_PASSWORD,null,null);    if ((proxyUsername == null) != (proxyPassword == null)) {      String msg="Proxy error: " + PROXY_USERNAME + " or "+ PROXY_PASSWORD+ " set without the other.";
        LOG.warn("Proxy host set without port. Using HTTPS default 443");        awsConf.setProxyPort(443);      } else {        LOG.warn("Proxy host set without port. Using HTTP default 80");        awsConf.setProxyPort(80);      }    }    final String proxyUsername=lookupPassword(bucket,conf,PROXY_USERNAME,null,null);    final String proxyPassword=lookupPassword(bucket,conf,PROXY_PASSWORD,null,null);    if ((proxyUsername == null) != (proxyPassword == null)) {      String msg="Proxy error: " + PROXY_USERNAME + " or "+ PROXY_PASSWORD+ " set without the other.";      LOG.error(msg);      throw new IllegalArgumentException(msg);    }    awsConf.setProxyUsername(proxyUsername);    awsConf.setProxyPassword(proxyPassword);    awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));    awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));
 else {        LOG.warn("Proxy host set without port. Using HTTP default 80");        awsConf.setProxyPort(80);      }    }    final String proxyUsername=lookupPassword(bucket,conf,PROXY_USERNAME,null,null);    final String proxyPassword=lookupPassword(bucket,conf,PROXY_PASSWORD,null,null);    if ((proxyUsername == null) != (proxyPassword == null)) {      String msg="Proxy error: " + PROXY_USERNAME + " or "+ PROXY_PASSWORD+ " set without the other.";      LOG.error(msg);      throw new IllegalArgumentException(msg);    }    awsConf.setProxyUsername(proxyUsername);    awsConf.setProxyPassword(proxyPassword);    awsConf.setProxyDomain(conf.getTrimmed(PROXY_DOMAIN));    awsConf.setProxyWorkstation(conf.getTrimmed(PROXY_WORKSTATION));    if (LOG.isDebugEnabled()) {      LOG.debug("Using proxy server {}:{} as user {} with password {} on " + "domain {} as workstation {}",awsConf.getProxyHost(),awsConf.getProxyPort(),String.valueOf(awsConf.getProxyUsername()),awsConf.getProxyPassword(),awsConf.getProxyDomain(),awsConf.getProxyWorkstation());
static void patchSecurityCredentialProviders(Configuration conf){  Collection<String> customCredentials=conf.getStringCollection(S3A_SECURITY_CREDENTIAL_PROVIDER_PATH);  Collection<String> hadoopCredentials=conf.getStringCollection(CREDENTIAL_PROVIDER_PATH);  if (!customCredentials.isEmpty()) {    List<String> all=Lists.newArrayList(customCredentials);    all.addAll(hadoopCredentials);    String joined=StringUtils.join(all,',');
public static void clearBucketOption(Configuration conf,String bucket,String genericKey){  final String baseKey=genericKey.startsWith(FS_S3A_PREFIX) ? genericKey.substring(FS_S3A_PREFIX.length()) : genericKey;  String k=FS_S3A_BUCKET_PREFIX + bucket + '.'+ baseKey;
@Override protected Map<Class<? extends Exception>,RetryPolicy> createExceptionMap(){  Map<Class<? extends Exception>,RetryPolicy> b=super.createExceptionMap();  Configuration conf=getConfiguration();  int limit=conf.getInt(S3GUARD_CONSISTENCY_RETRY_LIMIT,S3GUARD_CONSISTENCY_RETRY_LIMIT_DEFAULT);  long interval=conf.getTimeDuration(S3GUARD_CONSISTENCY_RETRY_INTERVAL,S3GUARD_CONSISTENCY_RETRY_INTERVAL_DEFAULT,TimeUnit.MILLISECONDS);  RetryPolicy retryPolicy=retryUpToMaximumCountWithProportionalSleep(limit,interval,TimeUnit.MILLISECONDS);
public void logAtDebug(Logger log){  Collection<String> prefixes=getCommonPrefixes();  Collection<S3ObjectSummary> summaries=getObjectSummaries();
public void logAtDebug(Logger log){  Collection<String> prefixes=getCommonPrefixes();  Collection<S3ObjectSummary> summaries=getObjectSummaries();  log.debug("Prefix count = {}; object count={}",prefixes.size(),summaries.size());  for (  S3ObjectSummary summary : summaries) {
public void writeFailed(Exception ex){
@Retries.RetryTranslated public String initiateMultiPartUpload(String destKey) throws IOException {
@Retries.RetryTranslated public CompleteMultipartUploadResult completeMPUwithRetries(String destKey,String uploadId,List<PartETag> partETags,long length,AtomicInteger errorCount) throws IOException {  checkNotNull(uploadId);  checkNotNull(partETags);
@Retries.RetryTranslated public int abortMultipartUploadsUnderPath(String prefix) throws IOException {
@Retries.RetryTranslated public int abortMultipartUploadsUnderPath(String prefix) throws IOException {  LOG.debug("Aborting multipart uploads under {}",prefix);  int count=0;  List<MultipartUpload> multipartUploads=owner.listMultipartUploads(prefix);
public UploadPartRequest newUploadPartRequest(String destKey,String uploadId,int partNumber,int size,InputStream uploadStream,File sourceFile,Long offset) throws PathIOException {  checkNotNull(uploadId);  checkArgument((uploadStream != null) ^ (sourceFile != null),"Data source");  checkArgument(size >= 0,"Invalid partition size %s",size);  checkArgument(partNumber > 0,"partNumber must be between 1 and %s inclusive, but is %s",DEFAULT_UPLOAD_PART_COUNT_LIMIT,partNumber);
@Retries.RetryTranslated public CompleteMultipartUploadResult commitUpload(String destKey,String uploadId,List<PartETag> partETags,long length,@Nullable BulkOperationState operationState) throws IOException {  checkNotNull(uploadId);  checkNotNull(partETags);
@Retries.RetryTranslated public SelectObjectContentResult select(final Path source,final SelectObjectContentRequest request,final String action) throws IOException {  String bucketName=request.getBucketName();  Preconditions.checkArgument(bucket.equals(bucketName),"wrong bucket: %s",bucketName);  if (LOG.isDebugEnabled()) {
@Retries.RetryTranslated public SelectObjectContentResult select(final Path source,final SelectObjectContentRequest request,final String action) throws IOException {  String bucketName=request.getBucketName();  Preconditions.checkArgument(bucket.equals(bucketName),"wrong bucket: %s",bucketName);  if (LOG.isDebugEnabled()) {    LOG.debug("Initiating select call {} {}",source,request.getExpression());
public void operationRetried(String text,Exception ex,int retries,boolean idempotent){  if (retries == 0) {
public static AWSSecurityTokenServiceClientBuilder builder(final AWSCredentialsProvider credentials,final ClientConfiguration awsConf,final String stsEndpoint,final String stsRegion){  final AWSSecurityTokenServiceClientBuilder builder=AWSSecurityTokenServiceClientBuilder.standard();  Preconditions.checkArgument(credentials != null,"No credentials");  builder.withClientConfiguration(awsConf);  builder.withCredentials(credentials);  boolean destIsStandardEndpoint=STS_STANDARD.equals(stsEndpoint);  if (isNotEmpty(stsEndpoint) && !destIsStandardEndpoint) {    Preconditions.checkArgument(isNotEmpty(stsRegion),"STS endpoint is set to %s but no signing region was provided",stsEndpoint);
  for (  String customSigner : customSigners) {    String[] parts=customSigner.split(":");    if (!(parts.length == 1 || parts.length == 2 || parts.length == 3)) {      String message="Invalid format (Expected name, name:SignerClass," + " name:SignerClass:SignerInitializerClass)" + " for CustomSigner: [" + customSigner + "]";      LOG.error(message);      throw new IllegalArgumentException(message);    }    if (parts.length == 1) {    } else {      maybeRegisterSigner(parts[0],parts[1],ownerConf);      if (parts.length == 3) {        Class<? extends AwsSignerInitializer> clazz=null;        try {          clazz=(Class<? extends AwsSignerInitializer>)ownerConf.getClassByName(parts[2]);        } catch (        ClassNotFoundException e) {          throw new RuntimeException(String.format("SignerInitializer class" + " [%s] not found for signer [%s]",parts[2],parts[0]),e);
public Token<AbstractS3ATokenIdentifier> createDelegationToken(final Optional<RoleModel.Policy> policy,final EncryptionSecrets encryptionSecrets,final Text renewer) throws IOException {  requireServiceStarted();  final AbstractS3ATokenIdentifier tokenIdentifier=createTokenIdentifier(policy,encryptionSecrets,renewer);  if (tokenIdentifier != null) {    Token<AbstractS3ATokenIdentifier> token=new Token<>(tokenIdentifier,secretManager);    token.setKind(getKind());
@Override protected void serviceInit(final Configuration conf) throws Exception {  super.serviceInit(conf);  checkState(hasDelegationTokenBinding(conf),E_DELEGATION_TOKENS_DISABLED);  Class<? extends AbstractDelegationTokenBinding> binding=conf.getClass(DelegationConstants.DELEGATION_TOKEN_BINDING,SessionTokenBinding.class,AbstractDelegationTokenBinding.class);  tokenBinding=binding.newInstance();  tokenBinding.bindToFileSystem(getCanonicalUri(),getStoreContext(),getPolicyProvider());  tokenBinding.init(conf);  tokenBindingName=tokenBinding.getKind().toString();
@Override protected void serviceStart() throws Exception {  super.serviceStart();  tokenBinding.start();  bindToAnyDelegationToken();
@VisibleForTesting public void bindToDelegationToken(final Token<AbstractS3ATokenIdentifier> token) throws IOException {  checkState(!credentialProviders.isPresent(),E_ALREADY_DEPLOYED);  boundDT=Optional.of(token);  AbstractS3ATokenIdentifier dti=extractIdentifier(token);
private void noteTokenCreated(final Token<AbstractS3ATokenIdentifier> token){
@VisibleForTesting public static Token<AbstractS3ATokenIdentifier> lookupToken(final Credentials credentials,final Text service,final Text kind) throws DelegationTokenIOException {
@VisibleForTesting public static Token<AbstractS3ATokenIdentifier> lookupToken(final Credentials credentials,final Text service,final Text kind) throws DelegationTokenIOException {  LOG.debug("Looking for token for service {} in credentials",service);  Token<?> token=credentials.getToken(service);  if (token != null) {    Text tokenKind=token.getKind();
protected void setWorkPath(Path workPath){
@Override public void abortJob(JobContext context,JobStatus.State state) throws IOException {
protected void maybeIgnore(boolean suppress,String action,IOException ex) throws IOException {  if (suppress) {
private synchronized ExecutorService buildThreadPool(JobContext context,int numThreads){  Preconditions.checkArgument(numThreads > 0,"Cannot create a thread pool with no threads");  if (threadPool == null) {
private MaybeIOE commit(final SinglePendingCommit commit,final String origin,final BulkOperationState operationState){
private MaybeIOE commit(final SinglePendingCommit commit,final String origin,final BulkOperationState operationState){  LOG.debug("Committing single commit {}",commit);  MaybeIOE outcome;  String destKey="unknown destination";  try {    commit.validate();    destKey=commit.getDestinationKey();    long l=innerCommit(commit,operationState);
private void abortSingleCommit(SinglePendingCommit commit) throws IOException {  String destKey=commit.getDestinationKey();  String origin=commit.getFilename() != null ? (" defined in " + commit.getFilename()) : "";  String uploadId=commit.getUploadId();
public MaybeIOE abortAllSinglePendingCommits(Path pendingDir,boolean recursive) throws IOException {  Preconditions.checkArgument(pendingDir != null,"null pendingDir");
  RemoteIterator<LocatedFileStatus> pendingFiles;  try {    pendingFiles=ls(pendingDir,recursive);  } catch (  FileNotFoundException fnfe) {    LOG.info("No directory to abort {}",pendingDir);    return MaybeIOE.NONE;  }  MaybeIOE outcome=MaybeIOE.NONE;  if (!pendingFiles.hasNext()) {    LOG.debug("No files to abort under {}",pendingDir);  }  while (pendingFiles.hasNext()) {    Path pendingFile=pendingFiles.next().getPath();    if (pendingFile.getName().endsWith(CommitConstants.PENDING_SUFFIX)) {      try {        abortSingleCommit(SinglePendingCommit.load(fs,pendingFile));      } catch (      FileNotFoundException e) {
public void revertCommit(SinglePendingCommit commit,BulkOperationState operationState) throws IOException {
public SinglePendingCommit uploadFileToPendingCommit(File localFile,Path destPath,String partition,long uploadPartSize,Progressable progress) throws IOException {
    long length=localFile.length();    SinglePendingCommit commitData=new SinglePendingCommit();    commitData.setDestinationKey(destKey);    commitData.setBucket(fs.getBucket());    commitData.touch(System.currentTimeMillis());    commitData.setUploadId(uploadId);    commitData.setUri(destURI);    commitData.setText(partition != null ? "partition: " + partition : "");    commitData.setLength(length);    long offset=0;    long numParts=(length / uploadPartSize + ((length % uploadPartSize) > 0 ? 1 : 0));    if (numParts == 0) {      numParts=1;    }    if (numParts > InternalConstants.DEFAULT_UPLOAD_PART_COUNT_LIMIT) {      throw new PathIOException(destPath.toString(),String.format("File to upload (size %d)" + " is too big to be uploaded in parts of size %d",numParts,length));
    for (int partNumber=1; partNumber <= numParts; partNumber+=1) {      long size=Math.min(length - offset,uploadPartSize);      UploadPartRequest part;      part=writeOperations.newUploadPartRequest(destKey,uploadId,partNumber,(int)size,null,localFile,offset);      part.setLastPart(partNumber == numParts);      UploadPartResult partResult=writeOperations.uploadPart(part);      offset+=uploadPartSize;      parts.add(partResult.getPartETag());    }    commitData.bindCommitData(parts);    statistics.commitUploaded(length);    progress.progress();    threw=false;    return commitData;  }  finally {    if (threw && uploadId != null) {
public static void verifyIsMagicCommitFS(S3AFileSystem fs) throws PathCommitException {  if (!fs.isMagicCommitEnabled()) {    String fsUri=fs.getUri().toString();
public PutTracker createTracker(Path path,String key){  final List<String> elements=splitPathToElements(path);  PutTracker tracker;  if (isMagicFile(elements)) {    if (isMagicCommitPath(elements)) {      final String destKey=keyOfFinalDestination(elements,key);      String pendingsetPath=key + CommitConstants.PENDING_SUFFIX;      owner.getInstrumentation().incrementCounter(Statistic.COMMITTER_MAGIC_FILES_CREATED,1);      tracker=new MagicCommitTracker(path,owner.getBucket(),key,destKey,pendingsetPath,owner.getWriteOperationHelper());
@Override public PathOutputCommitter createTaskCommitter(S3AFileSystem fileSystem,Path outputPath,TaskAttemptContext context) throws IOException {  AbstractS3ACommitterFactory factory=chooseCommitterFactory(fileSystem,outputPath,context.getConfiguration());  if (factory != null) {    PathOutputCommitter committer=factory.createTaskCommitter(fileSystem,outputPath,context);
private AbstractS3ACommitterFactory chooseCommitterFactory(S3AFileSystem fileSystem,Path outputPath,Configuration taskConf) throws PathCommitException {  AbstractS3ACommitterFactory factory;  Configuration fsConf=fileSystem.getConf();  String name=fsConf.getTrimmed(FS_S3A_COMMITTER_NAME,COMMITTER_NAME_FILE);  name=taskConf.getTrimmed(FS_S3A_COMMITTER_NAME,name);
private static void waitFor(Collection<Future<?>> futures){  int size=futures.size();
private static void waitFor(Collection<Future<?>> futures){  int size=futures.size();  LOG.debug("Waiting for {} tasks to complete",size);  int oldNumFinished=0;  while (true) {    int numFinished=(int)futures.stream().filter(Future::isDone).count();    if (oldNumFinished != numFinished) {
public static PendingSet load(FileSystem fs,Path path) throws IOException {
public static SuccessData load(FileSystem fs,Path path) throws IOException {
@Override public boolean aboutToComplete(String uploadId,List<PartETag> parts,long bytesWritten) throws IOException {  Preconditions.checkArgument(StringUtils.isNotEmpty(uploadId),"empty/null upload ID: " + uploadId);  Preconditions.checkArgument(parts != null,"No uploaded parts list");  Preconditions.checkArgument(!parts.isEmpty(),"No uploaded parts to save");  SinglePendingCommit commitData=new SinglePendingCommit();  commitData.touch(System.currentTimeMillis());  commitData.setDestinationKey(getDestKey());  commitData.setBucket(bucket);  commitData.setUri(path.toUri().toString());  commitData.setUploadId(uploadId);  commitData.setText("");  commitData.setLength(bytesWritten);  commitData.bindCommitData(parts);  byte[] bytes=commitData.toBytes();
@Override public boolean aboutToComplete(String uploadId,List<PartETag> parts,long bytesWritten) throws IOException {  Preconditions.checkArgument(StringUtils.isNotEmpty(uploadId),"empty/null upload ID: " + uploadId);  Preconditions.checkArgument(parts != null,"No uploaded parts list");  Preconditions.checkArgument(!parts.isEmpty(),"No uploaded parts to save");  SinglePendingCommit commitData=new SinglePendingCommit();  commitData.touch(System.currentTimeMillis());  commitData.setDestinationKey(getDestKey());  commitData.setBucket(bucket);  commitData.setUri(path.toUri().toString());  commitData.setUploadId(uploadId);  commitData.setText("");  commitData.setLength(bytesWritten);  commitData.bindCommitData(parts);  byte[] bytes=commitData.toBytes();  LOG.info("Uncommitted data pending to file {};" + " commit metadata for {} parts in {}. sixe: {} byte(s)",path.toUri(),parts.size(),pendingPartKey,bytesWritten);
@Override public void commitTask(TaskAttemptContext context) throws IOException {  try (DurationInfo d=new DurationInfo(LOG,"Commit task %s",context.getTaskAttemptID())){    PendingSet commits=innerCommitTask(context);
  Pair<PendingSet,List<Pair<LocatedFileStatus,IOException>>> loaded=actions.loadSinglePendingCommits(taskAttemptPath,true);  PendingSet pendingSet=loaded.getKey();  List<Pair<LocatedFileStatus,IOException>> failures=loaded.getValue();  if (!failures.isEmpty()) {    LOG.error("At least one commit file could not be read: failing");    abortPendingUploads(context,pendingSet.getCommits(),true);    throw failures.get(0).getValue();  }  String jobId=String.valueOf(context.getJobID());  String taskId=String.valueOf(context.getTaskAttemptID());  for (  SinglePendingCommit commit : pendingSet.getCommits()) {    commit.setJobId(jobId);    commit.setTaskId(taskId);  }  Path jobAttemptPath=getJobAttemptPath(context);  TaskAttemptID taskAttemptID=context.getTaskAttemptID();  Path taskOutcomePath=new Path(jobAttemptPath,taskAttemptID.getTaskID().toString() + CommitConstants.PENDINGSET_SUFFIX);
@Override public void setupJob(JobContext context) throws IOException {  Path outputPath=getOutputPath();  FileSystem fs=getDestFS();  ConflictResolution conflictResolution=getConflictResolutionMode(context,fs.getConf());
@Override public void preCommitJob(final JobContext context,final ActiveCommit pending) throws IOException {  super.preCommitJob(context,pending);  Path outputPath=getOutputPath();  FileSystem fs=getDestFS();  Configuration fsConf=fs.getConf();switch (getConflictResolutionMode(context,fsConf)) {case FAIL:    break;case APPEND:  break;case REPLACE:if (fs.delete(outputPath,true)) {
  Map<Path,String> partitions=new ConcurrentHashMap<>();  FileSystem sourceFS=pending.getSourceFS();  Tasks.Submitter submitter=buildSubmitter(context);  try (DurationInfo ignored=new DurationInfo(LOG,"Replacing partitions")){    Tasks.foreach(pending.getSourceFiles()).stopOnFailure().suppressExceptions(false).executeWith(submitter).run(path -> {      PendingSet pendingSet=PendingSet.load(sourceFS,path);      Path lastParent=null;      for (      SinglePendingCommit commit : pendingSet.getCommits()) {        Path parent=commit.destinationPath().getParent();        if (parent != null && !parent.equals(lastParent)) {          partitions.put(parent,"");          lastParent=parent;        }      }    });  }   FileSystem fs=getDestFS();  Tasks.foreach(partitions.keySet()).stopOnFailure().suppressExceptions(false).executeWith(submitter).run(partitionPath -> {
protected List<LocatedFileStatus> getTaskOutput(TaskAttemptContext context) throws IOException {  Path attemptPath=getTaskAttemptPath(context);  Preconditions.checkNotNull(attemptPath,"No attemptPath path in {}",this);
@Override public void setupJob(JobContext context) throws IOException {
@Override public void cleanupStagingDirs(){  Path workPath=getWorkPath();  if (workPath != null) {
@Override public boolean needsTaskCommit(TaskAttemptContext context) throws IOException {  try (DurationInfo d=new DurationInfo(LOG,"%s: needsTaskCommit() Task %s",getRole(),context.getTaskAttemptID())){    Path attemptPath=getTaskAttemptPath(context);    FileSystem fs=getTaskAttemptFilesystem(context);    FileStatus[] stats=fs.listStatus(attemptPath);
@Override public void commitTask(TaskAttemptContext context) throws IOException {  try (DurationInfo d=new DurationInfo(LOG,"%s: commit task %s",getRole(),context.getTaskAttemptID())){    int count=commitTaskInternal(context,getTaskOutput(context));
protected int commitTaskInternal(final TaskAttemptContext context,List<? extends FileStatus> taskOutput) throws IOException {  LOG.debug("{}: commitTaskInternal",getRole());  Configuration conf=context.getConfiguration();  final Path attemptPath=getTaskAttemptPath(context);  FileSystem attemptFS=getTaskAttemptFilesystem(context);
protected int commitTaskInternal(final TaskAttemptContext context,List<? extends FileStatus> taskOutput) throws IOException {  LOG.debug("{}: commitTaskInternal",getRole());  Configuration conf=context.getConfiguration();  final Path attemptPath=getTaskAttemptPath(context);  FileSystem attemptFS=getTaskAttemptFilesystem(context);  LOG.debug("{}: attempt path is {}",getRole(),attemptPath);  Path commitsAttemptPath=wrappedCommitter.getTaskAttemptPath(context);  FileSystem commitsFS=commitsAttemptPath.getFileSystem(conf);  int commitCount=taskOutput.size();  final Queue<SinglePendingCommit> commits=new ConcurrentLinkedQueue<>();
protected int commitTaskInternal(final TaskAttemptContext context,List<? extends FileStatus> taskOutput) throws IOException {  LOG.debug("{}: commitTaskInternal",getRole());  Configuration conf=context.getConfiguration();  final Path attemptPath=getTaskAttemptPath(context);  FileSystem attemptFS=getTaskAttemptFilesystem(context);  LOG.debug("{}: attempt path is {}",getRole(),attemptPath);  Path commitsAttemptPath=wrappedCommitter.getTaskAttemptPath(context);  FileSystem commitsFS=commitsAttemptPath.getFileSystem(conf);  int commitCount=taskOutput.size();  final Queue<SinglePendingCommit> commits=new ConcurrentLinkedQueue<>();  LOG.info("{}: uploading from staging directory to S3 {}",getRole(),attemptPath);
  if (taskOutput.isEmpty()) {    LOG.warn("{}: No files to commit",getRole());  } else {    boolean threw=true;    context.progress();    PendingSet pendingCommits=new PendingSet(commitCount);    try {      Tasks.foreach(taskOutput).stopOnFailure().suppressExceptions(false).executeWith(buildSubmitter(context)).run(stat -> {        Path path=stat.getPath();        File localFile=new File(path.toUri().getPath());        String relative=Paths.getRelativePath(attemptPath,path);        String partition=Paths.getPartition(relative);        String key=getFinalKey(relative,context);        Path destPath=getDestS3AFS().keyToQualifiedPath(key);        SinglePendingCommit commit=getCommitOperations().uploadFileToPendingCommit(localFile,destPath,partition,uploadPartSize,context);
    PendingSet pendingCommits=new PendingSet(commitCount);    try {      Tasks.foreach(taskOutput).stopOnFailure().suppressExceptions(false).executeWith(buildSubmitter(context)).run(stat -> {        Path path=stat.getPath();        File localFile=new File(path.toUri().getPath());        String relative=Paths.getRelativePath(attemptPath,path);        String partition=Paths.getPartition(relative);        String key=getFinalKey(relative,context);        Path destPath=getDestS3AFS().keyToQualifiedPath(key);        SinglePendingCommit commit=getCommitOperations().uploadFileToPendingCommit(localFile,destPath,partition,uploadPartSize,context);        LOG.debug("{}: adding pending commit {}",getRole(),commit);        commits.add(commit);      });      for (      SinglePendingCommit commit : commits) {        pendingCommits.add(commit);
        File localFile=new File(path.toUri().getPath());        String relative=Paths.getRelativePath(attemptPath,path);        String partition=Paths.getPartition(relative);        String key=getFinalKey(relative,context);        Path destPath=getDestS3AFS().keyToQualifiedPath(key);        SinglePendingCommit commit=getCommitOperations().uploadFileToPendingCommit(localFile,destPath,partition,uploadPartSize,context);        LOG.debug("{}: adding pending commit {}",getRole(),commit);        commits.add(commit);      });      for (      SinglePendingCommit commit : commits) {        pendingCommits.add(commit);      }      LOG.debug("Saving {} pending commit(s)) to file {}",pendingCommits.size(),commitsAttemptPath);      pendingCommits.save(commitsFS,commitsAttemptPath,false);      threw=false;    }  finally {
protected PathExistsException failDestinationExists(final Path path,final String description){
protected PathExistsException failDestinationExists(final Path path,final String description){  LOG.error("{}: Failing commit by job {} to write" + " to existing output path {}.",description,getJobContext().getJobID(),path);  try {    int limit=10;    RemoteIterator<LocatedFileStatus> lf=getDestFS().listFiles(path,true);    LOG.info("Partial Directory listing");    while (limit > 0 && lf.hasNext()) {      limit--;      LocatedFileStatus status=lf.next();
public void bulkDeleteRetried(DeleteObjectsRequest deleteRequest,Exception ex){
private void onDeleteThrottled(final DeleteObjectsRequest deleteRequest){  final List<DeleteObjectsRequest.KeyVersion> keys=deleteRequest.getKeys();  final int size=keys.size();  incrementStatistic(STORE_IO_THROTTLED,size);  instrumentation.addValueToQuantiles(STORE_IO_THROTTLE_RATE,size);
public void processResponse(final CopyResult copyResult) throws PathIOException {  String newRevisionId=policy.getRevisionId(copyResult);
@Retries.RetryTranslated public Boolean execute() throws IOException {  executeOnlyOnce();  StoreContext context=getStoreContext();  Path path=status.getPath();
@Retries.RetryTranslated public Boolean execute() throws IOException {  executeOnlyOnce();  StoreContext context=getStoreContext();  Path path=status.getPath();  LOG.debug("Delete path {} - recursive {}",path,recursive);  LOG.debug("Type = {}",status.isFile() ? "File" : (status.isEmptyDirectory() == Tristate.TRUE ? "Empty Directory" : "Directory"));  String key=context.pathToKey(path);  if (status.isDirectory()) {
  Path path=status.getPath();  LOG.debug("Delete path {} - recursive {}",path,recursive);  LOG.debug("Type = {}",status.isFile() ? "File" : (status.isEmptyDirectory() == Tristate.TRUE ? "Empty Directory" : "Directory"));  String key=context.pathToKey(path);  if (status.isDirectory()) {    LOG.debug("delete: Path is a directory: {}",path);    checkArgument(status.isEmptyDirectory() != Tristate.UNKNOWN,"File status must have directory emptiness computed");    if (!key.endsWith("/")) {      key=key + "/";    }    if ("/".equals(key)) {      LOG.error("S3A: Cannot delete the root directory." + " Path: {}. Recursive: {}",status.getPath(),recursive);      return false;    }    if (!recursive && status.isEmptyDirectory() == Tristate.FALSE) {      throw new PathIsNotEmptyDirectoryException(path.toString());    }    if (status.isEmptyDirectory() == Tristate.TRUE) {
    LOG.debug("delete: Path is a directory: {}",path);    checkArgument(status.isEmptyDirectory() != Tristate.UNKNOWN,"File status must have directory emptiness computed");    if (!key.endsWith("/")) {      key=key + "/";    }    if ("/".equals(key)) {      LOG.error("S3A: Cannot delete the root directory." + " Path: {}. Recursive: {}",status.getPath(),recursive);      return false;    }    if (!recursive && status.isEmptyDirectory() == Tristate.FALSE) {      throw new PathIsNotEmptyDirectoryException(path.toString());    }    if (status.isEmptyDirectory() == Tristate.TRUE) {      LOG.debug("deleting empty directory {}",path);      deleteObjectAtPath(path,key,false);    } else {      deleteDirectoryTree(path,key);    }  } else {
@Retries.RetryTranslated protected void deleteDirectoryTree(final Path path,final String dirKey) throws IOException {  operationState=S3Guard.initiateBulkWrite(metadataStore,BulkOperationState.OperationType.Delete,path);  try (DurationInfo ignored=new DurationInfo(LOG,false,"deleting %s",dirKey)){    resetDeleteList();    deleteFuture=null;
    LOG.debug("Getting objects for directory prefix {} to delete",dirKey);    final RemoteIterator<S3ALocatedFileStatus> locatedFiles=callbacks.listFilesAndEmptyDirectories(path,status,false,true);    while (locatedFiles.hasNext()) {      S3AFileStatus child=locatedFiles.next().toS3AFileStatus();      queueForDeletion(child);    }    LOG.debug("Deleting final batch of listed files");    submitNextBatch();    maybeAwaitCompletion(deleteFuture);    if (callbacks.allowAuthoritative(path)) {      LOG.debug("Path is authoritatively guarded;" + " listing files on S3 for completeness");      final RemoteIterator<S3AFileStatus> objects=callbacks.listObjects(path,dirKey);      while (objects.hasNext()) {        extraFilesDeleted++;        queueForDeletion(deletionKey(objects.next()),null);      }      if (extraFilesDeleted > 0) {
    maybeAwaitCompletion(deleteFuture);    if (callbacks.allowAuthoritative(path)) {      LOG.debug("Path is authoritatively guarded;" + " listing files on S3 for completeness");      final RemoteIterator<S3AFileStatus> objects=callbacks.listObjects(path,dirKey);      while (objects.hasNext()) {        extraFilesDeleted++;        queueForDeletion(deletionKey(objects.next()),null);      }      if (extraFilesDeleted > 0) {        LOG.debug("Raw S3 Scan found {} extra file(s) to delete",extraFilesDeleted);        submitNextBatch();        maybeAwaitCompletion(deleteFuture);      }    }    try (DurationInfo ignored2=new DurationInfo(LOG,false,"Delete metastore")){      metadataStore.deleteSubtree(path,operationState);    }   }  finally {    IOUtils.cleanupWithLogger(LOG,operationState);
private void queueForDeletion(final String key,@Nullable final Path deletePath) throws IOException {
@Retries.RetryTranslated private void deleteObjectAtPath(final Path path,final String key,final boolean isFile) throws IOException {
@Retries.RetryTranslated private void asyncDeleteAction(final BulkOperationState state,final List<DeleteObjectsRequest.KeyVersion> keyList,final List<Path> pathList,final boolean auditDeletedKeys) throws IOException {  try (DurationInfo ignored=new DurationInfo(LOG,false,"Delete page of keys")){    DeleteObjectsResult result=null;    List<Path> undeletedObjects=new ArrayList<>();    if (!keyList.isEmpty()) {      result=Invoker.once("Remove S3 Keys",status.getPath().toString(),() -> callbacks.removeKeys(keyList,false,undeletedObjects,state,!auditDeletedKeys));    }    if (!pathList.isEmpty()) {      metadataStore.deletePaths(pathList,state);    }    if (auditDeletedKeys && result != null) {      List<DeleteObjectsResult.DeletedObject> deletedObjects=result.getDeletedObjects();      if (deletedObjects.size() != keyList.size()) {        LOG.warn("Size mismatch in deletion operation. " + "Expected count of deleted files: {}; " + "actual: {}",keyList.size(),deletedObjects.size());        for (        DeleteObjectsResult.DeletedObject del : deletedObjects) {          keyList.removeIf(kv -> kv.getKey().equals(del.getKey()));        }        for (        DeleteObjectsRequest.KeyVersion kv : keyList) {
public List<Path> removeAllowedMarkers(DirectoryPolicy policy){  List<Path> removed=new ArrayList<>();  Iterator<Map.Entry<Path,Marker>> entries=surplusMarkers.entrySet().iterator();  while (entries.hasNext()) {    Map.Entry<Path,Marker> entry=entries.next();    Path path=entry.getKey();    if (policy.keepDirectoryMarkers(path)) {      entries.remove();
public Pair<List<Path>,List<Path>> splitUndeletedKeys(final MultiObjectDeleteException deleteException,final Collection<DeleteObjectsRequest.KeyVersion> keysToDelete){
@Retries.OnceTranslated private void completeActiveCopies(String reason) throws IOException {
private void queueToDelete(Path path,String key,String version){
private OperationDuration copyEmptyDirectoryMarkers(final String srcKey,final String dstKey,final DirMarkerTracker dirMarkerTracker) throws IOException {
private OperationDuration copyEmptyDirectoryMarkers(final String srcKey,final String dstKey,final DirMarkerTracker dirMarkerTracker) throws IOException {  LOG.debug("Copying markers from {}",dirMarkerTracker);  final StoreContext storeContext=getStoreContext();  Map<Path,DirMarkerTracker.Marker> leafMarkers=dirMarkerTracker.getLeafMarkers();  Map<Path,DirMarkerTracker.Marker> surplus=dirMarkerTracker.getSurplusMarkers();  DurationInfo duration=new DurationInfo(LOG,false,"copying %d leaf markers with %d surplus not copied",leafMarkers.size(),surplus.size());  for (  DirMarkerTracker.Marker entry : leafMarkers.values()) {    Path source=entry.getPath();    String key=entry.getKey();    String newDestKey=dstKey + key.substring(srcKey.length());    Path childDestPath=storeContext.keyToPath(newDestKey);
@Retries.RetryTranslated private void removeSourceObjects(final List<DeleteObjectsRequest.KeyVersion> keys,final List<Path> paths) throws IOException {  List<Path> undeletedObjects=new ArrayList<>();  try {    if (LOG.isDebugEnabled()) {
@Retries.RetryTranslated private void removeSourceObjects(final List<DeleteObjectsRequest.KeyVersion> keys,final List<Path> paths) throws IOException {  List<Path> undeletedObjects=new ArrayList<>();  try {    if (LOG.isDebugEnabled()) {      LOG.debug("Initiating delete operation for {} objects",keys.size());      for (      DeleteObjectsRequest.KeyVersion key : keys) {
  int nonauth=0;  final Queue<DDBPathMetadata> queue=new ArrayDeque<>();  final boolean isRoot=path.isRoot();  final DDBPathMetadata baseData=metastore.get(path);  if (baseData == null) {    throw new ExitUtil.ExitException(LauncherExitCodes.EXIT_NOT_FOUND,"No S3Guard entry for path " + path);  }  if (isRoot || isDirectory(baseData)) {    queue.add(baseData);  } else {    LOG.info("Path represents file");    return Pair.of(0,0);  }  while (!queue.isEmpty()) {    dirs++;    final DDBPathMetadata dir=queue.poll();    final Path p=dir.getFileStatus().getPath();
  if (isRoot || isDirectory(baseData)) {    queue.add(baseData);  } else {    LOG.info("Path represents file");    return Pair.of(0,0);  }  while (!queue.isEmpty()) {    dirs++;    final DDBPathMetadata dir=queue.poll();    final Path p=dir.getFileStatus().getPath();    LOG.debug("Directory {}",dir.prettyPrint());    if (!p.isRoot()) {      if (!dir.isAuthoritativeDir()) {        LOG.warn("Directory {} is not authoritative",p);        nonauth++;        verifyAuthDir(dir,requireAuth);
  } else {    LOG.info("Path represents file");    return Pair.of(0,0);  }  while (!queue.isEmpty()) {    dirs++;    final DDBPathMetadata dir=queue.poll();    final Path p=dir.getFileStatus().getPath();    LOG.debug("Directory {}",dir.prettyPrint());    if (!p.isRoot()) {      if (!dir.isAuthoritativeDir()) {        LOG.warn("Directory {} is not authoritative",p);        nonauth++;        verifyAuthDir(dir,requireAuth);      } else {        LOG.info("Directory {}",p);
        nonauth++;        verifyAuthDir(dir,requireAuth);      } else {        LOG.info("Directory {}",p);      }    } else {      LOG.info("Root directory {}",p);    }    if (recursive) {      final DirListingMetadata entry=metastore.listChildren(p);      if (entry != null) {        final Collection<PathMetadata> listing=entry.getListing();        int files=0, subdirs=0;        for (        PathMetadata e : listing) {          if (isDirectory(e)) {            queue.add((DDBPathMetadata)e);            subdirs++;
protected long dumpRawS3ObjectStore(final CsvFile csv) throws IOException {  S3AFileSystem fs=getFilesystem();  Path rootPath=fs.qualify(new Path("/"));  Listing listing=fs.getListing();  S3ListRequest request=listing.createListObjectsRequest("",null);  long count=0;  RemoteIterator<S3AFileStatus> st=listing.createFileStatusListingIterator(rootPath,request,ACCEPT_ALL,new Listing.AcceptAllButSelfAndS3nDirs(rootPath));  while (st.hasNext()) {    count++;    S3AFileStatus next=st.next();
private void dumpEntry(CsvFile csv,DDBPathMetadata md){
public static DumpS3GuardDynamoTable dumpStore(@Nullable final S3AFileSystem fs,@Nullable DynamoDBMetadataStore store,@Nullable Configuration conf,final File destFile,@Nullable URI uri) throws ExitUtil.ExitException {  ServiceLauncher<Service> serviceLauncher=new ServiceLauncher<>(NAME);  if (conf == null) {    conf=checkNotNull(fs,"No filesystem").getConf();  }  if (store == null) {    store=(DynamoDBMetadataStore)checkNotNull(fs,"No filesystem").getMetadataStore();  }  DumpS3GuardDynamoTable dump=new DumpS3GuardDynamoTable(fs,store,destFile,uri);  ExitUtil.ExitException ex=serviceLauncher.launchService(conf,dump,Collections.emptyList(),false,true);  if (ex != null && ex.getExitCode() != 0) {    throw ex;  }  LOG.info("Results:");  Pair<Long,Long> r=dump.getScanEntryResult();  LOG.info("Metastore entries: {}",r);  LOG.info("Metastore scan total {}, entries {}, tombstones {}",r.getLeft() + r.getRight(),r.getLeft(),r.getRight());  LOG.info("S3 count {}",dump.getRawObjectStoreCount());
  ServiceLauncher<Service> serviceLauncher=new ServiceLauncher<>(NAME);  if (conf == null) {    conf=checkNotNull(fs,"No filesystem").getConf();  }  if (store == null) {    store=(DynamoDBMetadataStore)checkNotNull(fs,"No filesystem").getMetadataStore();  }  DumpS3GuardDynamoTable dump=new DumpS3GuardDynamoTable(fs,store,destFile,uri);  ExitUtil.ExitException ex=serviceLauncher.launchService(conf,dump,Collections.emptyList(),false,true);  if (ex != null && ex.getExitCode() != 0) {    throw ex;  }  LOG.info("Results:");  Pair<Long,Long> r=dump.getScanEntryResult();  LOG.info("Metastore entries: {}",r);  LOG.info("Metastore scan total {}, entries {}, tombstones {}",r.getLeft() + r.getRight(),r.getLeft(),r.getRight());  LOG.info("S3 count {}",dump.getRawObjectStoreCount());  LOG.info("Treewalk Count {}",dump.getTreewalkCount());
  }  if (store == null) {    store=(DynamoDBMetadataStore)checkNotNull(fs,"No filesystem").getMetadataStore();  }  DumpS3GuardDynamoTable dump=new DumpS3GuardDynamoTable(fs,store,destFile,uri);  ExitUtil.ExitException ex=serviceLauncher.launchService(conf,dump,Collections.emptyList(),false,true);  if (ex != null && ex.getExitCode() != 0) {    throw ex;  }  LOG.info("Results:");  Pair<Long,Long> r=dump.getScanEntryResult();  LOG.info("Metastore entries: {}",r);  LOG.info("Metastore scan total {}, entries {}, tombstones {}",r.getLeft() + r.getRight(),r.getLeft(),r.getRight());  LOG.info("S3 count {}",dump.getRawObjectStoreCount());  LOG.info("Treewalk Count {}",dump.getTreewalkCount());  LOG.info("List Status Count {}",dump.getListStatusCount());  r=dump.getSecondScanResult();  if (r != null) {
private DynamoDB createDynamoDB(final Configuration conf,final String s3Region,final String bucket,final AWSCredentialsProvider credentials) throws IOException {  if (amazonDynamoDB == null) {    Preconditions.checkNotNull(conf);    final Class<? extends DynamoDBClientFactory> cls=conf.getClass(S3GUARD_DDB_CLIENT_FACTORY_IMPL,S3GUARD_DDB_CLIENT_FACTORY_IMPL_DEFAULT,DynamoDBClientFactory.class);
@Override @Retries.OnceRaw public void initialize(FileSystem fs,ITtlTimeProvider ttlTp) throws IOException {  Preconditions.checkNotNull(fs,"Null filesystem");  Preconditions.checkArgument(fs instanceof S3AFileSystem,"DynamoDBMetadataStore only supports S3A filesystem - not %s",fs);  bindToOwnerFilesystem((S3AFileSystem)fs);  final String bucket=owner.getBucket();  String confRegion=conf.getTrimmed(S3GUARD_DDB_REGION_KEY);  if (!StringUtils.isEmpty(confRegion)) {    region=confRegion;
@Override @Retries.OnceRaw public void initialize(FileSystem fs,ITtlTimeProvider ttlTp) throws IOException {  Preconditions.checkNotNull(fs,"Null filesystem");  Preconditions.checkArgument(fs instanceof S3AFileSystem,"DynamoDBMetadataStore only supports S3A filesystem - not %s",fs);  bindToOwnerFilesystem((S3AFileSystem)fs);  final String bucket=owner.getBucket();  String confRegion=conf.getTrimmed(S3GUARD_DDB_REGION_KEY);  if (!StringUtils.isEmpty(confRegion)) {    region=confRegion;    LOG.debug("Overriding S3 region with configured DynamoDB region: {}",region);  } else {    try {      region=owner.getBucketLocation();    } catch (    AccessDeniedException e) {      URI uri=owner.getUri();      String message="Failed to get bucket location as client lacks permission " + RolePolicies.S3_GET_BUCKET_LOCATION + " for "+ uri;
@Override @Retries.RetryTranslated public void forgetMetadata(Path path) throws IOException {
@Retries.RetryTranslated private void innerDelete(final Path path,final boolean tombstone,final AncestorState ancestorState) throws IOException {  checkPath(path);
@Override @Retries.RetryTranslated public void deleteSubtree(Path path,final BulkOperationState operationState) throws IOException {  checkPath(path);
@Override @Retries.RetryTranslated public void deleteSubtree(Path path,final BulkOperationState operationState) throws IOException {  checkPath(path);  LOG.debug("Deleting subtree from table {} in region {}: {}",tableName,region,path);  final PathMetadata meta=get(path);  if (meta == null) {
@Override @Retries.RetryTranslated public DDBPathMetadata get(Path path,boolean wantEmptyDirectoryFlag) throws IOException {  checkPath(path);
@Override @Retries.RetryTranslated public DDBPathMetadata get(Path path,boolean wantEmptyDirectoryFlag) throws IOException {  checkPath(path);  LOG.debug("Get from table {} in region {}: {}. wantEmptyDirectory={}",tableName,region,path,wantEmptyDirectoryFlag);  DDBPathMetadata result=innerGet(path,wantEmptyDirectoryFlag);
  final DDBPathMetadata meta;  if (path.isRoot()) {    meta=new DDBPathMetadata(makeDirStatus(username,path));  } else {    final Item item=getConsistentItem(path);    meta=itemToPathMetadata(item,username);    LOG.debug("Get from table {} in region {} returning for {}: {}",tableName,region,path,meta);  }  if (wantEmptyDirectoryFlag && meta != null && !meta.isDeleted()) {    final FileStatus status=meta.getFileStatus();    if (status.isDirectory()) {      final QuerySpec spec=new QuerySpec().withHashKey(pathToParentKeyAttribute(path)).withConsistentRead(true).withFilterExpression(IS_DELETED + " = :false").withValueMap(DELETE_TRACKING_VALUE_MAP);      boolean hasChildren=readOp.retry("get/hasChildren",path.toString(),true,() -> {        final IteratorSupport<Item,QueryOutcome> it=table.query(spec).iterator();        if (it.hasNext()) {          if (LOG.isDebugEnabled()) {
    meta=new DDBPathMetadata(makeDirStatus(username,path));  } else {    final Item item=getConsistentItem(path);    meta=itemToPathMetadata(item,username);    LOG.debug("Get from table {} in region {} returning for {}: {}",tableName,region,path,meta);  }  if (wantEmptyDirectoryFlag && meta != null && !meta.isDeleted()) {    final FileStatus status=meta.getFileStatus();    if (status.isDirectory()) {      final QuerySpec spec=new QuerySpec().withHashKey(pathToParentKeyAttribute(path)).withConsistentRead(true).withFilterExpression(IS_DELETED + " = :false").withValueMap(DELETE_TRACKING_VALUE_MAP);      boolean hasChildren=readOp.retry("get/hasChildren",path.toString(),true,() -> {        final IteratorSupport<Item,QueryOutcome> it=table.query(spec).iterator();        if (it.hasNext()) {          if (LOG.isDebugEnabled()) {            LOG.debug("Dir {} is non-empty",status.getPath());            while (it.hasNext()) {
@Override @Retries.RetryTranslated public DirListingMetadata listChildren(final Path path) throws IOException {  checkPath(path);
private Collection<DDBPathMetadata> completeAncestry(final Collection<DDBPathMetadata> pathsToCreate,final AncestorState ancestorState) throws IOException {  Map<Path,Pair<EntryOrigin,DDBPathMetadata>> ancestry=new HashMap<>();
private Collection<DDBPathMetadata> completeAncestry(final Collection<DDBPathMetadata> pathsToCreate,final AncestorState ancestorState) throws IOException {  Map<Path,Pair<EntryOrigin,DDBPathMetadata>> ancestry=new HashMap<>();  LOG.debug("Completing ancestry for {} paths",pathsToCreate.size());  List<DDBPathMetadata> sortedPaths=new ArrayList<>(pathsToCreate);  sortedPaths.sort(PathOrderComparators.TOPMOST_PM_FIRST);  for (  DDBPathMetadata entry : sortedPaths) {    Preconditions.checkArgument(entry != null);    Path path=entry.getFileStatus().getPath();
    Path path=entry.getFileStatus().getPath();    LOG.debug("Adding entry {}",path);    if (path.isRoot()) {      break;    }    DDBPathMetadata oldEntry=ancestorState.put(path,entry);    boolean addAncestors=true;    if (oldEntry != null) {      boolean oldWasDir=oldEntry.getFileStatus().isDirectory();      boolean newIsDir=entry.getFileStatus().isDirectory();      if ((oldWasDir && !newIsDir) || (!oldWasDir && newIsDir)) {        LOG.warn("Overwriting a S3Guard file created in the operation: {}",oldEntry);        LOG.warn("With new entry: {}",entry);        ancestorState.put(path,oldEntry);        throw new PathIOException(path.toString(),String.format("%s old %s new %s",E_INCONSISTENT_UPDATE,oldEntry,entry));      } else {
        LOG.warn("Overwriting a S3Guard file created in the operation: {}",oldEntry);        LOG.warn("With new entry: {}",entry);        ancestorState.put(path,oldEntry);        throw new PathIOException(path.toString(),String.format("%s old %s new %s",E_INCONSISTENT_UPDATE,oldEntry,entry));      } else {        LOG.debug("Directory at {} being updated with value {}",path,entry);        addAncestors=false;      }    }    ancestry.put(path,Pair.of(EntryOrigin.Requested,entry));    Path parent=path.getParent();    while (addAncestors && !parent.isRoot() && !ancestry.containsKey(parent)) {      if (!ancestorState.findEntry(parent,true)) {        DDBPathMetadata md;        Pair<EntryOrigin,DDBPathMetadata> newEntry;        final Item item=getConsistentItem(parent);        if (item != null && !itemToPathMetadata(item,username).isDeleted()) {
      } else {        LOG.debug("Directory at {} being updated with value {}",path,entry);        addAncestors=false;      }    }    ancestry.put(path,Pair.of(EntryOrigin.Requested,entry));    Path parent=path.getParent();    while (addAncestors && !parent.isRoot() && !ancestry.containsKey(parent)) {      if (!ancestorState.findEntry(parent,true)) {        DDBPathMetadata md;        Pair<EntryOrigin,DDBPathMetadata> newEntry;        final Item item=getConsistentItem(parent);        if (item != null && !itemToPathMetadata(item,username).isDeleted()) {          md=itemToPathMetadata(item,username);          LOG.debug("Found existing entry for parent: {}",md);          newEntry=Pair.of(EntryOrigin.Retrieved,md);          addAncestors=false;
@Override @Retries.RetryTranslated public void put(final PathMetadata meta,@Nullable final BulkOperationState operationState) throws IOException {
@Override @Retries.RetryTranslated public void put(final DirListingMetadata meta,final List<Path> unchangedEntries,@Nullable final BulkOperationState operationState) throws IOException {
@Override @Retries.RetryTranslated public long prune(PruneMode pruneMode,long cutoff,String keyPrefix) throws IOException {
    long delay=conf.getTimeDuration(S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY,S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT,TimeUnit.MILLISECONDS);    Set<Path> parentPathSet=new HashSet<>();    Set<Path> clearedParentPathSet=new HashSet<>();    FunctionsRaisingIOE.CallableRaisingIOE<Void> deleteBatchOperation=() -> {      deletionBatch.sort(PathOrderComparators.TOPMOST_PATH_LAST);      processBatchWriteRequest(state,pathToKey(deletionBatch),null);      removeAuthoritativeDirFlag(parentPathSet,state);      clearedParentPathSet.addAll(parentPathSet);      parentPathSet.clear();      return null;    };    for (    Item item : items) {      DDBPathMetadata md=PathMetadataDynamoDBTranslation.itemToPathMetadata(item,username);      Path path=md.getFileStatus().getPath();      boolean tombstone=md.isDeleted();
      if (!tombstone && parentPath != null && !parentPath.isRoot() && !clearedParentPathSet.contains(parentPath)) {        parentPathSet.add(parentPath);      }      itemCount++;      if (deletionBatch.size() == S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT) {        deleteBatchOperation.apply();        deletionBatch.clear();        if (delay > 0) {          Thread.sleep(delay);        }      }    }    if (!deletionBatch.isEmpty()) {      deleteBatchOperation.apply();    }  } catch (  InterruptedException e) {    Thread.currentThread().interrupt();    throw new InterruptedIOException("Pruning was interrupted");  }catch (  AmazonDynamoDBException e) {    throw translateDynamoDBException(keyPrefix,"Prune of " + keyPrefix + " failed",e);
private void removeAuthoritativeDirFlag(final Set<Path> pathSet,final AncestorState state) throws IOException {  AtomicReference<IOException> rIOException=new AtomicReference<>();  Set<DDBPathMetadata> metas=pathSet.stream().map(path -> {    try {      if (path.isRoot()) {        LOG.debug("ignoring root path");        return null;      }      if (state != null && state.get(path) != null) {        LOG.debug("Ignoring update of entry already in the state map");        return null;      }      DDBPathMetadata ddbPathMetadata=get(path);      if (ddbPathMetadata == null) {        LOG.debug("No parent {}; skipping",path);        return null;      }      if (ddbPathMetadata.isDeleted()) {
    try {      if (path.isRoot()) {        LOG.debug("ignoring root path");        return null;      }      if (state != null && state.get(path) != null) {        LOG.debug("Ignoring update of entry already in the state map");        return null;      }      DDBPathMetadata ddbPathMetadata=get(path);      if (ddbPathMetadata == null) {        LOG.debug("No parent {}; skipping",path);        return null;      }      if (ddbPathMetadata.isDeleted()) {        LOG.debug("Parent has been deleted {}; skipping",path);        return null;      }      if (!ddbPathMetadata.getFileStatus().isDirectory()) {
      }      DDBPathMetadata ddbPathMetadata=get(path);      if (ddbPathMetadata == null) {        LOG.debug("No parent {}; skipping",path);        return null;      }      if (ddbPathMetadata.isDeleted()) {        LOG.debug("Parent has been deleted {}; skipping",path);        return null;      }      if (!ddbPathMetadata.getFileStatus().isDirectory()) {        LOG.debug("Parent is not a directory {}; skipping",path);        return null;      }      LOG.debug("Setting isAuthoritativeDir==false on {}",ddbPathMetadata);      ddbPathMetadata.setAuthoritativeDir(false);      ddbPathMetadata.setLastUpdated(ttlTimeProvider.getNow());      return ddbPathMetadata;    } catch (    IOException e) {
      if (!ddbPathMetadata.getFileStatus().isDirectory()) {        LOG.debug("Parent is not a directory {}; skipping",path);        return null;      }      LOG.debug("Setting isAuthoritativeDir==false on {}",ddbPathMetadata);      ddbPathMetadata.setAuthoritativeDir(false);      ddbPathMetadata.setLastUpdated(ttlTimeProvider.getNow());      return ddbPathMetadata;    } catch (    IOException e) {      String msg=String.format("IOException while getting PathMetadata " + "on path: %s.",path);      LOG.error(msg,e);      rIOException.set(e);      return null;    }  }).filter(Objects::nonNull).collect(Collectors.toSet());  try {    LOG.debug("innerPut on metas: {}",metas);
@Retries.OnceRaw private PutItemOutcome putItem(Item item){
@Override @Retries.OnceRaw public void updateParameters(Map<String,String> parameters) throws IOException {  Preconditions.checkNotNull(table,"Not initialized");  TableDescription desc=getTableDescription(true);  ProvisionedThroughputDescription current=desc.getProvisionedThroughput();  long currentRead=current.getReadCapacityUnits();  long newRead=getLongParam(parameters,S3GUARD_DDB_TABLE_CAPACITY_READ_KEY,currentRead);  long currentWrite=current.getWriteCapacityUnits();  long newWrite=getLongParam(parameters,S3GUARD_DDB_TABLE_CAPACITY_WRITE_KEY,currentWrite);  if (currentRead == 0 || currentWrite == 0) {    throw new IOException(E_ON_DEMAND_NO_SET_CAPACITY);  }  if (newRead != currentRead || newWrite != currentWrite) {    LOG.info("Current table capacity is read: {}, write: {}",currentRead,currentWrite);    LOG.info("Changing capacity of table to read: {}, write: {}",newRead,newWrite);    tableHandler.provisionTableBlocking(newRead,newWrite);  } else {
void retryEvent(String text,IOException ex,int attempts,boolean idempotent){  if (S3AUtils.isThrottleException(ex)) {    instrumentation.throttled();    int eventCount=throttleEventCount.addAndGet(1);    if (attempts == 1 && eventCount < THROTTLE_EVENT_LOG_LIMIT) {      LOG.warn("DynamoDB IO limits reached in {};" + " consider increasing capacity: {}",text,ex.toString());
  }  Preconditions.checkArgument(operationState instanceof AncestorState,"Not an AncestorState %s",operationState);  final AncestorState state=(AncestorState)operationState;  final String simpleDestKey=pathToParentKey(dest);  final String destPathKey=simpleDestKey + "/";  final String opId=AncestorState.stateAsString(state);  LOG.debug("{}: marking directories under {} as authoritative",opId,destPathKey);  final List<DDBPathMetadata> dirsToUpdate=new ArrayList<>();synchronized (state) {    for (    Map.Entry<Path,DDBPathMetadata> entry : state.getAncestry().entrySet()) {      final Path path=entry.getKey();      final DDBPathMetadata md=entry.getValue();      final String key=pathToParentKey(path);      if (md.getFileStatus().isDirectory() && (key.equals(simpleDestKey) || key.startsWith(destPathKey))) {        md.setAuthoritativeDir(true);        md.setLastUpdated(ttlTimeProvider.getNow());
private static void logPut(@Nullable AncestorState state,Item[] items){  if (OPERATIONS_LOG.isDebugEnabled()) {    String stateStr=AncestorState.stateAsString(state);    for (    Item item : items) {      boolean tombstone=!itemExists(item);      boolean isDir=getBoolAttribute(item,IS_DIR,false);      boolean auth=getBoolAttribute(item,IS_AUTHORITATIVE,false);
private static void logDelete(@Nullable AncestorState state,PrimaryKey[] keysDeleted){  if (OPERATIONS_LOG.isDebugEnabled()) {    String stateStr=AncestorState.stateAsString(state);    for (    PrimaryKey key : keysDeleted) {
@VisibleForTesting @Retries.RetryTranslated Table initTable() throws IOException {  table=dynamoDB.getTable(tableName);  try {    try {
@VisibleForTesting @Retries.RetryTranslated Table initTable() throws IOException {  table=dynamoDB.getTable(tableName);  try {    try {      LOG.debug("Binding to table {}",tableName);      TableDescription description=table.describe();
      LOG.debug("Table state: {}",description);      tableArn=description.getTableArn();      final String status=description.getTableStatus();switch (status) {case "CREATING":        LOG.debug("Table {} in region {} is being created/updated. This may" + " indicate that the table is being operated by another " + "concurrent thread or process. Waiting for active...",tableName,region);      waitForTableActive(table);    break;case "DELETING":  throw new FileNotFoundException("DynamoDB table " + "'" + tableName + "' is being "+ "deleted in region "+ region);case "UPDATING":LOG.debug("Table is being updated.");break;case "ACTIVE":break;default:throw new IOException("Unknown DynamoDB table status " + status + ": tableName='"+ tableName+ "', region="+ region);}verifyVersionCompatibility();final Item versionMarker=getVersionMarkerItem();Long created=extractCreationTimeFromMarker(versionMarker);
@VisibleForTesting protected void verifyVersionCompatibility() throws IOException {  final Item versionMarkerItem=getVersionMarkerItem();  Item versionMarkerFromTag=null;  boolean canReadDdbTags=true;  try {    versionMarkerFromTag=getVersionMarkerFromTags(table,amazonDynamoDB);  } catch (  AccessDeniedException e) {    LOG.debug("Can not read tags of table.");    canReadDdbTags=false;  }  LOG.debug("versionMarkerItem: {};  versionMarkerFromTag: {}",versionMarkerItem,versionMarkerFromTag);  if (versionMarkerItem == null && versionMarkerFromTag == null) {    if (!isEmptyTable(tableName,amazonDynamoDB)) {      LOG.error("Table is not empty but missing the version maker. Failing.");      throw new IOException(E_NO_VERSION_MARKER_AND_NOT_EMPTY + " Table: " + tableName);    }    if (canReadDdbTags) {
  try {    versionMarkerFromTag=getVersionMarkerFromTags(table,amazonDynamoDB);  } catch (  AccessDeniedException e) {    LOG.debug("Can not read tags of table.");    canReadDdbTags=false;  }  LOG.debug("versionMarkerItem: {};  versionMarkerFromTag: {}",versionMarkerItem,versionMarkerFromTag);  if (versionMarkerItem == null && versionMarkerFromTag == null) {    if (!isEmptyTable(tableName,amazonDynamoDB)) {      LOG.error("Table is not empty but missing the version maker. Failing.");      throw new IOException(E_NO_VERSION_MARKER_AND_NOT_EMPTY + " Table: " + tableName);    }    if (canReadDdbTags) {      LOG.info("Table {} contains no version marker item and tag. " + "The table is empty, so the version marker will be added " + "as TAG and ITEM.",tableName);      putVersionMarkerItemToTable();      tagTableWithVersionMarker();    }    if (!canReadDdbTags) {
    canReadDdbTags=false;  }  LOG.debug("versionMarkerItem: {};  versionMarkerFromTag: {}",versionMarkerItem,versionMarkerFromTag);  if (versionMarkerItem == null && versionMarkerFromTag == null) {    if (!isEmptyTable(tableName,amazonDynamoDB)) {      LOG.error("Table is not empty but missing the version maker. Failing.");      throw new IOException(E_NO_VERSION_MARKER_AND_NOT_EMPTY + " Table: " + tableName);    }    if (canReadDdbTags) {      LOG.info("Table {} contains no version marker item and tag. " + "The table is empty, so the version marker will be added " + "as TAG and ITEM.",tableName);      putVersionMarkerItemToTable();      tagTableWithVersionMarker();    }    if (!canReadDdbTags) {      LOG.info("Table {} contains no version marker item and the tags are not readable. " + "The table is empty, so the ITEM version marker will be added .",tableName);      putVersionMarkerItemToTable();    }  }  if (versionMarkerItem == null && versionMarkerFromTag != null) {    final int tagVersionMarker=extractVersionFromMarker(versionMarkerFromTag);
      LOG.error("Table is not empty but missing the version maker. Failing.");      throw new IOException(E_NO_VERSION_MARKER_AND_NOT_EMPTY + " Table: " + tableName);    }    if (canReadDdbTags) {      LOG.info("Table {} contains no version marker item and tag. " + "The table is empty, so the version marker will be added " + "as TAG and ITEM.",tableName);      putVersionMarkerItemToTable();      tagTableWithVersionMarker();    }    if (!canReadDdbTags) {      LOG.info("Table {} contains no version marker item and the tags are not readable. " + "The table is empty, so the ITEM version marker will be added .",tableName);      putVersionMarkerItemToTable();    }  }  if (versionMarkerItem == null && versionMarkerFromTag != null) {    final int tagVersionMarker=extractVersionFromMarker(versionMarkerFromTag);    throwExceptionOnVersionMismatch(tagVersionMarker,tableName,E_INCOMPATIBLE_TAG_VERSION);    LOG.info("Table {} contains no version marker ITEM but contains " + "compatible version marker TAG. Restoring the version marker " + "item from tag.",tableName);    putVersionMarkerItemToTable();  }  if (versionMarkerItem != null && versionMarkerFromTag == null && canReadDdbTags) {
    }    if (!canReadDdbTags) {      LOG.info("Table {} contains no version marker item and the tags are not readable. " + "The table is empty, so the ITEM version marker will be added .",tableName);      putVersionMarkerItemToTable();    }  }  if (versionMarkerItem == null && versionMarkerFromTag != null) {    final int tagVersionMarker=extractVersionFromMarker(versionMarkerFromTag);    throwExceptionOnVersionMismatch(tagVersionMarker,tableName,E_INCOMPATIBLE_TAG_VERSION);    LOG.info("Table {} contains no version marker ITEM but contains " + "compatible version marker TAG. Restoring the version marker " + "item from tag.",tableName);    putVersionMarkerItemToTable();  }  if (versionMarkerItem != null && versionMarkerFromTag == null && canReadDdbTags) {    final int itemVersionMarker=extractVersionFromMarker(versionMarkerItem);    throwExceptionOnVersionMismatch(itemVersionMarker,tableName,E_INCOMPATIBLE_ITEM_VERSION);    LOG.info("Table {} contains no version marker TAG but contains " + "compatible version marker ITEM. Restoring the version marker " + "item from item.",tableName);    tagTableWithVersionMarker();  }  if (versionMarkerItem != null && versionMarkerFromTag != null) {    final int tagVersionMarker=extractVersionFromMarker(versionMarkerFromTag);
@Retries.OnceRaw private PutItemOutcome putItem(Item item){
private long importDir() throws IOException {  Preconditions.checkArgument(status.isDirectory());  long totalCountOfEntriesWritten=0;  final Path basePath=status.getPath();  final MetadataStore ms=getStore();
    long countOfFilesWritten=0;    long countOfDirsWritten=0;    RemoteIterator<S3ALocatedFileStatus> it=getFilesystem().listFilesAndEmptyDirectoriesForceNonAuth(basePath,true);    while (it.hasNext()) {      S3ALocatedFileStatus located=it.next();      S3AFileStatus child;      final Path path=located.getPath();      final boolean isDirectory=located.isDirectory();      if (isDirectory) {        child=DynamoDBMetadataStore.makeDirStatus(path,located.getOwner());        dirCache.add(path);        countOfDirsWritten++;      } else {        child=located.toS3AFileStatus();      }      int parentsWritten=putParentsIfNotPresent(child,operationState);
      if (!isDirectory) {        final PathMetadata existingEntry=S3Guard.getWithTtl(ms,path,null,false,true);        if (existingEntry != null) {          final S3AFileStatus existingStatus=existingEntry.getFileStatus();          if (existingStatus.isFile()) {            final String existingEtag=existingStatus.getETag();            final String childEtag=child.getETag();            if (child.getModificationTime() != existingStatus.getModificationTime() || existingStatus.getLen() != child.getLen() || existingEtag == null || !existingEtag.equals(childEtag)) {              if (childEtag.equals(existingEtag)) {                child.setVersionId(existingStatus.getVersionId());              }            } else {              child=null;            }          }        }        if (child != null) {          countOfFilesWritten++;        }      }      if (child != null) {
        if (existingEntry != null) {          final S3AFileStatus existingStatus=existingEntry.getFileStatus();          if (existingStatus.isFile()) {            final String existingEtag=existingStatus.getETag();            final String childEtag=child.getETag();            if (child.getModificationTime() != existingStatus.getModificationTime() || existingStatus.getLen() != child.getLen() || existingEtag == null || !existingEtag.equals(childEtag)) {              if (childEtag.equals(existingEtag)) {                child.setVersionId(existingStatus.getVersionId());              }            } else {              child=null;            }          }        }        if (child != null) {          countOfFilesWritten++;        }      }      if (child != null) {        String t=isDirectory ? "Dir " : "File";        if (verbose) {
            final String childEtag=child.getETag();            if (child.getModificationTime() != existingStatus.getModificationTime() || existingStatus.getLen() != child.getLen() || existingEtag == null || !existingEtag.equals(childEtag)) {              if (childEtag.equals(existingEtag)) {                child.setVersionId(existingStatus.getVersionId());              }            } else {              child=null;            }          }        }        if (child != null) {          countOfFilesWritten++;        }      }      if (child != null) {        String t=isDirectory ? "Dir " : "File";        if (verbose) {          LOG.info("{} {}",t,path);        } else {          LOG.debug("{} {}",t,path);        }        S3Guard.putWithTtl(ms,new PathMetadata(child),getFilesystem().getTtlTimeProvider(),operationState);
@Override public synchronized DirListingMetadata listChildren(Path p) throws IOException {  Path path=standardize(p);  DirListingMetadata listing=getDirListingMeta(path);  if (LOG.isDebugEnabled()) {
@Override public void move(@Nullable Collection<Path> pathsToDelete,@Nullable Collection<PathMetadata> pathsToCreate,@Nullable final BulkOperationState operationState) throws IOException {
@Override public void put(PathMetadata meta,final BulkOperationState operationState) throws IOException {  Preconditions.checkNotNull(meta);  S3AFileStatus status=meta.getFileStatus();  Path path=standardize(status.getPath());synchronized (this) {    if (LOG.isDebugEnabled()) {
@Override public synchronized void put(DirListingMetadata meta,final List<Path> unchangedEntries,final BulkOperationState operationState) throws IOException {  if (LOG.isDebugEnabled()) {
  LOG.debug("delete file entry for {}",path);  if (entry.hasPathMeta()) {    if (tombstone) {      PathMetadata pmd=PathMetadata.tombstone(path,ttlTimeProvider.getNow());      entry.setPathMetadata(pmd);    } else {      entry.setPathMetadata(null);    }  }  if (entry.hasDirMeta()) {    LOG.debug("removing listing of {}",path);    entry.setDirListingMetadata(null);  }  if (!entry.hasDirMeta() && !entry.hasPathMeta()) {    localCache.invalidate(entry);  }  Path parent=path.getParent();  if (parent != null) {    DirListingMetadata dir=getDirListingMeta(parent);
@Override public void fileCopied(final Path sourcePath,final S3ObjectAttributes sourceAttributes,final S3ObjectAttributes destAttributes,final Path destPath,final long blockSize,final boolean addAncestors) throws IOException {  final List<PathMetadata> entriesToAdd=new ArrayList<>(1);
@Override public void fileCopied(final Path sourcePath,final S3ObjectAttributes sourceAttributes,final S3ObjectAttributes destAttributes,final Path destPath,final long blockSize,final boolean addAncestors) throws IOException {  final List<PathMetadata> entriesToAdd=new ArrayList<>(1);  LOG.debug("Updating store with copied file {}",sourcePath);  MetadataStore store=getMetadataStore();synchronized (this) {    checkArgument(!pathsToDelete.contains(sourcePath),"File being renamed is already processed %s",destPath);    S3Guard.addMoveFile(store,pathsToDelete,entriesToAdd,sourcePath,destPath,sourceAttributes.getLen(),blockSize,getOwner(),destAttributes.getETag(),destAttributes.getVersionId());
@Override public int execute() throws ServiceLaunchException, IOException {  URI uri=getUri();  String host=uri.getHost();  String prefix="/" + host + "/";  DynamoDBMetadataStore ddbms=getStore();  S3GuardTableAccess tableAccess=new S3GuardTableAccess(ddbms);  ExpressionSpecBuilder builder=new ExpressionSpecBuilder();  builder.withKeyCondition(ExpressionSpecBuilder.S(PARENT).beginsWith(prefix));
  LOG.info("Scanning for entries with prefix {} to delete from {}",prefix,ddbms);  Iterable<DDBPathMetadata> entries=ddbms.wrapWithRetries(tableAccess.scanMetadata(builder));  List<Path> list=new ArrayList<>();  entries.iterator().forEachRemaining(e -> {    if (!(e instanceof S3GuardTableAccess.VersionMarker)) {      Path p=e.getFileStatus().getPath();      String type=e.getFileStatus().isFile() ? "file" : "directory";      boolean tombstone=e.isDeleted();      if (tombstone) {        type="tombstone " + type;      }      LOG.info("{} {}",type,p);      list.add(p);    }  });  int count=list.size();  filesFound=count;
        type="tombstone " + type;      }      LOG.info("{} {}",type,p);      list.add(p);    }  });  int count=list.size();  filesFound=count;  LOG.info("Found {} entries{}",count,(count == 0 ? " -nothing to purge" : ""));  if (count > 0) {    if (force) {      DurationInfo duration=new DurationInfo(LOG,"deleting %s entries from %s",count,ddbms.toString());      for (      Path path : list) {        ddbms.getInvoker().retry("delete",prefix,true,() -> tableAccess.delete(path));      }      duration.close();      long durationMillis=duration.value();      long timePerEntry=durationMillis / count;
public IOException renameFailed(Exception ex){
@Retries.OnceTranslated public static MetadataStore getMetadataStore(FileSystem fs,ITtlTimeProvider ttlTimeProvider) throws IOException {  Preconditions.checkNotNull(fs);  Configuration conf=fs.getConf();  Preconditions.checkNotNull(conf);  MetadataStore msInstance;  try {    Class<? extends MetadataStore> msClass=getMetadataStoreClass(conf);    msInstance=ReflectionUtils.newInstance(msClass,conf);
@Retries.OnceTranslated public static MetadataStore getMetadataStore(FileSystem fs,ITtlTimeProvider ttlTimeProvider) throws IOException {  Preconditions.checkNotNull(fs);  Configuration conf=fs.getConf();  Preconditions.checkNotNull(conf);  MetadataStore msInstance;  try {    Class<? extends MetadataStore> msClass=getMetadataStoreClass(conf);    msInstance=ReflectionUtils.newInstance(msClass,conf);    LOG.debug("Using {} metadata store for {} filesystem",msClass.getSimpleName(),fs.getScheme());    msInstance.initialize(fs,ttlTimeProvider);    return msInstance;  } catch (  FileNotFoundException e) {    throw e;  }catch (  RuntimeException|IOException e) {    String message="Failed to instantiate metadata store " + conf.get(S3_METADATA_STORE_IMPL) + " defined in "+ S3_METADATA_STORE_IMPL+ ": "+ e;
  Set<Path> deleted=dirMeta.listTombstones();  final Map<Path,PathMetadata> dirMetaMap=dirMeta.getListing().stream().collect(Collectors.toMap(pm -> pm.getFileStatus().getPath(),pm -> pm));  for (  S3AFileStatus s : backingStatuses) {    final Path statusPath=s.getPath();    if (deleted.contains(statusPath)) {      continue;    }    PathMetadata pathMetadata=dirMetaMap.get(statusPath);    boolean shouldUpdate;    if (pathMetadata != null) {      shouldUpdate=s.getModificationTime() > (pathMetadata.getFileStatus()).getModificationTime();      pathMetadata=new PathMetadata(s);    } else {      pathMetadata=new PathMetadata(s);      shouldUpdate=DIR_MERGE_UPDATES_ALL_RECORDS_NONAUTH;    }    if (shouldUpdate) {
    if (deleted.contains(statusPath)) {      continue;    }    PathMetadata pathMetadata=dirMetaMap.get(statusPath);    boolean shouldUpdate;    if (pathMetadata != null) {      shouldUpdate=s.getModificationTime() > (pathMetadata.getFileStatus()).getModificationTime();      pathMetadata=new PathMetadata(s);    } else {      pathMetadata=new PathMetadata(s);      shouldUpdate=DIR_MERGE_UPDATES_ALL_RECORDS_NONAUTH;    }    if (shouldUpdate) {      LOG.debug("Update ms with newer metadata of: {}",s);      entriesToAdd.add(pathMetadata);    }    dirMeta.put(pathMetadata);  }  if (!entriesToAdd.isEmpty()) {
  final PathMetadata pathMetadata=ms.get(path,needEmptyDirectoryFlag);  if (timeProvider == null) {    LOG.debug("timeProvider is null, returning pathMetadata as is");    return pathMetadata;  }  if (allowAuthoritative) {    LOG.debug("allowAuthoritative is true, returning pathMetadata as is");    return pathMetadata;  }  long ttl=timeProvider.getMetadataTtl();  if (pathMetadata != null) {    if (pathMetadata.getLastUpdated() == 0) {      LOG.debug("PathMetadata TTL for {} is 0, so it will be returned as " + "not expired.",path);      return pathMetadata;    }    if (!pathMetadata.isExpired(ttl,timeProvider.getNow())) {      return pathMetadata;    } else {
  final Queue<S3AFileStatus> queue=new ArrayDeque<>();  queue.add(root);  while (!queue.isEmpty()) {    final S3AFileStatus currentDir=queue.poll();    final Path currentDirPath=currentDir.getPath();    try {      List<FileStatus> s3DirListing=Arrays.asList(rawFS.listStatus(currentDirPath));      compareAuthoritativeDirectoryFlag(comparePairs,currentDirPath,s3DirListing);      s3DirListing.stream().filter(pm -> pm.isDirectory()).map(S3AFileStatus.class::cast).forEach(pm -> queue.add(pm));      final List<S3AFileStatus> children=s3DirListing.stream().filter(status -> !status.isDirectory()).map(S3AFileStatus.class::cast).collect(toList());      final List<ComparePair> compareResult=compareS3DirContentToMs(currentDir,children);      comparePairs.addAll(compareResult);      scannedItems++;      scannedItems+=children.size();    } catch (    FileNotFoundException e) {
      List<FileStatus> s3DirListing=Arrays.asList(rawFS.listStatus(currentDirPath));      compareAuthoritativeDirectoryFlag(comparePairs,currentDirPath,s3DirListing);      s3DirListing.stream().filter(pm -> pm.isDirectory()).map(S3AFileStatus.class::cast).forEach(pm -> queue.add(pm));      final List<S3AFileStatus> children=s3DirListing.stream().filter(status -> !status.isDirectory()).map(S3AFileStatus.class::cast).collect(toList());      final List<ComparePair> compareResult=compareS3DirContentToMs(currentDir,children);      comparePairs.addAll(compareResult);      scannedItems++;      scannedItems+=children.size();    } catch (    FileNotFoundException e) {      LOG.error("The path has been deleted since it was queued: " + currentDirPath,e);    }  }  stopwatch.stop();  S3GuardFsckViolationHandler handler=new S3GuardFsckViolationHandler(rawFS,metadataStore);  for (  ComparePair comparePair : comparePairs) {    handler.logError(comparePair);  }  LOG.info("Total scan time: {}s",stopwatch.now(TimeUnit.SECONDS));
protected ComparePair compareFileStatusToPathMetadata(S3AFileStatus s3FileStatus,PathMetadata msPathMetadata) throws IOException {  final Path path=s3FileStatus.getPath();  if (msPathMetadata != null) {
public List<ComparePair> checkDdbInternalConsistency(Path basePath) throws IOException {  Preconditions.checkArgument(basePath.isAbsolute(),"path must be absolute");  List<ComparePair> comparePairs=new ArrayList<>();  String rootStr=basePath.toString();
    if (baseMeta == null) {      throw new FileNotFoundException("Base element metadata is null. " + "This means the base path element is missing, or wrong path was " + "passed as base path to the internal ddb consistency checker.");    }  } else {    baseMeta=new DDBPathMetadata(new S3AFileStatus(Tristate.UNKNOWN,basePath,username));  }  DDBTreeNode root=new DDBTreeNode(baseMeta);  ddbTree.addNode(root);  ddbTree.setRoot(root);  ExpressionSpecBuilder builder=new ExpressionSpecBuilder();  builder.withCondition(ExpressionSpecBuilder.S("parent").beginsWith(pathToParentKey(basePath)));  final IteratorSupport<Item,ScanOutcome> resultIterator=table.scan(builder.buildForScan()).iterator();  resultIterator.forEachRemaining(item -> {    final DDBPathMetadata pmd=itemToPathMetadata(item,username);    DDBTreeNode ddbTreeNode=new DDBTreeNode(pmd);    ddbTree.addNode(ddbTreeNode);  });
public void logError(S3GuardFsck.ComparePair comparePair) throws IOException {  if (!comparePair.containsViolation()) {
public void doFix(S3GuardFsck.ComparePair comparePair) throws IOException {  if (!comparePair.containsViolation()) {
protected void handleComparePair(S3GuardFsck.ComparePair comparePair,StringBuilder sB,HandleMode handleMode) throws IOException {  for (  S3GuardFsck.Violation violation : comparePair.getViolations()) {    try {      ViolationHandler handler=violation.getHandler().getDeclaredConstructor(S3GuardFsck.ComparePair.class).newInstance(comparePair);switch (handleMode) {case FIX:        final String errorStr=handler.getError();      sB.append(errorStr);    break;case LOG:  final String fixStr=handler.fixViolation(rawFs,metadataStore);sB.append(fixStr);break;default:throw new UnsupportedOperationException("Unknown handleMode: " + handleMode);}} catch (NoSuchMethodException e) {LOG.error("Can not find declared constructor for handler: {}",violation.getHandler());}catch (IllegalAccessException|InstantiationException|InvocationTargetException e) {
protected void initS3AFileSystem(String path) throws IOException {
protected void initS3AFileSystem(String path) throws IOException {  LOG.debug("Initializing S3A FS to {}",path);  URI uri=toUri(path);  Configuration conf=new Configuration(getConf());  String nullStore=NullMetadataStore.class.getName();  conf.set(S3_METADATA_STORE_IMPL,nullStore);  String bucket=uri.getHost();  S3AUtils.setBucketOption(conf,bucket,S3_METADATA_STORE_IMPL,S3GUARD_METASTORE_NULL);  String updatedBucketOption=S3AUtils.getBucketOption(conf,bucket,S3_METADATA_STORE_IMPL);
    exit(ret,"");  } catch (  CommandFormat.UnknownOptionException e) {    errorln(e.getMessage());    printHelp();    exit(E_USAGE,e.getMessage());  }catch (  ExitUtil.ExitException e) {    LOG.debug("Exception raised",e);    exit(e.getExitCode(),e.toString());  }catch (  FileNotFoundException e) {    errorln(e.toString());    LOG.debug("Not found:",e);    exit(EXIT_NOT_FOUND,e.toString());  }catch (  Throwable e) {    if (e instanceof ExitCodeProvider) {      final ExitCodeProvider ec=(ExitCodeProvider)e;
    stream=FutureIOSupport.awaitFuture(builder.build());  } catch (  FileNotFoundException e) {    throw storeNotFound(e);  }  try {    if (toConsole) {      bytesRead=0;      @SuppressWarnings("IOResourceOpenedButNotSafelyClosed") Scanner scanner=new Scanner(new BufferedReader(new InputStreamReader(stream,StandardCharsets.UTF_8)));      scanner.useDelimiter("\n");      while (scanner.hasNextLine()) {        linesRead++;        String l=scanner.nextLine();        bytesRead+=l.length() + 1;        println(out,"%s",l);      }    } else {      FileSystem destFS=destPath.getFileSystem(getConf());
  describe("Listing root directory; for consistency allowing " + maxAttempts + " attempts");  for (int attempt=1; attempt <= maxAttempts; ++attempt) {    try {      super.testListEmptyRootDirectory();      break;    } catch (    AssertionError|FileNotFoundException e) {      if (attempt < maxAttempts) {        LOG.info("Attempt {} of {} for empty root directory test failed.  " + "This is likely caused by eventual consistency of S3 " + "listings.  Attempting retry.",attempt,maxAttempts,e);        try {          Thread.sleep(1000);        } catch (        InterruptedException e2) {          Thread.currentThread().interrupt();          fail("Test interrupted.");          break;        }      } else {
@Test public void testSeekToReadaheadAndRead() throws Throwable {  describe("Seek to just before readahead limit and call" + " InputStream.read(byte[])");  Path path=path("testSeekToReadaheadAndRead");  FileSystem fs=getFileSystem();  writeTestDataset(path);  try (FSDataInputStream in=fs.open(path)){    readAtEndAndReturn(in);    final byte[] temp=new byte[5];    int offset=READAHEAD - 1;    in.seek(offset);    int l=in.read(temp);    assertTrue("Reading in temp data",l > 0);
@Test public void testSeekToReadaheadExactlyAndRead() throws Throwable {  describe("Seek to exactly the readahead limit and call" + " InputStream.read(byte[])");  Path path=path("testSeekToReadaheadExactlyAndRead");  FileSystem fs=getFileSystem();  writeTestDataset(path);  try (FSDataInputStream in=fs.open(path)){    readAtEndAndReturn(in);    final byte[] temp=new byte[5];    int offset=READAHEAD;    in.seek(offset);    int l=in.read(temp);
@Test public void testSeekToReadaheadExactlyAndReadByte() throws Throwable {  describe("Seek to exactly the readahead limit and call" + " readByte()");  Path path=path("testSeekToReadaheadExactlyAndReadByte");  FileSystem fs=getFileSystem();  writeTestDataset(path);  try (FSDataInputStream in=fs.open(path)){    readAtEndAndReturn(in);    final byte[] temp=new byte[1];    int offset=READAHEAD;    in.seek(offset);    temp[0]=in.readByte();    assertDatasetEquals(READAHEAD,"read at end of boundary",temp,1);
protected void describe(String text,Object... args){
@Test @SuppressWarnings("deprecation") public void testBlockSize() throws Exception {  FileSystem fs=getFileSystem();  long defaultBlockSize=fs.getDefaultBlockSize();  assertEquals("incorrect blocksize",S3AFileSystem.DEFAULT_BLOCKSIZE,defaultBlockSize);  long newBlockSize=defaultBlockSize * 2;  fs.getConf().setLong(Constants.FS_S3A_BLOCK_SIZE,newBlockSize);  Path dir=path("testBlockSize");  Path file=new Path(dir,"file");  createFile(fs,file,true,dataset(1024,'a','z' - 'a'));  FileStatus fileStatus=fs.getFileStatus(file);  assertEquals("Double default block size in stat(): " + fileStatus,newBlockSize,fileStatus.getBlockSize());  boolean found=false;  FileStatus[] listing=fs.listStatus(dir);  for (  FileStatus stat : listing) {
@Test public void testEncryptionAlgorithmSetToDES() throws Throwable {  assumeEnabled();  intercept(IOException.class,"Unknown Server Side algorithm DES",() -> {    Configuration conf=super.createConfiguration();    conf.set(Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM,"DES");    S3AContract contract=(S3AContract)createContract(conf);    contract.init();    FileSystem fileSystem=contract.getTestFileSystem();    assertNotNull("null filesystem",fileSystem);    URI fsURI=fileSystem.getUri();
@Test public void testEncryptionAlgorithmSSECWithNoEncryptionKey() throws Throwable {  assumeEnabled();  intercept(IllegalArgumentException.class,"The value of property " + Constants.SERVER_SIDE_ENCRYPTION_KEY + " must not be null",() -> {    Configuration conf=super.createConfiguration();    conf.set(Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM,S3AEncryptionMethods.SSE_C.getMethod());    conf.set(Constants.SERVER_SIDE_ENCRYPTION_KEY,null);    S3AContract contract=(S3AContract)createContract(conf);    contract.init();    FileSystem fileSystem=contract.getTestFileSystem();    assertNotNull("null filesystem",fileSystem);    URI fsURI=fileSystem.getUri();
@Test public void testEncryptionAlgorithmSSECWithBlankEncryptionKey() throws Throwable {  intercept(IOException.class,S3AUtils.SSE_C_NO_KEY_ERROR,() -> {    Configuration conf=super.createConfiguration();    conf.set(Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM,S3AEncryptionMethods.SSE_C.getMethod());    conf.set(Constants.SERVER_SIDE_ENCRYPTION_KEY,"");    S3AContract contract=(S3AContract)createContract(conf);    contract.init();    FileSystem fileSystem=contract.getTestFileSystem();    assertNotNull("null filesystem",fileSystem);    URI fsURI=fileSystem.getUri();
@Test public void testEncryptionAlgorithmSSES3WithEncryptionKey() throws Throwable {  assumeEnabled();  intercept(IOException.class,S3AUtils.SSE_S3_WITH_KEY_ERROR,() -> {    Configuration conf=super.createConfiguration();    conf.set(Constants.SERVER_SIDE_ENCRYPTION_ALGORITHM,S3AEncryptionMethods.SSE_S3.getMethod());    conf.set(Constants.SERVER_SIDE_ENCRYPTION_KEY,"4niV/jPK5VFRHY+KNb6wtqYd4xXyMgdJ9XQJpcQUVbs=");    S3AContract contract=(S3AContract)createContract(conf);    contract.init();    FileSystem fileSystem=contract.getTestFileSystem();    assertNotNull("null filesystem",fileSystem);    URI fsURI=fileSystem.getUri();
  tmpFile.delete();  try {    URI localFileURI=tmpFile.toURI();    FileSystem localFS=FileSystem.get(localFileURI,getFileSystem().getConf());    Path localPath=new Path(localFileURI);    int len=10 * 1024;    byte[] data=dataset(len,'A','Z');    writeDataset(localFS,localPath,data,len,1024,true);    S3AFileSystem s3a=getFileSystem();    Path remotePath=methodPath();    verifyMetrics(() -> {      s3a.copyFromLocalFile(false,true,localPath,remotePath);      return "copy";    },with(INVOCATION_COPY_FROM_LOCAL_FILE,1),with(OBJECT_PUT_REQUESTS,1),with(OBJECT_PUT_BYTES,len));    verifyFileContents(s3a,remotePath,data);
@Test public void testEmptyFileChecksums() throws Throwable {  assumeNoDefaultEncryption();  final S3AFileSystem fs=getFileSystem();  Path file1=touchFile("file1");  EtagChecksum checksum1=fs.getFileChecksum(file1,0);
private void assertUploadsPresent(MultipartUtils.UploadIterator list,Set<MultipartTestUtils.IdKey> ourUploads) throws IOException {  Set<MultipartTestUtils.IdKey> uploads=new HashSet<>(ourUploads);  while (list.hasNext()) {    MultipartTestUtils.IdKey listing=toIdKey(list.next());    if (uploads.contains(listing)) {
private Path writeEventuallyConsistentData(final AmazonS3 s3ClientSpy,final Path testpath,final byte[] dataset,final int getObjectInconsistencyCount,final int getMetadataInconsistencyCount,final int copyInconsistencyCount) throws IOException {  writeDataset(fs,testpath,dataset,dataset.length,1024,true);  S3AFileStatus originalStatus=(S3AFileStatus)fs.getFileStatus(testpath);  writeDataset(fs,testpath,dataset,dataset.length / 2,1024,true);
private Path writeEventuallyConsistentData(final AmazonS3 s3ClientSpy,final Path testpath,final byte[] dataset,final int getObjectInconsistencyCount,final int getMetadataInconsistencyCount,final int copyInconsistencyCount) throws IOException {  writeDataset(fs,testpath,dataset,dataset.length,1024,true);  S3AFileStatus originalStatus=(S3AFileStatus)fs.getFileStatus(testpath);  writeDataset(fs,testpath,dataset,dataset.length / 2,1024,true);  LOG.debug("Original file info: {}: version={}, etag={}",testpath,originalStatus.getVersionId(),originalStatus.getETag());  S3AFileStatus newStatus=(S3AFileStatus)fs.getFileStatus(testpath);
private Path writeEventuallyConsistentData(final AmazonS3 s3ClientSpy,final Path testpath,final byte[] dataset,final int getObjectInconsistencyCount,final int getMetadataInconsistencyCount,final int copyInconsistencyCount) throws IOException {  writeDataset(fs,testpath,dataset,dataset.length,1024,true);  S3AFileStatus originalStatus=(S3AFileStatus)fs.getFileStatus(testpath);  writeDataset(fs,testpath,dataset,dataset.length / 2,1024,true);  LOG.debug("Original file info: {}: version={}, etag={}",testpath,originalStatus.getVersionId(),originalStatus.getETag());  S3AFileStatus newStatus=(S3AFileStatus)fs.getFileStatus(testpath);  LOG.debug("Updated file info: {}: version={}, etag={}",testpath,newStatus.getVersionId(),newStatus.getETag());
private Path writeEventuallyConsistentData(final AmazonS3 s3ClientSpy,final Path testpath,final byte[] dataset,final int getObjectInconsistencyCount,final int getMetadataInconsistencyCount,final int copyInconsistencyCount) throws IOException {  writeDataset(fs,testpath,dataset,dataset.length,1024,true);  S3AFileStatus originalStatus=(S3AFileStatus)fs.getFileStatus(testpath);  writeDataset(fs,testpath,dataset,dataset.length / 2,1024,true);  LOG.debug("Original file info: {}: version={}, etag={}",testpath,originalStatus.getVersionId(),originalStatus.getETag());  S3AFileStatus newStatus=(S3AFileStatus)fs.getFileStatus(testpath);  LOG.debug("Updated file info: {}: version={}, etag={}",testpath,newStatus.getVersionId(),newStatus.getETag());  LOG.debug("File {} will be inconsistent for {} HEAD and {} GET requests",testpath,getMetadataInconsistencyCount,getObjectInconsistencyCount);  stubTemporaryUnavailable(s3ClientSpy,getObjectInconsistencyCount,testpath,newStatus);  stubTemporaryWrongVersion(s3ClientSpy,getObjectInconsistencyCount,testpath,originalStatus);  if (versionCheckingIsOnServer()) {
private void stubTemporaryUnavailable(AmazonS3 s3ClientSpy,int inconsistentCallCount,Path testpath,S3AFileStatus newStatus){  Answer<S3Object> temporarilyUnavailableAnswer=new Answer<S3Object>(){    private int callCount=0;    @Override public S3Object answer(    InvocationOnMock invocation) throws Throwable {      callCount++;      if (callCount <= inconsistentCallCount) {
private void stubTemporaryWrongVersion(AmazonS3 s3ClientSpy,int inconsistentCallCount,Path testpath,S3AFileStatus originalStatus){  Answer<S3Object> temporarilyWrongVersionAnswer=new Answer<S3Object>(){    private int callCount=0;    @Override public S3Object answer(    InvocationOnMock invocation) throws Throwable {      callCount++;      S3Object s3Object=(S3Object)invocation.callRealMethod();      if (callCount <= inconsistentCallCount) {
private void stubTemporaryCopyInconsistency(AmazonS3 s3ClientSpy,Path testpath,S3AFileStatus newStatus,int copyInconsistentCallCount){  Answer<CopyObjectResult> temporarilyPreconditionsNotMetAnswer=new Answer<CopyObjectResult>(){    private int callCount=0;    @Override public CopyObjectResult answer(    InvocationOnMock invocation) throws Throwable {      callCount++;      if (callCount <= copyInconsistentCallCount) {        String message="preconditions not met on call " + callCount + " of "+ copyInconsistentCallCount;
private void stubTemporaryMetadataInconsistency(AmazonS3 s3ClientSpy,Path testpath,S3AFileStatus originalStatus,S3AFileStatus newStatus,int metadataInconsistentCallCount){  Answer<ObjectMetadata> temporarilyOldMetadataAnswer=new Answer<ObjectMetadata>(){    private int callCount=0;    @Override public ObjectMetadata answer(    InvocationOnMock invocation) throws Throwable {      ObjectMetadata objectMetadata=(ObjectMetadata)invocation.callRealMethod();      callCount++;      if (callCount <= metadataInconsistentCallCount) {
private void stubTemporaryNotFound(AmazonS3 s3ClientSpy,int inconsistentCallCount,Path testpath){  Answer<ObjectMetadata> notFound=new Answer<ObjectMetadata>(){    private int callCount=0;    @Override public ObjectMetadata answer(    InvocationOnMock invocation) throws Throwable {      callCount++;      if (callCount <= inconsistentCallCount) {
  Configuration conf2=new Configuration(conf);  S3AUtils.clearBucketOption(conf2,bucket,AWS_CREDENTIALS_PROVIDER);  S3AUtils.clearBucketOption(conf2,bucket,ACCESS_KEY);  S3AUtils.clearBucketOption(conf2,bucket,SECRET_KEY);  S3AUtils.clearBucketOption(conf2,bucket,SESSION_TOKEN);  MarshalledCredentials mc=fromSTSCredentials(sessionCreds);  updateConfigWithSessionCreds(conf2,mc);  conf2.set(AWS_CREDENTIALS_PROVIDER,TEMPORARY_AWS_CREDENTIALS);  try (S3AFileSystem fs=S3ATestUtils.createTestFileSystem(conf2)){    createAndVerifyFile(fs,path("testSTS"),TEST_FILE_SIZE);  }   conf2.set(SESSION_TOKEN,"invalid-" + sessionCreds.getSessionToken());  try (S3AFileSystem fs=S3ATestUtils.createTestFileSystem(conf2)){    createAndVerifyFile(fs,path("testSTSInvalidToken"),TEST_FILE_SIZE);    fail("Expected an access exception, but file access to " + fs.getUri() + " was allowed: "+ fs);  } catch (  AWSS3IOException|AWSBadRequestException ex) {
  S3AUtils.clearBucketOption(conf2,bucket,AWS_CREDENTIALS_PROVIDER);  S3AUtils.clearBucketOption(conf2,bucket,ACCESS_KEY);  S3AUtils.clearBucketOption(conf2,bucket,SECRET_KEY);  S3AUtils.clearBucketOption(conf2,bucket,SESSION_TOKEN);  MarshalledCredentials mc=fromSTSCredentials(sessionCreds);  updateConfigWithSessionCreds(conf2,mc);  conf2.set(AWS_CREDENTIALS_PROVIDER,TEMPORARY_AWS_CREDENTIALS);  try (S3AFileSystem fs=S3ATestUtils.createTestFileSystem(conf2)){    createAndVerifyFile(fs,path("testSTS"),TEST_FILE_SIZE);  }   conf2.set(SESSION_TOKEN,"invalid-" + sessionCreds.getSessionToken());  try (S3AFileSystem fs=S3ATestUtils.createTestFileSystem(conf2)){    createAndVerifyFile(fs,path("testSTSInvalidToken"),TEST_FILE_SIZE);    fail("Expected an access exception, but file access to " + fs.getUri() + " was allowed: "+ fs);  } catch (  AWSS3IOException|AWSBadRequestException ex) {    LOG.info("Expected Exception: {}",ex.toString());
@Test public void testSessionCredentialsRegionBadEndpoint() throws Throwable {  describe("Create a session with a bad region and expect fast failure");  IllegalArgumentException ex=expectedSessionRequestFailure(IllegalArgumentException.class," ",EU_IRELAND,"");
@Test public void testConsistentListLocatedStatusAfterPut() throws Exception {  final S3AFileSystem fs=getFileSystem();  String rootDir="doTestConsistentListLocatedStatusAfterPut";  fs.mkdirs(path(rootDir));  final int[] numOfPaths={0,1,5};  for (  int normalPathNum : numOfPaths) {    for (    int delayedPathNum : new int[]{0,2}) {
  for (; index < normalFileNum; index++) {    fileNames.add("file-" + index);  }  for (; index < normalFileNum + delayedFileNum; index++) {    fileNames.add("file-" + index + "-"+ DEFAULT_DELAY_KEY_SUBSTRING);  }  int filesAndEmptyDirectories=0;  for (  Path dir : testDirs) {    for (    String fileName : fileNames) {      writeTextFile(fs,new Path(dir,fileName),"I, " + fileName,false);      filesAndEmptyDirectories++;    }  }  final RemoteIterator<LocatedFileStatus> statusIterator=fs.listFiles(baseTestDir,recursive);  final Collection<Path> listedFiles=new HashSet<>();  for (; statusIterator.hasNext(); ) {    final FileStatus status=statusIterator.next();    assertTrue("FileStatus " + status + " is not a file!",status.isFile());    listedFiles.add(status.getPath());
@Test public void testInconsistentS3ClientDeletes() throws Throwable {  describe("Verify that delete adds tombstones which block entries" + " returned in (inconsistent) listings");  assumeV2ListAPI();  S3AFileSystem fs=getFileSystem();  Path root=path("testInconsistentS3ClientDeletes-" + DEFAULT_DELAY_KEY_SUBSTRING);  for (int i=0; i < 3; i++) {    fs.mkdirs(new Path(root,"dir-" + i));    touch(fs,new Path(root,"file-" + i));    for (int j=0; j < 3; j++) {      touch(fs,new Path(new Path(root,"dir-" + i),"file-" + i + "-"+ j));    }  }  clearInconsistency(fs);  String key=fs.pathToKey(root) + "/";  LOG.info("Listing objects before executing delete()");  ListObjectsV2Result preDeleteDelimited=listObjectsV2(fs,key,"/");  ListObjectsV2Result preDeleteUndelimited=listObjectsV2(fs,key,null);
private <T>void assertListSizeEqual(String message,List<T> expected,List<T> actual){  String leftContents=expected.stream().map(n -> n.toString()).collect(Collectors.joining("\n"));  String rightContents=actual.stream().map(n -> n.toString()).collect(Collectors.joining("\n"));  String summary="\nExpected:" + leftContents + "\n-----------\n"+ "Actual:"+ rightContents+ "\n-----------\n";  if (expected.size() != actual.size()) {
@Test public void testTombstoneExpiryGuardedDeleteRawCreate() throws Exception {  boolean allowAuthoritative=authoritative;  Path testFilePath=path("TEGDRC-" + UUID.randomUUID() + "/file");
    final AtomicLong now=new AtomicLong(1);    final AtomicLong metadataTtl=new AtomicLong(1);    ITtlTimeProvider testTimeProvider=new ITtlTimeProvider(){      @Override public long getNow(){        return now.get();      }      @Override public long getMetadataTtl(){        return metadataTtl.get();      }    };    guardedFs.setTtlTimeProvider(testTimeProvider);    createAndAwaitFs(guardedFs,testFilePath,originalText);    deleteGuardedTombstoned(guardedFs,testFilePath,now);    createAndAwaitFs(rawFS,testFilePath,newText);    checkListingDoesNotContainPath(guardedFs,testFilePath);    long willExpire=now.get() + metadataTtl.get() + 1L;    now.set(willExpire);
      @Override public long getMetadataTtl(){        return metadataTtl.get();      }    };    guardedFs.setTtlTimeProvider(testTimeProvider);    createAndAwaitFs(guardedFs,testFilePath,originalText);    deleteGuardedTombstoned(guardedFs,testFilePath,now);    createAndAwaitFs(rawFS,testFilePath,newText);    checkListingDoesNotContainPath(guardedFs,testFilePath);    long willExpire=now.get() + metadataTtl.get() + 1L;    now.set(willExpire);    LOG.info("willExpire: {}, ttlNow: {}; ttlTTL: {}",willExpire,testTimeProvider.getNow(),testTimeProvider.getMetadataTtl());    if (authoritative) {      intercept(FileNotFoundException.class,testFilePath.toString(),"File should not be present in the metedatastore in authoritative mode.",() -> readBytesToString(guardedFs,testFilePath,newText.length()));    } else {      String newRead=readBytesToString(guardedFs,testFilePath,newText.length());
private void outOfBandDeletes(final Path testFilePath,final boolean allowAuthoritative) throws Exception {  try {    String text="Hello, World!";    writeTextFile(guardedFs,testFilePath,text,true);    awaitFileStatus(rawFS,testFilePath);    deleteFile(rawFS,testFilePath);    FileStatus status=guardedFs.getFileStatus(testFilePath);
private void overwriteFile(String firstText,String secondText) throws Exception {  boolean allowAuthoritative=authoritative;  Path testFilePath=path("OverwriteFileTest-" + UUID.randomUUID());
private void overwriteFileInListing(String firstText,String secondText) throws Exception {  boolean allowAuthoritative=authoritative;
@Test public void testListingDelete() throws Exception {  boolean allowAuthoritative=authoritative;
  String testFile=testDir + "file-1-" + rUUID;  Path testDirPath=path(testDir);  Path testFilePath=guardedFs.qualify(path(testFile));  String text="Some random text";  try {    writeTextFile(guardedFs,testFilePath,text,true);    awaitFileStatus(rawFS,testFilePath);    final FileStatus[] origList=guardedFs.listStatus(testDirPath);    assertEquals("Added one file to the new dir, so the number of " + "files in the dir should be one.",1,origList.length);    final DirListingMetadata dirListingMetadata=realMs.listChildren(guardedFs.qualify(testDirPath));    assertListingAuthority(allowAuthoritative,dirListingMetadata);    deleteFile(rawFS,testFilePath);    interceptFuture(FileNotFoundException.class,"",rawFS.openFile(testFilePath).build());    intercept(FileNotFoundException.class,() -> rawFS.open(testFilePath).close());    S3AFileStatus status=(S3AFileStatus)guardedFs.getFileStatus(testFilePath);
@Test public void testDeleteIgnoresTombstones() throws Throwable {  describe("Verify that directory delete goes behind tombstones");  Path dir=path("oobdir");  Path testFilePath=new Path(dir,"file");  createAndAwaitFs(guardedFs,testFilePath,"Original File is long");
@Test public void testDeleteIgnoresTombstones() throws Throwable {  describe("Verify that directory delete goes behind tombstones");  Path dir=path("oobdir");  Path testFilePath=new Path(dir,"file");  createAndAwaitFs(guardedFs,testFilePath,"Original File is long");  LOG.info("Initial delete of guarded FS dir {}",dir);  guardedFs.delete(dir,true);  awaitDeletedFileDisappearance(guardedFs,testFilePath);  createAndAwaitFs(rawFS,testFilePath,"hi!");  awaitListingContainsChild(rawFS,dir,testFilePath);  Path sibling=new Path(dir,"sibling");  guardedFs.mkdirs(sibling);
@Test public void testDeleteIgnoresTombstones() throws Throwable {  describe("Verify that directory delete goes behind tombstones");  Path dir=path("oobdir");  Path testFilePath=new Path(dir,"file");  createAndAwaitFs(guardedFs,testFilePath,"Original File is long");  LOG.info("Initial delete of guarded FS dir {}",dir);  guardedFs.delete(dir,true);  awaitDeletedFileDisappearance(guardedFs,testFilePath);  createAndAwaitFs(rawFS,testFilePath,"hi!");  awaitListingContainsChild(rawFS,dir,testFilePath);  Path sibling=new Path(dir,"sibling");  guardedFs.mkdirs(sibling);  LOG.info("Deleting guarded FS dir {} with OOB child",dir);  guardedFs.delete(dir,true);
private void awaitListingContainsChild(S3AFileSystem fs,final Path dir,final Path testFilePath) throws Exception {
@Override public void setAmazonS3Client(AmazonS3 client){
static void cleanupParts(S3AFileSystem fs,Set<IdKey> keySet){  boolean anyFailure=false;  for (  IdKey ik : keySet) {    try {
public static IdKey createPartUpload(S3AFileSystem fs,String key,int len,int partNo) throws IOException {  WriteOperationHelper writeHelper=fs.getWriteOperationHelper();  byte[] data=dataset(len,'a','z');  InputStream in=new ByteArrayInputStream(data);  String uploadId=writeHelper.initiateMultiPartUpload(key);  UploadPartRequest req=writeHelper.newUploadPartRequest(key,uploadId,partNo,len,in,null,0L);  PartETag partEtag=fs.uploadPart(req).getPartETag();
public static void clearAnyUploads(S3AFileSystem fs,Path path){  try {    String key=fs.pathToKey(path);    MultipartUtils.UploadIterator uploads=fs.listUploads(key);    while (uploads.hasNext()) {      MultipartUpload upload=uploads.next();      fs.getWriteOperationHelper().abortMultipartUpload(upload.getKey(),upload.getUploadId(),LOG_EVENT);
public static void print(Logger log,S3ATestUtils.MetricDiff... metrics){  for (  S3ATestUtils.MetricDiff metric : metrics) {
@Test public void testCreateCredentialProvider() throws IOException {  describe("Create the credential provider");  Configuration conf=createValidRoleConf();  try (AssumedRoleCredentialProvider provider=new AssumedRoleCredentialProvider(uri,conf)){
@Test public void testCreateCredentialProviderNoURI() throws IOException {  describe("Create the credential provider");  Configuration conf=createValidRoleConf();  try (AssumedRoleCredentialProvider provider=new AssumedRoleCredentialProvider(null,conf)){
@Test public void testAssumeRoleCreateFS() throws IOException {  describe("Create an FS client with the role and do some basic IO");  String roleARN=getAssumedRoleARN();  Configuration conf=createAssumedRoleConfig(roleARN);  Path path=new Path(getFileSystem().getUri());
    SinglePendingCommit pending=fullOperations.uploadFileToPendingCommit(src,dest,"",uploadPartSize,progress);    pending.save(fs,new Path(readOnlyDir,name + CommitConstants.PENDING_SUFFIX),true);    assertTrue(src.delete());  }));  progress.assertCount("Process counter is not expected",range);  try {    Pair<PendingSet,List<Pair<LocatedFileStatus,IOException>>> pendingCommits=operations.loadSinglePendingCommits(readOnlyDir,true);    List<SinglePendingCommit> commits=pendingCommits.getLeft().getCommits();    assertEquals(range,commits.size());    try (CommitOperations.CommitContext commitContext=operations.initiateCommitOperation(uploadDest)){      commits.parallelStream().forEach((c) -> {        CommitOperations.MaybeIOE maybeIOE=commitContext.commit(c,"origin");        Path path=c.destinationPath();        assertCommitAccessDenied(path,maybeIOE);      });
@Override public void setup() throws Exception {  super.setup();  regionName=determineRegion(getFileSystem().getBucket());
protected FileStatus[] globFS(final S3AFileSystem fs,final Path path,final PathFilter filter,boolean expectAuthFailure,final int expectedCount) throws IOException {
protected FileStatus[] globFS(final S3AFileSystem fs,final Path path,final PathFilter filter,boolean expectAuthFailure,final int expectedCount) throws IOException {  LOG.info("Glob {}",path);  S3ATestUtils.MetricDiff getMetric=new S3ATestUtils.MetricDiff(fs,Statistic.OBJECT_METADATA_REQUESTS);  S3ATestUtils.MetricDiff listMetric=new S3ATestUtils.MetricDiff(fs,Statistic.OBJECT_LIST_REQUESTS);  FileStatus[] st;  try {    st=filter == null ? fs.globStatus(path) : fs.globStatus(path,filter);
  LOG.info("Glob {}",path);  S3ATestUtils.MetricDiff getMetric=new S3ATestUtils.MetricDiff(fs,Statistic.OBJECT_METADATA_REQUESTS);  S3ATestUtils.MetricDiff listMetric=new S3ATestUtils.MetricDiff(fs,Statistic.OBJECT_LIST_REQUESTS);  FileStatus[] st;  try {    st=filter == null ? fs.globStatus(path) : fs.globStatus(path,filter);    LOG.info("Metrics:\n {},\n {}",getMetric,listMetric);    if (expectAuthFailure) {      String resultStr;      if (st == null) {        resultStr="A null array";      } else {        resultStr=StringUtils.join(st,",");      }      fail(String.format("globStatus(%s) should have raised" + " an exception, but returned %s",path,resultStr));    }  } catch (  AccessDeniedException e) {
public static Configuration bindRolePolicy(final Configuration conf,final Policy policy) throws JsonProcessingException {  String p=MODEL.toJson(policy);
public static AbstractS3ATokenIdentifier lookupToken(Credentials submittedCredentials,URI uri,Text kind) throws IOException {  final Token<AbstractS3ATokenIdentifier> token=requireNonNull(lookupS3ADelegationToken(submittedCredentials,uri),"No Token for " + uri);  assertEquals("Kind of token " + token,kind,token.getKind());  AbstractS3ATokenIdentifier tid=token.decodeIdentifier();
protected void enableDelegationTokens(Configuration conf,String binding){  removeBaseAndBucketOverrides(conf,DELEGATION_TOKEN_BINDING);
@Test public void testCreate10Tokens() throws Throwable {  File file=fetchTokens(10);  String csv=FileUtils.readFileToString(file,"UTF-8");
 catch (      IOException e) {        ex=e;      }      timer.end("Request");      return new Outcome(id,startTime,timer,ex);    });  }  NanoTimerStats stats=new NanoTimerStats("Overall");  NanoTimerStats success=new NanoTimerStats("Successful");  NanoTimerStats throttled=new NanoTimerStats("Throttled");  List<Outcome> throttledEvents=new ArrayList<>();  for (int i=0; i < tokens; i++) {    Outcome outcome=completionService.take().get();    ContractTestUtils.NanoTimer timer=outcome.timer;    Exception ex=outcome.exception;    outcome.writeln(csvout);    stats.add(timer);
  NanoTimerStats stats=new NanoTimerStats("Overall");  NanoTimerStats success=new NanoTimerStats("Successful");  NanoTimerStats throttled=new NanoTimerStats("Throttled");  List<Outcome> throttledEvents=new ArrayList<>();  for (int i=0; i < tokens; i++) {    Outcome outcome=completionService.take().get();    ContractTestUtils.NanoTimer timer=outcome.timer;    Exception ex=outcome.exception;    outcome.writeln(csvout);    stats.add(timer);    if (ex != null) {      LOG.info("Throttled at event {}",i,ex);      throttled.add(timer);      throttledEvents.add(outcome);    } else {
    ContractTestUtils.NanoTimer timer=outcome.timer;    Exception ex=outcome.exception;    outcome.writeln(csvout);    stats.add(timer);    if (ex != null) {      LOG.info("Throttled at event {}",i,ex);      throttled.add(timer);      throttledEvents.add(outcome);    } else {      success.add(timer);    }  }  csvout.close();  jobTimer.end("Execution of fetch calls");  LOG.info("Summary file is " + csvFile);  LOG.info("Fetched {} tokens with {} throttle events\n: {}\n{}\n{}",tokens,throttled.getCount(),stats,throttled,success);  double duration=jobTimer.duration();
@Test public void testCommonCrawlLookup() throws Throwable {  FileSystem resourceFS=EXTRA_JOB_RESOURCE_PATH.getFileSystem(getConfiguration());  FileStatus status=resourceFS.getFileStatus(EXTRA_JOB_RESOURCE_PATH);
  job.setOutputKeyClass(Text.class);  job.setOutputValueClass(IntWritable.class);  FileInputFormat.addInputPath(job,input);  FileOutputFormat.setOutputPath(job,output);  job.setMaxMapAttempts(1);  job.setMaxReduceAttempts(1);  URI partitionUri=new URI(EXTRA_JOB_RESOURCE_PATH.toString() + "#_partition.lst");  job.addCacheFile(partitionUri);  describe("Executing Mock Job Submission to %s",output);  job.submit();  final JobStatus status=job.getStatus();  assertEquals("not a mock job",MockJob.NAME,status.getSchedulingInfo());  assertEquals("Job State",JobStatus.State.RUNNING,status.getState());  final Credentials submittedCredentials=requireNonNull(job.getSubmittedCredentials(),"job submitted credentials");  final Collection<Token<? extends TokenIdentifier>> tokens=submittedCredentials.getAllTokens();
  FileInputFormat.addInputPath(job,input);  FileOutputFormat.setOutputPath(job,output);  job.setMaxMapAttempts(1);  job.setMaxReduceAttempts(1);  URI partitionUri=new URI(EXTRA_JOB_RESOURCE_PATH.toString() + "#_partition.lst");  job.addCacheFile(partitionUri);  describe("Executing Mock Job Submission to %s",output);  job.submit();  final JobStatus status=job.getStatus();  assertEquals("not a mock job",MockJob.NAME,status.getSchedulingInfo());  assertEquals("Job State",JobStatus.State.RUNNING,status.getState());  final Credentials submittedCredentials=requireNonNull(job.getSubmittedCredentials(),"job submitted credentials");  final Collection<Token<? extends TokenIdentifier>> tokens=submittedCredentials.getAllTokens();  LOG.info("Token Count = {}",tokens.size());  for (  Token<? extends TokenIdentifier> token : tokens) {
@Test public void testCreateRoleModel() throws Throwable {  describe("self contained role model retrieval");  EnumSet<AWSPolicyProvider.AccessLevel> access=EnumSet.of(AWSPolicyProvider.AccessLevel.READ,AWSPolicyProvider.AccessLevel.WRITE);  S3AFileSystem fs=getFileSystem();  List<RoleModel.Statement> rules=fs.listAWSPolicyRules(access);  assertTrue("No AWS policy rules from FS",!rules.isEmpty());  String ruleset=new RoleModel().toJson(new RoleModel.Policy(rules));
@Test public void testAddTokensFromFileSystem() throws Throwable {  describe("verify FileSystem.addDelegationTokens() collects tokens");  S3AFileSystem fs=getFileSystem();  Credentials cred=new Credentials();  Token<?>[] tokens=fs.addDelegationTokens(YARN_RM,cred);  assertEquals("Number of tokens",1,tokens.length);  Token<?> token=requireNonNull(tokens[0],"token");
@Test public void testCanRetrieveTokenFromCurrentUserCreds() throws Throwable {  describe("Create a DT, add it to the current UGI credentials," + " then retrieve");  delegationTokens.start();  Credentials cred=createDelegationTokens();  UserGroupInformation ugi=UserGroupInformation.getCurrentUser();  ugi.addCredentials(cred);  Token<?>[] tokens=cred.getAllTokens().toArray(new Token<?>[0]);  Token<?> token0=tokens[0];  Text service=token0.getService();
  final Text tokenKind=getTokenKind();  AbstractS3ATokenIdentifier origTokenId=requireNonNull(lookupToken(creds,uri,tokenKind),"original");  final UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();  currentUser.addCredentials(creds);  requireNonNull(lookupToken(currentUser.getCredentials(),uri,tokenKind),"user credentials");  Configuration conf=new Configuration(getConfiguration());  String bucket=fs.getBucket();  disableFilesystemCaching(conf);  unsetHadoopCredentialProviders(conf);  removeBaseAndBucketOverrides(bucket,conf,ACCESS_KEY,SECRET_KEY,SESSION_TOKEN,SERVER_SIDE_ENCRYPTION_ALGORITHM,SERVER_SIDE_ENCRYPTION_KEY,DELEGATION_TOKEN_ROLE_ARN,DELEGATION_TOKEN_ENDPOINT);  conf.set(DELEGATION_TOKEN_ENDPOINT,"http://localhost:8080/");  bindProviderList(bucket,conf,CountInvocationsProvider.NAME);  long originalCount=CountInvocationsProvider.getInvocationCount();  Path testPath=path("testDTFileSystemClient");  try (S3AFileSystem delegatedFS=newS3AInstance(uri,conf)){
@Test public void testHDFSFetchDTCommand() throws Throwable {  describe("Use the HDFS fetchdt CLI to fetch a token");  ExitUtil.disableSystemExit();  S3AFileSystem fs=getFileSystem();  Configuration conf=fs.getConf();  URI fsUri=fs.getUri();  String fsurl=fsUri.toString();  File tokenfile=createTempTokenFile();  String tokenFilePath=tokenfile.getAbsolutePath();  doAs(bobUser,() -> DelegationTokenFetcher.main(conf,args("--webservice",fsurl,tokenFilePath)));  assertTrue("token file was not created: " + tokenfile,tokenfile.exists());  String s=DelegationTokenFetcher.printTokensToString(conf,new Path(tokenfile.toURI()),false);
protected String dtutil(int expected,String... args) throws Exception {  final ByteArrayOutputStream dtUtilContent=new ByteArrayOutputStream();  DtUtilShell dt=new DtUtilShell();  dt.setOut(new PrintStream(dtUtilContent));  dtUtilContent.reset();  int r=doAs(aliceUser,() -> ToolRunner.run(getConfiguration(),dt,args));  String s=dtUtilContent.toString();
@SuppressWarnings("OptionalGetWithoutIsPresent") protected AbstractS3ATokenIdentifier verifyCredentialPropagation(final S3AFileSystem fs,final MarshalledCredentials session,final Configuration conf) throws Exception {  describe("Verify Token Propagation");  unsetHadoopCredentialProviders(conf);  conf.set(DELEGATION_TOKEN_CREDENTIALS_PROVIDER,TemporaryAWSCredentialsProvider.NAME);  session.setSecretsInConfiguration(conf);  try (S3ADelegationTokens delegationTokens2=new S3ADelegationTokens()){    delegationTokens2.bindToFileSystem(fs.getCanonicalUri(),fs.createStoreContext(),fs.createDelegationOperations());    delegationTokens2.init(conf);    delegationTokens2.start();    final Token<AbstractS3ATokenIdentifier> newDT=delegationTokens2.getBoundOrNewDT(new EncryptionSecrets(),null);    delegationTokens2.resetTokenBindingToDT(newDT);    final AbstractS3ATokenIdentifier boundId=delegationTokens2.getDecodedIdentifier().get();
public static SuccessData validateSuccessFile(final Path outputPath,final String committerName,final S3AFileSystem fs,final String origin,final int minimumFileCount) throws IOException {  SuccessData successData=loadSuccessFile(fs,outputPath,origin);  String commitDetails=successData.toString();
public static SuccessData validateSuccessFile(final Path outputPath,final String committerName,final S3AFileSystem fs,final String origin,final int minimumFileCount) throws IOException {  SuccessData successData=loadSuccessFile(fs,outputPath,origin);  String commitDetails=successData.toString();  LOG.info("Committer name " + committerName + "\n{}",commitDetails);
public static SuccessData validateSuccessFile(final Path outputPath,final String committerName,final S3AFileSystem fs,final String origin,final int minimumFileCount) throws IOException {  SuccessData successData=loadSuccessFile(fs,outputPath,origin);  String commitDetails=successData.toString();  LOG.info("Committer name " + committerName + "\n{}",commitDetails);  LOG.info("Committer statistics: \n{}",successData.dumpMetrics("  "," = ","\n"));
public static SuccessData loadSuccessFile(final S3AFileSystem fs,final Path outputPath,final String origin) throws IOException {  ContractTestUtils.assertPathExists(fs,"Output directory " + outputPath + " from "+ origin+ " not found: Job may not have executed",outputPath);  Path success=new Path(outputPath,_SUCCESS);  FileStatus status=ContractTestUtils.verifyPathExists(fs,"job completion marker " + success + " from "+ origin+ " not found: Job may have failed",success);  assertTrue("_SUCCESS outout from " + origin + " is not a file "+ status,status.isFile());  assertTrue("0 byte success file " + success + " from "+ origin+ "; an S3A committer was not used",status.getLen() > 0);
protected Configuration patchConfigurationForCommitter(final Configuration jobConf){  jobConf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES,isUniqueFilenames());  bindCommitter(jobConf,CommitConstants.S3A_COMMITTER_FACTORY,committerName());  jobConf.setBoolean(KEY_SCALE_TESTS_ENABLED,isScaleTest());  String staging=stagingFilesDir.getRoot().getAbsolutePath();
private void verifyCommitExists(SinglePendingCommit commit) throws FileNotFoundException, ValidationFailure, IOException {  commit.validate();  Path path=getFileSystem().keyToQualifiedPath(commit.getDestinationKey());  FileStatus status=getFileSystem().getFileStatus(path);
private Path validatePendingCommitData(String filename,Path magicFile) throws IOException {  S3AFileSystem fs=getFileSystem();  Path pendingDataPath=new Path(magicFile.getParent(),filename + PENDING_SUFFIX);  FileStatus fileStatus=verifyPathExists(fs,"no pending file",pendingDataPath);  assertTrue("No data in " + fileStatus,fileStatus.getLen() > 0);  String data=read(fs,pendingDataPath);
  Path destFile3=new Path(subdir,"file3");  List<Path> destinations=Lists.newArrayList(destFile1,destFile2,destFile3);  List<SinglePendingCommit> commits=new ArrayList<>(3);  for (  Path destination : destinations) {    SinglePendingCommit commit1=actions.uploadFileToPendingCommit(localFile,destination,null,DEFAULT_MULTIPART_SIZE,progress);    commits.add(commit1);  }  resetFailures();  assertPathDoesNotExist("destination dir",destDir);  assertPathDoesNotExist("subdirectory",subdir);  LOG.info("Initiating commit operations");  try (CommitOperations.CommitContext commitContext=actions.initiateCommitOperation(destDir)){    MetricDiff writes=new MetricDiff(fs,Statistic.S3GUARD_METADATASTORE_RECORD_WRITES);    LOG.info("Commit #1");    commitContext.commitOrFail(commits.get(0));    final String firstCommitContextString=commitContext.toString();
public static AmazonS3 newMockS3Client(final ClientResults results,final ClientErrors errors){  AmazonS3Client mockClient=mock(AmazonS3Client.class);  final Object lock=new Object();  when(mockClient.initiateMultipartUpload(any(InitiateMultipartUploadRequest.class))).thenAnswer(invocation -> {
  when(mockClient.initiateMultipartUpload(any(InitiateMultipartUploadRequest.class))).thenAnswer(invocation -> {    LOG.debug("initiateMultipartUpload for {}",mockClient);synchronized (lock) {      if (results.requests.size() == errors.failOnInit) {        if (errors.recover) {          errors.failOnInit(-1);        }        throw new AmazonClientException("Mock Fail on init " + results.requests.size());      }      String uploadId=UUID.randomUUID().toString();      InitiateMultipartUploadRequest req=getArgumentAt(invocation,0,InitiateMultipartUploadRequest.class);      results.requests.put(uploadId,req);      results.activeUploads.put(uploadId,req.getKey());      results.uploads.add(uploadId);      return newResult(results.requests.get(uploadId),uploadId);    }  });  when(mockClient.uploadPart(any(UploadPartRequest.class))).thenAnswer(invocation -> {
  when(mockClient.uploadPart(any(UploadPartRequest.class))).thenAnswer(invocation -> {    LOG.debug("uploadPart for {}",mockClient);synchronized (lock) {      if (results.parts.size() == errors.failOnUpload) {        if (errors.recover) {          errors.failOnUpload(-1);        }        LOG.info("Triggering upload failure");        throw new AmazonClientException("Mock Fail on upload " + results.parts.size());      }      UploadPartRequest req=getArgumentAt(invocation,0,UploadPartRequest.class);      results.parts.add(req);      String etag=UUID.randomUUID().toString();      List<String> etags=results.tagsByUpload.get(req.getUploadId());      if (etags == null) {        etags=Lists.newArrayList();        results.tagsByUpload.put(req.getUploadId(),etags);
      List<String> etags=results.tagsByUpload.get(req.getUploadId());      if (etags == null) {        etags=Lists.newArrayList();        results.tagsByUpload.put(req.getUploadId(),etags);      }      etags.add(etag);      return newResult(req,etag);    }  });  when(mockClient.completeMultipartUpload(any(CompleteMultipartUploadRequest.class))).thenAnswer(invocation -> {    LOG.debug("completeMultipartUpload for {}",mockClient);synchronized (lock) {      if (results.commits.size() == errors.failOnCommit) {        if (errors.recover) {          errors.failOnCommit(-1);        }        throw new AmazonClientException("Mock Fail on commit " + results.commits.size());      }      CompleteMultipartUploadRequest req=getArgumentAt(invocation,0,CompleteMultipartUploadRequest.class);
      if (results.commits.size() == errors.failOnCommit) {        if (errors.recover) {          errors.failOnCommit(-1);        }        throw new AmazonClientException("Mock Fail on commit " + results.commits.size());      }      CompleteMultipartUploadRequest req=getArgumentAt(invocation,0,CompleteMultipartUploadRequest.class);      String uploadId=req.getUploadId();      removeUpload(results,uploadId);      results.commits.add(req);      return newResult(req);    }  });  doAnswer(invocation -> {    LOG.debug("abortMultipartUpload for {}",mockClient);synchronized (lock) {      if (results.aborts.size() == errors.failOnAbort) {        if (errors.recover) {
      CompleteMultipartUploadRequest req=getArgumentAt(invocation,0,CompleteMultipartUploadRequest.class);      String uploadId=req.getUploadId();      removeUpload(results,uploadId);      results.commits.add(req);      return newResult(req);    }  });  doAnswer(invocation -> {    LOG.debug("abortMultipartUpload for {}",mockClient);synchronized (lock) {      if (results.aborts.size() == errors.failOnAbort) {        if (errors.recover) {          errors.failOnAbort(-1);        }        throw new AmazonClientException("Mock Fail on abort " + results.aborts.size());      }      AbortMultipartUploadRequest req=getArgumentAt(invocation,0,AbortMultipartUploadRequest.class);      String id=req.getUploadId();
protected void describe(String text,Object... args){
@Test public void testValidateDefaultConflictMode() throws Throwable {  Configuration baseConf=new Configuration(true);  String[] sources=baseConf.getPropertySources(FS_S3A_COMMITTER_STAGING_CONFLICT_MODE);  String sourceStr=Arrays.stream(sources).collect(Collectors.joining(","));
    completedStage("overall",d);  });  final StringBuilder results=new StringBuilder();  results.append("\"Operation\"\t\"Duration\"\n");  Consumer<String> stage=(s) -> {    DurationInfo duration=completedStages.get(s);    results.append(String.format("\"%s\"\t\"%s\"\n",s,duration == null ? "" : duration));  };  stage.accept("teragen");  stage.accept("terasort");  stage.accept("teravalidate");  stage.accept("overall");  String text=results.toString();  File resultsFile=File.createTempFile("results",".csv");  FileUtils.write(resultsFile,text,StandardCharsets.UTF_8);
protected void dumpOutputTree(Path path) throws Exception {
@Test public void testRenameDirFailsInDelete() throws Throwable {  describe("rename with source read only; multi=%s",multiDelete);  S3AFileSystem fs=getFileSystem();  roleFS.mkdirs(writableDir);  List<Path> createdFiles=createFiles(fs,readOnlyDir,dirDepth,fileCount,dirCount);  int expectedFileCount=createdFiles.size();  assertFileCount("files ready to rename",roleFS,readOnlyDir,expectedFileCount);
private AccessDeniedException expectRenameForbidden(Path src,Path dest) throws Exception {  try (DurationInfo ignored=new DurationInfo(LOG,true,"rename(%s, %s)",src,dest)){    return forbidden("Renaming " + src + " to "+ dest,"",() -> {      boolean result=roleFS.rename(src,dest);
private AccessDeniedException expectRenameForbidden(Path src,Path dest) throws Exception {  try (DurationInfo ignored=new DurationInfo(LOG,true,"rename(%s, %s)",src,dest)){    return forbidden("Renaming " + src + " to "+ dest,"",() -> {      boolean result=roleFS.rename(src,dest);      LOG.error("Rename should have been forbidden but returned {}",result);
private AccessDeniedException expectRenameForbidden(Path src,Path dest) throws Exception {  try (DurationInfo ignored=new DurationInfo(LOG,true,"rename(%s, %s)",src,dest)){    return forbidden("Renaming " + src + " to "+ dest,"",() -> {      boolean result=roleFS.rename(src,dest);      LOG.error("Rename should have been forbidden but returned {}",result);      LOG.error("Source directory:\n{}",ContractTestUtils.ls(getFileSystem(),src.getParent()));
private <T>List<T> dump(List<T> l){  int c=1;  for (  T t : l) {
public <T>T exec(Callable<T> eval,ExpectedProbe... expectedA) throws Exception {  List<ExpectedProbe> expected=Arrays.asList(expectedA);  resetMetricDiffs();  assumeProbesEnabled(expected);  T r=eval.call();  String text="operation returning " + (r != null ? r.toString() : "null");
public <T>T exec(Callable<T> eval,ExpectedProbe... expectedA) throws Exception {  List<ExpectedProbe> expected=Arrays.asList(expectedA);  resetMetricDiffs();  assumeProbesEnabled(expected);  T r=eval.call();  String text="operation returning " + (r != null ? r.toString() : "null");  LOG.info("{}",text);  LOG.info("state {}",this);
private void doTestBatchWrite(int numDelete,int numPut,DynamoDBMetadataStore ms) throws IOException {  Path path=new Path("/ITestDynamoDBMetadataStore_testBatchWrite_" + numDelete + '_'+ numPut);  final Path root=fileSystem.makeQualified(path);  final Path oldDir=new Path(root,"oldDir");  final Path newDir=new Path(root,"newDir");
    newMetas.add(new PathMetadata(basicFileStatus(new Path(newDir,"child" + i),i,false)));  }  Collection<Path> pathsToDelete=null;  if (oldMetas != null) {    ms.put(new DirListingMetadata(oldDir,oldMetas,false),UNCHANGED_ENTRIES,putState);    assertEquals("Child count",0,ms.listChildren(newDir).withoutTombstones().numEntries());    Assertions.assertThat(ms.listChildren(oldDir).getListing()).describedAs("Old Directory listing").containsExactlyInAnyOrderElementsOf(oldMetas);    assertTrue(CollectionUtils.isEqualCollection(oldMetas,ms.listChildren(oldDir).getListing()));    pathsToDelete=new ArrayList<>(oldMetas.size());    for (    PathMetadata meta : oldMetas) {      pathsToDelete.add(meta.getFileStatus().getPath());    }  }  AncestorState state=checkNotNull(ms.initiateBulkWrite(BulkOperationState.OperationType.Put,newDir),"No state from initiateBulkWrite()");  assertEquals("bulk write destination",newDir,state.getDest());  ThrottleTracker throttleTracker=new ThrottleTracker(ms);  try (DurationInfo ignored=new DurationInfo(LOG,true,"Move")){    ms.move(pathsToDelete,newMetas,state);
  String subdir=base + "/subdir";  Path subDirPath=strToPath(subdir);  createNewDirs(base,subdir);  String subFile=subdir + "/file1";  Path subFilePath=strToPath(subFile);  putListStatusFiles(subdir,true,subFile);  final DDBPathMetadata subDirMetadataOrig=ms.get(subDirPath);  Assertions.assertThat(subDirMetadataOrig.isAuthoritativeDir()).describedAs("Subdirectory %s",subDirMetadataOrig).isTrue();  long now=getTime();  long oldTime=now - MINUTE;  putFile(subdir,oldTime,null);  getFile(subdir);  Path basePath=strToPath(base);  DirListingMetadata listing=ms.listChildren(basePath);  String childText=listing.prettyPrint();
@Test public void testDumpTable() throws Throwable {  describe("Dump the table contents, but not the S3 Store");  String target=System.getProperty("test.build.dir","target");  File buildDir=new File(target).getAbsoluteFile();  String name="ITestDynamoDBMetadataStore";  File destFile=new File(buildDir,name);  DumpS3GuardDynamoTable.dumpStore(null,ddbmsStatic,getFileSystem().getConf(),destFile,fsUri);  File storeFile=new File(buildDir,name + DumpS3GuardDynamoTable.SCAN_CSV);  try (BufferedReader in=new BufferedReader(new InputStreamReader(new FileInputStream(storeFile),Charset.forName("UTF-8")))){    for (    String line : org.apache.commons.io.IOUtils.readLines(in)) {
@Test public void test_900_instrumentation() throws Throwable {  describe("verify the owner FS gets updated after throttling events");  Assume.assumeTrue("No throttling expected",expectThrottling());  S3AFileSystem fs=getFileSystem();  String fsSummary=fs.toString();  S3AStorageStatistics statistics=fs.getStorageStatistics();  for (  StorageStatistics.LongStatistic statistic : statistics) {
@Test public void test_999_delete_all_entries() throws Throwable {  describe("Delete all entries from the table");  S3GuardTableAccess tableAccess=new S3GuardTableAccess(ddbms);  ExpressionSpecBuilder builder=new ExpressionSpecBuilder();  final String path="/test/";  builder.withCondition(ExpressionSpecBuilder.S(PARENT).beginsWith(path));  Iterable<DDBPathMetadata> entries=ddbms.wrapWithRetries(tableAccess.scanMetadata(builder));  List<Path> list=new ArrayList<>();  try {    entries.iterator().forEachRemaining(e -> {      Path p=e.getFileStatus().getPath();
  final ExecutorService executorService=Executors.newFixedThreadPool(THREADS);  final List<Callable<ExecutionOutcome>> tasks=new ArrayList<>(THREADS);  final AtomicInteger throttleExceptions=new AtomicInteger(0);  for (int i=0; i < THREADS; i++) {    tasks.add(() -> {      final ExecutionOutcome outcome=new ExecutionOutcome();      final ContractTestUtils.NanoTimer t=new ContractTestUtils.NanoTimer();      for (int j=0; j < operationsPerThread; j++) {        if (tracker.isThrottlingDetected() || throttleExceptions.get() > 0) {          outcome.skipped=true;          return outcome;        }        try {          action.call();          outcome.completed++;        } catch (        AWSServiceThrottledException e) {
  final List<Callable<ExecutionOutcome>> tasks=new ArrayList<>(THREADS);  final AtomicInteger throttleExceptions=new AtomicInteger(0);  for (int i=0; i < THREADS; i++) {    tasks.add(() -> {      final ExecutionOutcome outcome=new ExecutionOutcome();      final ContractTestUtils.NanoTimer t=new ContractTestUtils.NanoTimer();      for (int j=0; j < operationsPerThread; j++) {        if (tracker.isThrottlingDetected() || throttleExceptions.get() > 0) {          outcome.skipped=true;          return outcome;        }        try {          action.call();          outcome.completed++;        } catch (        AWSServiceThrottledException e) {          LOG.info("Operation [{}] raised a throttled exception " + e,j,e);
      final ContractTestUtils.NanoTimer t=new ContractTestUtils.NanoTimer();      for (int j=0; j < operationsPerThread; j++) {        if (tracker.isThrottlingDetected() || throttleExceptions.get() > 0) {          outcome.skipped=true;          return outcome;        }        try {          action.call();          outcome.completed++;        } catch (        AWSServiceThrottledException e) {          LOG.info("Operation [{}] raised a throttled exception " + e,j,e);          LOG.debug(e.toString(),e);          throttleExceptions.incrementAndGet();          outcome.throttleExceptions.add(e);          outcome.throttled++;        }catch (        Exception e) {
        } catch (        AWSServiceThrottledException e) {          LOG.info("Operation [{}] raised a throttled exception " + e,j,e);          LOG.debug(e.toString(),e);          throttleExceptions.incrementAndGet();          outcome.throttleExceptions.add(e);          outcome.throttled++;        }catch (        Exception e) {          LOG.error("Failed to execute {}",operation,e);          outcome.exceptions.add(e);          break;        }        tracker.probe();      }      LOG.info("Thread completed {} with in {} ms with outcome {}: {}",operation,t.elapsedTimeMs(),outcome,tracker);      return outcome;    });  }  final List<Future<ExecutionOutcome>> futures=executorService.invokeAll(tasks,getTestTimeoutMillis(),TimeUnit.MILLISECONDS);
          return new Thread(r,"testConcurrentTableCreations" + count.getAndIncrement());        }      });      ((ThreadPoolExecutor)executor).prestartAllCoreThreads();      Future<Exception>[] futures=new Future[concurrentOps];      for (int f=0; f < concurrentOps; f++) {        final int index=f;        futures[f]=executor.submit(new Callable<Exception>(){          @Override public Exception call() throws Exception {            ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();            Exception result=null;            try (DynamoDBMetadataStore store=new DynamoDBMetadataStore()){              store.initialize(conf,new S3Guard.TtlTimeProvider(conf));            } catch (            Exception e) {              LOG.error(e.getClass() + ": " + e.getMessage());              result=e;
@Test public void testCLIFsckWithParam() throws Exception {  LOG.info("This test serves the purpose to run fsck with the correct " + "parameters, so there will be no exception thrown.");  final int result=run(S3GuardTool.Fsck.NAME,"-check","s3a://" + getFileSystem().getBucket());
@Test public void testCLIFsckDDbInternalParam() throws Exception {  describe("This test serves the purpose to run fsck with the correct " + "parameters, so there will be no exception thrown.");  final int result=run(S3GuardTool.Fsck.NAME,"-" + Fsck.DDB_MS_CONSISTENCY_FLAG,"s3a://" + getFileSystem().getBucket());
@Test public void testCLIFsckDDbFixOnlyFails() throws Exception {  describe("This test serves the purpose to run fsck with the correct " + "parameters, so there will be no exception thrown.");  final int result=run(S3GuardTool.Fsck.NAME,"-" + Fsck.FIX_FLAG,"s3a://" + getFileSystem().getBucket());
@Test public void testCLIFsckDDbFixAndInternalSucceed() throws Exception {  describe("This test serves the purpose to run fsck with the correct " + "parameters, so there will be no exception thrown.");  final int result=run(S3GuardTool.Fsck.NAME,"-" + Fsck.FIX_FLAG,"-" + Fsck.DDB_MS_CONSISTENCY_FLAG,"s3a://" + getFileSystem().getBucket());
@Test public void testStoreInfo() throws Throwable {  S3GuardTool.BucketInfo cmd=toClose(new S3GuardTool.BucketInfo(getFileSystem().getConf()));  cmd.setStore(getMetadataStore());  try {    String output=exec(cmd,cmd.getName(),"-" + BucketInfo.GUARDED_FLAG,getFileSystem().getUri().toString());
@Test public void testSetCapacity() throws Throwable {  S3GuardTool cmd=toClose(new S3GuardTool.SetCapacity(getFileSystem().getConf()));  cmd.setStore(getMetadataStore());  try {    String output=exec(cmd,cmd.getName(),"-" + READ_FLAG,"100","-" + WRITE_FLAG,"100",getFileSystem().getUri().toString());
  S3GuardTool.Uploads cmd=new S3GuardTool.Uploads(fs.getConf());  ByteArrayOutputStream buf=new ByteArrayOutputStream();  allOptions.add(cmd.getName());  allOptions.addAll(Arrays.asList(options));  if (ageSeconds > 0) {    allOptions.add("-" + Uploads.SECONDS_FLAG);    allOptions.add(String.valueOf(ageSeconds));  }  allOptions.add(path.toString());  exec(0,"",cmd,buf,allOptions.toArray(new String[0]));  try (BufferedReader reader=new BufferedReader(new InputStreamReader(new ByteArrayInputStream(buf.toByteArray())))){    String line;    while ((line=reader.readLine()) != null) {      String[] fields=line.split("\\s");      if (fields.length == 4 && fields[0].equals(Uploads.TOTAL)) {        int parsedUploads=Integer.parseInt(fields[1]);
protected void describe(String text,Object... args){
  describe("Test workload of batched move() operations");  int width=getConf().getInt(KEY_DIRECTORY_COUNT,DEFAULT_DIRECTORY_COUNT);  int depth=width;  long operations=getConf().getLong(KEY_OPERATION_COUNT,DEFAULT_OPERATION_COUNT);  List<PathMetadata> origMetas=new ArrayList<>();  createDirTree(BUCKET_ROOT,depth,width,origMetas);  List<Path> origPaths=metasToPaths(origMetas);  List<PathMetadata> movedMetas=moveMetas(origMetas,BUCKET_ROOT,new Path(BUCKET_ROOT,"moved-here"));  List<Path> movedPaths=metasToPaths(movedMetas);  long count=1;  try (MetadataStore ms=createMetadataStore()){    try {      count=populateMetadataStore(origMetas,ms);      describe("Running move workload");      NanoTimer moveTimer=new NanoTimer();
private static void printTiming(Logger log,String op,NanoTimer timer,long count){  double msec=(double)timer.duration() / 1000;  double msecPerOp=msec / count;
  long blocks=filesize / uploadBlockSize;  long blocksPerMB=_1MB / uploadBlockSize;  S3AFileSystem fs=getFileSystem();  StorageStatistics storageStatistics=fs.getStorageStatistics();  String putRequests=Statistic.OBJECT_PUT_REQUESTS.getSymbol();  String putBytes=Statistic.OBJECT_PUT_BYTES.getSymbol();  Statistic putRequestsActive=Statistic.OBJECT_PUT_REQUESTS_ACTIVE;  Statistic putBytesPending=Statistic.OBJECT_PUT_BYTES_PENDING;  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  S3AInstrumentation.OutputStreamStatistics streamStatistics;  long blocksPer10MB=blocksPerMB * 10;  ProgressCallback progress=new ProgressCallback(timer);  try (FSDataOutputStream out=fs.create(fileToCreate,true,uploadBlockSize,progress)){    try {      streamStatistics=getOutputStreamStatistics(out);
  S3AInstrumentation.OutputStreamStatistics streamStatistics;  long blocksPer10MB=blocksPerMB * 10;  ProgressCallback progress=new ProgressCallback(timer);  try (FSDataOutputStream out=fs.create(fileToCreate,true,uploadBlockSize,progress)){    try {      streamStatistics=getOutputStreamStatistics(out);    } catch (    ClassCastException e) {      LOG.info("Wrapped output stream is not block stream: {}",out.getWrappedStream());      streamStatistics=null;    }    for (long block=1; block <= blocks; block++) {      out.write(data);      long written=block * uploadBlockSize;      if (block % blocksPer10MB == 0 || written == filesize) {        long percentage=written * 100 / filesize;        double elapsedTime=timer.elapsedTime() / 1.0e9;
  ProgressCallback progress=new ProgressCallback(timer);  try (FSDataOutputStream out=fs.create(fileToCreate,true,uploadBlockSize,progress)){    try {      streamStatistics=getOutputStreamStatistics(out);    } catch (    ClassCastException e) {      LOG.info("Wrapped output stream is not block stream: {}",out.getWrappedStream());      streamStatistics=null;    }    for (long block=1; block <= blocks; block++) {      out.write(data);      long written=block * uploadBlockSize;      if (block % blocksPer10MB == 0 || written == filesize) {        long percentage=written * 100 / filesize;        double elapsedTime=timer.elapsedTime() / 1.0e9;        double writtenMB=1.0 * written / _1MB;        LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " PUT %d bytes (%d pending) in %d operations (%d active);" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s",percentage,writtenMB,filesizeMB,storageStatistics.getLong(putBytes),gaugeValue(putBytesPending),storageStatistics.getLong(putRequests),gaugeValue(putRequestsActive),elapsedTime,writtenMB / elapsedTime));
  ProgressCallback progress=new ProgressCallback(timer);  try (FSDataOutputStream out=fs.create(fileToCreate,true,uploadBlockSize,progress)){    try {      streamStatistics=getOutputStreamStatistics(out);    } catch (    ClassCastException e) {      LOG.info("Wrapped output stream is not block stream: {}",out.getWrappedStream());      streamStatistics=null;    }    for (long block=1; block <= blocks; block++) {      out.write(data);      long written=block * uploadBlockSize;      if (block % blocksPer10MB == 0 || written == filesize) {        long percentage=written * 100 / filesize;        double elapsedTime=timer.elapsedTime() / 1.0e9;        double writtenMB=1.0 * written / _1MB;        LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " PUT %d bytes (%d pending) in %d operations (%d active);" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s",percentage,writtenMB,filesizeMB,storageStatistics.getLong(putBytes),gaugeValue(putBytesPending),storageStatistics.getLong(putRequests),gaugeValue(putRequestsActive),elapsedTime,writtenMB / elapsedTime));
      streamStatistics=null;    }    for (long block=1; block <= blocks; block++) {      out.write(data);      long written=block * uploadBlockSize;      if (block % blocksPer10MB == 0 || written == filesize) {        long percentage=written * 100 / filesize;        double elapsedTime=timer.elapsedTime() / 1.0e9;        double writtenMB=1.0 * written / _1MB;        LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " PUT %d bytes (%d pending) in %d operations (%d active);" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s",percentage,writtenMB,filesizeMB,storageStatistics.getLong(putBytes),gaugeValue(putBytesPending),storageStatistics.getLong(putRequests),gaugeValue(putRequestsActive),elapsedTime,writtenMB / elapsedTime));      }    }    LOG.info("Closing stream {}",out);    LOG.info("Statistics : {}",streamStatistics);    ContractTestUtils.NanoTimer closeTimer=new ContractTestUtils.NanoTimer();    out.close();    closeTimer.end("time to close() output stream");  }   timer.end("time to write %d MB in blocks of %d",filesizeMB,uploadBlockSize);
      long written=block * uploadBlockSize;      if (block % blocksPer10MB == 0 || written == filesize) {        long percentage=written * 100 / filesize;        double elapsedTime=timer.elapsedTime() / 1.0e9;        double writtenMB=1.0 * written / _1MB;        LOG.info(String.format("[%02d%%] Buffered %.2f MB out of %d MB;" + " PUT %d bytes (%d pending) in %d operations (%d active);" + " elapsedTime=%.2fs; write to buffer bandwidth=%.2f MB/s",percentage,writtenMB,filesizeMB,storageStatistics.getLong(putBytes),gaugeValue(putBytesPending),storageStatistics.getLong(putRequests),gaugeValue(putRequestsActive),elapsedTime,writtenMB / elapsedTime));      }    }    LOG.info("Closing stream {}",out);    LOG.info("Statistics : {}",streamStatistics);    ContractTestUtils.NanoTimer closeTimer=new ContractTestUtils.NanoTimer();    out.close();    closeTimer.end("time to close() output stream");  }   timer.end("time to write %d MB in blocks of %d",filesizeMB,uploadBlockSize);  logFSState();  bandwidth(timer,filesize);  LOG.info("Statistics after stream closed: {}",streamStatistics);
@Test public void test_040_PositionedReadHugeFile() throws Throwable {  assumeHugeFileExists();  final String encryption=getConf().getTrimmed(SERVER_SIDE_ENCRYPTION_ALGORITHM);  boolean encrypted=encryption != null;  if (encrypted) {
  long eof=size - 1;  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  ContractTestUtils.NanoTimer readAtByte0, readAtByte0Again, readAtEOF;  try (FSDataInputStream in=fs.open(hugefile,uploadBlockSize)){    readAtByte0=new ContractTestUtils.NanoTimer();    in.readFully(0,buffer);    readAtByte0.end("time to read data at start of file");    ops++;    readAtEOF=new ContractTestUtils.NanoTimer();    in.readFully(eof - bufferSize,buffer);    readAtEOF.end("time to read data at end of file");    ops++;    readAtByte0Again=new ContractTestUtils.NanoTimer();    in.readFully(0,buffer);    readAtByte0Again.end("time to read data at start of file again");
    readAtByte0=new ContractTestUtils.NanoTimer();    in.readFully(0,buffer);    readAtByte0.end("time to read data at start of file");    ops++;    readAtEOF=new ContractTestUtils.NanoTimer();    in.readFully(eof - bufferSize,buffer);    readAtEOF.end("time to read data at end of file");    ops++;    readAtByte0Again=new ContractTestUtils.NanoTimer();    in.readFully(0,buffer);    readAtByte0Again.end("time to read data at start of file again");    ops++;    LOG.info("Final stream state: {}",in);  }   long mb=Math.max(size / _1MB,1);  logFSState();
@Test public void test_050_readHugeFile() throws Throwable {  assumeHugeFileExists();  describe("Reading %s",hugefile);  S3AFileSystem fs=getFileSystem();  FileStatus status=fs.getFileStatus(hugefile);  long size=status.getLen();  long blocks=size / uploadBlockSize;  byte[] data=new byte[uploadBlockSize];  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  try (FSDataInputStream in=fs.open(hugefile,uploadBlockSize)){    for (long block=0; block < blocks; block++) {      in.readFully(data);    }    LOG.info("Final stream state: {}",in);  }   long mb=Math.max(size / _1MB,1);  timer.end("time to read file of %d MB ",mb);
@Test public void test_100_renameHugeFile() throws Throwable {  assumeHugeFileExists();  describe("renaming %s to %s",hugefile,hugefileRenamed);  S3AFileSystem fs=getFileSystem();  FileStatus status=fs.getFileStatus(hugefile);  long size=status.getLen();  fs.delete(hugefileRenamed,false);  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  fs.rename(hugefile,hugefileRenamed);  long mb=Math.max(size / _1MB,1);  timer.end("time to rename file of %d MB",mb);
  FileStatus status=fs.getFileStatus(hugefile);  long size=status.getLen();  fs.delete(hugefileRenamed,false);  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  fs.rename(hugefile,hugefileRenamed);  long mb=Math.max(size / _1MB,1);  timer.end("time to rename file of %d MB",mb);  LOG.info("Time per MB to rename = {} nS",toHuman(timer.nanosPerOperation(mb)));  bandwidth(timer,size);  logFSState();  FileStatus destFileStatus=fs.getFileStatus(hugefileRenamed);  assertEquals(size,destFileStatus.getLen());  ContractTestUtils.NanoTimer timer2=new ContractTestUtils.NanoTimer();  fs.rename(hugefileRenamed,hugefile);  timer2.end("Renaming back");
@Test public void test_900_dumpStats(){  StringBuilder sb=new StringBuilder();  getFileSystem().getStorageStatistics().forEach(kv -> sb.append(kv.toString()).append("\n"));
@Test public void test_020_DeleteThrottling() throws Throwable {  describe("test how S3 reacts to massive multipart deletion requests");  final File results=deleteFiles(requests,pageSize);
  File csvFile=new File(dataDir,String.format("delete-%03d-%04d-%s.csv",requestCount,entries,throttle));  describe("Issuing %d requests of size %d, saving log to %s",requestCount,entries,csvFile);  Path basePath=path("testDeleteObjectThrottling");  final S3AFileSystem fs=getFileSystem();  final String base=fs.pathToKey(basePath);  final List<DeleteObjectsRequest.KeyVersion> fileList=buildDeleteRequest(base,entries);  final FileWriter out=new FileWriter(csvFile);  Csvout csvout=new Csvout(out,"\t","\n");  Outcome.writeSchema(csvout);  final ContractTestUtils.NanoTimer jobTimer=new ContractTestUtils.NanoTimer();  for (int i=0; i < requestCount; i++) {    final int id=i;    completionService.submit(() -> {      final long startTime=System.currentTimeMillis();      Thread.currentThread().setName("#" + id);
 catch (      IOException e) {        ex=e;      }      timer.end("Request " + id);      return new Outcome(id,startTime,timer,ex);    });  }  NanoTimerStats stats=new NanoTimerStats("Overall");  NanoTimerStats success=new NanoTimerStats("Successful");  NanoTimerStats throttled=new NanoTimerStats("Throttled");  List<Outcome> throttledEvents=new ArrayList<>();  for (int i=0; i < requestCount; i++) {    Outcome outcome=completionService.take().get();    ContractTestUtils.NanoTimer timer=outcome.timer;    Exception ex=outcome.exception;    outcome.writeln(csvout);    stats.add(timer);
  NanoTimerStats stats=new NanoTimerStats("Overall");  NanoTimerStats success=new NanoTimerStats("Successful");  NanoTimerStats throttled=new NanoTimerStats("Throttled");  List<Outcome> throttledEvents=new ArrayList<>();  for (int i=0; i < requestCount; i++) {    Outcome outcome=completionService.take().get();    ContractTestUtils.NanoTimer timer=outcome.timer;    Exception ex=outcome.exception;    outcome.writeln(csvout);    stats.add(timer);    if (ex != null) {      LOG.info("Throttled at event {}",i,ex);      throttled.add(timer);      throttledEvents.add(outcome);    } else {
    ContractTestUtils.NanoTimer timer=outcome.timer;    Exception ex=outcome.exception;    outcome.writeln(csvout);    stats.add(timer);    if (ex != null) {      LOG.info("Throttled at event {}",i,ex);      throttled.add(timer);      throttledEvents.add(outcome);    } else {      success.add(timer);    }  }  csvout.close();  jobTimer.end("Execution of operations");  LOG.info("Summary file is " + csvFile);  LOG.info("Made {} requests with {} throttle events\n: {}\n{}\n{}",requestCount,throttled.getCount(),stats,throttled,success);  double duration=jobTimer.duration();
      }    }   }  LOG.info("Data generated...");  ExecutorService executor=Executors.newFixedThreadPool(concurrentRenames,new ThreadFactory(){    private AtomicInteger count=new AtomicInteger(0);    public Thread newThread(    Runnable r){      return new Thread(r,"testParallelRename" + count.getAndIncrement());    }  });  try {    ((ThreadPoolExecutor)executor).prestartAllCoreThreads();    Future<Boolean>[] futures=new Future[concurrentRenames];    for (int i=0; i < concurrentRenames; i++) {      final int index=i;      futures[i]=executor.submit(new Callable<Boolean>(){        @Override public Boolean call() throws Exception {          NanoTimer timer=new NanoTimer();          boolean result=fs.rename(source[index],target[index]);
  final Path finalParentDir=new Path(scaleTestDir,"finalParent");  final Path finalDir=new Path(finalParentDir,"final");  final S3AFileSystem fs=getFileSystem();  rm(fs,scaleTestDir,true,false);  fs.mkdirs(srcDir);  fs.mkdirs(finalParentDir);  createFiles(fs,srcDir,1,count,0);  FileStatus[] statuses=fs.listStatus(srcDir);  int nSrcFiles=statuses.length;  long sourceSize=Arrays.stream(statuses).mapToLong(FileStatus::getLen).sum();  assertEquals("Source file Count",count,nSrcFiles);  ContractTestUtils.NanoTimer renameTimer=new ContractTestUtils.NanoTimer();  try (DurationInfo ignored=new DurationInfo(LOG,"Rename %s to %s",srcDir,finalDir)){    rename(srcDir,finalDir);  }   renameTimer.end();
   renameTimer.end();  LOG.info("Effective rename bandwidth {} MB/s",renameTimer.bandwidthDescription(sourceSize));  LOG.info(String.format("Time to rename a file: %,03f milliseconds",(renameTimer.nanosPerOperation(count) * 1.0f) / 1.0e6));  Assertions.assertThat(lsR(fs,srcParentDir,true)).describedAs("Recursive listing of source dir %s",srcParentDir).isEqualTo(0);  assertPathDoesNotExist("not deleted after rename",new Path(srcDir,filenameOfIndex(0)));  assertPathDoesNotExist("not deleted after rename",new Path(srcDir,filenameOfIndex(count / 2)));  assertPathDoesNotExist("not deleted after rename",new Path(srcDir,filenameOfIndex(count - 1)));  Assertions.assertThat(lsR(fs,finalDir,true)).describedAs("size of recursive destination listFiles(%s)",finalDir).isEqualTo(count);  Assertions.assertThat(fs.listStatus(finalDir)).describedAs("size of destination listStatus(%s)",finalDir).hasSize(count);  assertPathExists("not renamed to dest dir",new Path(finalDir,filenameOfIndex(0)));  assertPathExists("not renamed to dest dir",new Path(finalDir,filenameOfIndex(count / 2)));  assertPathExists("not renamed to dest dir",new Path(finalDir,filenameOfIndex(count - 1)));  ContractTestUtils.NanoTimer deleteTimer=new ContractTestUtils.NanoTimer();  try (DurationInfo ignored=new DurationInfo(LOG,"Delete subtree %s",finalDir)){    assertDeleted(finalDir,true);
  int scale=getConf().getInt(KEY_DIRECTORY_COUNT,DEFAULT_DIRECTORY_COUNT);  int width=scale;  int depth=scale;  int files=scale;  MetricDiff metadataRequests=new MetricDiff(fs,OBJECT_METADATA_REQUESTS);  MetricDiff listRequests=new MetricDiff(fs,OBJECT_LIST_REQUESTS);  MetricDiff listContinueRequests=new MetricDiff(fs,OBJECT_CONTINUE_LIST_REQUESTS);  MetricDiff listStatusCalls=new MetricDiff(fs,INVOCATION_LIST_FILES);  MetricDiff getFileStatusCalls=new MetricDiff(fs,INVOCATION_GET_FILE_STATUS);  NanoTimer createTimer=new NanoTimer();  TreeScanResults created=createSubdirs(fs,listDir,depth,width,files,0);  int emptyDepth=1 * scale;  int emptyWidth=3 * scale;  created.add(createSubdirs(fs,listDir,emptyDepth,emptyWidth,0,0,"empty","f-",""));  createTimer.end("Time to create %s",created);
protected void logTimePerIOP(String operation,NanoTimer timer,long count){
    int remaining=blockSize;    long blockId=i + 1;    NanoTimer blockTimer=new NanoTimer();    int reads=0;    while (remaining > 0) {      NanoTimer readTimer=new NanoTimer();      int bytesRead=in.read(block,offset,remaining);      reads++;      if (bytesRead == 1) {        break;      }      remaining-=bytesRead;      offset+=bytesRead;      count+=bytesRead;      readTimer.end();      if (bytesRead != 0) {
      NanoTimer readTimer=new NanoTimer();      int bytesRead=in.read(block,offset,remaining);      reads++;      if (bytesRead == 1) {        break;      }      remaining-=bytesRead;      offset+=bytesRead;      count+=bytesRead;      readTimer.end();      if (bytesRead != 0) {        LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s",reads,bytesRead,blockSize - remaining,remaining,readTimer.duration(),readTimer.nanosPerOperation(bytesRead),readTimer.bandwidthDescription(bytesRead));      } else {        LOG.warn("0 bytes returned by read() operation #{}",reads);      }    }    blockTimer.end("Reading block %d in %d reads",blockId,reads);    String bw=blockTimer.bandwidthDescription(blockSize);
      offset+=bytesRead;      count+=bytesRead;      readTimer.end();      if (bytesRead != 0) {        LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s",reads,bytesRead,blockSize - remaining,remaining,readTimer.duration(),readTimer.nanosPerOperation(bytesRead),readTimer.bandwidthDescription(bytesRead));      } else {        LOG.warn("0 bytes returned by read() operation #{}",reads);      }    }    blockTimer.end("Reading block %d in %d reads",blockId,reads);    String bw=blockTimer.bandwidthDescription(blockSize);    LOG.info("Bandwidth of block {}: {} MB/s: ",blockId,bw);    if (bandwidth(blockTimer,blockSize) < minimumBandwidth) {      LOG.warn("Bandwidth {} too low on block {}: resetting connection",bw,blockId);      Assert.assertTrue("Bandwidth of " + bw + " too low after  "+ resetCount+ " attempts",resetCount <= maxResetCount);      resetCount++;      getS3AInputStream(in).resetConnection();
protected void executeSeekReadSequence(long blockSize,long readahead,S3AInputPolicy policy) throws IOException {  in=openTestFile(policy,readahead);  long len=testDataStatus.getLen();  NanoTimer timer=new NanoTimer();  long blockCount=len / blockSize;
  int readOps=0;  while (bytesRead < halfReadahead) {    int length=buffer.length - offset;    int read=in.read(currentPos,buffer,offset,length);    bytesRead+=read;    offset+=read;    readOps++;    assertEquals("open operations on request #" + readOps + " after reading "+ bytesRead+ " current position in stream "+ currentPos+ " in\n"+ fs+ "\n "+ in,1,streamStatistics.openOperations);    for (int i=currentPos; i < currentPos + read; i++) {      assertEquals("Wrong value from byte " + i,sourceData[i],buffer[i]);    }    currentPos+=read;  }  assertStreamOpenedExactlyOnce();  assertEquals(readahead,currentPos);  readTimer.end("read %d in %d operations",bytesRead,readOps);  bandwidth(readTimer,bytesRead);
  while (bytesRead < halfReadahead) {    int length=buffer.length - offset;    int read=in.read(currentPos,buffer,offset,length);    bytesRead+=read;    offset+=read;    readOps++;    assertEquals("open operations on request #" + readOps + " after reading "+ bytesRead+ " current position in stream "+ currentPos+ " in\n"+ fs+ "\n "+ in,1,streamStatistics.openOperations);    for (int i=currentPos; i < currentPos + read; i++) {      assertEquals("Wrong value from byte " + i,sourceData[i],buffer[i]);    }    currentPos+=read;  }  assertStreamOpenedExactlyOnce();  assertEquals(readahead,currentPos);  readTimer.end("read %d in %d operations",bytesRead,readOps);  bandwidth(readTimer,bytesRead);  LOG.info("Time per byte(): {} nS",toHuman(readTimer.nanosPerOperation(bytesRead)));
  assertEquals("Wrong value from read ",sourceData[currentPos],(int)buffer[currentPos]);  currentPos++;  describe("read() to EOF over \n%s",in);  long readCount=0;  NanoTimer timer=new NanoTimer();  LOG.info("seeking");  in.seek(currentPos);  LOG.info("reading");  while (currentPos < datasetLen) {    int r=in.read();    assertTrue("Negative read() at position " + currentPos + " in\n"+ in,r >= 0);    buffer[currentPos]=(byte)r;    assertEquals("Wrong value from read from\n" + in,sourceData[currentPos],r);    currentPos++;    readCount++;
protected List<String> parseToLines(final FSDataInputStream selection,int maxLines) throws IOException {  List<String> result=new ArrayList<>();  String stats;  try (Scanner scanner=new Scanner(new BufferedReader(new InputStreamReader(selection)))){    scanner.useDelimiter(CSV_INPUT_RECORD_DELIMITER_DEFAULT);    while (maxLines > 0) {      try {        String l=scanner.nextLine();
protected List<String> verifySelectionCount(final int min,final int max,final String expression,final List<String> selection){  int size=selection.size();  if (size < min || (max > -1 && size > max)) {    String listing=prepareToPrint(selection);
protected static <T extends Exception>T logIntercepted(T ex){
@SuppressWarnings("NestedAssignment") @Test public void testReadWholeFileClassicAPI() throws Throwable {  describe("create and read the whole file. Verifies setup working");  int lines;  try (BufferedReader reader=new BufferedReader(new InputStreamReader(getFileSystem().open(csvPath)))){    lines=0;    String line;    while ((line=reader.readLine()) != null) {      lines++;
  S3AFileSystem fs=getFileSystem();  int len=(int)fs.getFileStatus(path).getLen();  byte[] fullData=new byte[len];  int actualLen;  try (DurationInfo ignored=new DurationInfo(LOG,"Initial read of %s",path);FSDataInputStream sourceStream=select(fs,path,selectConf,SELECT_EVERYTHING)){    actualLen=IOUtils.read(sourceStream,fullData);  }   int seekRange=20;  try (FSDataInputStream seekStream=select(fs,path,selectConf,SELECT_EVERYTHING)){    SelectInputStream sis=(SelectInputStream)seekStream.getWrappedStream();    S3AInstrumentation.InputStreamStatistics streamStats=sis.getS3AStreamStatistics();    seekStream.seek(0);    assertEquals("first byte read",fullData[0],seekStream.read());    seekStream.seek(1);    seekStream.seek(1);    PathIOException ex=intercept(PathIOException.class,SelectInputStream.SEEK_UNSUPPORTED,() -> seekStream.seek(0));
    intercept(PathIOException.class,SelectInputStream.SEEK_UNSUPPORTED,() -> seekStream.readFully(0,buffer));    assertPosition(seekStream,pos + 1);    intercept(PathIOException.class,SelectInputStream.SEEK_UNSUPPORTED,() -> seekStream.readFully(pos,buffer));    seekStream.setReadahead(null);    assertEquals("Readahead in ",Constants.DEFAULT_READAHEAD_RANGE,sis.getReadahead());    long target=seekStream.getPos() + seekRange;    seek(seekStream,target);    assertPosition(seekStream,target);    assertEquals("byte at seek position",fullData[(int)seekStream.getPos()],seekStream.read());    assertEquals("Seek bytes skipped in " + streamStats,seekRange,streamStats.bytesSkippedOnSeek);    intercept(IllegalArgumentException.class,S3AInputStream.E_NEGATIVE_READAHEAD_VALUE,() -> seekStream.setReadahead(-1L));    int read=seekStream.read(seekStream.getPos() + 2,buffer,0,1);    assertEquals(1,read);    logIntercepted(expectSeekEOF(seekStream,actualLen * 2));    assertPosition(seekStream,actualLen);
    assertEquals("Readahead in ",Constants.DEFAULT_READAHEAD_RANGE,sis.getReadahead());    long target=seekStream.getPos() + seekRange;    seek(seekStream,target);    assertPosition(seekStream,target);    assertEquals("byte at seek position",fullData[(int)seekStream.getPos()],seekStream.read());    assertEquals("Seek bytes skipped in " + streamStats,seekRange,streamStats.bytesSkippedOnSeek);    intercept(IllegalArgumentException.class,S3AInputStream.E_NEGATIVE_READAHEAD_VALUE,() -> seekStream.setReadahead(-1L));    int read=seekStream.read(seekStream.getPos() + 2,buffer,0,1);    assertEquals(1,read);    logIntercepted(expectSeekEOF(seekStream,actualLen * 2));    assertPosition(seekStream,actualLen);    assertEquals(-1,seekStream.read());    LOG.info("Seek statistics {}",streamStats);    assertFalse("Failed to seek to new source in " + seekStream,seekStream.seekToNewSource(0));    seekStream.setReadahead(0L);
@Test public void testSelectFileExample() throws Throwable {  describe("Select the entire file, expect all rows but the header");  int len=(int)getFileSystem().getFileStatus(csvPath).getLen();  FutureDataInputStreamBuilder builder=getFileSystem().openFile(csvPath).must("fs.s3a.select.sql",SELECT_ODD_ENTRIES).must("fs.s3a.select.input.format","CSV").must("fs.s3a.select.input.compression","NONE").must("fs.s3a.select.input.csv.header","use").must("fs.s3a.select.output.format","CSV");  CompletableFuture<FSDataInputStream> future=builder.build();  try (FSDataInputStream select=future.get()){    byte[] bytes=new byte[len];    int actual=select.read(bytes);
@Test public void testSelectDatestampsConverted() throws Throwable {  describe("timestamp conversion in record IIO");  JobConf conf=createJobConf();  inputMust(conf,CSV_INPUT_HEADER,CSV_HEADER_OPT_USE);  inputMust(conf,CSV_OUTPUT_QUOTE_FIELDS,CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);  String sql=SELECT_TO_DATE;  List<String> records=expectRecordsRead(ALL_ROWS_COUNT,conf,sql);
@Test public void testCommentsSkipped() throws Throwable {  describe("Verify that comments are skipped");  selectConf.set(CSV_INPUT_HEADER,CSV_HEADER_OPT_USE);  List<String> lines=verifySelectionCount(ALL_ROWS_COUNT_WITH_HEADER,"select s.id",parseToLines(select(getFileSystem(),brokenCSV,selectConf,"SELECT * FROM S3OBJECT s")));
@Test public void testEmptyColumnsRegenerated() throws Throwable {  describe("if you ask for a column but your row doesn't have it," + " an empty column is inserted");  selectConf.set(CSV_INPUT_HEADER,CSV_HEADER_OPT_USE);  List<String> lines=verifySelectionCount(ALL_ROWS_COUNT_WITH_HEADER,"select s.oddrange",parseToLines(select(getFileSystem(),brokenCSV,selectConf,"SELECT s.oddrange FROM S3OBJECT s")));
@Test public void testLandsatToFile() throws Throwable {  describe("select part of the landsat to a file");  int lineCount=LINE_COUNT;  S3AFileSystem landsatFS=(S3AFileSystem)getLandsatGZ().getFileSystem(getConfiguration());  S3ATestUtils.MetricDiff selectCount=new S3ATestUtils.MetricDiff(landsatFS,Statistic.OBJECT_SELECT_REQUESTS);  run(selectConf,selectTool,D,v(CSV_OUTPUT_QUOTE_CHARACTER,"'"),D,v(CSV_OUTPUT_QUOTE_FIELDS,CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED),"select",o(OPT_HEADER),CSV_HEADER_OPT_USE,o(OPT_COMPRESSION),COMPRESSION_OPT_GZIP,o(OPT_LIMIT),Integer.toString(lineCount),o(OPT_OUTPUT),localFile.toString(),landsatSrc,SELECT_SUNNY_ROWS_NO_LIMIT);  List<String> lines=IOUtils.readLines(new FileInputStream(localFile),Charset.defaultCharset());
@Test public void testSelectCloudcoverIgnoreHeader() throws Throwable {  describe("select ignoring the header");  selectConf.set(CSV_INPUT_HEADER,CSV_HEADER_OPT_IGNORE);  String sql="SELECT\n" + "* from\n" + "S3OBJECT s WHERE\n"+ "s._3 = '0.0'\n"+ LIMITED;  List<String> list=selectLandsatFile(selectConf,sql);
@Test public void testSelectCloudcoverUseHeader() throws Throwable {  describe("select 100% cover using the header, " + "+ verify projection and incrementing select statistics");  S3ATestUtils.MetricDiff selectCount=new S3ATestUtils.MetricDiff(getLandsatFS(),Statistic.OBJECT_SELECT_REQUESTS);  List<String> list=selectLandsatFile(selectConf,SELECT_ENTITY_ID_ALL_CLOUDS);
@Test public void testFileContextIntegration() throws Throwable {  describe("Test that select works through FileContext");  FileContext fc=S3ATestUtils.createTestFileContext(getConfiguration());  List<String> list=parseToLines(select(fc,getLandsatGZ(),selectConf,SELECT_ENTITY_ID_ALL_CLOUDS),SELECT_LIMIT * 2);
    intercept(PathIOException.class,SelectInputStream.SEEK_UNSUPPORTED,() -> seekStream.readFully(0,buffer));    long target=seekStream.getPos() + seekRange;    seek(seekStream,target);    assertEquals("Seek position in " + seekStream,target,seekStream.getPos());    assertEquals("byte at seek position",dataset[(int)seekStream.getPos()],seekStream.read());    assertEquals("Seek bytes skipped in " + streamStats,seekRange,streamStats.bytesSkippedOnSeek);    long offset;    long increment=64 * _1KB;    for (offset=32 * _1KB; offset < actualLen; offset+=increment) {      seek(seekStream,offset);      assertEquals("Seek position in " + seekStream,offset,seekStream.getPos());      assertEquals("byte at seek position",dataset[(int)seekStream.getPos()],seekStream.read());    }    for (; offset < len; offset+=_1MB) {      seek(seekStream,offset);      assertEquals("Seek position in " + seekStream,offset,seekStream.getPos());
  jobConf.set(FS_S3A_COMMITTER_NAME,StagingCommitter.NAME);  jobConf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES,false);  final String query=ITestS3SelectLandsat.SELECT_PROCESSING_LEVEL_NO_LIMIT;  inputMust(jobConf,SELECT_SQL,query);  inputMust(jobConf,SELECT_INPUT_COMPRESSION,COMPRESSION_OPT_GZIP);  inputMust(jobConf,SELECT_INPUT_FORMAT,SELECT_FORMAT_CSV);  inputMust(jobConf,CSV_INPUT_HEADER,CSV_HEADER_OPT_USE);  inputMust(jobConf,SELECT_OUTPUT_FORMAT,SELECT_FORMAT_CSV);  inputMust(jobConf,CSV_OUTPUT_QUOTE_FIELDS,CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);  enablePassthroughCodec(jobConf,".gz");  try (DurationInfo ignored=new DurationInfo(LOG,"SQL " + query)){    int exitCode=job.waitForCompletion(true) ? 0 : 1;    assertEquals("Returned error code.",0,exitCode);  }   Path successPath=new Path(output,"_SUCCESS");  SuccessData success=SuccessData.load(fs,successPath);
  jobConf.setBoolean(FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES,false);  final String query=ITestS3SelectLandsat.SELECT_PROCESSING_LEVEL_NO_LIMIT;  inputMust(jobConf,SELECT_SQL,query);  inputMust(jobConf,SELECT_INPUT_COMPRESSION,COMPRESSION_OPT_GZIP);  inputMust(jobConf,SELECT_INPUT_FORMAT,SELECT_FORMAT_CSV);  inputMust(jobConf,CSV_INPUT_HEADER,CSV_HEADER_OPT_USE);  inputMust(jobConf,SELECT_OUTPUT_FORMAT,SELECT_FORMAT_CSV);  inputMust(jobConf,CSV_OUTPUT_QUOTE_FIELDS,CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);  enablePassthroughCodec(jobConf,".gz");  try (DurationInfo ignored=new DurationInfo(LOG,"SQL " + query)){    int exitCode=job.waitForCompletion(true) ? 0 : 1;    assertEquals("Returned error code.",0,exitCode);  }   Path successPath=new Path(output,"_SUCCESS");  SuccessData success=SuccessData.load(fs,successPath);  LOG.info("Job _SUCCESS\n{}",success);
  inputMust(jobConf,CSV_OUTPUT_QUOTE_FIELDS,CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED);  enablePassthroughCodec(jobConf,".gz");  try (DurationInfo ignored=new DurationInfo(LOG,"SQL " + query)){    int exitCode=job.waitForCompletion(true) ? 0 : 1;    assertEquals("Returned error code.",0,exitCode);  }   Path successPath=new Path(output,"_SUCCESS");  SuccessData success=SuccessData.load(fs,successPath);  LOG.info("Job _SUCCESS\n{}",success);  LOG.info("Results for query \n{}",query);  final AtomicLong parts=new AtomicLong(0);  S3AUtils.applyLocatedFiles(fs.listFiles(output,false),(status) -> {    Path path=status.getPath();    if (path.getName().startsWith("part-")) {      parts.incrementAndGet();      String result=readStringFromFile(path);
protected List<String> readOutput(final File outputFile) throws IOException {  try (FileReader reader=new FileReader(outputFile)){    final List<String> lines=org.apache.commons.io.IOUtils.readLines(reader);
  job.setJarByClass(WordCount.class);  job.setMapperClass(WordCount.TokenizerMapper.class);  job.setCombinerClass(WordCount.IntSumReducer.class);  job.setReducerClass(WordCount.IntSumReducer.class);  job.setOutputKeyClass(Text.class);  job.setOutputValueClass(IntWritable.class);  FileInputFormat.addInputPath(job,input);  FileOutputFormat.setOutputPath(job,output);  int exitCode=(job.waitForCompletion(true) ? 0 : 1);  assertEquals("Returned error code.",0,exitCode);  Path success=new Path(output,_SUCCESS);  FileStatus status=fs.getFileStatus(success);  assertTrue("0 byte success file - not a s3guard committer " + success,status.getLen() > 0);  SuccessData successData=SuccessData.load(fs,success);  String commitDetails=successData.toString();
public void init() throws IOException, InterruptedException {  when(mockClient.submitJob(any(JobID.class),any(String.class),any(Credentials.class))).thenAnswer(invocation -> {    final Object[] args=invocation.getArguments();    String name=(String)args[1];
public static Configuration propagateAccountOptions(Configuration source,String accountName){  Preconditions.checkArgument(StringUtils.isNotEmpty(accountName),"accountName");  final String accountPrefix=AZURE_AD_ACCOUNT_PREFIX + accountName + '.';
  boolean operationStatus=false;  boolean threadsEnabled=false;  int threadCount=this.threadCount;  ThreadPoolExecutor ioThreadPool=null;  long start=Time.monotonicNow();  threadCount=Math.min(contents.length,threadCount);  if (threadCount > 1) {    try {      ioThreadPool=getThreadPool(threadCount);      threadsEnabled=true;    } catch (    Exception e) {      LOG.warn("Failed to create thread pool with threads {} for operation {} on blob {}." + " Use config {} to set less number of threads. Setting config value to <= 1 will disable threads.",threadCount,operation,key,config);    }  } else {    LOG.warn("Disabling threads for {} operation as thread count {} is <= 1",operation,threadCount);  }  if (threadsEnabled) {
      ioThreadPool=getThreadPool(threadCount);      threadsEnabled=true;    } catch (    Exception e) {      LOG.warn("Failed to create thread pool with threads {} for operation {} on blob {}." + " Use config {} to set less number of threads. Setting config value to <= 1 will disable threads.",threadCount,operation,key,config);    }  } else {    LOG.warn("Disabling threads for {} operation as thread count {} is <= 1",operation,threadCount);  }  if (threadsEnabled) {    LOG.debug("Using thread pool for {} operation with threads {}",operation,threadCount);    boolean started=false;    AzureFileSystemThreadRunnable runnable=new AzureFileSystemThreadRunnable(contents,threadOperation,operation);    for (int i=0; i < threadCount && runnable.lastException == null && runnable.operationStatus; i++) {      try {        ioThreadPool.execute(runnable);        started=true;      } catch (      RejectedExecutionException ex) {
 else {    LOG.warn("Disabling threads for {} operation as thread count {} is <= 1",operation,threadCount);  }  if (threadsEnabled) {    LOG.debug("Using thread pool for {} operation with threads {}",operation,threadCount);    boolean started=false;    AzureFileSystemThreadRunnable runnable=new AzureFileSystemThreadRunnable(contents,threadOperation,operation);    for (int i=0; i < threadCount && runnable.lastException == null && runnable.operationStatus; i++) {      try {        ioThreadPool.execute(runnable);        started=true;      } catch (      RejectedExecutionException ex) {        LOG.error("Rejected execution of thread for {} operation on blob {}." + " Continuing with existing threads. Use config {} to set less number of threads" + " to avoid this error",operation,key,config);      }    }    ioThreadPool.shutdown();    try {      ioThreadPool.awaitTermination(Long.MAX_VALUE,TimeUnit.DAYS);
    AzureFileSystemThreadRunnable runnable=new AzureFileSystemThreadRunnable(contents,threadOperation,operation);    for (int i=0; i < threadCount && runnable.lastException == null && runnable.operationStatus; i++) {      try {        ioThreadPool.execute(runnable);        started=true;      } catch (      RejectedExecutionException ex) {        LOG.error("Rejected execution of thread for {} operation on blob {}." + " Continuing with existing threads. Use config {} to set less number of threads" + " to avoid this error",operation,key,config);      }    }    ioThreadPool.shutdown();    try {      ioThreadPool.awaitTermination(Long.MAX_VALUE,TimeUnit.DAYS);    } catch (    InterruptedException intrEx) {      ioThreadPool.shutdownNow();      Thread.currentThread().interrupt();      LOG.error("Threads got interrupted {} blob operation for {} ",operation,key);    }    int threadsNotUsed=threadCount - runnable.threadsUsed.get();
        started=true;      } catch (      RejectedExecutionException ex) {        LOG.error("Rejected execution of thread for {} operation on blob {}." + " Continuing with existing threads. Use config {} to set less number of threads" + " to avoid this error",operation,key,config);      }    }    ioThreadPool.shutdown();    try {      ioThreadPool.awaitTermination(Long.MAX_VALUE,TimeUnit.DAYS);    } catch (    InterruptedException intrEx) {      ioThreadPool.shutdownNow();      Thread.currentThread().interrupt();      LOG.error("Threads got interrupted {} blob operation for {} ",operation,key);    }    int threadsNotUsed=threadCount - runnable.threadsUsed.get();    if (threadsNotUsed > 0) {      LOG.warn("{} threads not used for {} operation on blob {}",threadsNotUsed,operation,key);    }    if (!started) {      threadsEnabled=false;
    ioThreadPool.shutdown();    try {      ioThreadPool.awaitTermination(Long.MAX_VALUE,TimeUnit.DAYS);    } catch (    InterruptedException intrEx) {      ioThreadPool.shutdownNow();      Thread.currentThread().interrupt();      LOG.error("Threads got interrupted {} blob operation for {} ",operation,key);    }    int threadsNotUsed=threadCount - runnable.threadsUsed.get();    if (threadsNotUsed > 0) {      LOG.warn("{} threads not used for {} operation on blob {}",threadsNotUsed,operation,key);    }    if (!started) {      threadsEnabled=false;      LOG.info("Not able to schedule threads to {} blob {}. Fall back to {} blob serially.",operation,key,operation);    } else {      IOException lastException=runnable.lastException;
      LOG.error("Threads got interrupted {} blob operation for {} ",operation,key);    }    int threadsNotUsed=threadCount - runnable.threadsUsed.get();    if (threadsNotUsed > 0) {      LOG.warn("{} threads not used for {} operation on blob {}",threadsNotUsed,operation,key);    }    if (!started) {      threadsEnabled=false;      LOG.info("Not able to schedule threads to {} blob {}. Fall back to {} blob serially.",operation,key,operation);    } else {      IOException lastException=runnable.lastException;      if (lastException == null && runnable.operationStatus && runnable.filesProcessed.get() < contents.length) {        LOG.error("{} failed as operation on subfolders and files failed.",operation);        lastException=new IOException(operation + " failed as operation on subfolders and files failed.");      }      if (lastException != null) {        throw lastException;      }      operationStatus=runnable.operationStatus;
      this.storageInteractionLayer=new StorageInterfaceImpl();    } else {      this.storageInteractionLayer=new SecureStorageInterfaceImpl(useLocalSasKeyMode,conf);    }  }  configureAzureStorageSession();  createAzureStorageSession();  pageBlobDirs=getDirectorySet(KEY_PAGE_BLOB_DIRECTORIES);  LOG.debug("Page blob directories:  {}",setToString(pageBlobDirs));  userAgentId=conf.get(USER_AGENT_ID_KEY,USER_AGENT_ID_DEFAULT);  blockBlobWithCompationDirs=getDirectorySet(KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES);  LOG.debug("Block blobs with compaction directories:  {}",setToString(blockBlobWithCompationDirs));  atomicRenameDirs=getDirectorySet(KEY_ATOMIC_RENAME_DIRECTORIES);  String hbaseRoot;  try {    hbaseRoot=verifyAndConvertToStandardFormat(sessionConfiguration.get("hbase.rootdir","hbase"));    if (hbaseRoot != null) {
  }  int cpuCores=2 * Runtime.getRuntime().availableProcessors();  concurrentWrites=sessionConfiguration.getInt(KEY_CONCURRENT_CONNECTION_VALUE_OUT,Math.min(cpuCores,DEFAULT_CONCURRENT_WRITES));  minBackoff=sessionConfiguration.getInt(KEY_MIN_BACKOFF_INTERVAL,DEFAULT_MIN_BACKOFF_INTERVAL);  maxBackoff=sessionConfiguration.getInt(KEY_MAX_BACKOFF_INTERVAL,DEFAULT_MAX_BACKOFF_INTERVAL);  deltaBackoff=sessionConfiguration.getInt(KEY_BACKOFF_INTERVAL,DEFAULT_BACKOFF_INTERVAL);  maxRetries=sessionConfiguration.getInt(KEY_MAX_IO_RETRIES,DEFAULT_MAX_RETRY_ATTEMPTS);  storageInteractionLayer.setRetryPolicyFactory(new RetryExponentialRetry(minBackoff,deltaBackoff,maxBackoff,maxRetries));  selfThrottlingEnabled=sessionConfiguration.getBoolean(KEY_SELF_THROTTLE_ENABLE,DEFAULT_SELF_THROTTLE_ENABLE);  selfThrottlingReadFactor=sessionConfiguration.getFloat(KEY_SELF_THROTTLE_READ_FACTOR,DEFAULT_SELF_THROTTLE_READ_FACTOR);  selfThrottlingWriteFactor=sessionConfiguration.getFloat(KEY_SELF_THROTTLE_WRITE_FACTOR,DEFAULT_SELF_THROTTLE_WRITE_FACTOR);  if (!selfThrottlingEnabled) {    autoThrottlingEnabled=sessionConfiguration.getBoolean(KEY_AUTO_THROTTLE_ENABLE,DEFAULT_AUTO_THROTTLE_ENABLE);    if (autoThrottlingEnabled) {      ClientThrottlingIntercept.initializeSingleton();    }  } else {
    }    instrumentation.setAccountName(accountName);    String containerName=getContainerFromAuthority(sessionUri);    instrumentation.setContainerName(containerName);    if (isStorageEmulatorAccount(accountName)) {      connectUsingCredentials(accountName,null,containerName);      return;    }    if (useSecureMode) {      connectToAzureStorageInSecureMode(accountName,containerName,sessionUri);      return;    }    String propertyValue=sessionConfiguration.get(KEY_ACCOUNT_SAS_PREFIX + containerName + "."+ accountName);    if (propertyValue != null) {      connectUsingSASCredentials(accountName,containerName,propertyValue);      return;    }    propertyValue=getAccountKeyFromConfiguration(accountName,sessionConfiguration);    if (StringUtils.isNotEmpty(propertyValue)) {
    final String errMsg=String.format("Storage session expected for URI '%s' but does not exist.",sessionUri);    throw new AssertionError(errMsg);  }  LOG.debug("Retrieving metadata for {}",key);  try {    if (checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist) {      return null;    }    if (key.equals("/")) {      return new FileMetadata(key,0,defaultPermissionNoBlobMetadata(),BlobMaterialization.Implicit,hadoopBlockSize);    }    CloudBlobWrapper blob=getBlobReference(key);    if (null != blob && blob.exists(getInstrumentedContext())) {      LOG.debug("Found {} as an explicit blob. Checking if it's a file or folder.",key);      try {        blob.downloadAttributes(getInstrumentedContext());        BlobProperties properties=blob.getProperties();        if (retrieveFolderAttribute(blob)) {
  LOG.debug("Retrieving metadata for {}",key);  try {    if (checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist) {      return null;    }    if (key.equals("/")) {      return new FileMetadata(key,0,defaultPermissionNoBlobMetadata(),BlobMaterialization.Implicit,hadoopBlockSize);    }    CloudBlobWrapper blob=getBlobReference(key);    if (null != blob && blob.exists(getInstrumentedContext())) {      LOG.debug("Found {} as an explicit blob. Checking if it's a file or folder.",key);      try {        blob.downloadAttributes(getInstrumentedContext());        BlobProperties properties=blob.getProperties();        if (retrieveFolderAttribute(blob)) {          LOG.debug("{} is a folder blob.",key);          return new FileMetadata(key,properties.getLastModified().getTime(),getPermissionStatus(blob),BlobMaterialization.Explicit,hadoopBlockSize);
    Throwable t=e.getCause();    if (t instanceof StorageException) {      StorageException se=(StorageException)t;      if ("LeaseIdMissing".equals(se.getErrorCode())) {        SelfRenewingLease lease=null;        try {          lease=acquireLease(key);          return delete(key,lease);        } catch (        AzureException e3) {          LOG.warn("Got unexpected exception trying to acquire lease on " + key + "."+ e3.getMessage());          throw e3;        } finally {          try {            if (lease != null) {              lease.free();
@Override public void rename(String srcKey,String dstKey,boolean acquireLease,SelfRenewingLease existingLease,boolean overwriteDestination) throws IOException {
@Override public SelfRenewingLease acquireLease(String key) throws AzureException {
  if (closed) {    return;  }  flush();  ioThreadPool.shutdown();  try {    if (!ioThreadPool.awaitTermination(CLOSE_UPLOAD_DELAY,TimeUnit.MINUTES)) {      LOG.error("Time out occurred while close() is waiting for IO request to" + " finish in append" + " for blob : {}",key);      NativeAzureFileSystemHelper.logAllLiveStackTraces();      throw new AzureException("Timed out waiting for IO requests to finish");    }  } catch (  InterruptedException ex) {    Thread.currentThread().interrupt();  }  if (firstError.get() == null && blobExist) {    try {      lease.free();    } catch (    StorageException ex) {
private void writeBlockRequestInternal(String blockId,ByteBuffer dataPayload,boolean bufferPoolBuffer){  IOException lastLocalException=null;  int uploadRetryAttempts=0;  while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {    try {      long startTime=System.nanoTime();      blob.uploadBlock(blockId,accessCondition,new ByteArrayInputStream(dataPayload.array()),dataPayload.position(),new BlobRequestOptions(),opContext);
private void writeBlockListRequestInternal(){  IOException lastLocalException=null;  int uploadRetryAttempts=0;  while (uploadRetryAttempts < MAX_BLOCK_UPLOAD_RETRIES) {    try {      long startTime=System.nanoTime();      blob.commitBlockList(blockEntries,accessCondition,new BlobRequestOptions(),opContext);
public void put(K key,V value){  if (isEnabled) {
    double reductionFactor=(consecutiveNoErrorCount * analysisPeriodMs >= RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) ? RAPID_SLEEP_DECREASE_FACTOR : SLEEP_DECREASE_FACTOR;    newSleepDuration=sleepDuration * reductionFactor;  } else   if (errorPercentage < MAX_EQUILIBRIUM_ERROR_PERCENTAGE) {    newSleepDuration=sleepDuration;  } else {    consecutiveNoErrorCount=0;    double additionalDelayNeeded=5 * analysisPeriodMs;    if (bytesSuccessful > 0) {      additionalDelayNeeded=(bytesSuccessful + bytesFailed) * periodMs / bytesSuccessful - periodMs;    }    newSleepDuration=additionalDelayNeeded / (operationsFailed + operationsSuccessful);    final double maxSleepDuration=analysisPeriodMs;    final double minSleepDuration=sleepDuration * SLEEP_INCREASE_FACTOR;    newSleepDuration=Math.max(newSleepDuration,minSleepDuration) + 1;    newSleepDuration=Math.min(newSleepDuration,maxSleepDuration);  }  if (LOG.isDebugEnabled()) {
@Override public URI getContainerSASUri(String accountName,String container) throws SASKeyGenerationException {
private CloudStorageAccount getSASKeyBasedStorageAccountInstance(String accountName) throws SASKeyGenerationException {
  super.initialize(uri,conf);  if (store == null) {    store=createDefaultStore(conf);  }  instrumentation=new AzureFileSystemInstrumentation(conf);  if (!conf.getBoolean(SKIP_AZURE_METRICS_PROPERTY_NAME,false)) {    AzureFileSystemMetricsSystem.fileSystemStarted();    metricsSourceName=newMetricsSourceName();    String sourceDesc="Azure Storage Volume File System metrics";    AzureFileSystemMetricsSystem.registerSource(metricsSourceName,sourceDesc,instrumentation);  }  store.initialize(uri,conf,instrumentation);  setConf(conf);  this.ugi=UserGroupInformation.getCurrentUser();  this.uri=URI.create(uri.getScheme() + "://" + uri.getAuthority());  this.workingDir=new Path("/user",UserGroupInformation.getCurrentUser().getShortUserName()).makeQualified(getUri(),getWorkingDirectory());  this.appendSupportEnabled=conf.getBoolean(APPEND_SUPPORT_ENABLE_PROPERTY_NAME,false);
private FSDataOutputStream create(Path f,FsPermission permission,boolean overwrite,boolean createParent,int bufferSize,short replication,long blockSize,Progressable progress,SelfRenewingLease parentFolderLease) throws FileAlreadyExistsException, IOException {
private boolean deleteWithAuthEnabled(Path f,boolean recursive,boolean skipParentFolderLastModifiedTimeUpdate) throws IOException {
    return false;  }  FileMetadata parentMetadata=null;  String parentKey=null;  if (parentPath != null) {    parentKey=pathToKey(parentPath);    try {      parentMetadata=store.retrieveMetadata(parentKey);    } catch (    IOException e) {      Throwable innerException=checkForAzureStorageException(e);      if (innerException instanceof StorageException) {        if (isFileNotFoundException((StorageException)innerException)) {          throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");        }      }      throw e;    }    if (parentMetadata == null) {      throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");
    if (parentMetadata == null) {      throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");    }    if (!parentMetadata.isDirectory()) {      throw new AzureException("File " + f + " has a parent directory "+ parentPath+ " which is also a file. Can't resolve.");    }  }  if (!metaFile.isDirectory()) {    if (parentPath != null && parentPath.getParent() != null) {      if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {        LOG.debug("Found an implicit parent directory while trying to" + " delete the file {}. Creating the directory blob for" + " it in {}.",f,parentKey);        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      } else {        if (!skipParentFolderLastModifiedTimeUpdate) {          updateParentFolderLastModifiedTime(key);        }      }    }    if (isStickyBitCheckViolated(metaFile,parentMetadata)) {      throw new WasbAuthorizationException(String.format("%s has sticky bit set. " + "File %s cannot be deleted.",parentPath,f));    }    try {
    if (!parentMetadata.isDirectory()) {      throw new AzureException("File " + f + " has a parent directory "+ parentPath+ " which is also a file. Can't resolve.");    }  }  if (!metaFile.isDirectory()) {    if (parentPath != null && parentPath.getParent() != null) {      if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {        LOG.debug("Found an implicit parent directory while trying to" + " delete the file {}. Creating the directory blob for" + " it in {}.",f,parentKey);        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      } else {        if (!skipParentFolderLastModifiedTimeUpdate) {          updateParentFolderLastModifiedTime(key);        }      }    }    if (isStickyBitCheckViolated(metaFile,parentMetadata)) {      throw new WasbAuthorizationException(String.format("%s has sticky bit set. " + "File %s cannot be deleted.",parentPath,f));    }    try {      if (store.delete(key)) {        instrumentation.fileDeleted();
        return false;      }      throw e;    }  } else {    LOG.debug("Directory Delete encountered: {}",f);    if (parentPath != null && parentPath.getParent() != null) {      if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {        LOG.debug("Found an implicit parent directory while trying to" + " delete the directory {}. Creating the directory blob for" + " it in {}. ",f,parentKey);        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      }    }    if (!metaFile.getKey().equals("/") && isStickyBitCheckViolated(metaFile,parentMetadata)) {      throw new WasbAuthorizationException(String.format("%s has sticky bit set. " + "File %s cannot be deleted.",parentPath,f));    }    ArrayList<FileMetadata> fileMetadataList=new ArrayList<>();    boolean isPartialDelete=false;    long start=Time.monotonicNow();    try {      isPartialDelete=getFolderContentsToDelete(metaFile,fileMetadataList);
    }  } else {    LOG.debug("Directory Delete encountered: {}",f);    if (parentPath != null && parentPath.getParent() != null) {      if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {        LOG.debug("Found an implicit parent directory while trying to" + " delete the directory {}. Creating the directory blob for" + " it in {}. ",f,parentKey);        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      }    }    if (!metaFile.getKey().equals("/") && isStickyBitCheckViolated(metaFile,parentMetadata)) {      throw new WasbAuthorizationException(String.format("%s has sticky bit set. " + "File %s cannot be deleted.",parentPath,f));    }    ArrayList<FileMetadata> fileMetadataList=new ArrayList<>();    boolean isPartialDelete=false;    long start=Time.monotonicNow();    try {      isPartialDelete=getFolderContentsToDelete(metaFile,fileMetadataList);    } catch (    IOException e) {      Throwable innerException=checkForAzureStorageException(e);
        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      }    }    if (!metaFile.getKey().equals("/") && isStickyBitCheckViolated(metaFile,parentMetadata)) {      throw new WasbAuthorizationException(String.format("%s has sticky bit set. " + "File %s cannot be deleted.",parentPath,f));    }    ArrayList<FileMetadata> fileMetadataList=new ArrayList<>();    boolean isPartialDelete=false;    long start=Time.monotonicNow();    try {      isPartialDelete=getFolderContentsToDelete(metaFile,fileMetadataList);    } catch (    IOException e) {      Throwable innerException=checkForAzureStorageException(e);      if (innerException instanceof StorageException && isFileNotFoundException((StorageException)innerException)) {        return false;      }      throw e;    }    long end=Time.monotonicNow();    LOG.debug("Time taken to list {} blobs for delete operation: {} ms",fileMetadataList.size(),(end - start));
private boolean deleteWithoutAuth(Path f,boolean recursive,boolean skipParentFolderLastModifiedTimeUpdate) throws IOException {
    return false;  }  if (!metaFile.isDirectory()) {    if (parentPath.getParent() != null) {      String parentKey=pathToKey(parentPath);      FileMetadata parentMetadata=null;      try {        parentMetadata=store.retrieveMetadata(parentKey);      } catch (      IOException e) {        Throwable innerException=checkForAzureStorageException(e);        if (innerException instanceof StorageException) {          if (isFileNotFoundException((StorageException)innerException)) {            throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");          }        }        throw e;      }      if (parentMetadata == null) {        throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");
        throw e;      }      if (parentMetadata == null) {        throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");      }      if (!parentMetadata.isDirectory()) {        throw new AzureException("File " + f + " has a parent directory "+ parentPath+ " which is also a file. Can't resolve.");      }      if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {        LOG.debug("Found an implicit parent directory while trying to" + " delete the file {}. Creating the directory blob for" + " it in {}.",f,parentKey);        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      } else {        if (!skipParentFolderLastModifiedTimeUpdate) {          updateParentFolderLastModifiedTime(key);        }      }    }    try {      if (store.delete(key)) {        instrumentation.fileDeleted();      } else {
        }      }    }    try {      if (store.delete(key)) {        instrumentation.fileDeleted();      } else {        return false;      }    } catch (    IOException e) {      Throwable innerException=checkForAzureStorageException(e);      if (innerException instanceof StorageException && isFileNotFoundException((StorageException)innerException)) {        return false;      }      throw e;    }  } else {    LOG.debug("Directory Delete encountered: {}",f);    if (parentPath.getParent() != null) {      String parentKey=pathToKey(parentPath);      FileMetadata parentMetadata=null;
 catch (      IOException e) {        Throwable innerException=checkForAzureStorageException(e);        if (innerException instanceof StorageException) {          if (isFileNotFoundException((StorageException)innerException)) {            throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");          }        }        throw e;      }      if (parentMetadata == null) {        throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");      }      if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {        LOG.debug("Found an implicit parent directory while trying to" + " delete the directory {}. Creating the directory blob for" + " it in {}. ",f,parentKey);        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      }    }    long start=Time.monotonicNow();    final FileMetadata[] contents;    try {      contents=store.list(key,AZURE_LIST_ALL,AZURE_UNBOUNDED_DEPTH);
        throw e;      }      if (parentMetadata == null) {        throw new IOException("File " + f + " has a parent directory "+ parentPath+ " whose metadata cannot be retrieved. Can't resolve");      }      if (parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit) {        LOG.debug("Found an implicit parent directory while trying to" + " delete the directory {}. Creating the directory blob for" + " it in {}. ",f,parentKey);        store.storeEmptyFolder(parentKey,createPermissionStatus(FsPermission.getDefault()));      }    }    long start=Time.monotonicNow();    final FileMetadata[] contents;    try {      contents=store.list(key,AZURE_LIST_ALL,AZURE_UNBOUNDED_DEPTH);    } catch (    IOException e) {      Throwable innerException=checkForAzureStorageException(e);      if (innerException instanceof StorageException && isFileNotFoundException((StorageException)innerException)) {        return false;      }      throw e;
  while (!foldersToProcess.empty()) {    FileMetadata currentFolder=foldersToProcess.pop();    Path currentPath=makeAbsolute(currentFolder.getPath());    boolean canDeleteChildren=true;    try {      performAuthCheck(currentPath,WasbAuthorizationOperations.WRITE,"delete",pathToDelete);    } catch (    WasbAuthorizationException we) {      LOG.debug("Authorization check failed for {}",currentPath);      canDeleteChildren=false;    }    if (canDeleteChildren) {      FileMetadata[] fileMetadataList=store.list(currentFolder.getKey(),AZURE_LIST_ALL,maxListingDepth);      for (      FileMetadata childItem : fileMetadataList) {        if (isStickyBitCheckViolated(childItem,currentFolder,false)) {          canDeleteChildren=false;          Path filePath=makeAbsolute(childItem.getPath());
    } catch (    WasbAuthorizationException we) {      LOG.debug("Authorization check failed for {}",currentPath);      canDeleteChildren=false;    }    if (canDeleteChildren) {      FileMetadata[] fileMetadataList=store.list(currentFolder.getKey(),AZURE_LIST_ALL,maxListingDepth);      for (      FileMetadata childItem : fileMetadataList) {        if (isStickyBitCheckViolated(childItem,currentFolder,false)) {          canDeleteChildren=false;          Path filePath=makeAbsolute(childItem.getPath());          LOG.error("User does not have permissions to delete {}. " + "Parent directory has sticky bit set.",filePath);        } else {          if (childItem.isDirectory()) {            foldersToProcess.push(childItem);          }          folderContentsMap.put(childItem.getKey(),childItem);        }      }    } else {
    if (canDeleteChildren) {      FileMetadata[] fileMetadataList=store.list(currentFolder.getKey(),AZURE_LIST_ALL,maxListingDepth);      for (      FileMetadata childItem : fileMetadataList) {        if (isStickyBitCheckViolated(childItem,currentFolder,false)) {          canDeleteChildren=false;          Path filePath=makeAbsolute(childItem.getPath());          LOG.error("User does not have permissions to delete {}. " + "Parent directory has sticky bit set.",filePath);        } else {          if (childItem.isDirectory()) {            foldersToProcess.push(childItem);          }          folderContentsMap.put(childItem.getKey(),childItem);        }      }    } else {      LOG.error("Authorization check failed. Files or folders under {} " + "will not be processed for deletion.",currentPath);    }    if (!canDeleteChildren) {      String pathToRemove=currentFolder.getKey();
@Override public FileStatus getFileStatus(Path f) throws FileNotFoundException, IOException {
private FileStatus getFileStatusInternal(Path f) throws FileNotFoundException, IOException {  Path absolutePath=makeAbsolute(f);  String key=pathToKey(absolutePath);  if (key.length() == 0) {    return new FileStatus(0,true,1,store.getHadoopBlockSize(),0,0,FsPermission.getDefault(),"","",absolutePath.makeQualified(getUri(),getWorkingDirectory()));  }  FileMetadata meta=null;  try {    meta=store.retrieveMetadata(key);  } catch (  Exception ex) {    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException)) {      throw new FileNotFoundException(String.format("%s is not found",key));    }    throw ex;  }  if (meta != null) {    if (meta.isDirectory()) {
@Override public FileStatus[] listStatus(Path f) throws FileNotFoundException, IOException {
  boolean renamed=conditionalRedoFolderRenames(listing);  if (renamed) {    listing=listWithErrorHandling(key,AZURE_LIST_ALL,1);  }  FileMetadata[] result=null;  if (key.equals("/")) {    ArrayList<FileMetadata> status=new ArrayList<>(listing.length);    for (    FileMetadata fileMetadata : listing) {      if (fileMetadata.isDirectory()) {        if (fileMetadata.getKey().equals(AZURE_TEMP_FOLDER)) {          continue;        }        status.add(updateFileStatusPath(fileMetadata,fileMetadata.getPath()));      } else {        status.add(updateFileStatusPath(fileMetadata,fileMetadata.getPath()));      }    }    result=status.toArray(new FileMetadata[0]);  } else {
private Path getAncestor(Path f) throws IOException {  for (Path current=f, parent=current.getParent(); parent != null; current=parent, parent=current.getParent()) {    String currentKey=pathToKey(current);    FileMetadata currentMetadata=store.retrieveMetadata(currentKey);    if (currentMetadata != null && currentMetadata.isDirectory()) {      Path ancestor=currentMetadata.getPath();
public boolean mkdirs(Path f,FsPermission permission,boolean noUmask) throws IOException {
@Override public FSDataInputStream open(Path f,int bufferSize) throws FileNotFoundException, IOException {
@Override public boolean rename(Path src,Path dst) throws FileNotFoundException, IOException {  FolderRenamePending renamePending=null;
  Path absoluteSrcPath=makeAbsolute(src);  Path srcParentFolder=absoluteSrcPath.getParent();  if (srcParentFolder == null) {    return false;  }  String srcKey=pathToKey(absoluteSrcPath);  if (srcKey.length() == 0) {    return false;  }  performAuthCheck(srcParentFolder,WasbAuthorizationOperations.WRITE,"rename",absoluteSrcPath);  if (this.azureAuthorization) {    try {      performStickyBitCheckForRenameOperation(absoluteSrcPath,srcParentFolder);    } catch (    FileNotFoundException ex) {      return false;    }catch (    IOException ex) {      Throwable innerException=checkForAzureStorageException(ex);
    } catch (    FileNotFoundException ex) {      return false;    }catch (    IOException ex) {      Throwable innerException=checkForAzureStorageException(ex);      if (innerException instanceof StorageException && isFileNotFoundException((StorageException)innerException)) {        LOG.debug("Encountered FileNotFound Exception when performing sticky bit check " + "on {}. Failing rename",srcKey);        return false;      }      throw ex;    }  }  Path absoluteDstPath=makeAbsolute(dst);  Path dstParentFolder=absoluteDstPath.getParent();  String dstKey=pathToKey(absoluteDstPath);  FileMetadata dstMetadata=null;  try {    dstMetadata=store.retrieveMetadata(dstKey);  } catch (  IOException ex) {
      if (innerException instanceof StorageException && isFileNotFoundException((StorageException)innerException)) {        LOG.debug("Encountered FileNotFound Exception when performing sticky bit check " + "on {}. Failing rename",srcKey);        return false;      }      throw ex;    }  }  Path absoluteDstPath=makeAbsolute(dst);  Path dstParentFolder=absoluteDstPath.getParent();  String dstKey=pathToKey(absoluteDstPath);  FileMetadata dstMetadata=null;  try {    dstMetadata=store.retrieveMetadata(dstKey);  } catch (  IOException ex) {    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    if (innerException instanceof StorageException) {      if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException)) {        LOG.debug("BlobNotFound exception encountered for Destination key : {}. " + "Swallowing the exception to handle race condition gracefully",dstKey);
        return false;      }      throw ex;    }  }  Path absoluteDstPath=makeAbsolute(dst);  Path dstParentFolder=absoluteDstPath.getParent();  String dstKey=pathToKey(absoluteDstPath);  FileMetadata dstMetadata=null;  try {    dstMetadata=store.retrieveMetadata(dstKey);  } catch (  IOException ex) {    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    if (innerException instanceof StorageException) {      if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException)) {        LOG.debug("BlobNotFound exception encountered for Destination key : {}. " + "Swallowing the exception to handle race condition gracefully",dstKey);      }    } else {      throw ex;
  try {    dstMetadata=store.retrieveMetadata(dstKey);  } catch (  IOException ex) {    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    if (innerException instanceof StorageException) {      if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException)) {        LOG.debug("BlobNotFound exception encountered for Destination key : {}. " + "Swallowing the exception to handle race condition gracefully",dstKey);      }    } else {      throw ex;    }  }  if (dstMetadata != null && dstMetadata.isDirectory()) {    performAuthCheck(absoluteDstPath,WasbAuthorizationOperations.WRITE,"rename",absoluteDstPath);    dstKey=pathToKey(makeAbsolute(new Path(dst,src.getName())));    LOG.debug("Destination {} " + " is a directory, adjusted the destination to be {}",dst,dstKey);  } else   if (dstMetadata != null) {    LOG.debug("Destination {}" + " is an already existing file, failing the rename.",dst);
    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    if (innerException instanceof StorageException) {      if (NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException)) {        LOG.debug("BlobNotFound exception encountered for Destination key : {}. " + "Swallowing the exception to handle race condition gracefully",dstKey);      }    } else {      throw ex;    }  }  if (dstMetadata != null && dstMetadata.isDirectory()) {    performAuthCheck(absoluteDstPath,WasbAuthorizationOperations.WRITE,"rename",absoluteDstPath);    dstKey=pathToKey(makeAbsolute(new Path(dst,src.getName())));    LOG.debug("Destination {} " + " is a directory, adjusted the destination to be {}",dst,dstKey);  } else   if (dstMetadata != null) {    LOG.debug("Destination {}" + " is an already existing file, failing the rename.",dst);    return false;  } else {    FileMetadata parentOfDestMetadata=null;
        LOG.debug("BlobNotFound exception encountered for Destination key : {}. " + "Swallowing the exception to handle race condition gracefully",dstKey);      }    } else {      throw ex;    }  }  if (dstMetadata != null && dstMetadata.isDirectory()) {    performAuthCheck(absoluteDstPath,WasbAuthorizationOperations.WRITE,"rename",absoluteDstPath);    dstKey=pathToKey(makeAbsolute(new Path(dst,src.getName())));    LOG.debug("Destination {} " + " is a directory, adjusted the destination to be {}",dst,dstKey);  } else   if (dstMetadata != null) {    LOG.debug("Destination {}" + " is an already existing file, failing the rename.",dst);    return false;  } else {    FileMetadata parentOfDestMetadata=null;    try {      parentOfDestMetadata=store.retrieveMetadata(pathToKey(absoluteDstPath.getParent()));    } catch (    IOException ex) {
 else     if (!parentOfDestMetadata.isDirectory()) {      LOG.debug("Parent of the destination {}" + " is a file, failing the rename.",dst);      return false;    } else {      performAuthCheck(dstParentFolder,WasbAuthorizationOperations.WRITE,"rename",absoluteDstPath);    }  }  FileMetadata srcMetadata=null;  try {    srcMetadata=store.retrieveMetadata(srcKey);  } catch (  IOException ex) {    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException)) {      LOG.debug("Source {} doesn't exists. Failing rename",src);      return false;    }    throw ex;  }  if (srcMetadata == null) {
      performAuthCheck(dstParentFolder,WasbAuthorizationOperations.WRITE,"rename",absoluteDstPath);    }  }  FileMetadata srcMetadata=null;  try {    srcMetadata=store.retrieveMetadata(srcKey);  } catch (  IOException ex) {    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    if (innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException)) {      LOG.debug("Source {} doesn't exists. Failing rename",src);      return false;    }    throw ex;  }  if (srcMetadata == null) {    LOG.debug("Source {} doesn't exist, failing the rename.",src);    return false;  } else   if (!srcMetadata.isDirectory()) {    LOG.debug("Source {} found as a file, renaming.",src);
          store.updateFolderLastModifiedTime(parentKey,lease);        } catch (        AzureException e) {          String errorCode="";          try {            StorageException e2=(StorageException)e.getCause();            errorCode=e2.getErrorCode();          } catch (          Exception e3) {          }          if (errorCode.equals("BlobNotFound")) {            throw new FileNotFoundException("Folder does not exist: " + parentKey);          }          LOG.warn("Got unexpected exception trying to get lease on {}. {}",parentKey,e.getMessage());          throw e;        } finally {          try {            if (lease != null) {              lease.free();
private void performStickyBitCheckForRenameOperation(Path srcPath,Path srcParentPath) throws FileNotFoundException, WasbAuthorizationException, IOException {  String srcKey=pathToKey(srcPath);  FileMetadata srcMetadata=null;  srcMetadata=store.retrieveMetadata(srcKey);  if (srcMetadata == null) {
public void recoverFilesWithDanglingTempData(Path root,Path destination) throws IOException {
public void deleteFilesWithDanglingTempData(Path root) throws IOException {
@VisibleForTesting public String getOwnerForPath(Path absolutePath) throws IOException {  String owner="";  FileMetadata meta=null;  String key=pathToKey(absolutePath);  try {    meta=store.retrieveMetadata(key);    if (meta != null) {      owner=meta.getOwner();
  String owner="";  FileMetadata meta=null;  String key=pathToKey(absolutePath);  try {    meta=store.retrieveMetadata(key);    if (meta != null) {      owner=meta.getOwner();      LOG.debug("Retrieved '{}' as owner for path - {}",owner,absolutePath);    } else {      LOG.debug("Cannot find file/folder - '{}'. Returning owner as empty string",absolutePath);    }  } catch (  IOException ex) {    Throwable innerException=NativeAzureFileSystemHelper.checkForAzureStorageException(ex);    boolean isfileNotFoundException=innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException)innerException);    if (!isfileNotFoundException) {      String errorMsg="Could not retrieve owner information for path - " + absolutePath;
@Override public void createBlobClient(CloudStorageAccount account){  String errorMsg="createBlobClient is an invalid operation in" + " SAS Key Mode";
@Override public void createBlobClient(URI baseUri){  String errorMsg="createBlobClient is an invalid operation in " + "SAS Key Mode";
@Override public void createBlobClient(URI baseUri,StorageCredentials credentials){  String errorMsg="createBlobClient is an invalid operation in SAS " + "Key Mode";
@Override public StorageCredentials getCredentials(){  String errorMsg="getCredentials is an invalid operation in SAS " + "Key Mode";
private Token<?> getDelegationToken(UserGroupInformation userGroupInformation) throws IOException {  if (this.delegationToken == null) {    Token<?> token=null;    for (    Token iterToken : userGroupInformation.getTokens()) {      if (iterToken.getKind().equals(WasbDelegationTokenIdentifier.TOKEN_KIND)) {        token=iterToken;
    sleepMultiple=(1 / writeFactor) - 1;  } else {    operationIsRead=true;    sleepMultiple=(1 / readFactor) - 1;  }  long sleepDuration=(long)(sleepMultiple * lastLatency);  if (sleepDuration < 0) {    sleepDuration=0;  }  if (sleepDuration > 0) {    try {      Thread.sleep(sleepDuration);    } catch (    InterruptedException ie) {      Thread.currentThread().interrupt();    }    sendEvent.getRequestResult().setStartDate(new Date());  }  if (LOG.isDebugEnabled()) {    boolean isFirstRequest=(lastLatency == 0);
  try {    final RetryPolicy.RetryAction a=(retryPolicy != null) ? retryPolicy.shouldRetry(ioe,retry,0,true) : RetryPolicy.RetryAction.FAIL;    boolean isRetry=a.action == RetryPolicy.RetryAction.RetryDecision.RETRY;    boolean isFailoverAndRetry=a.action == RetryPolicy.RetryAction.RetryDecision.FAILOVER_AND_RETRY;    if (isRetry || isFailoverAndRetry) {      LOG.debug("Retrying connect to Remote service:{}. Already tried {}" + " time(s); retry policy is {}, " + "delay {}ms.",url,retry,retryPolicy,a.delayMillis);      Thread.sleep(a.delayMillis);      return;    }  } catch (  InterruptedIOException e) {    LOG.warn(e.getMessage(),e);    Thread.currentThread().interrupt();    return;  }catch (  Exception e) {    LOG.warn("Original exception is ",ioe);    throw new WasbRemoteCallException(e.getMessage(),e);
@Override public void initialize(URI uri,Configuration configuration) throws IOException {  uri=ensureAuthority(uri,configuration);  super.initialize(uri,configuration);  setConf(configuration);
  abfsCounters=new AbfsCountersImpl(uri);  this.abfsStore=new AzureBlobFileSystemStore(uri,this.isSecureScheme(),configuration,abfsCounters);  LOG.trace("AzureBlobFileSystemStore init complete");  final AbfsConfiguration abfsConfiguration=abfsStore.getAbfsConfiguration();  this.setWorkingDirectory(this.getHomeDirectory());  if (abfsConfiguration.getCreateRemoteFileSystemDuringInitialization()) {    if (this.tryGetFileStatus(new Path(AbfsHttpConstants.ROOT_PATH)) == null) {      try {        this.createFileSystem();      } catch (      AzureBlobFileSystemException ex) {        checkException(null,ex,AzureServiceErrorCode.FILE_SYSTEM_ALREADY_EXISTS);      }    }  }  LOG.trace("Initiate check for delegation token manager");  if (UserGroupInformation.isSecurityEnabled()) {    this.delegationTokenEnabled=abfsConfiguration.isDelegationTokenManagerEnabled();    if (this.delegationTokenEnabled) {
  final AbfsConfiguration abfsConfiguration=abfsStore.getAbfsConfiguration();  this.setWorkingDirectory(this.getHomeDirectory());  if (abfsConfiguration.getCreateRemoteFileSystemDuringInitialization()) {    if (this.tryGetFileStatus(new Path(AbfsHttpConstants.ROOT_PATH)) == null) {      try {        this.createFileSystem();      } catch (      AzureBlobFileSystemException ex) {        checkException(null,ex,AzureServiceErrorCode.FILE_SYSTEM_ALREADY_EXISTS);      }    }  }  LOG.trace("Initiate check for delegation token manager");  if (UserGroupInformation.isSecurityEnabled()) {    this.delegationTokenEnabled=abfsConfiguration.isDelegationTokenManagerEnabled();    if (this.delegationTokenEnabled) {      LOG.debug("Initializing DelegationTokenManager for {}",uri);      this.delegationTokenManager=abfsConfiguration.getDelegationTokenManager();      delegationTokenManager.bind(getUri(),configuration);
  if (abfsConfiguration.getCreateRemoteFileSystemDuringInitialization()) {    if (this.tryGetFileStatus(new Path(AbfsHttpConstants.ROOT_PATH)) == null) {      try {        this.createFileSystem();      } catch (      AzureBlobFileSystemException ex) {        checkException(null,ex,AzureServiceErrorCode.FILE_SYSTEM_ALREADY_EXISTS);      }    }  }  LOG.trace("Initiate check for delegation token manager");  if (UserGroupInformation.isSecurityEnabled()) {    this.delegationTokenEnabled=abfsConfiguration.isDelegationTokenManagerEnabled();    if (this.delegationTokenEnabled) {      LOG.debug("Initializing DelegationTokenManager for {}",uri);      this.delegationTokenManager=abfsConfiguration.getDelegationTokenManager();      delegationTokenManager.bind(getUri(),configuration);      LOG.debug("Created DelegationTokenManager {}",delegationTokenManager);    }  }  AbfsClientThrottlingIntercept.initializeSingleton(abfsConfiguration.isAutoThrottlingEnabled());
@Override public FSDataInputStream open(final Path path,final int bufferSize) throws IOException {
@Override public FSDataOutputStream create(final Path f,final FsPermission permission,final boolean overwrite,final int bufferSize,final short replication,final long blockSize,final Progressable progress) throws IOException {
@Override public FSDataOutputStream append(final Path f,final int bufferSize,final Progressable progress) throws IOException {
public boolean rename(final Path src,final Path dst) throws IOException {
@Override public boolean delete(final Path f,final boolean recursive) throws IOException {
@Override public FileStatus[] listStatus(final Path f) throws IOException {
@Override public boolean mkdirs(final Path f,final FsPermission permission) throws IOException {
@Override public FileStatus getFileStatus(final Path f) throws IOException {
@Override public void setOwner(final Path path,final String owner,final String group) throws IOException {
@Override public void setXAttr(final Path path,final String name,final byte[] value,final EnumSet<XAttrSetFlag> flag) throws IOException {
@Override public byte[] getXAttr(final Path path,final String name) throws IOException {
@Override public void setPermission(final Path path,final FsPermission permission) throws IOException {
@Override public void modifyAclEntries(final Path path,final List<AclEntry> aclSpec) throws IOException {
@Override public void removeAclEntries(final Path path,final List<AclEntry> aclSpec) throws IOException {
@Override public void removeDefaultAcl(final Path path) throws IOException {
@Override public void removeAcl(final Path path) throws IOException {
@Override public void setAcl(final Path path,final List<AclEntry> aclSpec) throws IOException {
@Override public AclStatus getAclStatus(final Path path) throws IOException {
@Override public void access(final Path path,final FsAction mode) throws IOException {
public Hashtable<String,String> getPathStatus(final Path path) throws AzureBlobFileSystemException {  try (AbfsPerfInfo perfInfo=startTracking("getPathStatus","getPathStatus")){
public void setPathProperties(final Path path,final Hashtable<String,String> properties) throws AzureBlobFileSystemException {  try (AbfsPerfInfo perfInfo=startTracking("setPathProperties","setPathProperties")){
public OutputStream createFile(final Path path,final FileSystem.Statistics statistics,final boolean overwrite,final FsPermission permission,final FsPermission umask) throws AzureBlobFileSystemException {  try (AbfsPerfInfo perfInfo=startTracking("createFile","createPath")){    boolean isNamespaceEnabled=getIsNamespaceEnabled();
public void createDirectory(final Path path,final FsPermission permission,final FsPermission umask) throws AzureBlobFileSystemException {  try (AbfsPerfInfo perfInfo=startTracking("createDirectory","createPath")){    boolean isNamespaceEnabled=getIsNamespaceEnabled();
public AbfsInputStream openFileForRead(final Path path,final FileSystem.Statistics statistics) throws AzureBlobFileSystemException {  try (AbfsPerfInfo perfInfo=startTracking("openFileForRead","getPathStatus")){
public OutputStream openFileForWrite(final Path path,final FileSystem.Statistics statistics,final boolean overwrite) throws AzureBlobFileSystemException {  try (AbfsPerfInfo perfInfo=startTracking("openFileForWrite","getPathStatus")){
public void delete(final Path path,final boolean recursive) throws AzureBlobFileSystemException {  final Instant startAggregate=abfsPerfTracker.getLatencyInstant();  long countAggregate=0;  boolean shouldContinue=true;
public FileStatus getFileStatus(final Path path) throws IOException {  try (AbfsPerfInfo perfInfo=startTracking("getFileStatus","undetermined")){    boolean isNamespaceEnabled=getIsNamespaceEnabled();
@InterfaceStability.Unstable public FileStatus[] listStatus(final Path path,final String startFrom) throws IOException {  final Instant startAggregate=abfsPerfTracker.getLatencyInstant();  long countAggregate=0;  boolean shouldContinue=true;
public void access(final Path path,final FsAction mode) throws AzureBlobFileSystemException {
public static AzureADToken getTokenFromMsi(final String authEndpoint,final String tenantGuid,final String clientId,String authority,boolean bypassCache) throws IOException {  QueryParams qp=new QueryParams();  qp.add("api-version","2018-02-01");  qp.add("resource",RESOURCE_NAME);  if (tenantGuid != null && tenantGuid.length() > 0) {    authority=authority + tenantGuid;
    httperror=0;    ex=null;    try {      token=getTokenSingleCall(authEndpoint,body,headers,httpMethod,isMsi);    } catch (    HttpException e) {      httperror=e.httpErrorCode;      ex=e;    }catch (    IOException e) {      httperror=-1;      isRecoverableFailure=isRecoverableFailure(e);      ex=new HttpException(httperror,"",String.format("AzureADAuthenticator.getTokenCall threw %s : %s",e.getClass().getTypeName(),e.getMessage()),authEndpoint,"","");    }    succeeded=((httperror == 0) && (ex == null));    shouldRetry=!succeeded && isRecoverableFailure && tokenFetchRetryPolicy.shouldRetry(retryCount,httperror);    retryCount++;    if (shouldRetry) {
    jp.nextToken();    while (jp.hasCurrentToken()) {      if (jp.getCurrentToken() == JsonToken.FIELD_NAME) {        fieldName=jp.getCurrentName();        jp.nextToken();        fieldValue=jp.getText();        if (fieldName.equals("access_token")) {          token.setAccessToken(fieldValue);        }        if (fieldName.equals("expires_in")) {          expiryPeriodInSecs=Integer.parseInt(fieldValue);        }        if (fieldName.equals("expires_on")) {          expiresOnInSecs=Long.parseLong(fieldValue);        }      }      jp.nextToken();    }    jp.close();    if (expiresOnInSecs > 0) {
          expiryPeriodInSecs=Integer.parseInt(fieldValue);        }        if (fieldName.equals("expires_on")) {          expiresOnInSecs=Long.parseLong(fieldValue);        }      }      jp.nextToken();    }    jp.close();    if (expiresOnInSecs > 0) {      LOG.debug("Expiry based on expires_on: {}",expiresOnInSecs);      token.setExpiry(new Date(expiresOnInSecs * 1000));    } else {      if (isMsi) {        throw new UnsupportedOperationException("MSI Responded with invalid expires_on");      }      LOG.debug("Expiry based on expires_in: {}",expiryPeriodInSecs);      long expiry=System.currentTimeMillis();      expiry=expiry + expiryPeriodInSecs * 1000L;      token.setExpiry(new Date(expiry));
public Token<DelegationTokenIdentifier> getDelegationToken(String renewer) throws IOException {
    double reductionFactor=(consecutiveNoErrorCount * analysisPeriodMs >= RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS) ? RAPID_SLEEP_DECREASE_FACTOR : SLEEP_DECREASE_FACTOR;    newSleepDuration=sleepDuration * reductionFactor;  } else   if (errorPercentage < MAX_EQUILIBRIUM_ERROR_PERCENTAGE) {    newSleepDuration=sleepDuration;  } else {    consecutiveNoErrorCount=0;    double additionalDelayNeeded=5 * analysisPeriodMs;    if (bytesSuccessful > 0) {      additionalDelayNeeded=(bytesSuccessful + bytesFailed) * periodMs / bytesSuccessful - periodMs;    }    newSleepDuration=additionalDelayNeeded / (operationsFailed + operationsSuccessful);    final double maxSleepDuration=analysisPeriodMs;    final double minSleepDuration=sleepDuration * SLEEP_INCREASE_FACTOR;    newSleepDuration=Math.max(newSleepDuration,minSleepDuration) + 1;    newSleepDuration=Math.min(newSleepDuration,maxSleepDuration);  }  if (LOG.isDebugEnabled()) {
@Override public synchronized int read(final byte[] b,final int off,final int len) throws IOException {  if (b != null) {
  if (b == null) {    throw new IllegalArgumentException("null byte array passed in to read() method");  }  if (offset >= b.length) {    throw new IllegalArgumentException("offset greater than length of array");  }  if (length < 0) {    throw new IllegalArgumentException("requested read length is less than zero");  }  if (length > (b.length - offset)) {    throw new IllegalArgumentException("requested read length is more than will fit after requested offset in buffer");  }  final AbfsRestOperation op;  AbfsPerfTracker tracker=client.getAbfsPerfTracker();  try (AbfsPerfInfo perfInfo=new AbfsPerfInfo(tracker,"readRemote","read")){    LOG.trace("Trigger client.read for path={} position={} offset={} length={}",path,position,offset,length);    op=client.read(path,position,b,offset,length,tolerateOobAppends ? "*" : eTag,cachedSasToken.get());    cachedSasToken.update(op.getSasToken());    if (streamStatistics != null) {
@Override public synchronized void seek(long n) throws IOException {
  LOG.debug("requested seek to position {}",n);  if (closed) {    throw new IOException(FSExceptionMessages.STREAM_IS_CLOSED);  }  if (n < 0) {    throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);  }  if (n > contentLength) {    throw new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF);  }  if (streamStatistics != null) {    streamStatistics.seek(n,fCursor);  }  if (n >= fCursor - limit && n <= fCursor) {    bCursor=(int)(n - (fCursor - limit));    if (streamStatistics != null) {      streamStatistics.seekInBuffer();    }    return;  }  fCursor=n;
public static void dumpHeadersToDebugLog(final String origin,final Map<String,List<String>> headers){  if (LOG.isDebugEnabled()) {
private void offerToQueue(Instant trackerStart,String latencyDetails){  queue.offer(latencyDetails);  if (LOG.isDebugEnabled()) {    Instant trackerStop=Instant.now();    long elapsed=Duration.between(trackerStart,trackerStop).toMillis();
break;}AbfsIoUtils.dumpHeadersToDebugLog("Request Headers",httpOperation.getConnection().getRequestProperties());AbfsClientThrottlingIntercept.sendingRequest(operationType,abfsCounters);if (hasRequestBody) {httpOperation.sendRequest(buffer,bufferOffset,bufferLength);incrementCounter(AbfsStatistic.SEND_REQUESTS,1);incrementCounter(AbfsStatistic.BYTES_SENT,bufferLength);}httpOperation.processResponse(buffer,bufferOffset,bufferLength);incrementCounter(AbfsStatistic.GET_RESPONSES,1);incrementCounter(AbfsStatistic.BYTES_RECEIVED,httpOperation.getBytesReceived());} catch (IOException ex) {if (ex instanceof UnknownHostException) {LOG.warn(String.format("Unknown host name: %s. Retrying to resolve the host name...",httpOperation.getUrl().getHost()));}if (LOG.isDebugEnabled()) {if (httpOperation != null) {
AbfsIoUtils.dumpHeadersToDebugLog("Request Headers",httpOperation.getConnection().getRequestProperties());AbfsClientThrottlingIntercept.sendingRequest(operationType,abfsCounters);if (hasRequestBody) {httpOperation.sendRequest(buffer,bufferOffset,bufferLength);incrementCounter(AbfsStatistic.SEND_REQUESTS,1);incrementCounter(AbfsStatistic.BYTES_SENT,bufferLength);}httpOperation.processResponse(buffer,bufferOffset,bufferLength);incrementCounter(AbfsStatistic.GET_RESPONSES,1);incrementCounter(AbfsStatistic.BYTES_RECEIVED,httpOperation.getBytesReceived());} catch (IOException ex) {if (ex instanceof UnknownHostException) {LOG.warn(String.format("Unknown host name: %s. Retrying to resolve the host name...",httpOperation.getUrl().getHost()));}if (LOG.isDebugEnabled()) {if (httpOperation != null) {LOG.debug("HttpRequestFailure: " + httpOperation.toString(),ex);
incrementCounter(AbfsStatistic.BYTES_RECEIVED,httpOperation.getBytesReceived());} catch (IOException ex) {if (ex instanceof UnknownHostException) {LOG.warn(String.format("Unknown host name: %s. Retrying to resolve the host name...",httpOperation.getUrl().getHost()));}if (LOG.isDebugEnabled()) {if (httpOperation != null) {LOG.debug("HttpRequestFailure: " + httpOperation.toString(),ex);} else {LOG.debug("HttpRequestFailure: " + method + ","+ url,ex);}}if (!client.getRetryPolicy().shouldRetry(retryCount,-1)) {throw new InvalidAbfsRestOperationException(ex);}if (ex instanceof HttpException) {throw new AbfsRestOperationException((HttpException)ex);}return false;} finally {
public void signRequest(HttpURLConnection connection,final long contentLength) throws UnsupportedEncodingException {  String gmtTime=getGMTTime();  connection.setRequestProperty(HttpHeaderConfigurations.X_MS_DATE,gmtTime);  final String stringToSign=canonicalize(connection,accountName,contentLength);  final String computedBase64Signature=computeHmac256(stringToSign);  String signature=String.format("%s %s:%s","SharedKey",accountName,computedBase64Signature);  connection.setRequestProperty(HttpHeaderConfigurations.AUTHORIZATION,signature);
  int signedExpiryLen=3;  int start=token.indexOf(signedExpiry);  if (start == -1) {    return OffsetDateTime.MIN;  }  start+=signedExpiryLen;  int end=token.indexOf("&",start);  String seValue=(end == -1) ? token.substring(start) : token.substring(start,end);  try {    seValue=URLDecoder.decode(seValue,"utf-8");  } catch (  UnsupportedEncodingException ex) {    LOG.error("Error decoding se query parameter ({}) from SAS.",seValue,ex);    return OffsetDateTime.MIN;  }  OffsetDateTime seDate=OffsetDateTime.MIN;  try {    seDate=OffsetDateTime.parse(seValue,DateTimeFormatter.ISO_DATE_TIME);
    LOG.error("Error decoding se query parameter ({}) from SAS.",seValue,ex);    return OffsetDateTime.MIN;  }  OffsetDateTime seDate=OffsetDateTime.MIN;  try {    seDate=OffsetDateTime.parse(seValue,DateTimeFormatter.ISO_DATE_TIME);  } catch (  DateTimeParseException ex) {    LOG.error("Error parsing se query parameter ({}) from SAS.",seValue,ex);  }  String signedKeyExpiry="ske=";  int signedKeyExpiryLen=4;  start=token.indexOf(signedKeyExpiry);  if (start == -1) {    return seDate;  }  start+=signedKeyExpiryLen;  end=token.indexOf("&",start);  String skeValue=(end == -1) ? token.substring(start) : token.substring(start,end);
  } catch (  DateTimeParseException ex) {    LOG.error("Error parsing se query parameter ({}) from SAS.",seValue,ex);  }  String signedKeyExpiry="ske=";  int signedKeyExpiryLen=4;  start=token.indexOf(signedKeyExpiry);  if (start == -1) {    return seDate;  }  start+=signedKeyExpiryLen;  end=token.indexOf("&",start);  String skeValue=(end == -1) ? token.substring(start) : token.substring(start,end);  try {    skeValue=URLDecoder.decode(skeValue,"utf-8");  } catch (  UnsupportedEncodingException ex) {    LOG.error("Error decoding ske query parameter ({}) from SAS.",skeValue,ex);    return OffsetDateTime.MIN;
private static void loadMap(HashMap<String,String> cache,String fileLocation,int noOfFields,int keyIndex) throws IOException {
  LOG.debug("Loading identity map from file {}",fileLocation);  int errorRecord=0;  File file=new File(fileLocation);  LineIterator it=null;  try {    it=FileUtils.lineIterator(file,"UTF-8");    while (it.hasNext()) {      String line=it.nextLine();      if (!Strings.isNullOrEmpty(line.trim()) && !line.startsWith(HASH)) {        if (line.split(COLON).length != noOfFields) {          errorRecord+=1;          continue;        }        cache.put(line.split(COLON)[keyIndex],line);      }    }    LOG.debug("Loaded map stats - File: {}, Loaded: {}, Error: {} ",fileLocation,cache.size(),errorRecord);  } catch (  ArrayIndexOutOfBoundsException e) {
protected void describe(String text,Object... args){
private void createTestFileAndSetLength() throws IOException {  FileSystem fs=accountUsingInputStreamV1.getFileSystem();  if (fs.exists(TEST_FILE_PATH)) {    testFileStatus=fs.getFileStatus(TEST_FILE_PATH);    testFileLength=testFileStatus.getLen();
@Test public void test_0315_SequentialReadPerformance() throws IOException {  assumeHugeFileExists();  final int maxAttempts=10;  final double maxAcceptableRatio=1.01;  double v1ElapsedMs=0, v2ElapsedMs=0;  double ratio=Double.MAX_VALUE;  for (int i=0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {    v1ElapsedMs=sequentialRead(1,accountUsingInputStreamV1.getFileSystem(),false);    v2ElapsedMs=sequentialRead(2,accountUsingInputStreamV2.getFileSystem(),false);    ratio=v2ElapsedMs / v1ElapsedMs;
@Test public void test_0316_SequentialReadAfterReverseSeekPerformanceV2() throws IOException {  assumeHugeFileExists();  final int maxAttempts=10;  final double maxAcceptableRatio=1.01;  double beforeSeekElapsedMs=0, afterSeekElapsedMs=0;  double ratio=Double.MAX_VALUE;  for (int i=0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {    beforeSeekElapsedMs=sequentialRead(2,accountUsingInputStreamV2.getFileSystem(),false);    afterSeekElapsedMs=sequentialRead(2,accountUsingInputStreamV2.getFileSystem(),true);    ratio=afterSeekElapsedMs / beforeSeekElapsedMs;
private long sequentialRead(int version,FileSystem fs,boolean afterReverseSeek) throws IOException {  byte[] buffer=new byte[16 * KILOBYTE];  long totalBytesRead=0;  long bytesRead=0;  try (FSDataInputStream inputStream=fs.open(TEST_FILE_PATH)){    if (afterReverseSeek) {      while (bytesRead > 0 && totalBytesRead < 4 * MEGABYTE) {        bytesRead=inputStream.read(buffer);        totalBytesRead+=bytesRead;      }      totalBytesRead=0;      inputStream.seek(0);    }    NanoTimer timer=new NanoTimer();    while ((bytesRead=inputStream.read(buffer)) > 0) {      totalBytesRead+=bytesRead;    }    long elapsedTimeMs=timer.elapsedTimeMs();
@Test public void test_0317_RandomReadPerformance() throws IOException {  assumeHugeFileExists();  final int maxAttempts=10;  final double maxAcceptableRatio=0.10;  double v1ElapsedMs=0, v2ElapsedMs=0;  double ratio=Double.MAX_VALUE;  for (int i=0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {    v1ElapsedMs=randomRead(1,accountUsingInputStreamV1.getFileSystem());    v2ElapsedMs=randomRead(2,accountUsingInputStreamV2.getFileSystem());    ratio=v2ElapsedMs / v1ElapsedMs;
  assumeHugeFileExists();  final int minBytesToRead=2 * MEGABYTE;  Random random=new Random();  byte[] buffer=new byte[8 * KILOBYTE];  long totalBytesRead=0;  long bytesRead=0;  try (FSDataInputStream inputStream=fs.open(TEST_FILE_PATH)){    NanoTimer timer=new NanoTimer();    do {      bytesRead=inputStream.read(buffer);      totalBytesRead+=bytesRead;      inputStream.seek(random.nextInt((int)(testFileLength - buffer.length)));    } while (bytesRead > 0 && totalBytesRead < minBytesToRead);    long elapsedTimeMs=timer.elapsedTimeMs();    inputStream.close();
protected void assertInLog(String content,String term){  assertTrue("Empty log",!content.isEmpty());  if (!content.contains(term)) {    String message="No " + term + " found in logs";
protected void assertNotInLog(String content,String term){  assertTrue("Empty log",!content.isEmpty());  if (content.contains(term)) {    String message=term + " found in logs";
@Override public void setUp() throws Exception {  super.setUp();  Configuration conf=getConfiguration();  threads=AzureTestUtils.getTestPropertyInt(conf,"fs.azure.scale.test.list.performance.threads",NUMBER_OF_THREADS);  filesPerThread=AzureTestUtils.getTestPropertyInt(conf,"fs.azure.scale.test.list.performance.files",NUMBER_OF_FILES_PER_THREAD);  expectedFileCount=threads * filesPerThread;
  final String basePath=(fs.getWorkingDirectory().toUri().getPath() + "/" + TEST_DIR_PATH+ "/").substring(1);  ArrayList<Callable<Integer>> tasks=new ArrayList<>(threads);  fs.mkdirs(TEST_DIR_PATH);  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  for (int i=0; i < threads; i++) {    tasks.add(new Callable<Integer>(){      public Integer call(){        int written=0;        for (int j=0; j < filesPerThread; j++) {          String blobName=basePath + UUID.randomUUID().toString();          try {            CloudBlockBlob blob=container.getBlockBlobReference(blobName);            blob.uploadText("");            written++;          } catch (          Exception e) {
        int written=0;        for (int j=0; j < filesPerThread; j++) {          String blobName=basePath + UUID.randomUUID().toString();          try {            CloudBlockBlob blob=container.getBlockBlobReference(blobName);            blob.uploadText("");            written++;          } catch (          Exception e) {            LOG.error("Filed to write {}",blobName,e);            break;          }        }        LOG.info("Thread completed with {} files written",written);        return written;      }    });  }  List<Future<Integer>> futures=executorService.invokeAll(tasks,getTestTimeoutMillis(),TimeUnit.MILLISECONDS);  long elapsedMs=timer.elapsedTimeMs();
@Test public void test_0200_ListStatusPerformance() throws Exception {  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  FileStatus[] fileList=fs.listStatus(TEST_DIR_PATH);  long elapsedMs=timer.elapsedTimeMs();
@Test public void test_0200_ListStatusPerformance() throws Exception {  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  FileStatus[] fileList=fs.listStatus(TEST_DIR_PATH);  long elapsedMs=timer.elapsedTimeMs();  LOG.info(String.format("files=%1$d, elapsedMs=%2$d",fileList.length,elapsedMs));  Map<Path,FileStatus> foundInList=new HashMap<>(expectedFileCount);  for (  FileStatus fileStatus : fileList) {    foundInList.put(fileStatus.getPath(),fileStatus);
  Map<Path,FileStatus> foundInList=new HashMap<>(expectedFileCount);  for (  FileStatus fileStatus : fileList) {    foundInList.put(fileStatus.getPath(),fileStatus);    LOG.info("{}: {}",fileStatus.getPath(),fileStatus.isDirectory() ? "dir" : "file");  }  assertEquals("Mismatch between expected files and actual",expectedFileCount,fileList.length);  ContractTestUtils.NanoTimer initialStatusCallTimer=new ContractTestUtils.NanoTimer();  RemoteIterator<LocatedFileStatus> listing=fs.listFiles(TEST_DIR_PATH,true);  long initialListTime=initialStatusCallTimer.elapsedTimeMs();  timer=new ContractTestUtils.NanoTimer();  while (listing.hasNext()) {    FileStatus fileStatus=listing.next();    Path path=fileStatus.getPath();    FileStatus removed=foundInList.remove(path);    assertNotNull("Did not find " + path + "{} in the previous listing",removed);  }  elapsedMs=timer.elapsedTimeMs();
@Test public void test_0300_BulkDeletePerformance() throws Exception {  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  fs.delete(TEST_DIR_PATH,true);  long elapsedMs=timer.elapsedTimeMs();
  final AtomicInteger successfulRenameCount=new AtomicInteger(0);  final AtomicReference<IOException> unexpectedError=new AtomicReference<IOException>();  final Path dest=path("dest");  List<Thread> threads=new ArrayList<>();  for (int i=0; i < 10; i++) {    final int threadNumber=i;    Path src=path("test" + threadNumber);    threads.add(new Thread(() -> {      try {        latch.await(Long.MAX_VALUE,TimeUnit.SECONDS);      } catch (      InterruptedException e) {      }      try {        try (OutputStream output=fs.create(src)){          output.write(("Source file number " + threadNumber).getBytes());        }         if (fs.rename(src,dest)) {
@Test public void testDeleteThrowsExceptionWithLeaseExistsErrorMessage() throws Exception {  LOG.info("Starting test");  Path path=methodPath();  fs.create(path);  assertPathExists("test file",path);  NativeAzureFileSystem nfs=fs;  final String fullKey=nfs.pathToKey(nfs.makeAbsolute(path));  final AzureNativeFileSystemStore store=nfs.getStore();  final CountDownLatch leaseAttemptComplete=new CountDownLatch(1);  final CountDownLatch beginningDeleteAttempt=new CountDownLatch(1);  Thread t=new Thread(){    @Override public void run(){      SelfRenewingLease lease=null;      try {        lease=store.acquireLease(fullKey);
private void createTestFileAndSetLength() throws IOException {  if (fs.exists(TEST_FILE_PATH)) {    testFileStatus=fs.getFileStatus(TEST_FILE_PATH);    testFileLength=testFileStatus.getLen();
@Test public void testEnumContainers() throws Throwable {  describe("Enumerating all the WASB test containers");  int count=0;  CloudStorageAccount storageAccount=getTestAccount().getRealAccount();  CloudBlobClient blobClient=storageAccount.createCloudBlobClient();  Iterable<CloudBlobContainer> containers=blobClient.listContainers(CONTAINER_PREFIX);  for (  CloudBlobContainer container : containers) {    count++;
@Test public void testDeleteContainers() throws Throwable {  describe("Delete all the WASB test containers");  int count=0;  CloudStorageAccount storageAccount=getTestAccount().getRealAccount();  CloudBlobClient blobClient=storageAccount.createCloudBlobClient();  Iterable<CloudBlobContainer> containers=blobClient.listContainers(CONTAINER_PREFIX);  for (  CloudBlobContainer container : containers) {
protected void logTimePerIOP(String operation,ContractTestUtils.NanoTimer timer,long count){
private void logFSState(){  StorageStatistics statistics=getFileSystem().getStorageStatistics();  Iterator<StorageStatistics.LongStatistic> longStatistics=statistics.getLongStatistics();  while (longStatistics.hasNext()) {    StorageStatistics.LongStatistic next=longStatistics.next();
  byte[] data=SOURCE_DATA;  long blocks=filesize / UPLOAD_BLOCKSIZE;  long blocksPerMB=S_1M / UPLOAD_BLOCKSIZE;  NativeAzureFileSystem fs=getFileSystem();  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  long blocksPer10MB=blocksPerMB * 10;  fs.mkdirs(hugefile.getParent());  try (FSDataOutputStream out=fs.create(hugefile,true,UPLOAD_BLOCKSIZE,null)){    for (long block=1; block <= blocks; block++) {      out.write(data);      long written=block * UPLOAD_BLOCKSIZE;      if (block % blocksPer10MB == 0 || written == filesize) {        long percentage=written * 100 / filesize;        double elapsedTime=timer.elapsedTime() / NANOSEC;        double writtenMB=1.0 * written / S_1M;
  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  ContractTestUtils.NanoTimer readAtByte0, readAtByte0Again, readAtEOF;  try (FSDataInputStream in=openDataFile()){    readAtByte0=new ContractTestUtils.NanoTimer();    in.readFully(0,buffer);    readAtByte0.end("time to read data at start of file");    ops++;    readAtEOF=new ContractTestUtils.NanoTimer();    in.readFully(eof - bufferSize,buffer);    readAtEOF.end("time to read data at end of file");    ops++;    readAtByte0Again=new ContractTestUtils.NanoTimer();    in.readFully(0,buffer);    readAtByte0Again.end("time to read data at start of file again");    ops++;
    in.readFully(0,buffer);    readAtByte0.end("time to read data at start of file");    ops++;    readAtEOF=new ContractTestUtils.NanoTimer();    in.readFully(eof - bufferSize,buffer);    readAtEOF.end("time to read data at end of file");    ops++;    readAtByte0Again=new ContractTestUtils.NanoTimer();    in.readFully(0,buffer);    readAtByte0Again.end("time to read data at start of file again");    ops++;    LOG.info("Final stream state: {}",in);  }   long mb=Math.max(filesize / S_1M,1);  logFSState();  timer.end("time to performed positioned reads of %d MB ",mb);
@Test public void test_050_readHugeFile() throws Throwable {  assumeHugeFileExists();  describe("Reading %s",hugefile);  NativeAzureFileSystem fs=getFileSystem();  FileStatus status=fs.getFileStatus(hugefile);  long filesize=status.getLen();  long blocks=filesize / UPLOAD_BLOCKSIZE;  byte[] data=new byte[UPLOAD_BLOCKSIZE];  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  try (FSDataInputStream in=openDataFile()){    for (long block=0; block < blocks; block++) {      in.readFully(data);    }    LOG.info("Final stream state: {}",in);  }   long mb=Math.max(filesize / S_1M,1);  timer.end("time to read file of %d MB ",mb);
      int remaining=blockSize;      long blockId=i + 1;      NanoTimer blockTimer=new NanoTimer();      int reads=0;      while (remaining > 0) {        NanoTimer readTimer=new NanoTimer();        int bytesRead=in.read(block,offset,remaining);        reads++;        if (bytesRead == 1) {          break;        }        remaining-=bytesRead;        offset+=bytesRead;        count+=bytesRead;        readTimer.end();        if (bytesRead != 0) {
        NanoTimer readTimer=new NanoTimer();        int bytesRead=in.read(block,offset,remaining);        reads++;        if (bytesRead == 1) {          break;        }        remaining-=bytesRead;        offset+=bytesRead;        count+=bytesRead;        readTimer.end();        if (bytesRead != 0) {          LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s",reads,bytesRead,blockSize - remaining,remaining,readTimer.duration(),readTimer.nanosPerOperation(bytesRead),readTimer.bandwidthDescription(bytesRead));        } else {          LOG.warn("0 bytes returned by read() operation #{}",reads);        }      }      blockTimer.end("Reading block %d in %d reads",blockId,reads);      String bw=blockTimer.bandwidthDescription(blockSize);
        count+=bytesRead;        readTimer.end();        if (bytesRead != 0) {          LOG.debug("Bytes in read #{}: {} , block bytes: {}," + " remaining in block: {}" + " duration={} nS; ns/byte: {}, bandwidth={} MB/s",reads,bytesRead,blockSize - remaining,remaining,readTimer.duration(),readTimer.nanosPerOperation(bytesRead),readTimer.bandwidthDescription(bytesRead));        } else {          LOG.warn("0 bytes returned by read() operation #{}",reads);        }      }      blockTimer.end("Reading block %d in %d reads",blockId,reads);      String bw=blockTimer.bandwidthDescription(blockSize);      LOG.info("Bandwidth of block {}: {} MB/s: ",blockId,bw);      if (bandwidthInBytes(blockTimer,blockSize) < minimumBandwidth) {        LOG.warn("Bandwidth {} too low on block {}: resetting connection",bw,blockId);        Assert.assertTrue("Bandwidth of " + bw + " too low after "+ resetCount+ " attempts",resetCount <= maxResetCount);        resetCount++;      }    }  }  finally {    IOUtils.closeStream(in);
@Test public void test_100_renameHugeFile() throws Throwable {  assumeHugeFileExists();  describe("renaming %s to %s",hugefile,hugefileRenamed);  NativeAzureFileSystem fs=getFileSystem();  FileStatus status=fs.getFileStatus(hugefile);  long filesize=status.getLen();  fs.delete(hugefileRenamed,false);  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  fs.rename(hugefile,hugefileRenamed);  long mb=Math.max(filesize / S_1M,1);  timer.end("time to rename file of %d MB",mb);
  FileStatus status=fs.getFileStatus(hugefile);  long filesize=status.getLen();  fs.delete(hugefileRenamed,false);  ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();  fs.rename(hugefile,hugefileRenamed);  long mb=Math.max(filesize / S_1M,1);  timer.end("time to rename file of %d MB",mb);  LOG.info("Time per MB to rename = {} nS",toHuman(timer.nanosPerOperation(mb)));  bandwidth(timer,filesize);  logFSState();  FileStatus destFileStatus=fs.getFileStatus(hugefileRenamed);  assertEquals(filesize,destFileStatus.getLen());  ContractTestUtils.NanoTimer timer2=new ContractTestUtils.NanoTimer();  fs.rename(hugefileRenamed,hugefile);  timer2.end("Renaming back");
  final int FILE_SIZE=1000;  getBandwidthGaugeUpdater().suppressAutoUpdate();  Date start=new Date();  OutputStream outputStream=getFileSystem().create(filePath);  outputStream.write(nonZeroByteArray(FILE_SIZE));  outputStream.close();  long uploadDurationMs=new Date().getTime() - start.getTime();  logOpResponseCount("Creating a 1K file",base);  base=assertWebResponsesInRange(base,2,15);  getBandwidthGaugeUpdater().triggerUpdate(true);  long bytesWritten=AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation());  assertTrue("The bytes written in the last second " + bytesWritten + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",bytesWritten > (FILE_SIZE / 2) && bytesWritten < (FILE_SIZE * 2));  long totalBytesWritten=AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());  assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));  long uploadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_UPLOAD_RATE);
  outputStream.write(nonZeroByteArray(FILE_SIZE));  outputStream.close();  long uploadDurationMs=new Date().getTime() - start.getTime();  logOpResponseCount("Creating a 1K file",base);  base=assertWebResponsesInRange(base,2,15);  getBandwidthGaugeUpdater().triggerUpdate(true);  long bytesWritten=AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation());  assertTrue("The bytes written in the last second " + bytesWritten + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",bytesWritten > (FILE_SIZE / 2) && bytesWritten < (FILE_SIZE * 2));  long totalBytesWritten=AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());  assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));  long uploadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_UPLOAD_RATE);  LOG.info("Upload rate: " + uploadRate + " bytes/second.");  long expectedRate=(FILE_SIZE * 1000L) / uploadDurationMs;  assertTrue("The upload rate " + uploadRate + " is below the expected range of around "+ expectedRate+ " bytes/second that the unit test observed. This should never be"+ " the case since the test underestimates the rate by looking at "+ " end-to-end time instead of just block upload time.",uploadRate >= expectedRate);  long uploadLatency=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_UPLOAD_LATENCY);
  InputStream inputStream=getFileSystem().open(filePath);  int count=0;  while (inputStream.read() >= 0) {    count++;  }  inputStream.close();  long downloadDurationMs=new Date().getTime() - start.getTime();  assertEquals(FILE_SIZE,count);  logOpResponseCount("Reading a 1K file",base);  base=assertWebResponsesInRange(base,1,10);  getBandwidthGaugeUpdater().triggerUpdate(false);  long totalBytesRead=AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());  assertEquals(FILE_SIZE,totalBytesRead);  long bytesRead=AzureMetricsTestUtil.getCurrentBytesRead(getInstrumentation());  assertTrue("The bytes read in the last second " + bytesRead + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",bytesRead > (FILE_SIZE / 2) && bytesRead < (FILE_SIZE * 2));  long downloadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_DOWNLOAD_RATE);
  }  inputStream.close();  long downloadDurationMs=new Date().getTime() - start.getTime();  assertEquals(FILE_SIZE,count);  logOpResponseCount("Reading a 1K file",base);  base=assertWebResponsesInRange(base,1,10);  getBandwidthGaugeUpdater().triggerUpdate(false);  long totalBytesRead=AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());  assertEquals(FILE_SIZE,totalBytesRead);  long bytesRead=AzureMetricsTestUtil.getCurrentBytesRead(getInstrumentation());  assertTrue("The bytes read in the last second " + bytesRead + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",bytesRead > (FILE_SIZE / 2) && bytesRead < (FILE_SIZE * 2));  long downloadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_DOWNLOAD_RATE);  LOG.info("Download rate: " + downloadRate + " bytes/second.");  expectedRate=(FILE_SIZE * 1000L) / downloadDurationMs;  assertTrue("The download rate " + downloadRate + " is below the expected range of around "+ expectedRate+ " bytes/second that the unit test observed. This should never be"+ " the case since the test underestimates the rate by looking at "+ " end-to-end time instead of just block download time.",downloadRate >= expectedRate);  long downloadLatency=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_DOWNLOAD_LATENCY);
@Test public void testMetricsOnBigFileCreateRead() throws Exception {  long base=getBaseWebResponses();  assertEquals(0,AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation()));  Path filePath=new Path("/metricsTest_webResponses");  final int FILE_SIZE=100 * 1024 * 1024;  getBandwidthGaugeUpdater().suppressAutoUpdate();  OutputStream outputStream=getFileSystem().create(filePath);  outputStream.write(new byte[FILE_SIZE]);  outputStream.close();  logOpResponseCount("Creating a 100 MB file",base);  base=assertWebResponsesInRange(base,20,50);  getBandwidthGaugeUpdater().triggerUpdate(true);  long totalBytesWritten=AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());  assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));  long uploadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_UPLOAD_RATE);
  assertEquals(0,AzureMetricsTestUtil.getCurrentBytesWritten(getInstrumentation()));  Path filePath=new Path("/metricsTest_webResponses");  final int FILE_SIZE=100 * 1024 * 1024;  getBandwidthGaugeUpdater().suppressAutoUpdate();  OutputStream outputStream=getFileSystem().create(filePath);  outputStream.write(new byte[FILE_SIZE]);  outputStream.close();  logOpResponseCount("Creating a 100 MB file",base);  base=assertWebResponsesInRange(base,20,50);  getBandwidthGaugeUpdater().triggerUpdate(true);  long totalBytesWritten=AzureMetricsTestUtil.getCurrentTotalBytesWritten(getInstrumentation());  assertTrue("The total bytes written  " + totalBytesWritten + " is pretty far from the expected range of around "+ FILE_SIZE+ " bytes plus a little overhead.",totalBytesWritten >= FILE_SIZE && totalBytesWritten < (FILE_SIZE * 2));  long uploadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_UPLOAD_RATE);  LOG.info("Upload rate: " + uploadRate + " bytes/second.");  long uploadLatency=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_UPLOAD_LATENCY);
  long uploadLatency=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_UPLOAD_LATENCY);  LOG.info("Upload latency: " + uploadLatency);  assertTrue("The upload latency " + uploadLatency + " should be greater than zero now that I've just uploaded a file.",uploadLatency > 0);  InputStream inputStream=getFileSystem().open(filePath);  int count=0;  while (inputStream.read() >= 0) {    count++;  }  inputStream.close();  assertEquals(FILE_SIZE,count);  logOpResponseCount("Reading a 100 MB file",base);  base=assertWebResponsesInRange(base,20,40);  getBandwidthGaugeUpdater().triggerUpdate(false);  long totalBytesRead=AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());  assertEquals(FILE_SIZE,totalBytesRead);  long downloadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_DOWNLOAD_RATE);
  assertTrue("The upload latency " + uploadLatency + " should be greater than zero now that I've just uploaded a file.",uploadLatency > 0);  InputStream inputStream=getFileSystem().open(filePath);  int count=0;  while (inputStream.read() >= 0) {    count++;  }  inputStream.close();  assertEquals(FILE_SIZE,count);  logOpResponseCount("Reading a 100 MB file",base);  base=assertWebResponsesInRange(base,20,40);  getBandwidthGaugeUpdater().triggerUpdate(false);  long totalBytesRead=AzureMetricsTestUtil.getCurrentTotalBytesRead(getInstrumentation());  assertEquals(FILE_SIZE,totalBytesRead);  long downloadRate=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_DOWNLOAD_RATE);  LOG.info("Download rate: " + downloadRate + " bytes/second.");  long downloadLatency=AzureMetricsTestUtil.getLongGaugeValue(getInstrumentation(),WASB_DOWNLOAD_LATENCY);
private void logOpResponseCount(String opName,long base){
protected void describe(String text,Object... args){
@Test public void testSeekStatistics() throws IOException {  describe("Testing the values of statistics from seek operations in " + "AbfsInputStream");  AzureBlobFileSystem fs=getFileSystem();  AzureBlobFileSystemStore abfss=fs.getAbfsStore();  Path seekStatPath=path(getMethodName());  AbfsOutputStream out=null;  AbfsInputStream in=null;  try {    out=createAbfsOutputStreamWithFlushEnabled(fs,seekStatPath);    out.write(defBuffer);    out.hflush();    in=abfss.openFileForRead(seekStatPath,fs.getFsStatistics());    int result=in.read(defBuffer,0,ONE_MB);
  AzureBlobFileSystemStore abfss=fs.getAbfsStore();  Path seekStatPath=path(getMethodName());  AbfsOutputStream out=null;  AbfsInputStream in=null;  try {    out=createAbfsOutputStreamWithFlushEnabled(fs,seekStatPath);    out.write(defBuffer);    out.hflush();    in=abfss.openFileForRead(seekStatPath,fs.getFsStatistics());    int result=in.read(defBuffer,0,ONE_MB);    LOG.info("Result of read : {}",result);    for (int i=0; i < OPERATIONS; i++) {      in.seek(0);      in.seek(ONE_MB);    }    AbfsInputStreamStatisticsImpl stats=(AbfsInputStreamStatisticsImpl)in.getStreamStatistics();
    in=abfss.openFileForRead(seekStatPath,fs.getFsStatistics());    int result=in.read(defBuffer,0,ONE_MB);    LOG.info("Result of read : {}",result);    for (int i=0; i < OPERATIONS; i++) {      in.seek(0);      in.seek(ONE_MB);    }    AbfsInputStreamStatisticsImpl stats=(AbfsInputStreamStatisticsImpl)in.getStreamStatistics();    LOG.info("STATISTICS: {}",stats.toString());    assertEquals("Mismatch in seekOps value",2 * OPERATIONS,stats.getSeekOperations());    assertEquals("Mismatch in backwardSeekOps value",OPERATIONS,stats.getBackwardSeekOperations());    assertEquals("Mismatch in forwardSeekOps value",OPERATIONS,stats.getForwardSeekOperations());    assertEquals("Mismatch in bytesBackwardsOnSeek value",-1 * OPERATIONS * ONE_MB,stats.getBytesBackwardsOnSeek());    assertEquals("Mismatch in bytesSkippedOnSeek value",0,stats.getBytesSkippedOnSeek());    assertEquals("Mismatch in seekInBuffer value",2 * OPERATIONS,stats.getSeekInBuffer());    in.close();
@Test public void testReadStatistics() throws IOException {  describe("Testing the values of statistics from read operation in " + "AbfsInputStream");  AzureBlobFileSystem fs=getFileSystem();  AzureBlobFileSystemStore abfss=fs.getAbfsStore();  Path readStatPath=path(getMethodName());  AbfsOutputStream out=null;  AbfsInputStream in=null;  try {    out=createAbfsOutputStreamWithFlushEnabled(fs,readStatPath);    out.write(defBuffer);    out.hflush();    in=abfss.openFileForRead(readStatPath,fs.getFsStatistics());    for (int i=0; i < OPERATIONS; i++) {      in.read();    }    AbfsInputStreamStatisticsImpl stats=(AbfsInputStreamStatisticsImpl)in.getStreamStatistics();
  AbfsOutputStream out=null;  AbfsInputStream in=null;  try {    out=createAbfsOutputStreamWithFlushEnabled(fs,readStatPath);    out.write(defBuffer);    out.hflush();    in=abfss.openFileForRead(readStatPath,fs.getFsStatistics());    for (int i=0; i < OPERATIONS; i++) {      in.read();    }    AbfsInputStreamStatisticsImpl stats=(AbfsInputStreamStatisticsImpl)in.getStreamStatistics();    LOG.info("STATISTICS: {}",stats.toString());    assertEquals("Mismatch in bytesRead value",OPERATIONS,stats.getBytesRead());    assertEquals("Mismatch in readOps value",OPERATIONS,stats.getReadOperations());    assertEquals("Mismatch in remoteReadOps value",1,stats.getRemoteReadOperations());    in.close();
  describe("Testing AbfsInputStream operations with statistics as null");  AzureBlobFileSystem fs=getFileSystem();  Path nullStatFilePath=path(getMethodName());  byte[] oneKbBuff=new byte[ONE_KB];  AbfsInputStreamContext abfsInputStreamContext=new AbfsInputStreamContext(getConfiguration().getSasTokenRenewPeriodForStreamsInSeconds()).withReadBufferSize(getConfiguration().getReadBufferSize()).withReadAheadQueueDepth(getConfiguration().getReadAheadQueueDepth()).withStreamStatistics(null).build();  AbfsOutputStream out=null;  AbfsInputStream in=null;  try {    out=createAbfsOutputStreamWithFlushEnabled(fs,nullStatFilePath);    out.write(oneKbBuff);    out.hflush();    AbfsRestOperation abfsRestOperation=fs.getAbfsClient().getPathStatus(nullStatFilePath.toUri().getPath(),false);    in=new AbfsInputStream(fs.getAbfsClient(),null,nullStatFilePath.toUri().getPath(),ONE_KB,abfsInputStreamContext,abfsRestOperation.getResult().getResponseHeader("ETag"));    assertNotEquals("AbfsInputStream read() with null statistics should " + "work",-1,in.read());    in.seek(ONE_KB);
  AzureBlobFileSystem fs=getFileSystem();  Path getResponsePath=path(getMethodName());  Map<String,Long> metricMap;  String testResponseString="some response";  long getResponses, bytesReceived;  FSDataOutputStream out=null;  FSDataInputStream in=null;  try {    out=fs.create(getResponsePath);    out.write(testResponseString.getBytes());    out.hflush();    metricMap=fs.getInstrumentationMap();    long getResponsesBeforeTest=metricMap.get(CONNECTIONS_MADE.getStatName());    in=fs.open(getResponsePath);    int result=in.read();
  String testReadWriteOps="test this";  statistics.reset();  assertReadWriteOps("write",0,statistics.getWriteOps());  assertReadWriteOps("read",0,statistics.getReadOps());  FSDataOutputStream outForOneOperation=null;  FSDataInputStream inForOneOperation=null;  try {    outForOneOperation=fs.create(smallOperationsFile);    statistics.reset();    outForOneOperation.write(testReadWriteOps.getBytes());    assertReadWriteOps("write",1,statistics.getWriteOps());    outForOneOperation.hflush();    inForOneOperation=fs.open(smallOperationsFile);    statistics.reset();    int result=inForOneOperation.read(testReadWriteOps.getBytes(),0,testReadWriteOps.getBytes().length);
@Test @Ignore("HADOOP-16915") public void testRandomReadPerformance() throws Exception {  Assume.assumeFalse("This test does not support namespace enabled account",this.getFileSystem().getIsNamespaceEnabled());  createTestFile();  assumeHugeFileExists();  final AzureBlobFileSystem abFs=this.getFileSystem();  final NativeAzureFileSystem wasbFs=this.getWasbFileSystem();  final int maxAttempts=10;  final double maxAcceptableRatio=1.025;  double v1ElapsedMs=0, v2ElapsedMs=0;  double ratio=Double.MAX_VALUE;  for (int i=0; i < maxAttempts && ratio >= maxAcceptableRatio; i++) {    v1ElapsedMs=randomRead(1,wasbFs);    v2ElapsedMs=randomRead(2,abFs);    ratio=v2ElapsedMs / v1ElapsedMs;
private long sequentialRead(String version,FileSystem fs,boolean afterReverseSeek) throws IOException {  byte[] buffer=new byte[SEQUENTIAL_READ_BUFFER_SIZE];  long totalBytesRead=0;  long bytesRead=0;  try (FSDataInputStream inputStream=fs.open(TEST_FILE_PATH)){    if (afterReverseSeek) {      while (bytesRead > 0 && totalBytesRead < 4 * MEGABYTE) {        bytesRead=inputStream.read(buffer);        totalBytesRead+=bytesRead;      }      totalBytesRead=0;      inputStream.seek(0);    }    ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();    while ((bytesRead=inputStream.read(buffer)) > 0) {      totalBytesRead+=bytesRead;    }    long elapsedTimeMs=timer.elapsedTimeMs();
  assumeHugeFileExists();  final long minBytesToRead=2 * MEGABYTE;  Random random=new Random();  byte[] buffer=new byte[8 * KILOBYTE];  long totalBytesRead=0;  long bytesRead=0;  try (FSDataInputStream inputStream=fs.open(TEST_FILE_PATH)){    ContractTestUtils.NanoTimer timer=new ContractTestUtils.NanoTimer();    do {      bytesRead=inputStream.read(buffer);      totalBytesRead+=bytesRead;      inputStream.seek(random.nextInt((int)(TEST_FILE_SIZE - buffer.length)));    } while (bytesRead > 0 && totalBytesRead < minBytesToRead);    long elapsedTimeMs=timer.elapsedTimeMs();    inputStream.close();
@Override public void initialize(final Configuration configuration) throws IOException {  initialized=true;  owner=UserGroupInformation.getCurrentUser();
protected void innerBind(final URI uri,final Configuration conf) throws IOException {  Preconditions.checkState(initialized,"Not initialized");  Preconditions.checkState(fsURI == null,"already bound");  fsURI=uri;  canonicalServiceName=uri.toString();
public StubAbfsTokenIdentifier verifyCredentialsContainsToken(final Credentials credentials,final String serviceName,final String tokenService) throws IOException {  Token<? extends TokenIdentifier> token=credentials.getToken(new Text(serviceName));  assertEquals("Token Kind in " + token,StubAbfsTokenIdentifier.TOKEN_KIND,token.getKind());  assertEquals("Token Service Kind in " + token,tokenService,token.getService().toString());  StubAbfsTokenIdentifier abfsId=(StubAbfsTokenIdentifier)token.decodeIdentifier();
protected String dtutil(final int expected,final Configuration conf,final String... args) throws Exception {  final ByteArrayOutputStream dtUtilContent=new ByteArrayOutputStream();  DtUtilShell dt=new DtUtilShell();  dt.setOut(new PrintStream(dtUtilContent));  dtUtilContent.reset();  int r=doAs(aliceUser,() -> ToolRunner.run(conf,dt,args));  String s=dtUtilContent.toString();
@Test public void verifyGettingLatencyRecordsIsCheapWhenDisabled() throws Exception {  final double maxLatencyWhenDisabledMs=1000;  final double minLatencyWhenDisabledMs=0;  final long numTasks=1000;  long aggregateLatency=0;  AbfsPerfTracker abfsPerfTracker=new AbfsPerfTracker(accountName,filesystemName,false);  List<Callable<Long>> tasks=new ArrayList<>();  for (int i=0; i < numTasks; i++) {    tasks.add(() -> {      Instant startGet=Instant.now();      abfsPerfTracker.getClientLatency();      long latencyGet=Duration.between(startGet,Instant.now()).toMillis();
@Test public void verifyGettingLatencyRecordsIsCheapWhenEnabled() throws Exception {  final double maxLatencyWhenDisabledMs=5000;  final double minLatencyWhenDisabledMs=0;  final long numTasks=1000;  long aggregateLatency=0;  AbfsPerfTracker abfsPerfTracker=new AbfsPerfTracker(accountName,filesystemName,true);  List<Callable<Long>> tasks=new ArrayList<>();  for (int i=0; i < numTasks; i++) {    tasks.add(() -> {      Instant startRecord=Instant.now();      abfsPerfTracker.getClientLatency();      long latencyRecord=Duration.between(startRecord,Instant.now()).toMillis();
public void checkContainers() throws Throwable {  Assume.assumeTrue(this.getAuthType() == AuthType.SharedKey);  int count=0;  CloudStorageAccount storageAccount=AzureBlobStorageTestAccount.createTestAccount();  CloudBlobClient blobClient=storageAccount.createCloudBlobClient();  Iterable<CloudBlobContainer> containers=blobClient.listContainers(TEST_CONTAINER_PREFIX);  for (  CloudBlobContainer container : containers) {    count++;
public void deleteContainers() throws Throwable {  Assume.assumeTrue(this.getAuthType() == AuthType.SharedKey);  int count=0;  CloudStorageAccount storageAccount=AzureBlobStorageTestAccount.createTestAccount();  CloudBlobClient blobClient=storageAccount.createCloudBlobClient();  Iterable<CloudBlobContainer> containers=blobClient.listContainers(TEST_CONTAINER_PREFIX);  for (  CloudBlobContainer container : containers) {
    sb.append(path);  }  sb.append("\n");  sb.append("\n");  sb.append("\n");  sb.append("\n");  sb.append(sv);  sb.append("\n");  sb.append(sr);  sb.append("\n");  sb.append("\n");  sb.append("\n");  sb.append("\n");  sb.append("\n");  sb.append("\n");  String stringToSign=sb.toString();
  while (arg1.hasNext()) {    this.numOfValues+=1;    if (this.numOfValues % 100 == 0) {      reporter.setStatus("key: " + key.toString() + " numOfValues: "+ this.numOfValues);    }    if (this.numOfValues > this.maxNumOfValuesPerGroup) {      continue;    }    aRecord=((TaggedMapOutput)arg1.next()).clone(job);    Text tag=aRecord.getTag();    ResetableIterator data=retv.get(tag);    if (data == null) {      data=createResetableIterator();      retv.put(tag,data);    }    data.add(aRecord);  }  if (this.numOfValues > this.largestNumOfValues) {    this.largestNumOfValues=numOfValues;
    OptionsParser.usage();    return DistCpConstants.INVALID_ARGUMENT;  }  try {    context=new DistCpContext(OptionsParser.parse(argv));    checkSplitLargeFile();    setTargetPathExists();    LOG.info("Input Options: " + context);  } catch (  Throwable e) {    LOG.error("Invalid arguments: ",e);    System.err.println("Invalid arguments: " + e.getMessage());    OptionsParser.usage();    return DistCpConstants.INVALID_ARGUMENT;  }  try {    execute();  } catch (  InvalidInputException e) {
  try {    context=new DistCpContext(OptionsParser.parse(argv));    checkSplitLargeFile();    setTargetPathExists();    LOG.info("Input Options: " + context);  } catch (  Throwable e) {    LOG.error("Invalid arguments: ",e);    System.err.println("Invalid arguments: " + e.getMessage());    OptionsParser.usage();    return DistCpConstants.INVALID_ARGUMENT;  }  try {    execute();  } catch (  InvalidInputException e) {    LOG.error("Invalid input: ",e);    return DistCpConstants.INVALID_ARGUMENT;
    setTargetPathExists();    LOG.info("Input Options: " + context);  } catch (  Throwable e) {    LOG.error("Invalid arguments: ",e);    System.err.println("Invalid arguments: " + e.getMessage());    OptionsParser.usage();    return DistCpConstants.INVALID_ARGUMENT;  }  try {    execute();  } catch (  InvalidInputException e) {    LOG.error("Invalid input: ",e);    return DistCpConstants.INVALID_ARGUMENT;  }catch (  DuplicateFileException e) {    LOG.error("Duplicate files in input path: ",e);    return DistCpConstants.DUPLICATE_INPUT;
 catch (  Throwable e) {    LOG.error("Invalid arguments: ",e);    System.err.println("Invalid arguments: " + e.getMessage());    OptionsParser.usage();    return DistCpConstants.INVALID_ARGUMENT;  }  try {    execute();  } catch (  InvalidInputException e) {    LOG.error("Invalid input: ",e);    return DistCpConstants.INVALID_ARGUMENT;  }catch (  DuplicateFileException e) {    LOG.error("Duplicate files in input path: ",e);    return DistCpConstants.DUPLICATE_INPUT;  }catch (  AclsNotSupportedException e) {    LOG.error("ACLs not supported on at least one file system: ",e);
    OptionsParser.usage();    return DistCpConstants.INVALID_ARGUMENT;  }  try {    execute();  } catch (  InvalidInputException e) {    LOG.error("Invalid input: ",e);    return DistCpConstants.INVALID_ARGUMENT;  }catch (  DuplicateFileException e) {    LOG.error("Duplicate files in input path: ",e);    return DistCpConstants.DUPLICATE_INPUT;  }catch (  AclsNotSupportedException e) {    LOG.error("ACLs not supported on at least one file system: ",e);    return DistCpConstants.ACLS_NOT_SUPPORTED;  }catch (  XAttrsNotSupportedException e) {    LOG.error("XAttrs not supported on at least one file system: ",e);
  }  if (context.shouldAtomicCommit()) {    Path workDir=context.getAtomicWorkPath();    if (workDir == null) {      workDir=targetPath.getParent();    }    workDir=new Path(workDir,WIP_PREFIX + targetPath.getName() + rand.nextInt());    FileSystem workFS=workDir.getFileSystem(configuration);    if (!FileUtil.compareFs(targetFS,workFS)) {      throw new IllegalArgumentException("Work path " + workDir + " and target path "+ targetPath+ " are in different file system");    }    CopyOutputFormat.setWorkingDirectory(job,workDir);  } else {    CopyOutputFormat.setWorkingDirectory(job,targetPath);  }  CopyOutputFormat.setCommitDirectory(job,targetPath);  Path logPath=context.getLogPath();  if (logPath == null) {    logPath=new Path(metaFolder,"_logs");
@Override public boolean shouldCopy(Path path){  for (  Pattern filter : filters) {    if (filter.matcher(path.toString()).matches()) {
@VisibleForTesting protected void doBuildListingWithSnapshotDiff(SequenceFile.Writer fileListWriter,DistCpContext context) throws IOException {  ArrayList<DiffInfo> diffList=distCpSync.prepareDiffListForCopyListing();  Path sourceRoot=context.getSourcePaths().get(0);  FileSystem sourceFS=sourceRoot.getFileSystem(getConf());  try {    List<FileStatusInfo> fileStatuses=Lists.newArrayList();    for (    DiffInfo diff : diffList) {      diff.setTarget(new Path(context.getSourcePaths().get(0),diff.getTarget()));      if (diff.getType() == SnapshotDiffReport.DiffType.MODIFY) {        addToFileListing(fileListWriter,sourceRoot,diff.getTarget(),context);      } else       if (diff.getType() == SnapshotDiffReport.DiffType.CREATE) {        addToFileListing(fileListWriter,sourceRoot,diff.getTarget(),context);        FileStatus sourceStatus=sourceFS.getFileStatus(diff.getTarget());        if (sourceStatus.isDirectory()) {          if (LOG.isDebugEnabled()) {
      FileSystem sourceFS=path.getFileSystem(getConf());      final boolean preserveAcls=context.shouldPreserve(FileAttribute.ACL);      final boolean preserveXAttrs=context.shouldPreserve(FileAttribute.XATTR);      final boolean preserveRawXAttrs=context.shouldPreserveRawXattrs();      path=makeQualified(path);      FileStatus rootStatus=sourceFS.getFileStatus(path);      Path sourcePathRoot=computeSourceRootPath(rootStatus,context);      FileStatus[] sourceFiles=sourceFS.listStatus(path);      boolean explore=(sourceFiles != null && sourceFiles.length > 0);      if (!explore || rootStatus.isDirectory()) {        LinkedList<CopyListingFileStatus> rootCopyListingStatus=DistCpUtils.toCopyListingFileStatus(sourceFS,rootStatus,preserveAcls,preserveXAttrs,preserveRawXAttrs,context.getBlocksPerChunk());        writeToFileListingRoot(fileListWriter,rootCopyListingStatus,sourcePathRoot,context);      }      if (explore) {        ArrayList<FileStatus> sourceDirs=new ArrayList<FileStatus>();        for (        FileStatus sourceStatus : sourceFiles) {
      if (!explore || rootStatus.isDirectory()) {        LinkedList<CopyListingFileStatus> rootCopyListingStatus=DistCpUtils.toCopyListingFileStatus(sourceFS,rootStatus,preserveAcls,preserveXAttrs,preserveRawXAttrs,context.getBlocksPerChunk());        writeToFileListingRoot(fileListWriter,rootCopyListingStatus,sourcePathRoot,context);      }      if (explore) {        ArrayList<FileStatus> sourceDirs=new ArrayList<FileStatus>();        for (        FileStatus sourceStatus : sourceFiles) {          if (LOG.isDebugEnabled()) {            LOG.debug("Recording source-path: " + sourceStatus.getPath() + " for copy.");          }          LinkedList<CopyListingFileStatus> sourceCopyListingStatus=DistCpUtils.toCopyListingFileStatus(sourceFS,sourceStatus,preserveAcls && sourceStatus.isDirectory(),preserveXAttrs && sourceStatus.isDirectory(),preserveRawXAttrs && sourceStatus.isDirectory(),context.getBlocksPerChunk());          for (          CopyListingFileStatus fs : sourceCopyListingStatus) {            if (randomizeFileListing) {              addToFileListing(statusList,new FileStatusInfo(fs,sourcePathRoot),fileListWriter);            } else {              writeToFileListing(fileListWriter,fs,sourcePathRoot);            }          }          if (sourceStatus.isDirectory()) {
private void writeToFileListingRoot(SequenceFile.Writer fileListWriter,LinkedList<CopyListingFileStatus> fileStatus,Path sourcePathRoot,DistCpContext context) throws IOException {  boolean syncOrOverwrite=context.shouldSyncFolder() || context.shouldOverwrite();  for (  CopyListingFileStatus fs : fileStatus) {    if (fs.getPath().equals(sourcePathRoot) && fs.isDirectory() && syncOrOverwrite) {      if (LOG.isDebugEnabled()) {
private void writeToFileListing(SequenceFile.Writer fileListWriter,CopyListingFileStatus fileStatus,Path sourcePathRoot) throws IOException {  if (LOG.isDebugEnabled()) {
private void cleanup(Configuration conf){  Path metaFolder=new Path(conf.get(DistCpConstants.CONF_LABEL_META_FOLDER));  try {    FileSystem fs=metaFolder.getFileSystem(conf);
    return;  }  Path sourceListing=new Path(spath);  SequenceFile.Reader sourceReader=new SequenceFile.Reader(conf,SequenceFile.Reader.file(sourceListing));  Path targetRoot=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));  try {    CopyListingFileStatus srcFileStatus=new CopyListingFileStatus();    Text srcRelPath=new Text();    CopyListingFileStatus lastFileStatus=null;    LinkedList<Path> allChunkPaths=new LinkedList<Path>();    while (sourceReader.next(srcRelPath,srcFileStatus)) {      if (srcFileStatus.isDirectory()) {        continue;      }      Path targetFile=new Path(targetRoot.toString() + "/" + srcRelPath);      Path targetFileChunkPath=DistCpUtils.getSplitChunkPath(targetFile,srcFileStatus);      if (LOG.isDebugEnabled()) {
private void preserveFileAttributesForDirectories(Configuration conf) throws IOException {  String attrSymbols=conf.get(DistCpConstants.CONF_LABEL_PRESERVE_STATUS);  final boolean syncOrOverwrite=syncFolder || overwrite;
private void trackMissing(Configuration conf) throws IOException {  Path trackDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TRACK_MISSING));  Path sourceListing=new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));
private void trackMissing(Configuration conf) throws IOException {  Path trackDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TRACK_MISSING));  Path sourceListing=new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));  LOG.info("Tracking file changes to directory {}",trackDir);  Path sourceSortedListing=new Path(trackDir,DistCpConstants.SOURCE_SORTED_FILE);
private void trackMissing(Configuration conf) throws IOException {  Path trackDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TRACK_MISSING));  Path sourceListing=new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));  LOG.info("Tracking file changes to directory {}",trackDir);  Path sourceSortedListing=new Path(trackDir,DistCpConstants.SOURCE_SORTED_FILE);  LOG.info("Source listing {}",sourceSortedListing);  DistCpUtils.sortListing(conf,sourceListing,sourceSortedListing);  Path targetListing=new Path(trackDir,TARGET_LISTING_FILE);  Path sortedTargetListing=new Path(trackDir,TARGET_SORTED_FILE);  listTargetFiles(conf,targetListing,sortedTargetListing);
private void deleteMissing(Configuration conf) throws IOException {  LOG.info("-delete option is enabled. About to remove entries from " + "target that are missing in source");  long listingStart=System.currentTimeMillis();  Path sourceListing=new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));  FileSystem clusterFS=sourceListing.getFileSystem(conf);  Path sortedSourceListing=DistCpUtils.sortListing(conf,sourceListing);  long sourceListingCompleted=System.currentTimeMillis();
private void deleteMissing(Configuration conf) throws IOException {  LOG.info("-delete option is enabled. About to remove entries from " + "target that are missing in source");  long listingStart=System.currentTimeMillis();  Path sourceListing=new Path(conf.get(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH));  FileSystem clusterFS=sourceListing.getFileSystem(conf);  Path sortedSourceListing=DistCpUtils.sortListing(conf,sourceListing);  long sourceListingCompleted=System.currentTimeMillis();  LOG.info("Source listing completed in {}",formatDuration(sourceListingCompleted - listingStart));  Path targetListing=new Path(sourceListing.getParent(),"targetListing.seq");  Path sortedTargetListing=new Path(targetListing.toString() + "_sorted");  Path targetFinalPath=listTargetFiles(conf,targetListing,sortedTargetListing);  long totalLen=clusterFS.getFileStatus(sortedTargetListing).getLen();  SequenceFile.Reader sourceReader=new SequenceFile.Reader(conf,SequenceFile.Reader.file(sortedSourceListing));  SequenceFile.Reader targetReader=new SequenceFile.Reader(conf,SequenceFile.Reader.file(sortedTargetListing));  long deletionStart=System.currentTimeMillis();
  long skippedDeletes=0;  long deletedDirectories=0;  final DeletedDirTracker tracker=new DeletedDirTracker(1000);  try {    CopyListingFileStatus srcFileStatus=new CopyListingFileStatus();    Text srcRelPath=new Text();    CopyListingFileStatus trgtFileStatus=new CopyListingFileStatus();    Text trgtRelPath=new Text();    final FileSystem targetFS=targetFinalPath.getFileSystem(conf);    boolean showProgress;    boolean srcAvailable=sourceReader.next(srcRelPath,srcFileStatus);    while (targetReader.next(trgtRelPath,trgtFileStatus)) {      while (srcAvailable && trgtRelPath.compareTo(srcRelPath) > 0) {        srcAvailable=sourceReader.next(srcRelPath,srcFileStatus);      }      Path targetEntry=trgtFileStatus.getPath();
    CopyListingFileStatus trgtFileStatus=new CopyListingFileStatus();    Text trgtRelPath=new Text();    final FileSystem targetFS=targetFinalPath.getFileSystem(conf);    boolean showProgress;    boolean srcAvailable=sourceReader.next(srcRelPath,srcFileStatus);    while (targetReader.next(trgtRelPath,trgtFileStatus)) {      while (srcAvailable && trgtRelPath.compareTo(srcRelPath) > 0) {        srcAvailable=sourceReader.next(srcRelPath,srcFileStatus);      }      Path targetEntry=trgtFileStatus.getPath();      LOG.debug("Comparing {} and {}",srcFileStatus.getPath(),targetEntry);      if (srcAvailable && trgtRelPath.equals(srcRelPath))       continue;      if (tracker.shouldDelete(trgtFileStatus)) {        showProgress=true;        try {          if (targetFS.delete(targetEntry,true)) {
        srcAvailable=sourceReader.next(srcRelPath,srcFileStatus);      }      Path targetEntry=trgtFileStatus.getPath();      LOG.debug("Comparing {} and {}",srcFileStatus.getPath(),targetEntry);      if (srcAvailable && trgtRelPath.equals(srcRelPath))       continue;      if (tracker.shouldDelete(trgtFileStatus)) {        showProgress=true;        try {          if (targetFS.delete(targetEntry,true)) {            LOG.info("Deleted " + targetEntry + " - missing at source");            deletedEntries++;            if (trgtFileStatus.isDirectory()) {              deletedDirectories++;            } else {              filesDeleted++;            }          } else {
        showProgress=true;        try {          if (targetFS.delete(targetEntry,true)) {            LOG.info("Deleted " + targetEntry + " - missing at source");            deletedEntries++;            if (trgtFileStatus.isDirectory()) {              deletedDirectories++;            } else {              filesDeleted++;            }          } else {            LOG.info("delete({}) returned false ({})",targetEntry,trgtFileStatus);            missingDeletes++;          }        } catch (        IOException e) {          if (!ignoreFailures) {            throw e;
        try {          if (targetFS.delete(targetEntry,true)) {            LOG.info("Deleted " + targetEntry + " - missing at source");            deletedEntries++;            if (trgtFileStatus.isDirectory()) {              deletedDirectories++;            } else {              filesDeleted++;            }          } else {            LOG.info("delete({}) returned false ({})",targetEntry,trgtFileStatus);            missingDeletes++;          }        } catch (        IOException e) {          if (!ignoreFailures) {            throw e;          } else {
            deletedEntries++;            if (trgtFileStatus.isDirectory()) {              deletedDirectories++;            } else {              filesDeleted++;            }          } else {            LOG.info("delete({}) returned false ({})",targetEntry,trgtFileStatus);            missingDeletes++;          }        } catch (        IOException e) {          if (!ignoreFailures) {            throw e;          } else {            LOG.info("Failed to delete {}, ignoring exception {}",targetEntry,e.toString());            LOG.debug("Failed to delete {}",targetEntry,e);            failedDeletes++;
        } catch (        IOException e) {          if (!ignoreFailures) {            throw e;          } else {            LOG.info("Failed to delete {}, ignoring exception {}",targetEntry,e.toString());            LOG.debug("Failed to delete {}",targetEntry,e);            failedDeletes++;          }        }      } else {        LOG.debug("Skipping deletion of {}",targetEntry);        skippedDeletes++;        showProgress=false;      }      if (showProgress) {        taskAttemptContext.progress();        taskAttemptContext.setStatus("Deleting removed files from target. [" + targetReader.getPosition() * 100 / totalLen + "%]");      }    }    LOG.info("Completed deletion of files from {}",targetFS);
private Path listTargetFiles(final Configuration conf,final Path targetListing,final Path sortedTargetListing) throws IOException {  CopyListing target=new GlobbedCopyListing(new Configuration(conf),null);  Path targetFinalPath=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));  List<Path> targets=new ArrayList<>(1);  targets.add(targetFinalPath);  Path resultNonePath=Path.getPathWithoutSchemeAndAuthority(targetFinalPath).toString().startsWith(DistCpConstants.HDFS_RESERVED_RAW_DIRECTORY_NAME) ? DistCpConstants.RAW_NONE_PATH : DistCpConstants.NONE_PATH;  int threads=conf.getInt(DistCpConstants.CONF_LABEL_LISTSTATUS_THREADS,DistCpConstants.DEFAULT_LISTSTATUS_THREADS);
private void commitData(Configuration conf) throws IOException {  Path workDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));  Path finalDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));  FileSystem targetFS=workDir.getFileSystem(conf);
private void commitData(Configuration conf) throws IOException {  Path workDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));  Path finalDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));  FileSystem targetFS=workDir.getFileSystem(conf);  LOG.info("Atomic commit enabled. Moving " + workDir + " to "+ finalDir);  if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {
  Path workDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));  Path finalDir=new Path(conf.get(DistCpConstants.CONF_LABEL_TARGET_FINAL_PATH));  FileSystem targetFS=workDir.getFileSystem(conf);  LOG.info("Atomic commit enabled. Moving " + workDir + " to "+ finalDir);  if (targetFS.exists(finalDir) && targetFS.exists(workDir)) {    LOG.error("Pre-existing final-path found at: " + finalDir);    throw new IOException("Target-path can't be committed to because it " + "exists at " + finalDir + ". Copied data is in temp-dir: "+ workDir+ ". ");  }  boolean result=targetFS.rename(workDir,finalDir);  if (!result) {    LOG.warn("Rename failed. Perhaps data already moved. Verifying...");    result=targetFS.exists(finalDir) && !targetFS.exists(workDir);  }  if (result) {    LOG.info("Data committed successfully to " + finalDir);    taskAttemptContext.setStatus("Data committed successfully to " + finalDir);  } else {
  }  if (LOG.isDebugEnabled()) {    LOG.debug("concat " + targetFile + " allChunkSize+ "+ allChunkPaths.size());  }  FileSystem dstfs=targetFile.getFileSystem(conf);  FileSystem srcfs=sourceFile.getFileSystem(conf);  Path firstChunkFile=allChunkPaths.removeFirst();  Path[] restChunkFiles=new Path[allChunkPaths.size()];  allChunkPaths.toArray(restChunkFiles);  if (LOG.isDebugEnabled()) {    LOG.debug("concat: firstchunk: " + dstfs.getFileStatus(firstChunkFile));    int i=0;    for (    Path f : restChunkFiles) {      LOG.debug("concat: other chunk: " + i + ": "+ dstfs.getFileStatus(f));      ++i;    }  }  dstfs.concat(firstChunkFile,restChunkFiles);  if (LOG.isDebugEnabled()) {
@Override public void map(Text relPath,CopyListingFileStatus sourceFileStatus,Context context) throws IOException, InterruptedException {  Path sourcePath=sourceFileStatus.getPath();  if (LOG.isDebugEnabled())   LOG.debug("DistCpMapper::map(): Received " + sourcePath + ", "+ relPath);  Path target=new Path(targetWorkPath.makeQualified(targetFS.getUri(),targetFS.getWorkingDirectory()) + relPath.toString());  EnumSet<DistCpOptions.FileAttribute> fileAttributes=getFileAttributeSettings(context);  final boolean preserveRawXattrs=context.getConfiguration().getBoolean(DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS,false);  final String description="Copying " + sourcePath + " to "+ target;  context.setStatus(description);
    } catch (    FileNotFoundException e) {      throw new IOException(new RetriableFileCopyCommand.CopyReadException(e));    }    FileStatus targetStatus=null;    try {      targetStatus=targetFS.getFileStatus(target);    } catch (    FileNotFoundException ignore) {      if (LOG.isDebugEnabled())       LOG.debug("Path could not be found: " + target,ignore);    }    if (targetStatus != null && (targetStatus.isDirectory() != sourceCurrStatus.isDirectory())) {      throw new IOException("Can't replace " + target + ". Target is "+ getFileType(targetStatus)+ ", Source is "+ getFileType(sourceCurrStatus));    }    if (sourceCurrStatus.isDirectory()) {      createTargetDirsWithRetry(description,target,context,sourceStatus);      return;    }    FileAction action=checkUpdate(sourceFS,sourceCurrStatus,target,targetStatus);    Path tmpTarget=target;    if (action == FileAction.SKIP) {
      if (LOG.isDebugEnabled())       LOG.debug("Path could not be found: " + target,ignore);    }    if (targetStatus != null && (targetStatus.isDirectory() != sourceCurrStatus.isDirectory())) {      throw new IOException("Can't replace " + target + ". Target is "+ getFileType(targetStatus)+ ", Source is "+ getFileType(sourceCurrStatus));    }    if (sourceCurrStatus.isDirectory()) {      createTargetDirsWithRetry(description,target,context,sourceStatus);      return;    }    FileAction action=checkUpdate(sourceFS,sourceCurrStatus,target,targetStatus);    Path tmpTarget=target;    if (action == FileAction.SKIP) {      LOG.info("Skipping copy of " + sourceCurrStatus.getPath() + " to "+ target);      updateSkipCounters(context,sourceCurrStatus);      context.write(null,new Text("SKIP: " + sourceCurrStatus.getPath()));      if (verboseLog) {        context.write(null,new Text("FILE_SKIPPED: source=" + sourceFileStatus.getPath() + ", size="+ sourceFileStatus.getLen()+ " --> "+ "target="+ target+ ", size="+ (targetStatus == null ? 0 : targetStatus.getLen())));      }    } else {
private void handleFailures(IOException exception,CopyListingFileStatus sourceFileStatus,Path target,Context context) throws IOException, InterruptedException {
private long doCopy(CopyListingFileStatus source,Path target,Mapper.Context context,EnumSet<FileAttribute> fileAttributes,FileStatus sourceStatus) throws IOException {
private long doCopy(CopyListingFileStatus source,Path target,Mapper.Context context,EnumSet<FileAttribute> fileAttributes,FileStatus sourceStatus) throws IOException {  LOG.info("Copying {} to {}",source.getPath(),target);  final boolean toAppend=action == FileAction.APPEND;  final boolean useTempTarget=!toAppend && !directWrite;  Path targetPath=useTempTarget ? getTempFile(target,context) : target;
  final boolean toAppend=action == FileAction.APPEND;  final boolean useTempTarget=!toAppend && !directWrite;  Path targetPath=useTempTarget ? getTempFile(target,context) : target;  LOG.info("Writing to {} target file path {}",useTempTarget ? "temporary" : "direct",targetPath);  final Configuration configuration=context.getConfiguration();  FileSystem targetFS=target.getFileSystem(configuration);  try {    final Path sourcePath=source.getPath();    final FileSystem sourceFS=sourcePath.getFileSystem(configuration);    final FileChecksum sourceChecksum=fileAttributes.contains(FileAttribute.CHECKSUMTYPE) ? sourceFS.getFileChecksum(sourcePath) : null;    long offset=(action == FileAction.APPEND) ? targetFS.getFileStatus(target).getLen() : source.getChunkOffset();    long bytesRead=copyToFile(targetPath,targetFS,source,offset,context,fileAttributes,sourceChecksum,sourceStatus);    if (!source.isSplit()) {      DistCpUtils.compareFileLengthsAndChecksums(source.getLen(),sourceFS,sourcePath,sourceChecksum,targetFS,targetPath,skipCrc,source.getLen());    }    if (useTempTarget) {
private Path getTempFile(Path target,Mapper.Context context){  Path targetWorkPath=new Path(context.getConfiguration().get(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH));  Path root=target.equals(targetWorkPath) ? targetWorkPath.getParent() : targetWorkPath;  Path tempFile=new Path(root,".distcp.tmp." + context.getTaskAttemptID().toString() + "."+ String.valueOf(System.currentTimeMillis()));
private List<InputSplit> getSplits(Configuration configuration,int numSplits,long totalSizeBytes) throws IOException {  List<InputSplit> splits=new ArrayList<InputSplit>(numSplits);  long nBytesPerSplit=(long)Math.ceil(totalSizeBytes * 1.0 / numSplits);  CopyListingFileStatus srcFileStatus=new CopyListingFileStatus();  Text srcRelPath=new Text();  long currentSplitSize=0;  long lastSplitStart=0;  long lastPosition=0;  final Path listingFilePath=getListingFilePath(configuration);  if (LOG.isDebugEnabled()) {
  CopyListingFileStatus srcFileStatus=new CopyListingFileStatus();  Text srcRelPath=new Text();  long currentSplitSize=0;  long lastSplitStart=0;  long lastPosition=0;  final Path listingFilePath=getListingFilePath(configuration);  if (LOG.isDebugEnabled()) {    LOG.debug("Average bytes per map: " + nBytesPerSplit + ", Number of maps: "+ numSplits+ ", total size: "+ totalSizeBytes);  }  SequenceFile.Reader reader=null;  try {    reader=getListingFileReader(configuration);    while (reader.next(srcRelPath,srcFileStatus)) {      if (currentSplitSize + srcFileStatus.getChunkLength() > nBytesPerSplit && lastPosition != 0) {        FileSplit split=new FileSplit(listingFilePath,lastSplitStart,lastPosition - lastSplitStart,null);        if (LOG.isDebugEnabled()) {
  SequenceFile.Reader reader=null;  try {    reader=getListingFileReader(configuration);    while (reader.next(srcRelPath,srcFileStatus)) {      if (currentSplitSize + srcFileStatus.getChunkLength() > nBytesPerSplit && lastPosition != 0) {        FileSplit split=new FileSplit(listingFilePath,lastSplitStart,lastPosition - lastSplitStart,null);        if (LOG.isDebugEnabled()) {          LOG.debug("Creating split : " + split + ", bytes in split: "+ currentSplitSize);        }        splits.add(split);        lastSplitStart=lastPosition;        currentSplitSize=0;      }      currentSplitSize+=srcFileStatus.getChunkLength();      lastPosition=reader.getPosition();    }    if (lastPosition > lastSplitStart) {      FileSplit split=new FileSplit(listingFilePath,lastSplitStart,lastPosition - lastSplitStart,null);
public void release() throws IOException {  close();  if (!chunkContext.getFs().delete(chunkFilePath,false)) {
public DynamicInputChunk acquire(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {  String taskId=taskAttemptContext.getTaskAttemptID().getTaskID().toString();  Path acquiredFilePath=new Path(getChunkRootPath(),taskId);  if (fs.exists(acquiredFilePath)) {
@Override public List<InputSplit> getSplits(JobContext jobContext) throws IOException, InterruptedException {
public static LinkedList<CopyListingFileStatus> toCopyListingFileStatus(FileSystem fileSystem,FileStatus fileStatus,boolean preserveAcls,boolean preserveXAttrs,boolean preserveRawXAttrs,int blocksPerChunk) throws IOException {  LinkedList<CopyListingFileStatus> copyListingFileStatus=new LinkedList<CopyListingFileStatus>();  final CopyListingFileStatus clfs=toCopyListingFileStatusHelper(fileSystem,fileStatus,preserveAcls,preserveXAttrs,preserveRawXAttrs,0,fileStatus.getLen());  final long blockSize=fileStatus.getBlockSize();  if (LOG.isDebugEnabled()) {
    long curPos=0;    if (numBlocks <= blocksPerChunk) {      if (LOG.isDebugEnabled()) {        LOG.debug("  add file " + clfs);      }      copyListingFileStatus.add(clfs);    } else {      int i=0;      while (i < numBlocks) {        long curLength=0;        for (int j=0; j < blocksPerChunk && i < numBlocks; ++j, ++i) {          curLength+=blockLocations[i].getLength();        }        if (curLength > 0) {          CopyListingFileStatus clfs1=new CopyListingFileStatus(clfs);          clfs1.setChunkOffset(curPos);          clfs1.setChunkLength(curLength);
      copyListingFileStatus.add(clfs);    } else {      int i=0;      while (i < numBlocks) {        long curLength=0;        for (int j=0; j < blocksPerChunk && i < numBlocks; ++j, ++i) {          curLength+=blockLocations[i].getLength();        }        if (curLength > 0) {          CopyListingFileStatus clfs1=new CopyListingFileStatus(clfs);          clfs1.setChunkOffset(curPos);          clfs1.setChunkLength(curLength);          if (LOG.isDebugEnabled()) {            LOG.debug("  add file chunk " + clfs1);          }          copyListingFileStatus.add(clfs1);          curPos+=curLength;
    try {      validatePaths(new DistCpContext(options));      Assert.fail("Invalid inputs accepted");    } catch (    InvalidInputException ignore) {    }    TestDistCpUtils.delete(fs,"/tmp");    srcPaths.clear();    srcPaths.add(new Path("/tmp/in/1"));    fs.mkdirs(new Path("/tmp/in/1"));    fs.create(target).close();    try {      validatePaths(new DistCpContext(options));      Assert.fail("Invalid inputs accepted");    } catch (    InvalidInputException ignore) {    }    TestDistCpUtils.delete(fs,"/tmp");  } catch (  IOException e) {
    fs=FileSystem.get(getConf());    List<Path> srcPaths=new ArrayList<Path>();    srcPaths.add(new Path("/tmp/in/*/*"));    TestDistCpUtils.createFile(fs,"/tmp/in/src1/1.txt");    TestDistCpUtils.createFile(fs,"/tmp/in/src2/1.txt");    Path target=new Path("/tmp/out");    Path listingFile=new Path("/tmp/list");    final DistCpOptions options=new DistCpOptions.Builder(srcPaths,target).build();    final DistCpContext context=new DistCpContext(options);    CopyListing listing=CopyListing.getCopyListing(getConf(),CREDENTIALS,context);    try {      listing.buildListing(listingFile,context);      Assert.fail("Duplicates not detected");    } catch (    DuplicateFileException ignore) {    }  } catch (  IOException e) {
    final DistCpOptions options=new DistCpOptions.Builder(srcPaths,target).withSyncFolder(true).build();    CopyListing listing=new SimpleCopyListing(getConf(),CREDENTIALS);    try {      listing.buildListing(listingFile,new DistCpContext(options));      Assert.fail("Duplicates not detected");    } catch (    DuplicateFileException ignore) {    }    assertThat(listing.getBytesToCopy()).isEqualTo(10);    assertThat(listing.getNumberOfPaths()).isEqualTo(3);    TestDistCpUtils.delete(fs,"/tmp");    try {      listing.buildListing(listingFile,new DistCpContext(options));      Assert.fail("Invalid input not detected");    } catch (    InvalidInputException ignore) {    }    TestDistCpUtils.delete(fs,"/tmp");  } catch (  IOException e) {
private void validateFinalListing(Path pathToListFile,List<Path> srcFiles) throws IOException {  FileSystem fs=pathToListFile.getFileSystem(config);  try (SequenceFile.Reader reader=new SequenceFile.Reader(config,SequenceFile.Reader.file(pathToListFile))){    CopyListingFileStatus currentVal=new CopyListingFileStatus();    Text currentKey=new Text();    int idx=0;    while (reader.next(currentKey)) {      reader.getCurrentValue(currentVal);      Assert.assertEquals("srcFiles.size=" + srcFiles.size() + ", idx="+ idx,fs.makeQualified(srcFiles.get(idx)),currentVal.getPath());      if (LOG.isDebugEnabled()) {
    TestDistCpUtils.createFile(fs,decoyFile.toString());    TestDistCpUtils.createFile(fs,targetFile.toString());    List<Path> srcPaths=new ArrayList<Path>();    srcPaths.add(sourceFile);    DistCpOptions options=new DistCpOptions.Builder(srcPaths,targetFile).build();    CopyListing listing=new SimpleCopyListing(getConf(),CREDENTIALS);    final Path listFile=new Path(testRoot,"/tmp/fileList.seq");    listing.buildListing(listFile,new DistCpContext(options));    reader=new SequenceFile.Reader(getConf(),SequenceFile.Reader.file(listFile));    CopyListingFileStatus fileStatus=new CopyListingFileStatus();    Text relativePath=new Text();    Assert.assertTrue(reader.next(relativePath,fileStatus));    Assert.assertTrue(relativePath.toString().equals(""));  } catch (  Exception e) {    Assert.fail("Unexpected exception encountered.");
private void compareFiles(FileSystem fs,FileStatus srcStat,FileStatus dstStat) throws Exception {
  FSDataInputStream srcIn=fs.open(srcStat.getPath());  FSDataInputStream dstIn=fs.open(dstStat.getPath());  try {    byte[] readSrc=new byte[(int)HdfsClientConfigKeys.DFS_BLOCK_SIZE_DEFAULT];    byte[] readDst=new byte[(int)HdfsClientConfigKeys.DFS_BLOCK_SIZE_DEFAULT];    int srcBytesRead=0, tgtBytesRead=0;    int srcIdx=0, tgtIdx=0;    long totalComparedBytes=0;    while (true) {      if (srcBytesRead == 0) {        srcBytesRead=srcIn.read(readSrc);        srcIdx=0;      }      if (tgtBytesRead == 0) {        tgtBytesRead=dstIn.read(readDst);        tgtIdx=0;
      }      if (tgtBytesRead == 0) {        tgtBytesRead=dstIn.read(readDst);        tgtIdx=0;      }      if (srcBytesRead == 0 || tgtBytesRead == 0) {        LOG.info("______ compared src and dst files for " + totalComparedBytes + " bytes, content match.");        if (srcBytesRead != tgtBytesRead) {          Assert.fail("Read mismatching size, compared " + totalComparedBytes + " bytes between src and dst file "+ srcStat+ " and "+ dstStat);        }        if (totalComparedBytes != srcStat.getLen()) {          Assert.fail("Only read/compared " + totalComparedBytes + " bytes between src and dst file "+ srcStat+ " and "+ dstStat);        } else {          break;        }      }      for (; srcIdx < srcBytesRead && tgtIdx < tgtBytesRead; ++srcIdx, ++tgtIdx) {        if (readSrc[srcIdx] != readDst[tgtIdx]) {          Assert.fail("src and dst file does not match at " + totalComparedBytes + " between "+ srcStat+ " and "+ dstStat);        }        ++totalComparedBytes;
      if (tgtBytesRead == 0) {        tgtBytesRead=dstIn.read(readDst);        tgtIdx=0;      }      if (srcBytesRead == 0 || tgtBytesRead == 0) {        LOG.info("______ compared src and dst files for " + totalComparedBytes + " bytes, content match.");        if (srcBytesRead != tgtBytesRead) {          Assert.fail("Read mismatching size, compared " + totalComparedBytes + " bytes between src and dst file "+ srcStat+ " and "+ dstStat);        }        if (totalComparedBytes != srcStat.getLen()) {          Assert.fail("Only read/compared " + totalComparedBytes + " bytes between src and dst file "+ srcStat+ " and "+ dstStat);        } else {          break;        }      }      for (; srcIdx < srcBytesRead && tgtIdx < tgtBytesRead; ++srcIdx, ++tgtIdx) {        if (readSrc[srcIdx] != readDst[tgtIdx]) {          Assert.fail("src and dst file does not match at " + totalComparedBytes + " between "+ srcStat+ " and "+ dstStat);        }        ++totalComparedBytes;
private void copyAndVerify(final DistributedFileSystem fs,final FileEntry[] srcFiles,final FileStatus[] srcStats,final String testDst,final String[] args) throws Exception {  final String testRoot="/testdir";  FsShell shell=new FsShell(fs.getConf());  LOG.info("ls before distcp");
private void copyAndVerify(final DistributedFileSystem fs,final FileEntry[] srcFiles,final FileStatus[] srcStats,final String testDst,final String[] args) throws Exception {  final String testRoot="/testdir";  FsShell shell=new FsShell(fs.getConf());  LOG.info("ls before distcp");  LOG.info(execCmd(shell,"-lsr",testRoot));  LOG.info("_____ running distcp: " + args[0] + " "+ args[1]);  ToolRunner.run(conf,new DistCp(),args);  LOG.info("ls after distcp");
  byte[] contents2="contents2".getBytes();  Assert.assertEquals(contents1.length,contents2.length);  try {    addEntries(listFile,"srcdir");    createWithContents("srcdir/file1",contents1);    createWithContents("dstdir/file1",contents2);    Path target=new Path(root + "/dstdir");    runTest(listFile,target,false,false,false,true);    checkResult(target,1,"file1");    FSDataInputStream is=fs.open(new Path(root + "/dstdir/file1"));    byte[] dstContents=new byte[contents1.length];    is.readFully(dstContents);    is.close();    Assert.assertArrayEquals(contents1,dstContents);  } catch (  IOException e) {
protected Job distCpUpdateDeepDirectoryStructure(final Path destDir) throws Exception {  describe("Now do an incremental update with deletion of missing files");  Path srcDir=inputDir;
  ContractTestUtils.assertIsFile(remoteFS,outputFileNew1);  ContractTestUtils.assertPathExists(localFS,"tracking directory",trackDir);  Path sortedSourceListing=new Path(trackDir,DistCpConstants.SOURCE_SORTED_FILE);  ContractTestUtils.assertIsFile(localFS,sortedSourceListing);  Path sortedTargetListing=new Path(trackDir,DistCpConstants.TARGET_SORTED_FILE);  ContractTestUtils.assertIsFile(localFS,sortedTargetListing);  ContractTestUtils.assertPathsExist(remoteFS,"DistCP should have retained",outputFile2,outputFile3,outputFile4,outputSubDir4);  Map<String,Path> sourceFiles=new HashMap<>(10);  Map<String,Path> targetFiles=new HashMap<>(10);  try (SequenceFile.Reader sourceReader=new SequenceFile.Reader(conf,SequenceFile.Reader.file(sortedSourceListing));SequenceFile.Reader targetReader=new SequenceFile.Reader(conf,SequenceFile.Reader.file(sortedTargetListing))){    CopyListingFileStatus copyStatus=new CopyListingFileStatus();    Text name=new Text();    while (sourceReader.next(name,copyStatus)) {      String key=name.toString();      Path path=copyStatus.getPath();
  ContractTestUtils.assertIsFile(localFS,sortedTargetListing);  ContractTestUtils.assertPathsExist(remoteFS,"DistCP should have retained",outputFile2,outputFile3,outputFile4,outputSubDir4);  Map<String,Path> sourceFiles=new HashMap<>(10);  Map<String,Path> targetFiles=new HashMap<>(10);  try (SequenceFile.Reader sourceReader=new SequenceFile.Reader(conf,SequenceFile.Reader.file(sortedSourceListing));SequenceFile.Reader targetReader=new SequenceFile.Reader(conf,SequenceFile.Reader.file(sortedTargetListing))){    CopyListingFileStatus copyStatus=new CopyListingFileStatus();    Text name=new Text();    while (sourceReader.next(name,copyStatus)) {      String key=name.toString();      Path path=copyStatus.getPath();      LOG.info("{}: {}",key,path);      sourceFiles.put(key,path);    }    while (targetReader.next(name,copyStatus)) {      String key=name.toString();      Path path=copyStatus.getPath();
public void lsR(final String description,final FileSystem fs,final Path dir) throws IOException {  RemoteIterator<LocatedFileStatus> files=fs.listFiles(dir,true);
    CopyListing listing=new GlobbedCopyListing(conf,CREDENTIALS);    Path listingFile=new Path("/tmp1/" + String.valueOf(rand.nextLong()));    listing.buildListing(listingFile,context);    conf.set(CONF_LABEL_TARGET_WORK_PATH,targetBase);    conf.set(CONF_LABEL_TARGET_FINAL_PATH,targetBase);    OutputCommitter committer=new CopyCommitter(null,taskAttemptContext);    try {      committer.commitJob(jobContext);      if (!skipCrc) {        Assert.fail("Expected commit to fail");      }      Path sourcePath=new Path(sourceBase + srcFilename);      CopyListingFileStatus sourceCurrStatus=new CopyListingFileStatus(fs.getFileStatus(sourcePath));      Assert.assertFalse(DistCpUtils.checksumsAreEqual(fs,new Path(sourceBase + srcFilename),null,fs,new Path(targetBase + srcFilename),sourceCurrStatus.getLen()));    } catch (    IOException exception) {      if (skipCrc) {
protected static Configuration getConfigurationForCluster() throws IOException {  Configuration configuration=new Configuration();  System.setProperty("test.build.data","target/tmp/build/TEST_COPY_MAPPER/data");  configuration.set("hadoop.log.dir","target/tmp");  configuration.set("dfs.namenode.fs-limits.min-block-size","0");
protected static Configuration getConfigurationForCluster() throws IOException {  Configuration configuration=new Configuration();  System.setProperty("test.build.data","target/tmp/build/TEST_COPY_MAPPER/data");  configuration.set("hadoop.log.dir","target/tmp");  configuration.set("dfs.namenode.fs-limits.min-block-size","0");  LOG.debug("fs.default.name  == " + configuration.get("fs.default.name"));
  copyMapper.setup(context);  final Path path=new Path(SOURCE_PATH + "/1/3");  int manyBytes=100000000;  appendFile(path,manyBytes);  ScheduledExecutorService scheduledExecutorService=Executors.newSingleThreadScheduledExecutor();  Runnable task=new Runnable(){    public void run(){      try {        int maxAppendAttempts=20;        int appendCount=0;        while (appendCount < maxAppendAttempts) {          appendFile(path,1000);          Thread.sleep(200);          appendCount++;        }      } catch (      IOException|InterruptedException e) {
      try {        int maxAppendAttempts=20;        int appendCount=0;        while (appendCount < maxAppendAttempts) {          appendFile(path,1000);          Thread.sleep(200);          appendCount++;        }      } catch (      IOException|InterruptedException e) {        LOG.error("Exception encountered ",e);        Assert.fail("Test failed: " + e.getMessage());      }    }  };  scheduledExecutorService.schedule(task,10,TimeUnit.MILLISECONDS);  try {    copyMapper.map(new Text(DistCpUtils.getRelativePath(new Path(SOURCE_PATH),path)),new CopyListingFileStatus(cluster.getFileSystem().getFileStatus(path)),context);  } catch (  Exception ex) {
  try {    deleteState();    createSourceData();    FileSystem fs=cluster.getFileSystem();    CopyMapper copyMapper=new CopyMapper();    StubContext stubContext=new StubContext(getConfiguration(),null,0);    Mapper<Text,CopyListingFileStatus,Text,Text>.Context context=stubContext.getContext();    mkdirs(SOURCE_PATH + "/src/file");    touchFile(TARGET_PATH + "/src/file");    try {      copyMapper.setup(context);      copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(fs.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);    } catch (    IOException e) {      Assert.assertTrue(e.getMessage().startsWith("Can't replace"));    }  } catch (  Exception e) {
 catch (        Exception e) {          LOG.error("Exception encountered ",e);          throw new RuntimeException(e);        }      }    });    EnumSet<DistCpOptions.FileAttribute> preserveStatus=EnumSet.allOf(DistCpOptions.FileAttribute.class);    preserveStatus.remove(DistCpOptions.FileAttribute.ACL);    preserveStatus.remove(DistCpOptions.FileAttribute.XATTR);    context.getConfiguration().set(DistCpConstants.CONF_LABEL_PRESERVE_STATUS,DistCpUtils.packAttributes(preserveStatus));    touchFile(SOURCE_PATH + "/src/file");    mkdirs(TARGET_PATH);    cluster.getFileSystem().setPermission(new Path(TARGET_PATH),new FsPermission((short)511));    final FileSystem tmpFS=tmpUser.doAs(new PrivilegedAction<FileSystem>(){      @Override public FileSystem run(){        try {          return FileSystem.get(cluster.getConfiguration(0));
          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {          LOG.error("Exception encountered ",e);          Assert.fail("Test failed: " + e.getMessage());          throw new RuntimeException("Test ought to fail here");        }      }    });    tmpUser.doAs(new PrivilegedAction<Integer>(){      @Override public Integer run(){        try {          copyMapper.setup(context);          copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);          Assert.fail("Expected copy to fail");        } catch (        AccessControlException e) {          Assert.assertTrue("Got exception: " + e.getMessage(),true);        }catch (        Exception e) {
          StubContext stubContext=new StubContext(getConfiguration(),null,0);          return stubContext.getContext();        } catch (        Exception e) {          LOG.error("Exception encountered ",e);          throw new RuntimeException(e);        }      }    });    touchFile(SOURCE_PATH + "/src/file");    mkdirs(TARGET_PATH);    cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + "/src/file"),new FsPermission(FsAction.READ,FsAction.READ,FsAction.READ));    cluster.getFileSystem().setPermission(new Path(TARGET_PATH),new FsPermission((short)511));    final FileSystem tmpFS=tmpUser.doAs(new PrivilegedAction<FileSystem>(){      @Override public FileSystem run(){        try {          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {
    final FileSystem tmpFS=tmpUser.doAs(new PrivilegedAction<FileSystem>(){      @Override public FileSystem run(){        try {          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {          LOG.error("Exception encountered ",e);          Assert.fail("Test failed: " + e.getMessage());          throw new RuntimeException("Test ought to fail here");        }      }    });    tmpUser.doAs(new PrivilegedAction<Integer>(){      @Override public Integer run(){        try {          copyMapper.setup(context);          copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);        } catch (        Exception e) {
          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {          LOG.error("Exception encountered ",e);          Assert.fail("Test failed: " + e.getMessage());          throw new RuntimeException("Test ought to fail here");        }      }    });    tmpUser.doAs(new PrivilegedAction<Integer>(){      @Override public Integer run(){        try {          copyMapper.setup(context);          copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);          assertThat(stubContext.getWriter().values().size()).isEqualTo(1);          Assert.assertTrue(stubContext.getWriter().values().get(0).toString().startsWith("SKIP"));          Assert.assertTrue(stubContext.getWriter().values().get(0).toString().contains(SOURCE_PATH + "/src/file"));        } catch (        Exception e) {
          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {          LOG.error("Exception encountered ",e);          Assert.fail("Test failed: " + e.getMessage());          throw new RuntimeException("Test ought to fail here");        }      }    });    tmpUser.doAs(new PrivilegedAction<Integer>(){      @Override public Integer run(){        try {          copyMapper.setup(context);          copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);          Assert.fail("Didn't expect the file to be copied");        } catch (        AccessControlException ignore) {        }catch (        Exception e) {          if (e.getCause() == null || e.getCause().getCause() == null || !(e.getCause().getCause() instanceof AccessControlException)) {
  try {    deleteState();    createSourceData();    FileSystem fs=cluster.getFileSystem();    CopyMapper copyMapper=new CopyMapper();    StubContext stubContext=new StubContext(getConfiguration(),null,0);    Mapper<Text,CopyListingFileStatus,Text,Text>.Context context=stubContext.getContext();    touchFile(SOURCE_PATH + "/src/file");    mkdirs(TARGET_PATH + "/src/file");    try {      copyMapper.setup(context);      copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(fs.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);    } catch (    IOException e) {      Assert.assertTrue(e.getMessage().startsWith("Can't replace"));    }  } catch (  Exception e) {
          return stubContext.getContext();        } catch (        Exception e) {          LOG.error("Exception encountered when get stub context",e);          throw new RuntimeException(e);        }      }    });    touchFile(SOURCE_PATH + "/src/file");    mkdirs(TARGET_PATH);    cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + "/src/file"),new FsPermission(FsAction.NONE,FsAction.NONE,FsAction.NONE));    cluster.getFileSystem().setPermission(new Path(TARGET_PATH),new FsPermission((short)511));    context.getConfiguration().setBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(),ignoreFailures);    final FileSystem tmpFS=tmpUser.doAs(new PrivilegedAction<FileSystem>(){      @Override public FileSystem run(){        try {          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {
    mkdirs(TARGET_PATH);    cluster.getFileSystem().setPermission(new Path(SOURCE_PATH + "/src/file"),new FsPermission(FsAction.NONE,FsAction.NONE,FsAction.NONE));    cluster.getFileSystem().setPermission(new Path(TARGET_PATH),new FsPermission((short)511));    context.getConfiguration().setBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(),ignoreFailures);    final FileSystem tmpFS=tmpUser.doAs(new PrivilegedAction<FileSystem>(){      @Override public FileSystem run(){        try {          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {          LOG.error("Exception encountered when get FileSystem.",e);          throw new RuntimeException(e);        }      }    });    tmpUser.doAs(new PrivilegedAction<Integer>(){      @Override public Integer run(){        try {
    context.getConfiguration().setBoolean(DistCpOptionSwitch.IGNORE_FAILURES.getConfigLabel(),ignoreFailures);    final FileSystem tmpFS=tmpUser.doAs(new PrivilegedAction<FileSystem>(){      @Override public FileSystem run(){        try {          return FileSystem.get(cluster.getConfiguration(0));        } catch (        IOException e) {          LOG.error("Exception encountered when get FileSystem.",e);          throw new RuntimeException(e);        }      }    });    tmpUser.doAs(new PrivilegedAction<Integer>(){      @Override public Integer run(){        try {          copyMapper.setup(context);          copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);          Assert.assertTrue("Should have thrown an IOException if not " + "ignoring failures",ignoreFailures);
        } catch (        IOException e) {          LOG.error("Exception encountered when get FileSystem.",e);          throw new RuntimeException(e);        }      }    });    tmpUser.doAs(new PrivilegedAction<Integer>(){      @Override public Integer run(){        try {          copyMapper.setup(context);          copyMapper.map(new Text("/src/file"),new CopyListingFileStatus(tmpFS.getFileStatus(new Path(SOURCE_PATH + "/src/file"))),context);          Assert.assertTrue("Should have thrown an IOException if not " + "ignoring failures",ignoreFailures);        } catch (        IOException e) {          LOG.error("Unexpected exception encountered. ",e);          Assert.assertFalse("Should not have thrown an IOException if " + "ignoring failures",ignoreFailures);        }catch (        Exception e) {          LOG.error("Exception encountered when the mapper copies file.",e);
    }    job.getConfiguration().set(DistCpConstants.CONF_LABEL_TARGET_WORK_PATH,"");    CopyOutputFormat.setCommitDirectory(job,new Path("/tmp/commit"));    try {      JobContext context=new JobContextImpl(job.getConfiguration(),jobID);      outputFormat.checkOutputSpecs(context);      Assert.fail("No checking for invalid work path");    } catch (    IllegalStateException ignore) {    }    CopyOutputFormat.setWorkingDirectory(job,new Path("/tmp/work"));    CopyOutputFormat.setCommitDirectory(job,new Path("/tmp/commit"));    try {      JobContext context=new JobContextImpl(job.getConfiguration(),jobID);      outputFormat.checkOutputSpecs(context);    } catch (    IllegalStateException ignore) {      Assert.fail("Output spec check failed.");    }  } catch (  IOException e) {
protected void deletePaths(final List<CopyListingFileStatus> statusList,final AtomicInteger deletedFiles,final AtomicInteger deletedDirs){  for (  CopyListingFileStatus status : statusList) {    if (shouldDelete(status)) {      AtomicInteger r=status.isDirectory() ? deletedDirs : deletedFiles;      r.incrementAndGet();
private static Configuration getConfigurationForCluster(){  Configuration configuration=new Configuration();  System.setProperty("test.build.data","target/tmp/build/TEST_DYNAMIC_INPUT_FORMAT/data");  configuration.set("hadoop.log.dir","target/tmp");
private static Configuration getConfigurationForCluster(){  Configuration configuration=new Configuration();  System.setProperty("test.build.data","target/tmp/build/TEST_DYNAMIC_INPUT_FORMAT/data");  configuration.set("hadoop.log.dir","target/tmp");  LOG.debug("fs.default.name  == " + configuration.get("fs.default.name"));
  allTokens=ByteBuffer.wrap(dob.getData(),0,dob.getLength());  AMRMClientAsync.AbstractCallbackHandler allocListener=new RMCallbackHandler();  amRMClient=AMRMClientAsync.createAMRMClientAsync(1000,allocListener);  amRMClient.init(conf);  amRMClient.start();  containerListener=createNMCallbackHandler();  nmClientAsync=new NMClientAsyncImpl(containerListener);  nmClientAsync.init(conf);  nmClientAsync.start();  String appMasterHostname=NetUtils.getHostname();  amRMClient.registerApplicationMaster(appMasterHostname,-1,"");  Supplier<Boolean> exitCritera=this::isComplete;  Optional<Properties> namenodeProperties=Optional.empty();  if (launchNameNode) {    ContainerRequest nnContainerRequest=setupContainerAskForRM(amOptions.getNameNodeMemoryMB(),amOptions.getNameNodeVirtualCores(),0,amOptions.getNameNodeNodeLabelExpression());
  amRMClient.init(conf);  amRMClient.start();  containerListener=createNMCallbackHandler();  nmClientAsync=new NMClientAsyncImpl(containerListener);  nmClientAsync.init(conf);  nmClientAsync.start();  String appMasterHostname=NetUtils.getHostname();  amRMClient.registerApplicationMaster(appMasterHostname,-1,"");  Supplier<Boolean> exitCritera=this::isComplete;  Optional<Properties> namenodeProperties=Optional.empty();  if (launchNameNode) {    ContainerRequest nnContainerRequest=setupContainerAskForRM(amOptions.getNameNodeMemoryMB(),amOptions.getNameNodeVirtualCores(),0,amOptions.getNameNodeNodeLabelExpression());    LOG.info("Requested NameNode ask: " + nnContainerRequest.toString());    amRMClient.addContainerRequest(nnContainerRequest);    Path namenodeInfoPath=new Path(remoteStoragePath,DynoConstants.NN_INFO_FILE_NAME);
  String appMasterHostname=NetUtils.getHostname();  amRMClient.registerApplicationMaster(appMasterHostname,-1,"");  Supplier<Boolean> exitCritera=this::isComplete;  Optional<Properties> namenodeProperties=Optional.empty();  if (launchNameNode) {    ContainerRequest nnContainerRequest=setupContainerAskForRM(amOptions.getNameNodeMemoryMB(),amOptions.getNameNodeVirtualCores(),0,amOptions.getNameNodeNodeLabelExpression());    LOG.info("Requested NameNode ask: " + nnContainerRequest.toString());    amRMClient.addContainerRequest(nnContainerRequest);    Path namenodeInfoPath=new Path(remoteStoragePath,DynoConstants.NN_INFO_FILE_NAME);    LOG.info("Waiting on availability of NameNode information at " + namenodeInfoPath);    namenodeProperties=DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera,conf,namenodeInfoPath,LOG);    if (!namenodeProperties.isPresent()) {      cleanup();      return false;    }    namenodeServiceRpcAddress=DynoInfraUtils.getNameNodeServiceRpcAddr(namenodeProperties.get()).toString();
  if (launchNameNode) {    ContainerRequest nnContainerRequest=setupContainerAskForRM(amOptions.getNameNodeMemoryMB(),amOptions.getNameNodeVirtualCores(),0,amOptions.getNameNodeNodeLabelExpression());    LOG.info("Requested NameNode ask: " + nnContainerRequest.toString());    amRMClient.addContainerRequest(nnContainerRequest);    Path namenodeInfoPath=new Path(remoteStoragePath,DynoConstants.NN_INFO_FILE_NAME);    LOG.info("Waiting on availability of NameNode information at " + namenodeInfoPath);    namenodeProperties=DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera,conf,namenodeInfoPath,LOG);    if (!namenodeProperties.isPresent()) {      cleanup();      return false;    }    namenodeServiceRpcAddress=DynoInfraUtils.getNameNodeServiceRpcAddr(namenodeProperties.get()).toString();    LOG.info("NameNode information: " + namenodeProperties.get());    LOG.info("NameNode can be reached at: " + DynoInfraUtils.getNameNodeHdfsUri(namenodeProperties.get()).toString());    DynoInfraUtils.waitForNameNodeStartup(namenodeProperties.get(),exitCritera,LOG);  } else {
      cleanup();      return false;    }    namenodeServiceRpcAddress=DynoInfraUtils.getNameNodeServiceRpcAddr(namenodeProperties.get()).toString();    LOG.info("NameNode information: " + namenodeProperties.get());    LOG.info("NameNode can be reached at: " + DynoInfraUtils.getNameNodeHdfsUri(namenodeProperties.get()).toString());    DynoInfraUtils.waitForNameNodeStartup(namenodeProperties.get(),exitCritera,LOG);  } else {    LOG.info("Using remote NameNode with RPC address: " + namenodeServiceRpcAddress);  }  blockListFiles=Collections.synchronizedList(getDataNodeBlockListingFiles());  numTotalDataNodes=blockListFiles.size();  if (numTotalDataNodes == 0) {    LOG.error("No block listing files were found! Cannot run with 0 DataNodes.");    markCompleted();    return false;  }  numTotalDataNodeContainers=(int)Math.ceil(((double)numTotalDataNodes) / Math.max(1,amOptions.getDataNodesPerCluster()));
    LOG.info("NameNode information: " + namenodeProperties.get());    LOG.info("NameNode can be reached at: " + DynoInfraUtils.getNameNodeHdfsUri(namenodeProperties.get()).toString());    DynoInfraUtils.waitForNameNodeStartup(namenodeProperties.get(),exitCritera,LOG);  } else {    LOG.info("Using remote NameNode with RPC address: " + namenodeServiceRpcAddress);  }  blockListFiles=Collections.synchronizedList(getDataNodeBlockListingFiles());  numTotalDataNodes=blockListFiles.size();  if (numTotalDataNodes == 0) {    LOG.error("No block listing files were found! Cannot run with 0 DataNodes.");    markCompleted();    return false;  }  numTotalDataNodeContainers=(int)Math.ceil(((double)numTotalDataNodes) / Math.max(1,amOptions.getDataNodesPerCluster()));  LOG.info("Requesting {} DataNode containers with {} MB memory, {} vcores",numTotalDataNodeContainers,amOptions.getDataNodeMemoryMB(),amOptions.getDataNodeVirtualCores());  for (int i=0; i < numTotalDataNodeContainers; ++i) {    ContainerRequest datanodeAsk=setupContainerAskForRM(amOptions.getDataNodeMemoryMB(),amOptions.getDataNodeVirtualCores(),1,amOptions.getDataNodeNodeLabelExpression());
private List<LocalResource> getDataNodeBlockListingFiles() throws IOException {  Path blockListDirPath=new Path(System.getenv().get(DynoConstants.BLOCK_LIST_PATH_ENV));
public boolean run() throws IOException, YarnException {  LOG.info("Running Client");  yarnClient.start();  YarnClusterMetrics clusterMetrics=yarnClient.getYarnClusterMetrics();
public boolean run() throws IOException, YarnException {  LOG.info("Running Client");  yarnClient.start();  YarnClusterMetrics clusterMetrics=yarnClient.getYarnClusterMetrics();  LOG.info("Got Cluster metric info from ASM, numNodeManagers={}",clusterMetrics.getNumNodeManagers());  QueueInfo queueInfo=yarnClient.getQueueInfo(this.amQueue);
public boolean run() throws IOException, YarnException {  LOG.info("Running Client");  yarnClient.start();  YarnClusterMetrics clusterMetrics=yarnClient.getYarnClusterMetrics();  LOG.info("Got Cluster metric info from ASM, numNodeManagers={}",clusterMetrics.getNumNodeManagers());  QueueInfo queueInfo=yarnClient.getQueueInfo(this.amQueue);  LOG.info("Queue info: queueName={}, queueCurrentCapacity={}, " + "queueMaxCapacity={}, queueApplicationCount={}, " + "queueChildQueueCount={}",queueInfo.getQueueName(),queueInfo.getCurrentCapacity(),queueInfo.getMaximumCapacity(),queueInfo.getApplications().size(),queueInfo.getChildQueues().size());  YarnClientApplication app=yarnClient.createApplication();  GetNewApplicationResponse appResponse=app.getNewApplicationResponse();  long maxMem=appResponse.getMaximumResourceCapability().getMemorySize();
public boolean run() throws IOException, YarnException {  LOG.info("Running Client");  yarnClient.start();  YarnClusterMetrics clusterMetrics=yarnClient.getYarnClusterMetrics();  LOG.info("Got Cluster metric info from ASM, numNodeManagers={}",clusterMetrics.getNumNodeManagers());  QueueInfo queueInfo=yarnClient.getQueueInfo(this.amQueue);  LOG.info("Queue info: queueName={}, queueCurrentCapacity={}, " + "queueMaxCapacity={}, queueApplicationCount={}, " + "queueChildQueueCount={}",queueInfo.getQueueName(),queueInfo.getCurrentCapacity(),queueInfo.getMaximumCapacity(),queueInfo.getApplications().size(),queueInfo.getChildQueues().size());  YarnClientApplication app=yarnClient.createApplication();  GetNewApplicationResponse appResponse=app.getNewApplicationResponse();  long maxMem=appResponse.getMaximumResourceCapability().getMemorySize();  LOG.info("Max mem capabililty of resources in this cluster " + maxMem);  int maxVCores=appResponse.getMaximumResourceCapability().getVirtualCores();
  Resource capability=Records.newRecord(Resource.class);  capability.setMemorySize(amMemory);  capability.setVirtualCores(amVCores);  appContext.setResource(capability);  if (UserGroupInformation.isSecurityEnabled()) {    ByteBuffer fsTokens;    if (tokenFileLocation != null) {      fsTokens=ByteBuffer.wrap(Files.readAllBytes(Paths.get(tokenFileLocation)));    } else {      Credentials credentials=new Credentials();      String tokenRenewer=getConf().get(YarnConfiguration.RM_PRINCIPAL);      if (tokenRenewer == null || tokenRenewer.length() == 0) {        throw new IOException("Can't get Master Kerberos principal for the " + "RM to use as renewer");      }      final Token<?>[] tokens=fs.addDelegationTokens(tokenRenewer,credentials);      if (tokens != null) {
private List<String> getAMCommand(){  List<String> vargs=new ArrayList<>();  vargs.add(Environment.JAVA_HOME.$() + "/bin/java");  long appMasterHeapSize=Math.round(amMemory * 0.85);  vargs.add("-Xmx" + appMasterHeapSize + "m");  vargs.add(ApplicationMaster.class.getCanonicalName());  amOptions.addToVargs(vargs);  vargs.add("1>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout");  vargs.add("2>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr");
  String srcScheme=srcSchemes.iterator().next();  String srcPathString="[" + Joiner.on(",").join(srcPaths) + "]";  if (srcScheme == null || srcScheme.equals(FileSystem.getLocal(getConf()).getScheme()) || srcScheme.equals("jar")) {    List<File> srcFiles=srcURIs.stream().map(URI::getSchemeSpecificPart).map(File::new).collect(Collectors.toList());    Path dstPathBase=getRemoteStoragePath(getConf(),appId);    boolean shouldArchive=srcFiles.size() > 1 || srcFiles.get(0).isDirectory() || (resource.getType() == LocalResourceType.ARCHIVE && Arrays.stream(ARCHIVE_FILE_TYPES).noneMatch(suffix -> srcFiles.get(0).getName().endsWith(suffix)));    if (shouldArchive) {      if ("jar".equals(srcScheme)) {        throw new IllegalArgumentException(String.format("Resources in JARs " + "can't be zipped; resource %s is ARCHIVE and src is: %s",resource.getResourcePath(),srcPathString));      } else       if (resource.getType() != LocalResourceType.ARCHIVE) {        throw new IllegalArgumentException(String.format("Resource type is %s but srcPaths were: %s",resource.getType(),srcPathString));      }      dstPath=new Path(dstPathBase,resource.getResourcePath()).suffix(".zip");    } else {      dstPath=new Path(dstPathBase,srcFiles.get(0).getName());    }    FileSystem remoteFS=dstPath.getFileSystem(getConf());
 else       if (shouldArchive) {        List<File> filesToZip;        if (srcFiles.size() == 1 && srcFiles.get(0).isDirectory()) {          File[] childFiles=srcFiles.get(0).listFiles();          if (childFiles == null || childFiles.length == 0) {            throw new IllegalArgumentException("Specified a directory to archive with no contents");          }          filesToZip=Lists.newArrayList(childFiles);        } else {          filesToZip=srcFiles;        }        ZipOutputStream zout=new ZipOutputStream(outputStream);        for (        File fileToZip : filesToZip) {          addFileToZipRecursively(fileToZip.getParentFile(),fileToZip,zout);        }        zout.close();      } else {        try (InputStream inputStream=new FileInputStream(srcFiles.get(0))){
private boolean monitorInfraApplication() throws YarnException, IOException {  boolean loggedApplicationInfo=false;  boolean success=false;  Thread namenodeMonitoringThread=new Thread(() -> {    Supplier<Boolean> exitCritera=() -> Apps.isApplicationFinalState(infraAppState);    Optional<Properties> namenodeProperties=Optional.empty();    while (!exitCritera.get()) {      try {        if (!namenodeProperties.isPresent()) {          namenodeProperties=DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera,getConf(),getNameNodeInfoPath(),LOG);          if (namenodeProperties.isPresent()) {            Properties props=namenodeProperties.get();
private boolean monitorInfraApplication() throws YarnException, IOException {  boolean loggedApplicationInfo=false;  boolean success=false;  Thread namenodeMonitoringThread=new Thread(() -> {    Supplier<Boolean> exitCritera=() -> Apps.isApplicationFinalState(infraAppState);    Optional<Properties> namenodeProperties=Optional.empty();    while (!exitCritera.get()) {      try {        if (!namenodeProperties.isPresent()) {          namenodeProperties=DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera,getConf(),getNameNodeInfoPath(),LOG);          if (namenodeProperties.isPresent()) {            Properties props=namenodeProperties.get();            LOG.info("NameNode can be reached via HDFS at: {}",DynoInfraUtils.getNameNodeHdfsUri(props));
private boolean monitorInfraApplication() throws YarnException, IOException {  boolean loggedApplicationInfo=false;  boolean success=false;  Thread namenodeMonitoringThread=new Thread(() -> {    Supplier<Boolean> exitCritera=() -> Apps.isApplicationFinalState(infraAppState);    Optional<Properties> namenodeProperties=Optional.empty();    while (!exitCritera.get()) {      try {        if (!namenodeProperties.isPresent()) {          namenodeProperties=DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera,getConf(),getNameNodeInfoPath(),LOG);          if (namenodeProperties.isPresent()) {            Properties props=namenodeProperties.get();            LOG.info("NameNode can be reached via HDFS at: {}",DynoInfraUtils.getNameNodeHdfsUri(props));            LOG.info("NameNode web UI available at: {}",DynoInfraUtils.getNameNodeWebUri(props));
    while (!exitCritera.get()) {      try {        if (!namenodeProperties.isPresent()) {          namenodeProperties=DynoInfraUtils.waitForAndGetNameNodeProperties(exitCritera,getConf(),getNameNodeInfoPath(),LOG);          if (namenodeProperties.isPresent()) {            Properties props=namenodeProperties.get();            LOG.info("NameNode can be reached via HDFS at: {}",DynoInfraUtils.getNameNodeHdfsUri(props));            LOG.info("NameNode web UI available at: {}",DynoInfraUtils.getNameNodeWebUri(props));            LOG.info("NameNode can be tracked at: {}",DynoInfraUtils.getNameNodeTrackingUri(props));          } else {            break;          }        }        DynoInfraUtils.waitForNameNodeStartup(namenodeProperties.get(),exitCritera,LOG);        DynoInfraUtils.waitForNameNodeReadiness(namenodeProperties.get(),numTotalDataNodes,false,exitCritera,getConf(),LOG);        break;      } catch (      IOException ioe) {
      } catch (      IOException ioe) {        LOG.error("Unexpected exception while waiting for NameNode readiness",ioe);      }catch (      InterruptedException ie) {        return;      }    }    if (!Apps.isApplicationFinalState(infraAppState) && launchWorkloadJob) {      launchAndMonitorWorkloadDriver(namenodeProperties.get());    }  });  if (launchNameNode) {    namenodeMonitoringThread.start();  }  while (true) {    try {      Thread.sleep(1000);    } catch (    InterruptedException e) {      LOG.debug("Thread sleep in monitoring loop interrupted");    }    ApplicationReport report=yarnClient.getApplicationReport(infraAppId);
        LOG.error("Unexpected exception while waiting for NameNode readiness",ioe);      }catch (      InterruptedException ie) {        return;      }    }    if (!Apps.isApplicationFinalState(infraAppState) && launchWorkloadJob) {      launchAndMonitorWorkloadDriver(namenodeProperties.get());    }  });  if (launchNameNode) {    namenodeMonitoringThread.start();  }  while (true) {    try {      Thread.sleep(1000);    } catch (    InterruptedException e) {      LOG.debug("Thread sleep in monitoring loop interrupted");    }    ApplicationReport report=yarnClient.getApplicationReport(infraAppId);    if (report.getTrackingUrl() != null && !loggedApplicationInfo) {
  }  while (true) {    try {      Thread.sleep(1000);    } catch (    InterruptedException e) {      LOG.debug("Thread sleep in monitoring loop interrupted");    }    ApplicationReport report=yarnClient.getApplicationReport(infraAppId);    if (report.getTrackingUrl() != null && !loggedApplicationInfo) {      loggedApplicationInfo=true;      LOG.info("Track the application at: " + report.getTrackingUrl());      LOG.info("Kill the application using: yarn application -kill " + report.getApplicationId());    }    LOG.debug("Got application report from ASM for: appId={}, " + "clientToAMToken={}, appDiagnostics={}, appMasterHost={}, " + "appQueue={}, appMasterRpcPort={}, appStartTime={}, "+ "yarnAppState={}, distributedFinalState={}, appTrackingUrl={}, "+ "appUser={}",infraAppId.getId(),report.getClientToAMToken(),report.getDiagnostics(),report.getHost(),report.getQueue(),report.getRpcPort(),report.getStartTime(),report.getYarnApplicationState(),report.getFinalApplicationStatus(),report.getTrackingUrl(),report.getUser());    infraAppState=report.getYarnApplicationState();    if (infraAppState == YarnApplicationState.KILLED) {      if (!launchWorkloadJob) {        success=true;
    workloadConf.setInt(AuditReplayMapper.NUM_THREADS_KEY,workloadThreadsPerMapper);    workloadConf.setDouble(AuditReplayMapper.RATE_FACTOR_KEY,workloadRateFactor);    for (    Map.Entry<String,String> configPair : workloadExtraConfigs.entrySet()) {      workloadConf.set(configPair.getKey(),configPair.getValue());    }    workloadJob=WorkloadDriver.getJobForSubmission(workloadConf,nameNodeURI.toString(),workloadStartTime,AuditReplayMapper.class);    workloadJob.submit();    while (!Apps.isApplicationFinalState(infraAppState) && !isCompleted(workloadAppState)) {      workloadJob.monitorAndPrintJob();      Thread.sleep(5000);      workloadAppState=workloadJob.getJobState();    }    if (isCompleted(workloadAppState)) {      LOG.info("Workload job completed successfully!");    } else {      LOG.warn("Workload job failed.");    }  } catch (  Exception e) {
  LOG.info("Attempting to clean up remaining running applications.");  if (workloadJob != null) {    try {      workloadAppState=workloadJob.getJobState();    } catch (    IOException ioe) {      LOG.warn("Unable to fetch completion status of workload job. Will " + "proceed to attempt to kill it.",ioe);    }catch (    InterruptedException ie) {      Thread.currentThread().interrupt();      return;    }    if (!isCompleted(workloadAppState)) {      try {        LOG.info("Attempting to kill workload app: {}",workloadJob.getJobID());        workloadJob.killJob();        LOG.info("Killed workload app");      } catch (      IOException ioe) {
      workloadAppState=workloadJob.getJobState();    } catch (    IOException ioe) {      LOG.warn("Unable to fetch completion status of workload job. Will " + "proceed to attempt to kill it.",ioe);    }catch (    InterruptedException ie) {      Thread.currentThread().interrupt();      return;    }    if (!isCompleted(workloadAppState)) {      try {        LOG.info("Attempting to kill workload app: {}",workloadJob.getJobID());        workloadJob.killJob();        LOG.info("Killed workload app");      } catch (      IOException ioe) {        LOG.error("Unable to kill workload app ({})",workloadJob.getJobID(),ioe);      }    }  }  if (infraAppId != null && !Apps.isApplicationFinalState(infraAppState)) {    try {
    }catch (    InterruptedException ie) {      Thread.currentThread().interrupt();      return;    }    if (!isCompleted(workloadAppState)) {      try {        LOG.info("Attempting to kill workload app: {}",workloadJob.getJobID());        workloadJob.killJob();        LOG.info("Killed workload app");      } catch (      IOException ioe) {        LOG.error("Unable to kill workload app ({})",workloadJob.getJobID(),ioe);      }    }  }  if (infraAppId != null && !Apps.isApplicationFinalState(infraAppState)) {    try {      LOG.info("Attempting to kill infrastructure app: " + infraAppId);      forceKillApplication(infraAppId);      LOG.info("Killed infrastructure app");
public static File fetchHadoopTarball(File destinationDir,String version,Configuration conf,Logger log) throws IOException {
public static File fetchHadoopTarball(File destinationDir,String version,Configuration conf,Logger log) throws IOException {  log.info("Looking for Hadoop tarball for version: " + version);  File destinationFile=new File(destinationDir,String.format(HADOOP_TAR_FILENAME_FORMAT,version));  if (destinationFile.exists()) {
      long lastUnderRepBlocks=Long.MAX_VALUE;      try {        while (true) {          try {            Thread.sleep(TimeUnit.MINUTES.toMillis(1));            long underRepBlocks=Long.parseLong(fetchNameNodeJMXValue(nameNodeProperties,FSNAMESYSTEM_JMX_QUERY,JMX_MISSING_BLOCKS)) + Long.parseLong(fetchNameNodeJMXValue(nameNodeProperties,FSNAMESYSTEM_STATE_JMX_QUERY,JMX_UNDER_REPLICATED_BLOCKS));            long blockDecrease=lastUnderRepBlocks - underRepBlocks;            lastUnderRepBlocks=underRepBlocks;            if (blockDecrease < 0 || blockDecrease > (totalBlocks * 0.001)) {              continue;            }            String liveNodeListString=fetchNameNodeJMXValue(nameNodeProperties,NAMENODE_INFO_JMX_QUERY,JMX_LIVE_NODES_LIST);            Set<String> datanodesToReport=parseStaleDataNodeList(liveNodeListString,blockThreshold,log);            if (datanodesToReport.isEmpty() && doneWaiting.get()) {              log.info("BlockReportThread exiting; all DataNodes have " + "reported blocks");              break;
@SuppressWarnings("checkstyle:parameternumber") private static void waitForNameNodeJMXValue(String valueName,String jmxBeanQuery,String jmxProperty,double threshold,double printThreshold,boolean decreasing,Properties nameNodeProperties,Supplier<Boolean> shouldExit,Logger log) throws InterruptedException {  double lastPrintedValue=decreasing ? Double.MAX_VALUE : Double.MIN_VALUE;  double value;  int retryCount=0;  long startTime=Time.monotonicNow();  while (!shouldExit.get()) {    try {      value=Double.parseDouble(fetchNameNodeJMXValue(nameNodeProperties,jmxBeanQuery,jmxProperty));      if ((decreasing && value <= threshold) || (!decreasing && value >= threshold)) {
  for (JsonToken tok=parser.nextToken(); tok != null; tok=parser.nextToken()) {    if (tok == JsonToken.START_OBJECT) {      objectDepth++;    } else     if (tok == JsonToken.END_OBJECT) {      objectDepth--;    } else     if (tok == JsonToken.FIELD_NAME) {      if (objectDepth == 1) {        currentNodeAddr=parser.getCurrentName();      } else       if (objectDepth == 2) {        if (parser.getCurrentName().equals("numBlocks")) {          JsonToken valueToken=parser.nextToken();          if (valueToken != JsonToken.VALUE_NUMBER_INT || currentNodeAddr == null) {            throw new IOException(String.format("Malformed LiveNodes JSON; " + "got token = %s; currentNodeAddr = %s: %s",valueToken,currentNodeAddr,liveNodeJsonString));          }          int numBlocks=parser.getIntValue();          if (numBlocks < blockThreshold) {
    } else     if (tok == JsonToken.END_OBJECT) {      objectDepth--;    } else     if (tok == JsonToken.FIELD_NAME) {      if (objectDepth == 1) {        currentNodeAddr=parser.getCurrentName();      } else       if (objectDepth == 2) {        if (parser.getCurrentName().equals("numBlocks")) {          JsonToken valueToken=parser.nextToken();          if (valueToken != JsonToken.VALUE_NUMBER_INT || currentNodeAddr == null) {            throw new IOException(String.format("Malformed LiveNodes JSON; " + "got token = %s; currentNodeAddr = %s: %s",valueToken,currentNodeAddr,liveNodeJsonString));          }          int numBlocks=parser.getIntValue();          if (numBlocks < blockThreshold) {            log.debug(String.format("Queueing Datanode <%s> for block report; numBlocks = %d",currentNodeAddr,numBlocks));            dataNodesToReport.add(currentNodeAddr);          } else {
public static Job getJobForSubmission(Configuration baseConf,String nnURI,long startTimestampMs,Class<? extends WorkloadMapper<?,?,?,?>> mapperClass) throws IOException, InstantiationException, IllegalAccessException {  Configuration conf=new Configuration(baseConf);  conf.set(NN_URI,nnURI);  conf.setBoolean(MRJobConfig.MAP_SPECULATIVE,false);  String startTimeString=new SimpleDateFormat("yyyy/MM/dd HH:mm:ss z").format(new Date(startTimestampMs));
    t.addToQueue(AuditReplayCommand.getPoisonPill(highestTimestamp + 1));  }  Optional<Exception> threadException=Optional.empty();  for (  AuditReplayThread t : threads) {    t.join();    t.drainCounters(context);    t.drainCommandLatencies(context);    if (t.getException() != null) {      threadException=Optional.of(t.getException());    }  }  progressExecutor.shutdown();  if (threadException.isPresent()) {    throw new RuntimeException("Exception in AuditReplayThread",threadException.get());  }  LOG.info("Time taken to replay the logs in ms: " + (System.currentTimeMillis() - startTimestampMs));  long totalCommands=context.getCounter(REPLAYCOUNTERS.TOTALCOMMANDS).getValue();  if (totalCommands != 0) {    double percentageOfInvalidOps=context.getCounter(REPLAYCOUNTERS.TOTALINVALIDCOMMANDS).getValue() * 100.0 / totalCommands;
@Override public void run(){  long currentEpoch=System.currentTimeMillis();  long delay=startTimestampMs - currentEpoch;  try {    if (delay > 0) {
      LOG.info("Sleeping for " + delay + " ms");      Thread.sleep(delay);    } else {      LOG.warn("Starting late by " + (-1 * delay) + " ms");    }    AuditReplayCommand cmd=commandQueue.take();    while (!cmd.isPoison()) {      replayCountersMap.get(REPLAYCOUNTERS.TOTALCOMMANDS).increment(1);      delay=cmd.getDelay(TimeUnit.MILLISECONDS);      if (delay < -5) {        replayCountersMap.get(REPLAYCOUNTERS.LATECOMMANDS).increment(1);        replayCountersMap.get(REPLAYCOUNTERS.LATECOMMANDSTOTALTIME).increment(-1 * delay);      }      if (!replayLog(cmd)) {        replayCountersMap.get(REPLAYCOUNTERS.TOTALINVALIDCOMMANDS).increment(1);      }      cmd=commandQueue.take();    }  } catch (  InterruptedException e) {
 else {      LOG.warn("Starting late by " + (-1 * delay) + " ms");    }    AuditReplayCommand cmd=commandQueue.take();    while (!cmd.isPoison()) {      replayCountersMap.get(REPLAYCOUNTERS.TOTALCOMMANDS).increment(1);      delay=cmd.getDelay(TimeUnit.MILLISECONDS);      if (delay < -5) {        replayCountersMap.get(REPLAYCOUNTERS.LATECOMMANDS).increment(1);        replayCountersMap.get(REPLAYCOUNTERS.LATECOMMANDSTOTALTIME).increment(-1 * delay);      }      if (!replayLog(cmd)) {        replayCountersMap.get(REPLAYCOUNTERS.TOTALINVALIDCOMMANDS).increment(1);      }      cmd=commandQueue.take();    }  } catch (  InterruptedException e) {    LOG.error("Interrupted; exiting from thread.",e);  }catch (  Exception e) {
private void testAuditWorkloadWithOutput(String auditOutputPath) throws Exception {  long workloadStartTime=System.currentTimeMillis() + 10000;  Job workloadJob=WorkloadDriver.getJobForSubmission(conf,dfs.getUri().toString(),workloadStartTime,AuditReplayMapper.class);  boolean success=workloadJob.waitForCompletion(true);  assertTrue("workload job should succeed",success);  Counters counters=workloadJob.getCounters();  assertEquals(6,counters.findCounter(AuditReplayMapper.REPLAYCOUNTERS.TOTALCOMMANDS).getValue());  assertEquals(1,counters.findCounter(AuditReplayMapper.REPLAYCOUNTERS.TOTALINVALIDCOMMANDS).getValue());  assertTrue(dfs.getFileStatus(new Path("/tmp/test1")).isFile());  assertTrue(dfs.getFileStatus(new Path("/tmp/testDirRenamed")).isDirectory());  assertFalse(dfs.exists(new Path("/denied")));  assertTrue(dfs.exists(new Path(auditOutputPath)));  try (FSDataInputStream auditOutputFile=dfs.open(new Path(auditOutputPath,"part-r-00000"))){    String auditOutput=IOUtils.toString(auditOutputFile,StandardCharsets.UTF_8);
        isIgnoreFailures=true;      } else       if ("-log".equals(args[idx])) {        if (++idx == args.length) {          System.out.println("logdir not specified");          System.out.println(USAGE);          return -1;        }        logpath=new Path(args[idx]);      } else       if ('-' == args[idx].codePointAt(0)) {        System.out.println("Invalid switch " + args[idx]);        System.out.println(USAGE);        ToolRunner.printGenericCommandUsage(System.out);        return -1;      } else {        ops.add(new FileOperation(args[idx]));      }    }    if (ops.isEmpty()) {
        isIgnoreFailures=true;      } else       if ("-log".equals(args[idx])) {        if (++idx == args.length) {          System.out.println("logdir not specified");          System.out.println(USAGE);          return -1;        }        logpath=new Path(args[idx]);      } else       if ('-' == args[idx].codePointAt(0)) {        System.out.println("Invalid switch " + args[idx]);        System.out.println(USAGE);        ToolRunner.printGenericCommandUsage(System.out);        return -1;      } else {        ops.add(new FileOperation(args[idx]));      }    }    if (ops.isEmpty()) {
private boolean setup(List<FileOperation> ops,Path log) throws IOException {  final String randomId=getRandomId();  JobClient jClient=new JobClient(jobconf);  Path stagingArea;  try {    stagingArea=JobSubmissionFiles.getStagingDir(jClient.getClusterHandle(),jobconf);  } catch (  InterruptedException ie) {    throw new IOException(ie);  }  Path jobdir=new Path(stagingArea + NAME + "_"+ randomId);  FsPermission mapredSysPerms=new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);  FileSystem.mkdirs(jClient.getFs(),jobdir,mapredSysPerms);  LOG.info(JOB_DIR_LABEL + "=" + jobdir);  if (log == null) {    log=new Path(jobdir,"_logs");  }  FileOutputFormat.setOutputPath(jobconf,log);
      FileStatus srcstat=fs.getFileStatus(op.src);      if (srcstat.isDirectory() && op.isDifferent(srcstat)) {        ++opCount;        opWriter.append(new Text(op.src.toString()),op);      }      Stack<Path> pathstack=new Stack<Path>();      for (pathstack.push(op.src); !pathstack.empty(); ) {        for (        FileStatus stat : fs.listStatus(pathstack.pop())) {          if (stat.isDirectory()) {            pathstack.push(stat.getPath());          }          if (op.isDifferent(stat)) {            ++opCount;            if (++synCount > SYNC_FILE_MAX) {              opWriter.sync();              synCount=0;            }            Path f=stat.getPath();
void diffDistCp() throws IOException, RetryException {  RunningJobStatus job=getCurrentJob();  if (job != null) {    if (job.isComplete()) {      jobId=null;      if (job.isSuccessful()) {
@VisibleForTesting void updateStage(Stage value){  String oldStage=stage == null ? "null" : stage.name();  String newStage=value == null ? "null" : value.name();
  if (useSnapshotDiff) {    command.add("-diff");    command.add(LAST_SNAPSHOT_NAME);    command.add(CURRENT_SNAPSHOT_NAME);  }  command.add("-m");  command.add(mapNum + "");  command.add("-bandwidth");  command.add(bandWidth + "");  command.add(srcParam);  command.add(dstParam);  Configuration config=new Configuration(conf);  DistCp distCp;  try {    distCp=new DistCp(config,OptionsParser.parse(command.toArray(new String[]{})));    Job job=distCp.createAndSubmitJob();
private int continueJob() throws InterruptedException {  BalanceProcedureScheduler scheduler=new BalanceProcedureScheduler(getConf());  try {    scheduler.init(true);    while (true) {      Collection<BalanceJob> jobs=scheduler.getAllJobs();      int unfinished=0;      for (      BalanceJob job : jobs) {        if (!job.isJobDone()) {          unfinished++;        }        LOG.info(job.toString());      }      if (unfinished == 0) {        break;      }      Thread.sleep(TimeUnit.SECONDS.toMillis(10));    }  } catch (  IOException e) {
public void recoverJob(BalanceJob job) throws IOException {  FSDataInputStream in=null;  try {    Path logPath=getLatestStateJobPath(job);    FileSystem fs=FileSystem.get(workUri,conf);    in=fs.open(logPath);    job.readFields(in);
  } catch (  FileNotFoundException e) {    LOG.debug("Create work path {}",workPath);    fs.mkdirs(workPath);    return new BalanceJob[0];  }  BalanceJob[] jobs=new BalanceJob[statuses.length];  StringBuilder builder=new StringBuilder();  builder.append("List all jobs from journal [");  for (int i=0; i < statuses.length; i++) {    if (statuses[i].isDirectory()) {      jobs[i]=new BalanceJob.Builder<>().build();      jobs[i].setId(statuses[i].getPath().getName());      builder.append(jobs[i]);      if (i < statuses.length - 1) {        builder.append(", ");      }    }  }  builder.append("]");
void delay(BalanceJob job,long delayInMilliseconds){  delayQueue.add(new DelayWrapper(job,delayInMilliseconds));
@Override public boolean execute() throws IOException {  if (currentPhase < totalPhase) {
  }  long seed=r.nextLong();  r.setSeed(seed);  System.out.println(name.getMethodName() + " seed: " + seed);  conf=new HdfsConfiguration();  conf.set(SingleUGIResolver.USER,singleUser);  conf.set(SingleUGIResolver.GROUP,singleGroup);  conf.set(DFSConfigKeys.DFS_PROVIDER_STORAGEUUID,DFSConfigKeys.DFS_PROVIDER_STORAGEUUID_DEFAULT);  conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_PROVIDED_ENABLED,true);  conf.setClass(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_CLASS,TextFileRegionAliasMap.class,BlockAliasMap.class);  conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_WRITE_DIR,nnDirPath.toString());  conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_READ_FILE,new Path(nnDirPath,fileNameFromBlockPoolID(bpid)).toString());  conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER,"\t");  conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR_PROVIDED,new File(providedPath.toUri()).toString());  File imageDir=new File(providedPath.toUri());  if (!imageDir.exists()) {
  conf.setClass(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_CLASS,TextFileRegionAliasMap.class,BlockAliasMap.class);  conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_WRITE_DIR,nnDirPath.toString());  conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_READ_FILE,new Path(nnDirPath,fileNameFromBlockPoolID(bpid)).toString());  conf.set(DFSConfigKeys.DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER,"\t");  conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR_PROVIDED,new File(providedPath.toUri()).toString());  File imageDir=new File(providedPath.toUri());  if (!imageDir.exists()) {    LOG.info("Creating directory: " + imageDir);    imageDir.mkdirs();  }  File nnDir=new File(nnDirPath.toUri());  if (!nnDir.exists()) {    nnDir.mkdirs();  }  for (int i=0; i < numFiles; i++) {    File newFile=new File(new Path(providedPath,filePrefix + i + fileSuffix).toUri());    if (!newFile.exists()) {
private void setAndUnsetReplication(String filename) throws Exception {  Path file=new Path(filename);  FileSystem fs=cluster.getFileSystem();  short newReplication=4;
private void setAndUnsetReplication(String filename) throws Exception {  Path file=new Path(filename);  FileSystem fs=cluster.getFileSystem();  short newReplication=4;  LOG.info("Setting replication of file {} to {}",filename,newReplication);  fs.setReplication(file,newReplication);  DFSTestUtil.waitForReplication((DistributedFileSystem)fs,file,newReplication,10000);  DFSClient client=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),cluster.getConfiguration(0));  getAndCheckBlockLocations(client,filename,baseFileLen,1,newReplication);  newReplication=1;
private void verifyPathsWithHAFailoverIfNecessary(MiniDFSNNTopology topology,String providedNameservice) throws Exception {  List<Integer> nnIndexes=cluster.getNNIndexes(providedNameservice);  if (topology.isHA()) {    int nn1=nnIndexes.get(0);    int nn2=nnIndexes.get(1);    try {      verifyFileSystemContents(nn1);      fail("Read operation should fail as no Namenode is active");    } catch (    RemoteException e) {      LOG.info("verifyPaths failed!. Expected exception: {}" + e);    }    cluster.transitionToActive(nn1);    LOG.info("Verifying data from NN with index = {}",nn1);    verifyFileSystemContents(nn1);    cluster.transitionToStandby(nn1);    cluster.transitionToActive(nn2);
      fail("Read operation should fail as no Namenode is active");    } catch (    RemoteException e) {      LOG.info("verifyPaths failed!. Expected exception: {}" + e);    }    cluster.transitionToActive(nn1);    LOG.info("Verifying data from NN with index = {}",nn1);    verifyFileSystemContents(nn1);    cluster.transitionToStandby(nn1);    cluster.transitionToActive(nn2);    LOG.info("Verifying data from NN with index = {}",nn2);    verifyFileSystemContents(nn2);    cluster.shutdownNameNodes();    try {      verifyFileSystemContents(nn2);      fail("Read operation should fail as no Namenode is active");    } catch (    NullPointerException e) {
@Test public void testInMemoryAliasMapMultiTopologies() throws Exception {  MiniDFSNNTopology[] topologies=new MiniDFSNNTopology[]{MiniDFSNNTopology.simpleHATopology(),MiniDFSNNTopology.simpleFederatedTopology(3),MiniDFSNNTopology.simpleHAFederatedTopology(3)};  for (  MiniDFSNNTopology topology : topologies) {
@Test public void testProvidedWithHierarchicalTopology() throws Exception {  conf.setClass(ImageWriter.Options.UGI_CLASS,FsUGIResolver.class,UGIResolver.class);  String packageName="org.apache.hadoop.hdfs.server.blockmanagement";  String[] policies=new String[]{"BlockPlacementPolicyDefault","BlockPlacementPolicyRackFaultTolerant","BlockPlacementPolicyWithNodeGroup","BlockPlacementPolicyWithUpgradeDomain"};  createImage(new FSTreeWalk(providedPath,conf),nnDirPath,FixedBlockResolver.class);  String[] racks={"/pod0/rack0","/pod0/rack0","/pod0/rack1","/pod0/rack1","/pod1/rack0","/pod1/rack0","/pod1/rack1","/pod1/rack1"};  for (  String policy : policies) {
static void setupDataGeneratorConfig(Configuration conf){  boolean compress=isCompressionEmulationEnabled(conf);  if (compress) {    float ratio=getMapInputCompressionEmulationRatio(conf);
  int numCompressedFiles=0;  FileStatus[] outFileStatuses=fs.listStatus(inputDir,new Utils.OutputFileUtils.OutputFilesFilter());  for (  FileStatus status : outFileStatuses) {    if (compressionCodecs != null) {      CompressionCodec codec=compressionCodecs.getCodec(status.getPath());      if (codec != null) {        ++numCompressedFiles;        compressedDataSize+=status.getLen();      }    }  }  LOG.info("Gridmix is configured to use compressed input data.");  LOG.info("Total size of compressed input data : " + StringUtils.humanReadableInt(compressedDataSize));  LOG.info("Total number of compressed input data files : " + numCompressedFiles);  if (numCompressedFiles == 0) {    throw new RuntimeException("No compressed file found in the input" + " directory : " + inputDir.toString() + ". To enable compression"+ " emulation, run Gridmix either with "+ " an input directory containing compressed input file(s) or"+ " use the -generate option to (re)generate it. If compression"+ " emulation is not desired, disable it by setting '"+ COMPRESSION_EMULATION_ENABLE+ "' to 'false'.");  }  if (uncompressedDataSize > 0) {    double ratio=((double)compressedDataSize) / uncompressedDataSize;
  long byteCount=0;  long bytesSync=0;  for (Iterator it=dcFiles.iterator(); it.hasNext(); ) {    Map.Entry entry=(Map.Entry)it.next();    LongWritable fileSize=new LongWritable(Long.parseLong(entry.getValue().toString()));    BytesWritable filePath=new BytesWritable(entry.getKey().toString().getBytes(charsetUTF8));    byteCount+=fileSize.get();    bytesSync+=fileSize.get();    if (bytesSync > AVG_BYTES_PER_MAP) {      src_writer.sync();      bytesSync=fileSize.get();    }    src_writer.append(fileSize,filePath);  }  if (src_writer != null) {    src_writer.close();  }  fs.deleteOnExit(distCacheFilesList);
  for (Iterator it=dcFiles.iterator(); it.hasNext(); ) {    Map.Entry entry=(Map.Entry)it.next();    LongWritable fileSize=new LongWritable(Long.parseLong(entry.getValue().toString()));    BytesWritable filePath=new BytesWritable(entry.getKey().toString().getBytes(charsetUTF8));    byteCount+=fileSize.get();    bytesSync+=fileSize.get();    if (bytesSync > AVG_BYTES_PER_MAP) {      src_writer.sync();      bytesSync=fileSize.get();    }    src_writer.append(fileSize,filePath);  }  if (src_writer != null) {    src_writer.close();  }  fs.deleteOnExit(distCacheFilesList);  conf.setInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT,fileCount);  conf.setLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT,byteCount);
      } else {        configureRandomBytesDataGenerator();      }      job.submit();      return job;    }    private void configureRandomBytesDataGenerator(){      job.setMapperClass(GenDataMapper.class);      job.setNumReduceTasks(0);      job.setMapOutputKeyClass(NullWritable.class);      job.setMapOutputValueClass(BytesWritable.class);      job.setInputFormatClass(GenDataFormat.class);      job.setOutputFormatClass(RawBytesOutputFormat.class);      job.setJarByClass(GenerateData.class);      try {        FileInputFormat.addInputPath(job,new Path("ignored"));      } catch (      IOException e) {
protected int writeInputData(long genbytes,Path inputDir) throws IOException, InterruptedException {  if (genbytes > 0) {    final Configuration conf=getConf();    if (inputDir.getFileSystem(conf).exists(inputDir)) {
protected int writeInputData(long genbytes,Path inputDir) throws IOException, InterruptedException {  if (genbytes > 0) {    final Configuration conf=getConf();    if (inputDir.getFileSystem(conf).exists(inputDir)) {      LOG.error("Gridmix input data directory {} already exists " + "when -generate option is used.",inputDir);      return STARTUP_FAILED_ERROR;    }    CompressionEmulationUtil.setupDataGeneratorConfig(conf);    final GenerateData genData=new GenerateData(conf,inputDir,genbytes);    LOG.info("Generating {} of test data...",StringUtils.TraditionalBinaryPrefix.long2String(genbytes,"",1));    launchGridmixJob(genData);    FsShell shell=new FsShell(conf);    try {      LOG.info("Changing the permissions for inputPath {}",inputDir);      shell.run(new String[]{"-chmod","-R","777",inputDir.toString()});    } catch (    Exception e) {
private void startThreads(Configuration conf,String traceIn,Path ioPath,Path scratchDir,CountDownLatch startFlag,UserResolver userResolver) throws IOException {  try {    Path inputDir=getGridmixInputDataPath(ioPath);    GridmixJobSubmissionPolicy policy=getJobSubmissionPolicy(conf);
    int numThreads=conf.getInt(GRIDMIX_SUB_THR,noOfSubmitterThreads);    int queueDep=conf.getInt(GRIDMIX_QUE_DEP,5);    submitter=createJobSubmitter(monitor,numThreads,queueDep,new FilePool(conf,inputDir),userResolver,statistics);    distCacheEmulator=new DistributedCacheEmulator(conf,ioPath);    factory=createJobFactory(submitter,traceIn,scratchDir,conf,startFlag,userResolver);    factory.jobCreator.setDistCacheEmulator(distCacheEmulator);    if (policy == GridmixJobSubmissionPolicy.SERIAL) {      statistics.addJobStatsListeners(factory);    } else {      statistics.addClusterStatsObservers(factory);    }    statistics.addJobStatsListeners(summarizer.getExecutionSummarizer());    statistics.addClusterStatsObservers(summarizer.getClusterSummarizer());    monitor.start();    submitter.start();  } catch (  Exception e) {
      if ("-generate".equals(argv[i])) {        genbytes=StringUtils.TraditionalBinaryPrefix.string2long(argv[++i]);        if (genbytes <= 0) {          LOG.error("size of input data to be generated specified using " + "-generate option should be nonnegative.\n");          return ARGS_ERROR;        }      } else       if ("-users".equals(argv[i])) {        userRsrc=new URI(argv[++i]);      } else {        LOG.error("Unknown option " + argv[i] + " specified.\n");        printUsage(System.err);        return ARGS_ERROR;      }    }    if (userResolver.needsTargetUsersList()) {      if (userRsrc != null) {        if (!userResolver.setTargetUsers(userRsrc,conf)) {          LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
      } else {        LOG.error(userResolver.getClass() + " needs target user list. Use -users option.\n");        printUsage(System.err);        return ARGS_ERROR;      }    } else     if (userRsrc != null) {      LOG.warn("Ignoring the user resource '" + userRsrc + "'.");    }    ioPath=new Path(argv[argv.length - 2]);    traceIn=argv[argv.length - 1];  } catch (  Exception e) {    LOG.error(e.toString() + "\n");    if (LOG.isDebugEnabled()) {      e.printStackTrace();    }    printUsage(System.err);    return ARGS_ERROR;  }  final FileSystem inputFs=ioPath.getFileSystem(conf);
      startThreads(conf,traceIn,ioPath,scratchDir,startFlag,userResolver);      Path inputDir=getGridmixInputDataPath(ioPath);      exitCode=writeInputData(genbytes,inputDir);      if (exitCode != 0) {        return exitCode;      }      stats=GenerateData.publishDataStatistics(inputDir,genbytes,conf);      submitter.refreshFilePool();      boolean shouldGenerate=(genbytes > 0);      exitCode=setupEmulation(conf,traceIn,scratchDir,ioPath,shouldGenerate);      if (exitCode != 0) {        return exitCode;      }      summarizer.start(conf);      factory.start();      statistics.start();    } catch (    Throwable e) {
      Path inputDir=getGridmixInputDataPath(ioPath);      exitCode=writeInputData(genbytes,inputDir);      if (exitCode != 0) {        return exitCode;      }      stats=GenerateData.publishDataStatistics(inputDir,genbytes,conf);      submitter.refreshFilePool();      boolean shouldGenerate=(genbytes > 0);      exitCode=setupEmulation(conf,traceIn,scratchDir,ioPath,shouldGenerate);      if (exitCode != 0) {        return exitCode;      }      summarizer.start(conf);      factory.start();      statistics.start();    } catch (    Throwable e) {      LOG.error("Startup failed. " + e.toString() + "\n");
      if (exitCode != 0) {        return exitCode;      }      summarizer.start(conf);      factory.start();      statistics.start();    } catch (    Throwable e) {      LOG.error("Startup failed. " + e.toString() + "\n");      LOG.debug("Startup failed",e);      if (factory != null)       factory.abort();      exitCode=STARTUP_FAILED_ERROR;    } finally {      startFlag.countDown();    }    if (factory != null) {      factory.join(Long.MAX_VALUE);      final Throwable badTraceException=factory.error();
private static void scaleConfigParameter(Configuration sourceConf,Configuration destConf,String clusterValueKey,String jobValueKey,long defaultValue){  long simulatedClusterDefaultValue=destConf.getLong(clusterValueKey,defaultValue);  long originalClusterDefaultValue=sourceConf.getLong(clusterValueKey,defaultValue);  long originalJobValue=sourceConf.getLong(jobValueKey,defaultValue);  double scaleFactor=(double)originalJobValue / originalClusterDefaultValue;  long simulatedJobValue=(long)(scaleFactor * simulatedClusterDefaultValue);  if (LOG.isDebugEnabled()) {
public void submissionFailed(JobStats job){  String jobID=job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);
protected void onSuccess(Job job){
protected void onFailure(Job job){
  for (int i=0; i < reds; ++i) {    final TaskInfo info=jobdesc.getTaskInfo(TaskType.REDUCE,i);    reduceByteRatio[i]=info.getInputBytes() / (1.0 * mapOutputBytesTotal);    reduceRecordRatio[i]=info.getInputRecords() / (1.0 * mapOutputRecordsTotal);  }  final InputStriper striper=new InputStriper(inputDir,mapInputBytesTotal);  final List<InputSplit> splits=new ArrayList<InputSplit>();  for (int i=0; i < maps; ++i) {    final int nSpec=reds / maps + ((reds % maps) > i ? 1 : 0);    final long[] specBytes=new long[nSpec];    final long[] specRecords=new long[nSpec];    final ResourceUsageMetrics[] metrics=new ResourceUsageMetrics[nSpec];    for (int j=0; j < nSpec; ++j) {      final TaskInfo info=jobdesc.getTaskInfo(TaskType.REDUCE,i + j * maps);      specBytes[j]=info.getOutputBytes();      specRecords[j]=info.getOutputRecords();
static void setRandomTextDataGeneratorListSize(Configuration conf,int listSize){  if (LOG.isDebugEnabled()) {
static void setRandomTextDataGeneratorWordSize(Configuration conf,int wordSize){  if (LOG.isDebugEnabled()) {
@Override void buildSplits(FilePool inputDir) throws IOException {  final List<InputSplit> splits=new ArrayList<InputSplit>();  final int reds=(mapTasksOnly) ? 0 : jobdesc.getNumberReduces();  final int maps=jobdesc.getNumberMaps();  for (int i=0; i < maps; ++i) {    final int nSpec=reds / maps + ((reds % maps) > i ? 1 : 0);    final long[] redDurations=new long[nSpec];    for (int j=0; j < nSpec; ++j) {      final ReduceTaskAttemptInfo info=(ReduceTaskAttemptInfo)getSuccessfulAttemptInfo(TaskType.REDUCE,i + j * maps);      redDurations[j]=Math.min(reduceMaxSleepTime,info.getMergeRuntime() + info.getReduceRuntime());      if (LOG.isDebugEnabled()) {
public void addJobStats(JobStats stats){  int seq=GridmixJob.getJobSeqId(stats.getJob());  if (seq < 0) {
protected void checkLoadAndGetSlotsToBackfill() throws IOException, InterruptedException {  if (loadStatus.getJobLoad() <= 0) {    if (LOG.isDebugEnabled()) {      LOG.debug(System.currentTimeMillis() + " [JobLoad] Overloaded is " + Boolean.TRUE.toString()+ " NumJobsBackfill is "+ loadStatus.getJobLoad());    }    return;  }  int mapCapacity=loadStatus.getMapCapacity();  int reduceCapacity=loadStatus.getReduceCapacity();  if (mapCapacity < 0 || reduceCapacity < 0) {    return;  }  int maxMapLoad=(int)(overloadMapTaskMapSlotRatio * mapCapacity);  int maxReduceLoad=(int)(overloadReduceTaskReduceSlotRatio * reduceCapacity);  int totalMapTasks=ClusterStats.getSubmittedMapTasks();  int totalReduceTasks=ClusterStats.getSubmittedReduceTasks();  if (LOG.isDebugEnabled()) {    LOG.debug("Total submitted map tasks: " + totalMapTasks);
  if (loadStatus.getJobLoad() <= 0) {    if (LOG.isDebugEnabled()) {      LOG.debug(System.currentTimeMillis() + " [JobLoad] Overloaded is " + Boolean.TRUE.toString()+ " NumJobsBackfill is "+ loadStatus.getJobLoad());    }    return;  }  int mapCapacity=loadStatus.getMapCapacity();  int reduceCapacity=loadStatus.getReduceCapacity();  if (mapCapacity < 0 || reduceCapacity < 0) {    return;  }  int maxMapLoad=(int)(overloadMapTaskMapSlotRatio * mapCapacity);  int maxReduceLoad=(int)(overloadReduceTaskReduceSlotRatio * reduceCapacity);  int totalMapTasks=ClusterStats.getSubmittedMapTasks();  int totalReduceTasks=ClusterStats.getSubmittedReduceTasks();  if (LOG.isDebugEnabled()) {    LOG.debug("Total submitted map tasks: " + totalMapTasks);    LOG.debug("Total submitted reduce tasks: " + totalReduceTasks);
    if (LOG.isDebugEnabled()) {      LOG.debug(System.currentTimeMillis() + " [JobLoad] Overloaded is " + Boolean.TRUE.toString()+ " NumJobsBackfill is "+ loadStatus.getJobLoad());    }    return;  }  int mapCapacity=loadStatus.getMapCapacity();  int reduceCapacity=loadStatus.getReduceCapacity();  if (mapCapacity < 0 || reduceCapacity < 0) {    return;  }  int maxMapLoad=(int)(overloadMapTaskMapSlotRatio * mapCapacity);  int maxReduceLoad=(int)(overloadReduceTaskReduceSlotRatio * reduceCapacity);  int totalMapTasks=ClusterStats.getSubmittedMapTasks();  int totalReduceTasks=ClusterStats.getSubmittedReduceTasks();  if (LOG.isDebugEnabled()) {    LOG.debug("Total submitted map tasks: " + totalMapTasks);    LOG.debug("Total submitted reduce tasks: " + totalReduceTasks);    LOG.debug("Max map load: " + maxMapLoad);
        int currentReduceSlotsBackFill=(int)(maxReduceLoad - incompleteReduceTasks);        if (currentReduceSlotsBackFill <= 0) {          incompleteMapTasks=totalMapTasks;          if (LOG.isDebugEnabled()) {            LOG.debug("Terminating overload check due to high reduce load.");          }          break;        }      } else {        LOG.warn("Blacklisting empty job: " + id);        blacklistedJobs.add(id);      }    }    mapSlotsBackFill=(int)(maxMapLoad - incompleteMapTasks);    reduceSlotsBackFill=(int)(maxReduceLoad - incompleteReduceTasks);    blacklistedJobs.retainAll(seenJobIDs);    if (LOG.isDebugEnabled() && blacklistedJobs.size() > 0) {      LOG.debug("Blacklisted jobs count: " + blacklistedJobs.size());    }  }  loadStatus.updateMapLoad(mapSlotsBackFill);
public static void createHomeAndStagingDirectory(String user,Configuration conf){  try {    FileSystem fs=dfsCluster.getFileSystem();    String path="/user/" + user;    Path homeDirectory=new Path(path);    if (!fs.exists(homeDirectory)) {
@BeforeClass public static void setup() throws IOException {  final Configuration conf=new Configuration();  final FileSystem fs=FileSystem.getLocal(conf).getRaw();  fs.delete(base,true);  final Random r=new Random();  final long seed=r.nextLong();  r.setSeed(seed);
@SuppressWarnings({"unchecked","rawtypes"}) @Test(timeout=30000) public void testSleepMapper() throws Exception {  SleepJob.SleepMapper test=new SleepJob.SleepMapper();  Configuration conf=new Configuration();  conf.setInt(JobContext.NUM_REDUCES,2);  CompressionEmulationUtil.setCompressionEmulationEnabled(conf,true);  conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,true);  TaskAttemptID taskId=new TaskAttemptID();  FakeRecordLLReader reader=new FakeRecordLLReader();  LoadRecordGkNullWriter writer=new LoadRecordGkNullWriter();  OutputCommitter committer=new CustomOutputCommitter();  StatusReporter reporter=new TaskAttemptContextImpl.DummyReporter();  SleepSplit split=getSleepSplit();  MapContext<LongWritable,LongWritable,GridmixKey,NullWritable> mapcontext=new MapContextImpl<LongWritable,LongWritable,GridmixKey,NullWritable>(conf,taskId,reader,writer,committer,reporter,split);  Context context=new WrappedMapper<LongWritable,LongWritable,GridmixKey,NullWritable>().getMapContext(mapcontext);  long start=System.currentTimeMillis();
  CompressionEmulationUtil.setCompressionEmulationEnabled(conf,true);  conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,true);  TaskAttemptID taskId=new TaskAttemptID();  FakeRecordLLReader reader=new FakeRecordLLReader();  LoadRecordGkNullWriter writer=new LoadRecordGkNullWriter();  OutputCommitter committer=new CustomOutputCommitter();  StatusReporter reporter=new TaskAttemptContextImpl.DummyReporter();  SleepSplit split=getSleepSplit();  MapContext<LongWritable,LongWritable,GridmixKey,NullWritable> mapcontext=new MapContextImpl<LongWritable,LongWritable,GridmixKey,NullWritable>(conf,taskId,reader,writer,committer,reporter,split);  Context context=new WrappedMapper<LongWritable,LongWritable,GridmixKey,NullWritable>().getMapContext(mapcontext);  long start=System.currentTimeMillis();  LOG.info("start:" + start);  LongWritable key=new LongWritable(start + 2000);  LongWritable value=new LongWritable(start + 2000);  test.map(key,value,context);
static void lengthTest(GridmixRecord x,GridmixRecord y,int min,int max) throws Exception {  final Random r=new Random();  final long seed=r.nextLong();  r.setSeed(seed);
static void randomReplayTest(GridmixRecord x,GridmixRecord y,int min,int max) throws Exception {  final Random r=new Random();  final long seed=r.nextLong();  r.setSeed(seed);
static void eqSeedTest(GridmixRecord x,GridmixRecord y,int max) throws Exception {  final Random r=new Random();  final long s=r.nextLong();  r.setSeed(s);
static void binSortTest(GridmixRecord x,GridmixRecord y,int min,int max,WritableComparator cmp) throws Exception {  final Random r=new Random();  final long s=r.nextLong();  r.setSeed(s);
static void checkSpec(GridmixKey a,GridmixKey b) throws Exception {  final Random r=new Random();  final long s=r.nextLong();  r.setSeed(s);
@Test(timeout=500000) public void testReplaySubmit() throws Exception {  policy=GridmixJobSubmissionPolicy.REPLAY;  LOG.info(" Replay started at " + System.currentTimeMillis());  doSubmission(null,false);
@Test(timeout=500000) public void testStressSubmit() throws Exception {  policy=GridmixJobSubmissionPolicy.STRESS;  LOG.info(" Stress started at " + System.currentTimeMillis());  doSubmission(null,false);
@Test(timeout=500000) public void testSerialSubmit() throws Exception {  policy=GridmixJobSubmissionPolicy.SERIAL;  LOG.info("Serial started at " + System.currentTimeMillis());  doSubmission(JobCreator.LOADJOB.name(),false);
@Test(timeout=500000) public void testReplaySubmit() throws Exception {  policy=GridmixJobSubmissionPolicy.REPLAY;  LOG.info(" Replay started at " + System.currentTimeMillis());  doSubmission(JobCreator.LOADJOB.name(),false);
public static void testFactory(long targetBytes,long targetRecs) throws Exception {  final Configuration conf=new Configuration();  final GridmixKey key=new GridmixKey();  final GridmixRecord val=new GridmixRecord();
@Test public void testSerialSubmit() throws Exception {  policy=GridmixJobSubmissionPolicy.SERIAL;  LOG.info("Serial started at " + System.currentTimeMillis());  doSubmission(JobCreator.SLEEPJOB.name(),false);
@Test public void testReplaySubmit() throws Exception {  policy=GridmixJobSubmissionPolicy.REPLAY;  LOG.info(" Replay started at " + System.currentTimeMillis());  doSubmission(JobCreator.SLEEPJOB.name(),false);
@Test public void testStressSubmit() throws Exception {  policy=GridmixJobSubmissionPolicy.STRESS;  LOG.info(" Replay started at " + System.currentTimeMillis());  doSubmission(JobCreator.SLEEPJOB.name(),false);
@Override public void init(SubsetConfiguration conf){  Properties props=new Properties();  brokerList=conf.getString(BROKER_LIST);  if (LOG.isDebugEnabled()) {
  Instant instant=Instant.ofEpochMilli(timestamp);  LocalDateTime ldt=LocalDateTime.ofInstant(instant,zoneId);  String date=ldt.format(dateFormat);  String time=ldt.format(timeFormat);  jsonLines.append("{\"hostname\": \"" + hostname);  jsonLines.append("\", \"timestamp\": " + timestamp);  jsonLines.append(", \"date\": \"" + date);  jsonLines.append("\",\"time\": \"" + time);  jsonLines.append("\",\"name\": \"" + record.name() + "\" ");  for (  MetricsTag tag : record.tags()) {    jsonLines.append(", \"" + tag.name().toString().replaceAll("[\\p{Cc}]","") + "\": ");    jsonLines.append(" \"" + tag.value().toString() + "\"");  }  for (  AbstractMetric metric : record.metrics()) {    jsonLines.append(", \"" + metric.name().toString().replaceAll("[\\p{Cc}]","") + "\": ");    jsonLines.append(" \"" + metric.value().toString() + "\"");
      return null;    }    @Override public void visit(    MetricsVisitor visitor){    }  };  Iterable<AbstractMetric> metrics=Lists.newArrayList(metric);  when(record.name()).thenReturn("Kafka record name");  when(record.metrics()).thenReturn(metrics);  SubsetConfiguration conf=mock(SubsetConfiguration.class);  when(conf.getString(KafkaSink.BROKER_LIST)).thenReturn("localhost:9092");  String topic="myTestKafkaTopic";  when(conf.getString(KafkaSink.TOPIC)).thenReturn(topic);  kafkaSink=new KafkaSink();  kafkaSink.init(conf);  Producer<Integer,byte[]> mockProducer=mock(KafkaProducer.class);  kafkaSink.setProducer(mockProducer);  StringBuilder jsonLines=recordToJson(record);
private synchronized boolean release(String reason,Exception ex) throws IOException {  if (!released) {    reasonClosed=reason;    try {
private void setAuthDetails(URI endpoint,URI objectLocation,AccessToken authToken){  if (LOG.isDebugEnabled()) {
private StringEntity getAuthenticationRequst(AuthenticationRequest authenticationRequest) throws IOException {  final String data=JSONUtil.toJSON(new AuthenticationRequestWrapper(authenticationRequest));  if (LOG.isDebugEnabled()) {
private <M extends HttpUriRequest>IOException buildException(URI uri,M req,HttpResponse resp,int statusCode){  IOException fault;  String errorMessage=String.format("Method %s on %s failed, status code: %d," + " status line: %s",req.getMethod(),uri,statusCode,resp.getStatusLine());  if (LOG.isDebugEnabled()) {
private <M extends HttpUriRequest>HttpResponse exec(HttpClient client,M req) throws IOException {  HttpResponse resp=execWithDebugOutput(req,client);  int statusCode=resp.getStatusLine().getStatusCode();  if ((statusCode == HttpStatus.SC_UNAUTHORIZED || statusCode == HttpStatus.SC_BAD_REQUEST) && req instanceof AuthPostRequest && !useKeystoneAuthentication) {    if (LOG.isDebugEnabled()) {
@Override public void setWorkingDirectory(Path dir){  workingDir=makeAbsolute(dir);  if (LOG.isDebugEnabled()) {
    return null;  }  if (start < 0 || len < 0) {    throw new IllegalArgumentException("Negative start or len parameter" + " to getFileBlockLocations");  }  if (file.getLen() <= start) {    return new BlockLocation[0];  }  final FileStatus[] listOfFileBlocks=store.listSubPaths(file.getPath(),false,true);  List<URI> locations=new ArrayList<URI>();  if (listOfFileBlocks.length > 1) {    for (    FileStatus fileStatus : listOfFileBlocks) {      if (SwiftObjectPath.fromPath(uri,fileStatus.getPath()).equals(SwiftObjectPath.fromPath(uri,file.getPath()))) {        continue;      }      locations.addAll(store.getObjectLocation(fileStatus.getPath()));    }  } else {    locations=store.getObjectLocation(file.getPath());  }  if (locations.isEmpty()) {
@Override public boolean mkdirs(Path path,FsPermission permission) throws IOException {  if (LOG.isDebugEnabled()) {
private void forceMkdir(Path absolutePath) throws IOException {  if (LOG.isDebugEnabled()) {
@Override public FileStatus[] listStatus(Path path) throws IOException {  if (LOG.isDebugEnabled()) {
public void rename(Path src,Path dst) throws FileNotFoundException, SwiftOperationFailedException, IOException {  if (LOG.isDebugEnabled()) {
    throw new SwiftOperationFailedException("cannot rename root dir");  }  final SwiftFileStatus srcMetadata;  srcMetadata=getObjectMetadata(src);  SwiftFileStatus dstMetadata;  try {    dstMetadata=getObjectMetadata(dst);  } catch (  FileNotFoundException e) {    LOG.debug("Destination does not exist");    dstMetadata=null;  }  Path srcParent=src.getParent();  Path dstParent=dst.getParent();  if (dstParent != null && !dstParent.equals(srcParent)) {    SwiftFileStatus fileStatus;    try {      fileStatus=getObjectMetadata(dstParent);
          return;        }      }    } else {      destPath=toObjectPath(dst);    }    int childCount=childStats.size();    if (childCount == 0) {      copyThenDeleteObject(srcObject,destPath);    } else {      SwiftUtils.debug(LOG,"Source file appears to be partitioned." + " copying file and deleting children");      copyObject(srcObject,destPath);      for (      FileStatus stat : childStats) {        SwiftUtils.debug(LOG,"Deleting partitioned file %s ",stat);        deleteObject(stat.getPath());      }      swiftRestClient.delete(srcObject);    }  } else {    if (destExists && !destIsDir) {
private void logDirectory(String message,SwiftObjectPath objectPath,Iterable<FileStatus> statuses){  if (LOG.isDebugEnabled()) {
private void innerClose(String reason) throws IOException {  try {    if (httpStream != null) {      reasonClosed=reason;      if (LOG.isDebugEnabled()) {
@Override protected void finalize() throws Throwable {  if (httpStream != null) {
    throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);  }  long offset=targetPos - pos;  if (LOG.isDebugEnabled()) {    LOG.debug("Seek to " + targetPos + "; current pos ="+ pos+ "; offset="+ offset);  }  if (offset == 0) {    LOG.debug("seek is no-op");    return;  }  if (offset < 0) {    LOG.debug("seek is backwards");  } else   if ((rangeOffset + offset < bufferSize)) {    SwiftUtils.debug(LOG,"seek is within current stream" + "; pos= %d ; targetPos=%d; " + "offset= %d ; bufferOffset=%d",pos,targetPos,offset,rangeOffset);    try {      LOG.debug("chomping ");      chompBytes(offset);    } catch (    IOException e) {
    LOG.debug("seek is no-op");    return;  }  if (offset < 0) {    LOG.debug("seek is backwards");  } else   if ((rangeOffset + offset < bufferSize)) {    SwiftUtils.debug(LOG,"seek is within current stream" + "; pos= %d ; targetPos=%d; " + "offset= %d ; bufferOffset=%d",pos,targetPos,offset,rangeOffset);    try {      LOG.debug("chomping ");      chompBytes(offset);    } catch (    IOException e) {      LOG.debug("while chomping ",e);    }    if (targetPos - pos == 0) {      LOG.trace("chomping successful");      return;    }    LOG.trace("chomping failed");
  if (backupStream != null) {    backupStream.close();  }  if (closingUpload && partUpload && backupFile.length() == 0) {    SwiftUtils.debug(LOG,"skipping upload of 0 byte final partition");    delete(backupFile);  } else {    partUpload=true;    boolean uploadSuccess=false;    int attempt=0;    while (!uploadSuccess) {      try {        ++attempt;        bytesUploaded+=uploadFilePartAttempt(attempt);        uploadSuccess=true;      } catch (      IOException e) {
public static void noteAction(String action){  if (LOG.isDebugEnabled()) {
public static void debug(Logger log,String text,Object... args){  if (log.isDebugEnabled()) {
public static void debugEx(Logger log,String text,Exception ex){  if (log.isDebugEnabled()) {
@AfterClass public static void classTearDown() throws Exception {  if (lastFs != null) {    List<DurationStats> statistics=lastFs.getOperationStatistics();    for (    DurationStats stat : statistics) {
private void printf(String format,Object... args){  String msg=String.format(format,args);  System.out.printf(msg + "\n");
private void assertLocationValid(BlockLocation location) throws IOException {
@Test(timeout=SWIFT_TEST_TIMEOUT) public void testLocateDirectory() throws Throwable {  describe("verify that locating a directory is an error");  createFile(path("/test/filename"));  FileStatus status=fs.getFileStatus(path("/test"));
    int firstWriteLen=2048;    out.write(src,0,firstWriteLen);    long expected=getExpectedPartitionsWritten(firstWriteLen,PART_SIZE_BYTES,false);    SwiftUtils.debug(LOG,"First write: predict %d partitions written",expected);    assertPartitionsWritten("First write completed",out,expected);    int remainder=len - firstWriteLen;    SwiftUtils.debug(LOG,"remainder: writing: %d bytes",remainder);    out.write(src,firstWriteLen,remainder);    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,false);    assertPartitionsWritten("Remaining data",out,expected);    out.close();    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,true);    assertPartitionsWritten("Stream closed",out,expected);    Header[] headers=fs.getStore().getObjectHeaders(path,true);    for (    Header header : headers) {
    long expected=getExpectedPartitionsWritten(firstWriteLen,PART_SIZE_BYTES,false);    SwiftUtils.debug(LOG,"First write: predict %d partitions written",expected);    assertPartitionsWritten("First write completed",out,expected);    int remainder=len - firstWriteLen;    SwiftUtils.debug(LOG,"remainder: writing: %d bytes",remainder);    out.write(src,firstWriteLen,remainder);    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,false);    assertPartitionsWritten("Remaining data",out,expected);    out.close();    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,true);    assertPartitionsWritten("Stream closed",out,expected);    Header[] headers=fs.getStore().getObjectHeaders(path,true);    for (    Header header : headers) {      LOG.info(header.toString());    }    byte[] dest=readDataset(fs,path,len);
    int firstWriteLen=2048;    out.write(src,0,firstWriteLen);    long expected=getExpectedPartitionsWritten(firstWriteLen,PART_SIZE_BYTES,false);    SwiftUtils.debug(LOG,"First write: predict %d partitions written",expected);    assertPartitionsWritten("First write completed",out,expected);    int remainder=len - firstWriteLen;    SwiftUtils.debug(LOG,"remainder: writing: %d bytes",remainder);    out.write(src,firstWriteLen,remainder);    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,false);    assertPartitionsWritten("Remaining data",out,expected);    out.close();    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,true);    assertPartitionsWritten("Stream closed",out,expected);    Header[] headers=fs.getStore().getObjectHeaders(path,true);    for (    Header header : headers) {
    long expected=getExpectedPartitionsWritten(firstWriteLen,PART_SIZE_BYTES,false);    SwiftUtils.debug(LOG,"First write: predict %d partitions written",expected);    assertPartitionsWritten("First write completed",out,expected);    int remainder=len - firstWriteLen;    SwiftUtils.debug(LOG,"remainder: writing: %d bytes",remainder);    out.write(src,firstWriteLen,remainder);    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,false);    assertPartitionsWritten("Remaining data",out,expected);    out.close();    expected=getExpectedPartitionsWritten(len,PART_SIZE_BYTES,true);    assertPartitionsWritten("Stream closed",out,expected);    Header[] headers=fs.getStore().getObjectHeaders(path,true);    for (    Header header : headers) {      LOG.info(header.toString());    }    byte[] dest=readDataset(fs,path,len);
@Test(timeout=SWIFT_TEST_TIMEOUT) public void testConvertToPath() throws Throwable {  String initialpath="/dir/file1";  Path ipath=new Path(initialpath);  SwiftObjectPath objectPath=SwiftObjectPath.fromPath(new URI(initialpath),ipath);  URI endpoint=new URI(ENDPOINT);  URI uri=SwiftRestClient.pathToURI(objectPath,endpoint);
@Test(timeout=SWIFT_TEST_TIMEOUT) public void testConvertToPath() throws Throwable {  String initialpath="/dir/file1";  Path ipath=new Path(initialpath);  SwiftObjectPath objectPath=SwiftObjectPath.fromPath(new URI(initialpath),ipath);  URI endpoint=new URI(ENDPOINT);  URI uri=SwiftRestClient.pathToURI(objectPath,endpoint);  LOG.info("Inital Hadoop Path =" + initialpath);
@Test(timeout=SWIFT_TEST_TIMEOUT) public void testPutAndDelete() throws Throwable {  assumeEnabled();  SwiftRestClient client=createClient();  client.authenticate();  Path path=new Path("restTestPutAndDelete");  SwiftObjectPath sobject=SwiftObjectPath.fromPath(serviceURI,path);  byte[] stuff=new byte[1];  stuff[0]='a';  client.upload(sobject,new ByteArrayInputStream(stuff),stuff.length);  Duration head=new Duration();  Header[] responseHeaders=client.headRequest("expect success",sobject,SwiftRestClient.NEWEST);  head.finished();
@Test(timeout=SWIFT_TEST_TIMEOUT) public void testPutAndDelete() throws Throwable {  assumeEnabled();  SwiftRestClient client=createClient();  client.authenticate();  Path path=new Path("restTestPutAndDelete");  SwiftObjectPath sobject=SwiftObjectPath.fromPath(serviceURI,path);  byte[] stuff=new byte[1];  stuff[0]='a';  client.upload(sobject,new ByteArrayInputStream(stuff),stuff.length);  Duration head=new Duration();  Header[] responseHeaders=client.headRequest("expect success",sobject,SwiftRestClient.NEWEST);  head.finished();  LOG.info("head request duration " + head);  for (  Header header : responseHeaders) {
  byte[] stuff=new byte[1];  stuff[0]='a';  client.upload(sobject,new ByteArrayInputStream(stuff),stuff.length);  Duration head=new Duration();  Header[] responseHeaders=client.headRequest("expect success",sobject,SwiftRestClient.NEWEST);  head.finished();  LOG.info("head request duration " + head);  for (  Header header : responseHeaders) {    LOG.info(header.toString());  }  client.delete(sobject);  try {    Header[] headers=client.headRequest("expect fail",sobject,SwiftRestClient.NEWEST);    Assert.fail("Expected deleted file, but object is still present: " + sobject);  } catch (  FileNotFoundException e) {  }  for (  DurationStats stats : client.getOperationStatistics()) {
  FileStatus[] status2=(FileStatus[])fs.listStatus(dir);  ls2.finished();  assertEquals("Not enough entries in the directory",count,status2.length);  SwiftTestUtils.noteAction("Beginning read");  for (long l=0; l < count; l++) {    String name=String.format(format,l);    Path p=new Path(dir,"part-" + name);    Duration d=new Duration();    String result=SwiftTestUtils.readBytesToString(fs,p,name.length());    assertEquals(name,result);    d.finished();    readStats.add(d);  }  SwiftTestUtils.noteAction("Beginning delete");  Duration rm2=new Duration();  fs.delete(dir,true);
  assertEquals("Not enough entries in the directory",count,status2.length);  SwiftTestUtils.noteAction("Beginning read");  for (long l=0; l < count; l++) {    String name=String.format(format,l);    Path p=new Path(dir,"part-" + name);    Duration d=new Duration();    String result=SwiftTestUtils.readBytesToString(fs,p,name.length());    assertEquals(name,result);    d.finished();    readStats.add(d);  }  SwiftTestUtils.noteAction("Beginning delete");  Duration rm2=new Duration();  fs.delete(dir,true);  rm2.finished();  LOG.info(String.format("'filesystem','%s'",fs.getUri()));
  for (long l=0; l < count; l++) {    String name=String.format(format,l);    Path p=new Path(dir,"part-" + name);    Duration d=new Duration();    String result=SwiftTestUtils.readBytesToString(fs,p,name.length());    assertEquals(name,result);    d.finished();    readStats.add(d);  }  SwiftTestUtils.noteAction("Beginning delete");  Duration rm2=new Duration();  fs.delete(dir,true);  rm2.finished();  LOG.info(String.format("'filesystem','%s'",fs.getUri()));  LOG.info(writeStats.toString());  LOG.info(readStats.toString());
@POST @Path("/translator/{logFile : .+}") public void parseFile(@PathParam("logFile") String logFile) throws IOException, SkylineStoreException, ResourceEstimatorException {  logParserUtil.parseLog(logFile);
@GET @Path("/skylinestore/history/{pipelineId}/{runId}") @Produces(MediaType.APPLICATION_JSON) public String getHistoryResourceSkyline(@PathParam("pipelineId") String pipelineId,@PathParam("runId") String runId) throws SkylineStoreException {  RecurrenceId recurrenceId=new RecurrenceId(pipelineId,runId);  Map<RecurrenceId,List<ResourceSkyline>> jobHistory=skylineStore.getHistory(recurrenceId);  final String skyline=gson.toJson(jobHistory,skylineStoreType);
@GET @Path("/skylinestore/estimation/{pipelineId}") @Produces(MediaType.APPLICATION_JSON) public String getEstimatedResourceAllocation(@PathParam("pipelineId") String pipelineId) throws SkylineStoreException {  RLESparseResourceAllocation result=skylineStore.getEstimation(pipelineId);  final String skyline=gson.toJson(result,rleType);
@DELETE @Path("/skylinestore/history/{pipelineId}/{runId}") public void deleteHistoryResourceSkyline(@PathParam("pipelineId") String pipelineId,@PathParam("runId") String runId) throws SkylineStoreException {  RecurrenceId recurrenceId=new RecurrenceId(pipelineId,runId);  skylineStore.deleteHistory(recurrenceId);
  inputValidator.validate(recurrenceId,resourceSkylines);  writeLock.lock();  try {    final List<ResourceSkyline> filteredInput=eliminateNull(resourceSkylines);    if (filteredInput.size() > 0) {      if (skylineStore.containsKey(recurrenceId)) {        final List<ResourceSkyline> jobHistory=skylineStore.get(recurrenceId);        final List<String> oldJobIds=new ArrayList<>();        for (        final ResourceSkyline resourceSkyline : jobHistory) {          oldJobIds.add(resourceSkyline.getJobId());        }        if (!oldJobIds.isEmpty()) {          for (          ResourceSkyline elem : filteredInput) {            if (oldJobIds.contains(elem.getJobId())) {              StringBuilder errMsg=new StringBuilder();              errMsg.append("Trying to addHistory duplicate resource skylines for " + recurrenceId + ". Use updateHistory function instead.");
    final List<ResourceSkyline> filteredInput=eliminateNull(resourceSkylines);    if (filteredInput.size() > 0) {      if (skylineStore.containsKey(recurrenceId)) {        final List<ResourceSkyline> jobHistory=skylineStore.get(recurrenceId);        final List<String> oldJobIds=new ArrayList<>();        for (        final ResourceSkyline resourceSkyline : jobHistory) {          oldJobIds.add(resourceSkyline.getJobId());        }        if (!oldJobIds.isEmpty()) {          for (          ResourceSkyline elem : filteredInput) {            if (oldJobIds.contains(elem.getJobId())) {              StringBuilder errMsg=new StringBuilder();              errMsg.append("Trying to addHistory duplicate resource skylines for " + recurrenceId + ". Use updateHistory function instead.");              LOGGER.error(errMsg.toString());              throw new DuplicateRecurrenceIdException(errMsg.toString());            }          }        }        skylineStore.get(recurrenceId).addAll(filteredInput);
        final List<ResourceSkyline> jobHistory=skylineStore.get(recurrenceId);        final List<String> oldJobIds=new ArrayList<>();        for (        final ResourceSkyline resourceSkyline : jobHistory) {          oldJobIds.add(resourceSkyline.getJobId());        }        if (!oldJobIds.isEmpty()) {          for (          ResourceSkyline elem : filteredInput) {            if (oldJobIds.contains(elem.getJobId())) {              StringBuilder errMsg=new StringBuilder();              errMsg.append("Trying to addHistory duplicate resource skylines for " + recurrenceId + ". Use updateHistory function instead.");              LOGGER.error(errMsg.toString());              throw new DuplicateRecurrenceIdException(errMsg.toString());            }          }        }        skylineStore.get(recurrenceId).addAll(filteredInput);        LOGGER.info("Successfully addHistory new resource skylines for {}.",recurrenceId);      } else {        skylineStore.put(recurrenceId,filteredInput);
@Override public void addEstimation(String pipelineId,RLESparseResourceAllocation resourceSkyline) throws SkylineStoreException {  inputValidator.validate(pipelineId,resourceSkyline);  writeLock.lock();  try {    estimationStore.put(pipelineId,resourceSkyline);
@Override public final void updateHistory(final RecurrenceId recurrenceId,final List<ResourceSkyline> resourceSkylines) throws SkylineStoreException {  inputValidator.validate(recurrenceId,resourceSkylines);  writeLock.lock();  try {    if (skylineStore.containsKey(recurrenceId)) {      List<ResourceSkyline> filteredInput=eliminateNull(resourceSkylines);      if (filteredInput.size() > 0) {        skylineStore.put(recurrenceId,filteredInput);
  writeLock.lock();  try {    if (skylineStore.containsKey(recurrenceId)) {      List<ResourceSkyline> filteredInput=eliminateNull(resourceSkylines);      if (filteredInput.size() > 0) {        skylineStore.put(recurrenceId,filteredInput);        LOGGER.info("Successfully updateHistory resource skylines for {}.",recurrenceId);      } else {        StringBuilder errMsg=new StringBuilder();        errMsg.append("Trying to updateHistory " + recurrenceId + " with empty resource skyline");        LOGGER.error(errMsg.toString());        throw new EmptyResourceSkylineException(errMsg.toString());      }    } else {      StringBuilder errMsg=new StringBuilder();      errMsg.append("Trying to updateHistory non-existing resource skylines for " + recurrenceId);
@Override public final Map<RecurrenceId,List<ResourceSkyline>> getHistory(final RecurrenceId recurrenceId) throws SkylineStoreException {  inputValidator.validate(recurrenceId);  readLock.lock();  try {    String pipelineId=recurrenceId.getPipelineId();    if (pipelineId.equals("*")) {
  inputValidator.validate(recurrenceId);  readLock.lock();  try {    String pipelineId=recurrenceId.getPipelineId();    if (pipelineId.equals("*")) {      LOGGER.info("Successfully query resource skylines for {}.",recurrenceId);      return Collections.unmodifiableMap(skylineStore);    }    String runId=recurrenceId.getRunId();    Map<RecurrenceId,List<ResourceSkyline>> result=new HashMap<RecurrenceId,List<ResourceSkyline>>();    if (runId.equals("*")) {      for (      Map.Entry<RecurrenceId,List<ResourceSkyline>> entry : skylineStore.entrySet()) {        RecurrenceId index=entry.getKey();        if (index.getPipelineId().equals(pipelineId)) {          result.put(index,entry.getValue());        }      }      if (result.size() > 0) {
      for (      Map.Entry<RecurrenceId,List<ResourceSkyline>> entry : skylineStore.entrySet()) {        RecurrenceId index=entry.getKey();        if (index.getPipelineId().equals(pipelineId)) {          result.put(index,entry.getValue());        }      }      if (result.size() > 0) {        LOGGER.info("Successfully query resource skylines for {}.",recurrenceId);        return Collections.unmodifiableMap(result);      } else {        LOGGER.warn("Trying to getHistory non-existing resource skylines for {}.",recurrenceId);        return null;      }    }    if (skylineStore.containsKey(recurrenceId)) {      result.put(recurrenceId,skylineStore.get(recurrenceId));    } else {      LOGGER.warn("Trying to getHistory non-existing resource skylines for {}.",recurrenceId);      return null;
public final void validate(final RecurrenceId recurrenceId) throws SkylineStoreException {  if (recurrenceId == null) {    StringBuilder sb=new StringBuilder();    sb.append("Recurrence id is null, please try again by specifying" + " a valid Recurrence id.");
public final void validate(final String pipelineId) throws SkylineStoreException {  if (pipelineId == null) {    StringBuilder sb=new StringBuilder();    sb.append("pipelineId is null, please try again by specifying" + " a valid pipelineId.");
public final void validate(final RecurrenceId recurrenceId,final List<ResourceSkyline> resourceSkylines) throws SkylineStoreException {  validate(recurrenceId);  if (resourceSkylines == null) {    StringBuilder sb=new StringBuilder();    sb.append("ResourceSkylines for " + recurrenceId + " is null, please try again by "+ "specifying valid ResourceSkylines.");
public final void validate(final String pipelineId,final RLESparseResourceAllocation resourceOverTime) throws SkylineStoreException {  validate(pipelineId);  if (resourceOverTime == null) {    StringBuilder sb=new StringBuilder();    sb.append("Resource allocation for " + pipelineId + " is null.");
      indexJobITimeK=indexJobI * jobLen + timeK;      cJobITimeK=containerNums[timeK];      regularizationConstraint.set(uaPredict[indexJobITimeK],1 / cJobI);      generateOverAllocationConstraints(lpModel,cJobITimeK,oa,x,indexJobITimeK,timeK);      generateUnderAllocationConstraints(lpModel,cJobITimeK,uaPredict,ua,x,indexJobITimeK,timeK);    }  }  Expression objective=lpModel.addExpression("objective");  generateObjective(objective,numJobs,jobLen,oa,ua,eps);  final Result lpResult=lpModel.minimise();  final TreeMap<Long,Resource> treeMap=new TreeMap<>();  RLESparseResourceAllocation result=new RLESparseResourceAllocation(treeMap,new DefaultResourceCalculator());  ReservationInterval riAdd;  Resource containerSpec=resourceSkylines.get(0).getContainerSpec();  String pipelineId=((RecurrenceId)jobHistory.keySet().toArray()[0]).getPipelineId();  Resource resource;  for (int indexTimeK=0; indexTimeK < jobLen; indexTimeK++) {
  }  Queue<Pair<LoggedJob,JobTraceReader>> heap=new PriorityQueue<Pair<LoggedJob,JobTraceReader>>();  try {    LoggedJob job=reader.nextJob();    if (job == null) {      LOG.error("The job trace is empty");      return EMPTY_JOB_TRACE;    }    if (startsAfter > 0) {      LOG.info("starts-after time is specified. Initial job submit time : " + job.getSubmitTime());      long approximateTime=job.getSubmitTime() + startsAfter;      job=reader.nextJob();      long skippedCount=0;      while (job != null && job.getSubmitTime() < approximateTime) {        job=reader.nextJob();        skippedCount++;      }      LOG.debug("Considering jobs with submit time greater than " + startsAfter + " ms. Skipped "+ skippedCount+ " jobs.");
    LoggedJob job=reader.nextJob();    if (job == null) {      LOG.error("The job trace is empty");      return EMPTY_JOB_TRACE;    }    if (startsAfter > 0) {      LOG.info("starts-after time is specified. Initial job submit time : " + job.getSubmitTime());      long approximateTime=job.getSubmitTime() + startsAfter;      job=reader.nextJob();      long skippedCount=0;      while (job != null && job.getSubmitTime() < approximateTime) {        job=reader.nextJob();        skippedCount++;      }      LOG.debug("Considering jobs with submit time greater than " + startsAfter + " ms. Skipped "+ skippedCount+ " jobs.");      if (job == null) {        LOG.error("No more jobs to process in the trace with 'starts-after'" + " set to " + startsAfter + "ms.");
      long approximateTime=job.getSubmitTime() + startsAfter;      job=reader.nextJob();      long skippedCount=0;      while (job != null && job.getSubmitTime() < approximateTime) {        job=reader.nextJob();        skippedCount++;      }      LOG.debug("Considering jobs with submit time greater than " + startsAfter + " ms. Skipped "+ skippedCount+ " jobs.");      if (job == null) {        LOG.error("No more jobs to process in the trace with 'starts-after'" + " set to " + startsAfter + "ms.");        return EMPTY_JOB_TRACE;      }      LOG.info("The first job has a submit time of " + job.getSubmitTime());    }    firstJobSubmitTime=job.getSubmitTime();    long lastJobSubmitTime=firstJobSubmitTime;    int numberJobs=0;    long currentIntervalEnd=Long.MIN_VALUE;
    long lastJobSubmitTime=firstJobSubmitTime;    int numberJobs=0;    long currentIntervalEnd=Long.MIN_VALUE;    Path nextSegment=null;    Outputter<LoggedJob> tempGen=null;    if (debug) {      LOG.debug("The first job has a submit time of " + firstJobSubmitTime);    }    final Configuration conf=getConf();    try {      while (job != null) {        final Random tempNameGenerator=new Random();        lastJobSubmitTime=job.getSubmitTime();        ++numberJobs;        if (job.getSubmitTime() >= currentIntervalEnd) {          if (tempGen != null) {
        final Random tempNameGenerator=new Random();        lastJobSubmitTime=job.getSubmitTime();        ++numberJobs;        if (job.getSubmitTime() >= currentIntervalEnd) {          if (tempGen != null) {            tempGen.close();          }          nextSegment=null;          for (int i=0; i < 3 && nextSegment == null; ++i) {            try {              nextSegment=new Path(tempDir,"segment-" + tempNameGenerator.nextLong() + ".json.gz");              if (debug) {                LOG.debug("The next segment name is " + nextSegment);              }              FileSystem fs=nextSegment.getFileSystem(conf);              try {                if (!fs.exists(nextSegment)) {
private boolean setNextDirectoryInputStream() throws FileNotFoundException, IOException {  if (input != null) {    input.close();    LOG.info("File closed: " + currentFileName);    input=null;  }  if (inputCodec != null) {    CodecPool.returnDecompressor(inputDecompressor);    inputDecompressor=null;    inputCodec=null;  }  ++inputDirectoryCursor;  if (inputDirectoryCursor >= inputDirectoryFiles.length) {    return false;  }  fileFirstLine=true;  currentFileName=inputDirectoryFiles[inputDirectoryCursor];  LOG.info("\nOpening file " + currentFileName + "  *************************** .");
        }      }      task.setPreferredLocations(locations);    }    task.setTaskID(taskID);    if (startTime != null) {      task.setStartTime(Long.parseLong(startTime));    }    if (finishTime != null) {      task.setFinishTime(Long.parseLong(finishTime));    }    Pre21JobHistoryConstants.Values typ;    Pre21JobHistoryConstants.Values stat;    try {      stat=status == null ? null : Pre21JobHistoryConstants.Values.valueOf(status);    } catch (    IllegalArgumentException e) {      LOG.error("A task status you don't know about is \"" + status + "\".",e);      stat=null;    }    task.setTaskStatus(stat);    try {
      jobBeingTraced.getMapTasks().add(task);      tasksInCurrentJob.put(taskID,task);    }    task.setTaskID(taskID);    LoggedTaskAttempt attempt=attemptsInCurrentJob.get(attemptID);    boolean attemptAlreadyExists=attempt != null;    if (attempt == null) {      attempt=new LoggedTaskAttempt();      attempt.setAttemptID(attemptID);    }    if (!attemptAlreadyExists) {      attemptsInCurrentJob.put(attemptID,attempt);      task.getAttempts().add(attempt);    }    Pre21JobHistoryConstants.Values stat=null;    try {      stat=status == null ? null : Pre21JobHistoryConstants.Values.valueOf(status);    } catch (    IllegalArgumentException e) {
int run() throws IOException {  Pair<String,String> line=readBalancedLine();  while (line != null) {    if (debug && (lineNumber < 1000000L && lineNumber % 1000L == 0 || lineNumber % 1000000L == 0)) {
  try {    HadoopLogsAnalyzer analyzer=new HadoopLogsAnalyzer();    int result=ToolRunner.run(analyzer,args);    if (result == 0) {      return;    }    System.exit(result);  } catch (  FileNotFoundException e) {    LOG.error("",e);    e.printStackTrace(staticDebugOutput);    System.exit(1);  }catch (  IOException e) {    LOG.error("",e);    e.printStackTrace(staticDebugOutput);    System.exit(2);  }catch (  Exception e) {
void dumpParsedTask(){  LOG.info("ParsedTask details:" + obtainCounters() + "\n"+ obtainFailedDueToAttemptId()+ "\nPreferred Locations are:");  List<LoggedLocation> loc=getPreferredLocations();  for (  LoggedLocation l : loc) {
private void printSimulationInfo(){  if (printSimulation) {    LOG.info("------------------------------------");    LOG.info("# nodes = {}, # racks = {}, capacity " + "of each node {}.",numNMs,numRacks,nodeManagerResource);    LOG.info("------------------------------------");    LOG.info("# applications = {}, # total " + "tasks = {}, average # tasks per application = {}",numAMs,numTasks,(int)(Math.ceil((numTasks + 0.0) / numAMs)));    LOG.info("JobId\tQueue\tAMType\tDuration\t#Tasks");    for (    Map.Entry<String,AMSimulator> entry : amMap.entrySet()) {      AMSimulator am=entry.getValue();
private ReservationId submitReservationWhenSpecified() throws IOException, InterruptedException {  if (reservationRequest != null) {    UserGroupInformation ugi=UserGroupInformation.createRemoteUser(user);    ugi.doAs(new PrivilegedExceptionAction<Object>(){      @Override public Object run() throws YarnException, IOException {        rm.getClientRMService().submitReservation(reservationRequest);
@SuppressWarnings("checkstyle:parameternumber") public void init(int heartbeatInterval,List<ContainerSimulator> containerList,ResourceManager resourceManager,SLSRunner slsRunnner,long startTime,long finishTime,String simUser,String simQueue,boolean tracked,String oldApp,long baseTimeMS,Resource amResource,String nodeLabelExpr,Map<String,String> params,Map<ApplicationId,AMSimulator> appIdAMSim){  super.init(heartbeatInterval,containerList,resourceManager,slsRunnner,startTime,finishTime,simUser,simQueue,tracked,oldApp,baseTimeMS,amResource,nodeLabelExpr,params,appIdAMSim);  super.amtype="dag";  allContainers.addAll(containerList);  pendingContainers.addAll(containerList);  totalContainers=allContainers.size();
protected void processResponseQueue() throws Exception {  while (!responseQueue.isEmpty()) {    AllocateResponse response=responseQueue.take();    if (!response.getCompletedContainersStatuses().isEmpty()) {      for (      ContainerStatus cs : response.getCompletedContainersStatuses()) {        ContainerId containerId=cs.getContainerId();        if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {          if (assignedContainers.containsKey(containerId)) {
      for (      ContainerStatus cs : response.getCompletedContainersStatuses()) {        ContainerId containerId=cs.getContainerId();        if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {          if (assignedContainers.containsKey(containerId)) {            LOG.debug("Application {} has one container finished ({}).",appId,containerId);            ContainerSimulator containerSimulator=assignedContainers.remove(containerId);            finishedContainers++;            completedContainers.add(containerSimulator);          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          }          if (finishedContainers >= totalContainers) {            lastStep();          }        } else {          if (assignedContainers.containsKey(containerId)) {
          if (assignedContainers.containsKey(containerId)) {            LOG.debug("Application {} has one container finished ({}).",appId,containerId);            ContainerSimulator containerSimulator=assignedContainers.remove(containerId);            finishedContainers++;            completedContainers.add(containerSimulator);          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          }          if (finishedContainers >= totalContainers) {            lastStep();          }        } else {          if (assignedContainers.containsKey(containerId)) {            LOG.error("Application {} has one container killed ({}).",appId,containerId);            pendingContainers.add(assignedContainers.remove(containerId));          } else           if (amContainer.getId().equals(containerId)) {
            finishedContainers++;            completedContainers.add(containerSimulator);          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          }          if (finishedContainers >= totalContainers) {            lastStep();          }        } else {          if (assignedContainers.containsKey(containerId)) {            LOG.error("Application {} has one container killed ({}).",appId,containerId);            pendingContainers.add(assignedContainers.remove(containerId));          } else           if (amContainer.getId().equals(containerId)) {            LOG.error("Application {}'s AM is " + "going to be killed. Waiting for rescheduling...",appId);          }        }      }    }    if (isAMContainerRunning && (finishedContainers >= totalContainers)) {      isAMContainerRunning=false;
          }          if (finishedContainers >= totalContainers) {            lastStep();          }        } else {          if (assignedContainers.containsKey(containerId)) {            LOG.error("Application {} has one container killed ({}).",appId,containerId);            pendingContainers.add(assignedContainers.remove(containerId));          } else           if (amContainer.getId().equals(containerId)) {            LOG.error("Application {}'s AM is " + "going to be killed. Waiting for rescheduling...",appId);          }        }      }    }    if (isAMContainerRunning && (finishedContainers >= totalContainers)) {      isAMContainerRunning=false;      LOG.info("Application {} sends out event to clean up" + " its AM container.",appId);      isFinished=true;      break;    }    for (    Container container : response.getAllocatedContainers()) {      if (!scheduledContainers.isEmpty()) {
@Override @SuppressWarnings("unchecked") protected void processResponseQueue() throws Exception {  while (!responseQueue.isEmpty()) {    AllocateResponse response=responseQueue.take();    if (!response.getCompletedContainersStatuses().isEmpty()) {      for (      ContainerStatus cs : response.getCompletedContainersStatuses()) {        ContainerId containerId=cs.getContainerId();        if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {          if (assignedMaps.containsKey(containerId)) {
      for (      ContainerStatus cs : response.getCompletedContainersStatuses()) {        ContainerId containerId=cs.getContainerId();        if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {          if (assignedMaps.containsKey(containerId)) {            LOG.debug("Application {} has one mapper finished ({}).",appId,containerId);            assignedMaps.remove(containerId);            mapFinished++;            finishedContainers++;          } else           if (assignedReduces.containsKey(containerId)) {            LOG.debug("Application {} has one reducer finished ({}).",appId,containerId);            assignedReduces.remove(containerId);            reduceFinished++;            finishedContainers++;          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;
            assignedMaps.remove(containerId);            mapFinished++;            finishedContainers++;          } else           if (assignedReduces.containsKey(containerId)) {            LOG.debug("Application {} has one reducer finished ({}).",appId,containerId);            assignedReduces.remove(containerId);            reduceFinished++;            finishedContainers++;          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          }          if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {            lastStep();          }        } else {          if (assignedMaps.containsKey(containerId)) {
          } else           if (assignedReduces.containsKey(containerId)) {            LOG.debug("Application {} has one reducer finished ({}).",appId,containerId);            assignedReduces.remove(containerId);            reduceFinished++;            finishedContainers++;          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          }          if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {            lastStep();          }        } else {          if (assignedMaps.containsKey(containerId)) {            LOG.debug("Application {} has one mapper killed ({}).",appId,containerId);            pendingFailedMaps.add(assignedMaps.remove(containerId));          } else           if (assignedReduces.containsKey(containerId)) {
            assignedReduces.remove(containerId);            reduceFinished++;            finishedContainers++;          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          }          if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {            lastStep();          }        } else {          if (assignedMaps.containsKey(containerId)) {            LOG.debug("Application {} has one mapper killed ({}).",appId,containerId);            pendingFailedMaps.add(assignedMaps.remove(containerId));          } else           if (assignedReduces.containsKey(containerId)) {            LOG.debug("Application {} has one reducer killed ({}).",appId,containerId);            pendingFailedReduces.add(assignedReduces.remove(containerId));
          } else           if (amContainer.getId().equals(containerId)) {            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          }          if (mapFinished >= mapTotal && reduceFinished >= reduceTotal) {            lastStep();          }        } else {          if (assignedMaps.containsKey(containerId)) {            LOG.debug("Application {} has one mapper killed ({}).",appId,containerId);            pendingFailedMaps.add(assignedMaps.remove(containerId));          } else           if (assignedReduces.containsKey(containerId)) {            LOG.debug("Application {} has one reducer killed ({}).",appId,containerId);            pendingFailedReduces.add(assignedReduces.remove(containerId));          } else           if (amContainer.getId().equals(containerId)) {            LOG.info("Application {}'s AM is " + "going to be killed. Waiting for rescheduling...",appId);          }        }      }    }    if (isAMContainerRunning && (mapFinished >= mapTotal) && (reduceFinished >= reduceTotal)) {
            lastStep();          }        } else {          if (assignedMaps.containsKey(containerId)) {            LOG.debug("Application {} has one mapper killed ({}).",appId,containerId);            pendingFailedMaps.add(assignedMaps.remove(containerId));          } else           if (assignedReduces.containsKey(containerId)) {            LOG.debug("Application {} has one reducer killed ({}).",appId,containerId);            pendingFailedReduces.add(assignedReduces.remove(containerId));          } else           if (amContainer.getId().equals(containerId)) {            LOG.info("Application {}'s AM is " + "going to be killed. Waiting for rescheduling...",appId);          }        }      }    }    if (isAMContainerRunning && (mapFinished >= mapTotal) && (reduceFinished >= reduceTotal)) {      isAMContainerRunning=false;      LOG.debug("Application {} sends out event to clean up" + " its AM container.",appId);      isFinished=true;      break;
            LOG.debug("Application {} has one mapper killed ({}).",appId,containerId);            pendingFailedMaps.add(assignedMaps.remove(containerId));          } else           if (assignedReduces.containsKey(containerId)) {            LOG.debug("Application {} has one reducer killed ({}).",appId,containerId);            pendingFailedReduces.add(assignedReduces.remove(containerId));          } else           if (amContainer.getId().equals(containerId)) {            LOG.info("Application {}'s AM is " + "going to be killed. Waiting for rescheduling...",appId);          }        }      }    }    if (isAMContainerRunning && (mapFinished >= mapTotal) && (reduceFinished >= reduceTotal)) {      isAMContainerRunning=false;      LOG.debug("Application {} sends out event to clean up" + " its AM container.",appId);      isFinished=true;      break;    }    for (    Container container : response.getAllocatedContainers()) {      if (!scheduledMaps.isEmpty()) {        ContainerSimulator cs=scheduledMaps.remove();
  }  List<ResourceRequest> ask=null;  if (mapFinished != mapTotal) {    if (!pendingMaps.isEmpty()) {      ask=packageRequests(mergeLists(pendingMaps,scheduledMaps),PRIORITY_MAP);      LOG.debug("Application {} sends out request for {} mappers.",appId,pendingMaps.size());      scheduledMaps.addAll(pendingMaps);      pendingMaps.clear();    } else     if (!pendingFailedMaps.isEmpty()) {      ask=packageRequests(mergeLists(pendingFailedMaps,scheduledMaps),PRIORITY_MAP);      LOG.debug("Application {} sends out requests for {} failed mappers.",appId,pendingFailedMaps.size());      scheduledMaps.addAll(pendingFailedMaps);      pendingFailedMaps.clear();    }  } else   if (reduceFinished != reduceTotal) {    if (!pendingReduces.isEmpty()) {      ask=packageRequests(mergeLists(pendingReduces,scheduledReduces),PRIORITY_REDUCE);
      LOG.debug("Application {} sends out request for {} mappers.",appId,pendingMaps.size());      scheduledMaps.addAll(pendingMaps);      pendingMaps.clear();    } else     if (!pendingFailedMaps.isEmpty()) {      ask=packageRequests(mergeLists(pendingFailedMaps,scheduledMaps),PRIORITY_MAP);      LOG.debug("Application {} sends out requests for {} failed mappers.",appId,pendingFailedMaps.size());      scheduledMaps.addAll(pendingFailedMaps);      pendingFailedMaps.clear();    }  } else   if (reduceFinished != reduceTotal) {    if (!pendingReduces.isEmpty()) {      ask=packageRequests(mergeLists(pendingReduces,scheduledReduces),PRIORITY_REDUCE);      LOG.debug("Application {} sends out requests for {} reducers.",appId,pendingReduces.size());      scheduledReduces.addAll(pendingReduces);      pendingReduces.clear();    } else     if (!pendingFailedReduces.isEmpty()) {
@SuppressWarnings("checkstyle:parameternumber") public void init(int heartbeatInterval,List<ContainerSimulator> containerList,ResourceManager rm,SLSRunner se,long traceStartTime,long traceFinishTime,String user,String queue,boolean isTracked,String oldAppId,long baselineStartTimeMS,Resource amContainerResource,String nodeLabelExpr,Map<String,String> params,Map<ApplicationId,AMSimulator> appIdAMSim){  super.init(heartbeatInterval,containerList,rm,se,traceStartTime,traceFinishTime,user,queue,isTracked,oldAppId,baselineStartTimeMS,amContainerResource,nodeLabelExpr,params,appIdAMSim);  amtype="stream";  allStreams.addAll(containerList);  duration=traceFinishTime - traceStartTime;
@Override @SuppressWarnings("unchecked") protected void processResponseQueue() throws Exception {  while (!responseQueue.isEmpty()) {    AllocateResponse response=responseQueue.take();    if (!response.getCompletedContainersStatuses().isEmpty()) {      for (      ContainerStatus cs : response.getCompletedContainersStatuses()) {        ContainerId containerId=cs.getContainerId();        if (assignedStreams.containsKey(containerId)) {
@Override @SuppressWarnings("unchecked") protected void processResponseQueue() throws Exception {  while (!responseQueue.isEmpty()) {    AllocateResponse response=responseQueue.take();    if (!response.getCompletedContainersStatuses().isEmpty()) {      for (      ContainerStatus cs : response.getCompletedContainersStatuses()) {        ContainerId containerId=cs.getContainerId();        if (assignedStreams.containsKey(containerId)) {          LOG.debug("Application {} has one streamer finished ({}).",appId,containerId);          pendingStreams.add(assignedStreams.remove(containerId));        } else         if (amContainer.getId().equals(containerId)) {          if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {            isAMContainerRunning=false;            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          } else {
    if (!response.getCompletedContainersStatuses().isEmpty()) {      for (      ContainerStatus cs : response.getCompletedContainersStatuses()) {        ContainerId containerId=cs.getContainerId();        if (assignedStreams.containsKey(containerId)) {          LOG.debug("Application {} has one streamer finished ({}).",appId,containerId);          pendingStreams.add(assignedStreams.remove(containerId));        } else         if (amContainer.getId().equals(containerId)) {          if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {            isAMContainerRunning=false;            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          } else {            LOG.info("Application {}'s AM is " + "going to be killed. Waiting for rescheduling...",appId);            isAMContainerRunning=false;          }        }      }    }    if (isAMContainerRunning && (System.currentTimeMillis() - simulateStartTimeMS >= duration)) {
 else         if (amContainer.getId().equals(containerId)) {          if (cs.getExitStatus() == ContainerExitStatus.SUCCESS) {            isAMContainerRunning=false;            isFinished=true;            LOG.info("Application {} goes to finish.",appId);          } else {            LOG.info("Application {}'s AM is " + "going to be killed. Waiting for rescheduling...",appId);            isAMContainerRunning=false;          }        }      }    }    if (isAMContainerRunning && (System.currentTimeMillis() - simulateStartTimeMS >= duration)) {      LOG.debug("Application {} sends out event to clean up" + " its AM container.",appId);      isAMContainerRunning=false;      isFinished=true;      break;    }    for (    Container container : response.getAllocatedContainers()) {      if (!scheduledStreams.isEmpty()) {
@Override protected void sendContainerRequest() throws YarnException, IOException, InterruptedException {  List<ResourceRequest> ask=new ArrayList<>();  List<ContainerId> release=new ArrayList<>();  if (!isFinished) {    if (!pendingStreams.isEmpty()) {      ask=packageRequests(mergeLists(pendingStreams,scheduledStreams),PRIORITY_MAP);
@Override public void middleStep() throws Exception {  ContainerSimulator cs=null;synchronized (completedContainerList) {    while ((cs=containerQueue.poll()) != null) {      runningContainers.remove(cs.getId());      completedContainerList.add(cs.getId());
  ns.setResponseId(responseId++);  ns.setNodeHealthStatus(NodeHealthStatus.newInstance(true,"",0));  if (resourceUtilizationRatio > 0 && resourceUtilizationRatio <= 1) {    int pMemUsed=Math.round(node.getTotalCapability().getMemorySize() * resourceUtilizationRatio);    float cpuUsed=node.getTotalCapability().getVirtualCores() * resourceUtilizationRatio;    ResourceUtilization resourceUtilization=ResourceUtilization.newInstance(pMemUsed,pMemUsed,cpuUsed);    ns.setContainersUtilization(resourceUtilization);    ns.setNodeUtilization(resourceUtilization);  }  beatRequest.setNodeStatus(ns);  NodeHeartbeatResponse beatResponse=rm.getResourceTrackerService().nodeHeartbeat(beatRequest);  if (!beatResponse.getContainersToCleanup().isEmpty()) {synchronized (releasedContainerList) {      for (      ContainerId containerId : beatResponse.getContainersToCleanup()) {        if (amContainerList.contains(containerId)) {synchronized (amContainerList) {
    float cpuUsed=node.getTotalCapability().getVirtualCores() * resourceUtilizationRatio;    ResourceUtilization resourceUtilization=ResourceUtilization.newInstance(pMemUsed,pMemUsed,cpuUsed);    ns.setContainersUtilization(resourceUtilization);    ns.setNodeUtilization(resourceUtilization);  }  beatRequest.setNodeStatus(ns);  NodeHeartbeatResponse beatResponse=rm.getResourceTrackerService().nodeHeartbeat(beatRequest);  if (!beatResponse.getContainersToCleanup().isEmpty()) {synchronized (releasedContainerList) {      for (      ContainerId containerId : beatResponse.getContainersToCleanup()) {        if (amContainerList.contains(containerId)) {synchronized (amContainerList) {            amContainerList.remove(containerId);          }          LOG.debug("NodeManager {} releases an AM ({}).",node.getNodeID(),containerId);        } else {          cs=runningContainers.remove(containerId);
public void addNewContainer(Container container,long lifeTimeMS){
private void initMetricsCSVOutput(){  int timeIntervalMS=conf.getInt(SLSConfiguration.METRICS_RECORD_INTERVAL_MS,SLSConfiguration.METRICS_RECORD_INTERVAL_MS_DEFAULT);  File dir=new File(metricsOutputDir + "/metrics");  if (!dir.exists() && !dir.mkdirs()) {
@Test public void testWorkloadGenerateTime() throws IllegalArgumentException, IOException {  String workloadJson="{\"job_classes\": [], \"time_distribution\":[" + "{\"time\": 0, \"weight\": 1}, " + "{\"time\": 30, \"weight\": 0},"+ "{\"time\": 60, \"weight\": 2},"+ "{\"time\": 90, \"weight\": 1}"+ "]}";  ObjectMapper mapper=new ObjectMapper();  mapper.configure(INTERN_FIELD_NAMES,true);  mapper.configure(FAIL_ON_UNKNOWN_PROPERTIES,false);  SynthTraceJobProducer.Workload wl=mapper.readValue(workloadJson,SynthTraceJobProducer.Workload.class);  JDKRandomGenerator rand=new JDKRandomGenerator();  rand.setSeed(0);  wl.init(rand);  int bucket0=0;  int bucket1=0;  int bucket2=0;  int bucket3=0;  for (int i=0; i < 1000; ++i) {    long time=wl.generateSubmissionTime();
    LOG.info("Generated time " + time);    if (time < 30) {      bucket0++;    } else     if (time < 60) {      bucket1++;    } else     if (time < 90) {      bucket2++;    } else {      bucket3++;    }  }  Assert.assertTrue(bucket0 > 0);  Assert.assertTrue(bucket1 == 0);  Assert.assertTrue(bucket2 > 0);  Assert.assertTrue(bucket3 > 0);  Assert.assertTrue(bucket2 > bucket0);  Assert.assertTrue(bucket2 > bucket3);
@Test public void testMapReduce() throws IllegalArgumentException, IOException {  Configuration conf=new Configuration();  conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE,"src/test/resources/syn.json");  SynthTraceJobProducer stjp=new SynthTraceJobProducer(conf);
@Test public void testMapReduce() throws IllegalArgumentException, IOException {  Configuration conf=new Configuration();  conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE,"src/test/resources/syn.json");  SynthTraceJobProducer stjp=new SynthTraceJobProducer(conf);  LOG.info(stjp.toString());  SynthJob js=(SynthJob)stjp.getNextJob();  int jobCount=0;  while (js != null) {
@Test public void testGeneric() throws IllegalArgumentException, IOException {  Configuration conf=new Configuration();  conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE,"src/test/resources/syn_generic.json");  SynthTraceJobProducer stjp=new SynthTraceJobProducer(conf);
@Test public void testGeneric() throws IllegalArgumentException, IOException {  Configuration conf=new Configuration();  conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE,"src/test/resources/syn_generic.json");  SynthTraceJobProducer stjp=new SynthTraceJobProducer(conf);  LOG.info(stjp.toString());  SynthJob js=(SynthJob)stjp.getNextJob();  int jobCount=0;  while (js != null) {
@Test public void testStream() throws IllegalArgumentException, IOException {  Configuration conf=new Configuration();  conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE,"src/test/resources/syn_stream.json");  SynthTraceJobProducer stjp=new SynthTraceJobProducer(conf);
@Test public void testStream() throws IllegalArgumentException, IOException {  Configuration conf=new Configuration();  conf.set(SynthTraceJobProducer.SLS_SYNTHETIC_TRACE_FILE,"src/test/resources/syn_stream.json");  SynthTraceJobProducer stjp=new SynthTraceJobProducer(conf);  LOG.info(stjp.toString());  SynthJob js=(SynthJob)stjp.getNextJob();  int jobCount=0;  while (js != null) {
      }      f=null;    }    LOG.info("PipeMapRed exec " + Arrays.asList(argvSplit));    Environment childEnv=(Environment)StreamUtil.env().clone();    addJobConfToEnvironment(job_,childEnv);    addEnvironment(childEnv,job_.get("stream.addenvironment"));    envPut(childEnv,"TMPDIR",System.getProperty("java.io.tmpdir"));    ProcessBuilder builder=new ProcessBuilder(argvSplit);    builder.environment().putAll(childEnv.toMap());    sim=builder.start();    clientOut_=new DataOutputStream(new BufferedOutputStream(sim.getOutputStream(),BUFFER_SIZE));    clientIn_=new DataInputStream(new BufferedInputStream(sim.getInputStream(),BUFFER_SIZE));    clientErr_=new DataInputStream(new BufferedInputStream(sim.getErrorStream()));    startTime_=System.currentTimeMillis();  } catch (  IOException e) {    LOG.error("configuration exception",e);
void setStreamJobDetails(JobConf job){  String s=job.get("stream.minRecWrittenToEnableSkip_");  if (s != null) {    minRecWrittenToEnableSkip_=Long.parseLong(s);
void envPut(Properties env,String name,String value){  if (LOG.isDebugEnabled()) {
      LOG.info("mapRedFinished");      return;    }    if (clientOut_ != null) {      try {        clientOut_.flush();        clientOut_.close();      } catch (      IOException io) {        LOG.warn("{}",io);      }    }    try {      waitOutputThreads();    } catch (    IOException io) {      LOG.warn("{}",io);    }    if (sim != null)     sim.destroy();    LOG.info("mapRedFinished");  } catch (  RuntimeException e) {
void maybeLogRecord(){  if (numRecRead_ >= nextRecReadLog_) {    String info=numRecInfo();
    throw new IOException("MROutput/MRErrThread failed:",outerrThreadsThrowable);  }  try {    numRecRead_++;    maybeLogRecord();    if (numExceptions_ == 0) {      if (!this.ignoreKey) {        inWriter_.writeKey(key);      }      inWriter_.writeValue(value);      if (skipping) {        clientOut_.flush();      }    } else {      numRecSkipped_++;    }  } catch (  IOException io) {    numExceptions_++;    if (numExceptions_ > 1 || numRecWritten_ < minRecWrittenToEnableSkip_) {
void numRecStats(byte[] record,int start,int len) throws IOException {  numRec_++;  if (numRec_ == nextStatusRec_) {    String recordStr=new String(record,start,Math.min(len,statusMaxRecordChars_),"UTF-8");    nextStatusRec_+=100;    String status=getStatus(recordStr);
  if (jar_ != null && isLocalHadoop()) {    File wd=new File(".").getAbsoluteFile();    RunJar.unJar(new File(jar_),wd,MATCH_ANY);  }  jc_=new JobClient(jobConf_);  running_=null;  try {    running_=jc_.submitJob(jobConf_);    jobId_=running_.getID();    if (background_) {      LOG.info("Job is running in background.");    } else     if (!jc_.monitorAndPrintJob(jobConf_,running_)) {      LOG.error("Job not successful!");      return 1;    }    LOG.info("Output directory: " + output_);  } catch (  FileNotFoundException fe) {
  }  jc_=new JobClient(jobConf_);  running_=null;  try {    running_=jc_.submitJob(jobConf_);    jobId_=running_.getID();    if (background_) {      LOG.info("Job is running in background.");    } else     if (!jc_.monitorAndPrintJob(jobConf_,running_)) {      LOG.error("Job not successful!");      return 1;    }    LOG.info("Output directory: " + output_);  } catch (  FileNotFoundException fe) {    LOG.error("Error launching job , bad input path : " + fe.getMessage());    return 2;  }catch (  InvalidJobConfException je) {
void numRecStats(byte[] record,int start,int len) throws IOException {  numRec_++;  if (numRec_ == nextStatusRec_) {    String recordStr=new String(record,start,Math.min(len,statusMaxRecordChars_),"UTF-8");    nextStatusRec_+=100;    String status=getStatus(recordStr);
private static void addMandatoryResources(Map<String,ResourceInformation> res){  ResourceInformation ri;  if (!res.containsKey(MEMORY)) {
private static long getAllocation(Configuration conf,String resourceTypesKey,String schedulerKey,long schedulerDefault){  long value=conf.getLong(resourceTypesKey,-1L);  if (value == -1) {
private static Map<String,ResourceInformation> getResourceInformationMapFromConfig(Configuration conf){  Map<String,ResourceInformation> resourceInformationMap=new HashMap<>();  String[] resourceNames=conf.getStrings(YarnConfiguration.RESOURCE_TYPES);  if (resourceNames != null && resourceNames.length != 0) {    for (    String resourceName : resourceNames) {      String resourceUnits=conf.get(YarnConfiguration.RESOURCE_TYPES + "." + resourceName+ UNITS,"");      String resourceTypeName=conf.get(YarnConfiguration.RESOURCE_TYPES + "." + resourceName+ TYPE,ResourceTypes.COUNTABLE.toString());      Long minimumAllocation=conf.getLong(YarnConfiguration.RESOURCE_TYPES + "." + resourceName+ MINIMUM_ALLOCATION,0L);      Long maximumAllocation=conf.getLong(YarnConfiguration.RESOURCE_TYPES + "." + resourceName+ MAXIMUM_ALLOCATION,Long.MAX_VALUE);      if (resourceName == null || resourceName.isEmpty() || resourceUnits == null || resourceTypeName == null) {        throw new YarnRuntimeException("Incomplete configuration for resource type '" + resourceName + "'. One of name, units or type is configured incorrectly.");      }      ResourceTypes resourceType=ResourceTypes.valueOf(resourceTypeName);      String[] resourceTags=conf.getTrimmedStrings(YarnConfiguration.RESOURCE_TYPES + "." + resourceName+ TAGS);      Set<String> resourceTagSet=new HashSet<>();      Collections.addAll(resourceTagSet,resourceTags);
private static void addResourcesFileToConf(String resourceFile,Configuration conf){  try {    InputStream ris=getConfInputStream(resourceFile,conf);
private static void addResourceTypeInformation(String prop,String value,Map<String,ResourceInformation> nodeResources){  if (prop.startsWith(YarnConfiguration.NM_RESOURCES_PREFIX)) {
public static List<ResourceInformation> getRequestedResourcesFromConfig(Configuration configuration,String prefix){  List<ResourceInformation> result=new ArrayList<>();  Map<String,String> customResourcesMap=configuration.getValByRegex("^" + Pattern.quote(prefix) + YARN_IO_OPTIONAL+ "[^.]+$");  for (  Entry<String,String> resource : customResourcesMap.entrySet()) {    String resourceName=resource.getKey().substring(prefix.length());    Matcher matcher=RESOURCE_REQUEST_VALUE_PATTERN.matcher(resource.getValue());    if (!matcher.matches()) {      String errorMsg="Invalid resource request specified for property " + resource.getKey() + ": \""+ resource.getValue()+ "\", expected format is: value[ ][units]";
    response=solr.query(query);    Iterator<SolrDocument> list=response.getResults().listIterator();    while (list.hasNext()) {      SolrDocument d=list.next();      AppStoreEntry entry=new AppStoreEntry();      entry.setId(d.get("id").toString());      entry.setOrg(d.get("org_s").toString());      entry.setName(d.get("name_s").toString());      entry.setDesc(d.get("desc_s").toString());      if (d.get("icon_s") != null) {        entry.setIcon(d.get("icon_s").toString());      }      entry.setLike(Integer.parseInt(d.get("like_i").toString()));      entry.setDownload(Integer.parseInt(d.get("download_i").toString()));      apps.add(entry);    }  } catch (  SolrServerException|IOException e) {
  query.setFilterQueries("type_s:AppEntry");  query.setRows(40);  QueryResponse response;  try {    response=solr.query(query);    Iterator<SolrDocument> appList=response.getResults().listIterator();    while (appList.hasNext()) {      SolrDocument d=appList.next();      AppEntry entry=new AppEntry();      entry.setId(d.get("id").toString());      entry.setName(d.get("name_s").toString());      entry.setApp(d.get("app_s").toString());      entry.setYarnfile(mapper.readValue(d.get("yarnfile_s").toString(),Service.class));      list.add(entry);    }  } catch (  SolrServerException|IOException e) {
      entry.setDownload(Integer.parseInt(d.get("download_i").toString()));      Service yarnApp=mapper.readValue(d.get("yarnfile_s").toString(),Service.class);      String name;      try {        Random r=new Random();        int low=3;        int high=10;        int seed=r.nextInt(high - low) + low;        int seed2=r.nextInt(high - low) + low;        name=RandomWord.getNewWord(seed).toLowerCase() + "-" + RandomWord.getNewWord(seed2).toLowerCase();      } catch (      WordLengthException e) {        name="c" + java.util.UUID.randomUUID().toString().substring(0,11);      }      yarnApp.setName(name);      entry.setApp(yarnApp);    }  } catch (  SolrServerException|IOException e) {
  SolrQuery query=new SolrQuery();  query.setQuery("id:" + id);  query.setFilterQueries("type_s:AppEntry");  query.setRows(1);  QueryResponse response;  try {    response=solr.query(query);    Iterator<SolrDocument> appList=response.getResults().listIterator();    while (appList.hasNext()) {      SolrDocument d=appList.next();      entry.setId(d.get("id").toString());      entry.setApp(d.get("app_s").toString());      entry.setName(d.get("name_s").toString());      entry.setYarnfile(mapper.readValue(d.get("yarnfile_s").toString(),Service.class));    }  } catch (  SolrServerException|IOException e) {
  SolrClient solr=getSolrClient();  ObjectMapper mapper=new ObjectMapper();  mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES,false);  try {    SolrInputDocument buffer=new SolrInputDocument();    buffer.setField("id",java.util.UUID.randomUUID().toString().substring(0,11));    buffer.setField("org_s",app.getOrganization());    buffer.setField("name_s",app.getName());    buffer.setField("desc_s",app.getDescription());    if (app.getIcon() != null) {      buffer.setField("icon_s",app.getIcon());    }    buffer.setField("type_s","AppStoreEntry");    buffer.setField("like_i",0);    buffer.setField("download_i",0);    String yarnFile=mapper.writeValueAsString(app);
  SolrClient solr=getSolrClient();  ObjectMapper mapper=new ObjectMapper();  mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES,false);  try {    SolrInputDocument buffer=new SolrInputDocument();    buffer.setField("id",java.util.UUID.randomUUID().toString().substring(0,11));    buffer.setField("org_s",app.getOrg());    buffer.setField("name_s",app.getName());    buffer.setField("desc_s",app.getDesc());    if (app.getIcon() != null) {      buffer.setField("icon_s",app.getIcon());    }    buffer.setField("type_s","AppStoreEntry");    buffer.setField("like_i",app.getLike());    buffer.setField("download_i",app.getDownload());    String yarnFile=mapper.writeValueAsString(app);
  if (service != null) {    String name=service.getName();    String app="";    SolrQuery query=new SolrQuery();    query.setQuery("id:" + name);    query.setFilterQueries("type_s:AppEntry");    query.setRows(1);    QueryResponse response;    try {      response=solr.query(query);      Iterator<SolrDocument> appList=response.getResults().listIterator();      while (appList.hasNext()) {        SolrDocument d=appList.next();        app=d.get("app_s").toString();      }    } catch (    SolrServerException|IOException e) {
      sb.append("/");      sb.append("_HOST");      sb.append("@");      sb.append(temp[1]);      String keytab=System.getenv("KEYTAB");      if (!keytab.startsWith("file://")) {        keytab="file://" + keytab;      }      kerberos.setPrincipalName(sb.toString());      kerberos.setKeytab(keytab);      app.setKerberosPrincipal(kerberos);    }    response=asc.getApiClient().post(ClientResponse.class,mapper.writeValueAsString(app));    if (response.getStatus() >= 299) {      String message=response.getEntity(String.class);      throw new RuntimeException("Failed : HTTP error code : " + response.getStatus() + " error: "+ message);    }  } catch (  UniformInterfaceException|ClientHandlerException|IOException e) {
private void dumpOutDebugInfo(){  LOG.info("Dump debug output");  Map<String,String> envs=System.getenv();  for (  Map.Entry<String,String> env : envs.entrySet()) {
  }  if (fileExist(log4jPath)) {    try {      Log4jPropertyHelper.updateLog4jConfiguration(ApplicationMaster.class,log4jPath);    } catch (    Exception e) {      LOG.warn("Can not set up custom log4j properties. " + e);    }  }  appName=cliParser.getOptionValue("appname","DistributedShell");  if (cliParser.hasOption("help")) {    printUsage(opts);    return false;  }  if (cliParser.hasOption("debug")) {    dumpOutDebugInfo();  }  homeDirectory=cliParser.hasOption("homedir") ? new Path(cliParser.getOptionValue("homedir")) : new Path("/user/" + System.getenv(ApplicationConstants.Environment.USER.name()));  if (cliParser.hasOption("placement_spec")) {    String placementSpec=cliParser.getOptionValue("placement_spec");    String decodedSpec=getDecodedPlacementSpec(placementSpec);
    } catch (    Exception e) {      LOG.warn("Can not set up custom log4j properties. " + e);    }  }  appName=cliParser.getOptionValue("appname","DistributedShell");  if (cliParser.hasOption("help")) {    printUsage(opts);    return false;  }  if (cliParser.hasOption("debug")) {    dumpOutDebugInfo();  }  homeDirectory=cliParser.hasOption("homedir") ? new Path(cliParser.getOptionValue("homedir")) : new Path("/user/" + System.getenv(ApplicationConstants.Environment.USER.name()));  if (cliParser.hasOption("placement_spec")) {    String placementSpec=cliParser.getOptionValue("placement_spec");    String decodedSpec=getDecodedPlacementSpec(placementSpec);    LOG.info("Placement Spec received [{}]",decodedSpec);    this.numTotalContainers=0;    int globalNumOfContainers=Integer.parseInt(cliParser.getOptionValue("num_containers","0"));
private String getDecodedPlacementSpec(String placementSpecifications){  Base64.Decoder decoder=Base64.getDecoder();  byte[] decodedBytes=decoder.decode(placementSpecifications.getBytes(StandardCharsets.UTF_8));  String decodedSpec=new String(decodedBytes,StandardCharsets.UTF_8);
@SuppressWarnings({"unchecked"}) public void run() throws YarnException, IOException, InterruptedException {  LOG.info("Starting ApplicationMaster");  Credentials credentials=UserGroupInformation.getCurrentUser().getCredentials();  DataOutputBuffer dob=new DataOutputBuffer();  credentials.writeTokenStorageToStream(dob);  Iterator<Token<?>> iter=credentials.getAllTokens().iterator();  LOG.info("Executing with tokens:");  while (iter.hasNext()) {    Token<?> token=iter.next();
  if (timelineServiceV1Enabled) {    publishApplicationAttemptEvent(timelineClient,appAttemptID.toString(),DSEvent.DS_APP_ATTEMPT_START,domainId,appSubmitterUgi);  }  appMasterHostname=NetUtils.getHostname();  Map<Set<String>,PlacementConstraint> placementConstraintMap=null;  if (this.placementSpecs != null) {    placementConstraintMap=new HashMap<>();    for (    PlacementSpec spec : this.placementSpecs.values()) {      if (spec.constraint != null) {        Set<String> allocationTags=Strings.isNullOrEmpty(spec.sourceTag) ? Collections.emptySet() : Collections.singleton(spec.sourceTag);        placementConstraintMap.put(allocationTags,spec.constraint);      }    }  }  RegisterApplicationMasterResponse response=amRMClient.registerApplicationMaster(appMasterHostname,appMasterRpcPort,appMasterTrackingUrl,placementConstraintMap);  resourceProfiles=response.getResourceProfiles();  ResourceUtils.reinitializeResources(response.getResourceTypes());  long maxMem=response.getMaximumResourceCapability().getMemorySize();  LOG.info("Max mem capability of resources in this cluster " + maxMem);
  appMasterHostname=NetUtils.getHostname();  Map<Set<String>,PlacementConstraint> placementConstraintMap=null;  if (this.placementSpecs != null) {    placementConstraintMap=new HashMap<>();    for (    PlacementSpec spec : this.placementSpecs.values()) {      if (spec.constraint != null) {        Set<String> allocationTags=Strings.isNullOrEmpty(spec.sourceTag) ? Collections.emptySet() : Collections.singleton(spec.sourceTag);        placementConstraintMap.put(allocationTags,spec.constraint);      }    }  }  RegisterApplicationMasterResponse response=amRMClient.registerApplicationMaster(appMasterHostname,appMasterRpcPort,appMasterTrackingUrl,placementConstraintMap);  resourceProfiles=response.getResourceProfiles();  ResourceUtils.reinitializeResources(response.getResourceTypes());  long maxMem=response.getMaximumResourceCapability().getMemorySize();  LOG.info("Max mem capability of resources in this cluster " + maxMem);  int maxVCores=response.getMaximumResourceCapability().getVirtualCores();  LOG.info("Max vcores capability of resources in this cluster " + maxVCores);
private ContainerRequest setupContainerAskForRM(){  Priority pri=Priority.newInstance(requestPriority);  ContainerRequest request=new ContainerRequest(getTaskResourceCapability(),null,null,pri,0,true,null,ExecutionTypeRequest.newInstance(containerType,enforceExecType),containerResourceProfile);
private SchedulingRequest setupSchedulingRequest(PlacementSpec spec){  long allocId=allocIdCounter.incrementAndGet();  SchedulingRequest sReq=SchedulingRequest.newInstance(allocId,Priority.newInstance(requestPriority),ExecutionTypeRequest.newInstance(),Collections.singleton(spec.sourceTag),ResourceSizing.newInstance(getTaskResourceCapability()),null);  sReq.setPlacementConstraint(spec.constraint);
  final TimelineEntity entity=new TimelineEntity();  entity.setEntityId(container.getId().toString());  entity.setEntityType(DSEntity.DS_CONTAINER.toString());  entity.setDomainId(domainId);  entity.addPrimaryFilter(USER_TIMELINE_FILTER_NAME,ugi.getShortUserName());  entity.addPrimaryFilter(APPID_TIMELINE_FILTER_NAME,container.getId().getApplicationAttemptId().getApplicationId().toString());  TimelineEvent event=new TimelineEvent();  event.setTimestamp(System.currentTimeMillis());  event.setEventType(DSEvent.DS_CONTAINER_START.toString());  event.addEventInfo("Node",container.getNodeId().toString());  event.addEventInfo("Resources",container.getResource().toString());  entity.addEvent(event);  try {    processTimelineResponseErrors(putContainerEntity(timelineClient,container.getId().getApplicationAttemptId(),entity));  } catch (  YarnException|IOException|ClientHandlerException e) {
  entity.setEntityId(container.getContainerId().toString());  entity.setEntityType(DSEntity.DS_CONTAINER.toString());  entity.setDomainId(domainId);  entity.addPrimaryFilter(USER_TIMELINE_FILTER_NAME,ugi.getShortUserName());  entity.addPrimaryFilter(APPID_TIMELINE_FILTER_NAME,container.getContainerId().getApplicationAttemptId().getApplicationId().toString());  TimelineEvent event=new TimelineEvent();  event.setTimestamp(System.currentTimeMillis());  event.setEventType(DSEvent.DS_CONTAINER_END.toString());  event.addEventInfo("State",container.getState().name());  event.addEventInfo("Exit Status",container.getExitStatus());  event.addEventInfo(DIAGNOSTICS,container.getDiagnostics());  entity.addEvent(event);  try {    processTimelineResponseErrors(putContainerEntity(timelineClient,container.getContainerId().getApplicationAttemptId(),entity));  } catch (  YarnException|IOException|ClientHandlerException e) {
  final org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity entity=new org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity();  entity.setId(containerId.toString());  entity.setType(DSEntity.DS_CONTAINER.toString());  entity.addInfo("user",appSubmitterUgi.getShortUserName());  org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent event=new org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent();  event.setTimestamp(System.currentTimeMillis());  event.setId(DSEvent.DS_CONTAINER_END.toString());  event.addInfo(DIAGNOSTICS,diagnostics);  entity.addEvent(event);  try {    appSubmitterUgi.doAs((PrivilegedExceptionAction<Object>)() -> {      timelineV2Client.putEntitiesAsync(entity);      return null;    });  } catch (  Exception e) {
private void publishContainerStartFailedEvent(final ContainerId containerId,String diagnostics){  final TimelineEntity entityV1=new TimelineEntity();  entityV1.setEntityId(containerId.toString());  entityV1.setEntityType(DSEntity.DS_CONTAINER.toString());  entityV1.setDomainId(domainId);  entityV1.addPrimaryFilter(USER_TIMELINE_FILTER_NAME,appSubmitterUgi.getShortUserName());  entityV1.addPrimaryFilter(APPID_TIMELINE_FILTER_NAME,containerId.getApplicationAttemptId().getApplicationId().toString());  TimelineEvent eventV1=new TimelineEvent();  eventV1.setTimestamp(System.currentTimeMillis());  eventV1.setEventType(DSEvent.DS_CONTAINER_END.toString());  eventV1.addEventInfo(DIAGNOSTICS,diagnostics);  entityV1.addEvent(eventV1);  try {    processTimelineResponseErrors(putContainerEntity(timelineClient,containerId.getApplicationAttemptId(),entityV1));  } catch (  YarnException|IOException|ClientHandlerException e) {
  if (appEvent == DSEvent.DS_APP_ATTEMPT_START) {    entity.setCreatedTime(ts);  }  entity.addInfo("user",appSubmitterUgi.getShortUserName());  org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent event=new org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent();  event.setId(appEvent.toString());  event.setTimestamp(ts);  entity.addEvent(event);  entity.setIdPrefix(TimelineServiceHelper.invertLong(appAttemptID.getAttemptId()));  try {    appSubmitterUgi.doAs(new PrivilegedExceptionAction<Object>(){      @Override public TimelinePutResponse run() throws Exception {        timelineV2Client.putEntitiesAsync(entity);        return null;      }    });  } catch (  Exception e) {
public static void main(String[] args){  boolean result=false;  try {    Client client=new Client();    LOG.info("Initializing Client");    try {      boolean doRun=client.init(args);      if (!doRun) {        System.exit(0);      }    } catch (    IllegalArgumentException e) {      System.err.println(e.getLocalizedMessage());      client.printUsage();      System.exit(-1);    }    result=client.run();  } catch (  Throwable t) {
public boolean run() throws IOException, YarnException {  LOG.info("Running Client");  yarnClient.start();  YarnClusterMetrics clusterMetrics=yarnClient.getYarnClusterMetrics();
public boolean run() throws IOException, YarnException {  LOG.info("Running Client");  yarnClient.start();  YarnClusterMetrics clusterMetrics=yarnClient.getYarnClusterMetrics();  LOG.info("Got Cluster metric info from ASM" + ", numNodeManagers=" + clusterMetrics.getNumNodeManagers());  List<NodeReport> clusterNodeReports=yarnClient.getNodeReports(NodeState.RUNNING);  LOG.info("Got Cluster node info from ASM");  for (  NodeReport node : clusterNodeReports) {
  LOG.info("Running Client");  yarnClient.start();  YarnClusterMetrics clusterMetrics=yarnClient.getYarnClusterMetrics();  LOG.info("Got Cluster metric info from ASM" + ", numNodeManagers=" + clusterMetrics.getNumNodeManagers());  List<NodeReport> clusterNodeReports=yarnClient.getNodeReports(NodeState.RUNNING);  LOG.info("Got Cluster node info from ASM");  for (  NodeReport node : clusterNodeReports) {    LOG.info("Got node report from ASM for" + ", nodeId=" + node.getNodeId() + ", nodeAddress="+ node.getHttpAddress()+ ", nodeRackName="+ node.getRackName()+ ", nodeNumContainers="+ node.getNumContainers());  }  QueueInfo queueInfo=yarnClient.getQueueInfo(this.amQueue);  if (queueInfo == null) {    throw new IllegalArgumentException(String.format("Queue %s not present in scheduler configuration.",this.amQueue));  }  LOG.info("Queue info" + ", queueName=" + queueInfo.getQueueName() + ", queueCurrentCapacity="+ queueInfo.getCurrentCapacity()+ ", queueMaxCapacity="+ queueInfo.getMaximumCapacity()+ ", queueApplicationCount="+ queueInfo.getApplications().size()+ ", queueChildQueueCount="+ queueInfo.getChildQueues().size());  List<QueueUserACLInfo> listAclInfo=yarnClient.getQueueAclsInfo();  for (  QueueUserACLInfo aclInfo : listAclInfo) {    for (    QueueACL userAcl : aclInfo.getUserAcls()) {
  }  if (domainId != null && domainId.length() > 0 && toCreateDomain) {    prepareTimelineDomain();  }  Map<String,Resource> profiles;  try {    profiles=yarnClient.getResourceProfiles();  } catch (  YARNFeatureNotEnabledException re) {    profiles=null;  }  List<String> appProfiles=new ArrayList<>(2);  appProfiles.add(amResourceProfile);  appProfiles.add(containerResourceProfile);  for (  String appProfile : appProfiles) {    if (appProfile != null && !appProfile.isEmpty()) {      if (profiles == null) {        String message="Resource profiles is not enabled";        LOG.error(message);
private boolean monitorApplication(ApplicationId appId) throws YarnException, IOException {  while (true) {    try {      Thread.sleep(1000);    } catch (    InterruptedException e) {      LOG.debug("Thread sleep in monitoring loop interrupted");    }    ApplicationReport report=yarnClient.getApplicationReport(appId);    LOG.info("Got application report from ASM for" + ", appId=" + appId.getId() + ", clientToAMToken="+ report.getClientToAMToken()+ ", appDiagnostics="+ report.getDiagnostics()+ ", appMasterHost="+ report.getHost()+ ", appQueue="+ report.getQueue()+ ", appMasterRpcPort="+ report.getRpcPort()+ ", appStartTime="+ report.getStartTime()+ ", yarnAppState="+ report.getYarnApplicationState().toString()+ ", distributedFinalState="+ report.getFinalApplicationStatus().toString()+ ", appTrackingUrl="+ report.getTrackingUrl()+ ", appUser="+ report.getUser());    YarnApplicationState state=report.getYarnApplicationState();    FinalApplicationStatus dsStatus=report.getFinalApplicationStatus();    if (YarnApplicationState.FINISHED == state) {      if (FinalApplicationStatus.SUCCEEDED == dsStatus) {        LOG.info("Application has completed successfully. Breaking monitoring loop");        return true;      } else {
      Thread.sleep(1000);    } catch (    InterruptedException e) {      LOG.debug("Thread sleep in monitoring loop interrupted");    }    ApplicationReport report=yarnClient.getApplicationReport(appId);    LOG.info("Got application report from ASM for" + ", appId=" + appId.getId() + ", clientToAMToken="+ report.getClientToAMToken()+ ", appDiagnostics="+ report.getDiagnostics()+ ", appMasterHost="+ report.getHost()+ ", appQueue="+ report.getQueue()+ ", appMasterRpcPort="+ report.getRpcPort()+ ", appStartTime="+ report.getStartTime()+ ", yarnAppState="+ report.getYarnApplicationState().toString()+ ", distributedFinalState="+ report.getFinalApplicationStatus().toString()+ ", appTrackingUrl="+ report.getTrackingUrl()+ ", appUser="+ report.getUser());    YarnApplicationState state=report.getYarnApplicationState();    FinalApplicationStatus dsStatus=report.getFinalApplicationStatus();    if (YarnApplicationState.FINISHED == state) {      if (FinalApplicationStatus.SUCCEEDED == dsStatus) {        LOG.info("Application has completed successfully. Breaking monitoring loop");        return true;      } else {        LOG.info("Application did finished unsuccessfully." + " YarnState=" + state.toString() + ", DSFinalStatus="+ dsStatus.toString()+ ". Breaking monitoring loop");        return false;      }    } else     if (YarnApplicationState.KILLED == state || YarnApplicationState.FAILED == state) {
private void uploadFile(FileSystem fs,String fileSrcPath,String fileDstPath,String appId) throws IOException {  String relativePath=ApplicationMaster.getRelativePath(appName,appId,fileDstPath);  Path dst=new Path(fs.getHomeDirectory(),relativePath);
private void prepareTimelineDomain(){  TimelineClient timelineClient=null;  if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)) {    timelineClient=TimelineClient.createTimelineClient();    timelineClient.init(conf);    timelineClient.start();  } else {    LOG.warn("Cannot put the domain " + domainId + " because the timeline service is not enabled");    return;  }  try {    TimelineDomain domain=new TimelineDomain();    domain.setId(domainId);    domain.setReaders(viewACLs != null && viewACLs.length() > 0 ? viewACLs : " ");    domain.setWriters(modifyACLs != null && modifyACLs.length() > 0 ? modifyACLs : " ");    timelineClient.putDomain(domain);
  if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)) {    timelineClient=TimelineClient.createTimelineClient();    timelineClient.init(conf);    timelineClient.start();  } else {    LOG.warn("Cannot put the domain " + domainId + " because the timeline service is not enabled");    return;  }  try {    TimelineDomain domain=new TimelineDomain();    domain.setId(domainId);    domain.setReaders(viewACLs != null && viewACLs.length() > 0 ? viewACLs : " ");    domain.setWriters(modifyACLs != null && modifyACLs.length() > 0 ? modifyACLs : " ");    timelineClient.putDomain(domain);    LOG.info("Put the timeline domain: " + TimelineUtils.dumpTimelineRecordtoJSON(domain));  } catch (  Exception e) {
public static Map<String,PlacementSpec> parse(String specs) throws IllegalArgumentException {
@Override public void run() throws YarnException, IOException, InterruptedException {  super.run();  if (appAttemptID.getAttemptId() == 2) {    if (numAllocatedContainers.get() != 1 || numRequestedContainers.get() != numTotalContainers) {
@Test(timeout=90000) public void testDSShellWithNodeLabelExpression() throws Exception {  initializeNodeLabels();  NMContainerMonitor mon=new NMContainerMonitor();  Thread t=new Thread(mon);  t.start();  String[] args={"--jar",TestDistributedShell.APPMASTER_JAR,"--num_containers","4","--shell_command","sleep","--shell_args","15","--master_memory","512","--master_vcores","2","--container_memory","128","--container_vcores","1","--node_label_expression","x"};  LOG.info("Initializing DS Client");  final Client client=new Client(new Configuration(distShellTest.yarnCluster.getConfig()));  boolean initSuccess=client.init(args);  Assert.assertTrue(initSuccess);  LOG.info("Running DS Client");  boolean result=client.run();
@Test(timeout=90000) public void testDistributedShellWithPlacementConstraint() throws Exception {  NMContainerMonitor mon=new NMContainerMonitor();  Thread t=new Thread(mon);  t.start();  String[] args={"--jar",distShellTest.APPMASTER_JAR,"1","--shell_command",distShellTest.getSleepCommand(15),"--placement_spec","zk(1),NOTIN,NODE,zk:spark(1),NOTIN,NODE,zk"};  LOG.info("Initializing DS Client");  final Client client=new Client(new Configuration(distShellTest.yarnCluster.getConfig()));  boolean initSuccess=client.init(args);  Assert.assertTrue(initSuccess);  LOG.info("Running DS Client");  boolean result=client.run();
private void checkTimelineV2(boolean haveDomain,ApplicationId appId,boolean defaultFlow,ApplicationReport appReport) throws Exception {  LOG.info("Started checkTimelineV2 ");  String tmpRoot=timelineV2StorageDir + File.separator + "entities"+ File.separator;  File tmpRootFolder=new File(tmpRoot);  try {    Assert.assertTrue(tmpRootFolder.isDirectory());    String basePath=tmpRoot + YarnConfiguration.DEFAULT_RM_CLUSTER_ID + File.separator+ UserGroupInformation.getCurrentUser().getShortUserName()+ (defaultFlow ? File.separator + appReport.getName() + File.separator+ TimelineUtils.DEFAULT_FLOW_VERSION+ File.separator+ appReport.getStartTime()+ File.separator : File.separator + "test_flow_name" + File.separator+ "test_flow_version"+ File.separator+ "12345678"+ File.separator)+ appId.toString();
private File verifyEntityTypeFileExists(String basePath,String entityType,String entityfileName){  String outputDirPathForEntity=basePath + File.separator + entityType+ File.separator;
@Test public void testDSRestartWithPreviousRunningContainers() throws Exception {  String[] args={"--jar",APPMASTER_JAR,"--num_containers","1","--shell_command",getSleepCommand(8),"--master_memory","512","--container_memory","128","--keep_containers_across_application_attempts"};  LOG.info("Initializing DS Client");  Client client=new Client(TestDSFailedAppMaster.class.getName(),new Configuration(yarnCluster.getConfig()));  client.init(args);  LOG.info("Running DS Client");  boolean result=client.run();
@Test public void testDSAttemptFailuresValidityIntervalSucess() throws Exception {  String[] args={"--jar",APPMASTER_JAR,"--num_containers","1","--shell_command",getSleepCommand(8),"--master_memory","512","--container_memory","128","--attempt_failures_validity_interval","2500"};  LOG.info("Initializing DS Client");  Configuration conf=yarnCluster.getConfig();  conf.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS,2);  Client client=new Client(TestDSSleepingAppMaster.class.getName(),new Configuration(conf));  client.init(args);  LOG.info("Running DS Client");  boolean result=client.run();
@Test public void testDSAttemptFailuresValidityIntervalFailed() throws Exception {  String[] args={"--jar",APPMASTER_JAR,"--num_containers","1","--shell_command",getSleepCommand(8),"--master_memory","512","--container_memory","128","--attempt_failures_validity_interval","15000"};  LOG.info("Initializing DS Client");  Configuration conf=yarnCluster.getConfig();  conf.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS,2);  Client client=new Client(TestDSSleepingAppMaster.class.getName(),new Configuration(conf));  client.init(args);  LOG.info("Running DS Client");  boolean result=client.run();
public void testDSShellWithCommands() throws Exception {  String[] args={"--jar",APPMASTER_JAR,"--num_containers","2","--shell_command","\"echo output_ignored;echo output_expected\"","--master_memory","512","--master_vcores","2","--container_memory","128","--container_vcores","1"};  LOG.info("Initializing DS Client");  final Client client=new Client(new Configuration(yarnCluster.getConfig()));  boolean initSuccess=client.init(args);  Assert.assertTrue(initSuccess);  LOG.info("Running DS Client");  boolean result=client.run();
@Test public void testDSShellWithMultipleArgs() throws Exception {  String[] args={"--jar",APPMASTER_JAR,"--num_containers","4","--shell_command","echo","--shell_args","HADOOP YARN MAPREDUCE HDFS","--master_memory","512","--master_vcores","2","--container_memory","128","--container_vcores","1"};  LOG.info("Initializing DS Client");  final Client client=new Client(new Configuration(yarnCluster.getConfig()));  boolean initSuccess=client.init(args);  Assert.assertTrue(initSuccess);  LOG.info("Running DS Client");  boolean result=client.run();
  if (customShellScript.exists()) {    customShellScript.delete();  }  if (!customShellScript.createNewFile()) {    Assert.fail("Can not create custom shell script file.");  }  PrintWriter fileWriter=new PrintWriter(customShellScript);  fileWriter.write("echo testDSShellWithShellScript");  fileWriter.close();  System.out.println(customShellScript.getAbsolutePath());  String[] args={"--jar",APPMASTER_JAR,"--num_containers","1","--shell_script",customShellScript.getAbsolutePath(),"--master_memory","512","--master_vcores","2","--container_memory","128","--container_vcores","1"};  LOG.info("Initializing DS Client");  final Client client=new Client(new Configuration(yarnCluster.getConfig()));  boolean initSuccess=client.init(args);  Assert.assertTrue(initSuccess);  LOG.info("Running DS Client");  boolean result=client.run();
@Test public void testContainerLaunchFailureHandling() throws Exception {  String[] args={"--jar",APPMASTER_JAR,"--num_containers","2","--shell_command",Shell.WINDOWS ? "dir" : "ls","--master_memory","512","--container_memory","128"};  LOG.info("Initializing DS Client");  Client client=new Client(ContainerLaunchFailAppMaster.class.getName(),new Configuration(yarnCluster.getConfig()));  boolean initSuccess=client.init(args);  Assert.assertTrue(initSuccess);  LOG.info("Running DS Client");  boolean result=client.run();
    appContext.setApplicationName(appName);    Priority pri=Records.newRecord(Priority.class);    pri.setPriority(amPriority);    appContext.setPriority(pri);    appContext.setQueue(amQueue);    ContainerLaunchContext amContainer=Records.newRecord(ContainerLaunchContext.class);    appContext.setAMContainerSpec(amContainer);    appContext.setUnmanagedAM(true);    LOG.info("Setting unmanaged AM");    LOG.info("Submitting application to ASM");    rmClient.submitApplication(appContext);    ApplicationReport appReport=monitorApplication(appId,EnumSet.of(YarnApplicationState.ACCEPTED,YarnApplicationState.KILLED,YarnApplicationState.FAILED,YarnApplicationState.FINISHED));    if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {      ApplicationAttemptReport attemptReport=monitorCurrentAppAttempt(appId,YarnApplicationAttemptState.LAUNCHED);      ApplicationAttemptId attemptId=attemptReport.getApplicationAttemptId();
    ContainerLaunchContext amContainer=Records.newRecord(ContainerLaunchContext.class);    appContext.setAMContainerSpec(amContainer);    appContext.setUnmanagedAM(true);    LOG.info("Setting unmanaged AM");    LOG.info("Submitting application to ASM");    rmClient.submitApplication(appContext);    ApplicationReport appReport=monitorApplication(appId,EnumSet.of(YarnApplicationState.ACCEPTED,YarnApplicationState.KILLED,YarnApplicationState.FAILED,YarnApplicationState.FINISHED));    if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {      ApplicationAttemptReport attemptReport=monitorCurrentAppAttempt(appId,YarnApplicationAttemptState.LAUNCHED);      ApplicationAttemptId attemptId=attemptReport.getApplicationAttemptId();      LOG.info("Launching AM with application attempt id " + attemptId);      launchAM(attemptId);      appReport=monitorApplication(appId,EnumSet.of(YarnApplicationState.KILLED,YarnApplicationState.FAILED,YarnApplicationState.FINISHED));    }    YarnApplicationState appState=appReport.getYarnApplicationState();    FinalApplicationStatus appStatus=appReport.getFinalApplicationStatus();
    ApplicationReport appReport=monitorApplication(appId,EnumSet.of(YarnApplicationState.ACCEPTED,YarnApplicationState.KILLED,YarnApplicationState.FAILED,YarnApplicationState.FINISHED));    if (appReport.getYarnApplicationState() == YarnApplicationState.ACCEPTED) {      ApplicationAttemptReport attemptReport=monitorCurrentAppAttempt(appId,YarnApplicationAttemptState.LAUNCHED);      ApplicationAttemptId attemptId=attemptReport.getApplicationAttemptId();      LOG.info("Launching AM with application attempt id " + attemptId);      launchAM(attemptId);      appReport=monitorApplication(appId,EnumSet.of(YarnApplicationState.KILLED,YarnApplicationState.FAILED,YarnApplicationState.FINISHED));    }    YarnApplicationState appState=appReport.getYarnApplicationState();    FinalApplicationStatus appStatus=appReport.getFinalApplicationStatus();    LOG.info("App ended with state: " + appReport.getYarnApplicationState() + " and status: "+ appStatus);    boolean success;    if (YarnApplicationState.FINISHED == appState && FinalApplicationStatus.SUCCEEDED == appStatus) {      LOG.info("Application has completed successfully.");      success=true;    } else {
  while (true) {    if (attemptId == null) {      attemptId=rmClient.getApplicationReport(appId).getCurrentApplicationAttemptId();    }    ApplicationAttemptReport attemptReport=null;    if (attemptId != null) {      attemptReport=rmClient.getApplicationAttemptReport(attemptId);      if (attemptState.equals(attemptReport.getYarnApplicationAttemptState())) {        return attemptReport;      }    }    LOG.info("Current attempt state of " + appId + " is "+ (attemptReport == null ? " N/A " : attemptReport.getYarnApplicationAttemptState())+ ", waiting for current attempt to reach "+ attemptState);    try {      Thread.sleep(1000);    } catch (    InterruptedException e) {      LOG.warn("Interrupted while waiting for current attempt of " + appId + " to reach "+ attemptState);    }    if (System.currentTimeMillis() - startTime > AM_STATE_WAIT_TIMEOUT_MS) {      String errmsg="Timeout for waiting current attempt of " + appId + " to reach "+ attemptState;
  long foundAMCompletedTime=0;  StringBuilder expectedFinalState=new StringBuilder();  boolean first=true;  for (  YarnApplicationState state : finalState) {    if (first) {      first=false;      expectedFinalState.append(state.name());    } else {      expectedFinalState.append("," + state.name());    }  }  while (true) {    try {      Thread.sleep(1000);    } catch (    InterruptedException e) {      LOG.debug("Thread sleep in monitoring loop interrupted");    }    ApplicationReport report=rmClient.getApplicationReport(appId);
@BeforeClass public static void setup() throws InterruptedException, IOException {  LOG.info("Starting up YARN cluster");  conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB,128);  if (yarnCluster == null) {    yarnCluster=new MiniYARNCluster(TestUnmanagedAMLauncher.class.getSimpleName(),1,1,1);    yarnCluster.init(conf);    yarnCluster.start();    Configuration yarnClusterConfig=yarnCluster.getConfig();
@BeforeClass public static void setup() throws InterruptedException, IOException {  LOG.info("Starting up YARN cluster");  conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB,128);  if (yarnCluster == null) {    yarnCluster=new MiniYARNCluster(TestUnmanagedAMLauncher.class.getSimpleName(),1,1,1);    yarnCluster.init(conf);    yarnCluster.start();    Configuration yarnClusterConfig=yarnCluster.getConfig();    LOG.info("MiniYARN ResourceManager published address: " + yarnClusterConfig.get(YarnConfiguration.RM_ADDRESS));
@BeforeClass public static void setup() throws InterruptedException, IOException {  LOG.info("Starting up YARN cluster");  conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB,128);  if (yarnCluster == null) {    yarnCluster=new MiniYARNCluster(TestUnmanagedAMLauncher.class.getSimpleName(),1,1,1);    yarnCluster.init(conf);    yarnCluster.start();    Configuration yarnClusterConfig=yarnCluster.getConfig();    LOG.info("MiniYARN ResourceManager published address: " + yarnClusterConfig.get(YarnConfiguration.RM_ADDRESS));    LOG.info("MiniYARN ResourceManager published web address: " + yarnClusterConfig.get(YarnConfiguration.RM_WEBAPP_ADDRESS));    String webapp=yarnClusterConfig.get(YarnConfiguration.RM_WEBAPP_ADDRESS);    assertTrue("Web app address still unbound to a host at " + webapp,!webapp.startsWith("0.0.0.0"));
    String webapp=yarnClusterConfig.get(YarnConfiguration.RM_WEBAPP_ADDRESS);    assertTrue("Web app address still unbound to a host at " + webapp,!webapp.startsWith("0.0.0.0"));    LOG.info("Yarn webapp is at " + webapp);    URL url=Thread.currentThread().getContextClassLoader().getResource("yarn-site.xml");    if (url == null) {      throw new RuntimeException("Could not find 'yarn-site.xml' dummy file in classpath");    }    ByteArrayOutputStream bytesOut=new ByteArrayOutputStream();    yarnClusterConfig.writeXml(bytesOut);    bytesOut.close();    OutputStream os=new FileOutputStream(new File(url.getPath()));    os.write(bytesOut.toByteArray());    os.close();  }  try {    Thread.sleep(2000);  } catch (  InterruptedException e) {
  if (javaHome == null) {    LOG.error("JAVA_HOME not defined. Test not running.");    return;  }  String[] args={"--classpath",classpath,"--queue","default","--cmd",javaHome + "/bin/java -Xmx512m " + TestUnmanagedAMLauncher.class.getCanonicalName()+ " success"};  LOG.info("Initializing Launcher");  UnmanagedAMLauncher launcher=new UnmanagedAMLauncher(new Configuration(yarnCluster.getConfig())){    public void launchAM(    ApplicationAttemptId attemptId) throws IOException, YarnException {      YarnApplicationAttemptState attemptState=rmClient.getApplicationAttemptReport(attemptId).getYarnApplicationAttemptState();      Assert.assertTrue(attemptState.equals(YarnApplicationAttemptState.LAUNCHED));      super.launchAM(attemptId);    }  };  boolean initSuccess=launcher.init(args);  Assert.assertTrue(initSuccess);  LOG.info("Running Launcher");  boolean result=launcher.run();
  }  if (HAUtil.isHAEnabled(conf)) {    boolean useKerberos=UserGroupInformation.isSecurityEnabled();    List<String> rmServers=getRMHAWebAddresses(conf);    StringBuilder diagnosticsMsg=new StringBuilder();    for (    String host : rmServers) {      try {        Client client=Client.create();        client.setFollowRedirects(false);        StringBuilder sb=new StringBuilder();        sb.append(scheme).append(host).append(path);        if (!useKerberos) {          try {            String username=UserGroupInformation.getCurrentUser().getShortUserName();            sb.append("?user.name=").append(username);          } catch (          IOException e) {
      try {        Client client=Client.create();        client.setFollowRedirects(false);        StringBuilder sb=new StringBuilder();        sb.append(scheme).append(host).append(path);        if (!useKerberos) {          try {            String username=UserGroupInformation.getCurrentUser().getShortUserName();            sb.append("?user.name=").append(username);          } catch (          IOException e) {            LOG.debug("Fail to resolve username: {}",e);          }        }        Builder builder=client.resource(sb.toString()).type(MediaType.APPLICATION_JSON);        if (useKerberos) {          String[] server=host.split(":");          String challenge=YarnClientUtils.generateToken(server[0]);
        if (!useKerberos) {          try {            String username=UserGroupInformation.getCurrentUser().getShortUserName();            sb.append("?user.name=").append(username);          } catch (          IOException e) {            LOG.debug("Fail to resolve username: {}",e);          }        }        Builder builder=client.resource(sb.toString()).type(MediaType.APPLICATION_JSON);        if (useKerberos) {          String[] server=host.split(":");          String challenge=YarnClientUtils.generateToken(server[0]);          builder.header(HttpHeaders.AUTHORIZATION,"Negotiate " + challenge);          LOG.debug("Authorization: Negotiate {}",challenge);        }        ClientResponse test=builder.get(ClientResponse.class);        if (test.getStatus() == 200) {          return scheme + host;
          try {            String username=UserGroupInformation.getCurrentUser().getShortUserName();            sb.append("?user.name=").append(username);          } catch (          IOException e) {            LOG.debug("Fail to resolve username: {}",e);          }        }        Builder builder=client.resource(sb.toString()).type(MediaType.APPLICATION_JSON);        if (useKerberos) {          String[] server=host.split(":");          String challenge=YarnClientUtils.generateToken(server[0]);          builder.header(HttpHeaders.AUTHORIZATION,"Negotiate " + challenge);          LOG.debug("Authorization: Negotiate {}",challenge);        }        ClientResponse test=builder.get(ClientResponse.class);        if (test.getStatus() == 200) {          return scheme + host;        }      } catch (      Exception e) {
  String output;  if (response.getStatus() == 401) {    LOG.error("Authentication required");    return EXIT_EXCEPTION_THROWN;  }  if (response.getStatus() == 503) {    LOG.error("YARN Service is unavailable or disabled.");    return EXIT_EXCEPTION_THROWN;  }  try {    ServiceStatus ss=response.getEntity(ServiceStatus.class);    output=ss.getDiagnostics();  } catch (  Throwable t) {    output=response.getEntity(String.class);  }  if (output == null) {    output=response.getEntity(String.class);  }  if (response.getStatus() <= 299) {
    return EXIT_EXCEPTION_THROWN;  }  if (response.getStatus() == 503) {    LOG.error("YARN Service is unavailable or disabled.");    return EXIT_EXCEPTION_THROWN;  }  try {    ServiceStatus ss=response.getEntity(ServiceStatus.class);    output=ss.getDiagnostics();  } catch (  Throwable t) {    output=response.getEntity(String.class);  }  if (output == null) {    output=response.getEntity(String.class);  }  if (response.getStatus() <= 299) {    LOG.info(output);    return EXIT_SUCCESS;  } else {
    if (examplesDirStr == null) {      String yarnHome=System.getenv(ApplicationConstants.Environment.HADOOP_YARN_HOME.key());      examplesDirs=new String[]{yarnHome + "/share/hadoop/yarn/yarn-service-examples",yarnHome + "/yarn-service-examples"};    } else {      examplesDirs=StringUtils.split(examplesDirStr,":");    }    for (    String dir : examplesDirs) {      file=new File(MessageFormat.format("{0}/{1}/{2}.json",dir,fileName,fileName));      if (file.exists()) {        break;      }      file=new File(MessageFormat.format("{0}/{1}.json",dir,fileName));      if (file.exists()) {        break;      }    }  }  if (!file.exists()) {    throw new YarnException("File or example could not be found: " + fileName);  }  Path filePath=new Path(file.getAbsolutePath());
  int result=EXIT_SUCCESS;  try {    Service service=new Service();    service.setName(appName);    service.setState(ServiceState.FLEX);    for (    Map.Entry<String,String> entry : componentCounts.entrySet()) {      Component component=new Component();      component.setName(entry.getKey());      Long numberOfContainers=Long.parseLong(entry.getValue());      component.setNumberOfContainers(numberOfContainers);      service.addComponent(component);    }    String buffer=jsonSerDeser.toJson(service);    ClientResponse response=getApiClient(getServicePath(appName)).put(ClientResponse.class,buffer);    result=processResponse(response);  } catch (  Exception e) {
  } catch (  IllegalArgumentException e) {    appName=appIdOrName;    ServiceApiUtil.validateNameFormat(appName,getConfig());  }  try {    ClientResponse response=getApiClient(getServicePath(appName)).get(ClientResponse.class);    if (response.getStatus() == 404) {      StringBuilder sb=new StringBuilder();      sb.append(" Service ").append(appName).append(" not found");      return sb.toString();    }    if (response.getStatus() != 200) {      StringBuilder sb=new StringBuilder();      sb.append(appName).append(" Failed : HTTP error code : ").append(response.getStatus());      return sb.toString();    }    output=response.getEntity(String.class);  } catch (  Exception e) {
  try {    Service service=new Service();    service.setName(appName);    for (    String instance : componentInstances) {      String componentName=ServiceApiUtil.parseComponentName(instance);      Component component=service.getComponent(componentName);      if (component == null) {        component=new Component();        component.setName(componentName);        service.addComponent(component);      }      component.addDecommissionedInstance(instance);    }    String buffer=jsonSerDeser.toJson(service);    ClientResponse response=getApiClient(getServicePath(appName)).put(ClientResponse.class,buffer);    result=processResponse(response);  } catch (  Exception e) {
@Override protected void serviceInit(Configuration conf) throws Exception {  String dirPath=conf.get(YarnServiceConf.YARN_SERVICES_SYSTEM_SERVICE_DIRECTORY);  if (dirPath != null) {    systemServiceDir=new Path(dirPath);
@Override protected void serviceInit(Configuration conf) throws Exception {  String dirPath=conf.get(YarnServiceConf.YARN_SERVICES_SYSTEM_SERVICE_DIRECTORY);  if (dirPath != null) {    systemServiceDir=new Path(dirPath);    LOG.info("System Service Directory is configured to {}",systemServiceDir);    fs=systemServiceDir.getFileSystem(conf);    this.loginUGI=UserGroupInformation.isSecurityEnabled() ? UserGroupInformation.getLoginUser() : UserGroupInformation.getCurrentUser();
    if (services.isEmpty()) {      continue;    }    ServiceClient serviceClient=null;    try {      UserGroupInformation userUgi=getProxyUser(user);      serviceClient=createServiceClient(userUgi);      for (      Service service : services) {        LOG.info("POST: createService = {} user = {}",service,userUgi);        try {          launchServices(userUgi,serviceClient,service);        } catch (        IOException|UndeclaredThrowableException e) {          if (e.getCause() != null) {            LOG.warn(e.getCause().getMessage());          } else {            String message="Failed to create service " + service.getName() + " : ";
      UserGroupInformation userUgi=getProxyUser(user);      serviceClient=createServiceClient(userUgi);      for (      Service service : services) {        LOG.info("POST: createService = {} user = {}",service,userUgi);        try {          launchServices(userUgi,serviceClient,service);        } catch (        IOException|UndeclaredThrowableException e) {          if (e.getCause() != null) {            LOG.warn(e.getCause().getMessage());          } else {            String message="Failed to create service " + service.getName() + " : ";            LOG.error(message,e);          }        }      }    } catch (    InterruptedException e) {      LOG.warn("System service launcher thread interrupted",e);      break;
  if (service.getState() == ServiceState.STOPPED) {    userUgi.doAs(new PrivilegedExceptionAction<Void>(){      @Override public Void run() throws IOException, YarnException {        serviceClient.actionBuild(service);        return null;      }    });    LOG.info("Service {} version {} saved.",service.getName(),service.getVersion());  } else {    ApplicationId applicationId=userUgi.doAs(new PrivilegedExceptionAction<ApplicationId>(){      @Override public ApplicationId run() throws IOException, YarnException {        boolean tryStart=true;        try {          serviceClient.actionBuild(service);        } catch (        Exception e) {          if (e instanceof SliderException && ((SliderException)e).getExitCode() == SliderExitCodes.EXIT_INSTANCE_EXISTS) {
        serviceClient.actionBuild(service);        return null;      }    });    LOG.info("Service {} version {} saved.",service.getName(),service.getVersion());  } else {    ApplicationId applicationId=userUgi.doAs(new PrivilegedExceptionAction<ApplicationId>(){      @Override public ApplicationId run() throws IOException, YarnException {        boolean tryStart=true;        try {          serviceClient.actionBuild(service);        } catch (        Exception e) {          if (e instanceof SliderException && ((SliderException)e).getExitCode() == SliderExitCodes.EXIT_INSTANCE_EXISTS) {            LOG.info("Service {} already exists, will attempt to start " + "service",service.getName());          } else {            tryStart=false;
 else {    ApplicationId applicationId=userUgi.doAs(new PrivilegedExceptionAction<ApplicationId>(){      @Override public ApplicationId run() throws IOException, YarnException {        boolean tryStart=true;        try {          serviceClient.actionBuild(service);        } catch (        Exception e) {          if (e instanceof SliderException && ((SliderException)e).getExitCode() == SliderExitCodes.EXIT_INSTANCE_EXISTS) {            LOG.info("Service {} already exists, will attempt to start " + "service",service.getName());          } else {            tryStart=false;            LOG.info("Got exception saving {}, will not attempt to " + "start service",service.getName(),e);          }        }        if (tryStart) {          return serviceClient.actionStartAndGetId(service.getName());        } else {
    return;  }  try {    LOG.info("Scan for launch type on {}",systemServiceDir);    RemoteIterator<FileStatus> iterLaunchType=list(systemServiceDir);    while (iterLaunchType.hasNext()) {      FileStatus launchType=iterLaunchType.next();      if (!launchType.isDirectory()) {        LOG.debug("Scanner skips for unknown file {}",launchType.getPath());        continue;      }      if (launchType.getPath().getName().equals(SYNC)) {        scanForUserServiceDefinition(launchType.getPath(),syncUserServices);      } else       if (launchType.getPath().getName().equals(ASYNC)) {        scanForUserServiceDefinition(launchType.getPath(),asyncUserServices);      } else {        badDirSkipCounter++;
private void scanForUserServiceDefinition(Path userDirPath,Map<String,Set<Service>> userServices) throws IOException {
private void scanForUserServiceDefinition(Path userDirPath,Map<String,Set<Service>> userServices) throws IOException {  LOG.info("Scan for users on {}",userDirPath);  RemoteIterator<FileStatus> iterUsers=list(userDirPath);  while (iterUsers.hasNext()) {    FileStatus userDir=iterUsers.next();    if (!userDir.isDirectory()) {      LOG.info("Service definition {} doesn't belong to any user. Ignoring.. ",userDir.getPath().getName());      continue;    }    String userName=userDir.getPath().getName();    LOG.info("Scanning service definitions for user {}.",userName);    RemoteIterator<FileStatus> iterServices=list(userDir.getPath());    while (iterServices.hasNext()) {      FileStatus serviceCache=iterServices.next();      String filename=serviceCache.getPath().getName();      if (!serviceCache.isFile()) {
  while (iterUsers.hasNext()) {    FileStatus userDir=iterUsers.next();    if (!userDir.isDirectory()) {      LOG.info("Service definition {} doesn't belong to any user. Ignoring.. ",userDir.getPath().getName());      continue;    }    String userName=userDir.getPath().getName();    LOG.info("Scanning service definitions for user {}.",userName);    RemoteIterator<FileStatus> iterServices=list(userDir.getPath());    while (iterServices.hasNext()) {      FileStatus serviceCache=iterServices.next();      String filename=serviceCache.getPath().getName();      if (!serviceCache.isFile()) {        LOG.info("Scanner skips for unknown dir {}",filename);        continue;      }      if (!filename.endsWith(YARN_FILE_SUFFIX)) {
        continue;      }      if (!filename.endsWith(YARN_FILE_SUFFIX)) {        LOG.info("Scanner skips for unknown file extension, filename = {}",filename);        badFileNameExtensionSkipCounter++;        continue;      }      Service service=getServiceDefinition(serviceCache.getPath());      if (service != null) {        Set<Service> services=userServices.get(userName);        if (services == null) {          services=new HashSet<>();          userServices.put(userName,services);        }        if (!services.add(service)) {          int count=ignoredUserServices.containsKey(userName) ? ignoredUserServices.get(userName) : 0;          ignoredUserServices.put(userName,count + 1);          LOG.warn("Ignoring service {} for the user {} as it is already present," + " filename = {}",service.getName(),userName,filename);
private Service getServiceDefinition(Path filePath){  Service service=null;  try {
@GET @Path(VERSION) @Consumes({MediaType.APPLICATION_JSON}) @Produces({MediaType.APPLICATION_JSON + ";charset=utf-8"}) public Response getVersion(){  String version=VersionInfo.getBuildVersion();
@POST @Path(SERVICE_ROOT_PATH) @Consumes({MediaType.APPLICATION_JSON}) @Produces({MediaType.APPLICATION_JSON + ";charset=utf-8"}) public Response createService(@Context HttpServletRequest request,Service service){  ServiceStatus serviceStatus=new ServiceStatus();  try {    UserGroupInformation ugi=getProxyUser(request);
          ServiceClient sc=getServiceClient();          try {            sc.init(YARN_CONFIG);            sc.start();            ApplicationId applicationId=sc.actionCreate(service);            return applicationId;          }  finally {            sc.close();          }        }      });      serviceStatus.setDiagnostics("Application ID: " + applicationId);    }    serviceStatus.setState(ACCEPTED);    serviceStatus.setUri(CONTEXT_ROOT + SERVICE_ROOT_PATH + "/"+ service.getName());    return formatResponse(Status.ACCEPTED,serviceStatus);  } catch (  AccessControlException e) {    serviceStatus.setDiagnostics(e.getMessage());
            ApplicationId applicationId=sc.actionCreate(service);            return applicationId;          }  finally {            sc.close();          }        }      });      serviceStatus.setDiagnostics("Application ID: " + applicationId);    }    serviceStatus.setState(ACCEPTED);    serviceStatus.setUri(CONTEXT_ROOT + SERVICE_ROOT_PATH + "/"+ service.getName());    return formatResponse(Status.ACCEPTED,serviceStatus);  } catch (  AccessControlException e) {    serviceStatus.setDiagnostics(e.getMessage());    return formatResponse(Status.FORBIDDEN,e.getCause().getMessage());  }catch (  IllegalArgumentException e) {    return formatResponse(Status.BAD_REQUEST,e.getMessage());  }catch (  IOException|InterruptedException e) {
    }    UserGroupInformation ugi=getProxyUser(request);    LOG.info("GET: getService for appName = {} user = {}",appName,ugi);    Service app=getServiceFromClient(ugi,appName);    return Response.ok(app).build();  } catch (  AccessControlException e) {    return formatResponse(Status.FORBIDDEN,e.getMessage());  }catch (  IllegalArgumentException e) {    serviceStatus.setDiagnostics(e.getMessage());    serviceStatus.setCode(ERROR_CODE_APP_NAME_INVALID);    return Response.status(Status.NOT_FOUND).entity(serviceStatus).build();  }catch (  FileNotFoundException e) {    serviceStatus.setDiagnostics("Service " + appName + " not found");    serviceStatus.setCode(ERROR_CODE_APP_NAME_INVALID);    return Response.status(Status.NOT_FOUND).entity(serviceStatus).build();  }catch (  IOException|InterruptedException e) {
    Service app=getServiceFromClient(ugi,appName);    return Response.ok(app).build();  } catch (  AccessControlException e) {    return formatResponse(Status.FORBIDDEN,e.getMessage());  }catch (  IllegalArgumentException e) {    serviceStatus.setDiagnostics(e.getMessage());    serviceStatus.setCode(ERROR_CODE_APP_NAME_INVALID);    return Response.status(Status.NOT_FOUND).entity(serviceStatus).build();  }catch (  FileNotFoundException e) {    serviceStatus.setDiagnostics("Service " + appName + " not found");    serviceStatus.setCode(ERROR_CODE_APP_NAME_INVALID);    return Response.status(Status.NOT_FOUND).entity(serviceStatus).build();  }catch (  IOException|InterruptedException e) {    LOG.error("Get service failed: {}",e);    return formatResponse(Status.INTERNAL_SERVER_ERROR,e.getMessage());
    if (appName == null) {      throw new IllegalArgumentException("Service name can not be null.");    }    UserGroupInformation ugi=getProxyUser(request);    LOG.info("DELETE: deleteService for appName = {} user = {}",appName,ugi);    return stopService(appName,true,ugi);  } catch (  AccessControlException e) {    return formatResponse(Status.FORBIDDEN,e.getMessage());  }catch (  IllegalArgumentException e) {    return formatResponse(Status.BAD_REQUEST,e.getMessage());  }catch (  UndeclaredThrowableException e) {    LOG.error("Fail to stop service: {}",e);    return formatResponse(Status.BAD_REQUEST,e.getCause().getMessage());  }catch (  YarnException|FileNotFoundException e) {    return formatResponse(Status.NOT_FOUND,e.getMessage());  }catch (  Exception e) {
private Response stopService(String appName,boolean destroy,final UserGroupInformation ugi) throws Exception {  int result=ugi.doAs(new PrivilegedExceptionAction<Integer>(){    @Override public Integer run() throws Exception {      int result=0;      ServiceClient sc=getServiceClient();      try {        sc.init(YARN_CONFIG);        sc.start();        Exception stopException=null;        try {          result=sc.actionStop(appName,destroy);          if (result == EXIT_SUCCESS) {
      ServiceClient sc=getServiceClient();      try {        sc.init(YARN_CONFIG);        sc.start();        Exception stopException=null;        try {          result=sc.actionStop(appName,destroy);          if (result == EXIT_SUCCESS) {            LOG.info("Successfully stopped service {}",appName);          }        } catch (        Exception e) {          LOG.info("Got exception stopping service",e);          stopException=e;        }        if (destroy) {          result=sc.actionDestroy(appName);          if (result == EXIT_SUCCESS) {
@PUT @Path(SERVICE_PATH) @Consumes({MediaType.APPLICATION_JSON}) @Produces({MediaType.APPLICATION_JSON + ";charset=utf-8"}) public Response updateService(@Context HttpServletRequest request,@PathParam(SERVICE_NAME) String appName,Service updateServiceData){  try {    UserGroupInformation ugi=getProxyUser(request);
    if (updateServiceData.getState() != null && updateServiceData.getState() == ServiceState.STARTED) {      return startService(appName,ugi);    }    if (updateServiceData.getState() != null && (updateServiceData.getState() == ServiceState.UPGRADING || updateServiceData.getState() == ServiceState.UPGRADING_AUTO_FINALIZE) || updateServiceData.getState() == ServiceState.EXPRESS_UPGRADING) {      return upgradeService(updateServiceData,ugi);    }    if (updateServiceData.getState() != null && updateServiceData.getState() == CANCEL_UPGRADING) {      return cancelUpgradeService(appName,ugi);    }    if (updateServiceData.getLifetime() != null && updateServiceData.getLifetime() > 0) {      return updateLifetime(appName,updateServiceData,ugi);    }    for (    Component c : updateServiceData.getComponents()) {      if (c.getDecommissionedInstances().size() > 0) {        return decommissionInstances(updateServiceData,ugi);      }    }  } catch (  UndeclaredThrowableException e) {    return formatResponse(Status.BAD_REQUEST,e.getCause().getMessage());  }catch (  AccessControlException e) {    return formatResponse(Status.FORBIDDEN,e.getMessage());
    if (updateServiceData.getState() != null && (updateServiceData.getState() == ServiceState.UPGRADING || updateServiceData.getState() == ServiceState.UPGRADING_AUTO_FINALIZE) || updateServiceData.getState() == ServiceState.EXPRESS_UPGRADING) {      return upgradeService(updateServiceData,ugi);    }    if (updateServiceData.getState() != null && updateServiceData.getState() == CANCEL_UPGRADING) {      return cancelUpgradeService(appName,ugi);    }    if (updateServiceData.getLifetime() != null && updateServiceData.getLifetime() > 0) {      return updateLifetime(appName,updateServiceData,ugi);    }    for (    Component c : updateServiceData.getComponents()) {      if (c.getDecommissionedInstances().size() > 0) {        return decommissionInstances(updateServiceData,ugi);      }    }  } catch (  UndeclaredThrowableException e) {    return formatResponse(Status.BAD_REQUEST,e.getCause().getMessage());  }catch (  AccessControlException e) {    return formatResponse(Status.FORBIDDEN,e.getMessage());  }catch (  FileNotFoundException e) {    String message="Application is not found app: " + appName;
      return cancelUpgradeService(appName,ugi);    }    if (updateServiceData.getLifetime() != null && updateServiceData.getLifetime() > 0) {      return updateLifetime(appName,updateServiceData,ugi);    }    for (    Component c : updateServiceData.getComponents()) {      if (c.getDecommissionedInstances().size() > 0) {        return decommissionInstances(updateServiceData,ugi);      }    }  } catch (  UndeclaredThrowableException e) {    return formatResponse(Status.BAD_REQUEST,e.getCause().getMessage());  }catch (  AccessControlException e) {    return formatResponse(Status.FORBIDDEN,e.getMessage());  }catch (  FileNotFoundException e) {    String message="Application is not found app: " + appName;    LOG.error(message,e);    return formatResponse(Status.NOT_FOUND,e.getMessage());  }catch (  YarnException e) {
@PUT @Path(COMP_INSTANCE_LONG_PATH) @Consumes({MediaType.APPLICATION_JSON}) @Produces({RestApiConstants.MEDIA_TYPE_JSON_UTF8,MediaType.TEXT_PLAIN}) public Response updateComponentInstance(@Context HttpServletRequest request,@PathParam(SERVICE_NAME) String serviceName,@PathParam(COMPONENT_NAME) String componentName,@PathParam(COMP_INSTANCE_NAME) String compInstanceName,Container reqContainer){  try {    UserGroupInformation ugi=getProxyUser(request);
@GET @Path(COMP_INSTANCES_PATH) @Produces({RestApiConstants.MEDIA_TYPE_JSON_UTF8}) public Response getComponentInstances(@Context HttpServletRequest request,@PathParam(SERVICE_NAME) String serviceName,@QueryParam(PARAM_COMP_NAME) List<String> componentNames,@QueryParam(PARAM_VERSION) String version,@QueryParam(PARAM_CONTAINER_STATE) List<String> containerStates){  try {    UserGroupInformation ugi=getProxyUser(request);
    componentCountStrings.put(c.getName(),c.getNumberOfContainers().toString());  }  Integer result=ugi.doAs(new PrivilegedExceptionAction<Integer>(){    @Override public Integer run() throws YarnException, IOException {      int result=0;      ServiceClient sc=new ServiceClient();      try {        sc.init(YARN_CONFIG);        sc.start();        result=sc.actionFlex(appName,componentCountStrings);        return Integer.valueOf(result);      }  finally {        sc.close();      }    }  });  if (result == EXIT_SUCCESS) {    String message="Service " + appName + " is successfully flexed.";
private Response upgradeService(Service service,final UserGroupInformation ugi) throws IOException, InterruptedException {  ServiceStatus status=new ServiceStatus();  ugi.doAs((PrivilegedExceptionAction<Void>)() -> {    ServiceClient sc=getServiceClient();    try {      sc.init(YARN_CONFIG);      sc.start();      if (service.getState().equals(ServiceState.EXPRESS_UPGRADING)) {        sc.actionUpgradeExpress(service);      } else {        sc.initiateUpgrade(service);      }    }  finally {      sc.close();    }    return null;  });
    instances.addAll(c.getDecommissionedInstances());  }  Integer result=ugi.doAs(new PrivilegedExceptionAction<Integer>(){    @Override public Integer run() throws YarnException, IOException {      int result=0;      ServiceClient sc=new ServiceClient();      try {        sc.init(YARN_CONFIG);        sc.start();        result=sc.actionDecommissionInstances(appName,instances);        return Integer.valueOf(result);      }  finally {        sc.close();      }    }  });  if (result == EXIT_SUCCESS) {    String message="Service " + appName + " has successfully "+ "decommissioned instances.";
@Override protected void serviceStart() throws Exception {  bindAddress=getConfig().getSocketAddr(API_SERVER_ADDRESS,DEFAULT_API_SERVER_ADDRESS,DEFAULT_API_SERVER_PORT);
private void startWebApp() throws IOException {  URI uri=URI.create("http://" + NetUtils.getHostPortString(bindAddress));  apiServer=new HttpServer2.Builder().setName("api-server").setConf(getConfig()).setSecurityEnabled(UserGroupInformation.isSecurityEnabled()).setUsernameConfKey(RM_WEBAPP_SPNEGO_USER_NAME_KEY).setKeytabConfKey(RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY).addEndpoint(uri).build();  String apiPackages=ApiServer.class.getPackage().getName() + SEP + GenericExceptionHandler.class.getPackage().getName()+ SEP+ YarnJacksonJaxbJsonProvider.class.getPackage().getName();  apiServer.addJerseyResourcePackage(apiPackages,"/*");  try {    logger.info("Service starting up. Logging start...");    apiServer.start();
private void startWebApp() throws IOException {  URI uri=URI.create("http://" + NetUtils.getHostPortString(bindAddress));  apiServer=new HttpServer2.Builder().setName("api-server").setConf(getConfig()).setSecurityEnabled(UserGroupInformation.isSecurityEnabled()).setUsernameConfKey(RM_WEBAPP_SPNEGO_USER_NAME_KEY).setKeytabConfKey(RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY).addEndpoint(uri).build();  String apiPackages=ApiServer.class.getPackage().getName() + SEP + GenericExceptionHandler.class.getPackage().getName()+ SEP+ YarnJacksonJaxbJsonProvider.class.getPackage().getName();  apiServer.addJerseyResourcePackage(apiPackages,"/*");  try {    logger.info("Service starting up. Logging start...");    apiServer.start();    logger.info("Server status = {}",apiServer.toString());    for (    Configuration conf : apiServer.getWebAppContext().getConfigurations()) {
private void startWebApp() throws IOException {  URI uri=URI.create("http://" + NetUtils.getHostPortString(bindAddress));  apiServer=new HttpServer2.Builder().setName("api-server").setConf(getConfig()).setSecurityEnabled(UserGroupInformation.isSecurityEnabled()).setUsernameConfKey(RM_WEBAPP_SPNEGO_USER_NAME_KEY).setKeytabConfKey(RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY).addEndpoint(uri).build();  String apiPackages=ApiServer.class.getPackage().getName() + SEP + GenericExceptionHandler.class.getPackage().getName()+ SEP+ YarnJacksonJaxbJsonProvider.class.getPackage().getName();  apiServer.addJerseyResourcePackage(apiPackages,"/*");  try {    logger.info("Service starting up. Logging start...");    apiServer.start();    logger.info("Server status = {}",apiServer.toString());    for (    Configuration conf : apiServer.getWebAppContext().getConfigurations()) {      logger.info("Configurations = {}",conf);    }    logger.info("Context Path = {}",Collections.singletonList(apiServer.getWebAppContext().getContextPath()));    logger.info("ResourceBase = {}",Collections.singletonList(apiServer.getWebAppContext().getResourceBase()));    logger.info("War = {}",Collections.singletonList(apiServer.getWebAppContext().getWar()));  } catch (  Exception ex) {
@Override public FlexComponentsResponseProto flexComponents(FlexComponentsRequestProto request) throws IOException {  if (!request.getComponentsList().isEmpty()) {    for (    ComponentCountProto component : request.getComponentsList()) {      ComponentEvent event=new ComponentEvent(component.getName(),FLEX).setDesired(component.getNumberOfContainers());      context.scheduler.getDispatcher().getEventHandler().handle(event);
@Override public UpgradeServiceResponseProto upgrade(UpgradeServiceRequestProto request) throws IOException {  try {
@Override public CompInstancesUpgradeResponseProto upgrade(CompInstancesUpgradeRequestProto request) throws IOException, YarnException {  if (!request.getContainerIdsList().isEmpty()) {    for (    String containerId : request.getContainerIdsList()) {      ComponentInstanceEvent event=new ComponentInstanceEvent(ContainerId.fromString(containerId),ComponentInstanceEventType.UPGRADE);
@Override public DecommissionCompInstancesResponseProto decommissionCompInstances(DecommissionCompInstancesRequestProto request) throws IOException, YarnException {  if (!request.getCompInstancesList().isEmpty()) {    for (    String instance : request.getCompInstancesList()) {      String componentName=ServiceApiUtil.parseComponentName(instance);      ComponentEvent event=new ComponentEvent(componentName,DECOMMISSION_INSTANCE).setInstanceName(instance);      context.scheduler.getDispatcher().getEventHandler().handle(event);
public synchronized void resetContainerFailures(){  failureCountPerNode.clear();  context.scheduler.getAmRMClient().updateBlacklist(null,new ArrayList<>(blackListedNodes));
private void upgradeNextCompIfAny(boolean cancelUpgrade){  if (!componentsToUpgrade.isEmpty()) {    org.apache.hadoop.yarn.service.api.records.Component component=componentsToUpgrade.get(0);    serviceSpec.getComponent(component.getName()).getContainers().forEach(container -> {      ComponentInstanceEvent upgradeEvent=new ComponentInstanceEvent(ContainerId.fromString(container.getId()),!cancelUpgrade ? ComponentInstanceEventType.UPGRADE : ComponentInstanceEventType.CANCEL_UPGRADE);
private void dispatchNeedUpgradeEvents(boolean cancelUpgrade){  if (componentsToUpgrade != null) {    componentsToUpgrade.forEach(component -> {      ComponentEvent needUpgradeEvent=new ComponentEvent(component.getName(),!cancelUpgrade ? ComponentEventType.UPGRADE : ComponentEventType.CANCEL_UPGRADE).setTargetSpec(component).setUpgradeVersion(upgradeVersion);
    try {      Service targetSpec=ServiceApiUtil.loadServiceUpgrade(fs,getName(),upgradeVersion);      targetSpec.setId(serviceSpec.getId());      targetSpec.setState(ServiceState.STABLE);      Map<String,Component> allComps=scheduler.getAllComponents();      targetSpec.getComponents().forEach(compSpec -> {        Component comp=allComps.get(compSpec.getName());        compSpec.setState(comp.getComponentSpec().getState());      });      jsonSerDeser.save(fs.getFileSystem(),ServiceApiUtil.getServiceJsonPath(fs,getName()),targetSpec,true);    } catch (    IOException e) {      LOG.error("Upgrade did not complete because unable to re-write the" + " service definition",e);      return false;    }  }  try {    String upgradeVersionToDel=cancelUpgrade ? cancelledVersion : upgradeVersion;
      Map<String,Component> allComps=scheduler.getAllComponents();      targetSpec.getComponents().forEach(compSpec -> {        Component comp=allComps.get(compSpec.getName());        compSpec.setState(comp.getComponentSpec().getState());      });      jsonSerDeser.save(fs.getFileSystem(),ServiceApiUtil.getServiceJsonPath(fs,getName()),targetSpec,true);    } catch (    IOException e) {      LOG.error("Upgrade did not complete because unable to re-write the" + " service definition",e);      return false;    }  }  try {    String upgradeVersionToDel=cancelUpgrade ? cancelledVersion : upgradeVersion;    LOG.info("[SERVICE]: delete upgrade dir version {}",upgradeVersionToDel);    fs.deleteClusterUpgradeDir(getName(),upgradeVersionToDel);    for (    String comp : compsAffectedByUpgrade) {      String compDirVersionToDel=cancelUpgrade ? cancelledVersion : serviceSpec.getVersion();
private List<org.apache.hadoop.yarn.service.api.records.Component> resolveCompsToUpgrade(Service sourceSpec,Service targetSpec){  List<org.apache.hadoop.yarn.service.api.records.Component> compsNeedUpgradeList=componentsFinder.findTargetComponentSpecs(sourceSpec,targetSpec);  if (compsNeedUpgradeList != null) {    compsNeedUpgradeList.removeIf(component -> {      org.apache.hadoop.yarn.service.api.records.Component.RestartPolicyEnum restartPolicy=component.getRestartPolicy();      final ComponentRestartPolicy restartPolicyHandler=Component.getRestartPolicyHandler(restartPolicy);      if (!restartPolicyHandler.allowUpgrades()) {
private void setServiceState(org.apache.hadoop.yarn.service.api.records.ServiceState state){  org.apache.hadoop.yarn.service.api.records.ServiceState curState=serviceSpec.getState();  if (!curState.equals(state)) {    serviceSpec.setState(state);
  if (UserGroupInformation.isSecurityEnabled()) {    credentials=UserGroupInformation.getCurrentUser().getCredentials();    doSecureLogin();  }  SliderFileSystem fs=new SliderFileSystem(conf);  fs.setAppDir(appDir);  context.fs=fs;  loadApplicationJson(context,fs);  if (UserGroupInformation.isSecurityEnabled()) {    if (credentials != null) {      UserGroupInformation.getCurrentUser().addCredentials(credentials);    }    removeHdfsDelegationToken(UserGroupInformation.getLoginUser());  }  for (  Map.Entry<String,String> entry : context.service.getConfiguration().getProperties().entrySet()) {    conf.set(entry.getKey(),entry.getValue());  }  ContainerId amContainerId=getAMContainerId();  ApplicationAttemptId attemptId=amContainerId.getApplicationAttemptId();
@VisibleForTesting protected ByteBuffer recordTokensForContainers() throws IOException {  Credentials copy=new Credentials(UserGroupInformation.getCurrentUser().getCredentials());  Iterator<Token<?>> iter=copy.getAllTokens().iterator();  while (iter.hasNext()) {    Token<?> token=iter.next();
private void doSecureLogin() throws IOException, URISyntaxException {  File keytab=new File(String.format(KEYTAB_LOCATION,getServiceName()));  if (!keytab.exists()) {
private void doSecureLogin() throws IOException, URISyntaxException {  File keytab=new File(String.format(KEYTAB_LOCATION,getServiceName()));  if (!keytab.exists()) {    LOG.info("No keytab localized at " + keytab);    String preInstalledKeytab=context.service == null ? this.serviceKeytab : context.service.getKerberosPrincipal().getKeytab();    if (!StringUtils.isEmpty(preInstalledKeytab)) {      URI uri=new URI(preInstalledKeytab);      if (uri.getScheme().equals("file")) {        keytab=new File(uri);
  File keytab=new File(String.format(KEYTAB_LOCATION,getServiceName()));  if (!keytab.exists()) {    LOG.info("No keytab localized at " + keytab);    String preInstalledKeytab=context.service == null ? this.serviceKeytab : context.service.getKerberosPrincipal().getKeytab();    if (!StringUtils.isEmpty(preInstalledKeytab)) {      URI uri=new URI(preInstalledKeytab);      if (uri.getScheme().equals("file")) {        keytab=new File(uri);        LOG.info("Using pre-installed keytab from localhost: " + preInstalledKeytab);      }    }  }  if (!keytab.exists()) {    LOG.info("No keytab exists: " + keytab);    return;  }  String principal=context.service == null ? this.servicePrincipalName : context.service.getKerberosPrincipal().getPrincipalName();  if (StringUtils.isEmpty((principal))) {    principal=UserGroupInformation.getLoginUser().getShortUserName();
    if (!StringUtils.isEmpty(preInstalledKeytab)) {      URI uri=new URI(preInstalledKeytab);      if (uri.getScheme().equals("file")) {        keytab=new File(uri);        LOG.info("Using pre-installed keytab from localhost: " + preInstalledKeytab);      }    }  }  if (!keytab.exists()) {    LOG.info("No keytab exists: " + keytab);    return;  }  String principal=context.service == null ? this.servicePrincipalName : context.service.getKerberosPrincipal().getPrincipalName();  if (StringUtils.isEmpty((principal))) {    principal=UserGroupInformation.getLoginUser().getShortUserName();    LOG.info("No principal name specified.  Will use AM " + "login identity {} to attempt keytab-based login",principal);  }  LOG.info("User before logged in is: " + UserGroupInformation.getCurrentUser());  String principalName=SecurityUtil.getServerPrincipal(principal,ServiceUtils.getLocalHostName(getConfig()));  UserGroupInformation.loginUserFromKeytab(principalName,keytab.getAbsolutePath());
private void printSystemEnv(){  for (  Map.Entry<String,String> envs : System.getenv().entrySet()) {
    opts.addOption(YARNFILE_OPTION,true,"HDFS path to JSON service " + "specification");    opts.getOption(YARNFILE_OPTION).setRequired(true);    opts.addOption(SERVICE_NAME_OPTION,true,"Service name");    opts.getOption(SERVICE_NAME_OPTION).setRequired(true);    opts.addOption(KEYTAB_OPTION,true,"Service AM keytab");    opts.addOption(PRINCIPAL_NAME_OPTION,true,"Service AM keytab principal");    GenericOptionsParser parser=new GenericOptionsParser(conf,opts,args);    CommandLine cmdLine=parser.getCommandLine();    serviceMaster.serviceDefPath=cmdLine.getOptionValue(YARNFILE_OPTION);    serviceMaster.serviceName=cmdLine.getOptionValue(SERVICE_NAME_OPTION);    serviceMaster.serviceKeytab=cmdLine.getOptionValue(KEYTAB_OPTION);    serviceMaster.servicePrincipalName=cmdLine.getOptionValue(PRINCIPAL_NAME_OPTION);    serviceMaster.init(conf);    serviceMaster.start();  } catch (  Throwable t) {
public void buildInstance(ServiceContext context,Configuration configuration) throws YarnException, IOException {  app=context.service;  executorService=Executors.newScheduledThreadPool(10);  RegistryOperations registryClient=null;  if (UserGroupInformation.isSecurityEnabled() && !StringUtils.isEmpty(context.principal) && !StringUtils.isEmpty(context.keytab)) {    Configuration conf=getConfig();    String username=new HadoopKerberosName(context.principal.trim()).getServiceName();
@Override public void serviceStop() throws Exception {  LOG.info("Stopping service scheduler");  if (executorService != null) {    executorService.shutdownNow();  }  DefaultMetricsSystem.shutdown();  if (gracefulStop) {    if (YarnConfiguration.timelineServiceV2Enabled(getConfig())) {      final Map<ContainerId,ComponentInstance> liveInst=getLiveInstances();      for (      Map.Entry<ContainerId,ComponentInstance> instance : liveInst.entrySet()) {        if (!ComponentInstance.isFinalState(instance.getValue().getContainerSpec().getState())) {          LOG.info("{} Component instance state changed from {} to {}",instance.getValue().getCompInstanceName(),instance.getValue().getContainerSpec().getState(),ContainerState.STOPPED);          serviceTimelinePublisher.componentInstanceFinished(instance.getKey(),KILLED_AFTER_APP_COMPLETION,ContainerState.STOPPED,getDiagnostics().toString());        }      }      LOG.info("Service state changed to {}",finalApplicationStatus);      serviceTimelinePublisher.serviceAttemptUnregistered(context,finalApplicationStatus,diagnostics.toString());    }    amRMClient.unregisterApplicationMaster(finalApplicationStatus,diagnostics.toString(),"");
private void recoverComponents(RegisterApplicationMasterResponse response){  List<Container> containersFromPrevAttempt=response.getContainersFromPreviousAttempts();
private void recoverComponents(RegisterApplicationMasterResponse response){  List<Container> containersFromPrevAttempt=response.getContainersFromPreviousAttempts();  LOG.info("Received {} containers from previous attempt.",containersFromPrevAttempt.size());  Map<String,ServiceRecord> existingRecords=new HashMap<>();  List<String> existingComps=null;  try {    existingComps=yarnRegistryOperations.listComponents();
  Map<String,ServiceRecord> existingRecords=new HashMap<>();  List<String> existingComps=null;  try {    existingComps=yarnRegistryOperations.listComponents();    LOG.info("Found {} containers from ZK registry: {}",existingComps.size(),existingComps);  } catch (  Exception e) {    LOG.info("Could not read component paths: {}",e.getMessage());  }  if (existingComps != null) {    for (    String existingComp : existingComps) {      try {        ServiceRecord record=yarnRegistryOperations.getComponent(existingComp);        existingRecords.put(existingComp,record);      } catch (      Exception e) {        LOG.warn("Could not resolve record for component {}: {}",existingComp,e);      }    }  }  for (  Container container : containersFromPrevAttempt) {
    LOG.info("Could not read component paths: {}",e.getMessage());  }  if (existingComps != null) {    for (    String existingComp : existingComps) {      try {        ServiceRecord record=yarnRegistryOperations.getComponent(existingComp);        existingRecords.put(existingComp,record);      } catch (      Exception e) {        LOG.warn("Could not resolve record for component {}: {}",existingComp,e);      }    }  }  for (  Container container : containersFromPrevAttempt) {    LOG.info("Handling {} from previous attempt",container.getId());    ServiceRecord record=existingRecords.remove(RegistryPathUtils.encodeYarnID(container.getId().toString()));    if (record != null) {      Component comp=componentsById.get(container.getAllocationRequestId());      ComponentEvent event=new ComponentEvent(comp.getName(),CONTAINER_RECOVERED).setContainer(container).setInstance(comp.getComponentInstance(record.description));      comp.handle(event);
      ComponentEvent event=new ComponentEvent(comp.getName(),CONTAINER_RECOVERED).setContainer(container).setInstance(comp.getComponentInstance(record.description));      comp.handle(event);    } else {      LOG.info("Record not found in registry for container {} from previous" + " attempt, releasing",container.getId());      amRMClient.releaseAssignedContainer(container.getId());    }  }  ApplicationId appId=ApplicationId.fromString(app.getId());  existingRecords.forEach((encodedContainerId,record) -> {    String componentName=record.get(YarnRegistryAttributes.YARN_COMPONENT);    if (componentName != null) {      Component component=componentsByName.get(componentName);      if (component != null) {        ComponentInstance compInstance=component.getComponentInstance(record.description);        ContainerId containerId=ContainerId.fromString(record.get(YarnRegistryAttributes.YARN_ID));        if (containerId.getApplicationAttemptId().getApplicationId().equals(appId)) {          unRecoveredInstances.put(containerId,compInstance);
private void registerServiceInstance(ApplicationAttemptId attemptId,Service service) throws IOException {
private void registerServiceInstance(ApplicationAttemptId attemptId,Service service) throws IOException {  LOG.info("Registering " + attemptId + ", "+ service.getName()+ " into registry");  ServiceRecord serviceRecord=new ServiceRecord();  serviceRecord.set(YarnRegistryAttributes.YARN_ID,attemptId.getApplicationId().toString());  serviceRecord.set(YarnRegistryAttributes.YARN_PERSISTENCE,PersistencePolicies.APPLICATION);  serviceRecord.description="YarnServiceMaster";  executorService.submit(new Runnable(){    @Override public void run(){      try {        yarnRegistryOperations.registerSelf(serviceRecord,false);
private void registerServiceInstance(ApplicationAttemptId attemptId,Service service) throws IOException {  LOG.info("Registering " + attemptId + ", "+ service.getName()+ " into registry");  ServiceRecord serviceRecord=new ServiceRecord();  serviceRecord.set(YarnRegistryAttributes.YARN_ID,attemptId.getApplicationId().toString());  serviceRecord.set(YarnRegistryAttributes.YARN_PERSISTENCE,PersistencePolicies.APPLICATION);  serviceRecord.description="YarnServiceMaster";  executorService.submit(new Runnable(){    @Override public void run(){      try {        yarnRegistryOperations.registerSelf(serviceRecord,false);        LOG.info("Registered service under {}; absolute path {}",yarnRegistryOperations.getSelfRegistrationPath(),yarnRegistryOperations.getAbsoluteSelfRegistrationPath());        boolean isFirstAttempt=1 == attemptId.getAttemptId();        if (isFirstAttempt) {          yarnRegistryOperations.deleteChildren(yarnRegistryOperations.getSelfRegistrationPath(),true);        }      } catch (      IOException e) {
private boolean terminateServiceIfDominantComponentFinished(Component component){  boolean shouldTerminate=false;  boolean componentIsDominant=component.getComponentSpec().getConfiguration().getPropertyBool(CONTAINER_STATE_REPORT_AS_SERVICE_STATE,false);  if (componentIsDominant) {    ComponentRestartPolicy restartPolicy=component.getRestartPolicyHandler();    if (restartPolicy.shouldTerminate(component)) {      shouldTerminate=true;      boolean isSucceeded=restartPolicy.hasCompletedSuccessfully(component);      org.apache.hadoop.yarn.service.api.records.ComponentState state=isSucceeded ? org.apache.hadoop.yarn.service.api.records.ComponentState.SUCCEEDED : org.apache.hadoop.yarn.service.api.records.ComponentState.FAILED;
private boolean terminateServiceIfDominantComponentFinished(Component component){  boolean shouldTerminate=false;  boolean componentIsDominant=component.getComponentSpec().getConfiguration().getPropertyBool(CONTAINER_STATE_REPORT_AS_SERVICE_STATE,false);  if (componentIsDominant) {    ComponentRestartPolicy restartPolicy=component.getRestartPolicyHandler();    if (restartPolicy.shouldTerminate(component)) {      shouldTerminate=true;      boolean isSucceeded=restartPolicy.hasCompletedSuccessfully(component);      org.apache.hadoop.yarn.service.api.records.ComponentState state=isSucceeded ? org.apache.hadoop.yarn.service.api.records.ComponentState.SUCCEEDED : org.apache.hadoop.yarn.service.api.records.ComponentState.FAILED;      LOG.info("{} Component state changed from {} to {}",component.getName(),component.getComponentSpec().getState(),state);      component.getComponentSpec().setState(state);
private boolean terminateServiceIfAllComponentsFinished(){  boolean shouldTerminate=true;  Set<String> succeededComponents=new HashSet<>();  Set<String> failedComponents=new HashSet<>();  for (  Component comp : getAllComponents().values()) {    ComponentRestartPolicy restartPolicy=comp.getRestartPolicyHandler();    if (restartPolicy.shouldTerminate(comp)) {      if (restartPolicy.hasCompletedSuccessfully(comp)) {
        comp.getComponentSpec().setState(org.apache.hadoop.yarn.service.api.records.ComponentState.SUCCEEDED);      } else {        LOG.info("{} Component state changed from {} to {}",comp.getName(),comp.getComponentSpec().getState(),org.apache.hadoop.yarn.service.api.records.ComponentState.FAILED);        comp.getComponentSpec().setState(org.apache.hadoop.yarn.service.api.records.ComponentState.FAILED);      }      if (isTimelineServiceEnabled()) {        serviceTimelinePublisher.componentFinished(comp.getComponentSpec(),comp.getComponentSpec().getState(),systemClock.getTime());      }    } else {      shouldTerminate=false;      break;    }    long nFailed=comp.getNumFailedInstances();    if (nFailed > 0) {      failedComponents.add(comp.getName());    } else {      succeededComponents.add(comp.getName());    }  }  if (shouldTerminate) {
    if (examplesDirStr == null) {      String yarnHome=System.getenv(ApplicationConstants.Environment.HADOOP_YARN_HOME.key());      examplesDirs=new String[]{yarnHome + "/share/hadoop/yarn/yarn-service-examples",yarnHome + "/yarn-service-examples"};    } else {      examplesDirs=StringUtils.split(examplesDirStr,":");    }    for (    String dir : examplesDirs) {      file=new File(MessageFormat.format("{0}/{1}/{2}.json",dir,fileName,fileName));      if (file.exists()) {        break;      }      file=new File(MessageFormat.format("{0}/{1}.json",dir,fileName));      if (file.exists()) {        break;      }    }  }  if (!file.exists()) {    throw new YarnException("File or example could not be found: " + fileName);  }  Path filePath=new Path(file.getAbsolutePath());
    throw new YarnException(ErrorStrings.SERVICE_UPGRADE_DISABLED);  }  Service persistedService=ServiceApiUtil.loadService(fs,service.getName());  if (!StringUtils.isEmpty(persistedService.getId())) {    cachedAppInfo.put(persistedService.getName(),new AppInfo(ApplicationId.fromString(persistedService.getId()),persistedService.getKerberosPrincipal().getPrincipalName()));  }  if (persistedService.getVersion().equals(service.getVersion())) {    String message=service.getName() + " is already at version " + service.getVersion()+ ". There is nothing to upgrade.";    LOG.error(message);    throw new YarnException(message);  }  boolean foundNotNeverComp=false;  for (  Component comp : persistedService.getComponents()) {    if (!comp.getRestartPolicy().equals(Component.RestartPolicyEnum.NEVER)) {      foundNotNeverComp=true;      break;    }  }  if (!foundNotNeverComp) {    String message="All the components of the service " + service.getName() + " have "+ Component.RestartPolicyEnum.NEVER+ " restart policy, "+ "so it cannot be upgraded.";
  }  if (persistedService.getVersion().equals(service.getVersion())) {    String message=service.getName() + " is already at version " + service.getVersion()+ ". There is nothing to upgrade.";    LOG.error(message);    throw new YarnException(message);  }  boolean foundNotNeverComp=false;  for (  Component comp : persistedService.getComponents()) {    if (!comp.getRestartPolicy().equals(Component.RestartPolicyEnum.NEVER)) {      foundNotNeverComp=true;      break;    }  }  if (!foundNotNeverComp) {    String message="All the components of the service " + service.getName() + " have "+ Component.RestartPolicyEnum.NEVER+ " restart policy, "+ "so it cannot be upgraded.";    LOG.error(message);    throw new YarnException(message);  }  Service liveService=getStatus(service.getName());  if (!liveService.getState().equals(ServiceState.STABLE)) {
  if (StringUtils.isEmpty(persistedService.getId())) {    throw new YarnException(persistedService.getName() + " appId is null, may be not submitted " + "to YARN yet");  }  cachedAppInfo.put(persistedService.getName(),new AppInfo(ApplicationId.fromString(persistedService.getId()),persistedService.getKerberosPrincipal().getPrincipalName()));  for (  String instance : componentInstances) {    String componentName=ServiceApiUtil.parseComponentName(ServiceApiUtil.parseAndValidateComponentInstanceName(instance,appName,getConfig()));    Component component=persistedService.getComponent(componentName);    if (component == null) {      throw new IllegalArgumentException(instance + " does not exist !");    }    if (!component.getDecommissionedInstances().contains(instance)) {      component.addDecommissionedInstance(instance);      component.setNumberOfContainers(Math.max(0,component.getNumberOfContainers() - 1));    }  }  ServiceApiUtil.writeAppDefinition(fs,persistedService);  ApplicationReport appReport=yarnClient.getApplicationReport(ApplicationId.fromString(persistedService.getId()));  if (appReport.getYarnApplicationState() != RUNNING) {    String message=persistedService.getName() + " is at " + appReport.getYarnApplicationState()+ " state, decommission can only be "+ "invoked when service is running";
public int actionUpgrade(Service service,List<Container> compInstances) throws IOException, YarnException {  ApplicationReport appReport=yarnClient.getApplicationReport(getAppId(service.getName()));  if (appReport.getYarnApplicationState() != RUNNING) {    String message=service.getName() + " is at " + appReport.getYarnApplicationState()+ " state, upgrade can only be invoked when service is running.";
  FlexComponentsRequestProto.Builder requestBuilder=FlexComponentsRequestProto.newBuilder();  for (  Component persistedComp : persistedService.getComponents()) {    String name=persistedComp.getName();    if (componentCounts.containsKey(persistedComp.getName())) {      original.put(name,persistedComp.getNumberOfContainers());      persistedComp.setNumberOfContainers(componentCounts.get(name));      countBuilder.setName(persistedComp.getName()).setNumberOfContainers(persistedComp.getNumberOfContainers());      requestBuilder.addComponents(countBuilder.build());    }  }  if (original.size() < componentCounts.size()) {    componentCounts.keySet().removeAll(original.keySet());    throw new YarnException("Components " + componentCounts.keySet() + " do not exist in app definition.");  }  ServiceApiUtil.writeAppDefinition(fs,persistedService);  ApplicationId appId=getAppId(serviceName);  if (appId == null) {    String message="Application ID doesn't exist for " + serviceName;
      persistedComp.setNumberOfContainers(componentCounts.get(name));      countBuilder.setName(persistedComp.getName()).setNumberOfContainers(persistedComp.getNumberOfContainers());      requestBuilder.addComponents(countBuilder.build());    }  }  if (original.size() < componentCounts.size()) {    componentCounts.keySet().removeAll(original.keySet());    throw new YarnException("Components " + componentCounts.keySet() + " do not exist in app definition.");  }  ServiceApiUtil.writeAppDefinition(fs,persistedService);  ApplicationId appId=getAppId(serviceName);  if (appId == null) {    String message="Application ID doesn't exist for " + serviceName;    LOG.error(message);    throw new YarnException(message);  }  ApplicationReport appReport=yarnClient.getApplicationReport(appId);  if (appReport.getYarnApplicationState() != RUNNING) {    String message=serviceName + " is at " + appReport.getYarnApplicationState()+ " state, flex can only be invoked when service is running";
  if (original.size() < componentCounts.size()) {    componentCounts.keySet().removeAll(original.keySet());    throw new YarnException("Components " + componentCounts.keySet() + " do not exist in app definition.");  }  ServiceApiUtil.writeAppDefinition(fs,persistedService);  ApplicationId appId=getAppId(serviceName);  if (appId == null) {    String message="Application ID doesn't exist for " + serviceName;    LOG.error(message);    throw new YarnException(message);  }  ApplicationReport appReport=yarnClient.getApplicationReport(appId);  if (appReport.getYarnApplicationState() != RUNNING) {    String message=serviceName + " is at " + appReport.getYarnApplicationState()+ " state, flex can only be invoked when service is running";    LOG.error(message);    throw new YarnException(message);  }  Service liveService=getStatus(serviceName);
public int actionStop(String serviceName,boolean waitForAppStopped) throws YarnException, IOException {  ServiceApiUtil.validateNameFormat(serviceName,getConfig());  ApplicationId currentAppId=getAppId(serviceName);  if (currentAppId == null) {
public int actionStop(String serviceName,boolean waitForAppStopped) throws YarnException, IOException {  ServiceApiUtil.validateNameFormat(serviceName,getConfig());  ApplicationId currentAppId=getAppId(serviceName);  if (currentAppId == null) {    LOG.info("Application ID doesn't exist for service {}",serviceName);    cleanUpRegistry(serviceName);    return EXIT_COMMAND_ARGUMENT_ERROR;  }  ApplicationReport report=yarnClient.getApplicationReport(currentAppId);  if (terminatedStates.contains(report.getYarnApplicationState())) {    LOG.info("Service {} is already in a terminated state {}",serviceName,report.getYarnApplicationState());    cleanUpRegistry(serviceName);    return EXIT_COMMAND_ARGUMENT_ERROR;  }  if (preRunningStates.contains(report.getYarnApplicationState())) {    String msg=serviceName + " is at " + report.getYarnApplicationState()+ ", forcefully killed by user!";    yarnClient.killApplication(currentAppId,msg);
    return EXIT_COMMAND_ARGUMENT_ERROR;  }  if (preRunningStates.contains(report.getYarnApplicationState())) {    String msg=serviceName + " is at " + report.getYarnApplicationState()+ ", forcefully killed by user!";    yarnClient.killApplication(currentAppId,msg);    LOG.info(msg);    cleanUpRegistry(serviceName);    return EXIT_SUCCESS;  }  if (StringUtils.isEmpty(report.getHost())) {    throw new YarnException(serviceName + " AM hostname is empty");  }  LOG.info("Stopping service {}, with appId = {}",serviceName,currentAppId);  try {    ClientAMProtocol proxy=createAMProxy(serviceName,report);    cachedAppInfo.remove(serviceName);    if (proxy != null) {      StopRequestProto request=StopRequestProto.newBuilder().build();
    String msg=serviceName + " is at " + report.getYarnApplicationState()+ ", forcefully killed by user!";    yarnClient.killApplication(currentAppId,msg);    LOG.info(msg);    cleanUpRegistry(serviceName);    return EXIT_SUCCESS;  }  if (StringUtils.isEmpty(report.getHost())) {    throw new YarnException(serviceName + " AM hostname is empty");  }  LOG.info("Stopping service {}, with appId = {}",serviceName,currentAppId);  try {    ClientAMProtocol proxy=createAMProxy(serviceName,report);    cachedAppInfo.remove(serviceName);    if (proxy != null) {      StopRequestProto request=StopRequestProto.newBuilder().build();      proxy.stop(request);      LOG.info("Service " + serviceName + " is being gracefully stopped...");
    cachedAppInfo.remove(serviceName);    if (proxy != null) {      StopRequestProto request=StopRequestProto.newBuilder().build();      proxy.stop(request);      LOG.info("Service " + serviceName + " is being gracefully stopped...");    } else {      yarnClient.killApplication(currentAppId,serviceName + " is forcefully killed by user!");      LOG.info("Forcefully kill the service: " + serviceName);      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    if (!waitForAppStopped) {      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    long startTime=System.currentTimeMillis();    int pollCount=0;
      proxy.stop(request);      LOG.info("Service " + serviceName + " is being gracefully stopped...");    } else {      yarnClient.killApplication(currentAppId,serviceName + " is forcefully killed by user!");      LOG.info("Forcefully kill the service: " + serviceName);      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    if (!waitForAppStopped) {      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    long startTime=System.currentTimeMillis();    int pollCount=0;    while (true) {      Thread.sleep(2000);      report=yarnClient.getApplicationReport(currentAppId);
      yarnClient.killApplication(currentAppId,serviceName + " is forcefully killed by user!");      LOG.info("Forcefully kill the service: " + serviceName);      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    if (!waitForAppStopped) {      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    long startTime=System.currentTimeMillis();    int pollCount=0;    while (true) {      Thread.sleep(2000);      report=yarnClient.getApplicationReport(currentAppId);      if (terminatedStates.contains(report.getYarnApplicationState())) {        LOG.info("Service " + serviceName + " is stopped.");        break;
      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    if (!waitForAppStopped) {      cleanUpRegistry(serviceName);      return EXIT_SUCCESS;    }    long startTime=System.currentTimeMillis();    int pollCount=0;    while (true) {      Thread.sleep(2000);      report=yarnClient.getApplicationReport(currentAppId);      if (terminatedStates.contains(report.getYarnApplicationState())) {        LOG.info("Service " + serviceName + " is stopped.");        break;      }      if ((System.currentTimeMillis() - startTime) > 10000) {        LOG.info("Stop operation timeout stopping, forcefully kill the app " + serviceName);
@Override public int actionDestroy(String serviceName) throws YarnException, IOException {  ServiceApiUtil.validateNameFormat(serviceName,getConfig());  verifyNoLiveAppInRM(serviceName,"destroy");  Path appDir=fs.buildClusterDirPath(serviceName);  FileSystem fileSystem=fs.getFileSystem();  cachedAppInfo.remove(serviceName);  int ret=EXIT_SUCCESS;  if (fileSystem.exists(appDir)) {    if (fileSystem.delete(appDir,true)) {
@Override public int actionDestroy(String serviceName) throws YarnException, IOException {  ServiceApiUtil.validateNameFormat(serviceName,getConfig());  verifyNoLiveAppInRM(serviceName,"destroy");  Path appDir=fs.buildClusterDirPath(serviceName);  FileSystem fileSystem=fs.getFileSystem();  cachedAppInfo.remove(serviceName);  int ret=EXIT_SUCCESS;  if (fileSystem.exists(appDir)) {    if (fileSystem.delete(appDir,true)) {      LOG.info("Successfully deleted service dir for " + serviceName + ": "+ appDir);    } else {      String message="Failed to delete service + " + serviceName + " at:  "+ appDir;      LOG.info(message);      throw new YarnException(message);    }  } else {
  cachedAppInfo.remove(serviceName);  int ret=EXIT_SUCCESS;  if (fileSystem.exists(appDir)) {    if (fileSystem.delete(appDir,true)) {      LOG.info("Successfully deleted service dir for " + serviceName + ": "+ appDir);    } else {      String message="Failed to delete service + " + serviceName + " at:  "+ appDir;      LOG.info(message);      throw new YarnException(message);    }  } else {    LOG.info("Service '" + serviceName + "' doesn't exist at hdfs path: "+ appDir);    ret=EXIT_NOT_FOUND;  }  Path publicResourceDir=new Path(fs.getBasePath(),serviceName);  if (fileSystem.exists(publicResourceDir)) {    if (fileSystem.delete(publicResourceDir,true)) {
    if (fileSystem.delete(appDir,true)) {      LOG.info("Successfully deleted service dir for " + serviceName + ": "+ appDir);    } else {      String message="Failed to delete service + " + serviceName + " at:  "+ appDir;      LOG.info(message);      throw new YarnException(message);    }  } else {    LOG.info("Service '" + serviceName + "' doesn't exist at hdfs path: "+ appDir);    ret=EXIT_NOT_FOUND;  }  Path publicResourceDir=new Path(fs.getBasePath(),serviceName);  if (fileSystem.exists(publicResourceDir)) {    if (fileSystem.delete(publicResourceDir,true)) {      LOG.info("Successfully deleted public resource dir for " + serviceName + ": "+ publicResourceDir);    } else {      String message="Failed to delete public resource dir for service " + serviceName + " at:  "+ publicResourceDir;
    LOG.info("Service '" + serviceName + "' doesn't exist at hdfs path: "+ appDir);    ret=EXIT_NOT_FOUND;  }  Path publicResourceDir=new Path(fs.getBasePath(),serviceName);  if (fileSystem.exists(publicResourceDir)) {    if (fileSystem.delete(publicResourceDir,true)) {      LOG.info("Successfully deleted public resource dir for " + serviceName + ": "+ publicResourceDir);    } else {      String message="Failed to delete public resource dir for service " + serviceName + " at:  "+ publicResourceDir;      LOG.info(message);      throw new YarnException(message);    }  }  try {    deleteZKNode(serviceName);  } catch (  Exception e) {    throw new IOException("Could not delete zk node for " + serviceName,e);  }  if (!cleanUpRegistry(serviceName)) {
  Path publicResourceDir=new Path(fs.getBasePath(),serviceName);  if (fileSystem.exists(publicResourceDir)) {    if (fileSystem.delete(publicResourceDir,true)) {      LOG.info("Successfully deleted public resource dir for " + serviceName + ": "+ publicResourceDir);    } else {      String message="Failed to delete public resource dir for service " + serviceName + " at:  "+ publicResourceDir;      LOG.info(message);      throw new YarnException(message);    }  }  try {    deleteZKNode(serviceName);  } catch (  Exception e) {    throw new IOException("Could not delete zk node for " + serviceName,e);  }  if (!cleanUpRegistry(serviceName)) {    if (ret == EXIT_SUCCESS) {      ret=EXIT_OTHER_FAILURE;
      LOG.info("Successfully deleted public resource dir for " + serviceName + ": "+ publicResourceDir);    } else {      String message="Failed to delete public resource dir for service " + serviceName + " at:  "+ publicResourceDir;      LOG.info(message);      throw new YarnException(message);    }  }  try {    deleteZKNode(serviceName);  } catch (  Exception e) {    throw new IOException("Could not delete zk node for " + serviceName,e);  }  if (!cleanUpRegistry(serviceName)) {    if (ret == EXIT_SUCCESS) {      ret=EXIT_OTHER_FAILURE;    }  }  if (ret == EXIT_SUCCESS) {    LOG.info("Successfully destroyed service {}",serviceName);    return ret;
private boolean deleteZKNode(String serviceName) throws Exception {  CuratorFramework curatorFramework=getCuratorClient();  String user=RegistryUtils.currentUser();  String zkPath=ServiceRegistryUtils.mkServiceHomePath(user,serviceName);  if (curatorFramework.checkExists().forPath(zkPath) != null) {    curatorFramework.delete().deletingChildrenIfNeeded().forPath(zkPath);
protected Path addJarResource(String serviceName,Map<String,LocalResource> localResources) throws IOException, YarnException {  Path libPath=fs.buildClusterDirPath(serviceName);  ProviderUtils.addProviderJar(localResources,ServiceMaster.class,SERVICE_CORE_JAR,fs,libPath,"lib",false);  Path dependencyLibTarGzip=fs.getDependencyTarGzip();  if (actionDependency(null,false) == EXIT_SUCCESS) {
public ApplicationId actionStartAndGetId(String serviceName) throws YarnException, IOException {  ServiceApiUtil.validateNameFormat(serviceName,getConfig());  Service liveService=getStatus(serviceName);  if (liveService == null || !liveService.getState().equals(ServiceState.UPGRADING)) {    Path appDir=checkAppExistOnHdfs(serviceName);    Service service=ServiceApiUtil.loadService(fs,serviceName);    ServiceApiUtil.validateAndResolveService(service,fs,getConfig());    verifyNoLiveAppInRM(serviceName,"start");    ApplicationId appId=submitApp(service);    cachedAppInfo.put(serviceName,new AppInfo(appId,service.getKerberosPrincipal().getPrincipalName()));    service.setId(appId.toString());    Path appJson=ServiceApiUtil.writeAppDefinition(fs,appDir,service);
public ApplicationId actionStartAndGetId(String serviceName) throws YarnException, IOException {  ServiceApiUtil.validateNameFormat(serviceName,getConfig());  Service liveService=getStatus(serviceName);  if (liveService == null || !liveService.getState().equals(ServiceState.UPGRADING)) {    Path appDir=checkAppExistOnHdfs(serviceName);    Service service=ServiceApiUtil.loadService(fs,serviceName);    ServiceApiUtil.validateAndResolveService(service,fs,getConfig());    verifyNoLiveAppInRM(serviceName,"start");    ApplicationId appId=submitApp(service);    cachedAppInfo.put(serviceName,new AppInfo(appId,service.getKerberosPrincipal().getPrincipalName()));    service.setId(appId.toString());    Path appJson=ServiceApiUtil.writeAppDefinition(fs,appDir,service);    LOG.info("Persisted service " + service.getName() + " at "+ appJson);    return appId;  } else {
  if (!UserGroupInformation.isSecurityEnabled()) {    return;  }  String principalName=service.getKerberosPrincipal().getPrincipalName();  if (StringUtils.isEmpty(principalName)) {    LOG.warn("No Kerberos principal name specified for " + service.getName());    return;  }  if (StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {    LOG.warn("No Kerberos keytab specified for " + service.getName());    return;  }  URI keytabURI;  try {    keytabURI=new URI(service.getKerberosPrincipal().getKeytab());  } catch (  URISyntaxException e) {    throw new YarnException(e);  }  if ("file".equals(keytabURI.getScheme())) {
  if (StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {    LOG.warn("No Kerberos keytab specified for " + service.getName());    return;  }  URI keytabURI;  try {    keytabURI=new URI(service.getKerberosPrincipal().getKeytab());  } catch (  URISyntaxException e) {    throw new YarnException(e);  }  if ("file".equals(keytabURI.getScheme())) {    LOG.info("Using a keytab from localhost: " + keytabURI);  } else {    Path keytabPath=new Path(keytabURI);    if (!fileSystem.getFileSystem().exists(keytabPath)) {      LOG.warn(service.getName() + "'s keytab (principalName = " + principalName+ ") doesn't exist at: "+ keytabPath);      return;
public Service getStatus(String serviceName) throws IOException, YarnException {  ServiceApiUtil.validateNameFormat(serviceName,getConfig());  Service appSpec=new Service();  appSpec.setName(serviceName);  appSpec.setState(ServiceState.STOPPED);  ApplicationId currentAppId=getAppId(serviceName);  if (currentAppId == null) {
  }  appSpec.setId(currentAppId.toString());  ApplicationReport appReport=null;  try {    appReport=yarnClient.getApplicationReport(currentAppId);  } catch (  ApplicationNotFoundException e) {    LOG.info("application ID {} doesn't exist",currentAppId);    return appSpec;  }  if (appReport == null) {    LOG.warn("application ID {} is reported as null",currentAppId);    return appSpec;  }  appSpec.setState(convertState(appReport.getYarnApplicationState()));  ApplicationTimeout lifetime=appReport.getApplicationTimeouts().get(ApplicationTimeoutType.LIFETIME);  if (lifetime != null) {    appSpec.setLifetime(lifetime.getRemainingTime());  }  if (appReport.getYarnApplicationState() != RUNNING) {
public int actionDependency(String destinationFolder,boolean overwrite){  String currentUser=RegistryUtils.currentUser();
    dependencyLibTarGzip=fs.getDependencyTarGzip();  } else {    dependencyLibTarGzip=new Path(destinationFolder,YarnServiceConstants.DEPENDENCY_TAR_GZ_FILE_NAME + YarnServiceConstants.DEPENDENCY_TAR_GZ_FILE_EXT);  }  if (fs.isFile(dependencyLibTarGzip) && !overwrite) {    System.out.println(String.format("Dependency libs are already uploaded to %s.",dependencyLibTarGzip.toUri()));    return EXIT_SUCCESS;  }  String[] libDirs=ServiceUtils.getLibDirs();  if (libDirs.length > 0) {    File tempLibTarGzipFile=null;    try {      if (!checkPermissions(dependencyLibTarGzip)) {        return EXIT_UNAUTHORIZED;      }      tempLibTarGzipFile=File.createTempFile(YarnServiceConstants.DEPENDENCY_TAR_GZ_FILE_NAME + "_",YarnServiceConstants.DEPENDENCY_TAR_GZ_FILE_EXT);      tarGzipFolder(libDirs,tempLibTarGzipFile,createJarFilter());      fs.copyLocalFileToHdfs(tempLibTarGzipFile,dependencyLibTarGzip,new FsPermission(YarnServiceConstants.DEPENDENCY_DIR_PERMISSIONS));
private boolean checkPermissions(Path dependencyLibTarGzip) throws IOException {  AccessControlList yarnAdminAcl=new AccessControlList(getConfig().get(YarnConfiguration.YARN_ADMIN_ACL,YarnConfiguration.DEFAULT_YARN_ADMIN_ACL));  AccessControlList dfsAdminAcl=new AccessControlList(getConfig().get(DFSConfigKeys.DFS_ADMIN," "));  UserGroupInformation ugi=UserGroupInformation.getCurrentUser();  if (!yarnAdminAcl.isUserAllowed(ugi) && !dfsAdminAcl.isUserAllowed(ugi)) {
private void checkAndScheduleHealthThresholdMonitor(){  int healthThresholdPercent=YarnServiceConf.getInt(CONTAINER_HEALTH_THRESHOLD_PERCENT,DEFAULT_CONTAINER_HEALTH_THRESHOLD_PERCENT,componentSpec.getConfiguration(),scheduler.getConfig());  if (healthThresholdPercent == CONTAINER_HEALTH_THRESHOLD_PERCENT_DISABLED) {
private void checkAndScheduleHealthThresholdMonitor(){  int healthThresholdPercent=YarnServiceConf.getInt(CONTAINER_HEALTH_THRESHOLD_PERCENT,DEFAULT_CONTAINER_HEALTH_THRESHOLD_PERCENT,componentSpec.getConfiguration(),scheduler.getConfig());  if (healthThresholdPercent == CONTAINER_HEALTH_THRESHOLD_PERCENT_DISABLED) {    LOG.info("No health threshold monitor enabled for component {}",componentSpec.getName());    return;  }  if (healthThresholdPercent <= 0 || healthThresholdPercent > 100) {    LOG.error("Invalid health threshold percent {}% for component {}. Monitor not " + "enabled.",healthThresholdPercent,componentSpec.getName());    return;  }  long window=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  long initDelay=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  long pollFrequency=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  if (window <= 0) {    LOG.error("Invalid health monitor window {} secs for component {}. Monitor not " + "enabled.",window,componentSpec.getName());    return;  }  if (initDelay < 0) {
    LOG.info("No health threshold monitor enabled for component {}",componentSpec.getName());    return;  }  if (healthThresholdPercent <= 0 || healthThresholdPercent > 100) {    LOG.error("Invalid health threshold percent {}% for component {}. Monitor not " + "enabled.",healthThresholdPercent,componentSpec.getName());    return;  }  long window=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  long initDelay=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  long pollFrequency=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  if (window <= 0) {    LOG.error("Invalid health monitor window {} secs for component {}. Monitor not " + "enabled.",window,componentSpec.getName());    return;  }  if (initDelay < 0) {    LOG.error("Invalid health monitor init delay {} secs for component {}. " + "Monitor not enabled.",initDelay,componentSpec.getName());    return;  }  if (pollFrequency <= 0) {
  if (healthThresholdPercent <= 0 || healthThresholdPercent > 100) {    LOG.error("Invalid health threshold percent {}% for component {}. Monitor not " + "enabled.",healthThresholdPercent,componentSpec.getName());    return;  }  long window=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  long initDelay=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  long pollFrequency=YarnServiceConf.getLong(CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC,DEFAULT_CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC,componentSpec.getConfiguration(),scheduler.getConfig());  if (window <= 0) {    LOG.error("Invalid health monitor window {} secs for component {}. Monitor not " + "enabled.",window,componentSpec.getName());    return;  }  if (initDelay < 0) {    LOG.error("Invalid health monitor init delay {} secs for component {}. " + "Monitor not enabled.",initDelay,componentSpec.getName());    return;  }  if (pollFrequency <= 0) {    LOG.error("Invalid health monitor poll frequency {} secs for component {}. " + "Monitor not enabled.",pollFrequency,componentSpec.getName());    return;
private void assignContainerToCompInstance(Container container){  if (pendingInstances.size() == 0) {
@SuppressWarnings({"unchecked"}) public void requestContainers(long count){
  LOG.info("[COMPONENT {}] Requesting for {} container(s)",componentSpec.getName(),count);  org.apache.hadoop.yarn.service.api.records.Resource componentResource=componentSpec.getResource();  Resource resource=Resource.newInstance(componentResource.calcMemoryMB(),componentResource.getCpus());  if (componentResource.getAdditional() != null) {    for (    Map.Entry<String,ResourceInformation> entry : componentResource.getAdditional().entrySet()) {      String resourceName=entry.getKey();      if (resourceName.equals(org.apache.hadoop.yarn.api.records.ResourceInformation.MEMORY_URI) || resourceName.equals(org.apache.hadoop.yarn.api.records.ResourceInformation.VCORES_URI)) {        LOG.warn("Please set memory/vcore in the main section of resource, " + "ignoring this entry=" + resourceName);        continue;      }      ResourceInformation specInfo=entry.getValue();      org.apache.hadoop.yarn.api.records.ResourceInformation ri=org.apache.hadoop.yarn.api.records.ResourceInformation.newInstance(entry.getKey(),specInfo.getUnit(),specInfo.getValue(),specInfo.getTags(),specInfo.getAttributes());      resource.setResourceInformation(resourceName,ri);    }  }  if (!scheduler.hasAtLeastOnePlacementConstraint()) {    for (int i=0; i < count; i++) {      ContainerRequest request=ContainerRequest.newBuilder().capability(resource).priority(priority).allocationRequestId(allocateId).relaxLocality(true).build();
private void setComponentState(org.apache.hadoop.yarn.service.api.records.ComponentState state){  org.apache.hadoop.yarn.service.api.records.ComponentState curState=componentSpec.getState();  if (!curState.equals(state)) {    componentSpec.setState(state);
@VisibleForTesting static void handleComponentInstanceRelaunch(ComponentInstance compInstance,ComponentInstanceEvent event,boolean failureBeforeLaunch,String containerDiag){  Component comp=compInstance.getComponent();  boolean hasContainerFailed=failureBeforeLaunch || hasContainerFailed(event.getStatus());  ComponentRestartPolicy restartPolicy=comp.getRestartPolicyHandler();  ContainerState containerState=hasContainerFailed ? ContainerState.FAILED : ContainerState.SUCCEEDED;  if (compInstance.getContainerSpec() != null) {    compInstance.getContainerSpec().setState(containerState);  }  if (restartPolicy.shouldRelaunchInstance(compInstance,event.getStatus())) {    comp.requestContainers(1);    comp.reInsertPendingInstance(compInstance);    StringBuilder builder=new StringBuilder();    builder.append(compInstance.getCompInstanceId()).append(": ").append(event.getContainerId()).append(" completed. Reinsert back to pending list and requested ").append("a new container.").append(System.lineSeparator()).append(" exitStatus=").append(failureBeforeLaunch || event.getStatus() == null ? null : event.getStatus().getExitStatus()).append(", diagnostics=").append(failureBeforeLaunch ? FAILED_BEFORE_LAUNCH_DIAG : (event.getStatus() != null ? event.getStatus().getDiagnostics() : UPGRADE_FAILED));    if (event.getStatus() != null && event.getStatus().getExitStatus() != 0) {      LOG.error(builder.toString());    } else {
  boolean hasContainerFailed=failureBeforeLaunch || hasContainerFailed(event.getStatus());  ComponentRestartPolicy restartPolicy=comp.getRestartPolicyHandler();  ContainerState containerState=hasContainerFailed ? ContainerState.FAILED : ContainerState.SUCCEEDED;  if (compInstance.getContainerSpec() != null) {    compInstance.getContainerSpec().setState(containerState);  }  if (restartPolicy.shouldRelaunchInstance(compInstance,event.getStatus())) {    comp.requestContainers(1);    comp.reInsertPendingInstance(compInstance);    StringBuilder builder=new StringBuilder();    builder.append(compInstance.getCompInstanceId()).append(": ").append(event.getContainerId()).append(" completed. Reinsert back to pending list and requested ").append("a new container.").append(System.lineSeparator()).append(" exitStatus=").append(failureBeforeLaunch || event.getStatus() == null ? null : event.getStatus().getExitStatus()).append(", diagnostics=").append(failureBeforeLaunch ? FAILED_BEFORE_LAUNCH_DIAG : (event.getStatus() != null ? event.getStatus().getDiagnostics() : UPGRADE_FAILED));    if (event.getStatus() != null && event.getStatus().getExitStatus() != 0) {      LOG.error(builder.toString());    } else {      LOG.info(builder.toString());    }    if (compInstance.timelineServiceEnabled) {
      LOG.error(builder.toString());    } else {      LOG.info(builder.toString());    }    if (compInstance.timelineServiceEnabled) {      LOG.info("Publishing component instance status {} {} ",event.getContainerId(),containerState);      int exitStatus=failureBeforeLaunch || event.getStatus() == null ? ContainerExitStatus.INVALID : event.getStatus().getExitStatus();      compInstance.serviceTimelinePublisher.componentInstanceFinished(event.getContainerId(),exitStatus,containerState,containerDiag);    }  } else {    if (hasContainerFailed) {      comp.markAsFailed(compInstance);    } else {      comp.markAsSucceeded(compInstance);    }    if (compInstance.timelineServiceEnabled) {      int exitStatus=failureBeforeLaunch || event.getStatus() == null ? ContainerExitStatus.INVALID : event.getStatus().getExitStatus();      compInstance.serviceTimelinePublisher.componentInstanceFinished(event.getContainerId(),exitStatus,containerState,containerDiag);
public void setContainerState(ContainerState state){  this.writeLock.lock();  try {    ContainerState curState=containerSpec.getState();    if (!curState.equals(state)) {      containerSpec.setState(state);
    if (existingIP != null && newIP.equals(existingIP)) {      doRegistryUpdate=false;    }  }  ObjectMapper mapper=new ObjectMapper();  try {    Map<String,List<Map<String,String>>> ports=null;    ports=mapper.readValue(status.getExposedPorts(),new TypeReference<Map<String,List<Map<String,String>>>>(){    });    container.setExposedPorts(ports);  } catch (  IOException e) {    LOG.warn("Unable to process container ports mapping: {}",e);  }  setContainerStatus(status.getContainerId(),status);  if (containerRec != null && timelineServiceEnabled && doRegistryUpdate) {    serviceTimelinePublisher.componentInstanceIPHostUpdated(containerRec);  }  if (doRegistryUpdate) {    cleanupRegistry(status.getContainerId());
public void cleanupRegistryAndCompHdfsDir(ContainerId containerId){  cleanupRegistry(containerId);  try {    if (compInstanceDir != null && fs.exists(compInstanceDir)) {      boolean deleted=fs.delete(compInstanceDir,true);      if (!deleted) {
public ContainerLaunchContext completeContainerLaunch() throws IOException {  String cmdStr=ServiceUtils.join(commands," ",false);
public ContainerLaunchContext completeContainerLaunch() throws IOException {  String cmdStr=ServiceUtils.join(commands," ",false);  log.debug("Completed setting up container command {}",cmdStr);  containerLaunchContext.setCommands(commands);  if (log.isDebugEnabled()) {    log.debug("Environment variables");    for (    Map.Entry<String,String> envPair : envVars.entrySet()) {
private void dumpLocalResources(){  if (log.isDebugEnabled()) {    log.debug("{} resources: ",localResources.size());    for (    Map.Entry<String,LocalResource> entry : localResources.entrySet()) {      String key=entry.getKey();      LocalResource val=entry.getValue();
  if (desiredContainerCount == 0) {    return;  }  long readyContainerCount=component.getNumReadyInstances();  float thresholdFraction=(float)healthThresholdPercent / 100;  float readyContainerFraction=(float)readyContainerCount / desiredContainerCount;  boolean healthChanged=false;  if (Math.abs(readyContainerFraction - prevReadyContainerFraction) > .0000001) {    prevReadyContainerFraction=readyContainerFraction;    healthChanged=true;  }  String readyContainerPercentStr=String.format("%.2f",readyContainerFraction * 100);  if (readyContainerFraction < thresholdFraction) {    long currentTimestamp=System.nanoTime();    if (firstOccurrenceTimestamp == 0) {      firstOccurrenceTimestamp=currentTimestamp;      Date date=new Date();
      LOG.info("[COMPONENT {}] Health has gone below threshold. Starting health " + "threshold timer at ts = {} ({})",component.getName(),date.getTime(),date);    }    long elapsedTime=currentTimestamp - firstOccurrenceTimestamp;    long elapsedTimeSecs=TimeUnit.SECONDS.convert(elapsedTime,TimeUnit.NANOSECONDS);    LOG.warn("[COMPONENT {}] Current health {}% is below health threshold of " + "{}% for {} secs (threshold window = {} secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);    if (elapsedTime > healthThresholdWindowNanos) {      LOG.warn("[COMPONENT {}] Current health {}% has been below health " + "threshold of {}% for {} secs (threshold window = {} secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);      String exitDiag=String.format("Service is being killed because container health for component " + "%s was %s%% (health threshold = %d%%) for %d secs " + "(threshold window = %d secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);      component.getScheduler().getDiagnostics().append(exitDiag);      LOG.warn(exitDiag);      try {        Thread.sleep(5000);      } catch (      InterruptedException e) {        LOG.error("Interrupted on sleep while exiting.",e);      }      ExitUtil.terminate(-1);    }  } else {
    long elapsedTime=currentTimestamp - firstOccurrenceTimestamp;    long elapsedTimeSecs=TimeUnit.SECONDS.convert(elapsedTime,TimeUnit.NANOSECONDS);    LOG.warn("[COMPONENT {}] Current health {}% is below health threshold of " + "{}% for {} secs (threshold window = {} secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);    if (elapsedTime > healthThresholdWindowNanos) {      LOG.warn("[COMPONENT {}] Current health {}% has been below health " + "threshold of {}% for {} secs (threshold window = {} secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);      String exitDiag=String.format("Service is being killed because container health for component " + "%s was %s%% (health threshold = %d%%) for %d secs " + "(threshold window = %d secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);      component.getScheduler().getDiagnostics().append(exitDiag);      LOG.warn(exitDiag);      try {        Thread.sleep(5000);      } catch (      InterruptedException e) {        LOG.error("Interrupted on sleep while exiting.",e);      }      ExitUtil.terminate(-1);    }  } else {    String logMsg="[COMPONENT {}] Health threshold = {}%, Current health " + "= {}% (Current Ready count = {}, Desired count = {})";
    if (elapsedTime > healthThresholdWindowNanos) {      LOG.warn("[COMPONENT {}] Current health {}% has been below health " + "threshold of {}% for {} secs (threshold window = {} secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);      String exitDiag=String.format("Service is being killed because container health for component " + "%s was %s%% (health threshold = %d%%) for %d secs " + "(threshold window = %d secs)",component.getName(),readyContainerPercentStr,healthThresholdPercent,elapsedTimeSecs,healthThresholdWindowSecs);      component.getScheduler().getDiagnostics().append(exitDiag);      LOG.warn(exitDiag);      try {        Thread.sleep(5000);      } catch (      InterruptedException e) {        LOG.error("Interrupted on sleep while exiting.",e);      }      ExitUtil.terminate(-1);    }  } else {    String logMsg="[COMPONENT {}] Health threshold = {}%, Current health " + "= {}% (Current Ready count = {}, Desired count = {})";    if (healthChanged) {      LOG.info(logMsg,component.getName(),healthThresholdPercent,readyContainerPercentStr,readyContainerCount,desiredContainerCount);    } else {
    return status;  }  String ip=instance.getContainerStatus().getIPs().get(0);  HttpURLConnection connection=null;  try {    URL url=new URL(urlString.replace(HOST_TOKEN,ip));    connection=getConnection(url,this.timeout);    int rc=connection.getResponseCode();    if (rc < min || rc > max) {      String error="Probe " + url + " error code: "+ rc;      log.info(error);      status.fail(this,new IOException(error));    } else {      status.succeed(this);    }  } catch (  Throwable e) {    String error="Probe " + urlString + " failed for IP "+ ip+ ": "+ e;
public static synchronized void createConfigFileAndAddLocalResource(AbstractLauncher launcher,SliderFileSystem fs,ContainerLaunchService.ComponentLaunchContext compLaunchContext,Map<String,String> tokensForSubstitution,ComponentInstance instance,ServiceContext context,ProviderService.ResolvedLaunchParams resolvedParams) throws IOException {  Path compInstanceDir=initCompInstanceDir(fs,compLaunchContext,instance);  if (!fs.getFileSystem().exists(compInstanceDir)) {
  } else {    log.info("Component instance public resource dir already exists: " + compPublicResourceDir);  }  log.debug("Tokens substitution for component instance: {}{}{}" + instance.getCompInstanceName(),System.lineSeparator(),tokensForSubstitution);  for (  ConfigFile originalFile : compLaunchContext.getConfiguration().getFiles()) {    if (isStaticFile(originalFile)) {      continue;    }    ConfigFile configFile=originalFile.copy();    String fileName=new Path(configFile.getDestFile()).getName();    for (    Map.Entry<String,String> token : tokensForSubstitution.entrySet()) {      configFile.setDestFile(configFile.getDestFile().replaceAll(Pattern.quote(token.getKey()),token.getValue()));    }    Path remoteFile=null;    LocalResourceVisibility visibility=configFile.getVisibility();    if (visibility != null && visibility.equals(LocalResourceVisibility.PUBLIC)) {      remoteFile=new Path(compPublicResourceDir,fileName);    } else {
private static void addLocalResource(AbstractLauncher launcher,String symlink,LocalResource localResource,Path destFile,ProviderService.ResolvedLaunchParams resolvedParams){  if (destFile.isAbsolute()) {    launcher.addLocalResource(symlink,localResource,destFile.toString());
 catch (  ExecutionException e) {    log.info("Failed to load config file: " + configFile,e);    return;  }  org.apache.hadoop.conf.Configuration confCopy=new org.apache.hadoop.conf.Configuration(false);  for (  Map.Entry<String,String> entry : conf.entrySet()) {    confCopy.set(entry.getKey(),entry.getValue());  }  for (  Map.Entry<String,String> entry : configFile.getProperties().entrySet()) {    confCopy.set(entry.getKey(),entry.getValue());  }  for (  Map.Entry<String,String> entry : confCopy) {    String val=entry.getValue();    if (val != null) {      for (      Map.Entry<String,String> token : tokensForSubstitution.entrySet()) {        val=val.replaceAll(Pattern.quote(token.getKey()),token.getValue());        confCopy.set(entry.getKey(),val);      }    }  }  try (OutputStream output=fs.create(remoteFile)){
public ServiceRecord getComponent(String componentName) throws IOException {  String path=RegistryUtils.componentPath(user,serviceClass,instanceName,componentName);
public void deleteComponent(ComponentInstanceId instanceId,String containerId) throws IOException {  String path=RegistryUtils.componentPath(user,serviceClass,instanceName,containerId);
  if (serviceTimelinePublisher.isStopped()) {    log.warn("ServiceTimelinePublisher has stopped. " + "Not publishing any more metrics to ATS.");    return;  }  boolean isServiceMetrics=false;  boolean isComponentMetrics=false;  String appId=null;  for (  MetricsTag tag : record.tags()) {    if (tag.name().equals("type") && tag.value().equals("service")) {      isServiceMetrics=true;    } else     if (tag.name().equals("type") && tag.value().equals("component")) {      isComponentMetrics=true;      break;    } else     if (tag.name().equals("appId")) {      appId=tag.value();    }  }  if (isServiceMetrics && appId != null) {
  }  boolean isServiceMetrics=false;  boolean isComponentMetrics=false;  String appId=null;  for (  MetricsTag tag : record.tags()) {    if (tag.name().equals("type") && tag.value().equals("service")) {      isServiceMetrics=true;    } else     if (tag.name().equals("type") && tag.value().equals("component")) {      isComponentMetrics=true;      break;    } else     if (tag.name().equals("appId")) {      appId=tag.value();    }  }  if (isServiceMetrics && appId != null) {    log.debug("Publishing service metrics. {}",record);    serviceTimelinePublisher.publishMetrics(record.metrics(),appId,ServiceTimelineEntityType.SERVICE_ATTEMPT.toString(),record.timestamp());  } else   if (isComponentMetrics) {
private void putEntity(TimelineEntity entity){  try {    if (log.isDebugEnabled()) {
public static Configuration loadFromResource(String resource){  Configuration conf=new Configuration(false);  URL resURL=getResourceUrl(resource);  if (resURL != null) {
public void verifyDirectoryNonexistent(Path clusterDirectory) throws IOException, SliderException {  if (fileSystem.exists(clusterDirectory)) {
public LocalResource submitFile(File localFile,Path tempPath,String subdir,String destFileName) throws IOException {  Path src=new Path(localFile.toString());  Path subdirPath=new Path(tempPath,subdir);  fileSystem.mkdirs(subdirPath);  Path destPath=new Path(subdirPath,destFileName);
public static String generateToken(String server) throws IOException, InterruptedException {  UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();
public static String generateToken(String server) throws IOException, InterruptedException {  UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();  LOG.debug("The user credential is {}",currentUser);  String challenge=currentUser.doAs(new PrivilegedExceptionAction<String>(){    @Override public String run() throws Exception {      try {        Oid mechOid=KerberosUtil.getOidInstance("GSS_KRB5_MECH_OID");        GSSManager manager=GSSManager.getInstance();        GSSName serverName=manager.createName("HTTP@" + server,GSSName.NT_HOSTBASED_SERVICE);        GSSContext gssContext=manager.createContext(serverName.canonicalize(mechOid),mechOid,null,GSSContext.DEFAULT_LIFETIME);        gssContext.requestMutualAuth(true);        gssContext.requestCredDeleg(true);        byte[] inToken=new byte[0];        byte[] outToken=gssContext.initSecContext(inToken,0,inToken.length);        gssContext.dispose();
  String challenge=currentUser.doAs(new PrivilegedExceptionAction<String>(){    @Override public String run() throws Exception {      try {        Oid mechOid=KerberosUtil.getOidInstance("GSS_KRB5_MECH_OID");        GSSManager manager=GSSManager.getInstance();        GSSName serverName=manager.createName("HTTP@" + server,GSSName.NT_HOSTBASED_SERVICE);        GSSContext gssContext=manager.createContext(serverName.canonicalize(mechOid),mechOid,null,GSSContext.DEFAULT_LIFETIME);        gssContext.requestMutualAuth(true);        gssContext.requestCredDeleg(true);        byte[] inToken=new byte[0];        byte[] outToken=gssContext.initSecContext(inToken,0,inToken.length);        gssContext.dispose();        LOG.debug("Got valid challenge for host {}",serverName);        return new String(BASE_64_CODEC.encode(outToken),StandardCharsets.US_ASCII);      } catch (      GSSException|IllegalAccessException|NoSuchFieldException|ClassNotFoundException e) {
public static Builder connect(String url) throws URISyntaxException, IOException, InterruptedException {  boolean useKerberos=UserGroupInformation.isSecurityEnabled();  URI resource=new URI(url);  Client client=Client.create();  Builder builder=client.resource(url).type(MediaType.APPLICATION_JSON);  if (useKerberos) {    String challenge=generateToken(resource.getHost());    builder.header(HttpHeaders.AUTHORIZATION,"Negotiate " + challenge);
  try {    validateDockerClientConfiguration(service,conf);  } catch (  IOException e) {    throw new IllegalArgumentException(e);  }  Configuration globalConf=service.getConfiguration();  Set<String> componentNames=new HashSet<>();  List<Component> componentsToRemove=new ArrayList<>();  List<Component> componentsToAdd=new ArrayList<>();  for (  Component comp : service.getComponents()) {    int maxCompLength=RegistryConstants.MAX_FQDN_LABEL_LENGTH;    maxCompLength=maxCompLength - Long.toString(Long.MAX_VALUE).length();    if (dnsEnabled && comp.getName().length() > maxCompLength) {      throw new IllegalArgumentException(String.format(RestApiErrorMessages.ERROR_COMPONENT_NAME_INVALID,maxCompLength,comp.getName()));    }    if (service.getName().equals(comp.getName())) {      throw new IllegalArgumentException(String.format(RestApiErrorMessages.ERROR_COMPONENT_NAME_CONFLICTS_WITH_SERVICE_NAME,comp.getName(),service.getName()));
  Configuration globalConf=service.getConfiguration();  Set<String> componentNames=new HashSet<>();  List<Component> componentsToRemove=new ArrayList<>();  List<Component> componentsToAdd=new ArrayList<>();  for (  Component comp : service.getComponents()) {    int maxCompLength=RegistryConstants.MAX_FQDN_LABEL_LENGTH;    maxCompLength=maxCompLength - Long.toString(Long.MAX_VALUE).length();    if (dnsEnabled && comp.getName().length() > maxCompLength) {      throw new IllegalArgumentException(String.format(RestApiErrorMessages.ERROR_COMPONENT_NAME_INVALID,maxCompLength,comp.getName()));    }    if (service.getName().equals(comp.getName())) {      throw new IllegalArgumentException(String.format(RestApiErrorMessages.ERROR_COMPONENT_NAME_CONFLICTS_WITH_SERVICE_NAME,comp.getName(),service.getName()));    }    if (componentNames.contains(comp.getName())) {      throw new IllegalArgumentException("Component name collision: " + comp.getName());    }    if (comp.getArtifact() != null && comp.getArtifact().getType() == Artifact.TypeEnum.SERVICE) {      if (StringUtils.isEmpty(comp.getArtifact().getId())) {
    maxCompLength=maxCompLength - Long.toString(Long.MAX_VALUE).length();    if (dnsEnabled && comp.getName().length() > maxCompLength) {      throw new IllegalArgumentException(String.format(RestApiErrorMessages.ERROR_COMPONENT_NAME_INVALID,maxCompLength,comp.getName()));    }    if (service.getName().equals(comp.getName())) {      throw new IllegalArgumentException(String.format(RestApiErrorMessages.ERROR_COMPONENT_NAME_CONFLICTS_WITH_SERVICE_NAME,comp.getName(),service.getName()));    }    if (componentNames.contains(comp.getName())) {      throw new IllegalArgumentException("Component name collision: " + comp.getName());    }    if (comp.getArtifact() != null && comp.getArtifact().getType() == Artifact.TypeEnum.SERVICE) {      if (StringUtils.isEmpty(comp.getArtifact().getId())) {        throw new IllegalArgumentException(RestApiErrorMessages.ERROR_ARTIFACT_ID_INVALID);      }      LOG.info("Marking {} for removal",comp.getName());      componentsToRemove.add(comp);      List<Component> externalComponents=getComponents(fs,comp.getArtifact().getId());      for (      Component c : externalComponents) {        Component override=service.getComponent(c.getName());
private static void validateDockerClientConfiguration(Service service,org.apache.hadoop.conf.Configuration conf) throws IOException {  String dockerClientConfig=service.getDockerClientConfig();  if (!StringUtils.isEmpty(dockerClientConfig)) {    Path dockerClientConfigPath=new Path(dockerClientConfig);    FileSystem fs=dockerClientConfigPath.getFileSystem(conf);
public static Service loadService(SliderFileSystem fs,String serviceName) throws IOException {  Path serviceJson=getServiceJsonPath(fs,serviceName);
public static Service loadServiceUpgrade(SliderFileSystem fs,String serviceName,String version) throws IOException {  Path versionPath=fs.buildClusterUpgradeDirPath(serviceName,version);  Path versionedDef=new Path(versionPath,serviceName + ".json");
public static Service loadServiceFrom(SliderFileSystem fs,Path appDefPath) throws IOException {
public static void createDirAndPersistApp(SliderFileSystem fs,Path appDir,Service service) throws IOException, SliderException {  FsPermission appDirPermission=new FsPermission("750");  fs.createWithPermissions(appDir,appDirPermission);  Path appJson=writeAppDefinition(fs,appDir,service);
private static boolean serviceDependencySatisfied(Service service){  boolean result=true;  try {    List<String> dependencies=service.getDependencies();    org.apache.hadoop.conf.Configuration conf=new org.apache.hadoop.conf.Configuration();    if (dependencies != null && dependencies.size() > 0) {      ServiceClient sc=new ServiceClient();      sc.init(conf);      sc.start();      for (      String dependent : dependencies) {        Service dependentService=sc.getStatus(dependent);        if (dependentService.getState() == null || !dependentService.getState().equals(ServiceState.STABLE)) {          result=false;
      InetAddress.getByName(name);      return true;    } catch (    UnknownHostException e) {      return false;    }  }  String dnsURI=String.format("dns://%s",addr);  Hashtable<String,Object> env=new Hashtable<>();  env.put(Context.INITIAL_CONTEXT_FACTORY,"com.sun.jndi.dns.DnsContextFactory");  env.put(Context.PROVIDER_URL,dnsURI);  try {    DirContext ictx=new InitialDirContext(env);    Attributes attrs=ictx.getAttributes(name,new String[]{"A"});    if (attrs.size() > 0) {      return true;    }  } catch (  NameNotFoundException e) {  }catch (  NamingException e) {
  if (loader == null) {    throw new IOException("Class " + my_class + " does not have a classloader!");  }  String class_file=my_class.getName().replaceAll("\\.","/") + ".class";  Enumeration<URL> urlEnumeration=loader.getResources(class_file);  for (; urlEnumeration.hasMoreElements(); ) {    URL url=urlEnumeration.nextElement();    if ("jar".equals(url.getProtocol())) {      String toReturn=url.getPath();      if (toReturn.startsWith("file:")) {        toReturn=toReturn.substring("file:".length());      }      toReturn=toReturn.replaceAll("\\+","%2B");      toReturn=URLDecoder.decode(toReturn,"UTF-8");      String jarFilePath=toReturn.replaceAll("!.*$","");      return new File(jarFilePath);    } else {
public static void putAllJars(Map<String,LocalResource> providerResources,SliderFileSystem sliderFileSystem,Path tempPath,String libDir,String srcPath) throws IOException, SliderException {
public static void tarGzipFolder(String[] libDirs,File tarGzipFile,FilenameFilter filter) throws IOException {
public void deleteComponentDir(String serviceVersion,String compName) throws IOException {  Path path=getComponentDir(serviceVersion,compName);  if (fileSystem.exists(path)) {    fileSystem.delete(path,true);
public void deleteComponentsVersionDirIfEmpty(String serviceVersion) throws IOException {  Path path=new Path(new Path(getAppDir(),"components"),serviceVersion);  if (fileSystem.exists(path) && fileSystem.listStatus(path).length == 0) {    fileSystem.delete(path,true);
@Override protected Path getAppDir(){  Path path=new Path(new Path("target","apps"),service.getName());
          throw new RuntimeException(e);        }      }      return yarnRegistryView;    }    @Override protected AMRMClientAsync<AMRMClient.ContainerRequest> createAMRMClient(){      AMRMClientImpl client1=new AMRMClientImpl(){        @Override public AllocateResponse allocate(        float progressIndicator) throws YarnException, IOException {          AllocateResponse.AllocateResponseBuilder builder=AllocateResponse.newBuilder();synchronized (feedContainers) {            if (feedContainers.isEmpty()) {              LOG.info("Allocating........ no containers");            } else {              List<Container> allocatedContainers=new LinkedList<>();              Iterator<Container> itor=feedContainers.iterator();              while (itor.hasNext()) {                Container c=itor.next();                org.apache.hadoop.yarn.service.component.Component component=componentsById.get(c.getAllocationRequestId());
  conf.set(YarnConfiguration.NM_VMEM_PMEM_RATIO,"8");  conf.set(YarnConfiguration.NM_CONTAINER_MON_RESOURCE_CALCULATOR,LinuxResourceCalculatorPlugin.class.getName());  conf.set(YarnConfiguration.NM_CONTAINER_MON_PROCESS_TREE,ProcfsBasedProcessTree.class.getName());  conf.setBoolean(YarnConfiguration.YARN_MINICLUSTER_CONTROL_RESOURCE_MONITORING,true);  conf.setBoolean(TIMELINE_SERVICE_ENABLED,false);  conf.setInt(YarnConfiguration.NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE,100);  conf.setLong(DEBUG_NM_DELETE_DELAY_SEC,60000);  conf.setLong(AM_RESOURCE_MEM,526);  conf.setLong(YarnServiceConf.READINESS_CHECK_INTERVAL,5);  conf.setBoolean(NM_VMEM_CHECK_ENABLED,false);  conf.setBoolean(NM_PMEM_CHECK_ENABLED,false);  conf.set(HttpServer2.FILTER_INITIALIZER_PROPERTY,"org.apache.hadoop.security.AuthenticationFilterInitializer," + "org.apache.hadoop.security.HttpCrossOriginFilterInitializer");  zkCluster=new TestingCluster(1);  zkCluster.start();  conf.set(YarnConfiguration.RM_ZK_ADDRESS,zkCluster.getConnectString());
  }  conf.set(YARN_SERVICE_BASE_PATH,basedir.getAbsolutePath());  if (yarnCluster == null) {    yarnCluster=new MiniYARNCluster(this.getClass().getSimpleName(),1,numNodeManager,1,1);    yarnCluster.init(conf);    yarnCluster.start();    waitForNMsToRegister();    URL url=Thread.currentThread().getContextClassLoader().getResource("yarn-site.xml");    if (url == null) {      throw new RuntimeException("Could not find 'yarn-site.xml' dummy file in classpath");    }    Configuration yarnClusterConfig=yarnCluster.getConfig();    yarnClusterConfig.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,new File(url.getPath()).getParent());    ByteArrayOutputStream bytesOut=new ByteArrayOutputStream();    yarnClusterConfig.writeXml(bytesOut);    bytesOut.close();    OutputStream os=new FileOutputStream(new File(url.getPath()));
protected Multimap<String,String> waitForAllCompToBeReady(ServiceClient client,Service exampleApp) throws TimeoutException, InterruptedException {  int expectedTotalContainers=countTotalContainers(exampleApp);  Multimap<String,String> allContainers=HashMultimap.create();  GenericTestUtils.waitFor(() -> {    try {      Service retrievedApp=client.getStatus(exampleApp.getName());      int totalReadyContainers=0;      allContainers.clear();      LOG.info("Num Components " + retrievedApp.getComponents().size());      for (      Component component : retrievedApp.getComponents()) {
protected Multimap<String,String> waitForAllCompToBeReady(ServiceClient client,Service exampleApp) throws TimeoutException, InterruptedException {  int expectedTotalContainers=countTotalContainers(exampleApp);  Multimap<String,String> allContainers=HashMultimap.create();  GenericTestUtils.waitFor(() -> {    try {      Service retrievedApp=client.getStatus(exampleApp.getName());      int totalReadyContainers=0;      allContainers.clear();      LOG.info("Num Components " + retrievedApp.getComponents().size());      for (      Component component : retrievedApp.getComponents()) {        LOG.info("looking for  " + component.getName());
protected Multimap<String,String> waitForAllCompToBeReady(ServiceClient client,Service exampleApp) throws TimeoutException, InterruptedException {  int expectedTotalContainers=countTotalContainers(exampleApp);  Multimap<String,String> allContainers=HashMultimap.create();  GenericTestUtils.waitFor(() -> {    try {      Service retrievedApp=client.getStatus(exampleApp.getName());      int totalReadyContainers=0;      allContainers.clear();      LOG.info("Num Components " + retrievedApp.getComponents().size());      for (      Component component : retrievedApp.getComponents()) {        LOG.info("looking for  " + component.getName());        LOG.info(component.toString());        if (component.getContainers() != null) {          if (component.getContainers().size() == exampleApp.getComponent(component.getName()).getNumberOfContainers()) {            for (            Container container : component.getContainers()) {
    try {      Service retrievedApp=client.getStatus(exampleApp.getName());      int totalReadyContainers=0;      allContainers.clear();      LOG.info("Num Components " + retrievedApp.getComponents().size());      for (      Component component : retrievedApp.getComponents()) {        LOG.info("looking for  " + component.getName());        LOG.info(component.toString());        if (component.getContainers() != null) {          if (component.getContainers().size() == exampleApp.getComponent(component.getName()).getNumberOfContainers()) {            for (            Container container : component.getContainers()) {              LOG.info("Container state " + container.getState() + ", component "+ component.getName());              if (container.getState() == ContainerState.READY) {                totalReadyContainers++;                allContainers.put(component.getName(),container.getId());
      int totalReadyContainers=0;      allContainers.clear();      LOG.info("Num Components " + retrievedApp.getComponents().size());      for (      Component component : retrievedApp.getComponents()) {        LOG.info("looking for  " + component.getName());        LOG.info(component.toString());        if (component.getContainers() != null) {          if (component.getContainers().size() == exampleApp.getComponent(component.getName()).getNumberOfContainers()) {            for (            Container container : component.getContainers()) {              LOG.info("Container state " + container.getState() + ", component "+ component.getName());              if (container.getState() == ContainerState.READY) {                totalReadyContainers++;                allContainers.put(component.getName(),container.getId());                LOG.info("Found 1 ready container " + container.getId());              }            }          } else {
  compA.getConfiguration().getEnv().put("YARN_CONTAINER_RUNTIME_YARN_SYSFS_ENABLE","true");  Artifact artifact=new Artifact();  artifact.setType(Artifact.TypeEnum.TARBALL);  compA.artifact(artifact);  exampleApp.addComponent(compA);  try {    MockServiceAM am=new MockServiceAM(exampleApp);    am.init(conf);    am.start();    ServiceScheduler scheduler=am.context.scheduler;    scheduler.syncSysFs(exampleApp);    scheduler.close();    am.stop();    am.close();  } catch (  Exception e) {
  conf.setBoolean(YarnConfiguration.YARN_MINICLUSTER_FIXED_PORTS,true);  conf.setBoolean(YarnConfiguration.YARN_MINICLUSTER_USE_RPC,true);  conf.setInt(YarnConfiguration.RM_MAX_COMPLETED_APPLICATIONS,YarnConfiguration.DEFAULT_RM_MAX_COMPLETED_APPLICATIONS);  setConf(conf);  setupInternal(NUM_NMS);  ServiceClient client=createClient(getConf());  Service exampleApp=createExampleApplication();  client.actionCreate(exampleApp);  Multimap<String,String> containersBeforeFailure=waitForAllCompToBeReady(client,exampleApp);  LOG.info("Restart the resource manager");  getYarnCluster().restartResourceManager(getYarnCluster().getActiveRMIndex());  GenericTestUtils.waitFor(() -> getYarnCluster().getResourceManager().getServiceState() == org.apache.hadoop.service.Service.STATE.STARTED,2000,200000);  Assert.assertTrue("node managers connected",getYarnCluster().waitForNodeManagersToConnect(5000));  ApplicationId exampleAppId=ApplicationId.fromString(exampleApp.getId());  ApplicationAttemptId applicationAttemptId=client.getYarnClient().getApplicationReport(exampleAppId).getCurrentApplicationAttemptId();
  ApplicationId exampleAppId=ApplicationId.fromString(exampleApp.getId());  ApplicationAttemptId applicationAttemptId=client.getYarnClient().getApplicationReport(exampleAppId).getCurrentApplicationAttemptId();  LOG.info("Fail the application attempt {}",applicationAttemptId);  client.getYarnClient().failApplicationAttempt(applicationAttemptId);  GenericTestUtils.waitFor(() -> {    try {      ApplicationReport ar=client.getYarnClient().getApplicationReport(exampleAppId);      return ar.getCurrentApplicationAttemptId().getAttemptId() == 2 && ar.getYarnApplicationState() == YarnApplicationState.RUNNING;    } catch (    YarnException|IOException e) {      throw new RuntimeException("while waiting",e);    }  },2000,200000);  Multimap<String,String> containersAfterFailure=waitForAllCompToBeReady(client,exampleApp);  containersBeforeFailure.keys().forEach(compName -> {    Assert.assertEquals("num containers after by restart for " + compName,containersBeforeFailure.get(compName).size(),containersAfterFailure.get(compName) == null ? 0 : containersAfterFailure.get(compName).size());  });
  component.getConfiguration().getEnv().put("key1","val1");  client.initiateUpgrade(service);  waitForServiceToBeInState(client,service,ServiceState.UPGRADING);  SliderFileSystem fs=new SliderFileSystem(getConf());  Service fromFs=ServiceApiUtil.loadServiceUpgrade(fs,service.getName(),service.getVersion());  Assert.assertEquals(service.getName(),fromFs.getName());  Assert.assertEquals(service.getVersion(),fromFs.getVersion());  Service liveService=client.getStatus(service.getName());  client.actionUpgrade(service,liveService.getComponent(component.getName()).getContainers());  waitForAllCompToBeReady(client,service);  client.actionStart(service.getName());  waitForServiceToBeStable(client,service);  Service active=client.getStatus(service.getName());  Assert.assertEquals("component not stable",ComponentState.STABLE,active.getComponent(component.getName()).getState());  Assert.assertEquals("comp does not have new env","val1",active.getComponent(component.getName()).getConfiguration().getEnv("key1"));
  waitForServiceToBeStable(client,service);  Component component=service.getComponents().iterator().next();  service.setState(ServiceState.EXPRESS_UPGRADING);  service.setVersion("v2");  component.getConfiguration().getEnv().put("key1","val1");  Component component2=service.getComponent("compb");  component2.getConfiguration().getEnv().put("key2","val2");  client.actionUpgradeExpress(service);  waitForServiceToBeExpressUpgrading(client,service);  waitForServiceToBeStable(client,service);  Service active=client.getStatus(service.getName());  Assert.assertEquals("version mismatch",service.getVersion(),active.getVersion());  Assert.assertEquals("component not stable",ComponentState.STABLE,active.getComponent(component.getName()).getState());  Assert.assertEquals("compa does not have new env","val1",active.getComponent(component.getName()).getConfiguration().getEnv("key1"));  Assert.assertEquals("compb does not have new env","val2",active.getComponent(component2.getName()).getConfiguration().getEnv("key2"));
  waitForServiceToBeStable(client,service);  service.setState(ServiceState.UPGRADING);  service.setVersion("v2");  component.getConfiguration().getEnv().put("key1","val1");  client.initiateUpgrade(service);  waitForServiceToBeInState(client,service,ServiceState.UPGRADING);  Service liveService=client.getStatus(service.getName());  Container container=liveService.getComponent(component.getName()).getContainers().iterator().next();  client.actionUpgrade(service,Lists.newArrayList(container));  Thread.sleep(500);  client.actionCancelUpgrade(service.getName());  waitForServiceToBeStable(client,service);  Service active=client.getStatus(service.getName());  Assert.assertEquals("component not stable",ComponentState.STABLE,active.getComponent(component.getName()).getState());  Assert.assertEquals("comp does not have new env","val0",active.getComponent(component.getName()).getConfiguration().getEnv("key1"));
    Assert.assertNotEquals("Service state should not be STABLE",ServiceState.STABLE,service.getState());    Assert.assertEquals("Component state should be FLEXING",ComponentState.FLEXING,component.getState());    Assert.assertEquals("3 containers are expected to be running",3,component.getContainers().size());  }  compCounts=new HashMap<>();  compCounts.put("compa",4L);  exampleApp.getComponent("compa").setNumberOfContainers(4L);  client.flexByRestService(exampleApp.getName(),compCounts);  try {    waitForServiceToBeStable(client,exampleApp,10000);    Assert.fail("Service should not be in a stable state. It should throw " + "a timeout exception.");  } catch (  Exception e) {    service=client.getStatus(exampleApp.getName());    component=service.getComponent("compa");    Assert.assertNotEquals("Service state should not be STABLE",ServiceState.STABLE,service.getState());    Assert.assertEquals("Component state should be FLEXING",ComponentState.FLEXING,component.getState());
private int runCLI(String[] args) throws Exception {
@Test public void testOverride() throws Throwable {  Service orig=ExampleAppJson.loadResource(OVERRIDE_JSON);  Configuration global=orig.getConfiguration();  assertEquals("a",global.getProperty("g1"));  assertEquals("b",global.getProperty("g2"));  assertEquals(2,global.getFiles().size());  Configuration simple=orig.getComponent("simple").getConfiguration();  assertEquals(0,simple.getProperties().size());  assertEquals(1,simple.getFiles().size());  Configuration master=orig.getComponent("master").getConfiguration();  assertEquals("m",master.getProperty("name"));  assertEquals("overridden",master.getProperty("g1"));  assertEquals(0,master.getFiles().size());  Configuration worker=orig.getComponent("worker").getConfiguration();
  Configuration master=orig.getComponent("master").getConfiguration();  assertEquals("m",master.getProperty("name"));  assertEquals("overridden",master.getProperty("g1"));  assertEquals(0,master.getFiles().size());  Configuration worker=orig.getComponent("worker").getConfiguration();  LOG.info("worker = {}",worker);  assertEquals(3,worker.getProperties().size());  assertEquals(0,worker.getFiles().size());  assertEquals("worker",worker.getProperty("name"));  assertEquals("overridden-by-worker",worker.getProperty("g1"));  assertNull(worker.getProperty("g2"));  assertEquals("1000",worker.getProperty("timeout"));  SliderFileSystem sfs=ServiceTestUtils.initMockFs();  ServiceApiUtil.validateAndResolveService(orig,sfs,new YarnConfiguration());  global=orig.getConfiguration();
  assertEquals(2,global.getFiles().size());  simple=orig.getComponent("simple").getConfiguration();  assertEquals(2,simple.getProperties().size());  assertEquals("a",simple.getProperty("g1"));  assertEquals("b",simple.getProperty("g2"));  assertEquals(2,simple.getFiles().size());  Set<ConfigFile> files=new HashSet<>();  Map<String,String> props=new HashMap<>();  props.put("k1","overridden");  props.put("k2","v2");  files.add(new ConfigFile().destFile("file1").type(ConfigFile.TypeEnum.PROPERTIES).properties(props));  files.add(new ConfigFile().destFile("file2").type(ConfigFile.TypeEnum.XML).properties(Collections.singletonMap("k3","v3")));  assertTrue(files.contains(simple.getFiles().get(0)));  assertTrue(files.contains(simple.getFiles().get(1)));  master=orig.getComponent("master").getConfiguration();
  assertTrue(files.contains(simple.getFiles().get(1)));  master=orig.getComponent("master").getConfiguration();  LOG.info("master = {}",master);  assertEquals(3,master.getProperties().size());  assertEquals("m",master.getProperty("name"));  assertEquals("overridden",master.getProperty("g1"));  assertEquals("b",master.getProperty("g2"));  assertEquals(2,master.getFiles().size());  props.put("k1","v1");  files.clear();  files.add(new ConfigFile().destFile("file1").type(ConfigFile.TypeEnum.PROPERTIES).properties(props));  files.add(new ConfigFile().destFile("file2").type(ConfigFile.TypeEnum.XML).properties(Collections.singletonMap("k3","v3")));  assertTrue(files.contains(master.getFiles().get(0)));  assertTrue(files.contains(master.getFiles().get(1)));  worker=orig.getComponent("worker").getConfiguration();
  assertEquals(0,global.getProperties().size());  assertEquals(4,orig.getComponents().size());  simple=orig.getComponent("simple").getConfiguration();  assertEquals(3,simple.getProperties().size());  assertEquals("a",simple.getProperty("g1"));  assertEquals("b",simple.getProperty("g2"));  assertEquals("60",simple.getProperty("yarn.service.failure-count-reset.window"));  master=orig.getComponent("master").getConfiguration();  assertEquals(5,master.getProperties().size());  assertEquals("512M",master.getProperty("jvm.heapsize"));  assertEquals("overridden",master.getProperty("g1"));  assertEquals("b",master.getProperty("g2"));  assertEquals("is-overridden",master.getProperty("g3"));  assertEquals("60",simple.getProperty("yarn.service.failure-count-reset.window"));  Configuration worker=orig.getComponent("worker").getConfiguration();
  artifact.setType(Artifact.TypeEnum.SERVICE);  try {    ServiceApiUtil.validateAndResolveService(app,sfs,CONF_DNS_ENABLED);    Assert.fail(EXCEPTION_PREFIX + "service with no artifact id");  } catch (  IllegalArgumentException e) {    assertEquals(ERROR_ARTIFACT_ID_INVALID,e.getMessage());  }  artifact.setType(Artifact.TypeEnum.TARBALL);  try {    ServiceApiUtil.validateAndResolveService(app,sfs,CONF_DNS_ENABLED);    Assert.fail(EXCEPTION_PREFIX + "service with no artifact id");  } catch (  IllegalArgumentException e) {    assertEquals(String.format(ERROR_ARTIFACT_ID_FOR_COMP_INVALID,compName),e.getMessage());  }  artifact.setType(Artifact.TypeEnum.DOCKER);  artifact.setId("docker.io/centos:centos7");  try {
@OnWebSocketClose public void onClose(Session session,int status,String reason){  if (status == 1000) {
    while (mySession.isOpen()) {      mySession.getRemote().flush();      if (consoleReader.hasData()) {        String message=consoleReader.read();        mySession.getRemote().sendString(message);        mySession.getRemote().sendString("\r");      }      String message="1{}";      mySession.getRemote().sendString(message);      Thread.sleep(100);      mySession.getRemote().flush();    }    inputThread.join();  } catch (  IOException|InterruptedException e) {    try {      mySession.disconnect();    } catch (    IOException e1) {
@Override protected void serviceInit(Configuration conf) throws Exception {  this.maxThreadPoolSize=conf.getInt(YarnConfiguration.NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE,YarnConfiguration.DEFAULT_NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE);
      Set<String> allNodes=new HashSet<String>();      while (!stopped.get() && !Thread.currentThread().isInterrupted()) {        try {          event=events.take();        } catch (        InterruptedException e) {          if (!stopped.get()) {            LOG.error("Returning, thread interrupted",e);          }          return;        }        allNodes.add(event.getNodeId().toString());        int threadPoolSize=threadPool.getCorePoolSize();        if (threadPoolSize != maxThreadPoolSize) {          int nodeNum=allNodes.size();          int idealThreadPoolSize=Math.min(maxThreadPoolSize,nodeNum);          if (threadPoolSize < idealThreadPoolSize) {            int newThreadPoolSize=Math.min(maxThreadPoolSize,idealThreadPoolSize + INITIAL_THREAD_POOL_SIZE);
@Private @VisibleForTesting protected void populateNMTokens(List<NMToken> nmTokens){  for (  NMToken token : nmTokens) {    String nodeId=token.getNodeId().toString();    if (LOG.isDebugEnabled()) {      if (getNMTokenCache().containsToken(nodeId)) {
@Override public synchronized void requestContainerUpdate(Container container,UpdateContainerRequest updateContainerRequest){  Preconditions.checkNotNull(container,"Container cannot be null!!");  Preconditions.checkNotNull(updateContainerRequest,"UpdateContainerRequest cannot be null!!");
public synchronized ContainerManagementProtocolProxyData getProxy(String containerManagerBindAddr,ContainerId containerId) throws InvalidToken {  ContainerManagementProtocolProxyData proxy=cmProxy.get(containerManagerBindAddr);  while (proxy != null && !proxy.token.getIdentifier().equals(nmTokenCache.getToken(containerManagerBindAddr).getIdentifier())) {
private void addProxyToCache(String containerManagerBindAddr,ContainerManagementProtocolProxyData proxy){  while (cmProxy.size() >= maxConnectedNMs) {
private boolean tryCloseProxy(ContainerManagementProtocolProxyData proxy){  proxy.activeCallers--;  if (proxy.scheduledForClose && proxy.activeCallers < 0) {
@SuppressWarnings("unchecked") void put(Priority priority,String resourceName,ExecutionType execType,Resource capability,ResourceRequestInfo resReqInfo){  Map<String,Map<ExecutionType,TreeMap<Resource,ResourceRequestInfo>>> locationMap=remoteRequestsTable.get(priority);  if (locationMap == null) {    locationMap=new HashMap<>();    this.remoteRequestsTable.put(priority,locationMap);
@SuppressWarnings("unchecked") void put(Priority priority,String resourceName,ExecutionType execType,Resource capability,ResourceRequestInfo resReqInfo){  Map<String,Map<ExecutionType,TreeMap<Resource,ResourceRequestInfo>>> locationMap=remoteRequestsTable.get(priority);  if (locationMap == null) {    locationMap=new HashMap<>();    this.remoteRequestsTable.put(priority,locationMap);    LOG.debug("Added priority={}",priority);  }  Map<ExecutionType,TreeMap<Resource,ResourceRequestInfo>> execTypeMap=locationMap.get(resourceName);  if (execTypeMap == null) {    execTypeMap=new HashMap<>();    locationMap.put(resourceName,execTypeMap);    LOG.debug("Added resourceName={}",resourceName);  }  TreeMap<Resource,ResourceRequestInfo> capabilityMap=execTypeMap.get(execType);  if (capabilityMap == null) {    capabilityMap=new TreeMap<>(new AMRMClientImpl.ResourceReverseComparator());    execTypeMap.put(execType,capabilityMap);
ResourceRequestInfo remove(Priority priority,String resourceName,ExecutionType execType,Resource capability){  ResourceRequestInfo retVal=null;  Map<String,Map<ExecutionType,TreeMap<Resource,ResourceRequestInfo>>> locationMap=remoteRequestsTable.get(priority);  if (locationMap == null) {
ResourceRequestInfo decResourceRequest(Priority priority,String resourceName,ExecutionTypeRequest execTypeReq,Resource capability,T req){  ResourceRequestInfo resourceRequestInfo=get(priority,resourceName,execTypeReq.getExecutionType(),capability);  if (resourceRequestInfo == null) {
    if (isSecurityEnabled()) {      addLogAggregationDelegationToken(appContext.getAMContainerSpec());    }  } catch (  Exception e) {    LOG.warn("Failed to obtain delegation token for Log Aggregation Path",e);  }  rmClient.submitApplication(request);  int pollCount=0;  long startTime=System.currentTimeMillis();  EnumSet<YarnApplicationState> waitingStates=EnumSet.of(YarnApplicationState.NEW,YarnApplicationState.NEW_SAVING,YarnApplicationState.SUBMITTED);  EnumSet<YarnApplicationState> failToSubmitStates=EnumSet.of(YarnApplicationState.FAILED,YarnApplicationState.KILLED);  while (true) {    try {      ApplicationReport appReport=getApplicationReport(applicationId);      YarnApplicationState state=appReport.getYarnApplicationState();      if (!waitingStates.contains(state)) {        if (failToSubmitStates.contains(state)) {
    LOG.warn("Failed to obtain delegation token for Log Aggregation Path",e);  }  rmClient.submitApplication(request);  int pollCount=0;  long startTime=System.currentTimeMillis();  EnumSet<YarnApplicationState> waitingStates=EnumSet.of(YarnApplicationState.NEW,YarnApplicationState.NEW_SAVING,YarnApplicationState.SUBMITTED);  EnumSet<YarnApplicationState> failToSubmitStates=EnumSet.of(YarnApplicationState.FAILED,YarnApplicationState.KILLED);  while (true) {    try {      ApplicationReport appReport=getApplicationReport(applicationId);      YarnApplicationState state=appReport.getYarnApplicationState();      if (!waitingStates.contains(state)) {        if (failToSubmitStates.contains(state)) {          throw new YarnException("Failed to submit " + applicationId + " to YARN : "+ appReport.getDiagnostics());        }        LOG.info("Submitted application " + applicationId);        break;
    try {      ApplicationReport appReport=getApplicationReport(applicationId);      YarnApplicationState state=appReport.getYarnApplicationState();      if (!waitingStates.contains(state)) {        if (failToSubmitStates.contains(state)) {          throw new YarnException("Failed to submit " + applicationId + " to YARN : "+ appReport.getDiagnostics());        }        LOG.info("Submitted application " + applicationId);        break;      }      long elapsedMillis=System.currentTimeMillis() - startTime;      if (enforceAsyncAPITimeout() && elapsedMillis >= asyncApiPollTimeoutMillis) {        throw new YarnException("Timed out while waiting for application " + applicationId + " to be submitted successfully");      }      if (++pollCount % 10 == 0) {        LOG.info("Application submission is not finished, " + "submitted application " + applicationId + " is still in "+ state);      }      try {        Thread.sleep(submitPollIntervalMillis);
    dibb.reset(tokens);    credentials.readTokenStorageStream(dibb);    tokens.rewind();  }  Configuration conf=getConfig();  String masterPrincipal=YarnClientUtils.getRmPrincipal(conf);  if (StringUtils.isEmpty(masterPrincipal)) {    throw new IOException("Can't get Master Kerberos principal for use as renewer");  }  LOG.debug("Delegation Token Renewer: " + masterPrincipal);  LogAggregationFileControllerFactory factory=new LogAggregationFileControllerFactory(conf);  LogAggregationFileController fileController=factory.getFileControllerForWrite();  Path remoteRootLogDir=fileController.getRemoteRootLogDir();  FileSystem fs=remoteRootLogDir.getFileSystem(conf);  final org.apache.hadoop.security.token.Token<?>[] finalTokens=fs.addDelegationTokens(masterPrincipal,credentials);  if (finalTokens != null) {    for (    org.apache.hadoop.security.token.Token<?> token : finalTokens) {
private void addTimelineDelegationToken(ContainerLaunchContext clc) throws YarnException, IOException {  Credentials credentials=new Credentials();  DataInputByteBuffer dibb=new DataInputByteBuffer();  ByteBuffer tokens=clc.getTokens();  if (tokens != null) {    dibb.reset(tokens);    credentials.readTokenStorageStream(dibb);    tokens.rewind();  }  for (  org.apache.hadoop.security.token.Token<? extends TokenIdentifier> token : credentials.getAllTokens()) {    if (token.getKind().equals(TimelineDelegationTokenIdentifier.KIND_NAME)) {      return;    }  }  org.apache.hadoop.security.token.Token<TimelineDelegationTokenIdentifier> timelineDelegationToken=getTimelineDelegationToken();  if (timelineDelegationToken == null) {    return;  }  credentials.addToken(timelineService,timelineDelegationToken);
@Override public void failApplicationAttempt(ApplicationAttemptId attemptId) throws YarnException, IOException {
  request.setApplicationId(applicationId);  if (diagnostics != null) {    request.setDiagnostics(diagnostics);  }  try {    int pollCount=0;    long startTime=System.currentTimeMillis();    while (true) {      KillApplicationResponse response=rmClient.forceKillApplication(request);      if (response.getIsKillCompleted()) {        LOG.info("Killed application " + applicationId);        break;      }      long elapsedMillis=System.currentTimeMillis() - startTime;      if (enforceAsyncAPITimeout() && elapsedMillis >= this.asyncApiPollTimeoutMillis) {        throw new YarnException("Timed out while waiting for application " + applicationId + " to be killed.");      }      if (++pollCount % 10 == 0) {
@Override public void signalToContainer(ContainerId containerId,SignalContainerCommand command) throws YarnException, IOException {
  QueueMetrics queueMetrics=new QueueMetrics();  List<QueueInfo> queuesInfo;  if (queues.isEmpty()) {    try {      queuesInfo=client.getRootQueueInfos();    } catch (    Exception ie) {      LOG.error("Unable to get queue information",ie);      return queueMetrics;    }  } else {    queuesInfo=new ArrayList<>();    for (    String queueName : queues) {      try {        QueueInfo qInfo=client.getQueueInfo(queueName);        queuesInfo.add(qInfo);      } catch (      Exception ie) {
public static String generateToken(String server) throws IOException, InterruptedException {  UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();
public static String generateToken(String server) throws IOException, InterruptedException {  UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();  LOG.debug("The user credential is {}",currentUser);  String challenge=currentUser.doAs(new PrivilegedExceptionAction<String>(){    @Override public String run() throws Exception {      try {        Oid mechOid=KerberosUtil.getOidInstance("GSS_KRB5_MECH_OID");        GSSManager manager=GSSManager.getInstance();        GSSName serverName=manager.createName("HTTP@" + server,GSSName.NT_HOSTBASED_SERVICE);        GSSContext gssContext=manager.createContext(serverName.canonicalize(mechOid),mechOid,null,GSSContext.DEFAULT_LIFETIME);        gssContext.requestMutualAuth(true);        gssContext.requestCredDeleg(true);        byte[] inToken=new byte[0];        byte[] outToken=gssContext.initSecContext(inToken,0,inToken.length);        gssContext.dispose();
  String challenge=currentUser.doAs(new PrivilegedExceptionAction<String>(){    @Override public String run() throws Exception {      try {        Oid mechOid=KerberosUtil.getOidInstance("GSS_KRB5_MECH_OID");        GSSManager manager=GSSManager.getInstance();        GSSName serverName=manager.createName("HTTP@" + server,GSSName.NT_HOSTBASED_SERVICE);        GSSContext gssContext=manager.createContext(serverName.canonicalize(mechOid),mechOid,null,GSSContext.DEFAULT_LIFETIME);        gssContext.requestMutualAuth(true);        gssContext.requestCredDeleg(true);        byte[] inToken=new byte[0];        byte[] outToken=gssContext.initSecContext(inToken,0,inToken.length);        gssContext.dispose();        LOG.debug("Got valid challenge for host {}",serverName);        return new String(BASE_64_CODEC.encode(outToken),StandardCharsets.US_ASCII);      } catch (      GSSException|IllegalAccessException|NoSuchFieldException|ClassNotFoundException e) {
    @Override public ContainerReport getContainerReport(    String containerIdStr) throws YarnException, IOException {      ContainerReport mockReport=mock(ContainerReport.class);      doReturn(nodeId).when(mockReport).getAssignedNode();      doReturn("http://localhost:2345").when(mockReport).getNodeHttpAddress();      return mockReport;    }  };  cli.setConf(conf);  int exitCode=cli.run(new String[]{"-applicationId",appId.toString()});  LOG.info(sysOutStream.toString());  assertTrue(exitCode == 0);  assertTrue(sysOutStream.toString().contains(logMessage(containerId1,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId2,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId3,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId3,"stdout")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId3,"stdout1234")));
  LOG.info(sysOutStream.toString());  assertTrue(exitCode == 0);  assertTrue(sysOutStream.toString().contains(logMessage(containerId1,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId2,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId3,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId3,"stdout")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId3,"stdout1234")));  assertTrue(sysOutStream.toString().contains(createEmptyLog("empty")));  sysOutStream.reset();  exitCode=cli.run(new String[]{"-applicationId",appId.toString(),"-applicationAttemptId",appAttemptId1.toString()});  LOG.info(sysOutStream.toString());  assertTrue(exitCode == 0);  assertTrue(sysOutStream.toString().contains(logMessage(containerId1,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId2,"syslog")));  assertTrue(sysOutStream.toString().contains(logMessage(containerId3,"syslog")));
private int runTool(String... args) throws Exception {  errOutBytes.reset();  sysOutBytes.reset();  LOG.info("Running: NodeAttributesCLI " + Joiner.on(" ").join(args));  int ret=nodeAttributesCLI.run(args);  errOutput=new String(errOutBytes.toByteArray(),Charsets.UTF_8);  sysOutput=new String(sysOutBytes.toByteArray(),Charsets.UTF_8);
private int runTool(String... args) throws Exception {  errOutBytes.reset();  sysOutBytes.reset();  LOG.info("Running: NodeAttributesCLI " + Joiner.on(" ").join(args));  int ret=nodeAttributesCLI.run(args);  errOutput=new String(errOutBytes.toByteArray(),Charsets.UTF_8);  sysOutput=new String(sysOutBytes.toByteArray(),Charsets.UTF_8);  LOG.info("Err_output:\n" + errOutput);
@Override public void uncaughtException(Thread t,Throwable e){  if (ShutdownHookManager.get().isShutdownInProgress()) {
@Override public void uncaughtException(Thread t,Throwable e){  if (ShutdownHookManager.get().isShutdownInProgress()) {    LOG.error("Thread " + t + " threw an Throwable, but we are shutting "+ "down, so ignoring this",e);  } else   if (e instanceof Error) {    try {      LOG.error(FATAL,"Thread " + t + " threw an Error.  Shutting down now...",e);    } catch (    Throwable err) {    }    if (e instanceof OutOfMemoryError) {      try {        System.err.println("Halting due to Out Of Memory Error...");      } catch (      Throwable err) {      }      ExitUtil.halt(-1);    } else {      ExitUtil.terminate(-1);    }  } else {
public static <T>T createAHSProxy(final Configuration conf,final Class<T> protocol,InetSocketAddress ahsAddress) throws IOException {
@Public @Unstable public static <T>T createRMProxy(final Configuration configuration,final Class<T> protocol,UserGroupInformation user,final Token<? extends TokenIdentifier> token) throws IOException {  try {    String rmClusterId=configuration.get(YarnConfiguration.RM_CLUSTER_ID,YarnConfiguration.DEFAULT_RM_CLUSTER_ID);
@Override public void init(Configuration conf,RMProxy<T> proxy,Class<T> protocol){  this.protocol=protocol;  try {    YarnConfiguration yarnConf=new YarnConfiguration(conf);    InetSocketAddress rmAddress=proxy.getRMAddress(yarnConf,protocol);
    return putEntities(entities);  }  List<TimelineEntity> entitiesToDBStore=new ArrayList<TimelineEntity>();  List<TimelineEntity> entitiesToSummaryCache=new ArrayList<TimelineEntity>();  List<TimelineEntity> entitiesToEntityCache=new ArrayList<TimelineEntity>();  Path attemptDir=attemptDirCache.getAppAttemptDir(appAttemptId);  for (  TimelineEntity entity : entities) {    if (summaryEntityTypes.contains(entity.getEntityType())) {      entitiesToSummaryCache.add(entity);    } else {      if (groupId != null) {        entitiesToEntityCache.add(entity);      } else {        entitiesToDBStore.add(entity);      }    }  }  if (!entitiesToSummaryCache.isEmpty()) {    Path summaryLogPath=new Path(attemptDir,SUMMARY_LOG_PREFIX + appAttemptId.toString());
  List<TimelineEntity> entitiesToEntityCache=new ArrayList<TimelineEntity>();  Path attemptDir=attemptDirCache.getAppAttemptDir(appAttemptId);  for (  TimelineEntity entity : entities) {    if (summaryEntityTypes.contains(entity.getEntityType())) {      entitiesToSummaryCache.add(entity);    } else {      if (groupId != null) {        entitiesToEntityCache.add(entity);      } else {        entitiesToDBStore.add(entity);      }    }  }  if (!entitiesToSummaryCache.isEmpty()) {    Path summaryLogPath=new Path(attemptDir,SUMMARY_LOG_PREFIX + appAttemptId.toString());    LOG.debug("Writing summary log for {} to {}",appAttemptId,summaryLogPath);    this.logFDsCache.writeSummaryEntityLogs(fs,summaryLogPath,objMapper,appAttemptId,entitiesToSummaryCache,isAppendSupported);  }  if (!entitiesToEntityCache.isEmpty()) {
private void writeDomain(ApplicationAttemptId appAttemptId,TimelineDomain domain) throws IOException {  Path domainLogPath=new Path(attemptDirCache.getAppAttemptDir(appAttemptId),DOMAIN_LOG_PREFIX + appAttemptId.toString());
private static void putTimelineDataInJSONFile(String path,String type){  File jsonFile=new File(path);  if (!jsonFile.exists()) {
  } catch (  Exception e) {    LOG.error("Error when reading  " + e.getMessage());    e.printStackTrace(System.err);    return;  }  Configuration conf=new YarnConfiguration();  TimelineClient client=TimelineClient.createTimelineClient();  client.init(conf);  client.start();  try {    if (UserGroupInformation.isSecurityEnabled() && conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,false) && conf.get(YarnConfiguration.TIMELINE_HTTP_AUTH_TYPE).equals(KerberosAuthenticationHandler.TYPE)) {      Token<TimelineDelegationTokenIdentifier> token=client.getDelegationToken(UserGroupInformation.getCurrentUser().getUserName());      UserGroupInformation.getCurrentUser().addToken(token);    }    if (type.equals(ENTITY_DATA_TYPE)) {      TimelinePutResponse response=client.putEntities(entities.getEntities().toArray(new TimelineEntity[entities.getEntities().size()]));      if (response.getErrors().size() == 0) {
      Token<TimelineDelegationTokenIdentifier> token=client.getDelegationToken(UserGroupInformation.getCurrentUser().getUserName());      UserGroupInformation.getCurrentUser().addToken(token);    }    if (type.equals(ENTITY_DATA_TYPE)) {      TimelinePutResponse response=client.putEntities(entities.getEntities().toArray(new TimelineEntity[entities.getEntities().size()]));      if (response.getErrors().size() == 0) {        LOG.info("Timeline entities are successfully put");      } else {        for (        TimelinePutResponse.TimelinePutError error : response.getErrors()) {          LOG.error("TimelineEntity [" + error.getEntityType() + ":"+ error.getEntityId()+ "] is not successfully put. Error code: "+ error.getErrorCode());        }      }    } else     if (type.equals(DOMAIN_DATA_TYPE) && domains != null) {      boolean hasError=false;      for (      TimelineDomain domain : domains.getDomains()) {        try {          client.putDomain(domain);        } catch (        Exception e) {
    }    if (type.equals(ENTITY_DATA_TYPE)) {      TimelinePutResponse response=client.putEntities(entities.getEntities().toArray(new TimelineEntity[entities.getEntities().size()]));      if (response.getErrors().size() == 0) {        LOG.info("Timeline entities are successfully put");      } else {        for (        TimelinePutResponse.TimelinePutError error : response.getErrors()) {          LOG.error("TimelineEntity [" + error.getEntityType() + ":"+ error.getEntityId()+ "] is not successfully put. Error code: "+ error.getErrorCode());        }      }    } else     if (type.equals(DOMAIN_DATA_TYPE) && domains != null) {      boolean hasError=false;      for (      TimelineDomain domain : domains.getDomains()) {        try {          client.putDomain(domain);        } catch (        Exception e) {          LOG.error("Error when putting domain " + domain.getId(),e);          hasError=true;
  UserGroupInformation ugi=UserGroupInformation.getCurrentUser();  UserGroupInformation realUgi=ugi.getRealUser();  String doAsUser;  UserGroupInformation authUgi;  if (realUgi != null) {    authUgi=realUgi;    doAsUser=ugi.getShortUserName();  } else {    authUgi=ugi;    doAsUser=null;  }  DelegationTokenAuthenticatedURL.Token token=new DelegationTokenAuthenticatedURL.Token();  connector=new TimelineConnector(false,authUgi,doAsUser,token);  addIfService(connector);  String timelineReaderWebAppAddress=WebAppUtils.getTimelineReaderWebAppURLWithoutScheme(conf);  baseUri=TimelineConnector.constructResURI(conf,timelineReaderWebAppAddress,RESOURCE_URI_STR_V2);
@VisibleForTesting protected ClientResponse doGetUri(URI base,String path,MultivaluedMap<String,String> params) throws IOException {  ClientResponse resp=connector.getClient().resource(base).path(path).queryParams(params).accept(MediaType.APPLICATION_JSON).get(ClientResponse.class);  if (resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()) {    String msg="Response from the timeline reader server is " + ((resp == null) ? "null" : "not successful," + " HTTP error code: " + resp.getStatus() + ", Server response:\n"+ resp.getEntity(String.class));
    LOG.warn("Timeline token to be updated should be of kind " + TimelineDelegationTokenIdentifier.KIND_NAME);    return;  }  if (collectorAddr == null || collectorAddr.isEmpty()) {    collectorAddr=timelineServiceAddress;  }  String service=delegationToken.getService();  if ((service == null || service.isEmpty()) && (collectorAddr == null || collectorAddr.isEmpty())) {    LOG.warn("Timeline token does not have service and timeline service " + "address is not yet set. Not updating the token");    return;  }  if (currentTimelineToken != null && currentTimelineToken.equals(delegationToken)) {    return;  }  currentTimelineToken=delegationToken;  org.apache.hadoop.security.token.Token<TimelineDelegationTokenIdentifier> timelineToken=new org.apache.hadoop.security.token.Token<TimelineDelegationTokenIdentifier>(delegationToken.getIdentifier().array(),delegationToken.getPassword().array(),new Text(delegationToken.getKind()),service == null ? new Text() : new Text(service));  InetSocketAddress serviceAddr=(collectorAddr != null && !collectorAddr.isEmpty()) ? NetUtils.createSocketAddr(collectorAddr) : SecurityUtil.getTokenServiceAddr(timelineToken);  SecurityUtil.setTokenService(timelineToken,serviceAddr);  authUgi.addToken(timelineToken);
  try {    resp=authUgi.doAs(new PrivilegedExceptionAction<ClientResponse>(){      @Override public ClientResponse run() throws Exception {        return doPutObjects(base,path,params,obj);      }    });  } catch (  UndeclaredThrowableException ue) {    Throwable cause=ue.getCause();    if (cause instanceof IOException) {      throw (IOException)cause;    } else {      throw new IOException(cause);    }  }catch (  InterruptedException ie) {    throw (IOException)new InterruptedIOException().initCause(ie);  }  if (resp == null) {    String msg="Error getting HTTP response from the timeline server.";
    throw (IOException)new InterruptedIOException().initCause(ie);  }  if (resp == null) {    String msg="Error getting HTTP response from the timeline server.";    LOG.error(msg);    throw new YarnException(msg);  } else   if (resp.getStatusInfo().getStatusCode() == ClientResponse.Status.OK.getStatusCode()) {    try {      resp.close();    } catch (    ClientHandlerException che) {      LOG.warn("Error closing the HTTP response's inputstream. ",che);    }  } else {    String msg="";    try {      String stringType=resp.getEntity(String.class);      msg="Server response:\n" + stringType;
private int verifyRestEndPointAvailable() throws YarnException {  int retries=pollTimelineServiceAddress(this.maxServiceRetries);  if (timelineServiceAddress == null) {    String errMessage="TimelineClient has reached to max retry times : " + this.maxServiceRetries + ", but failed to fetch timeline service address. Please verify"+ " Timeline Auxiliary Service is configured in all the NMs";
  try {    resp=authUgi.doAs(new PrivilegedExceptionAction<ClientResponse>(){      @Override public ClientResponse run() throws Exception {        return doPostingObject(obj,path);      }    });  } catch (  UndeclaredThrowableException e) {    Throwable cause=e.getCause();    if (cause instanceof IOException) {      throw (IOException)cause;    } else {      throw new IOException(cause);    }  }catch (  InterruptedException ie) {    throw (IOException)new InterruptedIOException().initCause(ie);  }  if (resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()) {    String msg="Failed to get the response from the timeline server.";
        return doPostingObject(obj,path);      }    });  } catch (  UndeclaredThrowableException e) {    Throwable cause=e.getCause();    if (cause instanceof IOException) {      throw (IOException)cause;    } else {      throw new IOException(cause);    }  }catch (  InterruptedException ie) {    throw (IOException)new InterruptedIOException().initCause(ie);  }  if (resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()) {    String msg="Failed to get the response from the timeline server.";    LOG.error(msg);    if (resp != null) {      msg+=" HTTP error code: " + resp.getStatus();
@Private @VisibleForTesting public ClientResponse doPostingObject(Object object,String path){  WebResource webResource=client.resource(resURI);  if (path == null) {
        drained=eventQueue.isEmpty();        if (blockNewEvents) {synchronized (waitForDrained) {            if (drained) {              waitForDrained.notify();            }          }        }        Event event;        try {          event=eventQueue.take();        } catch (        InterruptedException ie) {          if (!stopped) {            LOG.warn("AsyncDispatcher thread interrupted",ie);          }          return;        }        if (event != null) {          dispatch(event);          if (printTrigger) {
@Override protected void serviceStop() throws Exception {  if (drainEventsOnStop) {    blockNewEvents=true;    LOG.info("AsyncDispatcher is draining to stop, ignoring any new events.");    long endTime=System.currentTimeMillis() + getConfig().getLong(YarnConfiguration.DISPATCHER_DRAIN_EVENTS_TIMEOUT,YarnConfiguration.DEFAULT_DISPATCHER_DRAIN_EVENTS_TIMEOUT);synchronized (waitForDrained) {      while (!isDrained() && eventHandlingThread != null && eventHandlingThread.isAlive() && System.currentTimeMillis() < endTime) {        waitForDrained.wait(100);
@SuppressWarnings("unchecked") protected void dispatch(Event event){
@SuppressWarnings("unchecked") @Override public void register(Class<? extends Enum> eventType,EventHandler handler){  EventHandler<Event> registeredHandler=(EventHandler<Event>)eventDispatchers.get(eventType);
@Override public void handle(T event){  try {    int qSize=eventQueue.size();    if (qSize != 0 && qSize % 1000 == 0) {
private Server createServer(Class<?> pbProtocol,InetSocketAddress addr,Configuration conf,SecretManager<? extends TokenIdentifier> secretManager,int numHandlers,BlockingService blockingService,String portRangeConfig) throws IOException {  RPC.setProtocolEngine(conf,pbProtocol,ProtobufRpcEngine2.class);  RPC.Server server=new RPC.Builder(conf).setProtocol(pbProtocol).setInstance(blockingService).setBindAddress(addr.getHostName()).setPort(addr.getPort()).setNumHandlers(numHandlers).setVerbose(false).setSecretManager(secretManager).setPortRangeConfig(portRangeConfig).build();
@Override public Object getProxy(Class protocol,InetSocketAddress addr,Configuration conf){
@Override public Server getServer(Class protocol,Object instance,InetSocketAddress addr,Configuration conf,SecretManager<? extends TokenIdentifier> secretManager,int numHandlers,String portRangeConfig){
  Path remoteRootLogDir=getRemoteRootLogDir();  try {    FsPermission perms=remoteFS.getFileStatus(remoteRootLogDir).getPermission();    if (!perms.equals(TLDIR_PERMISSIONS)) {      LOG.warn("Remote Root Log Dir [" + remoteRootLogDir + "] already exist, but with incorrect permissions. "+ "Expected: ["+ TLDIR_PERMISSIONS+ "], Found: ["+ perms+ "]."+ " The cluster may have problems with multiple users.");    }  } catch (  FileNotFoundException e) {    remoteExists=false;  }catch (  IOException e) {    throw new YarnRuntimeException("Failed to check permissions for dir [" + remoteRootLogDir + "]",e);  }  Path qualified=remoteRootLogDir.makeQualified(remoteFS.getUri(),remoteFS.getWorkingDirectory());  if (!remoteExists) {    LOG.warn("Remote Root Log Dir [" + remoteRootLogDir + "] does not exist. Attempting to create it.");    try {      remoteFS.mkdirs(qualified,new FsPermission(TLDIR_PERMISSIONS));      try {
    throw new YarnRuntimeException("Failed to check permissions for dir [" + remoteRootLogDir + "]",e);  }  Path qualified=remoteRootLogDir.makeQualified(remoteFS.getUri(),remoteFS.getWorkingDirectory());  if (!remoteExists) {    LOG.warn("Remote Root Log Dir [" + remoteRootLogDir + "] does not exist. Attempting to create it.");    try {      remoteFS.mkdirs(qualified,new FsPermission(TLDIR_PERMISSIONS));      try {        remoteFS.setPermission(qualified,new FsPermission(TLDIR_PERMISSIONS));      } catch (      UnsupportedOperationException use) {        LOG.info("Unable to set permissions for configured filesystem since" + " it does not support this",remoteFS.getScheme());        fsSupportsChmod=false;      }      UserGroupInformation loginUser=UserGroupInformation.getLoginUser();      String primaryGroupName=conf.get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR_GROUPNAME);      if (primaryGroupName == null || primaryGroupName.isEmpty()) {        try {
    try {      remoteFS.mkdirs(qualified,new FsPermission(TLDIR_PERMISSIONS));      try {        remoteFS.setPermission(qualified,new FsPermission(TLDIR_PERMISSIONS));      } catch (      UnsupportedOperationException use) {        LOG.info("Unable to set permissions for configured filesystem since" + " it does not support this",remoteFS.getScheme());        fsSupportsChmod=false;      }      UserGroupInformation loginUser=UserGroupInformation.getLoginUser();      String primaryGroupName=conf.get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR_GROUPNAME);      if (primaryGroupName == null || primaryGroupName.isEmpty()) {        try {          primaryGroupName=loginUser.getPrimaryGroupName();        } catch (        IOException e) {          LOG.warn("No primary group found. The remote root log directory" + " will be created with the HDFS superuser being its " + "group owner. JobHistoryServer may be unable to read "+ "the directory.");        }      } else {
        fsSupportsChmod=false;      }      UserGroupInformation loginUser=UserGroupInformation.getLoginUser();      String primaryGroupName=conf.get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR_GROUPNAME);      if (primaryGroupName == null || primaryGroupName.isEmpty()) {        try {          primaryGroupName=loginUser.getPrimaryGroupName();        } catch (        IOException e) {          LOG.warn("No primary group found. The remote root log directory" + " will be created with the HDFS superuser being its " + "group owner. JobHistoryServer may be unable to read "+ "the directory.");        }      } else {        if (LOG.isDebugEnabled()) {          LOG.debug("The group of remote root log directory has been " + "determined by the configuration and set to " + primaryGroupName);        }      }      if (primaryGroupName != null) {        try {          remoteFS.setOwner(qualified,loginUser.getShortUserName(),primaryGroupName);        } catch (        UnsupportedOperationException use) {
        try {          FileSystem remoteFS=getFileSystem(conf);          Path appDir=LogAggregationUtils.getRemoteAppLogDir(remoteRootLogDir,appId,user,remoteRootLogDirSuffix);          Path curDir=appDir.makeQualified(remoteFS.getUri(),remoteFS.getWorkingDirectory());          Path rootLogDir=remoteRootLogDir.makeQualified(remoteFS.getUri(),remoteFS.getWorkingDirectory());          LinkedList<Path> pathsToCreate=new LinkedList<>();          while (!curDir.equals(rootLogDir)) {            if (!checkExists(remoteFS,curDir,APP_DIR_PERMISSIONS)) {              pathsToCreate.addFirst(curDir);              curDir=curDir.getParent();            } else {              break;            }          }          for (          Path path : pathsToCreate) {            createDir(remoteFS,path,APP_DIR_PERMISSIONS);          }        } catch (        IOException e) {
    if (status.size() >= this.retentionSize) {      List<FileStatus> statusList=new ArrayList<FileStatus>(status);      Collections.sort(statusList,new Comparator<FileStatus>(){        public int compare(        FileStatus s1,        FileStatus s2){          return s1.getModificationTime() < s2.getModificationTime() ? -1 : s1.getModificationTime() > s2.getModificationTime() ? 1 : 0;        }      });      for (int i=0; i <= statusList.size() - this.retentionSize; i++) {        final FileStatus remove=statusList.get(i);        try {          userUgi.doAs(new PrivilegedExceptionAction<Object>(){            @Override public Object run() throws Exception {              remoteFS.delete(remove.getPath(),false);              return null;            }          });        } catch (        Exception e) {
      Collections.sort(statusList,new Comparator<FileStatus>(){        public int compare(        FileStatus s1,        FileStatus s2){          return s1.getModificationTime() < s2.getModificationTime() ? -1 : s1.getModificationTime() > s2.getModificationTime() ? 1 : 0;        }      });      for (int i=0; i <= statusList.size() - this.retentionSize; i++) {        final FileStatus remove=statusList.get(i);        try {          userUgi.doAs(new PrivilegedExceptionAction<Object>(){            @Override public Object run() throws Exception {              remoteFS.delete(remove.getPath(),false);              return null;            }          });        } catch (        Exception e) {          LOG.error("Failed to delete " + remove.getPath(),e);        }      }    }  } catch (  Exception e) {
    return;  }  ApplicationId appId=params.getAppId();  ContainerId containerId=params.getContainerId();  NodeId nodeId=params.getNodeId();  String appOwner=params.getAppOwner();  String logEntity=params.getLogEntity();  long start=params.getStartIndex();  long end=params.getEndIndex();  long startTime=params.getStartTime();  long endTime=params.getEndTime();  List<FileStatus> nodeFiles=null;  try {    nodeFiles=LogAggregationUtils.getRemoteNodeFileList(conf,appId,appOwner,this.fileController.getRemoteRootLogDir(),this.fileController.getRemoteRootLogDirSuffix());  } catch (  Exception ex) {    html.h1("Unable to locate any logs for container " + containerId.toString());
  String logEntity=params.getLogEntity();  long start=params.getStartIndex();  long end=params.getEndIndex();  long startTime=params.getStartTime();  long endTime=params.getEndTime();  List<FileStatus> nodeFiles=null;  try {    nodeFiles=LogAggregationUtils.getRemoteNodeFileList(conf,appId,appOwner,this.fileController.getRemoteRootLogDir(),this.fileController.getRemoteRootLogDirSuffix());  } catch (  Exception ex) {    html.h1("Unable to locate any logs for container " + containerId.toString());    LOG.error(ex.getMessage());    return;  }  Map<String,Long> checkSumFiles;  try {    checkSumFiles=fileController.parseCheckSumFiles(nodeFiles);
    nodeFiles=LogAggregationUtils.getRemoteNodeFileList(conf,appId,appOwner,this.fileController.getRemoteRootLogDir(),this.fileController.getRemoteRootLogDirSuffix());  } catch (  Exception ex) {    html.h1("Unable to locate any logs for container " + containerId.toString());    LOG.error(ex.getMessage());    return;  }  Map<String,Long> checkSumFiles;  try {    checkSumFiles=fileController.parseCheckSumFiles(nodeFiles);  } catch (  IOException ex) {    LOG.error("Error getting logs for " + logEntity,ex);    html.h1("Error getting logs for " + logEntity);    return;  }  List<FileStatus> fileToRead;  try {    fileToRead=fileController.getNodeLogFileToRead(nodeFiles,nodeId.toString(),appId);
      if (!checkAcls(conf,appId,user,appAcls,remoteUser)) {        html.h1().__("User [" + remoteUser + "] is not authorized to view the logs for "+ logEntity+ " in log file ["+ thisNodeFile.getPath().getName()+ "]").__();        LOG.error("User [" + remoteUser + "] is not authorized to view the logs for "+ logEntity);        continue;      }      String compressAlgo=indexedLogsMeta.getCompressName();      List<IndexedFileLogMeta> candidates=new ArrayList<>();      for (      IndexedPerAggregationLogMeta logMeta : indexedLogsMeta.getLogMetas()) {        for (        Entry<String,List<IndexedFileLogMeta>> meta : logMeta.getLogMetas().entrySet()) {          for (          IndexedFileLogMeta log : meta.getValue()) {            if (!log.getContainerId().equals(containerId.toString())) {              continue;            }            if (desiredLogType != null && !desiredLogType.isEmpty() && !desiredLogType.equals(log.getFileName())) {              continue;            }            candidates.add(log);          }        }      }      if (candidates.isEmpty()) {
      if (candidate.getLastModifiedTime() < startTime || candidate.getLastModifiedTime() > endTime) {        continue;      }      byte[] cbuf=new byte[bufferSize];      InputStream in=null;      try {        in=compressName.createDecompressionStream(new BoundedRangeFileInputStream(fsin,candidate.getStartIndex(),candidate.getFileCompressedSize()),decompressor,LogAggregationIndexedFileController.getFSInputBufferSize(conf));        long logLength=candidate.getFileSize();        html.pre().__("\n\n").__();        html.p().__("Log Type: " + candidate.getFileName()).__();        html.p().__("Log Upload Time: " + Times.format(candidate.getLastModifiedTime())).__();        html.p().__("Log Length: " + Long.toString(logLength)).__();        long[] range=checkParseRange(html,start,end,startTime,endTime,logLength,candidate.getFileName());        processContainerLog(html,range,in,bufferSize,cbuf);        foundLog=true;      } catch (      Exception ex) {
  try {    fsDataIStream=fileContext.open(remoteLogPath);    if (end == 0) {      return null;    }    long fileLength=end < 0 ? fileContext.getFileStatus(remoteLogPath).getLen() : end;    fsDataIStream.seek(fileLength - Integer.SIZE / Byte.SIZE - UUID_LENGTH);    int offset=fsDataIStream.readInt();    if (offset > 64 * 1024 * 1024) {      LOG.warn("The log meta size read from " + remoteLogPath + " is "+ offset);    }    byte[] uuidRead=new byte[UUID_LENGTH];    int uuidReadLen=fsDataIStream.read(uuidRead);    if (this.uuid == null) {      this.uuid=createUUID(appId);    }    if (uuidReadLen != UUID_LENGTH || !Arrays.equals(this.uuid,uuidRead)) {      if (LOG.isDebugEnabled()) {
private static String logErrorMessage(File logFile,Exception e){  String message="Error aggregating log file. Log file : " + logFile.getAbsolutePath() + ". "+ e.getMessage();
    record.increcleanupOldLogTimes();  }  closeWriter();  final Path renamedPath=record.getRollingMonitorInterval() <= 0 ? record.getRemoteNodeLogFileForApp() : new Path(record.getRemoteNodeLogFileForApp().getParent(),record.getRemoteNodeLogFileForApp().getName() + "_" + record.getLogUploadTimeStamp());  final boolean rename=record.isUploadedLogsInThisCycle();  try {    record.getUserUgi().doAs(new PrivilegedExceptionAction<Object>(){      @Override public Object run() throws Exception {        FileSystem remoteFS=record.getRemoteNodeLogFileForApp().getFileSystem(conf);        if (rename) {          remoteFS.rename(record.getRemoteNodeTmpLogFileForApp(),renamedPath);        } else {          remoteFS.delete(record.getRemoteNodeTmpLogFileForApp(),false);        }        return null;      }    });  } catch (  Exception e) {
          nodeFiles=HarFs.get(p.toUri(),conf).listStatusIterator(p);          continue;        }        if (!thisNodeFile.getPath().getName().contains(LogAggregationUtils.getNodeString(nodeId)) || thisNodeFile.getPath().getName().endsWith(LogAggregationUtils.TMP_FILE_SUFFIX)) {          continue;        }        long logUploadedTime=thisNodeFile.getModificationTime();        if (logUploadedTime < startTime || logUploadedTime > endTime) {          continue;        }        reader=new AggregatedLogFormat.LogReader(conf,thisNodeFile.getPath());        String owner=null;        Map<ApplicationAccessType,String> appAcls=null;        try {          owner=reader.getApplicationOwner();          appAcls=reader.getApplicationAcls();        } catch (        IOException e) {          LOG.error("Error getting logs for " + logEntity,e);
        if (logUploadedTime < startTime || logUploadedTime > endTime) {          continue;        }        reader=new AggregatedLogFormat.LogReader(conf,thisNodeFile.getPath());        String owner=null;        Map<ApplicationAccessType,String> appAcls=null;        try {          owner=reader.getApplicationOwner();          appAcls=reader.getApplicationAcls();        } catch (        IOException e) {          LOG.error("Error getting logs for " + logEntity,e);          continue;        }        String remoteUser=request().getRemoteUser();        if (!checkAcls(conf,appId,owner,appAcls,remoteUser)) {          html.h1().__("User [" + remoteUser + "] is not authorized to view the logs for "+ logEntity+ " in log file ["+ thisNodeFile.getPath().getName()+ "]").__();          LOG.error("User [" + remoteUser + "] is not authorized to view the logs for "+ logEntity);
          continue;        }        String remoteUser=request().getRemoteUser();        if (!checkAcls(conf,appId,owner,appAcls,remoteUser)) {          html.h1().__("User [" + remoteUser + "] is not authorized to view the logs for "+ logEntity+ " in log file ["+ thisNodeFile.getPath().getName()+ "]").__();          LOG.error("User [" + remoteUser + "] is not authorized to view the logs for "+ logEntity);          continue;        }        AggregatedLogFormat.ContainerLogsReader logReader=reader.getContainerLogsReader(containerId);        if (logReader == null) {          continue;        }        foundLog=readContainerLogs(html,logReader,start,end,desiredLogType,logUploadedTime,startTime,endTime);      } catch (      IOException ex) {        LOG.error("Error getting logs for " + logEntity,ex);        continue;      } finally {        if (reader != null) {
  if (null == addedLabelsToNode || addedLabelsToNode.isEmpty()) {    return;  }  Set<String> knownLabels=labelCollections.keySet();  for (  Entry<NodeId,Set<String>> entry : addedLabelsToNode.entrySet()) {    NodeId nodeId=entry.getKey();    Set<String> labels=entry.getValue();    if (!knownLabels.containsAll(labels)) {      String msg="Not all labels being added contained by known " + "label collections, please check" + ", added labels=[" + StringUtils.join(labels,",") + "]";      LOG.error(msg);      throw new IOException(msg);    }    if (!labels.isEmpty()) {      Set<String> newLabels=new HashSet<String>(getLabelsByNode(nodeId));      newLabels.addAll(labels);      if (newLabels.size() > 1) {        String msg=String.format("%d labels specified on host=%s after add labels to node" + ", please note that we do not support specifying multiple" + " labels on a single host for now.",newLabels.size(),nodeId.getHost());
protected void checkRemoveLabelsFromNode(Map<NodeId,Set<String>> removeLabelsFromNode) throws IOException {  Set<String> knownLabels=labelCollections.keySet();  for (  Entry<NodeId,Set<String>> entry : removeLabelsFromNode.entrySet()) {    NodeId nodeId=entry.getKey();    Set<String> labels=entry.getValue();    if (!knownLabels.containsAll(labels)) {      String msg="Not all labels being removed contained by known " + "label collections, please check" + ", removed labels=[" + StringUtils.join(labels,",") + "]";
      throw new IOException(msg);    }    Set<String> originalLabels=null;    boolean nodeExisted=false;    if (WILDCARD_PORT != nodeId.getPort()) {      Node nm=getNMInNodeSet(nodeId);      if (nm != null) {        originalLabels=nm.labels;        nodeExisted=true;      }    } else {      Host host=nodeCollections.get(nodeId.getHost());      if (null != host) {        originalLabels=host.labels;        nodeExisted=true;      }    }    if (!nodeExisted) {      String msg="Try to remove labels from NM=" + nodeId + ", but the NM doesn't existed";
      if (nm != null) {        originalLabels=nm.labels;        nodeExisted=true;      }    } else {      Host host=nodeCollections.get(nodeId.getHost());      if (null != host) {        originalLabels=host.labels;        nodeExisted=true;      }    }    if (!nodeExisted) {      String msg="Try to remove labels from NM=" + nodeId + ", but the NM doesn't existed";      LOG.error(msg);      throw new IOException(msg);    }    if (labels.isEmpty()) {      continue;    }    if (originalLabels == null || !originalLabels.containsAll(labels)) {
protected void initStore(Configuration conf,Path fsStorePath,StoreSchema schma,M mgr) throws IOException {  this.schema=schma;  this.fsWorkingPath=fsStorePath;  this.manager=mgr;  initFileSystem(conf);  fs.mkdirs(fsWorkingPath);
  loadFromMirror(mirrorPath,oldMirrorPath);  editLogPath=new Path(fsWorkingPath,schema.editLogName);  loadManagerFromEditLog(editLogPath);  Path writingMirrorPath=new Path(fsWorkingPath,schema.mirrorName + ".writing");  try (FSDataOutputStream os=fs.create(writingMirrorPath,true)){    StoreOp op=FSStoreOpHandler.getMirrorOp(storeType);    op.write(os,manager);  }   if (fs.exists(mirrorPath)) {    fs.delete(oldMirrorPath,false);    fs.rename(mirrorPath,oldMirrorPath);  }  fs.rename(writingMirrorPath,mirrorPath);  fs.delete(writingMirrorPath,false);  fs.delete(oldMirrorPath,false);  editlogOs=fs.create(editLogPath,true);  editlogOs.close();
  editLogPath=new Path(fsWorkingPath,schema.editLogName);  loadManagerFromEditLog(editLogPath);  Path writingMirrorPath=new Path(fsWorkingPath,schema.mirrorName + ".writing");  try (FSDataOutputStream os=fs.create(writingMirrorPath,true)){    StoreOp op=FSStoreOpHandler.getMirrorOp(storeType);    op.write(os,manager);  }   if (fs.exists(mirrorPath)) {    fs.delete(oldMirrorPath,false);    fs.rename(mirrorPath,oldMirrorPath);  }  fs.rename(writingMirrorPath,mirrorPath);  fs.delete(writingMirrorPath,false);  fs.delete(oldMirrorPath,false);  editlogOs=fs.create(editLogPath,true);  editlogOs.close();  LOG.info("Finished write mirror at:" + mirrorPath.toString());
public static YarnAuthorizationProvider getInstance(Configuration conf){synchronized (YarnAuthorizationProvider.class) {    if (authorizer == null) {      Class<?> authorizerClass=conf.getClass(YarnConfiguration.YARN_AUTHORIZATION_PROVIDER,ConfiguredYarnAuthorizer.class);      authorizer=(YarnAuthorizationProvider)ReflectionUtils.newInstance(authorizerClass,conf);      authorizer.init(conf);
public boolean checkAccess(UserGroupInformation callerUGI,ApplicationAccessType applicationAccessType,String applicationOwner,ApplicationId applicationId){
    FileAppender fApp;    File file=new File(System.getProperty("yarn.log.dir"),targetFilename);    try {      fApp=new FileAppender(layout,file.getAbsolutePath(),false);    } catch (    IOException ie) {      LOG.warn("Error creating file, can't dump logs to " + file.getAbsolutePath(),ie);      throw ie;    }    fApp.setName(AdHocLogDumper.AD_HOC_DUMPER_APPENDER);    fApp.setThreshold(targetLevel);    for (Enumeration appenders=Logger.getRootLogger().getAllAppenders(); appenders.hasMoreElements(); ) {      Object obj=appenders.nextElement();      if (obj instanceof AppenderSkeleton) {        AppenderSkeleton appender=(AppenderSkeleton)obj;        appenderLevels.put(appender.getName(),appender.getThreshold());        appender.setThreshold(currentEffectiveLevel);
    }  }  if (contents == null) {    throw new IOException("Failed to read Docker client configuration: " + configFile);  }  ObjectMapper mapper=new ObjectMapper();  JsonFactory factory=mapper.getJsonFactory();  JsonParser parser=factory.createJsonParser(contents);  JsonNode rootNode=mapper.readTree(parser);  Credentials credentials=new Credentials();  if (rootNode.has(CONFIG_AUTHS_KEY)) {    Iterator<String> iter=rootNode.get(CONFIG_AUTHS_KEY).getFieldNames();    for (; iter.hasNext(); ) {      String registryUrl=iter.next();      String registryCred=rootNode.get(CONFIG_AUTHS_KEY).get(registryUrl).get(CONFIG_AUTH_KEY).asText();      TokenIdentifier tokenId=new DockerCredentialTokenIdentifier(registryUrl,applicationId);      Token<DockerCredentialTokenIdentifier> token=new Token<>(tokenId.getBytes(),registryCred.getBytes(Charset.forName("UTF-8")),tokenId.getKind(),new Text(registryUrl));      credentials.addToken(new Text(registryUrl + "-" + applicationId),token);
public static Credentials getCredentialsFromTokensByteBuffer(ByteBuffer tokens) throws IOException {  Credentials credentials=new Credentials();  DataInputByteBuffer dibb=new DataInputByteBuffer();  tokens.rewind();  dibb.reset(tokens);  credentials.readTokenStorageStream(dibb);  tokens.rewind();  if (LOG.isDebugEnabled()) {    for (    Token token : credentials.getAllTokens()) {
public static boolean writeDockerCredentialsToPath(File outConfigFile,Credentials credentials) throws IOException {  boolean foundDockerCred=false;  if (credentials.numberOfTokens() > 0) {    ObjectMapper mapper=new ObjectMapper();    ObjectNode rootNode=mapper.createObjectNode();    ObjectNode registryUrlNode=mapper.createObjectNode();    for (    Token<? extends TokenIdentifier> tk : credentials.getAllTokens()) {      if (tk.getKind().equals(DockerCredentialTokenIdentifier.KIND)) {        foundDockerCred=true;        DockerCredentialTokenIdentifier ti=(DockerCredentialTokenIdentifier)tk.decodeIdentifier();        ObjectNode registryCredNode=mapper.createObjectNode();        registryUrlNode.put(ti.getRegistryUrl(),registryCredNode);        registryCredNode.put(CONFIG_AUTH_KEY,new String(tk.getPassword(),Charset.forName("UTF-8")));
    throw new IOException("Invalid resource",e);  }  LOG.debug("Starting to download {} {} {}",sCopy,resource.getType(),resource.getPattern());  final Path destinationTmp=new Path(destDirPath + "_tmp");  createDir(destinationTmp,cachePerms);  Path dFinal=files.makeQualified(new Path(destinationTmp,sCopy.getName()));  try {    if (userUgi == null) {      verifyAndCopy(dFinal);    } else {      userUgi.doAs(new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          verifyAndCopy(dFinal);          return null;        }      });    }    changePermissions(dFinal.getFileSystem(conf),dFinal);
@Override public float getCpuUsagePercent(){  BigInteger processTotalJiffies=getTotalProcessJiffies();
    in=new BufferedReader(fReader);    ProcessSmapMemoryInfo memoryMappingInfo=null;    List<String> lines=IOUtils.readLines(in);    for (    String line : lines) {      line=line.trim();      try {        Matcher address=ADDRESS_PATTERN.matcher(line);        if (address.find()) {          memoryMappingInfo=new ProcessSmapMemoryInfo(line);          memoryMappingInfo.setPermission(address.group(4));          pInfo.getMemoryInfoList().add(memoryMappingInfo);          continue;        }        Matcher memInfo=MEM_INFO_PATTERN.matcher(line);        if (memInfo.find()) {          String key=memInfo.group(1).trim();
          pInfo.getMemoryInfoList().add(memoryMappingInfo);          continue;        }        Matcher memInfo=MEM_INFO_PATTERN.matcher(line);        if (memInfo.find()) {          String key=memInfo.group(1).trim();          String value=memInfo.group(2).replace(KB,"").trim();          LOG.debug("MemInfo : {} : Value  : {}",key,value);          if (memoryMappingInfo != null) {            memoryMappingInfo.setMemInfo(key,value);          }        }      } catch (      Throwable t) {        LOG.warn("Error parsing smaps line : " + line + "; "+ t.getMessage());      }    }  } catch (  FileNotFoundException f) {    LOG.error(f.toString());  }catch (  IOException e) {    LOG.error(e.toString());
  String[] processesStr=processesInfoStr.split("\r\n");  Map<String,ProcessInfo> allProcs=new HashMap<String,ProcessInfo>();  final int procInfoSplitCount=4;  for (  String processStr : processesStr) {    if (processStr != null) {      String[] procInfo=processStr.split(",");      if (procInfo.length == procInfoSplitCount) {        try {          ProcessInfo pInfo=new ProcessInfo();          pInfo.pid=procInfo[0];          pInfo.vmem=Long.parseLong(procInfo[1]);          pInfo.workingSet=Long.parseLong(procInfo[2]);          pInfo.cpuTimeMs=Long.parseLong(procInfo[3]);          allProcs.put(pInfo.pid,pInfo);        } catch (        NumberFormatException nfe) {
  final int procInfoSplitCount=4;  for (  String processStr : processesStr) {    if (processStr != null) {      String[] procInfo=processStr.split(",");      if (procInfo.length == procInfoSplitCount) {        try {          ProcessInfo pInfo=new ProcessInfo();          pInfo.pid=procInfo[0];          pInfo.vmem=Long.parseLong(procInfo[1]);          pInfo.workingSet=Long.parseLong(procInfo[2]);          pInfo.cpuTimeMs=Long.parseLong(procInfo[3]);          allProcs.put(pInfo.pid,pInfo);        } catch (        NumberFormatException nfe) {          LOG.debug("Error parsing procInfo.",nfe);        }      } else {
@Override public Resource normalize(Resource r,Resource minimumResource,Resource maximumResource,Resource stepFactor){  if (stepFactor.getMemorySize() == 0) {
  double[] rhsShares=new double[maxLength];  double diff;  try {    if (singleType) {      double[] max=new double[2];      calculateShares(clusterRes,lhs,rhs,lhsShares,rhsShares,max);      diff=max[0] - max[1];    } else     if (maxLength == 2) {      diff=calculateSharesForTwoMandatoryResources(clusterRes,lhs,rhs,lhsShares,rhsShares);    } else {      calculateShares(clusterRes,lhs,rhs,lhsShares,rhsShares);      Arrays.sort(lhsShares);      Arrays.sort(rhsShares);      diff=compareShares(lhsShares,rhsShares);    }  } catch (  ArrayIndexOutOfBoundsException ex) {
protected void renderJSON(Object object){
protected void renderText(String s){
    }  }  rc.prefix=webApp.name();  Router.Dest dest=null;  try {    dest=router.resolve(method,pathInfo);  } catch (  WebAppException e) {    rc.error=e;    if (!e.getMessage().contains("not found")) {      rc.setStatus(res.SC_INTERNAL_SERVER_ERROR);      render(ErrorPage.class);      return;    }  }  if (dest == null) {    rc.setStatus(res.SC_NOT_FOUND);    render(ErrorPage.class);    return;  }  rc.devMode=devMode;
public static void removeCookie(HttpServletResponse res,String name,String path){
private Dest lookupRoute(WebApp.HTTP method,String path){  String key=path;  do {    Dest dest=routes.get(key);    if (dest != null && methodAllowed(method,dest)) {      if ((Object)key == path) {
    if (dest != null && methodAllowed(method,dest)) {      if ((Object)key == path) {        LOG.debug("exact match for {}: {}",key,dest.action);        return dest;      } else       if (isGoodMatch(dest,path)) {        LOG.debug("prefix match2 for {}: {}",key,dest.action);        return dest;      }      return resolveAction(method,dest,path);    }    Map.Entry<String,Dest> lower=routes.lowerEntry(key);    if (lower == null) {      return null;    }    dest=lower.getValue();    if (prefixMatches(dest,path)) {      if (methodAllowed(method,dest)) {        if (isGoodMatch(dest,path)) {
@SuppressWarnings("unchecked") private <T>Class<? extends T> load(Class<T> cls,String className){
@SuppressWarnings("unchecked") private <T>Class<? extends T> load(Class<T> cls,String className){  LOG.debug("trying: {}",className);  try {    Class<?> found=Class.forName(className);    if (cls.isAssignableFrom(found)) {
  LOG.info("Generating {} using {} and {}",new Object[]{outputName,specClass,implClass});  out=new PrintWriter(outputName + ".java","UTF-8");  hamlet=basename(outputName);  String pkg=pkgName(outputPkg,implClass.getPackage().getName());  puts(0,"// Generated by HamletGen. Do NOT edit!\n","package ",pkg,";\n","import java.io.PrintWriter;\n","import java.util.EnumSet;\n","import static java.util.EnumSet.*;\n","import static ",implClass.getName(),".EOpt.*;\n","import org.apache.hadoop.yarn.webapp.SubView;");  String implClassName=implClass.getSimpleName();  if (!implClass.getPackage().getName().equals(pkg)) {    puts(0,"import ",implClass.getName(),';');  }  puts(0,"\n","public class ",hamlet," extends ",implClassName," implements ",specClass.getSimpleName(),"._Html {\n","  public ",hamlet,"(PrintWriter out, int nestLevel,"," boolean wasInline) {\n","    super(out, nestLevel, wasInline);\n","  }\n\n","  static EnumSet<EOpt> opt(boolean endTag, boolean inline, ","boolean pre) {\n","    EnumSet<EOpt> opts = of(ENDTAG);\n","    if (!endTag) opts.remove(ENDTAG);\n","    if (inline) opts.add(INLINE);\n","    if (pre) opts.add(PRE);\n","    return opts;\n","  }");  initLut(specClass);  genImpl(specClass,implClassName,1);  LOG.info("Generating {} methods",hamlet);  genMethods(hamlet,top,1);  puts(0,"}");  out.close();
  LOG.info("Generating {} using {} and {}",new Object[]{outputName,specClass,implClass});  out=new PrintWriter(outputName + ".java","UTF-8");  hamlet=basename(outputName);  String pkg=pkgName(outputPkg,implClass.getPackage().getName());  puts(0,"// Generated by HamletGen. Do NOT edit!\n","package ",pkg,";\n","import java.io.PrintWriter;\n","import java.util.EnumSet;\n","import static java.util.EnumSet.*;\n","import static ",implClass.getName(),".EOpt.*;\n","import org.apache.hadoop.yarn.webapp.SubView;");  String implClassName=implClass.getSimpleName();  if (!implClass.getPackage().getName().equals(pkg)) {    puts(0,"import ",implClass.getName(),';');  }  puts(0,"\n","public class ",hamlet," extends ",implClassName," implements ",specClass.getSimpleName(),"._Html {\n","  public ",hamlet,"(PrintWriter out, int nestLevel,"," boolean wasInline) {\n","    super(out, nestLevel, wasInline);\n","  }\n\n","  static EnumSet<EOpt> opt(boolean endTag, boolean inline, ","boolean pre) {\n","    EnumSet<EOpt> opts = of(ENDTAG);\n","    if (!endTag) opts.remove(ENDTAG);\n","    if (inline) opts.add(INLINE);\n","    if (pre) opts.add(PRE);\n","    return opts;\n","  }");  initLut(specClass);  genImpl(specClass,implClassName,1);  LOG.info("Generating {} methods",hamlet);  genMethods(hamlet,top,1);  puts(0,"}");  out.close();
@Override public void render(){  int nestLevel=context().nestLevel();
    ContainerLaunchContext containerLaunchContext=recordFactory.newRecordInstance(ContainerLaunchContext.class);    ApplicationId applicationId=ApplicationId.newInstance(0,0);    ApplicationAttemptId applicationAttemptId=ApplicationAttemptId.newInstance(applicationId,0);    ContainerId containerId=ContainerId.newContainerId(applicationAttemptId,100);    NodeId nodeId=NodeId.newInstance("localhost",1234);    Resource resource=Resource.newInstance(1234,2);    ContainerTokenIdentifier containerTokenIdentifier=new ContainerTokenIdentifier(containerId,"localhost","user",resource,System.currentTimeMillis() + 10000,42,42,Priority.newInstance(0),0);    Token containerToken=newContainerToken(nodeId,"password".getBytes(),containerTokenIdentifier);    StartContainerRequest scRequest=StartContainerRequest.newInstance(containerLaunchContext,containerToken);    List<StartContainerRequest> list=new ArrayList<StartContainerRequest>();    list.add(scRequest);    StartContainersRequest allRequests=StartContainersRequest.newInstance(list);    try {      proxy.startContainers(allRequests);    } catch (    Exception e) {
  try {    ContainerManagementProtocol proxy=(ContainerManagementProtocol)rpc.getProxy(ContainerManagementProtocol.class,server.getListenerAddress(),conf);    ApplicationId applicationId=ApplicationId.newInstance(0,0);    ApplicationAttemptId applicationAttemptId=ApplicationAttemptId.newInstance(applicationId,0);    ContainerId containerId=ContainerId.newContainerId(applicationAttemptId,100);    NodeId nodeId=NodeId.newInstance("localhost",1234);    Resource resource=Resource.newInstance(1234,2);    ContainerTokenIdentifier containerTokenIdentifier=new ContainerTokenIdentifier(containerId,"localhost","user",resource,System.currentTimeMillis() + 10000,42,42,Priority.newInstance(0),0);    Token containerToken=newContainerToken(nodeId,"password".getBytes(),containerTokenIdentifier);    List<Token> increaseTokens=new ArrayList<>();    increaseTokens.add(containerToken);    ContainerUpdateRequest request=ContainerUpdateRequest.newInstance(increaseTokens);    try {      proxy.updateContainer(request);    } catch (    Exception e) {
      }    }  }  for (int i=0; i < methods.length; i++) {    Method m=methods[i];    int mod=m.getModifiers();    if (m.getDeclaringClass().equals(recordClass) && Modifier.isPublic(mod) && (!Modifier.isStatic(mod))) {      String name=m.getName();      if (name.startsWith("set") && (m.getParameterTypes().length == 1)) {        String propertyName=name.substring(3);        Type valueType=m.getGenericParameterTypes()[0];        GetSetPair p=ret.get(propertyName);        if (p != null && p.type.equals(valueType)) {          p.setMethod=m;        }      }    }  }  Iterator<Map.Entry<String,GetSetPair>> itr=ret.entrySet().iterator();  while (itr.hasNext()) {    Map.Entry<String,GetSetPair> cur=itr.next();    GetSetPair gsp=cur.getValue();
  for (int i=0; i < methods.length; i++) {    Method m=methods[i];    int mod=m.getModifiers();    if (m.getDeclaringClass().equals(recordClass) && Modifier.isPublic(mod) && (!Modifier.isStatic(mod))) {      String name=m.getName();      if (name.startsWith("set") && (m.getParameterTypes().length == 1)) {        String propertyName=name.substring(3);        Type valueType=m.getGenericParameterTypes()[0];        GetSetPair p=ret.get(propertyName);        if (p != null && p.type.equals(valueType)) {          p.setMethod=m;        }      }    }  }  Iterator<Map.Entry<String,GetSetPair>> itr=ret.entrySet().iterator();  while (itr.hasNext()) {    Map.Entry<String,GetSetPair> cur=itr.next();    GetSetPair gsp=cur.getValue();
    int mod=m.getModifiers();    if (m.getDeclaringClass().equals(recordClass) && Modifier.isPublic(mod) && (!Modifier.isStatic(mod))) {      String name=m.getName();      if (name.startsWith("set") && (m.getParameterTypes().length == 1)) {        String propertyName=name.substring(3);        Type valueType=m.getGenericParameterTypes()[0];        GetSetPair p=ret.get(propertyName);        if (p != null && p.type.equals(valueType)) {          p.setMethod=m;        }      }    }  }  Iterator<Map.Entry<String,GetSetPair>> itr=ret.entrySet().iterator();  while (itr.hasNext()) {    Map.Entry<String,GetSetPair> cur=itr.next();    GetSetPair gsp=cur.getValue();    if ((gsp.getMethod == null) || (gsp.setMethod == null)) {      LOG.info(String.format("Exclude potential property: %s\n",gsp.propertyName));
protected <R,P>void validatePBImplRecord(Class<R> recordClass,Class<P> protoClass) throws Exception {
      TimelineEvent event=new TimelineEvent();      event.setTimestamp(System.currentTimeMillis());      event.setEventType("event type " + i);      event.addEventInfo("key1","val1");      event.addEventInfo("key2","val2");      entity.addEvent(event);    }    entity.addRelatedEntity("test ref type 1","test ref id 1");    entity.addRelatedEntity("test ref type 2","test ref id 2");    entity.addPrimaryFilter("pkey1","pval1");    entity.addPrimaryFilter("pkey2","pval2");    entity.addOtherInfo("okey1","oval1");    entity.addOtherInfo("okey2","oval2");    entity.setDomainId("domain id " + j);    entities.addEntity(entity);  }  LOG.info("Entities in JSON:");
@Test public void testEvents() throws Exception {  TimelineEvents events=new TimelineEvents();  for (int j=0; j < 2; ++j) {    TimelineEvents.EventsOfOneEntity partEvents=new TimelineEvents.EventsOfOneEntity();    partEvents.setEntityId("entity id " + j);    partEvents.setEntityType("entity type " + j);    for (int i=0; i < 2; ++i) {      TimelineEvent event=new TimelineEvent();      event.setTimestamp(System.currentTimeMillis());      event.setEventType("event type " + i);      event.addEventInfo("key1","val1");      event.addEventInfo("key2","val2");      partEvents.addEvent(event);    }    events.addEvent(partEvents);  }  LOG.info("Events in JSON:");
  TimelinePutResponse TimelinePutErrors=new TimelinePutResponse();  TimelinePutError error1=new TimelinePutError();  error1.setEntityId("entity id 1");  error1.setEntityId("entity type 1");  error1.setErrorCode(TimelinePutError.NO_START_TIME);  TimelinePutErrors.addError(error1);  List<TimelinePutError> response=new ArrayList<TimelinePutError>();  response.add(error1);  TimelinePutError error2=new TimelinePutError();  error2.setEntityId("entity id 2");  error2.setEntityId("entity type 2");  error2.setErrorCode(TimelinePutError.IO_EXCEPTION);  response.add(error2);  TimelinePutErrors.addErrors(response);  LOG.info("Errors in JSON:");
  TimelineEvent event2=new TimelineEvent();  event2.setId("test event id 2");  event2.addInfo("test info key 1","test info value 1");  event2.addInfo("test info key 2",Arrays.asList("test info value 2","test info value 3"));  event2.addInfo("test info key 3",true);  Assert.assertTrue(event2.getInfo().get("test info key 3") instanceof Boolean);  event2.setTimestamp(2L);  entity.addEvent(event2);  Assert.assertFalse("event1 should not equal to event2! ",event1.equals(event2));  TimelineEvent event3=new TimelineEvent();  event3.setId("test event id 1");  event3.setTimestamp(1L);  Assert.assertEquals("event1 should equal to event3! ",event3,event1);  Assert.assertNotEquals("event1 should not equal to event2! ",event1,event2);  entity.setCreatedTime(0L);
  event2.setTimestamp(2L);  entity.addEvent(event2);  Assert.assertFalse("event1 should not equal to event2! ",event1.equals(event2));  TimelineEvent event3=new TimelineEvent();  event3.setId("test event id 1");  event3.setTimestamp(1L);  Assert.assertEquals("event1 should equal to event3! ",event3,event1);  Assert.assertNotEquals("event1 should not equal to event2! ",event1,event2);  entity.setCreatedTime(0L);  entity.addRelatesToEntity("test type 2","test id 2");  entity.addRelatesToEntity("test type 3","test id 3");  entity.addIsRelatedToEntity("test type 4","test id 4");  entity.addIsRelatedToEntity("test type 5","test id 5");  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(entity,true));  TimelineEntities entities=new TimelineEntities();
  appAttempt.setId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(0,1),1).toString());  ContainerEntity container=new ContainerEntity();  container.setId(ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(0,1),1),1).toString());  cluster.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow1.setParent(TimelineEntityType.YARN_CLUSTER.toString(),cluster.getId());  flow1.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  flow2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app2.getId());  app1.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  app1.addChild(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  appAttempt.setParent(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  app2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  appAttempt.addChild(TimelineEntityType.YARN_CONTAINER.toString(),container.getId());  container.setParent(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());
  ContainerEntity container=new ContainerEntity();  container.setId(ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(0,1),1),1).toString());  cluster.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow1.setParent(TimelineEntityType.YARN_CLUSTER.toString(),cluster.getId());  flow1.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  flow2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app2.getId());  app1.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  app1.addChild(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  appAttempt.setParent(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  app2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  appAttempt.addChild(TimelineEntityType.YARN_CONTAINER.toString(),container.getId());  container.setParent(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(cluster,true));
  container.setId(ContainerId.newContainerId(ApplicationAttemptId.newInstance(ApplicationId.newInstance(0,1),1),1).toString());  cluster.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow1.setParent(TimelineEntityType.YARN_CLUSTER.toString(),cluster.getId());  flow1.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  flow2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app2.getId());  app1.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  app1.addChild(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  appAttempt.setParent(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  app2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  appAttempt.addChild(TimelineEntityType.YARN_CONTAINER.toString(),container.getId());  container.setParent(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(cluster,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow1,true));
  cluster.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow1.setParent(TimelineEntityType.YARN_CLUSTER.toString(),cluster.getId());  flow1.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  flow2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app2.getId());  app1.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  app1.addChild(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  appAttempt.setParent(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  app2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  appAttempt.addChild(TimelineEntityType.YARN_CONTAINER.toString(),container.getId());  container.setParent(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(cluster,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow1,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow2,true));
  flow1.setParent(TimelineEntityType.YARN_CLUSTER.toString(),cluster.getId());  flow1.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  flow2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app2.getId());  app1.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  app1.addChild(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  appAttempt.setParent(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  app2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  appAttempt.addChild(TimelineEntityType.YARN_CONTAINER.toString(),container.getId());  container.setParent(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(cluster,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow1,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow2,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(app1,true));
  flow1.addChild(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  flow2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app2.getId());  app1.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  app1.addChild(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  appAttempt.setParent(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  app2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  appAttempt.addChild(TimelineEntityType.YARN_CONTAINER.toString(),container.getId());  container.setParent(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(cluster,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow1,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow2,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(app1,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(app2,true));
  flow2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  flow2.addChild(TimelineEntityType.YARN_APPLICATION.toString(),app2.getId());  app1.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  app1.addChild(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  appAttempt.setParent(TimelineEntityType.YARN_APPLICATION.toString(),app1.getId());  app2.setParent(TimelineEntityType.YARN_FLOW_RUN.toString(),flow2.getId());  appAttempt.addChild(TimelineEntityType.YARN_CONTAINER.toString(),container.getId());  container.setParent(TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),appAttempt.getId());  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(cluster,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow1,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(flow2,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(app1,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(app2,true));  LOG.info(TimelineUtils.dumpTimelineRecordtoJSON(appAttempt,true));
@Test public void testUser() throws Exception {  UserEntity user=new UserEntity();  user.setId("test user id");  user.addInfo("test info key 1","test info value 1");  user.addInfo("test info key 2","test info value 2");
@Test public void testQueue() throws Exception {  QueueEntity queue=new QueueEntity();  queue.setId("test queue id");  queue.addInfo("test info key 1","test info value 1");  queue.addInfo("test info key 2","test info value 2");  queue.setParent(TimelineEntityType.YARN_QUEUE.toString(),"test parent queue id");  queue.addChild(TimelineEntityType.YARN_QUEUE.toString(),"test child queue id 1");  queue.addChild(TimelineEntityType.YARN_QUEUE.toString(),"test child queue id 2");
@Before public void setup() throws Exception {  localFS=FileContext.getLocalFSFileContext();  localActiveDir=new File("target",this.getClass().getSimpleName() + "-activeDir").getAbsoluteFile();  localFS.delete(new Path(localActiveDir.getAbsolutePath()),true);  localActiveDir.mkdir();
@Before @After public void cleanupTestDir() throws Exception {  Path workDirPath=new Path(testWorkDir.getAbsolutePath());
  String stderr="stderr";  writeSrcFile(srcFilePath1,stdout,data + testContainerId1.toString() + stdout);  writeSrcFile(srcFilePath1,stderr,data + testContainerId1.toString() + stderr);  UserGroupInformation ugi=UserGroupInformation.getCurrentUser();  try (LogWriter logWriter=new LogWriter()){    logWriter.initialize(conf,remoteAppLogFile,ugi);    LogKey logKey=new LogKey(testContainerId1);    String randomUser="randomUser";    LogValue logValue=spy(new LogValue(Collections.singletonList(srcFileRoot.toString()),testContainerId1,randomUser));    when(logValue.getUser()).thenReturn(randomUser).thenReturn(ugi.getShortUserName());    logWriter.append(logKey,logValue);  }   BufferedReader in=new BufferedReader(new FileReader(new File(remoteAppLogFile.toUri().getRawPath())));  String line;  StringBuffer sb=new StringBuffer("");  while ((line=in.readLine()) != null) {
private void verifyFileControllerInstance(LogAggregationFileControllerFactory factory,Class<? extends LogAggregationFileController> className) throws IOException {  List<LogAggregationFileController> fileControllers=factory.getConfiguredLogAggregationFileControllerList();  FileSystem fs=FileSystem.get(getConf());  Path logPath=fileControllers.get(0).getRemoteAppLogDir(appId,APP_OWNER);
static LocalResource createJar(FileContext files,Path p,LocalResourceVisibility vis) throws IOException {
  lostDescendant=TEST_ROOT_DIR + File.separator + "lostDescendantPidFile";  File file=new File(shellScript);  FileUtils.writeStringToFile(file,"# rogue task\n" + "sleep 1\n" + "echo hello\n"+ "if [ $1 -ne 0 ]\n"+ "then\n"+ " sh " + shellScript + " $(($1-1))\n"+ "else\n"+ " echo $$ > "+ lowestDescendant+ "\n"+ "(sleep 300&\n"+ "echo $! > "+ lostDescendant+ ")\n"+ " while true\n do\n"+ "  sleep 5\n"+ " done\n"+ "fi",StandardCharsets.UTF_8);  Thread t=new RogueTaskThread();  t.start();  String pid=getRogueTaskPID();  LOG.info("Root process pid: " + pid);  ProcfsBasedProcessTree p=createProcessTree(pid);  p.updateProcessTree();  LOG.info("ProcessTree: " + p);  File leaf=new File(lowestDescendant);  while (!leaf.exists()) {    try {      Thread.sleep(500);    } catch (    InterruptedException ie) {
  }  p.updateProcessTree();  LOG.info("ProcessTree: " + p);  String lostpid=getPidFromPidFile(lostDescendant);  LOG.info("Orphaned pid: " + lostpid);  Assert.assertTrue("Child process owned by init escaped process tree.",p.contains(lostpid));  String processTreeDump=p.getProcessTreeDump();  destroyProcessTree(pid);  boolean isAlive=true;  for (int tries=100; tries > 0; tries--) {    if (isSetsidAvailable()) {      isAlive=isAnyProcessInTreeAlive(p);    } else {      isAlive=isAlive(pid);    }    if (!isAlive) {      break;
    memInfos[4]=new ProcessTreeSmapMemInfo("500");    memInfos[5]=new ProcessTreeSmapMemInfo("600");    String[] cmdLines=new String[numProcesses];    cmdLines[0]="proc1 arg1 arg2";    cmdLines[1]="process two arg3 arg4";    cmdLines[2]="proc(3) arg5 arg6";    cmdLines[3]="proc4 arg7 arg8";    cmdLines[4]="proc5 arg9 arg10";    cmdLines[5]="proc6 arg11 arg12";    createMemoryMappingInfo(memInfos);    writeStatFiles(procfsRootDir,pids,procInfos,memInfos);    writeCmdLineFiles(procfsRootDir,pids,cmdLines);    ProcfsBasedProcessTree processTree=createProcessTree("100",procfsRootDir.getAbsolutePath(),SystemClock.getInstance());    processTree.updateProcessTree();    String processTreeDump=processTree.getProcessTreeDump();
public static void setupPidDirs(File procfsRootDir,String[] pids) throws IOException {  for (  String pid : pids) {    File pidDir=new File(procfsRootDir,pid);    FileUtils.forceMkdir(pidDir);
private void logInstances(HttpServletRequest req,HttpServletResponse res,PrintWriter out){
private void logInstances(HttpServletRequest req,HttpServletResponse res,PrintWriter out){  LOG.info("request: {}",req);
private void logInstances(HttpServletRequest req,HttpServletResponse res,PrintWriter out){  LOG.info("request: {}",req);  LOG.info("response: {}",res);
@Override protected void serviceInit(Configuration conf) throws Exception {  String[] names=CsiConfigUtils.getCsiDriverNames(conf);  if (names != null && names.length > 0) {    for (    String driverName : names) {
@Override public void init(String driverName,Configuration conf) throws YarnException {  String driverEndpoint=CsiConfigUtils.getCsiDriverEndpoint(driverName,conf);
@Override public NodePublishVolumeResponse nodePublishVolume(NodePublishVolumeRequest request) throws YarnException, IOException {
@Override public NodePublishVolumeResponse nodePublishVolume(NodePublishVolumeRequest request) throws YarnException, IOException {  LOG.debug("Received nodePublishVolume call, request: {}",request);  Csi.NodePublishVolumeRequest req=ProtoTranslatorFactory.getTranslator(NodePublishVolumeRequest.class,Csi.NodePublishVolumeRequest.class).convertTo(request);
@Override public NodeUnpublishVolumeResponse nodeUnpublishVolume(NodeUnpublishVolumeRequest request) throws YarnException, IOException {
@Override public NodeUnpublishVolumeResponse nodeUnpublishVolume(NodeUnpublishVolumeRequest request) throws YarnException, IOException {  LOG.debug("Received nodeUnpublishVolume call, request: {}",request);  Csi.NodeUnpublishVolumeRequest req=ProtoTranslatorFactory.getTranslator(NodeUnpublishVolumeRequest.class,Csi.NodeUnpublishVolumeRequest.class).convertTo(request);
public void start() throws IOException {  EpollEventLoopGroup group=new EpollEventLoopGroup();  server=NettyServerBuilder.forAddress(GrpcHelper.getSocketAddress(socketAddress)).channelType(EpollServerDomainSocketChannel.class).workerEventLoopGroup(group).bossEventLoopGroup(group).addService(new FakeCsiIdentityService()).build();  server.start();
  ApplicationReportExt app=convertToApplicationReport(entity,field);  if (field == ApplicationReportField.USER_AND_ACLS) {    return app;  }  try {    checkAccess(app);    if (app.appReport.getCurrentApplicationAttemptId() != null) {      ApplicationAttemptReport appAttempt=getApplicationAttempt(app.appReport.getCurrentApplicationAttemptId(),false);      app.appReport.setHost(appAttempt.getHost());      app.appReport.setRpcPort(appAttempt.getRpcPort());      app.appReport.setTrackingUrl(appAttempt.getTrackingUrl());      app.appReport.setOriginalTrackingUrl(appAttempt.getOriginalTrackingUrl());    }  } catch (  AuthorizationException|ApplicationAttemptNotFoundException e) {    if (e instanceof AuthorizationException) {      LOG.warn("Failed to authorize when generating application report for " + app.appReport.getApplicationId() + ". Use a placeholder for its latest attempt id. ",e);    } else {
    webApp=WebApps.$for("applicationhistory",ApplicationHistoryClientService.class,ahsClientService,"ws").with(conf).withAttribute(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,conf.get(YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS)).withCSRFProtection(YarnConfiguration.TIMELINE_CSRF_PREFIX).withXFSProtection(YarnConfiguration.TIMELINE_XFS_PREFIX).at(bindAddress).build(ahsWebApp);    HttpServer2 httpServer=webApp.httpServer();    String[] names=conf.getTrimmedStrings(YarnConfiguration.TIMELINE_SERVICE_UI_NAMES);    WebAppContext webAppContext=httpServer.getWebAppContext();    for (    String name : names) {      String webPath=conf.get(YarnConfiguration.TIMELINE_SERVICE_UI_WEB_PATH_PREFIX + name);      String onDiskPath=conf.get(YarnConfiguration.TIMELINE_SERVICE_UI_ON_DISK_PATH_PREFIX + name);      WebAppContext uiWebAppContext=new WebAppContext();      uiWebAppContext.setContextPath(webPath);      if (onDiskPath.endsWith(".war")) {        uiWebAppContext.setWar(onDiskPath);      } else {        uiWebAppContext.setResourceBase(onDiskPath);      }      final String[] ALL_URLS={"/*"};      FilterHolder[] filterHolders=webAppContext.getServletHandler().getFilters();
          mergeApplicationHistoryData(historyData,startData);          readStartData=true;        } else         if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {          ApplicationFinishData finishData=parseApplicationFinishData(entry.value);          mergeApplicationHistoryData(historyData,finishData);          readFinishData=true;        }      }    }    if (!readStartData && !readFinishData) {      return null;    }    if (!readStartData) {      LOG.warn("Start information is missing for application " + appId);    }    if (!readFinishData) {      LOG.warn("Finish information is missing for application " + appId);    }    LOG.info("Completed reading history information of application " + appId);    return historyData;  } catch (  IOException e) {
    while (hfReader.hasNext()) {      HistoryFileReader.Entry entry=hfReader.next();      if (entry.key.id.startsWith(ConverterUtils.APPLICATION_ATTEMPT_PREFIX)) {        ApplicationAttemptId appAttemptId=ApplicationAttemptId.fromString(entry.key.id);        if (appAttemptId.getApplicationId().equals(appId)) {          ApplicationAttemptHistoryData historyData=historyDataMap.get(appAttemptId);          if (historyData == null) {            historyData=ApplicationAttemptHistoryData.newInstance(appAttemptId,null,-1,null,null,null,FinalApplicationStatus.UNDEFINED,null);            historyDataMap.put(appAttemptId,historyData);          }          if (entry.key.suffix.equals(START_DATA_SUFFIX)) {            mergeApplicationAttemptHistoryData(historyData,parseApplicationAttemptStartData(entry.value));          } else           if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {            mergeApplicationAttemptHistoryData(historyData,parseApplicationAttemptFinishData(entry.value));          }        }      }    }    LOG.info("Completed reading history information of all application" + " attempts of application " + appId);  } catch (  IOException e) {
          mergeApplicationAttemptHistoryData(historyData,startData);          readStartData=true;        } else         if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {          ApplicationAttemptFinishData finishData=parseApplicationAttemptFinishData(entry.value);          mergeApplicationAttemptHistoryData(historyData,finishData);          readFinishData=true;        }      }    }    if (!readStartData && !readFinishData) {      return null;    }    if (!readStartData) {      LOG.warn("Start information is missing for application attempt " + appAttemptId);    }    if (!readFinishData) {      LOG.warn("Finish information is missing for application attempt " + appAttemptId);    }    LOG.info("Completed reading history information of application attempt " + appAttemptId);    return historyData;  } catch (  IOException e) {
          mergeContainerHistoryData(historyData,startData);          readStartData=true;        } else         if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {          ContainerFinishData finishData=parseContainerFinishData(entry.value);          mergeContainerHistoryData(historyData,finishData);          readFinishData=true;        }      }    }    if (!readStartData && !readFinishData) {      return null;    }    if (!readStartData) {      LOG.warn("Start information is missing for container " + containerId);    }    if (!readFinishData) {      LOG.warn("Finish information is missing for container " + containerId);    }    LOG.info("Completed reading history information of container " + containerId);    return historyData;  } catch (  IOException e) {
    while (hfReader.hasNext()) {      HistoryFileReader.Entry entry=hfReader.next();      if (entry.key.id.startsWith(ConverterUtils.CONTAINER_PREFIX)) {        ContainerId containerId=ContainerId.fromString(entry.key.id);        if (containerId.getApplicationAttemptId().equals(appAttemptId)) {          ContainerHistoryData historyData=historyDataMap.get(containerId);          if (historyData == null) {            historyData=ContainerHistoryData.newInstance(containerId,null,null,null,Long.MIN_VALUE,Long.MAX_VALUE,null,Integer.MAX_VALUE,null);            historyDataMap.put(containerId,historyData);          }          if (entry.key.suffix.equals(START_DATA_SUFFIX)) {            mergeContainerHistoryData(historyData,parseContainerStartData(entry.value));          } else           if (entry.key.suffix.equals(FINISH_DATA_SUFFIX)) {            mergeContainerHistoryData(historyData,parseContainerFinishData(entry.value));          }        }      }    }    LOG.info("Completed reading history information of all containers" + " of application attempt " + appAttemptId);  } catch (  IOException e) {
        }      }    }    if (primaryFilters != null && !primaryFilters.isEmpty()) {      for (      Entry<String,Set<Object>> primaryFilter : primaryFilters.entrySet()) {        for (        Object primaryFilterValue : primaryFilter.getValue()) {          byte[] key=createPrimaryFilterKey(entity.getEntityId(),entity.getEntityType(),revStartTime,primaryFilter.getKey(),primaryFilterValue);          writeBatch.put(key,EMPTY_BYTES);          writePrimaryFilterEntries(writeBatch,primaryFilters,key,EMPTY_BYTES);        }      }    }    Map<String,Object> otherInfo=entity.getOtherInfo();    if (otherInfo != null && !otherInfo.isEmpty()) {      for (      Entry<String,Object> i : otherInfo.entrySet()) {        byte[] key=createOtherInfoKey(entity.getEntityId(),entity.getEntityType(),revStartTime,i.getKey());        byte[] value=GenericObjectMapper.write(i.getValue());        writeBatch.put(key,value);        writePrimaryFilterEntries(writeBatch,primaryFilters,key,value);      }    }    byte[] key=createDomainIdKey(entity.getEntityId(),entity.getEntityType(),revStartTime);    if (entity.getDomainId() == null || entity.getDomainId().length() == 0) {
    if (primaryFilters != null && !primaryFilters.isEmpty()) {      for (      Entry<String,Set<Object>> primaryFilter : primaryFilters.entrySet()) {        for (        Object primaryFilterValue : primaryFilter.getValue()) {          byte[] key=createPrimaryFilterKey(entity.getEntityId(),entity.getEntityType(),revStartTime,primaryFilter.getKey(),primaryFilterValue);          writeBatch.put(key,EMPTY_BYTES);          writePrimaryFilterEntries(writeBatch,primaryFilters,key,EMPTY_BYTES);        }      }    }    Map<String,Object> otherInfo=entity.getOtherInfo();    if (otherInfo != null && !otherInfo.isEmpty()) {      for (      Entry<String,Object> i : otherInfo.entrySet()) {        byte[] key=createOtherInfoKey(entity.getEntityId(),entity.getEntityType(),revStartTime,i.getKey());        byte[] value=GenericObjectMapper.write(i.getValue());        writeBatch.put(key,value);        writePrimaryFilterEntries(writeBatch,primaryFilters,key,value);      }    }    byte[] key=createDomainIdKey(entity.getEntityId(),entity.getEntityType(),revStartTime);    if (entity.getDomainId() == null || entity.getDomainId().length() == 0) {
    if (entity.getDomainId() == null || entity.getDomainId().length() == 0) {      if (!allowEmptyDomainId) {        handleError(entity,response,TimelinePutError.NO_DOMAIN);        return;      }    } else {      writeBatch.put(key,entity.getDomainId().getBytes(Charset.forName("UTF-8")));      writePrimaryFilterEntries(writeBatch,primaryFilters,key,entity.getDomainId().getBytes(Charset.forName("UTF-8")));    }    db.write(writeBatch);  } catch (  DBException de) {    LOG.error("Error putting entity " + entity.getEntityId() + " of type "+ entity.getEntityType(),de);    handleError(entity,response,TimelinePutError.IO_EXCEPTION);  }catch (  IOException e) {    LOG.error("Error putting entity " + entity.getEntityId() + " of type "+ entity.getEntityType(),e);    handleError(entity,response,TimelinePutError.IO_EXCEPTION);  } finally {
        return;      }    } else {      writeBatch.put(key,entity.getDomainId().getBytes(Charset.forName("UTF-8")));      writePrimaryFilterEntries(writeBatch,primaryFilters,key,entity.getDomainId().getBytes(Charset.forName("UTF-8")));    }    db.write(writeBatch);  } catch (  DBException de) {    LOG.error("Error putting entity " + entity.getEntityId() + " of type "+ entity.getEntityType(),de);    handleError(entity,response,TimelinePutError.IO_EXCEPTION);  }catch (  IOException e) {    LOG.error("Error putting entity " + entity.getEntityId() + " of type "+ entity.getEntityType(),e);    handleError(entity,response,TimelinePutError.IO_EXCEPTION);  } finally {    lock.unlock();    writeLocks.returnLock(lock);    IOUtils.cleanupWithLogger(LOG,writeBatch);
    byte[] typePrefix=kb.getBytesForLookup();    kb.add(reverseTimestamp);    if (!seeked) {      iterator.seek(kb.getBytesForLookup());    }    if (!iterator.hasNext()) {      return false;    }    byte[] entityKey=iterator.peekNext().getKey();    if (!prefixMatches(typePrefix,typePrefix.length,entityKey)) {      return false;    }    KeyParser kp=new KeyParser(entityKey,typePrefix.length + 8);    String entityId=kp.getNextString();    int prefixlen=kp.getOffset();    byte[] deletePrefix=new byte[prefixlen];    System.arraycopy(entityKey,0,deletePrefix,0,prefixlen);    writeBatch=db.createWriteBatch();
    writeBatch=db.createWriteBatch();    LOG.debug("Deleting entity type:{} id:{}",entityType,entityId);    writeBatch.delete(createStartTimeLookupKey(entityId,entityType));    EntityIdentifier entityIdentifier=new EntityIdentifier(entityId,entityType);    startTimeReadCache.remove(entityIdentifier);    startTimeWriteCache.remove(entityIdentifier);    for (; iterator.hasNext(); iterator.next()) {      byte[] key=iterator.peekNext().getKey();      if (!prefixMatches(entityKey,prefixlen,key)) {        break;      }      writeBatch.delete(key);      if (key.length == prefixlen) {        continue;      }      if (key[prefixlen] == PRIMARY_FILTERS_COLUMN[0]) {        kp=new KeyParser(key,prefixlen + PRIMARY_FILTERS_COLUMN.length);
      }      writeBatch.delete(key);      if (key.length == prefixlen) {        continue;      }      if (key[prefixlen] == PRIMARY_FILTERS_COLUMN[0]) {        kp=new KeyParser(key,prefixlen + PRIMARY_FILTERS_COLUMN.length);        String name=kp.getNextString();        Object value=GenericObjectMapper.read(key,kp.getOffset());        deleteKeysWithPrefix(writeBatch,addPrimaryFilterToKey(name,value,deletePrefix),pfIterator);        LOG.debug("Deleting entity type:{} id:{} primary filter entry {} {}",entityType,entityId,name,value);      } else       if (key[prefixlen] == RELATED_ENTITIES_COLUMN[0]) {        kp=new KeyParser(key,prefixlen + RELATED_ENTITIES_COLUMN.length);        String type=kp.getNextString();        String id=kp.getNextString();        byte[] relatedEntityStartTime=getStartTime(id,type);        if (relatedEntityStartTime == null) {
        LOG.debug("Deleting entity type:{} id:{} primary filter entry {} {}",entityType,entityId,name,value);      } else       if (key[prefixlen] == RELATED_ENTITIES_COLUMN[0]) {        kp=new KeyParser(key,prefixlen + RELATED_ENTITIES_COLUMN.length);        String type=kp.getNextString();        String id=kp.getNextString();        byte[] relatedEntityStartTime=getStartTime(id,type);        if (relatedEntityStartTime == null) {          LOG.warn("Found no start time for " + "related entity " + id + " of type "+ type+ " while "+ "deleting "+ entityId+ " of type "+ entityType);          continue;        }        writeBatch.delete(createReverseRelatedEntityKey(id,type,relatedEntityStartTime,entityId,entityType));        LOG.debug("Deleting entity type:{} id:{} from invisible reverse" + " related entity entry of type:{} id:{}",entityType,entityId,type,id);      } else       if (key[prefixlen] == INVISIBLE_REVERSE_RELATED_ENTITIES_COLUMN[0]) {        kp=new KeyParser(key,prefixlen + INVISIBLE_REVERSE_RELATED_ENTITIES_COLUMN.length);        String type=kp.getNextString();        String id=kp.getNextString();
      long typeCount=0;      deleteLock.writeLock().lock();      try {        iterator=getDbIterator(false);        pfIterator=getDbIterator(false);        if (deletionThread != null && deletionThread.isInterrupted()) {          throw new InterruptedException();        }        boolean seeked=false;        while (deleteNextEntity(entityType,reverseTimestamp,iterator,pfIterator,seeked)) {          typeCount++;          totalCount++;          seeked=true;          if (deletionThread != null && deletionThread.isInterrupted()) {            throw new InterruptedException();          }        }      } catch (      IOException e) {
        if (deletionThread != null && deletionThread.isInterrupted()) {          throw new InterruptedException();        }        boolean seeked=false;        while (deleteNextEntity(entityType,reverseTimestamp,iterator,pfIterator,seeked)) {          typeCount++;          totalCount++;          seeked=true;          if (deletionThread != null && deletionThread.isInterrupted()) {            throw new InterruptedException();          }        }      } catch (      IOException e) {        LOG.error("Got IOException while deleting entities for type " + entityType + ", continuing to next type",e);      } finally {        IOUtils.cleanupWithLogger(LOG,iterator,pfIterator);        deleteLock.writeLock().unlock();        if (typeCount > 0) {
private void checkVersion() throws IOException {  Version loadedVersion=loadVersion();
protected void setNextRollingTimeMillis(final long timestamp){  this.nextRollingCheckMillis=timestamp;
private synchronized void scheduleOldDBsForEviction(){  long evictionThreshold=computeCurrentCheckMillis(currentTimeMillis() - getTimeToLive());
private synchronized void scheduleOldDBsForEviction(){  long evictionThreshold=computeCurrentCheckMillis(currentTimeMillis() - getTimeToLive());  LOG.info("Scheduling " + getName() + " DBs older than "+ fdf.format(evictionThreshold)+ " for eviction");  Iterator<Entry<Long,DB>> iterator=rollingdbs.entrySet().iterator();  while (iterator.hasNext()) {    Entry<Long,DB> entry=iterator.next();    if (entry.getKey() < evictionThreshold) {
public synchronized void evictOldDBs(){  LOG.info("Evicting " + getName() + " DBs scheduled for eviction");  Iterator<Entry<Long,DB>> iterator=rollingdbsToEvict.entrySet().iterator();  while (iterator.hasNext()) {    Entry<Long,DB> entry=iterator.next();    IOUtils.cleanupWithLogger(LOG,entry.getValue());    String dbName=fdf.format(entry.getKey());    Path path=new Path(rollingDBPath,getName() + "." + dbName);    try {
      }      localFS.setPermission(dbPath,LEVELDB_DIR_UMASK);    }    if (!localFS.exists(domainDBPath)) {      if (!localFS.mkdirs(domainDBPath)) {        throw new IOException("Couldn't create directory for leveldb " + "timeline store " + domainDBPath);      }      localFS.setPermission(domainDBPath,LEVELDB_DIR_UMASK);    }    if (!localFS.exists(starttimeDBPath)) {      if (!localFS.mkdirs(starttimeDBPath)) {        throw new IOException("Couldn't create directory for leveldb " + "timeline store " + starttimeDBPath);      }      localFS.setPermission(starttimeDBPath,LEVELDB_DIR_UMASK);    }    if (!localFS.exists(ownerDBPath)) {      if (!localFS.mkdirs(ownerDBPath)) {        throw new IOException("Couldn't create directory for leveldb " + "timeline store " + ownerDBPath);      }      localFS.setPermission(ownerDBPath,LEVELDB_DIR_UMASK);    }  }   options.maxOpenFiles(conf.getInt(TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES,DEFAULT_TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES));  options.writeBufferSize(conf.getInt(TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE,DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE));
@Override public TimelineEntity getEntity(String entityId,String entityType,EnumSet<Field> fields) throws IOException {  Long revStartTime=getStartTimeLong(entityId,entityType);  if (revStartTime == null) {
            domainId=TimelineDataManager.DEFAULT_DOMAIN_ID;          } else {            domainId=new String(relatedDomainIdBytes,UTF_8);          }          if (!domainId.equals(entity.getDomainId())) {            TimelinePutError error=new TimelinePutError();            error.setEntityId(entity.getEntityId());            error.setEntityType(entity.getEntityType());            error.setErrorCode(TimelinePutError.FORBIDDEN_RELATION);            response.addError(error);            continue;          }          byte[] key=createRelatedEntityKey(relatedEntityId,relatedEntityType,relatedEntityStartTime,entity.getEntityId(),entity.getEntityType());          WriteBatch relatedWriteBatch=relatedRollingWriteBatch.getWriteBatch();          relatedWriteBatch.put(key,EMPTY_BYTES);          ++putCount;        }      }    }    RollingWriteBatch indexRollingWriteBatch=indexUpdates.get(roundedStartTime);
    WriteBatch indexWriteBatch=indexRollingWriteBatch.getWriteBatch();    putCount+=writePrimaryFilterEntries(indexWriteBatch,primaryFilters,markerKey,EMPTY_BYTES);  } catch (  IOException e) {    LOG.error("Error putting entity " + entity.getEntityId() + " of type "+ entity.getEntityType(),e);    TimelinePutError error=new TimelinePutError();    error.setEntityId(entity.getEntityId());    error.setEntityType(entity.getEntityType());    error.setErrorCode(TimelinePutError.IO_EXCEPTION);    response.addError(error);  }  for (  EntityIdentifier relatedEntity : relatedEntitiesWithoutStartTimes) {    try {      Long relatedEntityStartAndInsertTime=getAndSetStartTime(relatedEntity.getId(),relatedEntity.getType(),readReverseOrderedLong(revStartTime,0),null);      if (relatedEntityStartAndInsertTime == null) {        throw new IOException("Error setting start time for related entity");      }      long relatedStartTimeLong=relatedEntityStartAndInsertTime;
@VisibleForTesting long evictOldStartTimes(long minStartTime) throws IOException {
  ReadOptions readOptions=new ReadOptions();  readOptions.fillCache(false);  try (DBIterator iterator=starttimedb.iterator(readOptions)){    iterator.seekToFirst();    writeBatch=starttimedb.createWriteBatch();    while (iterator.hasNext()) {      Map.Entry<byte[],byte[]> current=iterator.next();      byte[] entityKey=current.getKey();      byte[] entityValue=current.getValue();      long startTime=readReverseOrderedLong(entityValue,0);      if (startTime < minStartTime) {        ++batchSize;        ++startTimesCount;        writeBatch.delete(entityKey);        if (batchSize >= writeBatchSize) {
  try (DBIterator iterator=starttimedb.iterator(readOptions)){    iterator.seekToFirst();    writeBatch=starttimedb.createWriteBatch();    while (iterator.hasNext()) {      Map.Entry<byte[],byte[]> current=iterator.next();      byte[] entityKey=current.getKey();      byte[] entityValue=current.getValue();      long startTime=readReverseOrderedLong(entityValue,0);      if (startTime < minStartTime) {        ++batchSize;        ++startTimesCount;        writeBatch.delete(entityKey);        if (batchSize >= writeBatchSize) {          LOG.debug("Preparing to delete a batch of {} old start times",batchSize);          starttimedb.write(writeBatch);
      long startTime=readReverseOrderedLong(entityValue,0);      if (startTime < minStartTime) {        ++batchSize;        ++startTimesCount;        writeBatch.delete(entityKey);        if (batchSize >= writeBatchSize) {          LOG.debug("Preparing to delete a batch of {} old start times",batchSize);          starttimedb.write(writeBatch);          LOG.debug("Deleted batch of {}. Total start times deleted" + " so far this cycle: {}",batchSize,startTimesCount);          IOUtils.cleanupWithLogger(LOG,writeBatch);          writeBatch=starttimedb.createWriteBatch();          batchSize=0;        }      }      ++totalCount;    }    LOG.debug("Preparing to delete a batch of {} old start times",batchSize);    starttimedb.write(writeBatch);
      if (startTime < minStartTime) {        ++batchSize;        ++startTimesCount;        writeBatch.delete(entityKey);        if (batchSize >= writeBatchSize) {          LOG.debug("Preparing to delete a batch of {} old start times",batchSize);          starttimedb.write(writeBatch);          LOG.debug("Deleted batch of {}. Total start times deleted" + " so far this cycle: {}",batchSize,startTimesCount);          IOUtils.cleanupWithLogger(LOG,writeBatch);          writeBatch=starttimedb.createWriteBatch();          batchSize=0;        }      }      ++totalCount;    }    LOG.debug("Preparing to delete a batch of {} old start times",batchSize);    starttimedb.write(writeBatch);    LOG.debug("Deleted batch of {}. Total start times deleted so far" + " this cycle: {}",batchSize,startTimesCount);
private void checkVersion() throws IOException {  Version loadedVersion=loadVersion();
  Options options=new Options();  Path dbPath=new Path(getConfig().get(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_STATE_STORE_PATH),DB_NAME);  FileSystem localFS=null;  try {    localFS=FileSystem.getLocal(getConfig());    if (!localFS.exists(dbPath)) {      if (!localFS.mkdirs(dbPath)) {        throw new IOException("Couldn't create directory for leveldb " + "timeline store " + dbPath);      }      localFS.setPermission(dbPath,LEVELDB_DIR_UMASK);    }  }  finally {    IOUtils.cleanupWithLogger(LOG,localFS);  }  JniDBFactory factory=new JniDBFactory();  try {    options.createIfMissing(false);    db=factory.open(new File(dbPath.toString()),options);
        throw new IOException("Couldn't create directory for leveldb " + "timeline store " + dbPath);      }      localFS.setPermission(dbPath,LEVELDB_DIR_UMASK);    }  }  finally {    IOUtils.cleanupWithLogger(LOG,localFS);  }  JniDBFactory factory=new JniDBFactory();  try {    options.createIfMissing(false);    db=factory.open(new File(dbPath.toString()),options);    LOG.info("Loading the existing database at th path: " + dbPath.toString());    checkVersion();  } catch (  NativeDB.DBException e) {    if (e.isNotFound() || e.getMessage().contains(" does not exist ")) {      try {        options.createIfMissing(true);        db=factory.open(new File(dbPath.toString()),options);
@Override public TimelineServiceState loadState() throws IOException {  LOG.info("Loading timeline service state from leveldb");  TimelineServiceState state=new TimelineServiceState();  int numKeys=loadTokenMasterKeys(state);  int numTokens=loadTokens(state);  loadLatestSequenceNumber(state);
private void checkVersion() throws IOException {  Version loadedVersion=loadVersion();
public boolean checkAccess(UserGroupInformation callerUGI,TimelineDomain domain) throws YarnException, IOException {  if (LOG.isDebugEnabled()) {
@POST @Consumes({MediaType.APPLICATION_JSON}) @Produces({MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8}) public TimelinePutResponse postEntities(@Context HttpServletRequest req,@Context HttpServletResponse res,TimelineEntities entities){  init(res);  UserGroupInformation callerUGI=getUser(req);  if (callerUGI == null) {    String msg="The owner of the posted timeline entities is not set";
@PUT @Path("/domain") @Consumes({MediaType.APPLICATION_JSON}) @Produces({MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8}) public TimelinePutResponse putDomain(@Context HttpServletRequest req,@Context HttpServletResponse res,TimelineDomain domain){  init(res);  UserGroupInformation callerUGI=getUser(req);  if (callerUGI == null) {    String msg="The owner of the posted timeline domain is not set";
  UserGroupInformation callerUGI=getUser(req);  if (callerUGI == null) {    String msg="The owner of the posted timeline domain is not set";    LOG.error(msg);    throw new ForbiddenException(msg);  }  domain.setOwner(callerUGI.getShortUserName());  try {    timelineDataManager.putDomain(domain,callerUGI);  } catch (  YarnException e) {    LOG.error(e.getMessage(),e);    throw new ForbiddenException(e);  }catch (  RuntimeException e) {    LOG.error("Error putting domain",e);    throw new WebApplicationException(e,Response.Status.INTERNAL_SERVER_ERROR);  }catch (  IOException e) {
    this.metrics.incrRMMasterSlaveSwitch(this.rmId);synchronized (this) {      for (      ResourceRequestSet requestSet : this.remotePendingAsks.values()) {        for (        ResourceRequest rr : requestSet.getRRs()) {          addResourceRequestToAsk(rr);        }      }      this.release.addAll(this.remotePendingRelease);      this.blacklistAdditions.addAll(this.remoteBlacklistedNodes);      this.change.putAll(this.remotePendingChange);      for (      List<SchedulingRequest> reqs : this.remotePendingSchedRequest.values()) {        this.schedulingRequest.addAll(reqs);      }    }    reRegisterApplicationMaster(this.amRegistrationRequest);    allocateRequest.setResponseId(0);    allocateResponse=allocate(allocateRequest);    return allocateResponse;  }catch (  Throwable t) {
private T getProxyInternal(boolean isFailover){  SubClusterInfo subClusterInfo;  T proxy=this.current;  try {    LOG.info("Failing over to the ResourceManager for SubClusterId: {}",subClusterId);    subClusterInfo=facade.getSubCluster(subClusterId,this.flushFacadeCacheForYarnRMAddr && isFailover);    updateRMAddress(subClusterInfo);    if (this.originalUser == null) {      InetSocketAddress rmAddress=rmProxy.getRMAddress(conf,protocol);
private T getProxyInternal(boolean isFailover){  SubClusterInfo subClusterInfo;  T proxy=this.current;  try {    LOG.info("Failing over to the ResourceManager for SubClusterId: {}",subClusterId);    subClusterInfo=facade.getSubCluster(subClusterId,this.flushFacadeCacheForYarnRMAddr && isFailover);    updateRMAddress(subClusterInfo);    if (this.originalUser == null) {      InetSocketAddress rmAddress=rmProxy.getRMAddress(conf,protocol);      LOG.info("Connecting to {} subClusterId {} with protocol {}" + " without a proxy user",rmAddress,subClusterId,protocol.getSimpleName());      proxy=createRMProxy(rmAddress);    } else {      proxy=this.originalUser.doAs(new PrivilegedExceptionAction<T>(){        @Override public T run() throws IOException {          InetSocketAddress rmAddress=rmProxy.getRMAddress(conf,protocol);
    LOG.info("Failing over to the ResourceManager for SubClusterId: {}",subClusterId);    subClusterInfo=facade.getSubCluster(subClusterId,this.flushFacadeCacheForYarnRMAddr && isFailover);    updateRMAddress(subClusterInfo);    if (this.originalUser == null) {      InetSocketAddress rmAddress=rmProxy.getRMAddress(conf,protocol);      LOG.info("Connecting to {} subClusterId {} with protocol {}" + " without a proxy user",rmAddress,subClusterId,protocol.getSimpleName());      proxy=createRMProxy(rmAddress);    } else {      proxy=this.originalUser.doAs(new PrivilegedExceptionAction<T>(){        @Override public T run() throws IOException {          InetSocketAddress rmAddress=rmProxy.getRMAddress(conf,protocol);          LOG.info("Connecting to {} subClusterId {} with protocol {} as user {}",rmAddress,subClusterId,protocol.getSimpleName(),originalUser);          return createRMProxy(rmAddress);        }      });    }  } catch (  Exception e) {
public static SubClusterPolicyConfiguration loadPolicyConfiguration(String queue,Configuration conf,FederationStateStoreFacade federationFacade){  SubClusterPolicyConfiguration configuration=null;  if (queue != null) {    try {      configuration=federationFacade.getPolicyConfiguration(queue);    } catch (    YarnException e) {      LOG.warn("Failed to get policy from FederationFacade with queue " + queue + ": "+ e.getMessage());    }  }  if (configuration == null) {    LOG.info("No policy configured for queue {} in StateStore," + " fallback to default queue",queue);    queue=YarnConfiguration.DEFAULT_FEDERATION_POLICY_KEY;    try {      configuration=federationFacade.getPolicyConfiguration(queue);    } catch (    YarnException e) {      LOG.warn("No fallback behavior defined in store, defaulting to XML " + "configuration fallback behavior.");    }  }  if (configuration == null) {
public static FederationAMRMProxyPolicy loadAMRMPolicy(String queue,FederationAMRMProxyPolicy oldPolicy,Configuration conf,FederationStateStoreFacade federationFacade,SubClusterId homeSubClusterId) throws FederationPolicyInitializationException {  SubClusterPolicyConfiguration configuration=loadPolicyConfiguration(queue,conf,federationFacade);  FederationPolicyInitializationContext context=new FederationPolicyInitializationContext(configuration,federationFacade.getSubClusterResolver(),federationFacade,homeSubClusterId);
@Override public void notifyOfResponse(SubClusterId subClusterId,AllocateResponse response) throws YarnException {  if (response.getAvailableResources() != null) {    headroom.put(subClusterId,response.getAvailableResources());
    } catch (    YarnException e) {    }    if (bookkeeper.isActiveAndEnabled(targetId)) {      bookkeeper.addLocalizedNodeRR(targetId,rr);      continue;    }    try {      targetIds=resolver.getSubClustersForRack(rr.getResourceName());    } catch (    YarnException e) {    }    if (targetIds != null && targetIds.size() > 0) {      boolean hasActive=false;      for (      SubClusterId tid : targetIds) {        if (bookkeeper.isActiveAndEnabled(tid)) {          bookkeeper.addRackRR(tid,rr);          hasActive=true;        }      }      if (hasActive) {        continue;
  if (blackListSubClusters != null) {    validSubClusters.removeAll(blackListSubClusters);  }  try {    SubClusterId targetId=null;    ResourceRequest nodeRequest=null;    ResourceRequest rackRequest=null;    ResourceRequest anyRequest=null;    for (    ResourceRequest rr : rrList) {      try {        targetId=resolver.getSubClusterForNode(rr.getResourceName());        nodeRequest=rr;      } catch (      YarnException e) {        LOG.error("Cannot resolve node : {}",e.getLocalizedMessage());      }      try {        resolver.getSubClustersForRack(rr.getResourceName());
        nodeRequest=rr;      } catch (      YarnException e) {        LOG.error("Cannot resolve node : {}",e.getLocalizedMessage());      }      try {        resolver.getSubClustersForRack(rr.getResourceName());        rackRequest=rr;      } catch (      YarnException e) {        LOG.error("Cannot resolve rack : {}",e.getLocalizedMessage());      }      if (ResourceRequest.isAnyLocation(rr.getResourceName())) {        anyRequest=rr;        continue;      }    }    if (nodeRequest == null) {      throw new YarnException("Missing node request");    }    if (rackRequest == null) {      throw new YarnException("Missing rack request");
      } catch (      YarnException e) {        LOG.error("Cannot resolve node : {}",e.getLocalizedMessage());      }      try {        resolver.getSubClustersForRack(rr.getResourceName());        rackRequest=rr;      } catch (      YarnException e) {        LOG.error("Cannot resolve rack : {}",e.getLocalizedMessage());      }      if (ResourceRequest.isAnyLocation(rr.getResourceName())) {        anyRequest=rr;        continue;      }    }    if (nodeRequest == null) {      throw new YarnException("Missing node request");    }    if (rackRequest == null) {      throw new YarnException("Missing rack request");    }    if (anyRequest == null) {
        resolver.getSubClustersForRack(rr.getResourceName());        rackRequest=rr;      } catch (      YarnException e) {        LOG.error("Cannot resolve rack : {}",e.getLocalizedMessage());      }      if (ResourceRequest.isAnyLocation(rr.getResourceName())) {        anyRequest=rr;        continue;      }    }    if (nodeRequest == null) {      throw new YarnException("Missing node request");    }    if (rackRequest == null) {      throw new YarnException("Missing rack request");    }    if (anyRequest == null) {      throw new YarnException("Missing any request");    }    LOG.info("Node request: " + nodeRequest.getResourceName() + ", Rack request: "+ rackRequest.getResourceName()+ ", Any request: "+ anyRequest.getResourceName());    if (validSubClusters.contains(targetId) && enabledSCs.contains(targetId)) {
    BufferedReader reader=null;    try {      file=Paths.get(fileName);    } catch (    InvalidPathException e) {      LOG.info("The configured machine list file path {} does not exist",fileName);      return;    }    try {      reader=Files.newBufferedReader(file,Charset.defaultCharset());      String line=null;      while ((line=reader.readLine()) != null) {        String[] tokens=line.split(",");        if (tokens.length == 3) {          String nodeName=tokens[NODE_NAME_INDEX].trim().toUpperCase();          SubClusterId subClusterId=SubClusterId.newInstance(tokens[SUBCLUSTER_ID_INDEX].trim());          String rackName=tokens[RACK_NAME_INDEX].trim().toUpperCase();
    try {      file=Paths.get(fileName);    } catch (    InvalidPathException e) {      LOG.info("The configured machine list file path {} does not exist",fileName);      return;    }    try {      reader=Files.newBufferedReader(file,Charset.defaultCharset());      String line=null;      while ((line=reader.readLine()) != null) {        String[] tokens=line.split(",");        if (tokens.length == 3) {          String nodeName=tokens[NODE_NAME_INDEX].trim().toUpperCase();          SubClusterId subClusterId=SubClusterId.newInstance(tokens[SUBCLUSTER_ID_INDEX].trim());          String rackName=tokens[RACK_NAME_INDEX].trim().toUpperCase();          if (LOG.isDebugEnabled()) {
  driverClass=conf.get(YarnConfiguration.FEDERATION_STATESTORE_SQL_JDBC_CLASS,YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_SQL_JDBC_CLASS);  maximumPoolSize=conf.getInt(YarnConfiguration.FEDERATION_STATESTORE_SQL_MAXCONNECTIONS,YarnConfiguration.DEFAULT_FEDERATION_STATESTORE_SQL_MAXCONNECTIONS);  userName=conf.get(YarnConfiguration.FEDERATION_STATESTORE_SQL_USERNAME);  password=conf.get(YarnConfiguration.FEDERATION_STATESTORE_SQL_PASSWORD);  url=conf.get(YarnConfiguration.FEDERATION_STATESTORE_SQL_URL);  try {    Class.forName(driverClass);  } catch (  ClassNotFoundException e) {    FederationStateStoreUtils.logAndThrowException(LOG,"Driver class not found.",e);  }  dataSource=new HikariDataSource();  dataSource.setDataSourceClassName(driverClass);  FederationStateStoreUtils.setUsername(dataSource,userName);  FederationStateStoreUtils.setPassword(dataSource,password);  FederationStateStoreUtils.setProperty(dataSource,FederationStateStoreUtils.FEDERATION_STORE_URL,url);  dataSource.setMaximumPoolSize(maximumPoolSize);
  SubClusterId subClusterId=request.getApplicationHomeSubCluster().getHomeSubCluster();  try {    cstmt=getCallableStatement(CALL_SP_ADD_APPLICATION_HOME_SUBCLUSTER);    cstmt.setString(1,appId.toString());    cstmt.setString(2,subClusterId.getId());    cstmt.registerOutParameter(3,java.sql.Types.VARCHAR);    cstmt.registerOutParameter(4,java.sql.Types.INTEGER);    long startTime=clock.getTime();    cstmt.executeUpdate();    long stopTime=clock.getTime();    subClusterHome=cstmt.getString(3);    SubClusterId subClusterIdHome=SubClusterId.newInstance(subClusterHome);    FederationStateStoreClientMetrics.succeededStateStoreCall(stopTime - startTime);    if (subClusterId.equals(subClusterIdHome)) {      if (cstmt.getInt(4) == 0) {
@Override public GetSubClusterPolicyConfigurationResponse getPolicyConfiguration(GetSubClusterPolicyConfigurationRequest request) throws YarnException {  FederationPolicyStoreInputValidator.validate(request);  CallableStatement cstmt=null;  SubClusterPolicyConfiguration subClusterPolicyConfiguration=null;  try {    cstmt=getCallableStatement(CALL_SP_GET_POLICY_CONFIGURATION);    cstmt.setString(1,request.getQueue());    cstmt.registerOutParameter(2,java.sql.Types.VARCHAR);    cstmt.registerOutParameter(3,java.sql.Types.VARBINARY);    long startTime=clock.getTime();    cstmt.executeUpdate();    long stopTime=clock.getTime();    if (cstmt.getString(2) != null && cstmt.getBytes(3) != null) {      subClusterPolicyConfiguration=SubClusterPolicyConfiguration.newInstance(request.getQueue(),cstmt.getString(2),ByteBuffer.wrap(cstmt.getBytes(3)));
public static void failedStateStoreCall(){  String methodName=Thread.currentThread().getStackTrace()[2].getMethodName();  MutableCounterLong methodMetric=API_TO_FAILED_CALLS.get(methodName);  if (methodMetric == null) {
public static void succeededStateStoreCall(long duration){  String methodName=Thread.currentThread().getStackTrace()[2].getMethodName();  MutableRate methodMetric=API_TO_SUCCESSFUL_CALLS.get(methodName);  MutableQuantiles methodQuantileMetric=API_TO_QUANTILE_METRICS.get(methodName);  if (methodMetric == null || methodQuantileMetric == null) {
public static void logAndThrowException(Logger log,String errMsg,Throwable t) throws YarnException {  if (t != null) {
public static void logAndThrowStoreException(Logger log,String errMsg) throws YarnException {
public static void logAndThrowInvalidInputException(Logger log,String errMsg) throws YarnException {
public static void logAndThrowRetriableException(Logger log,String errMsg,Throwable t) throws YarnException {  if (t != null) {
public static void setProperty(HikariDataSource dataSource,String property,String value){
    LOG.warn("Unexpected exception from listDirRegistry",e);  }  if (subclusters == null) {    LOG.info("Application {} does not exist in registry",appId);    return retMap;  }  for (  String scId : subclusters) {    LOG.info("Reading amrmToken for subcluster {} for {}",scId,appId);    String key=getRegistryKey(appId,scId);    try {      String tokenString=readRegistry(this.registry,this.user,key,true);      if (tokenString == null) {        throw new YarnException("Null string from readRegistry key " + key);      }      Token<AMRMTokenIdentifier> amrmToken=new Token<>();      amrmToken.decodeFromUrlString(tokenString);      amrmToken.setService(new Text());      retMap.put(scId,amrmToken);
public synchronized void removeAppFromRegistry(ApplicationId appId){  Map<String,Token<AMRMTokenIdentifier>> subClusterTokenMap=this.appSubClusterTokenMap.get(appId);
  updateBlacklist(blackList,opportContext);  opportContext.addToOutstandingReqs(oppResourceReqs);  Set<String> nodeBlackList=new HashSet<>(opportContext.getBlacklist());  Set<String> allocatedNodes=new HashSet<>();  List<Container> allocatedContainers=new ArrayList<>();  boolean continueLoop=true;  while (continueLoop) {    continueLoop=false;    List<Map<Resource,List<Allocation>>> allocations=new ArrayList<>();    for (    SchedulerRequestKey schedulerKey : opportContext.getOutstandingOpReqs().descendingKeySet()) {      int remAllocs=-1;      int maxAllocationsPerAMHeartbeat=getMaxAllocationsPerAMHeartbeat();      if (maxAllocationsPerAMHeartbeat > 0) {        remAllocs=maxAllocationsPerAMHeartbeat - allocatedContainers.size() - getTotalAllocations(allocations);        if (remAllocs <= 0) {
private Map<Resource,List<Allocation>> allocate(long rmIdentifier,OpportunisticContainerContext appContext,SchedulerRequestKey schedKey,ApplicationAttemptId appAttId,String userName,Set<String> blackList,Set<String> allocatedNodes,int maxAllocations) throws YarnException {  Map<Resource,List<Allocation>> containers=new HashMap<>();  for (  EnrichedResourceRequest enrichedAsk : appContext.getOutstandingOpReqs().get(schedKey).values()) {    int remainingAllocs=-1;    if (maxAllocations > 0) {      int totalAllocated=0;      for (      List<Allocation> allocs : containers.values()) {        totalAllocated+=allocs.size();      }      remainingAllocs=maxAllocations - totalAllocated;      if (remainingAllocs <= 0) {        LOG.info("Not allocating more containers as max allocations per AM " + "heartbeat {} has reached",getMaxAllocationsPerAMHeartbeat());        break;      }    }    allocateContainersInternal(rmIdentifier,appContext.getAppParams(),appContext.getContainerIdGenerator(),blackList,allocatedNodes,appAttId,appContext.getNodeMap(),userName,containers,enrichedAsk,remainingAllocs);    ResourceRequest anyAsk=enrichedAsk.getRequest();    if (!containers.isEmpty()) {
    return;  }  ResourceRequest anyAsk=enrichedAsk.getRequest();  int toAllocate=anyAsk.getNumContainers() - (allocations.isEmpty() ? 0 : allocations.get(anyAsk.getCapability()).size());  toAllocate=Math.min(toAllocate,appParams.getMaxAllocationsPerSchedulerKeyPerRound());  if (maxAllocations >= 0) {    toAllocate=Math.min(maxAllocations,toAllocate);  }  int numAllocated=0;  int loopIndex=OFF_SWITCH_LOOP;  if (enrichedAsk.getNodeMap().size() > 0) {    loopIndex=NODE_LOCAL_LOOP;  }  while (numAllocated < toAllocate) {    Collection<RemoteNode> nodeCandidates=findNodeCandidates(loopIndex,allNodes,blacklist,allocatedNodes,enrichedAsk);    for (    RemoteNode rNode : nodeCandidates) {      String rNodeHost=rNode.getNodeId().getHost();      if (blacklist.contains(rNodeHost)) {
  int loopIndex=OFF_SWITCH_LOOP;  if (enrichedAsk.getNodeMap().size() > 0) {    loopIndex=NODE_LOCAL_LOOP;  }  while (numAllocated < toAllocate) {    Collection<RemoteNode> nodeCandidates=findNodeCandidates(loopIndex,allNodes,blacklist,allocatedNodes,enrichedAsk);    for (    RemoteNode rNode : nodeCandidates) {      String rNodeHost=rNode.getNodeId().getHost();      if (blacklist.contains(rNodeHost)) {        LOG.info("Nodes for scheduling has a blacklisted node" + " [" + rNodeHost + "]..");        continue;      }      String location=ResourceRequest.ANY;      if (loopIndex == NODE_LOCAL_LOOP) {        if (enrichedAsk.getNodeMap().containsKey(rNodeHost)) {          location=rNodeHost;        } else {
      }      String location=ResourceRequest.ANY;      if (loopIndex == NODE_LOCAL_LOOP) {        if (enrichedAsk.getNodeMap().containsKey(rNodeHost)) {          location=rNodeHost;        } else {          continue;        }      } else       if (allocatedNodes.contains(rNodeHost)) {        LOG.info("Opportunistic container has already been allocated on {}.",rNodeHost);        continue;      }      if (loopIndex == RACK_LOCAL_LOOP) {        if (enrichedAsk.getRackMap().containsKey(rNode.getRackName())) {          location=rNode.getRackName();        } else {          continue;        }      }      Container container=createContainer(rmIdentifier,appParams,idCounter,id,userName,allocations,location,anyAsk,rNode);
    SchedulerRequestKey schedulerKey=SchedulerRequestKey.create(request);    Map<Resource,EnrichedResourceRequest> reqMap=outstandingOpReqs.get(schedulerKey);    if (reqMap == null) {      reqMap=new HashMap<>();      outstandingOpReqs.put(schedulerKey,reqMap);    }    EnrichedResourceRequest eReq=reqMap.get(request.getCapability());    if (eReq == null) {      eReq=new EnrichedResourceRequest(request);      reqMap.put(request.getCapability(),eReq);    }    if (ResourceRequest.isAnyLocation(request.getResourceName())) {      eReq.getRequest().setResourceName(ResourceRequest.ANY);      eReq.getRequest().setNumContainers(request.getNumContainers());    } else {      eReq.addLocation(request.getResourceName(),request.getNumContainers());    }    if (ResourceRequest.isAnyLocation(request.getResourceName())) {
@Override public byte[] createPassword(ContainerTokenIdentifier identifier){
protected byte[] retrievePasswordInternal(ContainerTokenIdentifier identifier,MasterKeyData masterKey) throws org.apache.hadoop.security.token.SecretManager.InvalidToken {
@Override protected byte[] createPassword(NMTokenIdentifier identifier){
protected byte[] retrivePasswordInternal(NMTokenIdentifier identifier,MasterKeyData masterKey){
  LOG.warn("Abnormal shutdown of UAMPoolManager, still {} UAMs in map",addressList.size());  for (  final String uamId : addressList) {    completionService.submit(new Callable<KillApplicationResponse>(){      @Override public KillApplicationResponse call() throws Exception {        try {          LOG.info("Force-killing UAM id " + uamId + " for application "+ appIdMap.get(uamId));          return unmanagedAppMasterMap.remove(uamId).forceKillApplication();        } catch (        Exception e) {          LOG.error("Failed to kill unmanaged application master",e);          return null;        }      }    });  }  for (int i=0; i < addressList.size(); ++i) {    try {      Future<KillApplicationResponse> future=completionService.take();      future.get();
public RegisterApplicationMasterResponse registerApplicationMaster(RegisterApplicationMasterRequest request) throws YarnException, IOException {  this.registerRequest=request;  LOG.info("Registering the Unmanaged application master {}",this.applicationId);  RegisterApplicationMasterResponse response=this.rmProxyRelayer.registerApplicationMaster(this.registerRequest);  this.heartbeatHandler.resetLastResponseId();  for (  Container container : response.getContainersFromPreviousAttempts()) {
  SubmitApplicationRequest submitRequest=this.recordFactory.newRecordInstance(SubmitApplicationRequest.class);  ApplicationSubmissionContext context=this.recordFactory.newRecordInstance(ApplicationSubmissionContext.class);  context.setApplicationId(appId);  context.setApplicationName(APP_NAME + "-" + appNameSuffix);  if (StringUtils.isBlank(this.queueName)) {    context.setQueue(this.conf.get(DEFAULT_QUEUE_CONFIG,YarnConfiguration.DEFAULT_QUEUE_NAME));  } else {    context.setQueue(this.queueName);  }  ContainerLaunchContext amContainer=this.recordFactory.newRecordInstance(ContainerLaunchContext.class);  Resource resource=BuilderUtils.newResource(1024,1);  context.setResource(resource);  context.setAMContainerSpec(amContainer);  submitRequest.setApplicationSubmissionContext(context);  context.setUnmanagedAM(true);  context.setKeepContainersAcrossApplicationAttempts(this.keepContainersAcrossApplicationAttempts);
public static Credentials parseCredentials(ContainerLaunchContext launchContext) throws IOException {  Credentials credentials=new Credentials();  ByteBuffer tokens=launchContext.getTokens();  if (tokens != null) {    DataInputByteBuffer buf=new DataInputByteBuffer();    tokens.rewind();    buf.reset(tokens);    credentials.readTokenStorageStream(buf);    if (LOG.isDebugEnabled()) {      for (      Token<? extends TokenIdentifier> tk : credentials.getAllTokens()) {
  } catch (  IllegalArgumentException e) {    puts("Invalid application attempt ID: " + attemptid);    return;  }  UserGroupInformation callerUGI=getCallerUGI();  ApplicationAttemptReport appAttemptReport;  try {    final GetApplicationAttemptReportRequest request=GetApplicationAttemptReportRequest.newInstance(appAttemptId);    if (callerUGI == null) {      appAttemptReport=getApplicationAttemptReport(request);    } else {      appAttemptReport=callerUGI.doAs(new PrivilegedExceptionAction<ApplicationAttemptReport>(){        @Override public ApplicationAttemptReport run() throws Exception {          return getApplicationAttemptReport(request);        }      });    }  } catch (  Exception e) {
  } catch (  Exception e) {    puts("Invalid Application ID: " + aid);    return;  }  UserGroupInformation callerUGI=getCallerUGI();  ApplicationReport appReport;  try {    final GetApplicationReportRequest request=GetApplicationReportRequest.newInstance(appID);    if (callerUGI == null) {      appReport=getApplicationReport(request);    } else {      appReport=callerUGI.doAs(new PrivilegedExceptionAction<ApplicationReport>(){        @Override public ApplicationReport run() throws Exception {          return getApplicationReport(request);        }      });    }  } catch (  Exception e) {
  } catch (  Exception e) {    String message="Failed to read the application " + appID + ".";    LOG.error(message,e);    html.p().__(message).__();    return;  }  if (appReport == null) {    puts("Application not found: " + aid);    return;  }  AppInfo app=new AppInfo(appReport);  setTitle(join("Application ",aid));  Collection<ApplicationAttemptReport> attempts;  try {    final GetApplicationAttemptsRequest request=GetApplicationAttemptsRequest.newInstance(appID);    if (callerUGI == null) {      attempts=getApplicationAttemptsReport(request);
      if (callerUGI == null) {        containerReport=getContainerReport(request);      } else {        containerReport=callerUGI.doAs(new PrivilegedExceptionAction<ContainerReport>(){          @Override public ContainerReport run() throws Exception {            ContainerReport report=null;            if (request.getContainerId() != null) {              try {                report=getContainerReport(request);              } catch (              ContainerNotFoundException ex) {                LOG.warn(ex.getMessage());              }            }            return report;          }        });      }    } catch (    Exception e) {      String message="Failed to read the AM container of the application attempt " + appAttemptReport.getApplicationAttemptId() + ".";
  } catch (  IllegalArgumentException e) {    puts("Invalid container ID: " + containerid);    return;  }  UserGroupInformation callerUGI=getCallerUGI();  ContainerReport containerReport=null;  try {    final GetContainerReportRequest request=GetContainerReportRequest.newInstance(containerId);    if (callerUGI == null) {      containerReport=getContainerReport(request);    } else {      containerReport=callerUGI.doAs(new PrivilegedExceptionAction<ContainerReport>(){        @Override public ContainerReport run() throws Exception {          return getContainerReport(request);        }      });    }  } catch (  Exception e) {
    appInfo=appInfoProvider.getApp(req,builder.getAppId(),clusterId);  } catch (  Exception ex) {    LOG.warn("Could not obtain appInfo object from provider.",ex);    return getContainerLogMeta(builder.build(),false);  }  if (Apps.isApplicationFinalState(appInfo.getAppState())) {    return getContainerLogMeta(builder.build(),false);  }  if (LogWebServiceUtils.isRunningState(appInfo.getAppState())) {    String appOwner=appInfo.getUser();    builder.setAppOwner(appOwner);    WrappedLogMetaRequest request=builder.build();    String nodeHttpAddress=null;    if (nmId != null && !nmId.isEmpty()) {      try {        nodeHttpAddress=getNMWebAddressFromRM(nmId);      } catch (      Exception ex) {
  final long length=LogWebServiceUtils.parseLongParam(size);  ApplicationId appId=containerId.getApplicationAttemptId().getApplicationId();  BasicAppInfo appInfo;  try {    appInfo=appInfoProvider.getApp(req,appId.toString(),clusterId);  } catch (  Exception ex) {    LOG.warn("Could not obtain appInfo object from provider.",ex);    return LogWebServiceUtils.sendStreamOutputResponse(factory,appId,null,null,containerIdStr,filename,format,length,false);  }  String appOwner=appInfo.getUser();  if (Apps.isApplicationFinalState(appInfo.getAppState())) {    return LogWebServiceUtils.sendStreamOutputResponse(factory,appId,appOwner,null,containerIdStr,filename,format,length,false);  }  if (LogWebServiceUtils.isRunningState(appInfo.getAppState())) {    String nodeHttpAddress=null;    if (nmId != null && !nmId.isEmpty()) {      try {
private static void init(){  factory=new LogAggregationFileControllerFactory(yarnConf);  base=JOINER.join(WebAppUtils.getHttpSchemePrefix(yarnConf),WebAppUtils.getTimelineReaderWebAppURLWithoutScheme(yarnConf),RESOURCE_URI_STR_V2);  defaultClusterid=yarnConf.get(YarnConfiguration.RM_CLUSTER_ID,YarnConfiguration.DEFAULT_RM_CLUSTER_ID);
@VisibleForTesting protected TimelineEntity getEntity(String path,MultivaluedMap<String,String> params) throws IOException {  ClientResponse resp=getClient().resource(base).path(path).queryParams(params).accept(MediaType.APPLICATION_JSON).type(MediaType.APPLICATION_JSON).get(ClientResponse.class);  if (resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()) {    String msg="Response from the timeline reader server is " + ((resp == null) ? "null" : "not successful," + " HTTP error code: " + resp.getStatus() + ", Server response:\n"+ resp.getEntity(String.class));
@Override public RegisterApplicationMasterResponse registerApplicationMaster(RegisterApplicationMasterRequest request) throws YarnException, IOException {  validateRunning();  ApplicationAttemptId attemptId=getAppIdentifier();
        for (        ContainerId containerId : applicationContainerIdMap.get(appId)) {          containersFromPreviousAttempt.add(Container.newInstance(containerId,null,null,null,null,null));        }      } else       if (!shouldReRegisterNext) {        throw new InvalidApplicationMasterRequestException(AMRMClientUtils.APP_ALREADY_REGISTERED_MESSAGE);      }    } else {      applicationContainerIdMap.put(appId,new ArrayList<ContainerId>());    }  }  shouldReRegisterNext=false;synchronized (registerSyncObj) {    registerSyncObj.notifyAll();    if (request.getRpcPort() > 1000) {      LOG.info("Register call in RM start waiting");      try {        registerSyncObj.wait();        LOG.info("Register call in RM wait finished");      } catch (      InterruptedException e) {
@Override public FinishApplicationMasterResponse finishApplicationMaster(FinishApplicationMasterRequest request) throws YarnException, IOException {  validateRunning();  ApplicationAttemptId attemptId=getAppIdentifier();
@SuppressWarnings("deprecation") @Override public AllocateResponse allocate(AllocateRequest request) throws YarnException, IOException {  validateRunning();  ApplicationAttemptId attemptId=getAppIdentifier();
  ApplicationAttemptId attemptId=getAppIdentifier();  LOG.info("Allocate from application attempt: " + attemptId);  ApplicationId appId=attemptId.getApplicationId();  if (shouldReRegisterNext) {    String message="AM is not registered, should re-register.";    LOG.warn(message);    throw new ApplicationMasterNotRegisteredException(message);  }synchronized (allocateSyncObj) {    if (shouldWaitForSyncNextAllocate) {      shouldWaitForSyncNextAllocate=false;      LOG.info("Allocate call in RM start waiting");      try {        allocateSyncObj.wait();        LOG.info("Allocate call in RM wait finished");      } catch (      InterruptedException e) {
  } catch (  FederationStateStoreInvalidInputException e) {    Assert.fail(e.getMessage());  }  try {    SubClusterRegisterRequest request=null;    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubClusterRegister Request."));  }  subClusterInfo=null;  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubClusterRegister Request."));  }  subClusterInfo=null;  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterIdNull,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);
    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterIdNull,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterIdInvalid,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {
  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterIdInvalid,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid SubCluster Id information."));
@Test public void testValidateSubClusterRegisterRequestTimestamp(){  SubClusterInfo subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeatNegative,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid timestamp information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTimeNegative,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
@Test public void testValidateSubClusterRegisterRequestAddress(){  SubClusterInfo subClusterInfo=SubClusterInfo.newInstance(subClusterId,addressNull,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,addressEmpty,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,addressEmpty,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressNull,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();
  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressNull,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressEmpty,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);
    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressEmpty,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressNull,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {
  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressNull,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));
  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressNull,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressEmpty,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());
    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressEmpty,rmWebServiceAddress,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Endpoint information."));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,addressNull,lastHeartBeat,stateNew,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();
@Test public void testValidateSubClusterRegisterRequestAddressInvalid(){  SubClusterInfo subClusterInfo=SubClusterInfo.newInstance(subClusterId,addressWrong,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressWrong,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressWrong,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressWrong,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();
  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressWrong,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,addressWrong,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);
    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,rmAdminServiceAddress,addressWrong,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,addressWrongPort,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {
  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,addressWrongPort,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));
  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,addressWrongPort,clientRMServiceAddress,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressWrongPort,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());
    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,addressWrongPort,rmAdminServiceAddress,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().contains("valid host:port authority:"));  }  subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMServiceAddress,clientRMServiceAddress,addressWrongPort,rmWebServiceAddress,lastHeartBeat,stateNull,lastStartTime,capability);  try {    SubClusterRegisterRequest request=SubClusterRegisterRequest.newInstance(subClusterInfo);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();
    FederationMembershipStateStoreInputValidator.validate(request);  } catch (  FederationStateStoreInvalidInputException e) {    Assert.fail(e.getMessage());  }  try {    SubClusterDeregisterRequest request=null;    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubClusterDeregister Request."));  }  try {    SubClusterDeregisterRequest request=SubClusterDeregisterRequest.newInstance(subClusterIdNull,stateLost);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubClusterDeregister Request."));  }  try {    SubClusterDeregisterRequest request=SubClusterDeregisterRequest.newInstance(subClusterIdNull,stateLost);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  try {    SubClusterDeregisterRequest request=SubClusterDeregisterRequest.newInstance(subClusterIdInvalid,stateLost);    FederationMembershipStateStoreInputValidator.validate(request);
  try {    SubClusterDeregisterRequest request=SubClusterDeregisterRequest.newInstance(subClusterIdNull,stateLost);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  try {    SubClusterDeregisterRequest request=SubClusterDeregisterRequest.newInstance(subClusterIdInvalid,stateLost);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid SubCluster Id information."));  }  try {
    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  try {    SubClusterDeregisterRequest request=SubClusterDeregisterRequest.newInstance(subClusterIdInvalid,stateLost);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid SubCluster Id information."));  }  try {    SubClusterDeregisterRequest request=SubClusterDeregisterRequest.newInstance(subClusterId,stateNull);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster State information."));
    FederationMembershipStateStoreInputValidator.validate(request);  } catch (  FederationStateStoreInvalidInputException e) {    Assert.fail(e.getMessage());  }  try {    SubClusterHeartbeatRequest request=null;    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubClusterHeartbeat Request."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterIdNull,lastHeartBeat,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubClusterHeartbeat Request."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterIdNull,lastHeartBeat,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterIdInvalid,lastHeartBeat,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);
  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterIdNull,lastHeartBeat,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterIdInvalid,lastHeartBeat,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid SubCluster Id information."));  }  try {
    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterIdInvalid,lastHeartBeat,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid SubCluster Id information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterId,lastHeartBeat,stateNull,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster State information."));
 catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid SubCluster Id information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterId,lastHeartBeat,stateNull,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster State information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterId,lastHeartBeatNegative,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster State information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterId,lastHeartBeatNegative,stateLost,capability);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Invalid timestamp information."));  }  try {    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterId,lastHeartBeat,stateLost,capabilityNull);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();
    FederationMembershipStateStoreInputValidator.validate(request);  } catch (  FederationStateStoreInvalidInputException e) {    Assert.fail(e.getMessage());  }  try {    GetSubClusterInfoRequest request=null;    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing GetSubClusterInfo Request."));  }  try {    GetSubClusterInfoRequest request=GetSubClusterInfoRequest.newInstance(subClusterIdNull);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {
    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing GetSubClusterInfo Request."));  }  try {    GetSubClusterInfoRequest request=GetSubClusterInfoRequest.newInstance(subClusterIdNull);    FederationMembershipStateStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  try {    GetSubClusterInfoRequest request=GetSubClusterInfoRequest.newInstance(subClusterIdInvalid);    FederationMembershipStateStoreInputValidator.validate(request);
    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    Assert.assertTrue(e.getMessage().startsWith("Missing AddApplicationHomeSubCluster Request."));  }  applicationHomeSubCluster=null;  try {    AddApplicationHomeSubClusterRequest request=AddApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    Assert.assertTrue(e.getMessage().startsWith("Missing ApplicationHomeSubCluster Info."));  }  applicationHomeSubCluster=ApplicationHomeSubCluster.newInstance(appId,subClusterIdNull);  try {    AddApplicationHomeSubClusterRequest request=AddApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);
    AddApplicationHomeSubClusterRequest request=AddApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    Assert.assertTrue(e.getMessage().startsWith("Missing ApplicationHomeSubCluster Info."));  }  applicationHomeSubCluster=ApplicationHomeSubCluster.newInstance(appId,subClusterIdNull);  try {    AddApplicationHomeSubClusterRequest request=AddApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  applicationHomeSubCluster=ApplicationHomeSubCluster.newInstance(appId,subClusterIdInvalid);  try {
    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    Assert.assertTrue(e.getMessage().startsWith("Missing UpdateApplicationHomeSubCluster Request."));  }  applicationHomeSubCluster=null;  try {    UpdateApplicationHomeSubClusterRequest request=UpdateApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    Assert.assertTrue(e.getMessage().startsWith("Missing ApplicationHomeSubCluster Info."));  }  applicationHomeSubCluster=ApplicationHomeSubCluster.newInstance(appId,subClusterIdNull);  try {    UpdateApplicationHomeSubClusterRequest request=UpdateApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);
    UpdateApplicationHomeSubClusterRequest request=UpdateApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    Assert.assertTrue(e.getMessage().startsWith("Missing ApplicationHomeSubCluster Info."));  }  applicationHomeSubCluster=ApplicationHomeSubCluster.newInstance(appId,subClusterIdNull);  try {    UpdateApplicationHomeSubClusterRequest request=UpdateApplicationHomeSubClusterRequest.newInstance(applicationHomeSubCluster);    FederationApplicationHomeSubClusterStoreInputValidator.validate(request);    Assert.fail();  } catch (  FederationStateStoreInvalidInputException e) {    LOG.info(e.getMessage());    Assert.assertTrue(e.getMessage().startsWith("Missing SubCluster Id information."));  }  applicationHomeSubCluster=ApplicationHomeSubCluster.newInstance(appId,subClusterIdInvalid);  try {
@Test public void testRoundRobinSimpleAllocation() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newBuilder().allocationRequestId(1).priority(PRIORITY_NORMAL).resourceName(ResourceRequest.ANY).capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(2).priority(PRIORITY_NORMAL).resourceName(ResourceRequest.ANY).capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(3).priority(PRIORITY_NORMAL).resourceName(ResourceRequest.ANY).capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build());  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h1",1234),"h1:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r1")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"luser");
@Test public void testNodeLocalAllocation() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newBuilder().allocationRequestId(1).priority(PRIORITY_NORMAL).resourceName(ResourceRequest.ANY).capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(2).priority(PRIORITY_NORMAL).resourceName("/r1").capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(2).priority(PRIORITY_NORMAL).resourceName("h1").capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(2).priority(PRIORITY_NORMAL).resourceName(ResourceRequest.ANY).capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(3).priority(PRIORITY_NORMAL).resourceName("/r1").capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(3).priority(PRIORITY_NORMAL).resourceName("h1").capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(3).priority(PRIORITY_NORMAL).resourceName(ResourceRequest.ANY).capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build());  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h1",1234),"h1:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r1")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"luser");
@Test public void testNodeLocalAllocationSameSchedKey() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newBuilder().allocationRequestId(2).numContainers(2).priority(PRIORITY_NORMAL).resourceName("/r1").capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(2).numContainers(2).priority(PRIORITY_NORMAL).resourceName("h1").capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build(),ResourceRequest.newBuilder().allocationRequestId(2).numContainers(2).priority(PRIORITY_NORMAL).resourceName(ResourceRequest.ANY).capability(CAPABILITY_1GB).relaxLocality(true).executionType(ExecutionType.OPPORTUNISTIC).build());  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h1",1234),"h1:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r1")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"luser");
@Test public void testOffSwitchAllocationWhenNoNodeOrRack() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newInstance(PRIORITY_NORMAL,"*",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"h6",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"/r3",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r2"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h5",1234),"h5:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h4",1234),"h4:1234","/r2")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"luser");
@Test public void testMaxAllocationsPerAMHeartbeat() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  allocator.setMaxAllocationsPerAMHeartbeat(2);  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newInstance(PRIORITY_NORMAL,"*",CAPABILITY_1GB,3,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"h6",CAPABILITY_1GB,3,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"/r3",CAPABILITY_1GB,3,true,null,OPPORTUNISTIC_REQ));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r2"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h5",1234),"h5:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h4",1234),"h4:1234","/r2")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"user1");
@Test public void testMaxAllocationsPerAMHeartbeat() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  allocator.setMaxAllocationsPerAMHeartbeat(2);  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newInstance(PRIORITY_NORMAL,"*",CAPABILITY_1GB,3,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"h6",CAPABILITY_1GB,3,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"/r3",CAPABILITY_1GB,3,true,null,OPPORTUNISTIC_REQ));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r2"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h5",1234),"h5:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h4",1234),"h4:1234","/r2")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"user1");  LOG.info("Containers: {}",containers);  Assert.assertEquals(2,containers.size());  containers=allocator.allocateContainers(blacklistRequest,new ArrayList<>(),appAttId,oppCntxt,1L,"user1");
@Test public void testMaxAllocationsPerAMHeartbeatDifferentSchedKey() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  allocator.setMaxAllocationsPerAMHeartbeat(2);  final ExecutionTypeRequest oppRequest=ExecutionTypeRequest.newInstance(ExecutionType.OPPORTUNISTIC,true);  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newInstance(Priority.newInstance(1),"*",CAPABILITY_1GB,1,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(Priority.newInstance(2),"h6",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(Priority.newInstance(3),"/r3",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r2"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h5",1234),"h5:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h4",1234),"h4:1234","/r2")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"user1");
@Test public void testMaxAllocationsPerAMHeartbeatDifferentSchedKey() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  allocator.setMaxAllocationsPerAMHeartbeat(2);  final ExecutionTypeRequest oppRequest=ExecutionTypeRequest.newInstance(ExecutionType.OPPORTUNISTIC,true);  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newInstance(Priority.newInstance(1),"*",CAPABILITY_1GB,1,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(Priority.newInstance(2),"h6",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(Priority.newInstance(3),"/r3",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r2"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h5",1234),"h5:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h4",1234),"h4:1234","/r2")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"user1");  LOG.info("Containers: {}",containers);  Assert.assertEquals(2,containers.size());  containers=allocator.allocateContainers(blacklistRequest,new ArrayList<>(),appAttId,oppCntxt,1L,"user1");
@Test public void testMaxAllocationsPerAMHeartbeatDifferentSchedKey() throws Exception {  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(new ArrayList<>(),new ArrayList<>());  allocator.setMaxAllocationsPerAMHeartbeat(2);  final ExecutionTypeRequest oppRequest=ExecutionTypeRequest.newInstance(ExecutionType.OPPORTUNISTIC,true);  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newInstance(Priority.newInstance(1),"*",CAPABILITY_1GB,1,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(Priority.newInstance(2),"h6",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(Priority.newInstance(3),"/r3",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r2"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h5",1234),"h5:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h4",1234),"h4:1234","/r2")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"user1");  LOG.info("Containers: {}",containers);  Assert.assertEquals(2,containers.size());  containers=allocator.allocateContainers(blacklistRequest,new ArrayList<>(),appAttId,oppCntxt,1L,"user1");  LOG.info("Containers: {}",containers);  Assert.assertEquals(2,containers.size());  containers=allocator.allocateContainers(blacklistRequest,new ArrayList<>(),appAttId,oppCntxt,1L,"user1");
@Test public void testAllocationLatencyMetrics() throws Exception {  oppCntxt=spy(oppCntxt);  OpportunisticSchedulerMetrics metrics=mock(OpportunisticSchedulerMetrics.class);  when(oppCntxt.getOppSchedulerMetrics()).thenReturn(metrics);  ResourceBlacklistRequest blacklistRequest=ResourceBlacklistRequest.newInstance(Collections.emptyList(),Collections.emptyList());  List<ResourceRequest> reqs=Arrays.asList(ResourceRequest.newInstance(PRIORITY_NORMAL,"*",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"h6",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ),ResourceRequest.newInstance(PRIORITY_NORMAL,"/r3",CAPABILITY_1GB,2,true,null,OPPORTUNISTIC_REQ));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  oppCntxt.updateNodeList(Arrays.asList(RemoteNode.newInstance(NodeId.newInstance("h3",1234),"h3:1234","/r2"),RemoteNode.newInstance(NodeId.newInstance("h2",1234),"h2:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h5",1234),"h5:1234","/r1"),RemoteNode.newInstance(NodeId.newInstance("h4",1234),"h4:1234","/r2")));  List<Container> containers=allocator.allocateContainers(blacklistRequest,reqs,appAttId,oppCntxt,1L,"luser");
      try {        launchUAM(attemptId);        registerApplicationMaster(RegisterApplicationMasterRequest.newInstance(null,1001,null),attemptId);      } catch (      Exception e) {        LOG.info("Register thread exception",e);      }    }  });  Object syncObj=MockResourceManagerFacade.getRegisterSyncObj();synchronized (syncObj) {    LOG.info("Starting register thread");    registerAMThread.start();    try {      LOG.info("Test main starts waiting");      syncObj.wait();      LOG.info("Test main wait finished");    } catch (    Exception e) {
    LOG.warn("{} is not active, returning terminated error",containerId);    return ExitCode.TERMINATED.getExitCode();  }  String pid=ProcessIdFileReader.getProcessId(pidPath);  if (pid == null) {    throw new IOException("Unable to determine pid for " + containerId);  }  LOG.info("Reacquiring {} with pid {}",containerId,pid);  ContainerLivenessContext livenessContext=new ContainerLivenessContext.Builder().setContainer(container).setUser(user).setPid(pid).build();  while (isContainerAlive(livenessContext)) {    Thread.sleep(1000);  }  final int sleepMsec=100;  int msecLeft=this.exitCodeFileTimeout;  String exitCodeFile=ContainerLaunch.getExitCodeFile(pidPath.toString());  File file=new File(exitCodeFile);  while (!file.exists() && msecLeft >= 0) {    if (!isContainerActive(containerId)) {
protected void logOutput(String output){  String shExecOutput=output;  if (shExecOutput != null) {    for (    String str : shExecOutput.split("\n")) {
public void cleanupBeforeRelaunch(Container container) throws IOException, InterruptedException {  if (container.getLocalizedResources() != null) {    Map<Path,Path> symLinks=resolveSymLinks(container.getLocalizedResources(),container.getUser());    for (    Map.Entry<Path,Path> symLink : symLinks.entrySet()) {
  InetSocketAddress nmAddr=ctx.getNmAddr();  String user=ctx.getUser();  String appId=ctx.getAppId();  String locId=ctx.getLocId();  LocalDirsHandlerService dirsHandler=ctx.getDirsHandler();  List<String> localDirs=dirsHandler.getLocalDirs();  List<String> logDirs=dirsHandler.getLogDirs();  createUserLocalDirs(localDirs,user);  createUserCacheDirs(localDirs,user);  createAppDirs(localDirs,user,appId);  createAppLogDirs(appId,logDirs,user);  Path appStorageDir=getWorkingDir(localDirs,user,appId);  String tokenFn=String.format(TOKEN_FILE_NAME_FMT,locId);  Path tokenDst=new Path(appStorageDir,tokenFn);  copyFile(nmPrivateContainerTokensPath,tokenDst,user);
  LocalDirsHandlerService dirsHandler=ctx.getDirsHandler();  List<String> localDirs=dirsHandler.getLocalDirs();  List<String> logDirs=dirsHandler.getLogDirs();  createUserLocalDirs(localDirs,user);  createUserCacheDirs(localDirs,user);  createAppDirs(localDirs,user,appId);  createAppLogDirs(appId,logDirs,user);  Path appStorageDir=getWorkingDir(localDirs,user,appId);  String tokenFn=String.format(TOKEN_FILE_NAME_FMT,locId);  Path tokenDst=new Path(appStorageDir,tokenFn);  copyFile(nmPrivateContainerTokensPath,tokenDst,user);  LOG.info("Copying from {} to {}",nmPrivateContainerTokensPath,tokenDst);  FileContext localizerFc=FileContext.getFileContext(lfs.getDefaultFileSystem(),getConf());  localizerFc.setUMask(lfs.getUMask());  localizerFc.setWorkingDirectory(appStorageDir);
  copyFile(nmPrivateTokensPath,tokenDst,user);  if (nmPrivateKeystorePath != null) {    Path keystoreDst=new Path(containerWorkDir,ContainerLaunch.KEYSTORE_FILE);    copyFile(nmPrivateKeystorePath,keystoreDst,user);  }  if (nmPrivateTruststorePath != null) {    Path truststoreDst=new Path(containerWorkDir,ContainerLaunch.TRUSTSTORE_FILE);    copyFile(nmPrivateTruststorePath,truststoreDst,user);  }  Path launchDst=new Path(containerWorkDir,ContainerLaunch.CONTAINER_SCRIPT);  copyFile(nmPrivateContainerScriptPath,launchDst,user);  LocalWrapperScriptBuilder sb=getLocalWrapperScriptBuilder(containerIdStr,containerWorkDir);  if (Shell.WINDOWS && sb.getWrapperScriptPath().toString().length() > WIN_MAX_PATH) {    throw new IOException(String.format("Cannot launch container using script at path %s, because it exceeds " + "the maximum supported path length of %d characters.  Consider " + "configuring shorter directories in %s.",sb.getWrapperScriptPath(),WIN_MAX_PATH,YarnConfiguration.NM_LOCAL_DIRS));  }  Path pidFile=getPidFilePath(containerId);  if (pidFile != null) {    sb.writeLocalWrapperScript(launchDst,pidFile);
  copyFile(nmPrivateContainerScriptPath,launchDst,user);  LocalWrapperScriptBuilder sb=getLocalWrapperScriptBuilder(containerIdStr,containerWorkDir);  if (Shell.WINDOWS && sb.getWrapperScriptPath().toString().length() > WIN_MAX_PATH) {    throw new IOException(String.format("Cannot launch container using script at path %s, because it exceeds " + "the maximum supported path length of %d characters.  Consider " + "configuring shorter directories in %s.",sb.getWrapperScriptPath(),WIN_MAX_PATH,YarnConfiguration.NM_LOCAL_DIRS));  }  Path pidFile=getPidFilePath(containerId);  if (pidFile != null) {    sb.writeLocalWrapperScript(launchDst,pidFile);  } else {    LOG.info("Container {} pid file not set. Returning terminated error",containerIdStr);    return ExitCode.TERMINATED.getExitCode();  }  Shell.CommandExecutor shExec=null;  try {    setScriptExecutable(launchDst,user);    setScriptExecutable(sb.getWrapperScriptPath(),user);    shExec=buildCommandExecutor(sb.getWrapperScriptPath().toString(),containerIdStr,user,pidFile,container.getResource(),new File(containerWorkDir.toUri().getPath()),container.getLaunchContext().getEnvironment());
protected CommandExecutor buildCommandExecutor(String wrapperScriptPath,String containerIdStr,String user,Path pidFile,Resource resource,File workDir,Map<String,String> environment){  String[] command=getRunCommand(wrapperScriptPath,containerIdStr,user,pidFile,this.getConf(),resource);
@Override public boolean signalContainer(ContainerSignalContext ctx) throws IOException {  String user=ctx.getUser();  String pid=ctx.getPid();  Signal signal=ctx.getSignal();
@Override public void deleteAsUser(DeletionAsUserContext ctx) throws IOException, InterruptedException {  Path subDir=ctx.getSubDir();  List<Path> baseDirs=ctx.getBasedirs();  if (baseDirs == null || baseDirs.size() == 0) {
void createUserCacheDirs(List<String> localDirs,String user) throws IOException {
public void delete(DeletionTask deletionTask){  if (debugDelay != -1) {
switch (entry.getValue().cause) {case DISK_FULL:        fullDirs.add(entry.getKey());      break;case OTHER:    errorDirs.add(entry.getKey());  break;default:LOG.warn(entry.getValue().cause + " is unknown for disk error.");break;}directoryErrorInfo.put(entry.getKey(),errorInformation);if (preCheckGoodDirs.contains(dir)) {LOG.warn("Directory " + dir + " error, "+ errorInformation.message+ ", removing from list of valid directories");setChanged=true;numFailures++;}}for (String dir : allLocalDirs) {if (!dirsFailedCheck.containsKey(dir)) {localDirs.add(dir);
@Override public void init(Context context) throws IOException {  Configuration conf=super.getConf();  this.nmContext=context;  try {    PrivilegedOperation checkSetupOp=new PrivilegedOperation(PrivilegedOperation.OperationType.CHECK_SETUP);    PrivilegedOperationExecutor privilegedOperationExecutor=getPrivilegedOperationExecutor();    privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,false);  } catch (  PrivilegedOperationException e) {    int exitCode=e.getExitCode();    LOG.warn("Exit code from container executor initialization is : {}",exitCode,e);    throw new IOException("Linux container executor not configured properly" + " (error=" + exitCode + ")",e);  }  try {    resourceHandlerChain=ResourceHandlerModule.getConfiguredResourceHandlerChain(conf,nmContext);    LOG.debug("Resource handler chain enabled = {}",(resourceHandlerChain != null));    if (resourceHandlerChain != null) {
  try {    PrivilegedOperation checkSetupOp=new PrivilegedOperation(PrivilegedOperation.OperationType.CHECK_SETUP);    PrivilegedOperationExecutor privilegedOperationExecutor=getPrivilegedOperationExecutor();    privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,false);  } catch (  PrivilegedOperationException e) {    int exitCode=e.getExitCode();    LOG.warn("Exit code from container executor initialization is : {}",exitCode,e);    throw new IOException("Linux container executor not configured properly" + " (error=" + exitCode + ")",e);  }  try {    resourceHandlerChain=ResourceHandlerModule.getConfiguredResourceHandlerChain(conf,nmContext);    LOG.debug("Resource handler chain enabled = {}",(resourceHandlerChain != null));    if (resourceHandlerChain != null) {      LOG.debug("Bootstrapping resource handler chain: {}",resourceHandlerChain);      resourceHandlerChain.bootstrap(conf);    }  } catch (  ResourceHandlerException e) {
    throw new IOException("Linux container executor not configured properly" + " (error=" + exitCode + ")",e);  }  try {    resourceHandlerChain=ResourceHandlerModule.getConfiguredResourceHandlerChain(conf,nmContext);    LOG.debug("Resource handler chain enabled = {}",(resourceHandlerChain != null));    if (resourceHandlerChain != null) {      LOG.debug("Bootstrapping resource handler chain: {}",resourceHandlerChain);      resourceHandlerChain.bootstrap(conf);    }  } catch (  ResourceHandlerException e) {    LOG.error("Failed to bootstrap configured resource subsystems! ",e);    throw new IOException("Failed to bootstrap configured resource subsystems!");  }  try {    if (linuxContainerRuntime == null) {      LinuxContainerRuntime runtime=new DelegatingLinuxContainerRuntime();      runtime.initialize(conf,nmContext);      this.linuxContainerRuntime=runtime;
        resourceOps.add(new PrivilegedOperation(PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP,resourcesOptions));        for (        PrivilegedOperation op : ops) {switch (op.getOperationType()) {case ADD_PID_TO_CGROUP:            resourceOps.add(op);          break;case TC_MODIFY_STATE:        tcCommandFile=op.getArguments().get(0);      break;case ADD_NUMA_PARAMS:    numaArgs=op.getArguments();  break;default:LOG.warn("PrivilegedOperation type unsupported in launch: {}",op.getOperationType());}}if (resourceOps.size() > 1) {try {PrivilegedOperation operation=PrivilegedOperationExecutor.squashCGroupOperations(resourceOps);resourcesOptions=operation.getArguments().get(0);} catch (PrivilegedOperationException e) {
case ADD_PID_TO_CGROUP:            resourceOps.add(op);          break;case TC_MODIFY_STATE:        tcCommandFile=op.getArguments().get(0);      break;case ADD_NUMA_PARAMS:    numaArgs=op.getArguments();  break;default:LOG.warn("PrivilegedOperation type unsupported in launch: {}",op.getOperationType());}}if (resourceOps.size() > 1) {try {PrivilegedOperation operation=PrivilegedOperationExecutor.squashCGroupOperations(resourceOps);resourcesOptions=operation.getArguments().get(0);} catch (PrivilegedOperationException e) {LOG.error("Failed to squash cgroup operations!",e);throw new ResourceHandlerException("Failed to squash cgroup operations!");}}}}} catch (ResourceHandlerException e) {
@Override public void deleteAsUser(DeletionAsUserContext ctx){  String user=ctx.getUser();  Path dir=ctx.getSubDir();  List<Path> baseDirs=ctx.getBasedirs();  verifyUsernamePattern(user);  String runAsUser=getRunAsUser(user);  String dirString=dir == null ? "" : dir.toUri().getPath();  PrivilegedOperation deleteAsUserOp=new PrivilegedOperation(PrivilegedOperation.OperationType.DELETE_AS_USER,(String)null);  deleteAsUserOp.appendArgs(runAsUser,user,Integer.toString(PrivilegedOperation.RunAsUserCommand.DELETE_AS_USER.getValue()),dirString);  List<String> pathsToDelete=new ArrayList<String>();  if (baseDirs == null || baseDirs.size() == 0) {
  String user=ctx.getUser();  Path dir=ctx.getSubDir();  List<Path> baseDirs=ctx.getBasedirs();  verifyUsernamePattern(user);  String runAsUser=getRunAsUser(user);  String dirString=dir == null ? "" : dir.toUri().getPath();  PrivilegedOperation deleteAsUserOp=new PrivilegedOperation(PrivilegedOperation.OperationType.DELETE_AS_USER,(String)null);  deleteAsUserOp.appendArgs(runAsUser,user,Integer.toString(PrivilegedOperation.RunAsUserCommand.DELETE_AS_USER.getValue()),dirString);  List<String> pathsToDelete=new ArrayList<String>();  if (baseDirs == null || baseDirs.size() == 0) {    LOG.info("Deleting absolute path : {}",dir);    pathsToDelete.add(dirString);  } else {    for (    Path baseDir : baseDirs) {      Path del=dir == null ? baseDir : new Path(baseDir,dir);
  if (baseDirs == null || baseDirs.size() == 0) {    LOG.info("Deleting absolute path : {}",dir);    pathsToDelete.add(dirString);  } else {    for (    Path baseDir : baseDirs) {      Path del=dir == null ? baseDir : new Path(baseDir,dir);      LOG.info("Deleting path : {}",del);      pathsToDelete.add(del.toString());      deleteAsUserOp.appendArgs(baseDir.toUri().getPath());    }  }  try {    Configuration conf=super.getConf();    PrivilegedOperationExecutor privilegedOperationExecutor=getPrivilegedOperationExecutor();    privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp,false);  } catch (  PrivilegedOperationException e) {    int exitCode=e.getExitCode();
@Override protected File[] readDirAsUser(String user,Path dir){  List<File> files=new ArrayList<>();  PrivilegedOperation listAsUserOp=new PrivilegedOperation(PrivilegedOperation.OperationType.LIST_AS_USER,(String)null);  String runAsUser=getRunAsUser(user);  String dirString="";  if (dir != null) {    dirString=dir.toUri().getPath();  }  listAsUserOp.appendArgs(runAsUser,user,Integer.toString(PrivilegedOperation.RunAsUserCommand.LIST_AS_USER.getValue()),dirString);  try {    PrivilegedOperationExecutor privOpExecutor=getPrivilegedOperationExecutor();    String results=privOpExecutor.executePrivilegedOperation(listAsUserOp,true);    for (    String file : results.split("\n")) {      if (!file.startsWith("main :")) {        files.add(new File(new File(dirString),file));      }    }  } catch (  PrivilegedOperationException e) {
public void removeDockerContainer(String containerId){  try {    PrivilegedOperationExecutor privOpExecutor=PrivilegedOperationExecutor.getInstance(super.getConf());    if (DockerCommandExecutor.isRemovable(DockerCommandExecutor.getContainerStatus(containerId,privOpExecutor,nmContext))) {
@VisibleForTesting void postComplete(final ContainerId containerId){  try {    if (resourceHandlerChain != null) {
private void logDiskStatus(boolean newDiskFailure,boolean diskTurnedGood){  if (newDiskFailure) {    String report=getDisksHealthReport(false);
public static void logSuccess(String user,String operation,String target,ApplicationId appId,ContainerId containerId){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target){  if (LOG.isInfoEnabled()) {
private void stopRecoveryStore() throws IOException {  if (null != nmStore) {    nmStore.stop();    if (null != context) {      if (context.getDecommissioned() && nmStore.canRecover()) {        LOG.info("Removing state store due to decommission");        Configuration conf=getConfig();        Path recoveryRoot=new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));
  if (this.resyncingWithRM.getAndSet(true)) {  } else {    new Thread(){      @Override public void run(){        try {          if (!rmWorkPreservingRestartEnabled) {            LOG.info("Cleaning up running containers on resync");            containerManager.cleanupContainersOnNMResync();            if (context.getKnownCollectors() != null) {              context.getKnownCollectors().clear();            }          } else {            LOG.info("Preserving containers on resync");            reregisterCollectors();          }          ((NodeStatusUpdaterImpl)nodeStatusUpdater).rebootNodeStatusUpdaterAndRegisterWithRM();        } catch (        YarnRuntimeException e) {
private void initAndStartNodeManager(Configuration conf,boolean hasToReboot){  try {    if (!Shell.WINDOWS) {      if (!Shell.checkIsBashSupported()) {        String message="Failing NodeManager start since we're on a " + "Unix-based system but bash doesn't seem to be available.";
private void initAndStartNodeManager(Configuration conf,boolean hasToReboot){  try {    if (!Shell.WINDOWS) {      if (!Shell.checkIsBashSupported()) {        String message="Failing NodeManager start since we're on a " + "Unix-based system but bash doesn't seem to be available.";        LOG.error(message);        throw new YarnRuntimeException(message);      }    }    if (hasToReboot && null != nodeManagerShutdownHook) {      ShutdownHookManager.get().removeShutdownHook(nodeManagerShutdownHook);    }    nodeManagerShutdownHook=new CompositeServiceShutdownHook(this);    ShutdownHookManager.get().addShutdownHook(nodeManagerShutdownHook,SHUTDOWN_HOOK_PRIORITY);    this.shouldExitOnShutdownEvent=true;    this.init(conf);    this.start();  } catch (  Throwable t) {
@Override protected void serviceInit(Configuration conf) throws Exception {  this.totalResource=NodeManagerHardwareUtils.getNodeResources(conf);  long memoryMb=totalResource.getMemorySize();  float vMemToPMem=conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO,YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);  long virtualMemoryMb=(long)Math.ceil(memoryMb * vMemToPMem);  int virtualCores=totalResource.getVirtualCores();  updateConfiguredResourcesViaPlugins(totalResource);
  long physicalMemoryMb=memoryMb;  int physicalCores=virtualCores;  ResourceCalculatorPlugin rcp=ResourceCalculatorPlugin.getNodeResourceMonitorPlugin(conf);  if (rcp != null) {    physicalMemoryMb=rcp.getPhysicalMemorySize() / (1024 * 1024);    physicalCores=rcp.getNumProcessors();  }  this.physicalResource=Resource.newInstance(physicalMemoryMb,physicalCores);  this.tokenKeepAliveEnabled=isTokenKeepAliveEnabled(conf);  this.tokenRemovalDelayMs=conf.getInt(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);  this.minimumResourceManagerVersion=conf.get(YarnConfiguration.NM_RESOURCEMANAGER_MINIMUM_VERSION,YarnConfiguration.DEFAULT_NM_RESOURCEMANAGER_MINIMUM_VERSION);  nodeLabelsHandler=createNMNodeLabelsHandler(nodeLabelsProvider);  nodeAttributesHandler=createNMNodeAttributesHandler(nodeAttributesProvider);  durationToTrackStoppedContainers=conf.getLong(YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS,600000);  if (durationToTrackStoppedContainers < 0) {    String message="Invalid configuration for " + YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS + " default "+ "value is 10Min(600000).";
synchronized (shutdownMonitor) {    if (this.isStopped) {      LOG.info("Currently being shutdown. Aborting reboot");      return;    }    this.isStopped=true;    sendOutofBandHeartBeat();    try {      statusUpdater.join();      registerWithRM();      statusUpdater=new Thread(statusUpdaterRunnable,"Node Status Updater");      this.isStopped=false;      statusUpdater.start();      LOG.info("NodeStatusUpdater thread is reRegistered and restarted");    } catch (    Exception e) {      String errorMessage="Unexpected error rebooting NodeStatusUpdater";
@VisibleForTesting protected void registerWithRM() throws YarnException, IOException {  RegisterNodeManagerResponse regNMResponse;  Set<NodeLabel> nodeLabels=nodeLabelsHandler.getNodeLabelsForRegistration();  Set<NodeAttribute> nodeAttributes=nodeAttributesHandler.getNodeAttributesForRegistration();synchronized (this.context) {    List<NMContainerStatus> containerReports=getNMContainerStatuses();    NodeStatus nodeStatus=getNodeStatus(0);    RegisterNodeManagerRequest request=RegisterNodeManagerRequest.newInstance(nodeId,httpPort,totalResource,nodeManagerVersionId,containerReports,getRunningApplications(),nodeLabels,physicalResource,nodeAttributes,nodeStatus);    if (containerReports != null && !containerReports.isEmpty()) {
      throw new YarnRuntimeException("Shutting down the Node Manager. " + message);    }    if (VersionUtil.compareVersions(rmVersion,minimumResourceManagerVersion) < 0) {      String message="The Resource Manager's version (" + rmVersion + ") is less than the minimum "+ "allowed version "+ minimumResourceManagerVersion;      throw new YarnRuntimeException("Shutting down the Node Manager on RM " + "version error, " + message);    }  }  this.registeredWithRM=true;  MasterKey masterKey=regNMResponse.getContainerTokenMasterKey();  if (masterKey != null) {    this.context.getContainerTokenSecretManager().setMasterKey(masterKey);  }  masterKey=regNMResponse.getNMTokenMasterKey();  if (masterKey != null) {    this.context.getNMTokenSecretManager().setMasterKey(masterKey);  }  StringBuilder successfullRegistrationMsg=new StringBuilder();  successfullRegistrationMsg.append("Registered with ResourceManager as ").append(this.nodeId);  Resource newResource=regNMResponse.getResource();  if (newResource != null) {
@VisibleForTesting protected NodeStatus getNodeStatus(int responseId) throws IOException {  NodeHealthStatus nodeHealthStatus=this.context.getNodeHealthStatus();  nodeHealthStatus.setHealthReport(healthChecker.getHealthReport());  nodeHealthStatus.setIsNodeHealthy(healthChecker.isHealthy());  nodeHealthStatus.setLastHealthReportTime(healthChecker.getLastHealthReportTime());
@VisibleForTesting protected List<ContainerStatus> getContainerStatuses() throws IOException {  List<ContainerStatus> containerStatuses=new ArrayList<ContainerStatus>();  for (  Container container : this.context.getContainers().values()) {    ContainerId containerId=container.getContainerId();    ApplicationId applicationId=containerId.getApplicationAttemptId().getApplicationId();    org.apache.hadoop.yarn.api.records.ContainerStatus containerStatus=container.cloneAndGetContainerStatus();    if (containerStatus.getState() == ContainerState.COMPLETE) {      if (isApplicationStopped(applicationId)) {
    ContainerId containerId=container.getContainerId();    ApplicationId applicationId=containerId.getApplicationAttemptId().getApplicationId();    org.apache.hadoop.yarn.api.records.ContainerStatus containerStatus=container.cloneAndGetContainerStatus();    if (containerStatus.getState() == ContainerState.COMPLETE) {      if (isApplicationStopped(applicationId)) {        LOG.debug("{} is completing, remove {} from NM context.",applicationId,containerId);        context.getContainers().remove(containerId);        pendingCompletedContainers.put(containerId,containerStatus);      } else {        if (!isContainerRecentlyStopped(containerId)) {          pendingCompletedContainers.put(containerId,containerStatus);        }      }      addCompletedContainer(containerId);    } else {      containerStatuses.add(containerStatus);    }  }  containerStatuses.addAll(pendingCompletedContainers.values());
synchronized (recentlyStoppedContainers) {    long currentTime=System.currentTimeMillis();    Iterator<Entry<ContainerId,Long>> i=recentlyStoppedContainers.entrySet().iterator();    while (i.hasNext()) {      Entry<ContainerId,Long> mapEntry=i.next();      ContainerId cid=mapEntry.getKey();      if (mapEntry.getValue() >= currentTime) {        break;      }      if (!context.getContainers().containsKey(cid)) {        ApplicationId appId=cid.getApplicationAttemptId().getApplicationId();        if (isApplicationStopped(appId)) {          i.remove();          try {            context.getNMStateStore().removeContainer(cid);          } catch (          IOException e) {
@Override protected String[] getRunCommand(String command,String groupId,String userName,Path pidFile,Configuration conf){  File f=new File(command);  if (LOG.isDebugEnabled()) {
@Override protected void copyFile(Path src,Path dst,String owner) throws IOException {
@Override protected void createDir(Path dirPath,FsPermission perms,boolean createParent,String owner) throws IOException {  perms=new FsPermission(DIR_PERM);
@Override protected void setScriptExecutable(Path script,String owner) throws IOException {
@Override public Path localizeClasspathJar(Path jarPath,Path target,String owner) throws IOException {
  String appId=ctx.getAppId();  String locId=ctx.getLocId();  LocalDirsHandlerService dirsHandler=ctx.getDirsHandler();  List<String> localDirs=dirsHandler.getLocalDirs();  List<String> logDirs=dirsHandler.getLogDirs();  Path classpathJarPrivateDir=dirsHandler.getLocalPathForWrite(ResourceLocalizationService.NM_PRIVATE_DIR);  createUserLocalDirs(localDirs,user);  createUserCacheDirs(localDirs,user);  createAppDirs(localDirs,user,appId);  createAppLogDirs(appId,logDirs,user);  Path appStorageDir=getWorkingDir(localDirs,user,appId);  String tokenFn=String.format(ContainerExecutor.TOKEN_FILE_NAME_FMT,locId);  Path tokenDst=new Path(appStorageDir,tokenFn);  copyFile(nmPrivateContainerTokensPath,tokenDst,user);  File cwdApp=new File(appStorageDir.toString());
public void recover() throws IOException {  LOG.info("Recovering AMRMProxyService");  RecoveredAMRMProxyState state=this.nmContext.getNMStateStore().loadAMRMProxyState();  this.secretManager.recover(state);  LOG.info("Recovering {} running applications for AMRMProxy",state.getAppContexts().size());  for (  Map.Entry<ApplicationAttemptId,Map<String,byte[]>> entry : state.getAppContexts().entrySet()) {    ApplicationAttemptId attemptId=entry.getKey();
      Token<AMRMTokenIdentifier> amrmToken=null;      for (      Map.Entry<String,byte[]> contextEntry : entry.getValue().entrySet()) {        if (contextEntry.getKey().equals(NMSS_USER_KEY)) {          user=new String(contextEntry.getValue(),"UTF-8");        } else         if (contextEntry.getKey().equals(NMSS_AMRMTOKEN_KEY)) {          amrmToken=new Token<>();          amrmToken.decodeFromUrlString(new String(contextEntry.getValue(),"UTF-8"));          amrmToken.setService(new Text());        }      }      if (amrmToken == null) {        throw new IOException("No amrmToken found for app attempt " + attemptId);      }      if (user == null) {        throw new IOException("No user found for app attempt " + attemptId);      }      Token<AMRMTokenIdentifier> localToken=this.secretManager.createAndGetAMRMToken(attemptId);      Credentials amCred=null;      for (      Container container : this.nmContext.getContainers().values()) {
        } else         if (contextEntry.getKey().equals(NMSS_AMRMTOKEN_KEY)) {          amrmToken=new Token<>();          amrmToken.decodeFromUrlString(new String(contextEntry.getValue(),"UTF-8"));          amrmToken.setService(new Text());        }      }      if (amrmToken == null) {        throw new IOException("No amrmToken found for app attempt " + attemptId);      }      if (user == null) {        throw new IOException("No user found for app attempt " + attemptId);      }      Token<AMRMTokenIdentifier> localToken=this.secretManager.createAndGetAMRMToken(attemptId);      Credentials amCred=null;      for (      Container container : this.nmContext.getContainers().values()) {        LOG.debug("From NM Context container {}",container.getContainerId());        if (container.getContainerId().getApplicationAttemptId().equals(attemptId) && container.getContainerTokenIdentifier() != null) {          LOG.debug("Container type {}",container.getContainerTokenIdentifier().getContainerType());          if (container.getContainerTokenIdentifier().getContainerType() == ContainerType.APPLICATION_MASTER) {
          amrmToken.decodeFromUrlString(new String(contextEntry.getValue(),"UTF-8"));          amrmToken.setService(new Text());        }      }      if (amrmToken == null) {        throw new IOException("No amrmToken found for app attempt " + attemptId);      }      if (user == null) {        throw new IOException("No user found for app attempt " + attemptId);      }      Token<AMRMTokenIdentifier> localToken=this.secretManager.createAndGetAMRMToken(attemptId);      Credentials amCred=null;      for (      Container container : this.nmContext.getContainers().values()) {        LOG.debug("From NM Context container {}",container.getContainerId());        if (container.getContainerId().getApplicationAttemptId().equals(attemptId) && container.getContainerTokenIdentifier() != null) {          LOG.debug("Container type {}",container.getContainerTokenIdentifier().getContainerType());          if (container.getContainerTokenIdentifier().getContainerType() == ContainerType.APPLICATION_MASTER) {            LOG.info("AM container {} found in context, has credentials: {}",container.getContainerId(),(container.getCredentials() != null));            amCred=container.getCredentials();
      }      if (amrmToken == null) {        throw new IOException("No amrmToken found for app attempt " + attemptId);      }      if (user == null) {        throw new IOException("No user found for app attempt " + attemptId);      }      Token<AMRMTokenIdentifier> localToken=this.secretManager.createAndGetAMRMToken(attemptId);      Credentials amCred=null;      for (      Container container : this.nmContext.getContainers().values()) {        LOG.debug("From NM Context container {}",container.getContainerId());        if (container.getContainerId().getApplicationAttemptId().equals(attemptId) && container.getContainerTokenIdentifier() != null) {          LOG.debug("Container type {}",container.getContainerTokenIdentifier().getContainerType());          if (container.getContainerTokenIdentifier().getContainerType() == ContainerType.APPLICATION_MASTER) {            LOG.info("AM container {} found in context, has credentials: {}",container.getContainerId(),(container.getCredentials() != null));            amCred=container.getCredentials();          }        }      }      if (amCred == null) {        LOG.error("No credentials found for AM container of {}. " + "Yarn registry access might not work",attemptId);
@Override public RegisterApplicationMasterResponse registerApplicationMaster(RegisterApplicationMasterRequest request) throws YarnException, IOException {  long startTime=clock.getTime();  try {    RequestInterceptorChainWrapper pipeline=authorizeAndGetInterceptorChain();
@Override public RegisterApplicationMasterResponse registerApplicationMaster(RegisterApplicationMasterRequest request) throws YarnException, IOException {  long startTime=clock.getTime();  try {    RequestInterceptorChainWrapper pipeline=authorizeAndGetInterceptorChain();    LOG.info("Registering application master." + " Host:" + request.getHost() + " Port:"+ request.getRpcPort()+ " Tracking Url:"+ request.getTrackingUrl()+ " for application "+ pipeline.getApplicationAttemptId());    RegisterApplicationMasterResponse response=pipeline.getRootInterceptor().registerApplicationMaster(request);    long endTime=clock.getTime();    this.metrics.succeededRegisterAMRequests(endTime - startTime);
@Override public FinishApplicationMasterResponse finishApplicationMaster(FinishApplicationMasterRequest request) throws YarnException, IOException {  long startTime=clock.getTime();  try {    RequestInterceptorChainWrapper pipeline=authorizeAndGetInterceptorChain();
@Override public FinishApplicationMasterResponse finishApplicationMaster(FinishApplicationMasterRequest request) throws YarnException, IOException {  long startTime=clock.getTime();  try {    RequestInterceptorChainWrapper pipeline=authorizeAndGetInterceptorChain();    LOG.info("Finishing application master for {}. Tracking Url: {}",pipeline.getApplicationAttemptId(),request.getTrackingUrl());    FinishApplicationMasterResponse response=pipeline.getRootInterceptor().finishApplicationMaster(request);    long endTime=clock.getTime();    this.metrics.succeededFinishAMRequests(endTime - startTime);
@Override public AllocateResponse allocate(AllocateRequest request) throws YarnException, IOException {  long startTime=clock.getTime();  try {    AMRMTokenIdentifier amrmTokenIdentifier=YarnServerSecurityUtils.authorizeRequest();    RequestInterceptorChainWrapper pipeline=getInterceptorChain(amrmTokenIdentifier);    AllocateResponse allocateResponse=pipeline.getRootInterceptor().allocate(request);    updateAMRMTokens(amrmTokenIdentifier,pipeline,allocateResponse);    long endTime=clock.getTime();    this.metrics.succeededAllocateRequests(endTime - startTime);
protected void initializePipeline(ApplicationAttemptId applicationAttemptId,String user,Token<AMRMTokenIdentifier> amrmToken,Token<AMRMTokenIdentifier> localToken,Map<String,byte[]> recoveredDataMap,boolean isRecovery,Credentials credentials){  RequestInterceptorChainWrapper chainWrapper=null;synchronized (applPipelineMap) {    if (applPipelineMap.containsKey(applicationAttemptId.getApplicationId())) {      LOG.warn("Request to start an already existing appId was received. " + " This can happen if an application failed and a new attempt " + "was created on this machine.  ApplicationId: "+ applicationAttemptId.toString());      RequestInterceptorChainWrapper chainWrapperBackup=this.applPipelineMap.get(applicationAttemptId.getApplicationId());      if (chainWrapperBackup != null && chainWrapperBackup.getApplicationAttemptId() != null && !chainWrapperBackup.getApplicationAttemptId().equals(applicationAttemptId)) {
          LOG.warn("Failed to shutdown the request processing pipeline for app:" + applicationAttemptId.getApplicationId(),ex);        }      } else {        return;      }    }    chainWrapper=new RequestInterceptorChainWrapper();    this.applPipelineMap.put(applicationAttemptId.getApplicationId(),chainWrapper);  }  LOG.info("Initializing request processing pipeline for application. " + " ApplicationId:" + applicationAttemptId + " for the user: "+ user);  try {    RequestInterceptor interceptorChain=this.createRequestInterceptorChain();    interceptorChain.init(createApplicationMasterContext(this.nmContext,applicationAttemptId,user,amrmToken,localToken,credentials,this.registry));    if (isRecovery) {      if (recoveredDataMap == null) {        throw new YarnRuntimeException("null recoveredDataMap recieved for recover");      }      interceptorChain.recover(recoveredDataMap);    }    chainWrapper.init(interceptorChain,applicationAttemptId);    if (!isRecovery && this.nmContext.getNMStateStore() != null) {
protected void stopApplication(ApplicationId applicationId){  Preconditions.checkArgument(applicationId != null,"applicationId is null");  RequestInterceptorChainWrapper pipeline=this.applPipelineMap.remove(applicationId);  if (pipeline == null) {
  Preconditions.checkArgument(applicationId != null,"applicationId is null");  RequestInterceptorChainWrapper pipeline=this.applPipelineMap.remove(applicationId);  if (pipeline == null) {    LOG.info("No interceptor pipeline for application {}," + " likely because its AM is not run in this node.",applicationId);  } else {    this.secretManager.applicationMasterFinished(pipeline.getApplicationAttemptId());    LOG.info("Stopping the request processing pipeline for application: " + applicationId);    try {      pipeline.getRootInterceptor().shutdown();    } catch (    Throwable ex) {      LOG.warn("Failed to shutdown the request processing pipeline for app:" + applicationId,ex);    }    if (this.nmContext.getNMStateStore() != null) {      try {        this.nmContext.getNMStateStore().removeAMRMProxyAppContext(pipeline.getApplicationAttemptId());      } catch (      IOException e) {
  AMRMProxyApplicationContextImpl context=(AMRMProxyApplicationContextImpl)pipeline.getRootInterceptor().getApplicationContext();  if (allocateResponse.getAMRMToken() != null) {    LOG.info("RM rolled master-key for amrm-tokens");    org.apache.hadoop.yarn.api.records.Token token=allocateResponse.getAMRMToken();    allocateResponse.setAMRMToken(null);    org.apache.hadoop.security.token.Token<AMRMTokenIdentifier> newToken=ConverterUtils.convertFromYarn(token,(Text)null);    if (context.setAMRMToken(newToken) && this.nmContext.getNMStateStore() != null) {      try {        this.nmContext.getNMStateStore().storeAMRMProxyAppContextEntry(context.getApplicationAttemptId(),NMSS_AMRMTOKEN_KEY,newToken.encodeToUrlString().getBytes("UTF-8"));      } catch (      IOException e) {        LOG.error("Error storing AMRMProxy application context entry for " + context.getApplicationAttemptId(),e);      }    }  }  MasterKeyData nextMasterKey=this.secretManager.getNextMasterKeyData();  if (nextMasterKey != null && nextMasterKey.getMasterKey().getKeyId() != amrmTokenIdentifier.getKeyId()) {    Token<AMRMTokenIdentifier> localToken=context.getLocalAMRMToken();    if (nextMasterKey.getMasterKey().getKeyId() != context.getLocalAMRMTokenKeyId()) {
public void applicationMasterFinished(ApplicationAttemptId appAttemptId){  this.writeLock.lock();  try {
public Token<AMRMTokenIdentifier> createAndGetAMRMToken(ApplicationAttemptId appAttemptId){  this.writeLock.lock();  try {
@Override public byte[] retrievePassword(AMRMTokenIdentifier identifier) throws InvalidToken {  this.readLock.lock();  try {    ApplicationAttemptId applicationAttemptId=identifier.getApplicationAttemptId();
@Override @Private protected byte[] createPassword(AMRMTokenIdentifier identifier){  this.readLock.lock();  try {    ApplicationAttemptId applicationAttemptId=identifier.getApplicationAttemptId();
      appOwner.addCredentials(appContext.getCredentials());    }  }  this.attemptId=appContext.getApplicationAttemptId();  ApplicationId appId=this.attemptId.getApplicationId();  this.homeSubClusterId=SubClusterId.newInstance(YarnConfiguration.getClusterId(conf));  this.homeRMRelayer=new AMRMClientRelayer(createHomeRMProxy(appContext,ApplicationMasterProtocol.class,appOwner),appId,this.homeSubClusterId.toString());  this.homeHeartbeartHandler=createHomeHeartbeartHandler(conf,appId,this.homeRMRelayer);  this.homeHeartbeartHandler.setUGI(appOwner);  this.homeHeartbeartHandler.setDaemon(true);  this.homeHeartbeartHandler.start();  this.lastAllocateResponse=RECORD_FACTORY.newRecordInstance(AllocateResponse.class);  this.lastAllocateResponse.setResponseId(AMRMClientUtils.PRE_REGISTER_RESPONSE_ID);  this.federationFacade=FederationStateStoreFacade.getInstance();  this.subClusterResolver=this.federationFacade.getSubClusterResolver();  this.policyInterpreter=null;  this.uamPool.init(conf);
  if (recoveredDataMap == null) {    return;  }  try {    if (recoveredDataMap.containsKey(NMSS_REG_REQUEST_KEY)) {      RegisterApplicationMasterRequestProto pb=RegisterApplicationMasterRequestProto.parseFrom(recoveredDataMap.get(NMSS_REG_REQUEST_KEY));      this.amRegistrationRequest=new RegisterApplicationMasterRequestPBImpl(pb);      LOG.info("amRegistrationRequest recovered for {}",this.attemptId);      this.homeRMRelayer.setAMRegistrationRequest(this.amRegistrationRequest);    }    if (recoveredDataMap.containsKey(NMSS_REG_RESPONSE_KEY)) {      RegisterApplicationMasterResponseProto pb=RegisterApplicationMasterResponseProto.parseFrom(recoveredDataMap.get(NMSS_REG_RESPONSE_KEY));      this.amRegistrationResponse=new RegisterApplicationMasterResponsePBImpl(pb);      LOG.info("amRegistrationResponse recovered for {}",this.attemptId);    }    Map<String,Token<AMRMTokenIdentifier>> uamMap;    if (this.registryClient != null) {      uamMap=this.registryClient.loadStateFromRegistry(this.attemptId.getApplicationId());
    }    if (recoveredDataMap.containsKey(NMSS_REG_RESPONSE_KEY)) {      RegisterApplicationMasterResponseProto pb=RegisterApplicationMasterResponseProto.parseFrom(recoveredDataMap.get(NMSS_REG_RESPONSE_KEY));      this.amRegistrationResponse=new RegisterApplicationMasterResponsePBImpl(pb);      LOG.info("amRegistrationResponse recovered for {}",this.attemptId);    }    Map<String,Token<AMRMTokenIdentifier>> uamMap;    if (this.registryClient != null) {      uamMap=this.registryClient.loadStateFromRegistry(this.attemptId.getApplicationId());      LOG.info("Found {} existing UAMs for application {} in Yarn Registry",uamMap.size(),this.attemptId.getApplicationId());    } else {      uamMap=new HashMap<>();      for (      Entry<String,byte[]> entry : recoveredDataMap.entrySet()) {        if (entry.getKey().startsWith(NMSS_SECONDARY_SC_PREFIX)) {          String scId=entry.getKey().substring(NMSS_SECONDARY_SC_PREFIX.length());          Token<AMRMTokenIdentifier> amrmToken=new Token<>();          amrmToken.decodeFromUrlString(new String(entry.getValue(),STRING_TO_BYTE_FORMAT));
      RegisterApplicationMasterResponseProto pb=RegisterApplicationMasterResponseProto.parseFrom(recoveredDataMap.get(NMSS_REG_RESPONSE_KEY));      this.amRegistrationResponse=new RegisterApplicationMasterResponsePBImpl(pb);      LOG.info("amRegistrationResponse recovered for {}",this.attemptId);    }    Map<String,Token<AMRMTokenIdentifier>> uamMap;    if (this.registryClient != null) {      uamMap=this.registryClient.loadStateFromRegistry(this.attemptId.getApplicationId());      LOG.info("Found {} existing UAMs for application {} in Yarn Registry",uamMap.size(),this.attemptId.getApplicationId());    } else {      uamMap=new HashMap<>();      for (      Entry<String,byte[]> entry : recoveredDataMap.entrySet()) {        if (entry.getKey().startsWith(NMSS_SECONDARY_SC_PREFIX)) {          String scId=entry.getKey().substring(NMSS_SECONDARY_SC_PREFIX.length());          Token<AMRMTokenIdentifier> amrmToken=new Token<>();          amrmToken.decodeFromUrlString(new String(entry.getValue(),STRING_TO_BYTE_FORMAT));          uamMap.put(scId,amrmToken);
          String scId=entry.getKey().substring(NMSS_SECONDARY_SC_PREFIX.length());          Token<AMRMTokenIdentifier> amrmToken=new Token<>();          amrmToken.decodeFromUrlString(new String(entry.getValue(),STRING_TO_BYTE_FORMAT));          uamMap.put(scId,amrmToken);          LOG.debug("Recovered UAM in {} from NMSS",scId);        }      }      LOG.info("Found {} existing UAMs for application {} in NMStateStore",uamMap.size(),this.attemptId.getApplicationId());    }    int containers=0;    for (    Map.Entry<String,Token<AMRMTokenIdentifier>> entry : uamMap.entrySet()) {      SubClusterId subClusterId=SubClusterId.newInstance(entry.getKey());      YarnConfiguration config=new YarnConfiguration(getConf());      FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());      try {        this.uamPool.reAttachUAM(subClusterId.getId(),config,this.attemptId.getApplicationId(),this.amRegistrationResponse.getQueue(),getApplicationContext().getUser(),this.homeSubClusterId.getId(),entry.getValue(),subClusterId.toString());        this.secondaryRelayers.put(subClusterId.getId(),this.uamPool.getAMRMClientRelayer(subClusterId.getId()));        RegisterApplicationMasterResponse response=this.uamPool.registerApplicationMaster(subClusterId.getId(),this.amRegistrationRequest);
          amrmToken.decodeFromUrlString(new String(entry.getValue(),STRING_TO_BYTE_FORMAT));          uamMap.put(scId,amrmToken);          LOG.debug("Recovered UAM in {} from NMSS",scId);        }      }      LOG.info("Found {} existing UAMs for application {} in NMStateStore",uamMap.size(),this.attemptId.getApplicationId());    }    int containers=0;    for (    Map.Entry<String,Token<AMRMTokenIdentifier>> entry : uamMap.entrySet()) {      SubClusterId subClusterId=SubClusterId.newInstance(entry.getKey());      YarnConfiguration config=new YarnConfiguration(getConf());      FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());      try {        this.uamPool.reAttachUAM(subClusterId.getId(),config,this.attemptId.getApplicationId(),this.amRegistrationResponse.getQueue(),getApplicationContext().getUser(),this.homeSubClusterId.getId(),entry.getValue(),subClusterId.toString());        this.secondaryRelayers.put(subClusterId.getId(),this.uamPool.getAMRMClientRelayer(subClusterId.getId()));        RegisterApplicationMasterResponse response=this.uamPool.registerApplicationMaster(subClusterId.getId(),this.amRegistrationRequest);        lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);        for (        Container container : response.getContainersFromPreviousAttempts()) {
          uamMap.put(scId,amrmToken);          LOG.debug("Recovered UAM in {} from NMSS",scId);        }      }      LOG.info("Found {} existing UAMs for application {} in NMStateStore",uamMap.size(),this.attemptId.getApplicationId());    }    int containers=0;    for (    Map.Entry<String,Token<AMRMTokenIdentifier>> entry : uamMap.entrySet()) {      SubClusterId subClusterId=SubClusterId.newInstance(entry.getKey());      YarnConfiguration config=new YarnConfiguration(getConf());      FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());      try {        this.uamPool.reAttachUAM(subClusterId.getId(),config,this.attemptId.getApplicationId(),this.amRegistrationResponse.getQueue(),getApplicationContext().getUser(),this.homeSubClusterId.getId(),entry.getValue(),subClusterId.toString());        this.secondaryRelayers.put(subClusterId.getId(),this.uamPool.getAMRMClientRelayer(subClusterId.getId()));        RegisterApplicationMasterResponse response=this.uamPool.registerApplicationMaster(subClusterId.getId(),this.amRegistrationRequest);        lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);        for (        Container container : response.getContainersFromPreviousAttempts()) {          containerIdToSubClusterIdMap.put(container.getId(),subClusterId);
    for (    Map.Entry<String,Token<AMRMTokenIdentifier>> entry : uamMap.entrySet()) {      SubClusterId subClusterId=SubClusterId.newInstance(entry.getKey());      YarnConfiguration config=new YarnConfiguration(getConf());      FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());      try {        this.uamPool.reAttachUAM(subClusterId.getId(),config,this.attemptId.getApplicationId(),this.amRegistrationResponse.getQueue(),getApplicationContext().getUser(),this.homeSubClusterId.getId(),entry.getValue(),subClusterId.toString());        this.secondaryRelayers.put(subClusterId.getId(),this.uamPool.getAMRMClientRelayer(subClusterId.getId()));        RegisterApplicationMasterResponse response=this.uamPool.registerApplicationMaster(subClusterId.getId(),this.amRegistrationRequest);        lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);        for (        Container container : response.getContainersFromPreviousAttempts()) {          containerIdToSubClusterIdMap.put(container.getId(),subClusterId);          containers++;          LOG.debug("  From subcluster {} running container {}",subClusterId,container.getId());        }        LOG.info("Recovered {} running containers from UAM in {}",response.getContainersFromPreviousAttempts().size(),subClusterId);      } catch (      Exception e) {
      YarnConfiguration config=new YarnConfiguration(getConf());      FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());      try {        this.uamPool.reAttachUAM(subClusterId.getId(),config,this.attemptId.getApplicationId(),this.amRegistrationResponse.getQueue(),getApplicationContext().getUser(),this.homeSubClusterId.getId(),entry.getValue(),subClusterId.toString());        this.secondaryRelayers.put(subClusterId.getId(),this.uamPool.getAMRMClientRelayer(subClusterId.getId()));        RegisterApplicationMasterResponse response=this.uamPool.registerApplicationMaster(subClusterId.getId(),this.amRegistrationRequest);        lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);        for (        Container container : response.getContainersFromPreviousAttempts()) {          containerIdToSubClusterIdMap.put(container.getId(),subClusterId);          containers++;          LOG.debug("  From subcluster {} running container {}",subClusterId,container.getId());        }        LOG.info("Recovered {} running containers from UAM in {}",response.getContainersFromPreviousAttempts().size(),subClusterId);      } catch (      Exception e) {        LOG.error("Error reattaching UAM to " + subClusterId + " for "+ this.attemptId,e);      }    }    UserGroupInformation appSubmitter=UserGroupInformation.createRemoteUser(getApplicationContext().getUser());
    if (getNMStateStore() != null) {      try {        RegisterApplicationMasterRequestPBImpl pb=(RegisterApplicationMasterRequestPBImpl)this.amRegistrationRequest;        getNMStateStore().storeAMRMProxyAppContextEntry(this.attemptId,NMSS_REG_REQUEST_KEY,pb.getProto().toByteArray());      } catch (      Exception e) {        LOG.error("Error storing AMRMProxy application context entry for " + this.attemptId,e);      }    }  }  if (this.amRegistrationResponse != null) {    return this.amRegistrationResponse;  }  this.amRegistrationResponse=this.homeRMRelayer.registerApplicationMaster(request);  if (this.amRegistrationResponse.getContainersFromPreviousAttempts() != null) {    cacheAllocatedContainers(this.amRegistrationResponse.getContainersFromPreviousAttempts(),this.homeSubClusterId);  }  ApplicationId appId=this.attemptId.getApplicationId();  reAttachUAMAndMergeRegisterResponse(this.amRegistrationResponse,appId);  if (getNMStateStore() != null) {    try {
 catch (      Exception e) {        LOG.error("Error storing AMRMProxy application context entry for " + this.attemptId,e);      }    }  }  if (this.amRegistrationResponse != null) {    return this.amRegistrationResponse;  }  this.amRegistrationResponse=this.homeRMRelayer.registerApplicationMaster(request);  if (this.amRegistrationResponse.getContainersFromPreviousAttempts() != null) {    cacheAllocatedContainers(this.amRegistrationResponse.getContainersFromPreviousAttempts(),this.homeSubClusterId);  }  ApplicationId appId=this.attemptId.getApplicationId();  reAttachUAMAndMergeRegisterResponse(this.amRegistrationResponse,appId);  if (getNMStateStore() != null) {    try {      RegisterApplicationMasterResponsePBImpl pb=(RegisterApplicationMasterResponsePBImpl)this.amRegistrationResponse;      getNMStateStore().storeAMRMProxyAppContextEntry(this.attemptId,NMSS_REG_RESPONSE_KEY,pb.getProto().toByteArray());    } catch (    Exception e) {      LOG.error("Error storing AMRMProxy application context entry for " + this.attemptId,e);
@Override public FinishApplicationMasterResponse finishApplicationMaster(FinishApplicationMasterRequest request) throws YarnException, IOException {  this.finishAMCalled=true;  boolean failedToUnRegister=false;  ExecutorCompletionService<FinishApplicationMasterResponseInfo> compSvc=null;  Set<String> subClusterIds=this.uamPool.getAllUAMIds();  if (subClusterIds.size() > 0) {    final FinishApplicationMasterRequest finishRequest=request;    compSvc=new ExecutorCompletionService<FinishApplicationMasterResponseInfo>(this.threadpool);
@Override public FinishApplicationMasterResponse finishApplicationMaster(FinishApplicationMasterRequest request) throws YarnException, IOException {  this.finishAMCalled=true;  boolean failedToUnRegister=false;  ExecutorCompletionService<FinishApplicationMasterResponseInfo> compSvc=null;  Set<String> subClusterIds=this.uamPool.getAllUAMIds();  if (subClusterIds.size() > 0) {    final FinishApplicationMasterRequest finishRequest=request;    compSvc=new ExecutorCompletionService<FinishApplicationMasterResponseInfo>(this.threadpool);    LOG.info("Sending finish application request to {} sub-cluster RMs",subClusterIds.size());    for (    final String subClusterId : subClusterIds) {      compSvc.submit(new Callable<FinishApplicationMasterResponseInfo>(){        @Override public FinishApplicationMasterResponseInfo call() throws Exception {
          LOG.info("Sending finish application request to RM {}",subClusterId);          FinishApplicationMasterResponse uamResponse=null;          try {            uamResponse=uamPool.finishApplicationMaster(subClusterId,finishRequest);            if (uamResponse.getIsUnregistered()) {              secondaryRelayers.remove(subClusterId);              if (getNMStateStore() != null) {                getNMStateStore().removeAMRMProxyAppContextEntry(attemptId,NMSS_SECONDARY_SC_PREFIX + subClusterId);              }            }          } catch (          Throwable e) {            LOG.warn("Failed to finish unmanaged application master: " + "RM address: " + subClusterId + " ApplicationId: "+ attemptId,e);          }          return new FinishApplicationMasterResponseInfo(uamResponse,subClusterId);        }      });    }  }  FinishApplicationMasterResponse homeResponse=this.homeRMRelayer.finishApplicationMaster(request);  this.homeHeartbeartHandler.shutdown();  if (subClusterIds.size() > 0) {
              secondaryRelayers.remove(subClusterId);              if (getNMStateStore() != null) {                getNMStateStore().removeAMRMProxyAppContextEntry(attemptId,NMSS_SECONDARY_SC_PREFIX + subClusterId);              }            }          } catch (          Throwable e) {            LOG.warn("Failed to finish unmanaged application master: " + "RM address: " + subClusterId + " ApplicationId: "+ attemptId,e);          }          return new FinishApplicationMasterResponseInfo(uamResponse,subClusterId);        }      });    }  }  FinishApplicationMasterResponse homeResponse=this.homeRMRelayer.finishApplicationMaster(request);  this.homeHeartbeartHandler.shutdown();  if (subClusterIds.size() > 0) {    LOG.info("Waiting for finish application response from {} sub-cluster RMs",subClusterIds.size());    for (int i=0; i < subClusterIds.size(); ++i) {      try {        Future<FinishApplicationMasterResponseInfo> future=compSvc.take();        FinishApplicationMasterResponseInfo uamResponse=future.get();
  ExecutorCompletionService<RegisterApplicationMasterResponse> completionService=new ExecutorCompletionService<>(this.threadpool);  for (  Entry<String,Token<AMRMTokenIdentifier>> entry : uamMap.entrySet()) {    final SubClusterId subClusterId=SubClusterId.newInstance(entry.getKey());    final Token<AMRMTokenIdentifier> amrmToken=entry.getValue();    completionService.submit(new Callable<RegisterApplicationMasterResponse>(){      @Override public RegisterApplicationMasterResponse call() throws Exception {        RegisterApplicationMasterResponse response=null;        try {          YarnConfiguration config=new YarnConfiguration(getConf());          FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());          uamPool.reAttachUAM(subClusterId.getId(),config,appId,amRegistrationResponse.getQueue(),getApplicationContext().getUser(),homeSubClusterId.getId(),amrmToken,subClusterId.toString());          secondaryRelayers.put(subClusterId.getId(),uamPool.getAMRMClientRelayer(subClusterId.getId()));          response=uamPool.registerApplicationMaster(subClusterId.getId(),amRegistrationRequest);          lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);          if (response != null && response.getContainersFromPreviousAttempts() != null) {
  for (  Entry<String,Token<AMRMTokenIdentifier>> entry : uamMap.entrySet()) {    final SubClusterId subClusterId=SubClusterId.newInstance(entry.getKey());    final Token<AMRMTokenIdentifier> amrmToken=entry.getValue();    completionService.submit(new Callable<RegisterApplicationMasterResponse>(){      @Override public RegisterApplicationMasterResponse call() throws Exception {        RegisterApplicationMasterResponse response=null;        try {          YarnConfiguration config=new YarnConfiguration(getConf());          FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());          uamPool.reAttachUAM(subClusterId.getId(),config,appId,amRegistrationResponse.getQueue(),getApplicationContext().getUser(),homeSubClusterId.getId(),amrmToken,subClusterId.toString());          secondaryRelayers.put(subClusterId.getId(),uamPool.getAMRMClientRelayer(subClusterId.getId()));          response=uamPool.registerApplicationMaster(subClusterId.getId(),amRegistrationRequest);          lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);          if (response != null && response.getContainersFromPreviousAttempts() != null) {            cacheAllocatedContainers(response.getContainersFromPreviousAttempts(),subClusterId);
          FederationProxyProviderUtil.updateConfForFederation(config,subClusterId.getId());          uamPool.reAttachUAM(subClusterId.getId(),config,appId,amRegistrationResponse.getQueue(),getApplicationContext().getUser(),homeSubClusterId.getId(),amrmToken,subClusterId.toString());          secondaryRelayers.put(subClusterId.getId(),uamPool.getAMRMClientRelayer(subClusterId.getId()));          response=uamPool.registerApplicationMaster(subClusterId.getId(),amRegistrationRequest);          lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);          if (response != null && response.getContainersFromPreviousAttempts() != null) {            cacheAllocatedContainers(response.getContainersFromPreviousAttempts(),subClusterId);          }          LOG.info("UAM {} reattached for {}",subClusterId,appId);        } catch (        Throwable e) {          LOG.error("Reattaching UAM " + subClusterId + " failed for "+ appId,e);        }        return response;      }    });  }  for (int i=0; i < uamMap.size(); i++) {    try {      Future<RegisterApplicationMasterResponse> future=completionService.take();
      lastSCResponseTime.put(subClusterId,clock.getTime() - subClusterTimeOut);    }  }  this.uamRegisterFutures.clear();  for (  final SubClusterId scId : newSubClusters) {    Future<?> future=this.threadpool.submit(new Runnable(){      @Override public void run(){        String subClusterId=scId.getId();        YarnConfiguration config=new YarnConfiguration(getConf());        FederationProxyProviderUtil.updateConfForFederation(config,subClusterId);        RegisterApplicationMasterResponse uamResponse=null;        Token<AMRMTokenIdentifier> token=null;        try {          token=uamPool.launchUAM(subClusterId,config,attemptId.getApplicationId(),amRegistrationResponse.getQueue(),getApplicationContext().getUser(),homeSubClusterId.toString(),true,subClusterId);          secondaryRelayers.put(subClusterId,uamPool.getAMRMClientRelayer(subClusterId));          uamResponse=uamPool.registerApplicationMaster(subClusterId,amRegistrationRequest);        } catch (        Throwable e) {
  this.uamRegisterFutures.clear();  for (  final SubClusterId scId : newSubClusters) {    Future<?> future=this.threadpool.submit(new Runnable(){      @Override public void run(){        String subClusterId=scId.getId();        YarnConfiguration config=new YarnConfiguration(getConf());        FederationProxyProviderUtil.updateConfForFederation(config,subClusterId);        RegisterApplicationMasterResponse uamResponse=null;        Token<AMRMTokenIdentifier> token=null;        try {          token=uamPool.launchUAM(subClusterId,config,attemptId.getApplicationId(),amRegistrationResponse.getQueue(),getApplicationContext().getUser(),homeSubClusterId.toString(),true,subClusterId);          secondaryRelayers.put(subClusterId,uamPool.getAMRMClientRelayer(subClusterId));          uamResponse=uamPool.registerApplicationMaster(subClusterId,amRegistrationRequest);        } catch (        Throwable e) {          LOG.error("Failed to register application master: " + subClusterId + " Application: "+ attemptId,e);
        String subClusterId=scId.getId();        YarnConfiguration config=new YarnConfiguration(getConf());        FederationProxyProviderUtil.updateConfForFederation(config,subClusterId);        RegisterApplicationMasterResponse uamResponse=null;        Token<AMRMTokenIdentifier> token=null;        try {          token=uamPool.launchUAM(subClusterId,config,attemptId.getApplicationId(),amRegistrationResponse.getQueue(),getApplicationContext().getUser(),homeSubClusterId.toString(),true,subClusterId);          secondaryRelayers.put(subClusterId,uamPool.getAMRMClientRelayer(subClusterId));          uamResponse=uamPool.registerApplicationMaster(subClusterId,amRegistrationRequest);        } catch (        Throwable e) {          LOG.error("Failed to register application master: " + subClusterId + " Application: "+ attemptId,e);          return;        }        uamRegistrations.put(scId,uamResponse);        LOG.info("Successfully registered unmanaged application master: " + subClusterId + " ApplicationId: "+ attemptId);        try {
          secondaryRelayers.put(subClusterId,uamPool.getAMRMClientRelayer(subClusterId));          uamResponse=uamPool.registerApplicationMaster(subClusterId,amRegistrationRequest);        } catch (        Throwable e) {          LOG.error("Failed to register application master: " + subClusterId + " Application: "+ attemptId,e);          return;        }        uamRegistrations.put(scId,uamResponse);        LOG.info("Successfully registered unmanaged application master: " + subClusterId + " ApplicationId: "+ attemptId);        try {          uamPool.allocateAsync(subClusterId,requests.get(scId),new HeartbeatCallBack(scId,true));        } catch (        Throwable e) {          LOG.error("Failed to allocate async to " + subClusterId + " Application: "+ attemptId,e);        }        try {          if (registryClient != null) {            registryClient.writeAMRMTokenForUAM(attemptId.getApplicationId(),subClusterId,token);          } else           if (getNMStateStore() != null) {
private void removeFinishedContainersFromCache(List<ContainerStatus> finishedContainers){  for (  ContainerStatus container : finishedContainers) {
private void cacheAllocatedContainers(List<Container> containers,SubClusterId subClusterId){  for (  Container container : containers) {
private boolean warnIfNotExists(ContainerId containerId,String actionName){  if (!this.containerIdToSubClusterIdMap.containsKey(containerId)) {
protected final synchronized void addService(String name,AuxiliaryService service,AuxServiceRecord serviceRecord){
  final String className=getClassName(service);  if (className == null || className.isEmpty()) {    throw new YarnRuntimeException("Class name not provided for auxiliary " + "service " + sName);  }  if (fromConfiguration) {    final String appLocalClassPath=conf.get(String.format(YarnConfiguration.NM_AUX_SERVICES_CLASSPATH,sName));    if (appLocalClassPath != null && !appLocalClassPath.isEmpty()) {      return createAuxServiceFromLocalClasspath(service,appLocalClassPath,conf);    }  }  AuxServiceConfiguration serviceConf=service.getConfiguration();  List<Path> destFiles=new ArrayList<>();  if (serviceConf != null) {    List<AuxServiceFile> files=serviceConf.getFiles();    if (files != null) {      for (      AuxServiceFile file : files) {        destFiles.add(maybeDownloadJars(sName,className,file.getSrcFile(),file.getType(),conf));      }    }  }  if (destFiles.size() > 0) {
private synchronized void maybeRemoveAuxService(String sName){  AuxiliaryService s;  s=serviceMap.remove(sName);  serviceRecordMap.remove(sName);  serviceMetaData.remove(sName);  if (s != null) {
  AuxiliaryService s;  try {    Preconditions.checkArgument(validateAuxServiceName(sName),"The auxiliary service name: " + sName + " is invalid. "+ "The valid service name should only contain a-zA-Z0-9_ "+ "and cannot start with numbers.");    s=createAuxService(service,conf,fromConfiguration);    if (s == null) {      throw new YarnRuntimeException("No auxiliary service class loaded for" + " " + sName);    }    if (!sName.equals(s.getName())) {      LOG.warn("The Auxiliary Service named '" + sName + "' in the "+ "configuration is for "+ s.getClass()+ " which has "+ "a name of '"+ s.getName()+ "'. Because these are "+ "not the same tools trying to send ServiceData and read "+ "Service Meta Data may have issues unless the refer to "+ "the name in the config.");    }    s.setAuxiliaryLocalPathHandler(auxiliaryLocalPathHandler);    setStateStoreDir(sName,s);    Configuration customConf=new Configuration(conf);    if (service.getConfiguration() != null) {      for (      Entry<String,String> entry : service.getConfiguration().getProperties().entrySet()) {        customConf.set(entry.getKey(),entry.getValue());      }    }    s.init(customConf);
    Preconditions.checkArgument(validateAuxServiceName(sName),"The auxiliary service name: " + sName + " is invalid. "+ "The valid service name should only contain a-zA-Z0-9_ "+ "and cannot start with numbers.");    s=createAuxService(service,conf,fromConfiguration);    if (s == null) {      throw new YarnRuntimeException("No auxiliary service class loaded for" + " " + sName);    }    if (!sName.equals(s.getName())) {      LOG.warn("The Auxiliary Service named '" + sName + "' in the "+ "configuration is for "+ s.getClass()+ " which has "+ "a name of '"+ s.getName()+ "'. Because these are "+ "not the same tools trying to send ServiceData and read "+ "Service Meta Data may have issues unless the refer to "+ "the name in the config.");    }    s.setAuxiliaryLocalPathHandler(auxiliaryLocalPathHandler);    setStateStoreDir(sName,s);    Configuration customConf=new Configuration(conf);    if (service.getConfiguration() != null) {      for (      Entry<String,String> entry : service.getConfiguration().getProperties().entrySet()) {        customConf.set(entry.getKey(),entry.getValue());      }    }    s.init(customConf);    LOG.info("Initialized auxiliary service " + sName);  } catch (  RuntimeException e) {
private boolean checkManifestPermissions(FileStatus status) throws IOException {  if ((status.getPermission().toShort() & 0022) != 0) {
  if (!manifestFS.exists(manifest)) {    LOG.warn("Manifest file " + manifest + " doesn't exist");    return null;  }  FileStatus status;  try {    status=manifestFS.getFileStatus(manifest);  } catch (  FileNotFoundException e) {    LOG.warn("Manifest file " + manifest + " doesn't exist");    return null;  }  if (!status.isFile()) {    LOG.warn("Manifest file " + manifest + " is not a file");  }  if (!checkManifestOwnerAndPermissions(status)) {    return null;  }  if (status.getModificationTime() == manifestModifyTS) {    return null;
  boolean foundChanges=false;  if (services.getServices() != null) {    for (    AuxServiceRecord service : services.getServices()) {      AuxServiceRecord existingService=serviceRecordMap.get(service.getName());      loadedAuxServices.add(service.getName());      if (existingService != null && existingService.equals(service)) {        LOG.debug("Auxiliary service already loaded: {}",service.getName());        continue;      }      foundChanges=true;      try {        maybeRemoveAuxService(service.getName());        AuxiliaryService s=initAuxService(service,conf,false);        if (startServices) {          startAuxService(service.getName(),s,service);        }        addService(service.getName(),s,service);
@Override public void stateChanged(Service service){
@Override public void handle(AuxServicesEvent event){
@SuppressWarnings("unchecked") private void recover() throws IOException, URISyntaxException {  NMStateStoreService stateStore=context.getNMStateStore();  if (stateStore.canRecover()) {    rsrcLocalizationSrvc.recoverLocalizedResources(stateStore.loadLocalizationState());    RecoveredApplicationsState appsState=stateStore.loadApplicationsState();    try (RecoveryIterator<ContainerManagerApplicationProto> rasIterator=appsState.getIterator()){      while (rasIterator.hasNext()) {        ContainerManagerApplicationProto proto=rasIterator.next();
private void recoverApplication(ContainerManagerApplicationProto p) throws IOException {  ApplicationId appId=new ApplicationIdPBImpl(p.getId());  Credentials creds=new Credentials();  creds.readTokenStorageStream(new DataInputStream(p.getCredentials().newInput()));  List<ApplicationACLMapProto> aclProtoList=p.getAclsList();  Map<ApplicationAccessType,String> acls=new HashMap<ApplicationAccessType,String>(aclProtoList.size());  for (  ApplicationACLMapProto aclProto : aclProtoList) {    acls.put(ProtoUtils.convertFromProtoFormat(aclProto.getAccessType()),aclProto.getAcl());  }  LogAggregationContext logAggregationContext=null;  if (p.getLogAggregationContext() != null) {    logAggregationContext=new LogAggregationContextPBImpl(p.getLogAggregationContext());  }  FlowContext fc=null;  if (p.getFlowContext() != null) {    FlowContextProto fcp=p.getFlowContext();    fc=new FlowContext(fcp.getFlowName(),fcp.getFlowVersion(),fcp.getFlowRunId());
  creds.readTokenStorageStream(new DataInputStream(p.getCredentials().newInput()));  List<ApplicationACLMapProto> aclProtoList=p.getAclsList();  Map<ApplicationAccessType,String> acls=new HashMap<ApplicationAccessType,String>(aclProtoList.size());  for (  ApplicationACLMapProto aclProto : aclProtoList) {    acls.put(ProtoUtils.convertFromProtoFormat(aclProto.getAccessType()),aclProto.getAcl());  }  LogAggregationContext logAggregationContext=null;  if (p.getLogAggregationContext() != null) {    logAggregationContext=new LogAggregationContextPBImpl(p.getLogAggregationContext());  }  FlowContext fc=null;  if (p.getFlowContext() != null) {    FlowContextProto fcp=p.getFlowContext();    fc=new FlowContext(fcp.getFlowName(),fcp.getFlowVersion(),fcp.getFlowRunId());    LOG.debug("Recovering Flow context: {} for an application {}",fc,appId);  } else {    fc=new FlowContext(TimelineUtils.generateDefaultFlowName(null,appId),YarnConfiguration.DEFAULT_FLOW_VERSION,appId.getClusterTimestamp());
  if (delayedRpcServerStart) {    connectAddress=NetUtils.getConnectAddress(initialAddress);  } else {    server.start();    connectAddress=NetUtils.getConnectAddress(server);  }  NodeId nodeId=buildNodeId(connectAddress,hostOverride);  ((NodeManager.NMContext)context).setNodeId(nodeId);  this.context.getNMTokenSecretManager().setNodeId(nodeId);  this.context.getContainerTokenSecretManager().setNodeId(nodeId);  super.serviceStart();  if (delayedRpcServerStart) {    waitForRecoveredContainers();    server.start();    connectAddress=NetUtils.getConnectAddress(server);    NodeId serverNode=buildNodeId(connectAddress,hostOverride);
  if (delayedRpcServerStart) {    connectAddress=NetUtils.getConnectAddress(initialAddress);  } else {    server.start();    connectAddress=NetUtils.getConnectAddress(server);  }  NodeId nodeId=buildNodeId(connectAddress,hostOverride);  ((NodeManager.NMContext)context).setNodeId(nodeId);  this.context.getNMTokenSecretManager().setNodeId(nodeId);  this.context.getContainerTokenSecretManager().setNodeId(nodeId);  super.serviceStart();  if (delayedRpcServerStart) {    waitForRecoveredContainers();    server.start();    connectAddress=NetUtils.getConnectAddress(server);    NodeId serverNode=buildNodeId(connectAddress,hostOverride);
  LOG.info("Applications still running : " + applications.keySet());  if (this.context.getNMStateStore().canRecover() && !this.context.getDecommissioned()) {    if (getConfig().getBoolean(YarnConfiguration.NM_RECOVERY_SUPERVISED,YarnConfiguration.DEFAULT_NM_RECOVERY_SUPERVISED)) {      return;    }  }  List<ApplicationId> appIds=new ArrayList<ApplicationId>(applications.keySet());  this.handle(new CMgrCompletedAppsEvent(appIds,CMgrCompletedAppsEvent.Reason.ON_SHUTDOWN));  LOG.info("Waiting for Applications to be Finished");  long waitStartTime=System.currentTimeMillis();  while (!applications.isEmpty() && System.currentTimeMillis() - waitStartTime < waitForContainersOnShutdownMillis) {    try {      Thread.sleep(1000);    } catch (    InterruptedException ex) {      LOG.warn("Interrupted while sleeping on applications finish on shutdown",ex);    }  }  if (applications.isEmpty()) {    LOG.info("All applications in FINISHED state");
  LOG.info("Waiting for containers to be killed");  this.handle(new CMgrCompletedContainersEvent(containerIds,CMgrCompletedContainersEvent.Reason.ON_NODEMANAGER_RESYNC));  boolean allContainersCompleted=false;  while (!containers.isEmpty() && !allContainersCompleted) {    allContainersCompleted=true;    for (    Entry<ContainerId,Container> container : containers.entrySet()) {      if (((ContainerImpl)container.getValue()).getCurrentState() != ContainerState.COMPLETE) {        allContainersCompleted=false;        try {          Thread.sleep(1000);        } catch (        InterruptedException ex) {          LOG.warn("Interrupted while sleeping on container kill on resync",ex);        }        break;      }    }  }  if (allContainersCompleted) {    LOG.info("All containers in DONE state");
@SuppressWarnings("unchecked") protected void startContainerInternal(ContainerTokenIdentifier containerTokenIdentifier,StartContainerRequest request,String remoteUser) throws YarnException, IOException {  ContainerId containerId=containerTokenIdentifier.getContainerID();  String containerIdStr=containerId.toString();  String user=containerTokenIdentifier.getApplicationSubmitter();
    } else     if (rsrc.getValue().getVisibility() == null) {      throw new YarnException("Null resource visibility for local resource " + rsrc.getKey() + " : "+ rsrc.getValue());    }  }  Credentials credentials=YarnServerSecurityUtils.parseCredentials(launchContext);  long containerStartTime=SystemClock.getInstance().getTime();  Container container=new ContainerImpl(getConfig(),this.dispatcher,launchContext,credentials,metrics,containerTokenIdentifier,context,containerStartTime);  ApplicationId applicationID=containerId.getApplicationAttemptId().getApplicationId();  if (context.getContainers().putIfAbsent(containerId,container) != null) {    NMAuditLogger.logFailure(remoteUser,AuditConstants.START_CONTAINER,"ContainerManagerImpl","Container already running on this node!",applicationID,containerId);    throw RPCUtil.getRemoteException("Container " + containerIdStr + " already is running on this node!!");  }  this.readLock.lock();  try {    if (!isServiceStopped()) {      if (!context.getApplications().containsKey(applicationID)) {        FlowContext flowContext=getFlowContext(launchContext,applicationID);        Application application=new ApplicationImpl(dispatcher,user,flowContext,applicationID,credentials,context);
    throw RPCUtil.getRemoteException("Container " + containerIdStr + " already is running on this node!!");  }  this.readLock.lock();  try {    if (!isServiceStopped()) {      if (!context.getApplications().containsKey(applicationID)) {        FlowContext flowContext=getFlowContext(launchContext,applicationID);        Application application=new ApplicationImpl(dispatcher,user,flowContext,applicationID,credentials,context);        if (context.getApplications().putIfAbsent(applicationID,application) == null) {          LOG.info("Creating a new application reference for app " + applicationID);          LogAggregationContext logAggregationContext=containerTokenIdentifier.getLogAggregationContext();          Map<ApplicationAccessType,String> appAcls=container.getLaunchContext().getApplicationACLs();          context.getNMStateStore().storeApplication(applicationID,buildAppProto(applicationID,user,credentials,appAcls,logAggregationContext,flowContext));          dispatcher.getEventHandler().handle(new ApplicationInitEvent(applicationID,appAcls,logAggregationContext));        }      } else       if (containerTokenIdentifier.getContainerType() == ContainerType.APPLICATION_MASTER) {        FlowContext flowContext=getFlowContext(launchContext,applicationID);
  this.readLock.lock();  try {    if (!isServiceStopped()) {      if (!context.getApplications().containsKey(applicationID)) {        FlowContext flowContext=getFlowContext(launchContext,applicationID);        Application application=new ApplicationImpl(dispatcher,user,flowContext,applicationID,credentials,context);        if (context.getApplications().putIfAbsent(applicationID,application) == null) {          LOG.info("Creating a new application reference for app " + applicationID);          LogAggregationContext logAggregationContext=containerTokenIdentifier.getLogAggregationContext();          Map<ApplicationAccessType,String> appAcls=container.getLaunchContext().getApplicationACLs();          context.getNMStateStore().storeApplication(applicationID,buildAppProto(applicationID,user,credentials,appAcls,logAggregationContext,flowContext));          dispatcher.getEventHandler().handle(new ApplicationInitEvent(applicationID,appAcls,logAggregationContext));        }      } else       if (containerTokenIdentifier.getContainerType() == ContainerType.APPLICATION_MASTER) {        FlowContext flowContext=getFlowContext(launchContext,applicationID);        if (flowContext != null) {
@SuppressWarnings("unchecked") protected void stopContainerInternal(ContainerId containerID,String remoteUser) throws YarnException, IOException {  String containerIDStr=containerID.toString();  Container container=this.context.getContainers().get(containerID);
protected ContainerStatus getContainerStatusInternal(ContainerId containerID,NMTokenIdentifier nmTokenIdentifier,String remoteUser) throws YarnException {  String containerIDStr=containerID.toString();  Container container=this.context.getContainers().get(containerID);
  sb.append(status.getCapability()).append(", ");  sb.append("Diagnostics: ");  sb.append(LOG.isDebugEnabled() ? status.getDiagnostics() : "...");  sb.append(", ");  sb.append("ExitStatus: ");  sb.append(status.getExitStatus()).append(", ");  sb.append("IP: ");  sb.append(status.getIPs()).append(", ");  sb.append("Host: ");  sb.append(status.getHost()).append(", ");  sb.append("ExposedPorts: ");  sb.append(status.getExposedPorts()).append(", ");  sb.append("ContainerSubState: ");  sb.append(status.getContainerSubState());  sb.append("]");
@SuppressWarnings("unchecked") @Override public void handle(ContainerManagerEvent event){switch (event.getType()) {case FINISH_APPS:    CMgrCompletedAppsEvent appsFinishedEvent=(CMgrCompletedAppsEvent)event;  for (  ApplicationId appID : appsFinishedEvent.getAppsToCleanup()) {    Application app=this.context.getApplications().get(appID);    if (app == null) {
    String diagnostic="";    if (appsFinishedEvent.getReason() == CMgrCompletedAppsEvent.Reason.ON_SHUTDOWN) {      diagnostic="Application killed on shutdown";    } else     if (appsFinishedEvent.getReason() == CMgrCompletedAppsEvent.Reason.BY_RESOURCEMANAGER) {      diagnostic="Application killed by ResourceManager";    }    this.dispatcher.getEventHandler().handle(new ApplicationFinishEvent(appID,diagnostic));  }break;case FINISH_CONTAINERS:CMgrCompletedContainersEvent containersFinishedEvent=(CMgrCompletedContainersEvent)event;for (ContainerId containerId : containersFinishedEvent.getContainersToCleanup()) {ApplicationId appId=containerId.getApplicationAttemptId().getApplicationId();Application app=this.context.getApplications().get(appId);if (app == null) {LOG.warn("couldn't find app " + appId + " while processing"+ " FINISH_CONTAINERS event");continue;}Container container=app.getContainers().get(containerId);
public void reInitializeContainer(ContainerId containerId,ContainerLaunchContext reInitLaunchContext,boolean autoCommit) throws YarnException {
@SuppressWarnings("unchecked") private void internalSignalToContainer(SignalContainerRequest request,String sentBy){  ContainerId containerId=request.getContainerId();  Container container=this.context.getContainers().get(containerId);  if (container != null) {
private List<LocalizationStatus> getLocalizationStatusesInternal(ContainerId containerID,NMTokenIdentifier nmTokenIdentifier,String remoteUser) throws YarnException {  Container container=this.context.getContainers().get(containerID);
@Override public void handle(ApplicationEvent event){  this.writeLock.lock();  try {    ApplicationId applicationID=event.getApplicationID();
@Override public void handle(ContainerEvent event){  this.writeLock.lock();  try {    ContainerId containerID=event.getContainerID();
        lfs.delete(subDir,true);      } catch (      IOException e) {        error=true;        LOG.warn("Failed to delete " + subDir);      }    } else {      for (      Path baseDir : baseDirs) {        Path del=subDir == null ? baseDir : new Path(baseDir,subDir);        LOG.debug("NM deleting path : {}",del);        try {          lfs.delete(del,true);        } catch (        IOException e) {          error=true;          LOG.warn("Failed to delete " + subDir);        }      }    }  } else {    try {
@Override public void run(){  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();
@Override public void run(){  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();  LOG.info("Cleaning up container " + containerIdStr);  try {    context.getNMStateStore().storeContainerKilled(containerId);  } catch (  IOException e) {    LOG.error("Unable to mark container " + containerId + " killed in store",e);  }  boolean alreadyLaunched=!launch.markLaunched() || launch.isLaunchCompleted();  if (!alreadyLaunched) {    LOG.info("Container " + containerIdStr + " not launched."+ " No cleanup needed to be done");    return;  }  LOG.debug("Marking container {} as inactive",containerIdStr);  exec.deactivateContainer(containerId);  Path pidFilePath=launch.getPidFilePath();
private void signalProcess(String processId,String user,String containerIdStr) throws IOException {
private void signalProcess(String processId,String user,String containerIdStr) throws IOException {  LOG.debug("Sending signal to pid {} as user {} for container {}",processId,user,containerIdStr);  final ContainerExecutor.Signal signal=sleepDelayBeforeSigKill > 0 ? ContainerExecutor.Signal.TERM : ContainerExecutor.Signal.KILL;  boolean result=sendSignal(user,processId,signal);
protected int prepareForLaunch(ContainerStartContext ctx) throws IOException {  ContainerId containerId=container.getContainerId();  if (container.isMarkedForKilling()) {
protected void handleContainerExitCode(int exitCode,Path containerLogDir){  ContainerId containerId=container.getContainerId();
public void signalContainer(SignalContainerCommand command) throws IOException {  ContainerId containerId=container.getContainerTokenIdentifier().getContainerID();  String containerIdStr=containerId.toString();  String user=container.getUser();  Signal signal=translateCommandToSignal(command);  if (signal.equals(Signal.NULL)) {
  String containerIdStr=containerId.toString();  String user=container.getUser();  Signal signal=translateCommandToSignal(command);  if (signal.equals(Signal.NULL)) {    LOG.info("ignore signal command " + command);    return;  }  LOG.info("Sending signal " + command + " to container "+ containerIdStr);  boolean alreadyLaunched=!containerAlreadyLaunched.compareAndSet(false,true);  if (!alreadyLaunched) {    LOG.info("Container " + containerIdStr + " not launched."+ " Not sending the signal");    return;  }  LOG.debug("Getting pid for container {} to send signal to from pid" + " file {}",containerIdStr,(pidFilePath != null ? pidFilePath.toString() : "null"));  try {    String processId=getContainerPid();    if (processId != null) {
  if (signal.equals(Signal.NULL)) {    LOG.info("ignore signal command " + command);    return;  }  LOG.info("Sending signal " + command + " to container "+ containerIdStr);  boolean alreadyLaunched=!containerAlreadyLaunched.compareAndSet(false,true);  if (!alreadyLaunched) {    LOG.info("Container " + containerIdStr + " not launched."+ " Not sending the signal");    return;  }  LOG.debug("Getting pid for container {} to send signal to from pid" + " file {}",containerIdStr,(pidFilePath != null ? pidFilePath.toString() : "null"));  try {    String processId=getContainerPid();    if (processId != null) {      LOG.debug("Sending signal to pid {} as user {} for container {}",processId,user,containerIdStr);      boolean result=exec.signalContainer(new ContainerSignalContext.Builder().setContainer(container).setUser(user).setPid(processId).setSignal(signal).build());      String diagnostics="Sent signal " + command + " ("+ signal+ ") to pid "+ processId+ " as user "+ user+ " for container "+ containerIdStr+ ", result="+ (result ? "success" : "failed");
public void pauseContainer() throws IOException {  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();
public void pauseContainer() throws IOException {  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();  LOG.info("Pausing the container " + containerIdStr);  if (!shouldPauseContainer.compareAndSet(false,true)) {
  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();  LOG.info("Pausing the container " + containerIdStr);  if (!shouldPauseContainer.compareAndSet(false,true)) {    LOG.info("Container " + containerId + " not paused as "+ "resume already called");    return;  }  try {    exec.pauseContainer(container);    dispatcher.getEventHandler().handle(new ContainerEvent(containerId,ContainerEventType.CONTAINER_PAUSED));    try {      this.context.getNMStateStore().storeContainerPaused(container.getContainerId());    } catch (    IOException e) {      LOG.warn("Could not store container [" + container.getContainerId() + "] state. The Container has been paused.",e);    }  } catch (  Exception e) {    String message="Exception when trying to pause container " + containerIdStr + ": "+ StringUtils.stringifyException(e);
public void resumeContainer() throws IOException {  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();
public void resumeContainer() throws IOException {  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();  LOG.info("Resuming the container " + containerIdStr);  boolean alreadyPaused=!shouldPauseContainer.compareAndSet(false,true);  if (!alreadyPaused) {
  String containerIdStr=containerId.toString();  LOG.info("Resuming the container " + containerIdStr);  boolean alreadyPaused=!shouldPauseContainer.compareAndSet(false,true);  if (!alreadyPaused) {    LOG.info("Container " + containerIdStr + " not paused."+ " No resume necessary");    return;  }  try {    exec.resumeContainer(container);    dispatcher.getEventHandler().handle(new ContainerEvent(containerId,ContainerEventType.CONTAINER_RESUMED));    try {      this.context.getNMStateStore().removeContainerPaused(container.getContainerId());    } catch (    IOException e) {      LOG.warn("Could not store container [" + container.getContainerId() + "] state. The Container has been resumed.",e);    }  } catch (  Exception e) {    String message="Exception when trying to resume container " + containerIdStr + ": "+ StringUtils.stringifyException(e);
protected void cleanupContainerFiles(Path containerWorkDir){
  try {    Path containerWorkDir=getContainerWorkDir();    cleanupContainerFiles(containerWorkDir);    containerLogDir=getContainerLogDir();    Map<Path,List<String>> localResources=getLocalizedResources();    String appIdStr=app.getAppId().toString();    Path nmPrivateContainerScriptPath=getNmPrivateContainerScriptPath(appIdStr,containerIdStr);    Path nmPrivateTokensPath=getNmPrivateTokensPath(appIdStr,containerIdStr);    Path nmPrivateKeystorePath=(container.getCredentials().getSecretKey(AMSecretKeys.YARN_APPLICATION_AM_KEYSTORE) == null) ? null : getNmPrivateKeystorePath(appIdStr,containerIdStr);    Path nmPrivateTruststorePath=(container.getCredentials().getSecretKey(AMSecretKeys.YARN_APPLICATION_AM_TRUSTSTORE) == null) ? null : getNmPrivateTruststorePath(appIdStr,containerIdStr);    try {      pidFilePath=getPidFilePath(appIdStr,containerIdStr);    } catch (    IOException e) {      String pidFileSubpath=getPidFileSubpath(appIdStr,containerIdStr);      pidFilePath=dirsHandler.getLocalPathForWrite(pidFileSubpath);
 catch (    IOException e) {      String pidFileSubpath=getPidFileSubpath(appIdStr,containerIdStr);      pidFilePath=dirsHandler.getLocalPathForWrite(pidFileSubpath);    }    LOG.info("Relaunch container with " + "workDir = " + containerWorkDir.toString() + ", logDir = "+ containerLogDir.toString()+ ", nmPrivateContainerScriptPath = "+ nmPrivateContainerScriptPath.toString()+ ", nmPrivateTokensPath = "+ nmPrivateTokensPath.toString()+ ", pidFilePath = "+ pidFilePath.toString());    List<String> localDirs=dirsHandler.getLocalDirs();    List<String> logDirs=dirsHandler.getLogDirs();    List<String> containerLocalDirs=getContainerLocalDirs(localDirs);    List<String> containerLogDirs=getContainerLogDirs(logDirs);    List<String> filecacheDirs=getNMFilecacheDirs(localDirs);    List<String> userLocalDirs=getUserLocalDirs(localDirs);    List<String> userFilecacheDirs=getUserFilecacheDirs(localDirs);    List<String> applicationLocalDirs=getApplicationLocalDirs(localDirs,appIdStr);    if (!dirsHandler.areDisksHealthy()) {      ret=ContainerExitStatus.DISKS_FAILED;      throw new IOException("Most of the disks failed. " + dirsHandler.getDisksHealthReport(false));
launch=new RecoveredContainerLaunch(context,getConfig(),dispatcher,exec,app,event.getContainer(),dirsHandler,containerManager);containerLauncher.submit(launch);running.put(containerId,launch);break;case RECOVER_PAUSED_CONTAINER:app=context.getApplications().get(containerId.getApplicationAttemptId().getApplicationId());launch=new RecoverPausedContainerLaunch(context,getConfig(),dispatcher,exec,app,event.getContainer(),dirsHandler,containerManager);containerLauncher.submit(launch);break;case CLEANUP_CONTAINER:cleanup(event,containerId,true);break;case CLEANUP_CONTAINER_FOR_REINIT:cleanup(event,containerId,false);break;case SIGNAL_CONTAINER:SignalContainersLauncherEvent signalEvent=(SignalContainersLauncherEvent)event;ContainerLaunch runningContainer=running.get(containerId);if (runningContainer == null) {
ContainerLaunch runningContainer=running.get(containerId);if (runningContainer == null) {LOG.info("Container " + containerId + " not running, nothing to signal.");return;}try {runningContainer.signalContainer(signalEvent.getCommand());} catch (IOException e) {LOG.warn("Got exception while signaling container " + containerId + " with command "+ signalEvent.getCommand());}break;case PAUSE_CONTAINER:ContainerLaunch launchedContainer=running.get(containerId);if (launchedContainer == null) {return;}try {launchedContainer.pauseContainer();} catch (Exception e) {
 catch (IOException e) {LOG.warn("Got exception while signaling container " + containerId + " with command "+ signalEvent.getCommand());}break;case PAUSE_CONTAINER:ContainerLaunch launchedContainer=running.get(containerId);if (launchedContainer == null) {return;}try {launchedContainer.pauseContainer();} catch (Exception e) {LOG.info("Got exception while pausing container: " + StringUtils.stringifyException(e));}break;case RESUME_CONTAINER:ContainerLaunch launchCont=running.get(containerId);if (launchCont == null) {return;}try {
  dispatcher.getEventHandler().handle(new ContainerEvent(containerId,ContainerEventType.RECOVER_PAUSED_CONTAINER));  boolean notInterrupted=true;  try {    File pidFile=locatePidFile(appIdStr,containerIdStr);    if (pidFile != null) {      String pidPathStr=pidFile.getPath();      pidFilePath=new Path(pidPathStr);      exec.activateContainer(containerId,pidFilePath);      retCode=exec.reacquireContainer(new ContainerReacquisitionContext.Builder().setContainer(container).setUser(container.getUser()).setContainerId(containerId).build());    } else {      LOG.warn("Unable to locate pid file for container " + containerIdStr);    }  } catch (  InterruptedException|InterruptedIOException e) {    LOG.warn("Interrupted while waiting for exit code from " + containerId);    notInterrupted=false;  }catch (  IOException e) {
      retCode=exec.reacquireContainer(new ContainerReacquisitionContext.Builder().setContainer(container).setUser(container.getUser()).setContainerId(containerId).build());    } else {      LOG.warn("Unable to locate pid file for container " + containerIdStr);    }  } catch (  InterruptedException|InterruptedIOException e) {    LOG.warn("Interrupted while waiting for exit code from " + containerId);    notInterrupted=false;  }catch (  IOException e) {    LOG.error("Unable to kill the paused container " + containerIdStr,e);  } finally {    if (notInterrupted) {      this.completed.set(true);      exec.deactivateContainer(containerId);      try {        getContext().getNMStateStore().storeContainerCompleted(containerId,retCode);      } catch (      IOException e) {
 catch (  InterruptedException|InterruptedIOException e) {    LOG.warn("Interrupted while waiting for exit code from " + containerId);    notInterrupted=false;  }catch (  IOException e) {    LOG.error("Unable to kill the paused container " + containerIdStr,e);  } finally {    if (notInterrupted) {      this.completed.set(true);      exec.deactivateContainer(containerId);      try {        getContext().getNMStateStore().storeContainerCompleted(containerId,retCode);      } catch (      IOException e) {        LOG.error("Unable to set exit code for container " + containerId);      }    }  }  if (retCode != 0) {    LOG.warn("Recovered container exited with a non-zero exit code " + retCode);
  dispatcher.getEventHandler().handle(new ContainerEvent(containerId,ContainerEventType.CONTAINER_LAUNCHED));  boolean notInterrupted=true;  try {    File pidFile=locatePidFile(appIdStr,containerIdStr);    if (pidFile != null) {      String pidPathStr=pidFile.getPath();      pidFilePath=new Path(pidPathStr);      exec.activateContainer(containerId,pidFilePath);      retCode=exec.reacquireContainer(new ContainerReacquisitionContext.Builder().setContainer(container).setUser(container.getUser()).setContainerId(containerId).build());    } else {      LOG.warn("Unable to locate pid file for container " + containerIdStr);    }  } catch (  InterruptedException|InterruptedIOException e) {    LOG.warn("Interrupted while waiting for exit code from " + containerId);    notInterrupted=false;  }catch (  IOException e) {
      retCode=exec.reacquireContainer(new ContainerReacquisitionContext.Builder().setContainer(container).setUser(container.getUser()).setContainerId(containerId).build());    } else {      LOG.warn("Unable to locate pid file for container " + containerIdStr);    }  } catch (  InterruptedException|InterruptedIOException e) {    LOG.warn("Interrupted while waiting for exit code from " + containerId);    notInterrupted=false;  }catch (  IOException e) {    LOG.error("Unable to recover container " + containerIdStr,e);  } finally {    if (notInterrupted) {      this.completed.set(true);      exec.deactivateContainer(containerId);      try {        getContext().getNMStateStore().storeContainerCompleted(containerId,retCode);      } catch (      IOException e) {
 catch (  InterruptedException|InterruptedIOException e) {    LOG.warn("Interrupted while waiting for exit code from " + containerId);    notInterrupted=false;  }catch (  IOException e) {    LOG.error("Unable to recover container " + containerIdStr,e);  } finally {    if (notInterrupted) {      this.completed.set(true);      exec.deactivateContainer(containerId);      try {        getContext().getNMStateStore().storeContainerCompleted(containerId,retCode);      } catch (      IOException e) {        LOG.error("Unable to set exit code for container " + containerId);      }    }  }  if (retCode != 0) {    LOG.warn("Recovered container exited with a non-zero exit code " + retCode);
public String executePrivilegedOperation(List<String> prefixCommands,PrivilegedOperation operation,File workingDir,Map<String,String> env,boolean grabOutput,boolean inheritParentEnv) throws PrivilegedOperationException {  String[] fullCommandArray=getPrivilegedOperationExecutionCommand(prefixCommands,operation);  ShellCommandExecutor exec=new ShellCommandExecutor(fullCommandArray,workingDir,env,0L,inheritParentEnv);  try {    exec.execute();    if (LOG.isDebugEnabled()) {      LOG.debug("command array:");
public String executePrivilegedOperation(List<String> prefixCommands,PrivilegedOperation operation,File workingDir,Map<String,String> env,boolean grabOutput,boolean inheritParentEnv) throws PrivilegedOperationException {  String[] fullCommandArray=getPrivilegedOperationExecutionCommand(prefixCommands,operation);  ShellCommandExecutor exec=new ShellCommandExecutor(fullCommandArray,workingDir,env,0L,inheritParentEnv);  try {    exec.execute();    if (LOG.isDebugEnabled()) {      LOG.debug("command array:");      LOG.debug(Arrays.toString(fullCommandArray));      LOG.debug("Privileged Execution Operation Output:");
public IOStreamPair executePrivilegedInteractiveOperation(List<String> prefixCommands,PrivilegedOperation operation) throws PrivilegedOperationException, InterruptedException {  String[] fullCommandArray=getPrivilegedOperationExecutionCommand(prefixCommands,operation);  ProcessBuilder pb=new ProcessBuilder(fullCommandArray);  OutputStream stdin;  InputStream stdout;  try {    pb.redirectErrorStream(true);    Process p=pb.start();    stdin=p.getOutputStream();    stdout=p.getInputStream();    if (LOG.isDebugEnabled()) {      LOG.debug("command array:");
        resetCGroupParameters();        LOG.info("Listener stopped before starting");        return;      }    }    LOG.info(String.format("Listening on %s with %s",yarnCGroupPath,oomListenerPath));    executor=Executors.newFixedThreadPool(2);    Future<String> errorListener=executor.submit(() -> IOUtils.toString(process.getErrorStream(),Charset.defaultCharset()));    InputStream events=process.getInputStream();    byte[] event=new byte[8];    int read;    while ((read=events.read(event)) == event.length) {      resolveOOM(executor);    }    if (read != -1) {      LOG.warn(String.format("Characters returned from event hander: %d",read));    }    int exitCode=process.waitFor();    String error=errorListener.get();
private void mountCGroupController(CGroupController controller) throws ResourceHandlerException {  String existingMountPath=getControllerPath(controller);  String requestedMountPath=new File(cGroupsMountConfig.getMountPath(),controller.getName()).getAbsolutePath();  if (existingMountPath == null || !requestedMountPath.equals(existingMountPath)) {    rwLock.writeLock().lock();    try {      String mountOptions;      if (existingMountPath != null) {        mountOptions=Joiner.on(',').join(parsedMtab.get(existingMountPath));      } else {        mountOptions=controller.getName();      }      String cGroupKV=mountOptions + "=" + requestedMountPath;      PrivilegedOperation.OperationType opType=PrivilegedOperation.OperationType.MOUNT_CGROUPS;      PrivilegedOperation op=new PrivilegedOperation(opType);      op.appendArgs(cGroupPrefix,cGroupKV);
    rwLock.writeLock().lock();    try {      String mountOptions;      if (existingMountPath != null) {        mountOptions=Joiner.on(',').join(parsedMtab.get(existingMountPath));      } else {        mountOptions=controller.getName();      }      String cGroupKV=mountOptions + "=" + requestedMountPath;      PrivilegedOperation.OperationType opType=PrivilegedOperation.OperationType.MOUNT_CGROUPS;      PrivilegedOperation op=new PrivilegedOperation(opType);      op.appendArgs(cGroupPrefix,cGroupKV);      LOG.info("Mounting controller " + controller.getName() + " at "+ requestedMountPath);      privilegedOperationExecutor.executePrivilegedOperation(op,false);      controllerPaths.put(controller,requestedMountPath);    } catch (    PrivilegedOperationException e) {
@Override public String createCGroup(CGroupController controller,String cGroupId) throws ResourceHandlerException {  String path=getPathForCGroup(controller,cGroupId);
private void logLineFromTasksFile(File cgf){  String str;  if (LOG.isDebugEnabled()) {    try (BufferedReader inl=new BufferedReader(new InputStreamReader(new FileInputStream(cgf + "/tasks"),"UTF-8"))){      str=inl.readLine();      if (str != null) {
@Override public void deleteCGroup(CGroupController controller,String cGroupId) throws ResourceHandlerException {  boolean deleted=false;  String cGroupPath=getPathForCGroup(controller,cGroupId);
@Override public void updateCGroupParam(CGroupController controller,String cGroupId,String param,String value) throws ResourceHandlerException {  String cGroupParamPath=getPathForCGroupParam(controller,cGroupId,param);  PrintWriter pw=null;
@Override public float getCpuUsagePercent(){  float cgroupUsage=cgroup.getCpuUsagePercent();  if (LOG.isDebugEnabled()) {    float procfsUsage=procfs.getCpuUsagePercent();
@Override public float getCpuUsagePercent(){  float cgroupUsage=cgroup.getCpuUsagePercent();  if (LOG.isDebugEnabled()) {    float procfsUsage=procfs.getCpuUsagePercent();    LOG.debug("CPU Comparison:" + procfsUsage + " "+ cgroupUsage);
@Override public long getRssMemorySize(int olderThanAge){  if (LOG.isDebugEnabled()) {
@Override public long getVirtualMemorySize(int olderThanAge){  if (LOG.isDebugEnabled()) {
private boolean sigKill(Container container){  boolean containerKilled=false;  boolean finished=false;  try {    while (!finished) {      String[] pids=cgroups.getCGroupParam(CGroupsHandler.CGroupController.MEMORY,container.getContainerId().toString(),CGROUP_PROCS_FILE).split("\n");      finished=true;      for (      String pid : pids) {        if (pid != null && !pid.isEmpty()) {
@Override public List<PrivilegedOperation> postComplete(ContainerId containerId) throws ResourceHandlerException {
public static NetworkTagMappingManager getManager(Configuration conf){  Class<? extends NetworkTagMappingManager> managerClass=conf.getClass(YarnConfiguration.NM_NETWORK_TAG_MAPPING_MANAGER,NetworkTagMappingJsonManager.class,NetworkTagMappingManager.class);
private static CGroupsHandler getInitializedCGroupsHandler(Configuration conf) throws ResourceHandlerException {  if (cGroupsHandler == null) {synchronized (CGroupsHandler.class) {      if (cGroupsHandler == null) {        cGroupsHandler=new CGroupsHandlerImpl(conf,PrivilegedOperationExecutor.getInstance(conf));
@Override public List<PrivilegedOperation> reacquireContainer(ContainerId containerId) throws ResourceHandlerException {  String containerIdStr=containerId.toString();
@Override public List<PrivilegedOperation> reacquireContainer(ContainerId containerId) throws ResourceHandlerException {  String containerIdStr=containerId.toString();  LOG.debug("Attempting to reacquire classId for container: {}",containerIdStr);  String classIdStrFromFile=cGroupsHandler.getCGroupParam(CGroupsHandler.CGroupController.NET_CLS,containerIdStr,CGroupsHandler.CGROUP_PARAM_CLASSID);  int classId=trafficController.getClassIdFromFileContents(classIdStrFromFile);
@Override public List<PrivilegedOperation> postComplete(ContainerId containerId) throws ResourceHandlerException {
private boolean checkIfAlreadyBootstrapped(String state) throws ResourceHandlerException {  List<String> regexes=new ArrayList<>();  regexes.add(String.format("^qdisc htb %d: root(.)*$",ROOT_QDISC_HANDLE));  regexes.add(String.format("^filter parent %d: protocol ip " + "(.)*cgroup(.)*$",ROOT_QDISC_HANDLE));  regexes.add(String.format("^class htb %d:%d root(.)*$",ROOT_QDISC_HANDLE,ROOT_CLASS_ID));  regexes.add(String.format("^class htb %d:%d parent %d:%d(.)*$",ROOT_QDISC_HANDLE,DEFAULT_CLASS_ID,ROOT_QDISC_HANDLE,ROOT_CLASS_ID));  regexes.add(String.format("^class htb %d:%d parent %d:%d(.)*$",ROOT_QDISC_HANDLE,YARN_ROOT_CLASS_ID,ROOT_QDISC_HANDLE,ROOT_CLASS_ID));  for (  String regex : regexes) {    Pattern pattern=Pattern.compile(regex,Pattern.MULTILINE);    if (pattern.matcher(state).find()) {
private String readState() throws ResourceHandlerException {  BatchBuilder builder=new BatchBuilder(PrivilegedOperation.OperationType.TC_READ_STATE).readState();  PrivilegedOperation op=builder.commitBatchToTempFile();  try {    String output=privilegedOperationExecutor.executePrivilegedOperation(op,true);
private void reacquireContainerClasses(String state){  String tcClassesStr=state.substring(state.indexOf("class"));  String[] tcClasses=Pattern.compile("$",Pattern.MULTILINE).split(tcClassesStr);  Pattern tcClassPattern=Pattern.compile(String.format("class htb %d:(\\d+) .*",ROOT_QDISC_HANDLE));synchronized (classIdSet) {    for (    String tcClassSplit : tcClasses) {      String tcClass=tcClassSplit.trim();      if (!tcClass.isEmpty()) {        Matcher classMatcher=tcClassPattern.matcher(tcClass);        if (classMatcher.matches()) {          int classId=Integer.parseInt(classMatcher.group(1));          if (classId >= MIN_CONTAINER_CLASS_ID) {            classIdSet.set(classId - MIN_CONTAINER_CLASS_ID);
public Map<Integer,Integer> readStats() throws ResourceHandlerException {  BatchBuilder builder=new BatchBuilder(PrivilegedOperation.OperationType.TC_READ_STATS).readClasses();  PrivilegedOperation op=builder.commitBatchToTempFile();  try {    String output=privilegedOperationExecutor.executePrivilegedOperation(op,true);
public Map<Integer,Integer> readStats() throws ResourceHandlerException {  BatchBuilder builder=new BatchBuilder(PrivilegedOperation.OperationType.TC_READ_STATS).readClasses();  PrivilegedOperation op=builder.commitBatchToTempFile();  try {    String output=privilegedOperationExecutor.executePrivilegedOperation(op,true);    LOG.debug("TC stats output:{}",output);    Map<Integer,Integer> classIdBytesStats=parseStatsString(output);
public int getClassIdFromFileContents(String input){  String classIdStr=String.format("%08x",Integer.parseInt(input));
public synchronized void updateFpga(String requestor,FpgaDevice device,String newIPID,String newHash){  device.setIPID(newIPID);  device.setAocxHash(newHash);
public synchronized void updateFpga(String requestor,FpgaDevice device,String newIPID,String newHash){  device.setIPID(newIPID);  device.setAocxHash(newHash);  LOG.info("Update IPID to " + newIPID + " for this allocated device: "+ device);
@Override public List<PrivilegedOperation> preStart(Container container) throws ResourceHandlerException {  List<PrivilegedOperation> ret=new ArrayList<>();  String containerIdStr=container.getContainerId().toString();  Resource requestedResource=container.getResource();  cGroupsHandler.createCGroup(CGroupsHandler.CGroupController.DEVICES,containerIdStr);  long deviceCount=requestedResource.getResourceValue(FPGA_URI);
  Resource requestedResource=container.getResource();  cGroupsHandler.createCGroup(CGroupsHandler.CGroupController.DEVICES,containerIdStr);  long deviceCount=requestedResource.getResourceValue(FPGA_URI);  LOG.info(containerIdStr + " requested " + deviceCount+ " Intel FPGA(s)");  String ipFilePath=null;  try {    final String requestedIPID=getRequestedIPID(container);    String localizedIPIDHash=null;    ipFilePath=vendorPlugin.retrieveIPfilePath(requestedIPID,container.getWorkDir(),container.getResourceSet().getLocalizedResources());    if (ipFilePath != null) {      try (FileInputStream fis=new FileInputStream(ipFilePath)){        localizedIPIDHash=DigestUtils.sha256Hex(fis);      } catch (      IOException e) {        throw new ResourceHandlerException("Could not calculate SHA-256",e);      }    }    FpgaResourceAllocator.FpgaAllocation allocation=allocator.assignFpga(vendorPlugin.getFpgaType(),deviceCount,container,localizedIPIDHash);
      } catch (      IOException e) {        throw new ResourceHandlerException("Could not calculate SHA-256",e);      }    }    FpgaResourceAllocator.FpgaAllocation allocation=allocator.assignFpga(vendorPlugin.getFpgaType(),deviceCount,container,localizedIPIDHash);    LOG.info("FpgaAllocation:" + allocation);    PrivilegedOperation privilegedOperation=new PrivilegedOperation(PrivilegedOperation.OperationType.FPGA,Arrays.asList(CONTAINER_ID_CLI_OPTION,containerIdStr));    if (!allocation.getDenied().isEmpty()) {      List<Integer> denied=new ArrayList<>();      allocation.getDenied().forEach(device -> denied.add(device.getMinor()));      privilegedOperation.appendArgs(Arrays.asList(EXCLUDED_FPGAS_CLI_OPTION,StringUtils.join(",",denied)));    }    privilegedOperationExecutor.executePrivilegedOperation(privilegedOperation,true);    if (deviceCount > 0) {      ipFilePath=vendorPlugin.retrieveIPfilePath(getRequestedIPID(container),container.getWorkDir(),container.getResourceSet().getLocalizedResources());      if (ipFilePath == null) {        LOG.warn("FPGA plugin failed to downloaded IP, please check the" + " value of environment viable: " + REQUEST_FPGA_IP_ID_KEY + " if you want YARN to program the device");      } else {
private synchronized GpuAllocation internalAssignGpus(Container container) throws ResourceHandlerException {  Resource requestedResource=container.getResource();  ContainerId containerId=container.getContainerId();  int numRequestedGpuDevices=getRequestedGpus(requestedResource);  if (numRequestedGpuDevices > 0) {    if (LOG.isDebugEnabled()) {
public synchronized void unassignGpus(ContainerId containerId){  if (LOG.isDebugEnabled()) {
@Override public List<PrivilegedOperation> bootstrap(Configuration configuration) throws ResourceHandlerException {  List<GpuDevice> usableGpus;  try {    usableGpus=gpuDiscoverer.getGpusUsableByYarn();    if (usableGpus == null || usableGpus.isEmpty()) {      String message="GPU is enabled on the NodeManager, but couldn't find " + "any usable GPU devices, please double check configuration!";
public boolean isResourcesAvailable(Resource resource){
private NumaResourceAllocation allocate(ContainerId containerId,Resource resource){  for (int index=0; index < numaNodesList.size(); index++) {    NumaNodeResource numaNode=numaNodesList.get((currentAssignNode + index) % numaNodesList.size());    if (numaNode.isResourcesAvailable(resource)) {      numaNode.assignResources(resource,containerId);
    NumaNodeResource numaNode=numaNodesList.get((currentAssignNode + index) % numaNodesList.size());    if (numaNode.isResourcesAvailable(resource)) {      numaNode.assignResources(resource,containerId);      LOG.info("Assigning NUMA node " + numaNode.getNodeId() + " for memory, "+ numaNode.getNodeId()+ " for cpus for the "+ containerId);      currentAssignNode=(currentAssignNode + index + 1) % numaNodesList.size();      return new NumaResourceAllocation(numaNode.getNodeId(),resource.getMemorySize(),numaNode.getNodeId(),resource.getVirtualCores());    }  }  long memoryRequirement=resource.getMemorySize();  Map<String,Long> memoryAllocations=Maps.newHashMap();  for (  NumaNodeResource numaNode : numaNodesList) {    long memoryRemaining=numaNode.assignAvailableMemory(memoryRequirement,containerId);    memoryAllocations.put(numaNode.getNodeId(),memoryRequirement - memoryRemaining);    memoryRequirement=memoryRemaining;    if (memoryRequirement == 0) {      break;    }  }  if (memoryRequirement != 0) {
    if (memoryRequirement == 0) {      break;    }  }  if (memoryRequirement != 0) {    LOG.info("There is no available memory:" + resource.getMemorySize() + " in numa nodes for "+ containerId);    releaseNumaResource(containerId);    return null;  }  int cpusRequirement=resource.getVirtualCores();  Map<String,Integer> cpuAllocations=Maps.newHashMap();  for (int index=0; index < numaNodesList.size(); index++) {    NumaNodeResource numaNode=numaNodesList.get((currentAssignNode + index) % numaNodesList.size());    int cpusRemaining=numaNode.assignAvailableCpus(cpusRequirement,containerId);    cpuAllocations.put(numaNode.getNodeId(),cpusRequirement - cpusRemaining);    cpusRequirement=cpusRemaining;    if (cpusRequirement == 0) {      currentAssignNode=(currentAssignNode + index + 1) % numaNodesList.size();
  if (memoryRequirement != 0) {    LOG.info("There is no available memory:" + resource.getMemorySize() + " in numa nodes for "+ containerId);    releaseNumaResource(containerId);    return null;  }  int cpusRequirement=resource.getVirtualCores();  Map<String,Integer> cpuAllocations=Maps.newHashMap();  for (int index=0; index < numaNodesList.size(); index++) {    NumaNodeResource numaNode=numaNodesList.get((currentAssignNode + index) % numaNodesList.size());    int cpusRemaining=numaNode.assignAvailableCpus(cpusRequirement,containerId);    cpuAllocations.put(numaNode.getNodeId(),cpusRequirement - cpusRemaining);    cpusRequirement=cpusRemaining;    if (cpusRequirement == 0) {      currentAssignNode=(currentAssignNode + index + 1) % numaNodesList.size();      break;    }  }  if (cpusRequirement != 0) {
public synchronized void releaseNumaResource(ContainerId containerId){
private String runDockerVolumeCommand(DockerVolumeCommand dockerVolumeCommand,Container container) throws ContainerExecutionException {  try {    String commandFile=dockerClient.writeCommandToTempFile(dockerVolumeCommand,container.getContainerId(),nmContext);    PrivilegedOperation privOp=new PrivilegedOperation(PrivilegedOperation.OperationType.RUN_DOCKER_CMD);    privOp.appendArgs(commandFile);    String output=privilegedOperationExecutor.executePrivilegedOperation(null,privOp,null,null,true,false);
private void setHostname(DockerRunCommand runCommand,String containerIdStr,String network,String name) throws ContainerExecutionException {  if (network.equalsIgnoreCase("host")) {    if (name != null && !name.isEmpty()) {
  dockerExecCommand.setTTY();  List<String> command=new ArrayList<String>();  StringBuilder sb=new StringBuilder();  sb.append("/bin/");  sb.append(ctx.getShell());  command.add(sb.toString());  command.add("-i");  dockerExecCommand.setOverrideCommandWithArgs(command);  String commandFile=dockerClient.writeCommandToTempFile(dockerExecCommand,ContainerId.fromString(containerId),nmContext);  PrivilegedOperation privOp=new PrivilegedOperation(PrivilegedOperation.OperationType.EXEC_CONTAINER);  privOp.appendArgs(commandFile);  privOp.disableFailureLogging();  IOStreamPair output;  try {    output=privilegedOperationExecutor.executePrivilegedInteractiveOperation(null,privOp);
@Override public String[] getIpAndHost(Container container){  ContainerId containerId=container.getContainerId();  String containerIdStr=containerId.toString();  DockerInspectCommand inspectCommand=new DockerInspectCommand(containerIdStr).getIpAndHost();  try {    String output=executeDockerInspect(containerId,inspectCommand);
          network=defaultNetwork;        }      } catch (      NullPointerException e) {        network=defaultNetwork;      }      boolean useHostNetwork=network.equalsIgnoreCase("host");      if (useHostNetwork) {        InetAddress address;        try {          address=InetAddress.getLocalHost();          ips=address.getHostAddress();        } catch (        UnknownHostException e) {          LOG.error("Can not determine IP for container:" + containerId);        }      }    }    String[] ipAndHost=new String[2];    ipAndHost[0]=ips;    ipAndHost[1]=host;    return ipAndHost;
      } catch (      NullPointerException e) {        network=defaultNetwork;      }      boolean useHostNetwork=network.equalsIgnoreCase("host");      if (useHostNetwork) {        InetAddress address;        try {          address=InetAddress.getLocalHost();          ips=address.getHostAddress();        } catch (        UnknownHostException e) {          LOG.error("Can not determine IP for container:" + containerId);        }      }    }    String[] ipAndHost=new String[2];    ipAndHost[0]=ips;    ipAndHost[1]=host;    return ipAndHost;  } catch (  ContainerExecutionException e) {
public void pullImageFromRemote(String containerIdStr,String imageName) throws ContainerExecutionException {  long start=System.currentTimeMillis();  DockerPullCommand dockerPullCommand=new DockerPullCommand(imageName);
public void pullImageFromRemote(String containerIdStr,String imageName) throws ContainerExecutionException {  long start=System.currentTimeMillis();  DockerPullCommand dockerPullCommand=new DockerPullCommand(imageName);  LOG.debug("now pulling docker image. image name: {}, container: {}",imageName,containerIdStr);  DockerCommandExecutor.executeDockerCommand(dockerPullCommand,containerIdStr,null,privilegedOperationExecutor,false,nmContext);  long end=System.currentTimeMillis();  long pullImageTimeMs=end - start;
  DockerInspectCommand inspectCommand=new DockerInspectCommand(containerId.toString()).get(new String[]{DockerInspectCommand.STATUS_TEMPLATE,DockerInspectCommand.STOPSIGNAL_TEMPLATE},delimiter);  try {    String output=executeDockerInspect(containerId,inspectCommand).trim();    if (!output.isEmpty()) {      String[] statusAndSignal=StringUtils.split(output,delimiter);      containerStatus=DockerCommandExecutor.parseContainerStatus(statusAndSignal[0]);      if (statusAndSignal.length > 1) {        stopSignal=statusAndSignal[1];      }    }  } catch (  ContainerExecutionException|PrivilegedOperationException e) {    LOG.debug("{} inspect failed, skipping stop",containerId,e);    return;  }  if (DockerCommandExecutor.isStoppable(containerStatus)) {    DockerKillCommand dockerStopCommand=new DockerKillCommand(containerId.toString()).setSignal(stopSignal);    DockerCommandExecutor.executeDockerCommand(dockerStopCommand,containerId.toString(),env,privilegedOperationExecutor,false,nmContext);  } else {
private String executeDockerInspect(ContainerId containerId,DockerInspectCommand inspectCommand) throws ContainerExecutionException, PrivilegedOperationException {  String commandFile=dockerClient.writeCommandToTempFile(inspectCommand,containerId,nmContext);  PrivilegedOperation privOp=new PrivilegedOperation(PrivilegedOperation.OperationType.RUN_DOCKER_CMD);  privOp.appendArgs(commandFile);  String output=privilegedOperationExecutor.executePrivilegedOperation(null,privOp,null,null,true,false);
private void handleContainerKill(ContainerRuntimeContext ctx,Map<String,String> env,ContainerExecutor.Signal signal) throws ContainerExecutionException {  Container container=ctx.getContainer();  ContainerVolumePublisher publisher=new ContainerVolumePublisher(container,container.getCsiVolumesRootDir(),this);  try {    publisher.unpublishVolumes();  } catch (  YarnException|IOException e) {    throw new ContainerExecutionException(e);  }  boolean serviceMode=Boolean.parseBoolean(env.get(ENV_DOCKER_CONTAINER_DOCKER_SERVICE_MODE));  if (isContainerRequestedAsPrivileged(container) || serviceMode) {    String containerId=container.getContainerId().toString();    DockerCommandExecutor.DockerContainerStatus containerStatus=DockerCommandExecutor.getContainerStatus(containerId,privilegedOperationExecutor,nmContext);    if (DockerCommandExecutor.isKillable(containerStatus)) {      DockerKillCommand dockerKillCommand=new DockerKillCommand(containerId).setSignal(signal.name());      DockerCommandExecutor.executeDockerCommand(dockerKillCommand,containerId,env,privilegedOperationExecutor,false,nmContext);    } else {
private void handleContainerRemove(String containerId,Map<String,String> env) throws ContainerExecutionException {  String delayedRemoval=env.get(ENV_DOCKER_CONTAINER_DELAYED_REMOVAL);  if (delayedRemovalAllowed && delayedRemoval != null && delayedRemoval.equalsIgnoreCase("true")) {
protected void initiateCsiClients(Configuration config) throws ContainerExecutionException {  String[] driverNames=CsiConfigUtils.getCsiDriverNames(config);  if (driverNames != null && driverNames.length > 0) {    for (    String driverName : driverNames) {      try {        InetSocketAddress adaptorServiceAddress=CsiConfigUtils.getCsiAdaptorAddressForDriver(driverName,config);
@Override public void start(){  int reapRuncLayerMountsInterval=conf.getInt(NM_REAP_RUNC_LAYER_MOUNTS_INTERVAL,DEFAULT_NM_REAP_RUNC_LAYER_MOUNTS_INTERVAL);  exec=HadoopExecutors.newScheduledThreadPool(1);  exec.scheduleAtFixedRate(new Runnable(){    @Override public void run(){      try {        PrivilegedOperation launchOp=new PrivilegedOperation(PrivilegedOperation.OperationType.REAP_RUNC_LAYER_MOUNTS);        launchOp.appendArgs(Integer.toString(layersToKeep));        try {          String stdout=privilegedOperationExecutor.executePrivilegedOperation(null,launchOp,null,null,false,false);          if (stdout != null) {
protected RuncManifestToResourcesPlugin chooseManifestToResourcesPlugin() throws ContainerExecutionException {  String pluginName=conf.get(NM_RUNC_MANIFEST_TO_RESOURCES_PLUGIN,DEFAULT_NM_RUNC_MANIFEST_TO_RESOURCES_PLUGIN);
public static DockerContainerStatus getContainerStatus(String containerId,PrivilegedOperationExecutor privilegedOperationExecutor,Context nmContext){  try {    String currentContainerStatus=executeStatusCommand(containerId,privilegedOperationExecutor,nmContext);    DockerContainerStatus dockerContainerStatus=parseContainerStatus(currentContainerStatus);
      LocalizerStatus status=createStatus();      LocalizerHeartbeatResponse response=nodemanager.heartbeat(status);switch (response.getLocalizerAction()) {case LIVE:        List<ResourceLocalizationSpec> newRsrcs=response.getResourceSpecs();      for (      ResourceLocalizationSpec newRsrc : newRsrcs) {        if (!pendingResources.containsKey(newRsrc.getResource())) {          pendingResources.put(newRsrc.getResource(),cs.submit(download(new Path(newRsrc.getDestinationDirectory().getFile()),newRsrc.getResource(),ugi)));        }      }    break;case DIE:  for (  Future<Path> pending : pendingResources.values()) {    pending.cancel(true);  }status=createStatus();try {nodemanager.heartbeat(status);} catch (YarnException e) {e.printStackTrace(System.out);
    String appId=argv[1];    String locId=argv[2];    InetSocketAddress nmAddr=new InetSocketAddress(argv[3],Integer.parseInt(argv[4]));    String tokenFileName=argv[5];    String[] sLocaldirs=Arrays.copyOfRange(argv,6,argv.length);    ArrayList<Path> localDirs=new ArrayList<>(sLocaldirs.length);    for (    String sLocaldir : sLocaldirs) {      localDirs.add(new Path(sLocaldir));    }    final String uid=UserGroupInformation.getCurrentUser().getShortUserName();    if (!user.equals(uid)) {      LOG.warn("Localization running as " + uid + " not "+ user);    }    ContainerLocalizer localizer=new ContainerLocalizer(FileContext.getLocalFSFileContext(),user,appId,locId,tokenFileName,localDirs,RecordFactoryProvider.getRecordFactory(null));    localizer.runLocalization(nmAddr);  } catch (  Throwable e) {    e.printStackTrace(System.out);
  LocalizedResource rsrc=localrsrc.get(req);switch (event.getType()) {case LOCALIZED:    if (useLocalCacheDirectoryManager) {      inProgressLocalResourcesMap.remove(req);    }  break;case REQUEST:if (rsrc != null && (!isResourcePresent(rsrc))) {  LOG.info("Resource " + rsrc.getLocalPath() + " is missing, localizing it again");  removeResource(req);  rsrc=null;}if (null == rsrc) {rsrc=new LocalizedResource(req,dispatcher);localrsrc.put(req,rsrc);}break;case RELEASE:if (null == rsrc) {ResourceReleaseEvent relEvent=(ResourceReleaseEvent)event;
break;case RECOVERED:if (rsrc != null) {LOG.warn("Ignoring attempt to recover existing resource " + rsrc);return;}rsrc=recoverResource(req,(ResourceRecoveredEvent)event);localrsrc.put(req,rsrc);break;}if (rsrc == null) {LOG.warn("Received " + event.getType() + " event for request "+ req+ " but localized resource is missing");return;}rsrc.handle(event);if (event.getType() == ResourceEventType.RELEASE) {if (rsrc.getState() == ResourceState.DOWNLOADING && rsrc.getRefCount() <= 0 && rsrc.getRequest().getVisibility() != LocalResourceVisibility.PUBLIC) {removeResource(req);}}if (event.getType() == ResourceEventType.LOCALIZED) {
@Override public boolean remove(LocalizedResource rem,DeletionService delService){  LocalizedResource rsrc=localrsrc.get(rem.getRequest());  if (null == rsrc) {
    File file=new File(uniquePath.toUri().getRawPath());    if (!file.exists()) {      rPath=uniquePath;      break;    }    LOG.warn("Directory " + uniquePath + " already exists, "+ "try next one.");    if (delService != null) {      FileDeletionTask deletionTask=new FileDeletionTask(delService,getUser(),uniquePath,null);      delService.delete(deletionTask);    }  }  Path localPath=new Path(rPath,req.getPath().getName());  LocalizedResource rsrc=localrsrc.get(req);  if (rsrc == null) {    LOG.warn("Resource " + req + " has been removed"+ " and will no longer be localized");    return null;  }  rsrc.setLocalPath(localPath);  LocalResource lr=LocalResource.newInstance(req.getResource(),req.getType(),req.getVisibility(),req.getSize(),req.getTimestamp());
@Override public void handle(ResourceEvent event){  this.writeLock.lock();  try {    Path resourcePath=event.getLocalResourceRequest().getPath();
private void validateConf(Configuration conf){  int perDirFileLimit=conf.getInt(YarnConfiguration.NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY,YarnConfiguration.DEFAULT_NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY);  if (perDirFileLimit <= 36) {
private void recoverTrackerResources(LocalResourcesTracker tracker,LocalResourceTrackerState state) throws URISyntaxException, IOException {  try (RecoveryIterator<LocalizedResourceProto> it=state.getCompletedResourcesIterator()){    while (it != null && it.hasNext()) {      LocalizedResourceProto proto=it.next();      LocalResource rsrc=new LocalResourcePBImpl(proto.getResource());      LocalResourceRequest req=new LocalResourceRequest(rsrc);
private void recoverTrackerResources(LocalResourcesTracker tracker,LocalResourceTrackerState state) throws URISyntaxException, IOException {  try (RecoveryIterator<LocalizedResourceProto> it=state.getCompletedResourcesIterator()){    while (it != null && it.hasNext()) {      LocalizedResourceProto proto=it.next();      LocalResource rsrc=new LocalResourcePBImpl(proto.getResource());      LocalResourceRequest req=new LocalResourceRequest(rsrc);      LOG.debug("Recovering localized resource {} at {}",req,proto.getLocalPath());      tracker.handle(new ResourceRecoveredEvent(req,new Path(proto.getLocalPath()),proto.getSize()));    }  }   try (RecoveryIterator<Map.Entry<LocalResourceProto,Path>> it=state.getStartedResourcesIterator()){    while (it != null && it.hasNext()) {      Map.Entry<LocalResourceProto,Path> entry=it.next();      LocalResource rsrc=new LocalResourcePBImpl(entry.getKey());      LocalResourceRequest req=new LocalResourceRequest(rsrc);      Path localPath=entry.getValue();      tracker.handle(new ResourceRecoveredEvent(req,localPath,0));
@Override public void serviceStart() throws Exception {  cacheCleanup.scheduleWithFixedDelay(new CacheCleanup(dispatcher),cacheCleanupPeriod,cacheCleanupPeriod,TimeUnit.MILLISECONDS);  server=createServer();  server.start();  localizationServerAddress=getConfig().updateConnectAddr(YarnConfiguration.NM_BIND_HOST,YarnConfiguration.NM_LOCALIZER_ADDRESS,YarnConfiguration.DEFAULT_NM_LOCALIZER_ADDRESS,server.getListenerAddress());
private void deleteAppLogDir(FileContext fs,DeletionService del,String logDir) throws IOException {  RemoteIterator<FileStatus> fileStatuses=fs.listStatus(new Path(logDir));  if (fileStatuses != null) {    while (fileStatuses.hasNext()) {      FileStatus fileStatus=fileStatuses.next();      String appName=fileStatus.getPath().getName();      if (appName.matches("^application_\\d+_\\d+_DEL_\\d+$")) {
@SuppressWarnings("unchecked") @Override public Token<LocalizerTokenIdentifier> selectToken(Text service,Collection<Token<? extends TokenIdentifier>> tokens){  LOG.debug("Using localizerTokenSelector.");  for (  Token<? extends TokenIdentifier> token : tokens) {
    fs.mkdirs(directoryPath,DIRECTORY_PERMISSION);    tempPath=new Path(directoryPath,getTemporaryFileName(actualPath));    if (!uploadFile(actualPath,tempPath)) {      LOG.warn("Could not copy the file to the shared cache at " + tempPath);      return false;    }    fs.setPermission(tempPath,FILE_PERMISSION);    Path finalPath=new Path(directoryPath,actualPath.getName());    if (!fs.rename(tempPath,finalPath)) {      LOG.warn("The file already exists under " + finalPath + ". Ignoring this attempt.");      deleteTempFile(tempPath);      return false;    }    if (!notifySharedCacheManager(checksumVal,actualPath.getName())) {      fs.delete(finalPath,false);      return false;    }    short replication=(short)conf.getInt(YarnConfiguration.SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR,YarnConfiguration.DEFAULT_SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR);
  Class<? extends ContainerLogAggregationPolicy> policyClass=null;  if (this.logAggregationContext != null) {    String className=this.logAggregationContext.getLogAggregationPolicyClassName();    if (className != null) {      try {        Class<?> policyFromContext=conf.getClassByName(className);        if (ContainerLogAggregationPolicy.class.isAssignableFrom(policyFromContext)) {          policyClass=policyFromContext.asSubclass(ContainerLogAggregationPolicy.class);        } else {          LOG.warn(this.appId + " specified invalid log aggregation policy " + className);        }      } catch (      ClassNotFoundException cnfe) {        LOG.warn(this.appId + " specified invalid log aggregation policy " + className);      }    }  }  if (policyClass == null) {    policyClass=conf.getClass(YarnConfiguration.NM_LOG_AGG_POLICY_CLASS,AllContainerLogAggregationPolicy.class,ContainerLogAggregationPolicy.class);  } else {
    return;  }  addCredentials();  Set<ContainerId> pendingContainerInThisCycle=new HashSet<ContainerId>();  this.pendingContainers.drainTo(pendingContainerInThisCycle);  Set<ContainerId> finishedContainers=new HashSet<ContainerId>(pendingContainerInThisCycle);  if (this.context.getApplications().get(this.appId) != null) {    for (    Container container : this.context.getApplications().get(this.appId).getContainers().values()) {      ContainerType containerType=container.getContainerTokenIdentifier().getContainerType();      if (shouldUploadLogs(new ContainerLogContext(container.getContainerId(),containerType,0))) {        pendingContainerInThisCycle.add(container.getContainerId());      }    }  }  if (pendingContainerInThisCycle.isEmpty()) {    LOG.debug("No pending container in this cycle");    sendLogAggregationReport(true,"",appFinished);    return;  }  logAggregationTimes++;
      if (shouldUploadLogs(new ContainerLogContext(container.getContainerId(),containerType,0))) {        pendingContainerInThisCycle.add(container.getContainerId());      }    }  }  if (pendingContainerInThisCycle.isEmpty()) {    LOG.debug("No pending container in this cycle");    sendLogAggregationReport(true,"",appFinished);    return;  }  logAggregationTimes++;  LOG.debug("Cycle #{} of log aggregator",logAggregationTimes);  String diagnosticMessage="";  boolean logAggregationSucceedInThisCycle=true;  DeletionTask deletionTask=null;  try {    try {      logAggregationFileController.initializeWriter(logControllerContext);    } catch (    IOException e1) {
              LOG.error("Failed to get log file size " + e1);            }          }        }        deletionTask=new FileDeletionTask(delService,this.userUgi.getShortUserName(),null,uploadedFilePathsInThisCycleList);      }      if (finishedContainers.contains(container)) {        containerLogAggregators.remove(container);      }    }    logControllerContext.setUploadedLogsInThisCycle(uploadedLogsInThisCycle);    logControllerContext.setLogUploadTimeStamp(System.currentTimeMillis());    logControllerContext.increLogAggregationTimes();    try {      this.logAggregationFileController.postWrite(logControllerContext);      diagnosticMessage="Log uploaded successfully for Application: " + appId + " in NodeManager: "+ LogAggregationUtils.getNodeString(nodeId)+ " at "+ Times.format(logControllerContext.getLogUploadTimeStamp())+ "\n";    } catch (    Exception e) {      diagnosticMessage=e.getMessage();      renameTemporaryLogFileFailed=true;      logAggregationSucceedInThisCycle=false;    }  }  finally {
private void addCredentials(){  if (UserGroupInformation.isSecurityEnabled()) {    Credentials systemCredentials=context.getSystemCredentialsForApps().get(appId);    if (systemCredentials != null) {
          }          uploadLogsForContainers(false);        } else {          wait(THREAD_SLEEP_TIME);        }      } catch (      InterruptedException e) {        LOG.warn("PendingContainers queue is interrupted");        this.appFinishing.set(true);      }catch (      LogAggregationDFSException e) {        this.appFinishing.set(true);        throw e;      }    }  }  if (this.aborted.get()) {    return;  }  try {    uploadLogsForContainers(true);    doAppLogAggregationPostCleanUp();  } catch (  LogAggregationDFSException e) {
@Override public void startContainerLogAggregation(ContainerLogContext logContext){  if (shouldUploadLogs(logContext)) {
private static long calculateRollingMonitorInterval(Configuration conf){  long interval=conf.getLong(YarnConfiguration.NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS,YarnConfiguration.DEFAULT_NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS);  if (interval <= 0) {
protected void serviceInit(Configuration conf) throws Exception {  int threadPoolSize=getAggregatorThreadPoolSize(conf);  this.threadPool=HadoopExecutors.newFixedThreadPool(threadPoolSize,new ThreadFactoryBuilder().setNameFormat("LogAggregationService #%d").build());  rollingMonitorInterval=calculateRollingMonitorInterval(conf);
private void recover() throws IOException {  if (stateStore.canRecover()) {    RecoveredLogDeleterState state=stateStore.loadLogDeleterState();    long now=System.currentTimeMillis();    for (    Map.Entry<ApplicationId,LogDeleterProto> entry : state.getLogDeleterMap().entrySet()) {      ApplicationId appId=entry.getKey();      LogDeleterProto proto=entry.getValue();      long deleteDelayMsec=proto.getDeletionTime() - now;
@SuppressWarnings("unchecked") @Override public void handle(LogHandlerEvent event){switch (event.getType()) {case APPLICATION_STARTED:    LogHandlerAppStartedEvent appStartedEvent=(LogHandlerAppStartedEvent)event;  this.appOwners.put(appStartedEvent.getApplicationId(),appStartedEvent.getUser());this.dispatcher.getEventHandler().handle(new ApplicationEvent(appStartedEvent.getApplicationId(),ApplicationEventType.APPLICATION_LOG_HANDLING_INITED));break;case CONTAINER_FINISHED:break;case APPLICATION_FINISHED:LogHandlerAppFinishedEvent appFinishedEvent=(LogHandlerAppFinishedEvent)event;ApplicationId appId=appFinishedEvent.getApplicationId();
@SuppressWarnings("unchecked") @Override public void handle(LogHandlerEvent event){switch (event.getType()) {case APPLICATION_STARTED:    LogHandlerAppStartedEvent appStartedEvent=(LogHandlerAppStartedEvent)event;  this.appOwners.put(appStartedEvent.getApplicationId(),appStartedEvent.getUser());this.dispatcher.getEventHandler().handle(new ApplicationEvent(appStartedEvent.getApplicationId(),ApplicationEventType.APPLICATION_LOG_HANDLING_INITED));break;case CONTAINER_FINISHED:break;case APPLICATION_FINISHED:LogHandlerAppFinishedEvent appFinishedEvent=(LogHandlerAppFinishedEvent)event;ApplicationId appId=appFinishedEvent.getApplicationId();LOG.info("Scheduling Log Deletion for application: " + appId + ", with delay of "+ this.deleteDelaySeconds+ " seconds");String user=appOwners.remove(appId);if (user == null) {
case CONTAINER_FINISHED:break;case APPLICATION_FINISHED:LogHandlerAppFinishedEvent appFinishedEvent=(LogHandlerAppFinishedEvent)event;ApplicationId appId=appFinishedEvent.getApplicationId();LOG.info("Scheduling Log Deletion for application: " + appId + ", with delay of "+ this.deleteDelaySeconds+ " seconds");String user=appOwners.remove(appId);if (user == null) {LOG.error("Unable to locate user for " + appId);NonAggregatingLogHandler.this.dispatcher.getEventHandler().handle(new ApplicationEvent(appId,ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED));break;}LogDeleterRunnable logDeleter=new LogDeleterRunnable(user,appId);long deletionTimestamp=System.currentTimeMillis() + this.deleteDelaySeconds * 1000;LogDeleterProto deleterProto=LogDeleterProto.newBuilder().setUser(user).setDeletionTime(deletionTimestamp).build();try {stateStore.storeLogDeleter(appId,deleterProto);} catch (IOException e) {
  processTreeClass=this.conf.getClass(YarnConfiguration.NM_CONTAINER_MON_PROCESS_TREE,null,ResourceCalculatorProcessTree.class);  LOG.info("Using ResourceCalculatorProcessTree: {}",this.processTreeClass);  this.containerMetricsEnabled=this.conf.getBoolean(YarnConfiguration.NM_CONTAINER_METRICS_ENABLE,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_ENABLE);  this.containerMetricsPeriodMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_PERIOD_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS);  this.containerMetricsUnregisterDelayMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS);  long configuredPMemForContainers=NodeManagerHardwareUtils.getContainerMemoryMB(this.resourceCalculatorPlugin,this.conf);  int configuredVCoresForContainers=NodeManagerHardwareUtils.getVCores(this.resourceCalculatorPlugin,this.conf);  vmemRatio=this.conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO,YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);  Preconditions.checkArgument(vmemRatio > 0.99f,YarnConfiguration.NM_VMEM_PMEM_RATIO + " should be at least 1.0");  Resource resourcesForContainers=Resource.newInstance(configuredPMemForContainers,configuredVCoresForContainers);  setAllocatedResourcesForContainers(resourcesForContainers);  pmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);  vmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);  elasticMemoryEnforcement=this.conf.getBoolean(YarnConfiguration.NM_ELASTIC_MEMORY_CONTROL_ENABLED,YarnConfiguration.DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED);  strictMemoryEnforcement=conf.getBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENFORCED,YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENFORCED);
  LOG.info("Using ResourceCalculatorProcessTree: {}",this.processTreeClass);  this.containerMetricsEnabled=this.conf.getBoolean(YarnConfiguration.NM_CONTAINER_METRICS_ENABLE,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_ENABLE);  this.containerMetricsPeriodMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_PERIOD_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS);  this.containerMetricsUnregisterDelayMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS);  long configuredPMemForContainers=NodeManagerHardwareUtils.getContainerMemoryMB(this.resourceCalculatorPlugin,this.conf);  int configuredVCoresForContainers=NodeManagerHardwareUtils.getVCores(this.resourceCalculatorPlugin,this.conf);  vmemRatio=this.conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO,YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);  Preconditions.checkArgument(vmemRatio > 0.99f,YarnConfiguration.NM_VMEM_PMEM_RATIO + " should be at least 1.0");  Resource resourcesForContainers=Resource.newInstance(configuredPMemForContainers,configuredVCoresForContainers);  setAllocatedResourcesForContainers(resourcesForContainers);  pmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);  vmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);  elasticMemoryEnforcement=this.conf.getBoolean(YarnConfiguration.NM_ELASTIC_MEMORY_CONTROL_ENABLED,YarnConfiguration.DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED);  strictMemoryEnforcement=conf.getBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENFORCED,YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENFORCED);  LOG.info("Physical memory check enabled: {}",pmemCheckEnabled);
  this.containerMetricsEnabled=this.conf.getBoolean(YarnConfiguration.NM_CONTAINER_METRICS_ENABLE,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_ENABLE);  this.containerMetricsPeriodMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_PERIOD_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS);  this.containerMetricsUnregisterDelayMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS);  long configuredPMemForContainers=NodeManagerHardwareUtils.getContainerMemoryMB(this.resourceCalculatorPlugin,this.conf);  int configuredVCoresForContainers=NodeManagerHardwareUtils.getVCores(this.resourceCalculatorPlugin,this.conf);  vmemRatio=this.conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO,YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);  Preconditions.checkArgument(vmemRatio > 0.99f,YarnConfiguration.NM_VMEM_PMEM_RATIO + " should be at least 1.0");  Resource resourcesForContainers=Resource.newInstance(configuredPMemForContainers,configuredVCoresForContainers);  setAllocatedResourcesForContainers(resourcesForContainers);  pmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);  vmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);  elasticMemoryEnforcement=this.conf.getBoolean(YarnConfiguration.NM_ELASTIC_MEMORY_CONTROL_ENABLED,YarnConfiguration.DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED);  strictMemoryEnforcement=conf.getBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENFORCED,YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENFORCED);  LOG.info("Physical memory check enabled: {}",pmemCheckEnabled);  LOG.info("Virtual memory check enabled: {}",vmemCheckEnabled);
  this.containerMetricsPeriodMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_PERIOD_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS);  this.containerMetricsUnregisterDelayMs=this.conf.getLong(YarnConfiguration.NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS,YarnConfiguration.DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS);  long configuredPMemForContainers=NodeManagerHardwareUtils.getContainerMemoryMB(this.resourceCalculatorPlugin,this.conf);  int configuredVCoresForContainers=NodeManagerHardwareUtils.getVCores(this.resourceCalculatorPlugin,this.conf);  vmemRatio=this.conf.getFloat(YarnConfiguration.NM_VMEM_PMEM_RATIO,YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO);  Preconditions.checkArgument(vmemRatio > 0.99f,YarnConfiguration.NM_VMEM_PMEM_RATIO + " should be at least 1.0");  Resource resourcesForContainers=Resource.newInstance(configuredPMemForContainers,configuredVCoresForContainers);  setAllocatedResourcesForContainers(resourcesForContainers);  pmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);  vmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);  elasticMemoryEnforcement=this.conf.getBoolean(YarnConfiguration.NM_ELASTIC_MEMORY_CONTROL_ENABLED,YarnConfiguration.DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED);  strictMemoryEnforcement=conf.getBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENFORCED,YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENFORCED);  LOG.info("Physical memory check enabled: {}",pmemCheckEnabled);  LOG.info("Virtual memory check enabled: {}",vmemCheckEnabled);  LOG.info("Elastic memory control enabled: {}",elasticMemoryEnforcement);
  setAllocatedResourcesForContainers(resourcesForContainers);  pmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_PMEM_CHECK_ENABLED);  vmemCheckEnabled=this.conf.getBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED,YarnConfiguration.DEFAULT_NM_VMEM_CHECK_ENABLED);  elasticMemoryEnforcement=this.conf.getBoolean(YarnConfiguration.NM_ELASTIC_MEMORY_CONTROL_ENABLED,YarnConfiguration.DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED);  strictMemoryEnforcement=conf.getBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENFORCED,YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENFORCED);  LOG.info("Physical memory check enabled: {}",pmemCheckEnabled);  LOG.info("Virtual memory check enabled: {}",vmemCheckEnabled);  LOG.info("Elastic memory control enabled: {}",elasticMemoryEnforcement);  LOG.info("Strict memory control enabled: {}",strictMemoryEnforcement);  if (elasticMemoryEnforcement) {    if (!CGroupElasticMemoryController.isAvailable()) {      throw new YarnException("CGroup Elastic Memory controller enabled but " + "it is not available. Exiting.");    } else {      this.oomListenerThread=new CGroupElasticMemoryController(conf,context,ResourceHandlerModule.getCGroupsHandler(),pmemCheckEnabled,vmemCheckEnabled,pmemCheckEnabled ? maxPmemAllottedForContainers : maxVmemAllottedForContainers);    }  }  containersMonitorEnabled=isContainerMonitorEnabled() && monitoringInterval > 0;
@Override public void setAllocatedResourcesForContainers(final Resource resource){
private void onStopMonitoringContainer(ContainersMonitorEvent monitoringEvent,ContainerId containerId){
private void onStartMonitoringContainer(ContainersMonitorEvent monitoringEvent,ContainerId containerId){  ContainerStartMonitoringEvent startEvent=(ContainerStartMonitoringEvent)monitoringEvent;
  Map<String,ResourcePlugin> pluginMap=Maps.newHashMap();  for (  String resourceName : plugins) {    resourceName=resourceName.trim();    ensurePluginIsSupported(resourceName);    if (!isPluginDuplicate(pluginMap,resourceName)) {      ResourcePlugin plugin=null;      if (resourceName.equals(GPU_URI)) {        final GpuDiscoverer gpuDiscoverer=new GpuDiscoverer();        final GpuNodeResourceUpdateHandler updateHandler=new GpuNodeResourceUpdateHandler(gpuDiscoverer,conf);        plugin=new GpuResourcePlugin(updateHandler,gpuDiscoverer);      } else       if (resourceName.equals(FPGA_URI)) {        plugin=new FpgaResourcePlugin();      }      if (plugin == null) {        throw new YarnException("This shouldn't happen, plugin=" + resourceName + " should be loaded and initialized");      }      plugin.initialize(context);
private void ensurePluginIsSupported(String resourceName) throws YarnException {  if (!SUPPORTED_RESOURCE_PLUGINS.contains(resourceName)) {    String msg="Trying to initialize resource plugin with name=" + resourceName + ", it is not supported, list of supported plugins:"+ StringUtils.join(",",SUPPORTED_RESOURCE_PLUGINS);
  for (  String pluginClassName : pluginClassNames) {    Class<?> pluginClazz=Class.forName(pluginClassName);    if (!DevicePlugin.class.isAssignableFrom(pluginClazz)) {      throw new YarnRuntimeException("Class: " + pluginClassName + " not instance of "+ DevicePlugin.class.getCanonicalName());    }    checkInterfaceCompatibility(DevicePlugin.class,pluginClazz);    DevicePlugin dpInstance=(DevicePlugin)ReflectionUtils.newInstance(pluginClazz,configuration);    DeviceRegisterRequest request=null;    try {      request=dpInstance.getRegisterRequestInfo();    } catch (    Exception e) {      throw new YarnRuntimeException("Exception thrown from plugin's" + " getRegisterRequestInfo:" + e.getMessage());    }    String resourceName=request.getResourceName();    if (pluginMap.containsKey(resourceName)) {      throw new YarnRuntimeException(resourceName + " already registered! Please change resource type name" + " or configure correct resource type name"+ " in resource-types.xml for "+ pluginClassName);    }    if (!isConfiguredResourceName(resourceName)) {
    Class<?> pluginClazz=Class.forName(pluginClassName);    if (!DevicePlugin.class.isAssignableFrom(pluginClazz)) {      throw new YarnRuntimeException("Class: " + pluginClassName + " not instance of "+ DevicePlugin.class.getCanonicalName());    }    checkInterfaceCompatibility(DevicePlugin.class,pluginClazz);    DevicePlugin dpInstance=(DevicePlugin)ReflectionUtils.newInstance(pluginClazz,configuration);    DeviceRegisterRequest request=null;    try {      request=dpInstance.getRegisterRequestInfo();    } catch (    Exception e) {      throw new YarnRuntimeException("Exception thrown from plugin's" + " getRegisterRequestInfo:" + e.getMessage());    }    String resourceName=request.getResourceName();    if (pluginMap.containsKey(resourceName)) {      throw new YarnRuntimeException(resourceName + " already registered! Please change resource type name" + " or configure correct resource type name"+ " in resource-types.xml for "+ pluginClassName);    }    if (!isConfiguredResourceName(resourceName)) {      throw new YarnRuntimeException(resourceName + " is not configured inside " + YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE+ " , please configure it first");
    DeviceRegisterRequest request=null;    try {      request=dpInstance.getRegisterRequestInfo();    } catch (    Exception e) {      throw new YarnRuntimeException("Exception thrown from plugin's" + " getRegisterRequestInfo:" + e.getMessage());    }    String resourceName=request.getResourceName();    if (pluginMap.containsKey(resourceName)) {      throw new YarnRuntimeException(resourceName + " already registered! Please change resource type name" + " or configure correct resource type name"+ " in resource-types.xml for "+ pluginClassName);    }    if (!isConfiguredResourceName(resourceName)) {      throw new YarnRuntimeException(resourceName + " is not configured inside " + YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE+ " , please configure it first");    }    LOG.info("New resource type: {} registered successfully by {}",resourceName,pluginClassName);    DevicePluginAdapter pluginAdapter=new DevicePluginAdapter(resourceName,dpInstance,deviceMappingManager);    LOG.info("Adapter of {} created. Initializing..",pluginClassName);    try {      pluginAdapter.initialize(context);
    } catch (    Exception e) {      throw new YarnRuntimeException("Exception thrown from plugin's" + " getRegisterRequestInfo:" + e.getMessage());    }    String resourceName=request.getResourceName();    if (pluginMap.containsKey(resourceName)) {      throw new YarnRuntimeException(resourceName + " already registered! Please change resource type name" + " or configure correct resource type name"+ " in resource-types.xml for "+ pluginClassName);    }    if (!isConfiguredResourceName(resourceName)) {      throw new YarnRuntimeException(resourceName + " is not configured inside " + YarnConfiguration.RESOURCE_TYPES_CONFIGURATION_FILE+ " , please configure it first");    }    LOG.info("New resource type: {} registered successfully by {}",resourceName,pluginClassName);    DevicePluginAdapter pluginAdapter=new DevicePluginAdapter(resourceName,dpInstance,deviceMappingManager);    LOG.info("Adapter of {} created. Initializing..",pluginClassName);    try {      pluginAdapter.initialize(context);    } catch (    YarnException e) {      throw new YarnRuntimeException("Adapter of " + pluginClassName + " init failed!");    }    LOG.info("Adapter of {} init success!",pluginClassName);
@VisibleForTesting public void checkInterfaceCompatibility(Class<?> expectedClass,Class<?> actualClass) throws YarnRuntimeException {
@VisibleForTesting public void checkInterfaceCompatibility(Class<?> expectedClass,Class<?> actualClass) throws YarnRuntimeException {  LOG.debug("Checking implemented interface's compatibility: {}",expectedClass.getSimpleName());  Method[] expectedDevicePluginMethods=expectedClass.getMethods();  boolean found;  for (  Method method : expectedDevicePluginMethods) {    found=false;
@VisibleForTesting public void checkInterfaceCompatibility(Class<?> expectedClass,Class<?> actualClass) throws YarnRuntimeException {  LOG.debug("Checking implemented interface's compatibility: {}",expectedClass.getSimpleName());  Method[] expectedDevicePluginMethods=expectedClass.getMethods();  boolean found;  for (  Method method : expectedDevicePluginMethods) {    found=false;    LOG.debug("Try to find method: {}",method.getName());    for (    Method m : actualClass.getDeclaredMethods()) {      if (m.getName().equals(method.getName())) {
  LOG.info("Use {} as script name.",binaryName);  boolean found=false;  String envBinaryPath=envProvider.apply(ENV_SCRIPT_PATH);  if (envBinaryPath != null) {    this.binaryPath=getScriptFromEnvSetting(envBinaryPath);    found=binaryPath != null;  }  if (!found) {    if (envBinaryPath != null) {      LOG.warn("Script {} does not exist, falling back " + "to $HADOOP_COMMON_HOME/sbin/DevicePluginScript/",envBinaryPath);    }    this.binaryPath=getScriptFromHadoopCommon(envProvider,binaryName);    found=binaryPath != null;  }  if (!found) {    LOG.info("Script not found under" + " $HADOOP_COMMON_HOME/sbin/DevicePluginScript/," + " falling back to default search directories");    this.binaryPath=getScriptFromSearchDirs(binaryName,scriptPaths);    found=binaryPath != null;
private Set<Device> parseOutput(String output){  Set<Device> devices=new HashSet<>();
private String getScriptFromEnvSetting(String envBinaryPath){
private String getScriptFromHadoopCommon(Function<String,String> envProvider,String binaryName){  String scriptPath=null;  String hadoopCommon=envProvider.apply(HADOOP_COMMON_HOME);  if (hadoopCommon != null) {    String targetPath=hadoopCommon + "/sbin/DevicePluginScript/" + binaryName;
private String getScriptFromHadoopCommon(Function<String,String> envProvider,String binaryName){  String scriptPath=null;  String hadoopCommon=envProvider.apply(HADOOP_COMMON_HOME);  if (hadoopCommon != null) {    String targetPath=hadoopCommon + "/sbin/DevicePluginScript/" + binaryName;    LOG.info("Checking script {}: ",targetPath);    if (new File(targetPath).exists()) {
private String getScriptFromSearchDirs(String binaryName,String[] scriptPaths){  String scriptPath=null;  for (  String dir : scriptPaths) {    File f=new File(dir,binaryName);    if (f.exists()) {
private Device toDevice(Path p,MutableInt counter){  CommandExecutor executor=commandExecutorProvider.apply(new String[]{"stat","-L","-c","%t:%T:%F",p.toString()});  try {
private Device toDevice(Path p,MutableInt counter){  CommandExecutor executor=commandExecutorProvider.apply(new String[]{"stat","-L","-c","%t:%T:%F",p.toString()});  try {    LOG.info("Checking device file: {}",p);    executor.execute();    String statOutput=executor.getOutput();    String[] stat=statOutput.trim().split(":");    int major=Integer.parseInt(stat[0],16);    int minor=Integer.parseInt(stat[1],16);    char devType=getDevType(p,stat[2]);    int deviceNumber=makeDev(major,minor);
private Device toDevice(Path p,MutableInt counter){  CommandExecutor executor=commandExecutorProvider.apply(new String[]{"stat","-L","-c","%t:%T:%F",p.toString()});  try {    LOG.info("Checking device file: {}",p);    executor.execute();    String statOutput=executor.getOutput();    String[] stat=statOutput.trim().split(":");    int major=Integer.parseInt(stat[0],16);    int minor=Integer.parseInt(stat[1],16);    char devType=getDevType(p,stat[2]);    int deviceNumber=makeDev(major,minor);    LOG.info("Device: major: {}, minor: {}, devNo: {}, type: {}",major,minor,deviceNumber,devType);    String sysPath=udev.getSysPath(deviceNumber,devType);
@Override public DeviceRuntimeSpec onDevicesAllocated(Set<Device> allocatedDevices,YarnRuntimeType yarnRuntime) throws Exception {
private String getMajorNumber(String devName){  String output=null;  try {
private String getMajorNumber(String devName){  String output=null;  try {    LOG.debug("Get major numbers from /dev/{}",devName);    output=shellExecutor.getMajorMinorInfo(devName);    String[] strs=output.trim().split(":");
  int num=0;  String policy=envs.get(TOPOLOGY_POLICY_ENV_KEY);  if (policy == null) {    policy=TOPOLOGY_POLICY_PACK;  }  if (cTable == null) {    LOG.error("No cost table initialized!");    return;  }  List<Map.Entry<Set<Device>,Integer>> combinationsToCost=cTable.get(count);  Iterator<Map.Entry<Set<Device>,Integer>> iterator=combinationsToCost.iterator();  if (policy.equalsIgnoreCase(TOPOLOGY_POLICY_SPREAD)) {    iterator=((LinkedList)combinationsToCost).descendingIterator();  }  while (iterator.hasNext()) {    Map.Entry<Set<Device>,Integer> element=iterator.next();    if (availableDevices.containsAll(element.getKey())) {      allocation.addAll(element.getKey());
public void searchBinary() throws Exception {  if (pathOfGpuBinary != null) {
    return;  }  String envBinaryPath=System.getenv(ENV_BINARY_PATH);  if (null != envBinaryPath) {    if (new File(envBinaryPath).exists()) {      pathOfGpuBinary=envBinaryPath;      LOG.info("Use nvidia gpu binary: " + pathOfGpuBinary);      return;    }  }  LOG.info("Search binary..");  File binaryFile;  boolean found=false;  for (  String dir : DEFAULT_BINARY_SEARCH_DIRS) {    binaryFile=new File(dir,DEFAULT_BINARY_NAME);    if (binaryFile.exists()) {      found=true;      pathOfGpuBinary=binaryFile.getAbsolutePath();
  if (null != envBinaryPath) {    if (new File(envBinaryPath).exists()) {      pathOfGpuBinary=envBinaryPath;      LOG.info("Use nvidia gpu binary: " + pathOfGpuBinary);      return;    }  }  LOG.info("Search binary..");  File binaryFile;  boolean found=false;  for (  String dir : DEFAULT_BINARY_SEARCH_DIRS) {    binaryFile=new File(dir,DEFAULT_BINARY_NAME);    if (binaryFile.exists()) {      found=true;      pathOfGpuBinary=binaryFile.getAbsolutePath();      LOG.info("Found binary:" + pathOfGpuBinary);      break;
public synchronized void addDeviceSet(String resourceName,Set<Device> deviceSet){
private synchronized DeviceAllocation internalAssignDevices(String resourceName,Container container) throws ResourceHandlerException {  Resource requestedResource=container.getResource();  ContainerId containerId=container.getContainerId();  int requestedDeviceCount=getRequestedDeviceCount(resourceName,requestedResource);
public synchronized void cleanupAssignedDevices(String resourceName,ContainerId containerId){  Iterator<Map.Entry<Device,ContainerId>> iter=allUsedDevices.get(resourceName).entrySet().iterator();  Map.Entry<Device,ContainerId> entry;  while (iter.hasNext()) {    entry=iter.next();    if (entry.getValue().equals(containerId)) {
private void defaultScheduleAction(Set<Device> allowed,Map<Device,ContainerId> used,Set<Device> assigned,ContainerId containerId,int count){
@Override public void initialize(Context context) throws YarnException {  deviceDockerCommandPlugin=new DeviceResourceDockerRuntimePluginImpl(resourceName,devicePlugin,this);  deviceResourceUpdater=new DeviceResourceUpdaterImpl(resourceName,devicePlugin);
@Override public void updateDockerRunCommand(DockerRunCommand dockerRunCommand,Container container) throws ContainerExecutionException {  String containerId=container.getContainerId().toString();
  String containerId=container.getContainerId().toString();  LOG.debug("Try to update docker run command for: {}",containerId);  if (!requestedDevice(resourceName,container)) {    return;  }  DeviceRuntimeSpec deviceRuntimeSpec=getRuntimeSpec(container);  if (deviceRuntimeSpec == null) {    LOG.warn("The device plugin: " + devicePlugin.getClass().getCanonicalName() + " returns null device runtime spec value for container: "+ containerId);    return;  }  dockerRunCommand.addRuntime(deviceRuntimeSpec.getContainerRuntime());  LOG.debug("Handle docker container runtime type: {} for container: {}",deviceRuntimeSpec.getContainerRuntime(),containerId);  Set<MountDeviceSpec> deviceMounts=deviceRuntimeSpec.getDeviceMounts();  LOG.debug("Handle device mounts: {} for container: {}",deviceMounts,containerId);  for (  MountDeviceSpec mountDeviceSpec : deviceMounts) {    dockerRunCommand.addDevice(mountDeviceSpec.getDevicePathInHost(),mountDeviceSpec.getDevicePathInContainer());  }  Set<MountVolumeSpec> mountVolumeSpecs=deviceRuntimeSpec.getVolumeMounts();
    LOG.warn("The device plugin: " + devicePlugin.getClass().getCanonicalName() + " returns null device runtime spec value for container: "+ containerId);    return;  }  dockerRunCommand.addRuntime(deviceRuntimeSpec.getContainerRuntime());  LOG.debug("Handle docker container runtime type: {} for container: {}",deviceRuntimeSpec.getContainerRuntime(),containerId);  Set<MountDeviceSpec> deviceMounts=deviceRuntimeSpec.getDeviceMounts();  LOG.debug("Handle device mounts: {} for container: {}",deviceMounts,containerId);  for (  MountDeviceSpec mountDeviceSpec : deviceMounts) {    dockerRunCommand.addDevice(mountDeviceSpec.getDevicePathInHost(),mountDeviceSpec.getDevicePathInContainer());  }  Set<MountVolumeSpec> mountVolumeSpecs=deviceRuntimeSpec.getVolumeMounts();  LOG.debug("Handle volume mounts: {} for container: {}",mountVolumeSpecs,containerId);  for (  MountVolumeSpec mountVolumeSpec : mountVolumeSpecs) {    if (mountVolumeSpec.getReadOnly()) {      dockerRunCommand.addReadOnlyMountLocation(mountVolumeSpec.getHostPath(),mountVolumeSpec.getMountPath());    } else {      dockerRunCommand.addReadWriteMountLocation(mountVolumeSpec.getHostPath(),mountVolumeSpec.getMountPath());
public synchronized DeviceRuntimeSpec getRuntimeSpec(Container container){  ContainerId containerId=container.getContainerId();  DeviceRuntimeSpec deviceRuntimeSpec=cachedSpec.get(containerId);  if (deviceRuntimeSpec == null) {    Set<Device> allocated=getAllocatedDevices(container);    if (allocated == null || allocated.size() == 0) {
@Override public synchronized List<PrivilegedOperation> preStart(Container container) throws ResourceHandlerException {  String containerIdStr=container.getContainerId().toString();  DeviceMappingManager.DeviceAllocation allocation=deviceMappingManager.assignDevices(resourceName,container);
    LOG.error("Undefined script");    return Optional.empty();  }  File f=new File(path);  if (!f.exists()) {    LOG.error("Script does not exist");    return Optional.empty();  }  if (!FileUtil.canExecute(f)) {    LOG.error("Script is not executable");    return Optional.empty();  }  ShellCommandExecutor shell=new ShellCommandExecutor(new String[]{path},null,null,MAX_EXEC_TIMEOUT_MS);  try {    shell.execute();    String output=shell.getOutput();    return Optional.of(output);  } catch (  IOException e) {
private AbstractFpgaVendorPlugin createFpgaVendorPlugin(Configuration conf){  String vendorPluginClass=conf.get(YarnConfiguration.NM_FPGA_VENDOR_PLUGIN,YarnConfiguration.DEFAULT_NM_FPGA_VENDOR_PLUGIN);
  if (initialized) {    return true;  }  String pluginDefaultBinaryName=DEFAULT_BINARY_NAME;  String executable=config.get(YarnConfiguration.NM_FPGA_PATH_TO_EXEC,pluginDefaultBinaryName);  File binaryPath=new File(executable);  if (!binaryPath.exists()) {    LOG.warn("Failed to find FPGA discoverer executable configured in " + YarnConfiguration.NM_FPGA_PATH_TO_EXEC + ", please check! Try default path");    executable=pluginDefaultBinaryName;    String pluginDefaultPreferredPath=getDefaultPathToExecutable();    if (null == pluginDefaultPreferredPath) {      LOG.warn("Failed to find FPGA discoverer executable from system " + " environment " + ALTERAOCLSDKROOT_NAME + ", please check your environment!");    } else {      binaryPath=new File(pluginDefaultPreferredPath + "/bin",pluginDefaultBinaryName);      if (binaryPath.exists()) {        executable=binaryPath.getAbsolutePath();
@Override public String retrieveIPfilePath(String id,String dstDir,Map<Path,List<String>> localizedResources){  String ipFilePath=null;
@Override public boolean configureIP(String ipPath,FpgaDevice device){  Shell.ShellCommandExecutor shexec;  String aclName;  aclName=device.getAliasDevName();  shexec=new Shell.ShellCommandExecutor(new String[]{this.pathToExecutable,"program",aclName,ipPath});  try {    shexec.execute();    if (0 == shexec.getExitCode()) {
@Override public boolean configureIP(String ipPath,FpgaDevice device){  Shell.ShellCommandExecutor shexec;  String aclName;  aclName=device.getAliasDevName();  shexec=new Shell.ShellCommandExecutor(new String[]{this.pathToExecutable,"program",aclName,ipPath});  try {    shexec.execute();    if (0 == shexec.getExitCode()) {      LOG.debug("{}",shexec.getOutput());
@Override public boolean configureIP(String ipPath,FpgaDevice device){  Shell.ShellCommandExecutor shexec;  String aclName;  aclName=device.getAliasDevName();  shexec=new Shell.ShellCommandExecutor(new String[]{this.pathToExecutable,"program",aclName,ipPath});  try {    shexec.execute();    if (0 == shexec.getExitCode()) {      LOG.debug("{}",shexec.getOutput());      LOG.info("Intel aocl program " + ipPath + " to "+ aclName+ " successfully");    } else {      LOG.error("Device programming failed, aocl output is:");      LOG.error(shexec.getOutput());      return false;    }  } catch (  IOException e) {
  Shell.ShellCommandExecutor shexec;  String aclName;  aclName=device.getAliasDevName();  shexec=new Shell.ShellCommandExecutor(new String[]{this.pathToExecutable,"program",aclName,ipPath});  try {    shexec.execute();    if (0 == shexec.getExitCode()) {      LOG.debug("{}",shexec.getOutput());      LOG.info("Intel aocl program " + ipPath + " to "+ aclName+ " successfully");    } else {      LOG.error("Device programming failed, aocl output is:");      LOG.error(shexec.getOutput());      return false;    }  } catch (  IOException e) {    LOG.error("Intel aocl program " + ipPath + " to "+ aclName+ " failed!",e);
public synchronized GpuDeviceInformation getGpuDeviceInformation() throws YarnException {  if (numOfErrorExecutionSinceLastSucceed == MAX_REPEATED_ERROR_ALLOWED) {    String msg=getErrorMessageOfScriptExecutionThresholdReached();
public synchronized GpuDeviceInformation getGpuDeviceInformation() throws YarnException {  if (numOfErrorExecutionSinceLastSucceed == MAX_REPEATED_ERROR_ALLOWED) {    String msg=getErrorMessageOfScriptExecutionThresholdReached();    LOG.error(msg);    throw new YarnException(msg);  }  try {    lastDiscoveredGpuInformation=nvidiaBinaryHelper.getGpuDeviceInformation(pathOfGpuBinary);  } catch (  IOException e) {    numOfErrorExecutionSinceLastSucceed++;    String msg=getErrorMessageOfScriptExecution(e.getMessage());    LOG.debug(msg);    throw new YarnException(msg,e);  }catch (  YarnException e) {    numOfErrorExecutionSinceLastSucceed++;    String msg=getFailedToParseErrorMessage(e.getMessage());
private List<GpuDevice> parseGpuDevicesFromAutoDiscoveredGpuInfo() throws YarnException {  if (lastDiscoveredGpuInformation == null) {    String msg=YarnConfiguration.NM_GPU_ALLOWED_DEVICES + " is set to " + YarnConfiguration.AUTOMATICALLY_DISCOVER_GPU_DEVICES+ ", however automatically discovering "+ "GPU information failed, please check NodeManager log for more"+ " details, as an alternative, admin can specify "+ YarnConfiguration.NM_GPU_ALLOWED_DEVICES+ " manually to enable GPU isolation.";
      if (splitByColon.length != 2) {        throwIfNecessary(GpuDeviceSpecificationException.createWithWrongValueSpecified(device,devices),getConf());        LOG.warn("Wrong GPU specification string {}, ignored",device);      }      GpuDevice gpuDevice;      try {        gpuDevice=parseGpuDevice(splitByColon);      } catch (      NumberFormatException e) {        throwIfNecessary(GpuDeviceSpecificationException.createWithWrongValueSpecified(device,devices,e),getConf());        LOG.warn("Cannot parse GPU device numbers: {}",device);        continue;      }      if (!gpuDevices.contains(gpuDevice)) {        gpuDevices.add(gpuDevice);      } else {        throwIfNecessary(GpuDeviceSpecificationException.createWithDuplicateValueSpecified(device,devices),getConf());        LOG.warn("CPU device is duplicated: {}",device);
public synchronized void initialize(Configuration config,NvidiaBinaryHelper nvidiaHelper) throws YarnException {  setConf(config);  this.nvidiaBinaryHelper=nvidiaHelper;  if (isAutoDiscoveryEnabled()) {    numOfErrorExecutionSinceLastSucceed=0;    lookUpAutoDiscoveryBinary(config);    try {      LOG.info("Trying to discover GPU information ...");      GpuDeviceInformation info=getGpuDeviceInformation();
@Override public void updateConfiguredResource(Resource res) throws YarnException {  LOG.info("Initializing configured GPU resources for the NodeManager.");  List<GpuDevice> usableGpus=gpuDiscoverer.getGpusUsableByYarn();  if (usableGpus == null || usableGpus.isEmpty()) {    String message="GPU is enabled, " + "but could not find any usable GPUs on the NodeManager!";
private void checkErrorCount() throws YarnException {  if (numOfErrorExecutionSinceLastSucceed == MAX_REPEATED_ERROR_ALLOWED) {    String msg="Failed to execute GPU device information detection script for " + MAX_REPEATED_ERROR_ALLOWED + " times, skip following executions.";
private void init() throws ContainerExecutionException {  String endpoint=conf.get(YarnConfiguration.NVIDIA_DOCKER_PLUGIN_V1_ENDPOINT,YarnConfiguration.DEFAULT_NVIDIA_DOCKER_PLUGIN_V1_ENDPOINT);  if (null == endpoint || endpoint.isEmpty()) {
  }  String cliOptions;  try {    URL url=new URL(endpoint);    URLConnection uc=url.openConnection();    uc.setRequestProperty("X-Requested-With","Curl");    StringWriter writer=new StringWriter();    IOUtils.copy(uc.getInputStream(),writer,"utf-8");    cliOptions=writer.toString();    LOG.info("Additional docker CLI options from plugin to run GPU " + "containers:" + cliOptions);    for (    String str : cliOptions.split(" ")) {      str=str.trim();      if (str.startsWith(DEVICE_OPTION)) {        addToCommand(DEVICE_OPTION,getValue(str));      } else       if (str.startsWith(VOLUME_DRIVER_OPTION)) {        volumeDriver=getValue(str);
@SuppressWarnings("unchecked") private void reclaimOpportunisticContainerResources(Container container){  List<Container> extraOppContainersToReclaim=pickOpportunisticContainersToReclaimResources(container.getContainerId());  for (  Container contToReclaim : extraOppContainersToReclaim) {    String preemptionAction=usePauseEventForPreemption == true ? "paused" : "killed";
private void startContainer(Container container){
private void shedQueuedOpportunisticContainers(){  int numAllowed=this.queuingLimit.getMaxQueueLength();  Iterator<Container> containerIter=queuedOpportunisticContainers.values().iterator();  while (containerIter.hasNext()) {    Container container=containerIter.next();    if (container.getContainerState() != ContainerState.PAUSED) {      if (numAllowed <= 0) {        container.sendKillEvent(ContainerExitStatus.KILLED_BY_CONTAINER_SCHEDULER,"Container De-queued to meet NM queuing limits.");        containerIter.remove();
public Map<String,String> publishVolumes() throws YarnException, IOException {  LOG.info("publishing volumes");  Map<String,String> volumeMounts=new HashMap<>();  List<VolumeMetaData> volumes=getVolumes();
public void unpublishVolumes() throws YarnException, IOException {  LOG.info("Un-publishing Volumes");  List<VolumeMetaData> volumes=getVolumes();
private Map<String,String> publishVolume(VolumeMetaData volume) throws IOException, YarnException {  Map<String,String> bindVolumes=new HashMap<>();  File localMount=getLocalVolumeMountPath(localMountRoot,volume.getVolumeId().toString());  File localStaging=getLocalVolumeStagingPath(localMountRoot,volume.getVolumeId().toString());
static boolean shouldRun(String script,String healthScript){  if (healthScript == null || healthScript.trim().isEmpty()) {
private void markStoreUnHealthy(DBException dbErr){
@Override public void storeContainer(ContainerId containerId,int containerVersion,long startTime,StartContainerRequest startRequest) throws IOException {  String idStr=containerId.toString();
@Override public void storeContainerQueued(ContainerId containerId) throws IOException {
private void removeContainerQueued(ContainerId containerId) throws IOException {
@Override public void storeContainerPaused(ContainerId containerId) throws IOException {
@Override public void removeContainerPaused(ContainerId containerId) throws IOException {
@Override public void storeContainerDiagnostics(ContainerId containerId,StringBuilder diagnostics) throws IOException {
@Override public void storeContainerLaunched(ContainerId containerId) throws IOException {
@Override public void storeContainerUpdateToken(ContainerId containerId,ContainerTokenIdentifier containerTokenIdentifier) throws IOException {
@Override public void storeContainerKilled(ContainerId containerId) throws IOException {
@Override public void storeContainerCompleted(ContainerId containerId,int exitCode) throws IOException {
@Override public void removeContainer(ContainerId containerId) throws IOException {
@Override public void storeApplication(ApplicationId appId,ContainerManagerApplicationProto p) throws IOException {
@Override public void removeApplication(ApplicationId appId) throws IOException {
@Override public void finishResourceLocalization(String user,ApplicationId appId,LocalizedResourceProto proto) throws IOException {  String localPath=proto.getLocalPath();  String startedKey=getResourceStartedKey(user,appId,localPath);  String completedKey=getResourceCompletedKey(user,appId,localPath);
@Override public void removeLocalizedResource(String user,ApplicationId appId,Path localPath) throws IOException {  String localPathStr=localPath.toString();  String startedKey=getResourceStartedKey(user,appId,localPathStr);  String completedKey=getResourceCompletedKey(user,appId,localPathStr);
@Override public void storeAssignedResources(Container container,String resourceType,List<Serializable> assignedResources) throws IOException {  if (LOG.isDebugEnabled()) {
        iter.next();        result.setNextMasterKey(parseMasterKey(entry.getValue()));        LOG.info("Recovered for AMRMProxy: next master key id " + result.getNextMasterKey().getKeyId());      } else {        int idEndPos;        ApplicationAttemptId attemptId;        try {          idEndPos=key.indexOf('/',AMRMPROXY_KEY_PREFIX.length());          if (idEndPos < 0) {            throw new IOException("Unable to determine attemptId in key: " + key);          }          attemptId=ApplicationAttemptId.fromString(key.substring(AMRMPROXY_KEY_PREFIX.length(),idEndPos));        } catch (        Exception e) {          LOG.warn("Unknown key " + key + ", remove and move on",e);          unknownKeys.add(key);          continue;
protected DB openDatabase(Configuration conf) throws IOException {  Path storeRoot=createStorageDir(conf);  Options options=new Options();  options.createIfMissing(false);
protected void checkVersion() throws IOException {  Version loadedVersion=loadVersion();
@Private public synchronized void setMasterKey(MasterKey masterKeyRecord){  if (super.currentMasterKey == null || super.currentMasterKey.getMasterKey().getKeyId() != masterKeyRecord.getKeyId()) {
public synchronized void setNodeId(NodeId nodeId){  nodeHostAddr=nodeId.toString();
@Private public synchronized void setMasterKey(MasterKey masterKey){  if (super.currentMasterKey == null || super.currentMasterKey.getMasterKey().getKeyId() != masterKey.getKeyId()) {
public synchronized void appFinished(ApplicationId appId){  List<ApplicationAttemptId> appAttemptList=appToAppAttemptMap.get(appId);  if (appAttemptList != null) {
public synchronized void setNodeId(NodeId nodeId){
@Override protected void serviceInit(Configuration conf) throws Exception {  dispatcher=createDispatcher();  dispatcher.register(NMTimelineEventType.class,new ForwardingEventHandler());  addIfService(dispatcher);  this.nmLoginUGI=UserGroupInformation.isSecurityEnabled() ? UserGroupInformation.getLoginUser() : UserGroupInformation.getCurrentUser();
        memoryMetric.setRealtimeAggregationOp(TimelineMetricOperation.SUM);        memoryMetric.addValue(currentTimeMillis,pmemUsage);        entity.addMetric(memoryMetric);      }      if (cpuUsagePercentPerCore != ResourceCalculatorProcessTree.UNAVAILABLE) {        TimelineMetric cpuMetric=new TimelineMetric();        cpuMetric.setId(ContainerMetric.CPU.toString());        cpuMetric.setRealtimeAggregationOp(TimelineMetricOperation.SUM);        cpuMetric.addValue(currentTimeMillis,Math.round(cpuUsagePercentPerCore));        entity.addMetric(cpuMetric);      }      ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();      try {        TimelineV2Client timelineClient=getTimelineClient(appId);        if (timelineClient != null) {          timelineClient.putEntitiesAsync(entity);        } else {
        entity.addMetric(memoryMetric);      }      if (cpuUsagePercentPerCore != ResourceCalculatorProcessTree.UNAVAILABLE) {        TimelineMetric cpuMetric=new TimelineMetric();        cpuMetric.setId(ContainerMetric.CPU.toString());        cpuMetric.setRealtimeAggregationOp(TimelineMetricOperation.SUM);        cpuMetric.addValue(currentTimeMillis,Math.round(cpuUsagePercentPerCore));        entity.addMetric(cpuMetric);      }      ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();      try {        TimelineV2Client timelineClient=getTimelineClient(appId);        if (timelineClient != null) {          timelineClient.putEntitiesAsync(entity);        } else {          LOG.error("Seems like client has been removed before the container" + " metric could be published for " + container.getContainerId());        }      } catch (      IOException e) {
      }      if (cpuUsagePercentPerCore != ResourceCalculatorProcessTree.UNAVAILABLE) {        TimelineMetric cpuMetric=new TimelineMetric();        cpuMetric.setId(ContainerMetric.CPU.toString());        cpuMetric.setRealtimeAggregationOp(TimelineMetricOperation.SUM);        cpuMetric.addValue(currentTimeMillis,Math.round(cpuUsagePercentPerCore));        entity.addMetric(cpuMetric);      }      ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();      try {        TimelineV2Client timelineClient=getTimelineClient(appId);        if (timelineClient != null) {          timelineClient.putEntitiesAsync(entity);        } else {          LOG.error("Seems like client has been removed before the container" + " metric could be published for " + container.getContainerId());        }      } catch (      IOException e) {        LOG.error("Failed to publish Container metrics for container " + container.getContainerId());
        TimelineMetric cpuMetric=new TimelineMetric();        cpuMetric.setId(ContainerMetric.CPU.toString());        cpuMetric.setRealtimeAggregationOp(TimelineMetricOperation.SUM);        cpuMetric.addValue(currentTimeMillis,Math.round(cpuUsagePercentPerCore));        entity.addMetric(cpuMetric);      }      ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();      try {        TimelineV2Client timelineClient=getTimelineClient(appId);        if (timelineClient != null) {          timelineClient.putEntitiesAsync(entity);        } else {          LOG.error("Seems like client has been removed before the container" + " metric could be published for " + container.getContainerId());        }      } catch (      IOException e) {        LOG.error("Failed to publish Container metrics for container " + container.getContainerId());        LOG.debug("Failed to publish Container metrics for container {}",container.getContainerId(),e);
        cpuMetric.setId(ContainerMetric.CPU.toString());        cpuMetric.setRealtimeAggregationOp(TimelineMetricOperation.SUM);        cpuMetric.addValue(currentTimeMillis,Math.round(cpuUsagePercentPerCore));        entity.addMetric(cpuMetric);      }      ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();      try {        TimelineV2Client timelineClient=getTimelineClient(appId);        if (timelineClient != null) {          timelineClient.putEntitiesAsync(entity);        } else {          LOG.error("Seems like client has been removed before the container" + " metric could be published for " + container.getContainerId());        }      } catch (      IOException e) {        LOG.error("Failed to publish Container metrics for container " + container.getContainerId());        LOG.debug("Failed to publish Container metrics for container {}",container.getContainerId(),e);      }catch (      YarnException e) {
private void publishContainerLocalizationEvent(ContainerLocalizationEvent event,String eventType){  if (publishNMContainerEvents) {    Container container=event.getContainer();    ContainerId containerId=container.getContainerId();    TimelineEntity entity=createContainerEntity(containerId);    TimelineEvent tEvent=new TimelineEvent();    tEvent.setId(eventType);    tEvent.setTimestamp(event.getTimestamp());    entity.addEvent(tEvent);    ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();    try {      TimelineV2Client timelineClient=getTimelineClient(appId);      if (timelineClient != null) {        timelineClient.putEntitiesAsync(entity);      } else {
    Container container=event.getContainer();    ContainerId containerId=container.getContainerId();    TimelineEntity entity=createContainerEntity(containerId);    TimelineEvent tEvent=new TimelineEvent();    tEvent.setId(eventType);    tEvent.setTimestamp(event.getTimestamp());    entity.addEvent(tEvent);    ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();    try {      TimelineV2Client timelineClient=getTimelineClient(appId);      if (timelineClient != null) {        timelineClient.putEntitiesAsync(entity);      } else {        LOG.error("Seems like client has been removed before the event" + " could be published for " + container.getContainerId());      }    } catch (    IOException e) {
    ContainerId containerId=container.getContainerId();    TimelineEntity entity=createContainerEntity(containerId);    TimelineEvent tEvent=new TimelineEvent();    tEvent.setId(eventType);    tEvent.setTimestamp(event.getTimestamp());    entity.addEvent(tEvent);    ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();    try {      TimelineV2Client timelineClient=getTimelineClient(appId);      if (timelineClient != null) {        timelineClient.putEntitiesAsync(entity);      } else {        LOG.error("Seems like client has been removed before the event" + " could be published for " + container.getContainerId());      }    } catch (    IOException e) {      LOG.error("Failed to publish Container metrics for container " + container.getContainerId());
    TimelineEvent tEvent=new TimelineEvent();    tEvent.setId(eventType);    tEvent.setTimestamp(event.getTimestamp());    entity.addEvent(tEvent);    ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();    try {      TimelineV2Client timelineClient=getTimelineClient(appId);      if (timelineClient != null) {        timelineClient.putEntitiesAsync(entity);      } else {        LOG.error("Seems like client has been removed before the event" + " could be published for " + container.getContainerId());      }    } catch (    IOException e) {      LOG.error("Failed to publish Container metrics for container " + container.getContainerId());      LOG.debug("Failed to publish Container metrics for container {}",container.getContainerId(),e);    }catch (    YarnException e) {
    tEvent.setId(eventType);    tEvent.setTimestamp(event.getTimestamp());    entity.addEvent(tEvent);    ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();    try {      TimelineV2Client timelineClient=getTimelineClient(appId);      if (timelineClient != null) {        timelineClient.putEntitiesAsync(entity);      } else {        LOG.error("Seems like client has been removed before the event" + " could be published for " + container.getContainerId());      }    } catch (    IOException e) {      LOG.error("Failed to publish Container metrics for container " + container.getContainerId());      LOG.debug("Failed to publish Container metrics for container {}",container.getContainerId(),e);    }catch (    YarnException e) {      LOG.error("Failed to publish Container metrics for container " + container.getContainerId(),e.getMessage());
private void putEntity(TimelineEntity entity,ApplicationId appId){  try {    if (LOG.isDebugEnabled()) {
private void createCgroup(String controller,String groupName) throws IOException {  String path=pathForCgroup(controller,groupName);
private void updateCgroup(String controller,String groupName,String param,String value) throws IOException {  String path=pathForCgroup(controller,groupName);  param=controller + "." + param;
private void logLineFromTasksFile(File cgf){  String str;  if (LOG.isDebugEnabled()) {    try (BufferedReader inl=new BufferedReader(new InputStreamReader(new FileInputStream(cgf + "/tasks"),"UTF-8"))){      str=inl.readLine();      if (str != null) {
@VisibleForTesting boolean deleteCgroup(String cgroupPath){  boolean deleted=false;
public static Resource getNodeResources(Configuration configuration){  Configuration conf=new Configuration(configuration);  String memory=ResourceInformation.MEMORY_MB.getName();  String vcores=ResourceInformation.VCORES.getName();  Resource ret=Resource.newInstance(0,0);  Map<String,ResourceInformation> resourceInformation=ResourceUtils.getNodeResourceInformation(conf);  for (  Map.Entry<String,ResourceInformation> entry : resourceInformation.entrySet()) {    ret.setResourceInformation(entry.getKey(),entry.getValue());
  Configuration conf=new Configuration(configuration);  String memory=ResourceInformation.MEMORY_MB.getName();  String vcores=ResourceInformation.VCORES.getName();  Resource ret=Resource.newInstance(0,0);  Map<String,ResourceInformation> resourceInformation=ResourceUtils.getNodeResourceInformation(conf);  for (  Map.Entry<String,ResourceInformation> entry : resourceInformation.entrySet()) {    ret.setResourceInformation(entry.getKey(),entry.getValue());    LOG.debug("Setting key {} to {}",entry.getKey(),entry.getValue());  }  if (resourceInformation.containsKey(memory)) {    Long value=resourceInformation.get(memory).getValue();    if (value > Integer.MAX_VALUE) {      throw new YarnRuntimeException("Value '" + value + "' for resource memory is more than the maximum for an integer.");    }    ResourceInformation memResInfo=resourceInformation.get(memory);    if (memResInfo.getValue() == 0) {      ret.setMemorySize(getContainerMemoryMB(conf));
  }  if (resourceInformation.containsKey(memory)) {    Long value=resourceInformation.get(memory).getValue();    if (value > Integer.MAX_VALUE) {      throw new YarnRuntimeException("Value '" + value + "' for resource memory is more than the maximum for an integer.");    }    ResourceInformation memResInfo=resourceInformation.get(memory);    if (memResInfo.getValue() == 0) {      ret.setMemorySize(getContainerMemoryMB(conf));      LOG.debug("Set memory to {}",ret.getMemorySize());    }  }  if (resourceInformation.containsKey(vcores)) {    Long value=resourceInformation.get(vcores).getValue();    if (value > Integer.MAX_VALUE) {      throw new YarnRuntimeException("Value '" + value + "' for resource vcores is more than the maximum for an integer.");    }    ResourceInformation vcoresResInfo=resourceInformation.get(vcores);    if (vcoresResInfo.getValue() == 0) {      ret.setVirtualCores(getVCores(conf));
    Long value=resourceInformation.get(memory).getValue();    if (value > Integer.MAX_VALUE) {      throw new YarnRuntimeException("Value '" + value + "' for resource memory is more than the maximum for an integer.");    }    ResourceInformation memResInfo=resourceInformation.get(memory);    if (memResInfo.getValue() == 0) {      ret.setMemorySize(getContainerMemoryMB(conf));      LOG.debug("Set memory to {}",ret.getMemorySize());    }  }  if (resourceInformation.containsKey(vcores)) {    Long value=resourceInformation.get(vcores).getValue();    if (value > Integer.MAX_VALUE) {      throw new YarnRuntimeException("Value '" + value + "' for resource vcores is more than the maximum for an integer.");    }    ResourceInformation vcoresResInfo=resourceInformation.get(vcores);    if (vcoresResInfo.getValue() == 0) {      ret.setVirtualCores(getVCores(conf));      LOG.debug("Set vcores to {}",ret.getVirtualCores());
    String command="bash";    String[] containerPath=containerURI.getPath().split("/");    String cId=containerPath[2];    if (containerPath.length == 4) {      for (      ShellContainerCommand c : ShellContainerCommand.values()) {        if (c.name().equalsIgnoreCase(containerPath[3])) {          command=containerPath[3].toLowerCase();        }      }    }    Container container=nmContext.getContainers().get(ContainerId.fromString(cId));    if (!checkAuthorization(session,container)) {      session.close(1008,"Forbidden");      return;    }    if (checkInsecureSetup()) {      session.close(1003,"Nonsecure mode is unsupported.");      return;    }    LOG.info(session.getRemoteAddress().getHostString() + " connected!");
      for (      ShellContainerCommand c : ShellContainerCommand.values()) {        if (c.name().equalsIgnoreCase(containerPath[3])) {          command=containerPath[3].toLowerCase();        }      }    }    Container container=nmContext.getContainers().get(ContainerId.fromString(cId));    if (!checkAuthorization(session,container)) {      session.close(1008,"Forbidden");      return;    }    if (checkInsecureSetup()) {      session.close(1003,"Nonsecure mode is unsupported.");      return;    }    LOG.info(session.getRemoteAddress().getHostString() + " connected!");    LOG.info("Making interactive connection to running docker container with ID: " + cId);    ContainerExecContext execContext=new ContainerExecContext.Builder().setContainer(container).setNMLocalPath(nmContext.getLocalDirsHandler()).setShell(command).build();    pair=exec.execContainer(execContext);  } catch (  Exception e) {
          StringBuilder sb=new StringBuilder();          String endOfFile="End of LogType:" + outputFileName;          sb.append(endOfFile + ".");          if (isRunning) {            sb.append("This log file belongs to a running container (" + containerIdStr + ") and so may not be complete."+ "\n");          } else {            sb.append("\n");          }          sb.append(StringUtils.repeat("*",endOfFile.length() + 50) + "\n\n");          os.write(sb.toString().getBytes(Charset.forName("UTF-8")));          ApplicationId appId=containerId.getApplicationAttemptId().getApplicationId();          Application app=nmContext.getApplications().get(appId);          String appOwner=app == null ? null : app.getUser();          try {            ContainerLogsRequest logRequest=new ContainerLogsRequest();            logRequest.setAppId(appId);
  if (initializersClasses != null) {    for (    Class<?> initializer : initializersClasses) {      if (initializer.getName().equals(AuthenticationFilterInitializer.class.getName())) {        hasHadoopAuthFilterInitializer=true;        break;      }      targets.add(initializer.getName());    }  }  if (!hasHadoopAuthFilterInitializer) {    targets.add(AuthenticationFilterInitializer.class.getName());    conf.set(filterInitializerConfKey,StringUtils.join(",",targets));  }  ContainerShellWebSocket.init(nmContext);  LOG.info("Instantiating NMWebApp at " + bindAddress);  try {    this.webApp=WebApps.$for("node",Context.class,this.nmContext,"ws").at(bindAddress).withServlet("ContainerShellWebSocket","/container/*",ContainerShellWebSocketServlet.class,params,false).withServlet("Terminal","/terminal/*",TerminalServlet.class,terminalParams,false).with(conf).withHttpSpnegoPrincipalKey(YarnConfiguration.NM_WEBAPP_SPNEGO_USER_NAME_KEY).withHttpSpnegoKeytabKey(YarnConfiguration.NM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY).withCSRFProtection(YarnConfiguration.NM_CSRF_PREFIX).withXFSProtection(YarnConfiguration.NM_XFS_PREFIX).start(this.nmWebApp);    this.port=this.webApp.httpServer().getConnectorAddress(0).getPort();  } catch (  Exception e) {
@Override @SuppressWarnings("unchecked") protected ResourceLocalizationService createResourceLocalizationService(ContainerExecutor exec,DeletionService deletionContext,Context context,NodeManagerMetrics metrics){  return new ResourceLocalizationService(super.dispatcher,exec,deletionContext,super.dirsHandler,context,metrics){    @Override public void handle(    LocalizationEvent event){switch (event.getType()) {case INIT_APPLICATION_RESOURCES:        Application app=((ApplicationLocalizationEvent)event).getApplication();      dispatcher.getEventHandler().handle(new ApplicationInitedEvent(app.getAppId()));    break;case LOCALIZE_CONTAINER_RESOURCES:  ContainerLocalizationRequestEvent rsrcReqs=(ContainerLocalizationRequestEvent)event;for (Collection<LocalResourceRequest> rc : rsrcReqs.getRequestedResources().values()) {  for (  LocalResourceRequest req : rc) {
  Path workSpacePath=new Path(workSpace.getAbsolutePath());  files.mkdir(workSpacePath,null,true);  FileUtil.chmod(workSpace.getAbsolutePath(),"777");  File localDir=new File(workSpace.getAbsoluteFile(),"localDir");  files.mkdir(new Path(localDir.getAbsolutePath()),new FsPermission("777"),false);  File logDir=new File(workSpace.getAbsoluteFile(),"logDir");  files.mkdir(new Path(logDir.getAbsolutePath()),new FsPermission("777"),false);  String exec_path=System.getProperty("container-executor.path");  if (exec_path != null && !exec_path.isEmpty()) {    conf=new Configuration(false);    conf.setClass("fs.AbstractFileSystem.file.impl",org.apache.hadoop.fs.local.LocalFs.class,org.apache.hadoop.fs.AbstractFileSystem.class);    appSubmitter=System.getProperty("application.submitter");    if (appSubmitter == null || appSubmitter.isEmpty()) {      appSubmitter="nobody";    }    conf.set(YarnConfiguration.NM_NONSECURE_MODE_LOCAL_USER_KEY,appSubmitter);
    assertThat(result.get(9)).isEqualTo("-classpath");    assertThat(result.get(12)).isEqualTo("-Xmx256m");    assertThat(result.get(13)).isEqualTo("-Dlog4j.configuration=container-log4j.properties");    assertThat(result.get(14)).isEqualTo(String.format("-Dyarn.app.container.log.dir=%s/application_0/12345",mockExec.getConf().get(YarnConfiguration.NM_LOG_DIRS)));    assertThat(result.get(15)).isEqualTo("-Dyarn.app.container.log.filesize=0");    assertThat(result.get(16)).isEqualTo("-Dhadoop.root.logger=INFO,CLA");    assertThat(result.get(17)).isEqualTo("-Dhadoop.root.logfile=container-localizer-syslog");    assertThat(result.get(18)).isEqualTo("org.apache.hadoop.yarn.server." + "nodemanager.containermanager.localizer.ContainerLocalizer");    assertThat(result.get(19)).isEqualTo("test");    assertThat(result.get(20)).isEqualTo("application_0");    assertThat(result.get(21)).isEqualTo("12345");    assertThat(result.get(22)).isEqualTo("localhost");    assertThat(result.get(23)).isEqualTo("8040");    assertThat(result.get(24)).isEqualTo("nmPrivateCTokensPath");  } catch (  InterruptedException e) {
  nm.init(conf);  Object[] services=nm.getServices().toArray();  Object lastService=services[services.length - 1];  Assert.assertTrue("last service is NOT the node status updater",lastService instanceof NodeStatusUpdater);  new Thread(){    public void run(){      try {        nm.start();      } catch (      Throwable e) {        TestNodeStatusUpdater.this.nmStartError=e;        throw new YarnRuntimeException(e);      }    }  }.start();  System.out.println(" ----- thread already started.." + nm.getServiceState());  int waitCount=0;  while (nm.getServiceState() == STATE.INITED && waitCount++ != 50) {
@Test public void testSignalContainerToContainerManager() throws Exception {  nm=new NodeManager(){    @Override protected NodeStatusUpdater createNodeStatusUpdater(    Context context,    Dispatcher dispatcher,    NodeHealthCheckerService healthChecker){      return new MyNodeStatusUpdater(context,dispatcher,healthChecker,metrics,true);    }    @Override protected ContainerManagerImpl createContainerManager(    Context context,    ContainerExecutor exec,    DeletionService del,    NodeStatusUpdater nodeStatusUpdater,    ApplicationACLsManager aclsManager,    LocalDirsHandlerService diskhandler){      return new MyContainerManager(context,exec,del,nodeStatusUpdater,metrics,diskhandler);    }  };  YarnConfiguration conf=createNMConfig();  nm.init(conf);  nm.start();  System.out.println(" ----- thread already started.." + nm.getServiceState());  int waitCount=0;  while (nm.getServiceState() == STATE.INITED && waitCount++ != 20) {    LOG.info("Waiting for NM to start..");    if (nmStartError != null) {
  LOG.info("Start the Resource Tracker to mock heartbeats");  Server resourceTracker=getMockResourceTracker(resource);  resourceTracker.start();  LOG.info("Start the Node Manager");  NodeManager nodeManager=new NodeManager();  YarnConfiguration nmConf=new YarnConfiguration();  nmConf.setSocketAddr(YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS,resourceTracker.getListenerAddress());  nmConf.set(YarnConfiguration.NM_LOCALIZER_ADDRESS,"0.0.0.0:0");  nodeManager.init(nmConf);  nodeManager.start();  LOG.info("Initially the Node Manager should have the default resources");  ContainerManager containerManager=nodeManager.getContainerManager();  ContainersMonitor containerMonitor=containerManager.getContainersMonitor();  assertEquals(8,containerMonitor.getVCoresAllocatedForContainers());  assertEquals(8 * GB,containerMonitor.getPmemAllocatedForContainers());
  YarnConfiguration nmConf=new YarnConfiguration();  nmConf.setSocketAddr(YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS,resourceTracker.getListenerAddress());  nmConf.set(YarnConfiguration.NM_LOCALIZER_ADDRESS,"0.0.0.0:0");  nodeManager.init(nmConf);  nodeManager.start();  LOG.info("Initially the Node Manager should have the default resources");  ContainerManager containerManager=nodeManager.getContainerManager();  ContainersMonitor containerMonitor=containerManager.getContainersMonitor();  assertEquals(8,containerMonitor.getVCoresAllocatedForContainers());  assertEquals(8 * GB,containerMonitor.getPmemAllocatedForContainers());  LOG.info("The first heartbeat should trigger a resource change to {}",resource);  GenericTestUtils.waitFor(() -> containerMonitor.getVCoresAllocatedForContainers() == 1,100,2 * 1000);  assertEquals(8 * GB,containerMonitor.getPmemAllocatedForContainers());  resource.setVirtualCores(5);  resource.setMemorySize(4 * 1024);
protected <T,R>List<R> runInParallel(List<T> testContexts,final Function<T,R> func){  ExecutorCompletionService<R> completionService=new ExecutorCompletionService<R>(this.getThreadPool());
protected <T,R>List<R> runInParallel(List<T> testContexts,final Function<T,R> func){  ExecutorCompletionService<R> completionService=new ExecutorCompletionService<R>(this.getThreadPool());  LOG.info("Sending requests to endpoints asynchronously. Number of test contexts=" + testContexts.size());  for (int index=0; index < testContexts.size(); index++) {    final T testContext=testContexts.get(index);
protected <T,R>List<R> runInParallel(List<T> testContexts,final Function<T,R> func){  ExecutorCompletionService<R> completionService=new ExecutorCompletionService<R>(this.getThreadPool());  LOG.info("Sending requests to endpoints asynchronously. Number of test contexts=" + testContexts.size());  for (int index=0; index < testContexts.size(); index++) {    final T testContext=testContexts.get(index);    LOG.info("Adding request to threadpool for test context: " + testContext.toString());    completionService.submit(new Callable<R>(){      @Override public R call() throws Exception {
protected <T,R>List<R> runInParallel(List<T> testContexts,final Function<T,R> func){  ExecutorCompletionService<R> completionService=new ExecutorCompletionService<R>(this.getThreadPool());  LOG.info("Sending requests to endpoints asynchronously. Number of test contexts=" + testContexts.size());  for (int index=0; index < testContexts.size(); index++) {    final T testContext=testContexts.get(index);    LOG.info("Adding request to threadpool for test context: " + testContext.toString());    completionService.submit(new Callable<R>(){      @Override public R call() throws Exception {        LOG.info("Sending request. Test context:" + testContext.toString());        R response=null;        try {          response=func.invoke(testContext);
    final T testContext=testContexts.get(index);    LOG.info("Adding request to threadpool for test context: " + testContext.toString());    completionService.submit(new Callable<R>(){      @Override public R call() throws Exception {        LOG.info("Sending request. Test context:" + testContext.toString());        R response=null;        try {          response=func.invoke(testContext);          LOG.info("Successfully sent request for context: " + testContext.toString());        } catch (        Throwable ex) {          LOG.error("Failed to process request for context: " + testContext);          response=null;        }        return response;      }    });  }  ArrayList<R> responseList=new ArrayList<R>();
          response=func.invoke(testContext);          LOG.info("Successfully sent request for context: " + testContext.toString());        } catch (        Throwable ex) {          LOG.error("Failed to process request for context: " + testContext);          response=null;        }        return response;      }    });  }  ArrayList<R> responseList=new ArrayList<R>();  LOG.info("Waiting for responses from endpoints. Number of contexts=" + testContexts.size());  for (int i=0; i < testContexts.size(); ++i) {    try {      final Future<R> future=completionService.take();      final R response=future.get(3000,TimeUnit.MILLISECONDS);      responseList.add(response);    } catch (    Throwable e) {
protected <T>List<RegisterApplicationMasterResponseInfo<T>> registerApplicationMastersInParallel(final ArrayList<T> testContexts){  List<RegisterApplicationMasterResponseInfo<T>> responses=runInParallel(testContexts,new Function<T,RegisterApplicationMasterResponseInfo<T>>(){    @Override public RegisterApplicationMasterResponseInfo<T> invoke(    T testContext){      RegisterApplicationMasterResponseInfo<T> response=null;      try {        int index=testContexts.indexOf(testContext);        response=new RegisterApplicationMasterResponseInfo<T>(registerApplicationMaster(index),testContext);        Assert.assertNotNull(response.getResponse());        Assert.assertEquals(Integer.toString(index),response.getResponse().getQueue());
protected <T>List<FinishApplicationMasterResponseInfo<T>> finishApplicationMastersInParallel(final ArrayList<T> testContexts){  List<FinishApplicationMasterResponseInfo<T>> responses=runInParallel(testContexts,new Function<T,FinishApplicationMasterResponseInfo<T>>(){    @Override public FinishApplicationMasterResponseInfo<T> invoke(    T testContext){      FinishApplicationMasterResponseInfo<T> response=null;      try {        response=new FinishApplicationMasterResponseInfo<T>(finishApplicationMaster(testContexts.indexOf(testContext),FinalApplicationStatus.SUCCEEDED),testContext);        Assert.assertNotNull(response.getResponse());
private ArrayList<String> CreateTestRequestIdentifiers(int numberOfRequests){  ArrayList<String> testContexts=new ArrayList<String>();
private ArrayList<String> CreateTestRequestIdentifiers(int numberOfRequests){  ArrayList<String> testContexts=new ArrayList<String>();  LOG.info("Creating " + numberOfRequests + " contexts for testing");  for (int ep=0; ep < numberOfRequests; ep++) {    testContexts.add("test-endpoint-" + Integer.toString(ep));
@Test public void testFinishMulitpleApplicationMastersInParallel() throws Exception {  int numberOfRequests=5;  ArrayList<String> testContexts=new ArrayList<String>();
@Test public void testFinishMulitpleApplicationMastersInParallel() throws Exception {  int numberOfRequests=5;  ArrayList<String> testContexts=new ArrayList<String>();  LOG.info("Creating " + numberOfRequests + " contexts for testing");  for (int i=0; i < numberOfRequests; i++) {    testContexts.add("test-endpoint-" + Integer.toString(i));
@Test public void testAllocateAndReleaseContainersForMultipleAMInParallel() throws Exception {  int numberOfApps=6;  ArrayList<Integer> tempAppIds=new ArrayList<Integer>();  for (int i=0; i < numberOfApps; i++) {    tempAppIds.add(new Integer(i));  }  final ArrayList<Integer> appIds=tempAppIds;  List<Integer> responses=runInParallel(appIds,new Function<Integer,Integer>(){    @Override public Integer invoke(    Integer testAppId){      try {        RegisterApplicationMasterResponse registerResponse=registerApplicationMaster(testAppId);        Assert.assertNotNull("response is null",registerResponse);        List<Container> containers=getContainersAndAssert(testAppId,10);        releaseContainersAndAssert(testAppId,containers);        LOG.info("Sucessfully registered application master with appId: " + testAppId);      } catch (      Throwable ex) {
  List<Container> containers=new ArrayList<Container>(numberOfResourceRequests);  List<ResourceRequest> askList=new ArrayList<ResourceRequest>(numberOfResourceRequests);  for (int testAppId=0; testAppId < numberOfResourceRequests; testAppId++) {    askList.add(createResourceRequest("test-node-" + Integer.toString(testAppId),6000,2,testAppId % 5,1));  }  allocateRequest.setAskList(askList);  AllocateResponse allocateResponse=allocate(appId,allocateRequest);  Assert.assertNotNull("allocate() returned null response",allocateResponse);  Assert.assertNull("new AMRMToken from RM should have been nulled by AMRMProxyService",allocateResponse.getAMRMToken());  containers.addAll(allocateResponse.getAllocatedContainers());  int numHeartbeat=0;  while (containers.size() < askList.size() && numHeartbeat++ < 10) {    allocateResponse=allocate(appId,Records.newRecord(AllocateRequest.class));    Assert.assertNotNull("allocate() returned null response",allocateResponse);    Assert.assertNull("new AMRMToken from RM should have been nulled by AMRMProxyService",allocateResponse.getAMRMToken());    containers.addAll(allocateResponse.getAllocatedContainers());
  List<ResourceRequest> askList=new ArrayList<ResourceRequest>(numberOfResourceRequests);  for (int testAppId=0; testAppId < numberOfResourceRequests; testAppId++) {    askList.add(createResourceRequest("test-node-" + Integer.toString(testAppId),6000,2,testAppId % 5,1));  }  allocateRequest.setAskList(askList);  AllocateResponse allocateResponse=allocate(appId,allocateRequest);  Assert.assertNotNull("allocate() returned null response",allocateResponse);  Assert.assertNull("new AMRMToken from RM should have been nulled by AMRMProxyService",allocateResponse.getAMRMToken());  containers.addAll(allocateResponse.getAllocatedContainers());  int numHeartbeat=0;  while (containers.size() < askList.size() && numHeartbeat++ < 10) {    allocateResponse=allocate(appId,Records.newRecord(AllocateRequest.class));    Assert.assertNotNull("allocate() returned null response",allocateResponse);    Assert.assertNull("new AMRMToken from RM should have been nulled by AMRMProxyService",allocateResponse.getAMRMToken());    containers.addAll(allocateResponse.getAllocatedContainers());    LOG.info("Number of allocated containers in this request: " + Integer.toString(allocateResponse.getAllocatedContainers().size()));
  }  allocateRequest.setReleaseList(relList);  AllocateResponse allocateResponse=allocate(appId,allocateRequest);  Assert.assertNotNull(allocateResponse);  Assert.assertNull("new AMRMToken from RM should have been nulled by AMRMProxyService",allocateResponse.getAMRMToken());  List<ContainerId> containersForReleasedContainerIds=new ArrayList<>();  List<ContainerId> newlyFinished=getCompletedContainerIds(allocateResponse.getCompletedContainersStatuses());  containersForReleasedContainerIds.addAll(newlyFinished);  int numHeartbeat=0;  while (containersForReleasedContainerIds.size() < relList.size() && numHeartbeat++ < 10) {    allocateResponse=allocate(appId,Records.newRecord(AllocateRequest.class));    Assert.assertNotNull(allocateResponse);    Assert.assertNull("new AMRMToken from RM should have been nulled by AMRMProxyService",allocateResponse.getAMRMToken());    newlyFinished=getCompletedContainerIds(allocateResponse.getCompletedContainersStatuses());    containersForReleasedContainerIds.addAll(newlyFinished);    LOG.info("Number of containers received in this request: " + Integer.toString(allocateResponse.getAllocatedContainers().size()));
  AllocateResponse allocateResponse=interceptor.allocate(allocateRequest);  Assert.assertNotNull("allocate() returned null response",allocateResponse);  checkAMRMToken(allocateResponse.getAMRMToken());  lastResponseId=allocateResponse.getResponseId();  containers.addAll(allocateResponse.getAllocatedContainers());  LOG.info("Number of allocated containers in the original request: " + Integer.toString(allocateResponse.getAllocatedContainers().size()));  int numHeartbeat=0;  while (containers.size() < numberOfAllocationExcepted && numHeartbeat++ < 10) {    allocateRequest=Records.newRecord(AllocateRequest.class);    allocateRequest.setResponseId(lastResponseId);    allocateResponse=interceptor.allocate(allocateRequest);    Assert.assertNotNull("allocate() returned null response",allocateResponse);    checkAMRMToken(allocateResponse.getAMRMToken());    lastResponseId=allocateResponse.getResponseId();    interceptor.drainAllAsyncQueue(false);
  Assert.assertNotNull("allocate() returned null response",allocateResponse);  checkAMRMToken(allocateResponse.getAMRMToken());  lastResponseId=allocateResponse.getResponseId();  containers.addAll(allocateResponse.getAllocatedContainers());  LOG.info("Number of allocated containers in the original request: " + Integer.toString(allocateResponse.getAllocatedContainers().size()));  int numHeartbeat=0;  while (containers.size() < numberOfAllocationExcepted && numHeartbeat++ < 10) {    allocateRequest=Records.newRecord(AllocateRequest.class);    allocateRequest.setResponseId(lastResponseId);    allocateResponse=interceptor.allocate(allocateRequest);    Assert.assertNotNull("allocate() returned null response",allocateResponse);    checkAMRMToken(allocateResponse.getAMRMToken());    lastResponseId=allocateResponse.getResponseId();    interceptor.drainAllAsyncQueue(false);    containers.addAll(allocateResponse.getAllocatedContainers());
private void releaseContainersAndAssert(List<Container> containers) throws Exception {  Assert.assertTrue(containers.size() > 0);  AllocateRequest allocateRequest=Records.newRecord(AllocateRequest.class);  List<ContainerId> relList=new ArrayList<ContainerId>(containers.size());  for (  Container container : containers) {    relList.add(container.getId());  }  allocateRequest.setReleaseList(relList);  allocateRequest.setResponseId(lastResponseId);  AllocateResponse allocateResponse=interceptor.allocate(allocateRequest);  Assert.assertNotNull(allocateResponse);  checkAMRMToken(allocateResponse.getAMRMToken());  lastResponseId=allocateResponse.getResponseId();  List<ContainerId> containersForReleasedContainerIds=new ArrayList<ContainerId>();  List<ContainerId> newlyFinished=getCompletedContainerIds(allocateResponse.getCompletedContainersStatuses());  containersForReleasedContainerIds.addAll(newlyFinished);
  lastResponseId=allocateResponse.getResponseId();  List<ContainerId> containersForReleasedContainerIds=new ArrayList<ContainerId>();  List<ContainerId> newlyFinished=getCompletedContainerIds(allocateResponse.getCompletedContainersStatuses());  containersForReleasedContainerIds.addAll(newlyFinished);  LOG.info("Number of containers received in the original request: " + Integer.toString(newlyFinished.size()));  int numHeartbeat=0;  while (containersForReleasedContainerIds.size() < relList.size() && numHeartbeat++ < 10) {    allocateRequest=Records.newRecord(AllocateRequest.class);    allocateRequest.setResponseId(lastResponseId);    allocateResponse=interceptor.allocate(allocateRequest);    Assert.assertNotNull(allocateResponse);    checkAMRMToken(allocateResponse.getAMRMToken());    lastResponseId=allocateResponse.getResponseId();    interceptor.drainAllAsyncQueue(false);    newlyFinished=getCompletedContainerIds(allocateResponse.getCompletedContainersStatuses());
  List<ContainerId> containersForReleasedContainerIds=new ArrayList<ContainerId>();  List<ContainerId> newlyFinished=getCompletedContainerIds(allocateResponse.getCompletedContainersStatuses());  containersForReleasedContainerIds.addAll(newlyFinished);  LOG.info("Number of containers received in the original request: " + Integer.toString(newlyFinished.size()));  int numHeartbeat=0;  while (containersForReleasedContainerIds.size() < relList.size() && numHeartbeat++ < 10) {    allocateRequest=Records.newRecord(AllocateRequest.class);    allocateRequest.setResponseId(lastResponseId);    allocateResponse=interceptor.allocate(allocateRequest);    Assert.assertNotNull(allocateResponse);    checkAMRMToken(allocateResponse.getAMRMToken());    lastResponseId=allocateResponse.getResponseId();    interceptor.drainAllAsyncQueue(false);    newlyFinished=getCompletedContainerIds(allocateResponse.getCompletedContainersStatuses());    containersForReleasedContainerIds.addAll(newlyFinished);
  registerReq.setHost(Integer.toString(testAppId));  registerReq.setRpcPort(testAppId);  registerReq.setTrackingUrl("");  UserGroupInformation ugi=interceptor.getUGIWithToken(interceptor.getAttemptId());  ugi.doAs(new PrivilegedExceptionAction<Object>(){    @Override public Object run() throws Exception {      RegisterApplicationMasterResponse registerResponse=interceptor.registerApplicationMaster(registerReq);      Assert.assertNotNull(registerResponse);      lastResponseId=0;      Assert.assertEquals(0,interceptor.getUnmanagedAMPoolSize());      registerSubCluster(SubClusterId.newInstance("SC-1"));      registerSubCluster(SubClusterId.newInstance(HOME_SC_ID));      int numberOfContainers=3;      List<Container> containers=getContainersAndAssert(numberOfContainers,numberOfContainers * 2);      for (      Container c : containers) {
@Before public void setup() throws IOException {  localFS.delete(new Path(localDir.getAbsolutePath()),true);  localFS.delete(new Path(tmpDir.getAbsolutePath()),true);  localFS.delete(new Path(localLogDir.getAbsolutePath()),true);  localFS.delete(new Path(remoteLogDir.getAbsolutePath()),true);  localDir.mkdir();  tmpDir.mkdir();  localLogDir.mkdir();  remoteLogDir.mkdir();
@Before public void setup() throws IOException {  localFS.delete(new Path(localDir.getAbsolutePath()),true);  localFS.delete(new Path(tmpDir.getAbsolutePath()),true);  localFS.delete(new Path(localLogDir.getAbsolutePath()),true);  localFS.delete(new Path(remoteLogDir.getAbsolutePath()),true);  localDir.mkdir();  tmpDir.mkdir();  localLogDir.mkdir();  remoteLogDir.mkdir();  LOG.info("Created localDir in " + localDir.getAbsolutePath());
protected DeletionService createDeletionService(){  return new DeletionService(exec){    @Override public void delete(    DeletionTask deletionTask){
public static void waitForContainerState(ContainerManagementProtocol containerManager,ContainerId containerID,List<ContainerState> finalStates,int timeOutMax) throws InterruptedException, YarnException, IOException {  List<ContainerId> list=new ArrayList<ContainerId>();  list.add(containerID);  GetContainerStatusesRequest request=GetContainerStatusesRequest.newInstance(list);  ContainerStatus containerStatus=null;  HashSet<ContainerState> fStates=new HashSet<>(finalStates);  int timeoutSecs=0;  do {    Thread.sleep(1000);    containerStatus=containerManager.getContainerStatuses(request).getContainerStatuses().get(0);
public static void waitForApplicationState(ContainerManagerImpl containerManager,ApplicationId appID,ApplicationState finalState) throws InterruptedException {  Application app=containerManager.getContext().getApplications().get(appID);  int timeout=0;  while (!(app.getApplicationState().equals(finalState)) && timeout++ < 15) {
@Override @Before public void setup() throws IOException {  localFS.delete(new Path(localDir.getAbsolutePath()),true);  localFS.delete(new Path(tmpDir.getAbsolutePath()),true);  localFS.delete(new Path(localLogDir.getAbsolutePath()),true);  localFS.delete(new Path(remoteLogDir.getAbsolutePath()),true);  localDir.mkdir();  tmpDir.mkdir();  localLogDir.mkdir();  remoteLogDir.mkdir();
@Override @Before public void setup() throws IOException {  localFS.delete(new Path(localDir.getAbsolutePath()),true);  localFS.delete(new Path(tmpDir.getAbsolutePath()),true);  localFS.delete(new Path(localLogDir.getAbsolutePath()),true);  localFS.delete(new Path(remoteLogDir.getAbsolutePath()),true);  localDir.mkdir();  tmpDir.mkdir();  localLogDir.mkdir();  remoteLogDir.mkdir();  LOG.info("Created localDir in " + localDir.getAbsolutePath());
  StartContainersRequest allRequests=StartContainersRequest.newInstance(list);  containerManager.startContainers(allRequests);  int timeoutSecs=0;  while (!processStartFile.exists() && timeoutSecs++ < 20) {    Thread.sleep(1000);    LOG.info("Waiting for process start-file to be created");  }  Assert.assertTrue("ProcessStartFile doesn't exist!",processStartFile.exists());  BufferedReader reader=new BufferedReader(new FileReader(processStartFile));  String pid=reader.readLine().trim();  Assert.assertEquals(null,reader.readLine());  reader.close();  reader=new BufferedReader(new FileReader(childProcessStartFile));  String child=reader.readLine().trim();  Assert.assertEquals(null,reader.readLine());  reader.close();
  List<PrivilegedOperation> ops=new ArrayList<>();  ops.add(opTasksNone);  ops.add(opDisallowed);  try {    PrivilegedOperationExecutor.squashCGroupOperations(ops);    Assert.fail("Expected squash operation to fail with an exception!");  } catch (  PrivilegedOperationException e) {    LOG.info("Caught expected exception : " + e);  }  ops.clear();  ops.add(opTasksNone);  ops.add(opTasksInvalid);  try {    PrivilegedOperationExecutor.squashCGroupOperations(ops);    Assert.fail("Expected squash operation to fail with an exception!");  } catch (  PrivilegedOperationException e) {
  File emptyMtab=createEmptyCgroups();  try {    CGroupsHandler cGroupsHandler=new CGroupsHandlerImpl(createMountConfiguration(),privilegedOperationExecutorMock,emptyMtab.getAbsolutePath());    PrivilegedOperation expectedOp=new PrivilegedOperation(PrivilegedOperation.OperationType.MOUNT_CGROUPS);    String controllerKV=controller.getName() + "=" + tmpPath+ Path.SEPARATOR+ controller.getName();    expectedOp.appendArgs(hierarchy,controllerKV);    cGroupsHandler.initializeCGroupController(controller);    try {      ArgumentCaptor<PrivilegedOperation> opCaptor=ArgumentCaptor.forClass(PrivilegedOperation.class);      verify(privilegedOperationExecutorMock).executePrivilegedOperation(opCaptor.capture(),eq(false));      Assert.assertEquals(expectedOp,opCaptor.getValue());      verifyNoMoreInteractions(privilegedOperationExecutorMock);      cGroupsHandler.initializeCGroupController(controller);      verifyNoMoreInteractions(privilegedOperationExecutorMock);    } catch (    PrivilegedOperationException e) {
    PrivilegedOperation expectedOp=new PrivilegedOperation(PrivilegedOperation.OperationType.MOUNT_CGROUPS);    String controllerKV=controller.getName() + "=" + tmpPath+ Path.SEPARATOR+ controller.getName();    expectedOp.appendArgs(hierarchy,controllerKV);    cGroupsHandler.initializeCGroupController(controller);    try {      ArgumentCaptor<PrivilegedOperation> opCaptor=ArgumentCaptor.forClass(PrivilegedOperation.class);      verify(privilegedOperationExecutorMock).executePrivilegedOperation(opCaptor.capture(),eq(false));      Assert.assertEquals(expectedOp,opCaptor.getValue());      verifyNoMoreInteractions(privilegedOperationExecutorMock);      cGroupsHandler.initializeCGroupController(controller);      verifyNoMoreInteractions(privilegedOperationExecutorMock);    } catch (    PrivilegedOperationException e) {      LOG.error("Caught exception: " + e);      assertTrue("Unexpected PrivilegedOperationException from mock!",false);    }  } catch (  ResourceHandlerException e) {
  try {    String path=cGroupsHandler.createCGroup(controller,testCGroup);    assertTrue(new File(expectedPath).exists());    Assert.assertEquals(expectedPath,path);    String param="test_param";    String paramValue="test_param_value";    cGroupsHandler.updateCGroupParam(controller,testCGroup,param,paramValue);    String paramPath=expectedPath + Path.SEPARATOR + controller.getName()+ "."+ param;    File paramFile=new File(paramPath);    assertTrue(paramFile.exists());    try {      Assert.assertEquals(paramValue,new String(Files.readAllBytes(paramFile.toPath())));    } catch (    IOException e) {      LOG.error("Caught exception: " + e);      Assert.fail("Unexpected IOException trying to read cgroup param!");
  conf.setBoolean(YarnConfiguration.NM_RECOVERY_ENABLED,true);  TrafficController trafficController=new TrafficController(conf,privilegedOperationExecutorMock);  try {    when(privilegedOperationExecutorMock.executePrivilegedOperation(any(PrivilegedOperation.class),eq(true))).thenReturn(DEFAULT_TC_STATE_EXAMPLE);    trafficController.bootstrap(DEVICE,ROOT_BANDWIDTH_MBIT,YARN_BANDWIDTH_MBIT);    ArgumentCaptor<PrivilegedOperation> readOpCaptor=ArgumentCaptor.forClass(PrivilegedOperation.class);    verify(privilegedOperationExecutorMock,times(1)).executePrivilegedOperation(readOpCaptor.capture(),eq(true));    List<PrivilegedOperation> readOps=readOpCaptor.getAllValues();    verifyTrafficControlOperation(readOps.get(0),PrivilegedOperation.OperationType.TC_READ_STATE,Arrays.asList(READ_QDISC_CMD,READ_FILTER_CMD,READ_CLASS_CMD));    ArgumentCaptor<PrivilegedOperation> writeOpCaptor=ArgumentCaptor.forClass(PrivilegedOperation.class);    verify(privilegedOperationExecutorMock,times(2)).executePrivilegedOperation(writeOpCaptor.capture(),eq(false));    List<PrivilegedOperation> writeOps=writeOpCaptor.getAllValues();    verifyTrafficControlOperation(writeOps.get(0),PrivilegedOperation.OperationType.TC_MODIFY_STATE,Arrays.asList(WIPE_STATE_CMD));    verifyTrafficControlOperation(writeOps.get(1),PrivilegedOperation.OperationType.TC_MODIFY_STATE,Arrays.asList(ADD_ROOT_QDISC_CMD,ADD_CGROUP_FILTER_CMD,ADD_ROOT_CLASS_CMD,ADD_DEFAULT_CLASS_CMD,ADD_YARN_CLASS_CMD));  } catch (  ResourceHandlerException|PrivilegedOperationException|IOException e) {
    Assert.assertTrue(classId >= MIN_CONTAINER_CLASS_ID);    Assert.assertEquals(String.format(FORMAT_CONTAINER_CLASS_STR,classId),trafficController.getStringForNetClsClassId(classId));    TrafficController.BatchBuilder builder=trafficController.new BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE).addContainerClass(classId,CONTAINER_BANDWIDTH_MBIT,false);    PrivilegedOperation addClassOp=builder.commitBatchToTempFile();    String expectedAddClassCmd=String.format(FORMAT_ADD_CONTAINER_CLASS_TO_DEVICE,classId,YARN_BANDWIDTH_MBIT);    verifyTrafficControlOperation(addClassOp,PrivilegedOperation.OperationType.TC_MODIFY_STATE,Arrays.asList(expectedAddClassCmd));    TrafficController.BatchBuilder strictModeBuilder=trafficController.new BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE).addContainerClass(classId,CONTAINER_BANDWIDTH_MBIT,true);    PrivilegedOperation addClassStrictModeOp=strictModeBuilder.commitBatchToTempFile();    String expectedAddClassStrictModeCmd=String.format(FORMAT_ADD_CONTAINER_CLASS_TO_DEVICE,classId,CONTAINER_BANDWIDTH_MBIT);    verifyTrafficControlOperation(addClassStrictModeOp,PrivilegedOperation.OperationType.TC_MODIFY_STATE,Arrays.asList(expectedAddClassStrictModeCmd));    TrafficController.BatchBuilder deleteBuilder=trafficController.new BatchBuilder(PrivilegedOperation.OperationType.TC_MODIFY_STATE).deleteContainerClass(classId);    PrivilegedOperation deleteClassOp=deleteBuilder.commitBatchToTempFile();    String expectedDeleteClassCmd=String.format(FORAMT_DELETE_CONTAINER_CLASS_FROM_DEVICE,classId);    verifyTrafficControlOperation(deleteClassOp,PrivilegedOperation.OperationType.TC_MODIFY_STATE,Arrays.asList(expectedDeleteClassCmd));  } catch (  ResourceHandlerException|IOException e) {
  when(mockApplicationId.toString()).thenReturn("applicationId");  when(appAttemptId.getApplicationId()).thenReturn(mockApplicationId);  when(cId.getApplicationAttemptId()).thenReturn(appAttemptId);  when(container.getLaunchContext()).thenReturn(context);  when(context.getEnvironment()).thenReturn(env);  when(container.getUser()).thenReturn(submittingUser);  String uid="";  String gid="";  Shell.ShellCommandExecutor shexec1=new Shell.ShellCommandExecutor(new String[]{"id","-u",runAsUser});  Shell.ShellCommandExecutor shexec2=new Shell.ShellCommandExecutor(new String[]{"id","-g",runAsUser});  Shell.ShellCommandExecutor shexec3=new Shell.ShellCommandExecutor(new String[]{"id","-G",runAsUser});  try {    shexec1.execute();    uid=shexec1.getOutput().replaceAll("\n$","");  } catch (  Exception e) {
  when(container.getUser()).thenReturn(submittingUser);  String uid="";  String gid="";  Shell.ShellCommandExecutor shexec1=new Shell.ShellCommandExecutor(new String[]{"id","-u",runAsUser});  Shell.ShellCommandExecutor shexec2=new Shell.ShellCommandExecutor(new String[]{"id","-g",runAsUser});  Shell.ShellCommandExecutor shexec3=new Shell.ShellCommandExecutor(new String[]{"id","-G",runAsUser});  try {    shexec1.execute();    uid=shexec1.getOutput().replaceAll("\n$","");  } catch (  Exception e) {    LOG.info("Could not run id -u command: " + e);  }  try {    shexec2.execute();    gid=shexec2.getOutput().replaceAll("\n$","");  } catch (  Exception e) {
  Shell.ShellCommandExecutor shexec3=new Shell.ShellCommandExecutor(new String[]{"id","-G",runAsUser});  try {    shexec1.execute();    uid=shexec1.getOutput().replaceAll("\n$","");  } catch (  Exception e) {    LOG.info("Could not run id -u command: " + e);  }  try {    shexec2.execute();    gid=shexec2.getOutput().replaceAll("\n$","");  } catch (  Exception e) {    LOG.info("Could not run id -g command: " + e);  }  try {    shexec3.execute();    groups=shexec3.getOutput().replace("\n"," ").split(" ");  } catch (  Exception e) {
  ConcurrentMap<ContainerId,Container> containerMap=mock(ConcurrentMap.class);  when(mockNMContext.getLocalDirsHandler()).thenReturn(localDirsHandler);  when(mockNMContext.getResourcePluginManager()).thenReturn(resourcePluginManager);  when(mockNMContext.getContainers()).thenReturn(containerMap);  when(containerMap.get(any())).thenReturn(container);  ContainerManager mockContainerManager=mock(ContainerManager.class);  ResourceLocalizationService mockLocalzationService=mock(ResourceLocalizationService.class);  LocalizedResource mockLocalizedResource=mock(LocalizedResource.class);  when(mockLocalizedResource.getLocalPath()).thenReturn(new Path("/local/layer1"));  when(mockLocalzationService.getLocalizedResource(any(),anyString(),any())).thenReturn(mockLocalizedResource);  when(mockContainerManager.getResourceLocalizationService()).thenReturn(mockLocalzationService);  when(mockNMContext.getContainerManager()).thenReturn(mockContainerManager);  try {    when(localDirsHandler.getLocalPathForWrite(anyString())).thenReturn(new Path(tmpPath));  } catch (  IOException ioe) {
  Assert.assertEquals("  docker-command=run",dockerCommands.get(counter++));  Assert.assertEquals("  group-add=" + String.join(",",groups),dockerCommands.get(counter++));  Assert.assertEquals("  hostname=ctr-e11-1518975676334-14532816-01-000001",dockerCommands.get(counter++));  Assert.assertEquals("  image=busybox:latest",dockerCommands.get(counter++));  Assert.assertEquals("  launch-command=bash,/test_container_work_dir/launch_container.sh",dockerCommands.get(counter++));  Assert.assertEquals("  mounts=" + "/test_container_log_dir:/test_container_log_dir:rw," + "/test_application_local_dir:/test_application_local_dir:rw,"+ "/test_filecache_dir:/test_filecache_dir:ro,"+ "/test_user_filecache_dir:/test_user_filecache_dir:ro",dockerCommands.get(counter++));  Assert.assertEquals("  name=container_e11_1518975676334_14532816_01_000001",dockerCommands.get(counter++));  Assert.assertEquals("  net=sdn2",dockerCommands.get(counter++));  Assert.assertEquals("  user=" + uidGidPair,dockerCommands.get(counter++));  Assert.assertEquals("  workdir=/test_container_work_dir",dockerCommands.get(counter));  env.put(DockerLinuxContainerRuntime.ENV_DOCKER_CONTAINER_NETWORK,customNetwork3);  try {    runtime.launchContainer(builder.build());    Assert.fail("Disallowed network : " + customNetwork3 + "did not trigger launch failure.");  } catch (  ContainerExecutionException e) {
  ConcurrentMap<ContainerId,Container> containerMap=mock(ConcurrentMap.class);  when(mockNMContext.getLocalDirsHandler()).thenReturn(localDirsHandler);  when(mockNMContext.getResourcePluginManager()).thenReturn(resourcePluginManager);  when(mockNMContext.getContainers()).thenReturn(containerMap);  when(containerMap.get(any())).thenReturn(container);  ContainerManager mockContainerManager=mock(ContainerManager.class);  ResourceLocalizationService mockLocalzationService=mock(ResourceLocalizationService.class);  LocalizedResource mockLocalizedResource=mock(LocalizedResource.class);  when(mockLocalizedResource.getLocalPath()).thenReturn(new Path("/local/layer1"));  when(mockLocalzationService.getLocalizedResource(any(),anyString(),any())).thenReturn(mockLocalizedResource);  when(mockContainerManager.getResourceLocalizationService()).thenReturn(mockLocalzationService);  when(mockNMContext.getContainerManager()).thenReturn(mockContainerManager);  try {    when(localDirsHandler.getLocalPathForWrite(anyString())).thenReturn(new Path(tmpPath));  } catch (  IOException ioe) {
private void writeContainerLogs(File appLogDir,ContainerId containerId,String[] fileName,String[] emptyFiles) throws IOException {  String containerStr=containerId.toString();  File containerLogDir=new File(appLogDir,containerStr);  boolean created=containerLogDir.mkdirs();
    LogKey key=new LogKey();    valueStream=reader.next(key);    while (valueStream != null) {      LOG.info("Found container " + key.toString());      Map<String,String> perContainerMap=new HashMap<String,String>();      logMap.put(key.toString(),perContainerMap);      while (true) {        try {          ByteArrayOutputStream baos=new ByteArrayOutputStream();          PrintStream ps=new PrintStream(baos);          LogReader.readAContainerLogsForALogType(valueStream,ps);          String writtenLines[]=baos.toString().split(System.getProperty("line.separator"));          Assert.assertEquals("LogType:",writtenLines[0].substring(0,8));          String fileType=writtenLines[0].substring(8);          fileTypes.add(fileType);
private int numOfLogsAvailable(LogAggregationService logAggregationService,ApplicationId appId,boolean sizeLimited,String lastLogFile) throws IOException {  Path appLogDir=logAggregationService.getLogAggregationFileController(conf).getRemoteAppLogDir(appId,this.user);  RemoteIterator<FileStatus> nodeFiles=null;  try {    Path qualifiedLogDir=FileContext.getFileContext(this.conf).makeQualified(appLogDir);    nodeFiles=FileContext.getFileContext(qualifiedLogDir.toUri(),this.conf).listStatus(appLogDir);  } catch (  FileNotFoundException fnf) {    LOG.info("Context file not vailable: " + fnf);    return -1;  }  int count=0;  while (nodeFiles.hasNext()) {    FileStatus status=nodeFiles.next();    String filename=status.getPath().getName();    if (filename.contains(LogAggregationUtils.TMP_FILE_SUFFIX) || (lastLogFile != null && filename.contains(lastLogFile) && sizeLimited)) {      LOG.info("fileName :" + filename);
  try {    Path qualifiedLogDir=FileContext.getFileContext(this.conf).makeQualified(appLogDir);    nodeFiles=FileContext.getFileContext(qualifiedLogDir.toUri(),this.conf).listStatus(appLogDir);  } catch (  FileNotFoundException fnf) {    LOG.info("Context file not vailable: " + fnf);    return -1;  }  int count=0;  while (nodeFiles.hasNext()) {    FileStatus status=nodeFiles.next();    String filename=status.getPath().getName();    if (filename.contains(LogAggregationUtils.TMP_FILE_SUFFIX) || (lastLogFile != null && filename.contains(lastLogFile) && sizeLimited)) {      LOG.info("fileName :" + filename);      LOG.info("lastLogFile :" + lastLogFile);      return -1;    }    if (filename.contains(LogAggregationUtils.getNodeString(logAggregationService.getNodeId()))) {
        totalBoostAgainstMedian+=bstAgainstMedian;        totalBoostAgainstMin+=bstAgainstMinimum;        count++;        if (maxBoostAgainstMedian < bstAgainstMedian) {          maxBoostAgainstMedian=bstAgainstMedian;        }        if (maxBoostAgainstMin < bstAgainstMinimum) {          maxBoostAgainstMin=bstAgainstMinimum;        }        totalBoostAgainstMinCertainModel+=bstAgainstMinimum;        totalBoostAgainstMedianCertainModel+=bstAgainstMedian;        if (maxBoostAgainstMinCertainModel < bstAgainstMinimum) {          maxBoostAgainstMinCertainModel=bstAgainstMinimum;        }        if (maxBoostAgainstMedianCertainModel < bstAgainstMedian) {          maxBoostAgainstMedianCertainModel=bstAgainstMedian;        }        countOfEachModel++;      }    }    LOG.info("Model:{}, The best performance boost against median value is " + "{}",model,maxBoostAgainstMedianCertainModel);
        totalBoostAgainstMin+=bstAgainstMinimum;        count++;        if (maxBoostAgainstMedian < bstAgainstMedian) {          maxBoostAgainstMedian=bstAgainstMedian;        }        if (maxBoostAgainstMin < bstAgainstMinimum) {          maxBoostAgainstMin=bstAgainstMinimum;        }        totalBoostAgainstMinCertainModel+=bstAgainstMinimum;        totalBoostAgainstMedianCertainModel+=bstAgainstMedian;        if (maxBoostAgainstMinCertainModel < bstAgainstMinimum) {          maxBoostAgainstMinCertainModel=bstAgainstMinimum;        }        if (maxBoostAgainstMedianCertainModel < bstAgainstMedian) {          maxBoostAgainstMedianCertainModel=bstAgainstMedian;        }        countOfEachModel++;      }    }    LOG.info("Model:{}, The best performance boost against median value is " + "{}",model,maxBoostAgainstMedianCertainModel);    LOG.info("Model:{}, The aggregated average performance boost against " + "median value is {}",model,totalBoostAgainstMedianCertainModel / countOfEachModel);
        count++;        if (maxBoostAgainstMedian < bstAgainstMedian) {          maxBoostAgainstMedian=bstAgainstMedian;        }        if (maxBoostAgainstMin < bstAgainstMinimum) {          maxBoostAgainstMin=bstAgainstMinimum;        }        totalBoostAgainstMinCertainModel+=bstAgainstMinimum;        totalBoostAgainstMedianCertainModel+=bstAgainstMedian;        if (maxBoostAgainstMinCertainModel < bstAgainstMinimum) {          maxBoostAgainstMinCertainModel=bstAgainstMinimum;        }        if (maxBoostAgainstMedianCertainModel < bstAgainstMedian) {          maxBoostAgainstMedianCertainModel=bstAgainstMedian;        }        countOfEachModel++;      }    }    LOG.info("Model:{}, The best performance boost against median value is " + "{}",model,maxBoostAgainstMedianCertainModel);    LOG.info("Model:{}, The aggregated average performance boost against " + "median value is {}",model,totalBoostAgainstMedianCertainModel / countOfEachModel);    LOG.info("Model:{}, The best performance boost against min value is {}",model,maxBoostAgainstMinCertainModel);
        }        if (maxBoostAgainstMin < bstAgainstMinimum) {          maxBoostAgainstMin=bstAgainstMinimum;        }        totalBoostAgainstMinCertainModel+=bstAgainstMinimum;        totalBoostAgainstMedianCertainModel+=bstAgainstMedian;        if (maxBoostAgainstMinCertainModel < bstAgainstMinimum) {          maxBoostAgainstMinCertainModel=bstAgainstMinimum;        }        if (maxBoostAgainstMedianCertainModel < bstAgainstMedian) {          maxBoostAgainstMedianCertainModel=bstAgainstMedian;        }        countOfEachModel++;      }    }    LOG.info("Model:{}, The best performance boost against median value is " + "{}",model,maxBoostAgainstMedianCertainModel);    LOG.info("Model:{}, The aggregated average performance boost against " + "median value is {}",model,totalBoostAgainstMedianCertainModel / countOfEachModel);    LOG.info("Model:{}, The best performance boost against min value is {}",model,maxBoostAgainstMinCertainModel);    LOG.info("Model:{}, The aggregated average performance boost against " + "min value is {}",model,totalBoostAgainstMinCertainModel / countOfEachModel);  }  LOG.info("For all, the best performance boost against median value is " + maxBoostAgainstMedian);  LOG.info("For all, the aggregated average performance boost against median " + "value is " + totalBoostAgainstMedian / count);
@Override public void onWebSocketText(String message){
@Test public void testWebServerWithServlet(){  int port=startNMWebAppServer("0.0.0.0");
  LOG.info("bind to port: " + port);  StringBuilder sb=new StringBuilder();  sb.append("ws://localhost:").append(port).append("/container/abc/");  String dest=sb.toString();  WebSocketClient client=new WebSocketClient();  try {    ContainerShellClientSocketTest socket=new ContainerShellClientSocketTest();    client.start();    URI echoUri=new URI(dest);    Future<Session> future=client.connect(socket,echoUri);    Session session=future.get();    session.getRemote().sendString("hello world");    session.close();    client.stop();  } catch (  Throwable t) {
    ContainerShellClientSocketTest socket=new ContainerShellClientSocketTest();    client.start();    URI echoUri=new URI(dest);    Future<Session> future=client.connect(socket,echoUri);    Session session=future.get();    session.getRemote().sendString("hello world");    session.close();    client.stop();  } catch (  Throwable t) {    LOG.error("Failed to connect WebSocket and send message to server",t);  } finally {    try {      client.stop();      server.close();    } catch (    Exception e) {
@SuppressWarnings("unchecked") @Override public UpdateNodeResourceResponse updateNodeResource(UpdateNodeResourceRequest request) throws YarnException, IOException {  final String operation="updateNodeResource";  UserGroupInformation user=checkAcls(operation);  checkRMStatus(user.getShortUserName(),operation,"update node resource.");  Map<NodeId,ResourceOption> nodeResourceMap=request.getNodeResourceMap();  Set<NodeId> nodeIds=nodeResourceMap.keySet();  for (  NodeId nodeId : nodeIds) {    RMNode node=this.rm.getRMContext().getRMNodes().get(nodeId);    if (node == null) {
  for (  NodeId nodeId : nodeIds) {    RMNode node=this.rm.getRMContext().getRMNodes().get(nodeId);    if (node == null) {      LOG.error("Resource update get failed on all nodes due to change " + "resource on an unrecognized node: " + nodeId);      throw RPCUtil.getRemoteException("Resource update get failed on all nodes due to change resource " + "on an unrecognized node: " + nodeId);    }  }  boolean allSuccess=true;  for (  Map.Entry<NodeId,ResourceOption> entry : nodeResourceMap.entrySet()) {    ResourceOption newResourceOption=entry.getValue();    NodeId nodeId=entry.getKey();    RMNode node=this.rm.getRMContext().getRMNodes().get(nodeId);    if (node == null) {      LOG.warn("Resource update get failed on an unrecognized node: " + nodeId);      allSuccess=false;    } else {      this.rm.getRMContext().getDispatcher().getEventHandler().handle(new RMNodeResourceUpdateEvent(nodeId,newResourceOption));
private void addPlacementConstraintHandler(Configuration conf){  String placementConstraintsHandler=conf.get(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_HANDLER,YarnConfiguration.DISABLED_RM_PLACEMENT_CONSTRAINTS_HANDLER);  if (placementConstraintsHandler.equals(YarnConfiguration.DISABLED_RM_PLACEMENT_CONSTRAINTS_HANDLER)) {
@Override public FinishApplicationMasterResponse finishApplicationMaster(FinishApplicationMasterRequest request) throws YarnException, IOException {  ApplicationAttemptId applicationAttemptId=YarnServerSecurityUtils.authorizeRequest().getApplicationAttemptId();  ApplicationId appId=applicationAttemptId.getApplicationId();  RMApp rmApp=rmContext.getRMApps().get(applicationAttemptId.getApplicationId());  if (timelineServiceV2Enabled) {    ((RMAppImpl)rmApp).removeCollectorData();  }  if (rmApp.isAppFinalStateStored()) {    LOG.info(rmApp.getApplicationId() + " unregistered successfully. ");    return FinishApplicationMasterResponse.newInstance(true);  }  AllocateResponseLock lock=responseMap.get(applicationAttemptId);  if (lock == null) {    throwApplicationDoesNotExistInCacheException(applicationAttemptId);  }synchronized (lock) {    if (!hasApplicationMasterRegistered(applicationAttemptId)) {      String message="Application Master is trying to unregister before registering for: " + appId;
private void throwApplicationDoesNotExistInCacheException(ApplicationAttemptId appAttemptId) throws InvalidApplicationMasterRequestException {  String message="Application doesn't exist in cache " + appAttemptId;
@Override public AllocateResponse allocate(AllocateRequest request) throws YarnException, IOException {  AMRMTokenIdentifier amrmTokenIdentifier=YarnServerSecurityUtils.authorizeRequest();  ApplicationAttemptId appAttemptId=amrmTokenIdentifier.getApplicationAttemptId();  this.amLivelinessMonitor.receivedPing(appAttemptId);  AllocateResponseLock lock=responseMap.get(appAttemptId);  if (lock == null) {    String message="Application attempt " + appAttemptId + " doesn't exist in ApplicationMasterService cache.";
    if (!hasApplicationMasterRegistered(appAttemptId)) {      String message="AM is not registered for known application attempt: " + appAttemptId + " or RM had restarted after AM registered. "+ " AM should re-register.";      throw new ApplicationMasterNotRegisteredException(message);    }    if (AMRMClientUtils.getNextResponseId(request.getResponseId()) == lastResponse.getResponseId()) {      return lastResponse;    } else     if (request.getResponseId() != lastResponse.getResponseId()) {      throw new InvalidApplicationMasterRequestException(AMRMClientUtils.assembleInvalidResponseIdExceptionMessage(appAttemptId,lastResponse.getResponseId(),request.getResponseId()));    }    AllocateResponse response=recordFactory.newRecordInstance(AllocateResponse.class);    this.amsProcessingChain.allocate(amrmTokenIdentifier.getApplicationAttemptId(),request,response);    MasterKeyData nextMasterKey=this.rmContext.getAMRMTokenSecretManager().getNextMasterKeyData();    if (nextMasterKey != null && nextMasterKey.getMasterKey().getKeyId() != amrmTokenIdentifier.getKeyId()) {      RMApp app=this.rmContext.getRMApps().get(appAttemptId.getApplicationId());      RMAppAttempt appAttempt=app.getRMAppAttempt(appAttemptId);      RMAppAttemptImpl appAttemptImpl=(RMAppAttemptImpl)appAttempt;      Token<AMRMTokenIdentifier> amrmToken=appAttempt.getAMRMToken();
public void registerAppAttempt(ApplicationAttemptId attemptId){  AllocateResponse response=recordFactory.newRecordInstance(AllocateResponse.class);  response.setResponseId(AMRMClientUtils.PRE_REGISTER_RESPONSE_ID);
public void unregisterAttempt(ApplicationAttemptId attemptId){
ApplicationId getNewApplicationId(){  ApplicationId applicationId=org.apache.hadoop.yarn.server.utils.BuilderUtils.newApplicationId(recordFactory,ResourceManager.getClusterTimeStamp(),applicationCounter.incrementAndGet());
    LOG.warn("Unable to get the current user.",ie);    RMAuditLogger.logFailure(user,AuditConstants.SUBMIT_APP_REQUEST,ie.getMessage(),"ClientRMService","Exception in submitting application",applicationId,callerContext,submissionContext.getQueue());    throw RPCUtil.getRemoteException(ie);  }  checkTags(submissionContext.getApplicationTags());  if (timelineServiceV2Enabled) {    String value=null;    try {      for (      String tag : submissionContext.getApplicationTags()) {        if (tag.startsWith(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX + ":") || tag.startsWith(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX.toLowerCase() + ":")) {          value=tag.substring(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX.length() + 1);          Long.valueOf(value);        }      }    } catch (    NumberFormatException e) {      LOG.warn("Invalid to flow run: " + value + ". Flow run should be a long integer",e);      RMAuditLogger.logFailure(user,AuditConstants.SUBMIT_APP_REQUEST,e.getMessage(),"ClientRMService","Exception in submitting application",applicationId,submissionContext.getQueue());      throw RPCUtil.getRemoteException(e);
  if (timelineServiceV2Enabled) {    String value=null;    try {      for (      String tag : submissionContext.getApplicationTags()) {        if (tag.startsWith(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX + ":") || tag.startsWith(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX.toLowerCase() + ":")) {          value=tag.substring(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX.length() + 1);          Long.valueOf(value);        }      }    } catch (    NumberFormatException e) {      LOG.warn("Invalid to flow run: " + value + ". Flow run should be a long integer",e);      RMAuditLogger.logFailure(user,AuditConstants.SUBMIT_APP_REQUEST,e.getMessage(),"ClientRMService","Exception in submitting application",applicationId,submissionContext.getQueue());      throw RPCUtil.getRemoteException(e);    }  }  if (rmContext.getRMApps().get(applicationId) != null) {    LOG.info("This is an earlier submitted application: " + applicationId);    return SubmitApplicationResponse.newInstance();  }  ByteBuffer tokenConf=submissionContext.getAMContainerSpec().getTokensConf();
  RMAuditLogger.ArgsBuilder arguments=new RMAuditLogger.ArgsBuilder().append(Keys.QUEUENAME,request.getQueueName()).append(Keys.INCLUDEAPPS,String.valueOf(request.getIncludeApplications())).append(Keys.INCLUDECHILDQUEUES,String.valueOf(request.getIncludeChildQueues())).append(Keys.RECURSIVE,String.valueOf(request.getRecursive()));  try {    QueueInfo queueInfo=scheduler.getQueueInfo(request.getQueueName(),request.getIncludeChildQueues(),request.getRecursive());    List<ApplicationReport> appReports=EMPTY_APPS_REPORT;    if (request.getIncludeApplications()) {      List<ApplicationAttemptId> apps=scheduler.getAppsInQueue(request.getQueueName());      appReports=new ArrayList<ApplicationReport>(apps.size());      for (      ApplicationAttemptId app : apps) {        RMApp rmApp=rmContext.getRMApps().get(app.getApplicationId());        if (rmApp != null) {          if (!checkAccess(callerUGI,rmApp.getUser(),ApplicationAccessType.VIEW_APP,rmApp)) {            continue;          }          appReports.add(rmApp.createAndGetApplicationReport(callerUGI.getUserName(),true));        }      }    }    queueInfo.setApplications(appReports);    response.setQueueInfo(queueInfo);
private void refreshScheduler(String planName,ReservationDefinition contract,String reservationId){  if ((contract.getArrival() - clock.getTime()) < reservationSystem.getPlanFollowerTimeStep()) {
private void refreshScheduler(String planName,ReservationDefinition contract,String reservationId){  if ((contract.getArrival() - clock.getTime()) < reservationSystem.getPlanFollowerTimeStep()) {    LOG.debug("Reservation {} is within threshold so attempting to" + " create synchronously.",reservationId);    reservationSystem.synchronizePlan(planName,true);
public synchronized void remove(NodeId nodeId){  DecommissioningNodeContext context=decomNodes.get(nodeId);  if (context != null) {
@Override public void registerApplicationMaster(ApplicationAttemptId applicationAttemptId,RegisterApplicationMasterRequest request,RegisterApplicationMasterResponse response) throws IOException, YarnException {  RMApp app=getRmContext().getRMApps().get(applicationAttemptId.getApplicationId());
  }  if (app.getApplicationSubmissionContext().getKeepContainersAcrossApplicationAttempts()) {    List<Container> transferredContainers=getScheduler().getTransferredContainers(applicationAttemptId);    if (!transferredContainers.isEmpty()) {      response.setContainersFromPreviousAttempts(transferredContainers);      rmContext.getNMTokenSecretManager().clearNodeSetForAttempt(applicationAttemptId);      List<NMToken> nmTokens=new ArrayList<NMToken>();      for (      Container container : transferredContainers) {        try {          NMToken token=getRmContext().getNMTokenSecretManager().createAndGetNMToken(app.getUser(),applicationAttemptId,container);          if (null != token) {            nmTokens.add(token);          }        } catch (        IllegalArgumentException e) {          if (e.getCause() instanceof UnknownHostException) {            throw (UnknownHostException)e.getCause();          }        }      }      response.setNMTokensFromPreviousAttempts(nmTokens);
  List<RMNode> nodesToDecom=new ArrayList<RMNode>();  HostDetails hostDetails;  gracefulDecommissionableNodes.clear();  if (graceful) {    hostDetails=hostsReader.getLazyLoadedHostDetails();  } else {    hostDetails=hostsReader.getHostDetails();  }  Set<String> includes=hostDetails.getIncludedHosts();  Map<String,Integer> excludes=hostDetails.getExcludedMap();  for (  RMNode n : this.rmContext.getRMNodes().values()) {    NodeState s=n.getState();    boolean isExcluded=!isValidNode(n.getHostName(),includes,excludes.keySet());    String nodeStr="node " + n.getNodeID() + " with state "+ s;    if (!isExcluded) {      if (s == NodeState.DECOMMISSIONING) {
 else {    hostDetails=hostsReader.getHostDetails();  }  Set<String> includes=hostDetails.getIncludedHosts();  Map<String,Integer> excludes=hostDetails.getExcludedMap();  for (  RMNode n : this.rmContext.getRMNodes().values()) {    NodeState s=n.getState();    boolean isExcluded=!isValidNode(n.getHostName(),includes,excludes.keySet());    String nodeStr="node " + n.getNodeID() + " with state "+ s;    if (!isExcluded) {      if (s == NodeState.DECOMMISSIONING) {        LOG.info("Recommission " + nodeStr);        nodesToRecom.add(n);      }    } else {      if (graceful) {        Integer timeoutToUse=(excludes.get(n.getHostName()) != null) ? excludes.get(n.getHostName()) : timeout;
  Map<String,Integer> excludes=hostDetails.getExcludedMap();  for (  RMNode n : this.rmContext.getRMNodes().values()) {    NodeState s=n.getState();    boolean isExcluded=!isValidNode(n.getHostName(),includes,excludes.keySet());    String nodeStr="node " + n.getNodeID() + " with state "+ s;    if (!isExcluded) {      if (s == NodeState.DECOMMISSIONING) {        LOG.info("Recommission " + nodeStr);        nodesToRecom.add(n);      }    } else {      if (graceful) {        Integer timeoutToUse=(excludes.get(n.getHostName()) != null) ? excludes.get(n.getHostName()) : timeout;        if (s != NodeState.DECOMMISSIONED && s != NodeState.DECOMMISSIONING) {          LOG.info("Gracefully decommission " + nodeStr);          nodesToDecom.add(n);
    String nodeStr="node " + n.getNodeID() + " with state "+ s;    if (!isExcluded) {      if (s == NodeState.DECOMMISSIONING) {        LOG.info("Recommission " + nodeStr);        nodesToRecom.add(n);      }    } else {      if (graceful) {        Integer timeoutToUse=(excludes.get(n.getHostName()) != null) ? excludes.get(n.getHostName()) : timeout;        if (s != NodeState.DECOMMISSIONED && s != NodeState.DECOMMISSIONING) {          LOG.info("Gracefully decommission " + nodeStr);          nodesToDecom.add(n);          gracefulDecommissionableNodes.add(n);        } else         if (s == NodeState.DECOMMISSIONING && !Objects.equals(n.getDecommissioningTimeout(),timeoutToUse)) {          LOG.info("Update " + nodeStr + " timeout to be "+ timeoutToUse);          nodesToDecom.add(n);
        LOG.info("Recommission " + nodeStr);        nodesToRecom.add(n);      }    } else {      if (graceful) {        Integer timeoutToUse=(excludes.get(n.getHostName()) != null) ? excludes.get(n.getHostName()) : timeout;        if (s != NodeState.DECOMMISSIONED && s != NodeState.DECOMMISSIONING) {          LOG.info("Gracefully decommission " + nodeStr);          nodesToDecom.add(n);          gracefulDecommissionableNodes.add(n);        } else         if (s == NodeState.DECOMMISSIONING && !Objects.equals(n.getDecommissioningTimeout(),timeoutToUse)) {          LOG.info("Update " + nodeStr + " timeout to be "+ timeoutToUse);          nodesToDecom.add(n);          gracefulDecommissionableNodes.add(n);        } else {          LOG.info("No action for " + nodeStr);
protected synchronized void checkAppNumCompletedLimit(){  while (completedAppsInStateStore > this.maxCompletedAppsInStateStore) {    ApplicationId removeId=completedApps.get(completedApps.size() - completedAppsInStateStore);    RMApp removeApp=rmContext.getRMApps().get(removeId);
@Override public void handle(RMAppManagerEvent event){  ApplicationId applicationId=event.getApplicationId();
private void copyPlacementQueueToSubmissionContext(ApplicationPlacementContext placementContext,ApplicationSubmissionContext context){  if (placementContext != null && !StringUtils.equalsIgnoreCase(context.getQueue(),placementContext.getQueue())) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId,ContainerId containerId,Resource resource){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId,ContainerId containerId,Resource resource,String queueName,String partition){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,InetAddress ip,ArgsBuilder args){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId,CallerContext callerContext,String queueName,String partition){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId,ApplicationAttemptId attemptId){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId,CallerContext callerContext){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId,CallerContext callerContext,String queueName){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId,InetAddress ip){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target,ApplicationId appId){  if (LOG.isInfoEnabled()) {
public static void logSuccess(String user,String operation,String target){  if (LOG.isInfoEnabled()) {
protected ResourceScheduler createScheduler(){  String schedulerClassName=conf.get(YarnConfiguration.RM_SCHEDULER,YarnConfiguration.DEFAULT_RM_SCHEDULER);
protected SystemServiceManager createServiceManager(){  String schedulerClassName=YarnConfiguration.DEFAULT_YARN_API_SYSTEM_SERVICES_CLASS;
    builder.withAttribute(WebAppProxy.PROXY_HOST_ATTRIBUTE,proxyParts[0]);  }  WebAppContext uiWebAppContext=null;  if (getConfig().getBoolean(YarnConfiguration.YARN_WEBAPP_UI2_ENABLE,YarnConfiguration.DEFAULT_YARN_WEBAPP_UI2_ENABLE)) {    String onDiskPath=getConfig().get(YarnConfiguration.YARN_WEBAPP_UI2_WARFILE_PATH);    uiWebAppContext=new WebAppContext();    uiWebAppContext.setContextPath(UI2_WEBAPP_NAME);    if (null == onDiskPath) {      String war="hadoop-yarn-ui-" + VersionInfo.getVersion() + ".war";      URL url=getClass().getClassLoader().getResource(war);      if (null == url) {        onDiskPath=getWebAppsPath("ui2");      } else {        onDiskPath=url.getFile();      }    }    if (onDiskPath == null || onDiskPath.isEmpty()) {      LOG.error("No war file or webapps found for ui2 !");
  if (getConfig().getBoolean(YarnConfiguration.YARN_WEBAPP_UI2_ENABLE,YarnConfiguration.DEFAULT_YARN_WEBAPP_UI2_ENABLE)) {    String onDiskPath=getConfig().get(YarnConfiguration.YARN_WEBAPP_UI2_WARFILE_PATH);    uiWebAppContext=new WebAppContext();    uiWebAppContext.setContextPath(UI2_WEBAPP_NAME);    if (null == onDiskPath) {      String war="hadoop-yarn-ui-" + VersionInfo.getVersion() + ".war";      URL url=getClass().getClassLoader().getResource(war);      if (null == url) {        onDiskPath=getWebAppsPath("ui2");      } else {        onDiskPath=url.getFile();      }    }    if (onDiskPath == null || onDiskPath.isEmpty()) {      LOG.error("No war file or webapps found for ui2 !");    } else {      if (onDiskPath.endsWith(".war")) {
    if (argv.length >= 1) {      if (argv[0].equals("-format-state-store")) {        deleteRMStateStore(conf);      } else       if (argv[0].equals("-format-conf-store")) {        deleteRMConfStore(conf);      } else       if (argv[0].equals("-remove-application-from-state-store") && argv.length == 2) {        removeApplication(conf,argv[1]);      } else {        printUsage(System.err);      }    } else {      ResourceManager resourceManager=new ResourceManager();      ShutdownHookManager.get().addShutdownHook(new CompositeServiceShutdownHook(resourceManager),SHUTDOWN_HOOK_PRIORITY);      resourceManager.init(conf);      resourceManager.start();    }  } catch (  Throwable t) {
@VisibleForTesting static void removeApplication(Configuration conf,String applicationId) throws Exception {  RMStateStore rmStore=RMStateStoreFactory.getStore(conf);  rmStore.setResourceManager(new ResourceManager());  rmStore.init(conf);  rmStore.start();  try {    ApplicationId removeAppId=ApplicationId.fromString(applicationId);
@SuppressWarnings("unchecked") @VisibleForTesting void handleNMContainerStatus(NMContainerStatus containerStatus,NodeId nodeId){  ApplicationAttemptId appAttemptId=containerStatus.getContainerId().getApplicationAttemptId();  RMApp rmApp=rmContext.getRMApps().get(appAttemptId.getApplicationId());  if (rmApp == null) {
@SuppressWarnings("unchecked") @Override public RegisterNodeManagerResponse registerNodeManager(RegisterNodeManagerRequest request) throws YarnException, IOException {  NodeId nodeId=request.getNodeId();  String host=nodeId.getHost();  int cmPort=nodeId.getPort();  int httpPort=request.getHttpPort();  Resource capability=request.getResource();  String nodeManagerVersion=request.getNMVersion();  Resource physicalResource=request.getPhysicalResource();  NodeStatus nodeStatus=request.getNodeStatus();  RegisterNodeManagerResponse response=recordFactory.newRecordInstance(RegisterNodeManagerResponse.class);  if (!minimumNodeManagerVersion.equals("NONE")) {    if (minimumNodeManagerVersion.equals("EqualToRM")) {      minimumNodeManagerVersion=YarnVersionInfo.getVersion();    }    if ((nodeManagerVersion == null) || (VersionUtil.compareVersions(nodeManagerVersion,minimumNodeManagerVersion)) < 0) {      String message="Disallowed NodeManager Version " + nodeManagerVersion + ", is less than the minimum version "+ minimumNodeManagerVersion+ " sending SHUTDOWN signal to "+ "NodeManager.";
      String message="Disallowed NodeManager Version " + nodeManagerVersion + ", is less than the minimum version "+ minimumNodeManagerVersion+ " sending SHUTDOWN signal to "+ "NodeManager.";      LOG.info(message);      response.setDiagnosticsMessage(message);      response.setNodeAction(NodeAction.SHUTDOWN);      return response;    }  }  if (checkIpHostnameInRegistration) {    InetSocketAddress nmAddress=NetUtils.createSocketAddrForHost(host,cmPort);    InetAddress inetAddress=Server.getRemoteIp();    if (inetAddress != null && nmAddress.isUnresolved()) {      final String message="hostname cannot be resolved (ip=" + inetAddress.getHostAddress() + ", hostname="+ host+ ")";      LOG.warn("Unresolved nodemanager registration: " + message);      response.setDiagnosticsMessage(message);      response.setNodeAction(NodeAction.SHUTDOWN);      return response;    }  }  if (!this.nodesListManager.isValidNode(host) && !isNodeInDecommissioning(nodeId)) {
      LOG.warn("Unresolved nodemanager registration: " + message);      response.setDiagnosticsMessage(message);      response.setNodeAction(NodeAction.SHUTDOWN);      return response;    }  }  if (!this.nodesListManager.isValidNode(host) && !isNodeInDecommissioning(nodeId)) {    String message="Disallowed NodeManager from  " + host + ", Sending SHUTDOWN signal to the NodeManager.";    LOG.info(message);    response.setDiagnosticsMessage(message);    response.setNodeAction(NodeAction.SHUTDOWN);    return response;  }  String nid=nodeId.toString();  Resource dynamicLoadCapability=loadNodeResourceFromDRConfiguration(nid);  if (dynamicLoadCapability != null) {    LOG.debug("Resource for node: {} is adjusted from: {} to: {} due to" + " settings in dynamic-resources.xml.",nid,capability,dynamicLoadCapability);    capability=dynamicLoadCapability;
  }  String nid=nodeId.toString();  Resource dynamicLoadCapability=loadNodeResourceFromDRConfiguration(nid);  if (dynamicLoadCapability != null) {    LOG.debug("Resource for node: {} is adjusted from: {} to: {} due to" + " settings in dynamic-resources.xml.",nid,capability,dynamicLoadCapability);    capability=dynamicLoadCapability;    response.setResource(capability);  }  if (capability.getMemorySize() < minAllocMb || capability.getVirtualCores() < minAllocVcores) {    String message="NodeManager from  " + host + " doesn't satisfy minimum allocations, Sending SHUTDOWN"+ " signal to the NodeManager. Node capabilities are "+ capability+ "; minimums are "+ minAllocMb+ "mb and "+ minAllocVcores+ " vcores";    LOG.info(message);    response.setDiagnosticsMessage(message);    response.setNodeAction(NodeAction.SHUTDOWN);    return response;  }  response.setContainerTokenMasterKey(containerTokenSecretManager.getCurrentKey());  response.setNMTokenMasterKey(nmTokenSecretManager.getCurrentKey());  RMNode rmNode=new RMNodeImpl(nodeId,rmContext,host,cmPort,httpPort,resolve(host),capability,nodeManagerVersion,physicalResource);
    LOG.debug("Resource for node: {} is adjusted from: {} to: {} due to" + " settings in dynamic-resources.xml.",nid,capability,dynamicLoadCapability);    capability=dynamicLoadCapability;    response.setResource(capability);  }  if (capability.getMemorySize() < minAllocMb || capability.getVirtualCores() < minAllocVcores) {    String message="NodeManager from  " + host + " doesn't satisfy minimum allocations, Sending SHUTDOWN"+ " signal to the NodeManager. Node capabilities are "+ capability+ "; minimums are "+ minAllocMb+ "mb and "+ minAllocVcores+ " vcores";    LOG.info(message);    response.setDiagnosticsMessage(message);    response.setNodeAction(NodeAction.SHUTDOWN);    return response;  }  response.setContainerTokenMasterKey(containerTokenSecretManager.getCurrentKey());  response.setNMTokenMasterKey(nmTokenSecretManager.getCurrentKey());  RMNode rmNode=new RMNodeImpl(nodeId,rmContext,host,cmPort,httpPort,resolve(host),capability,nodeManagerVersion,physicalResource);  RMNode oldNode=this.rmContext.getRMNodes().putIfAbsent(nodeId,rmNode);  if (oldNode == null) {    RMNodeStartedEvent startEvent=new RMNodeStartedEvent(nodeId,request.getNMContainerStatuses(),request.getRunningApplications(),nodeStatus);
        LOG.debug("Found the number of previous cached log aggregation " + "status from nodemanager:" + nodeId + " is :"+ request.getLogAggregationReportsForApps().size());      }      startEvent.setLogAggregationReportsForApps(request.getLogAggregationReportsForApps());    }    this.rmContext.getDispatcher().getEventHandler().handle(startEvent);  } else {    LOG.info("Reconnect from the node at: " + host);    this.nmLivelinessMonitor.unregister(nodeId);    if (CollectionUtils.isEmpty(request.getRunningApplications()) && rmNode.getState() != NodeState.DECOMMISSIONING && rmNode.getHttpPort() != oldNode.getHttpPort()) {switch (rmNode.getState()) {case RUNNING:        ClusterMetrics.getMetrics().decrNumActiveNodes();      break;case UNHEALTHY:    ClusterMetrics.getMetrics().decrNumUnhealthyNMs();  break;default:LOG.debug("Unexpected Rmnode state");}this.rmContext.getDispatcher().getEventHandler().handle(new NodeRemovedSchedulerEvent(rmNode));this.rmContext.getRMNodes().put(nodeId,rmNode);
@SuppressWarnings("unchecked") @Override public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request) throws YarnException, IOException {  NodeStatus remoteNodeStatus=request.getNodeStatus();  NodeId nodeId=remoteNodeStatus.getNodeId();  if (!this.nodesListManager.isValidNode(nodeId.getHost()) && !isNodeInDecommissioning(nodeId)) {    String message="Disallowed NodeManager nodeId: " + nodeId + " hostname: "+ nodeId.getHost();
  NodeStatus remoteNodeStatus=request.getNodeStatus();  NodeId nodeId=remoteNodeStatus.getNodeId();  if (!this.nodesListManager.isValidNode(nodeId.getHost()) && !isNodeInDecommissioning(nodeId)) {    String message="Disallowed NodeManager nodeId: " + nodeId + " hostname: "+ nodeId.getHost();    LOG.info(message);    return YarnServerBuilderUtils.newNodeHeartbeatResponse(NodeAction.SHUTDOWN,message);  }  RMNode rmNode=this.rmContext.getRMNodes().get(nodeId);  if (rmNode == null) {    String message="Node not found resyncing " + remoteNodeStatus.getNodeId();    LOG.info(message);    return YarnServerBuilderUtils.newNodeHeartbeatResponse(NodeAction.RESYNC,message);  }  this.nmLivelinessMonitor.receivedPing(nodeId);  this.decommissioningWatcher.update(rmNode,remoteNodeStatus);  NodeHeartbeatResponse lastNodeHeartbeatResponse=rmNode.getLastNodeHeartBeatResponse();  if (getNextResponseId(remoteNodeStatus.getResponseId()) == lastNodeHeartbeatResponse.getResponseId()) {
    LOG.info(message);    return YarnServerBuilderUtils.newNodeHeartbeatResponse(NodeAction.SHUTDOWN,message);  }  RMNode rmNode=this.rmContext.getRMNodes().get(nodeId);  if (rmNode == null) {    String message="Node not found resyncing " + remoteNodeStatus.getNodeId();    LOG.info(message);    return YarnServerBuilderUtils.newNodeHeartbeatResponse(NodeAction.RESYNC,message);  }  this.nmLivelinessMonitor.receivedPing(nodeId);  this.decommissioningWatcher.update(rmNode,remoteNodeStatus);  NodeHeartbeatResponse lastNodeHeartbeatResponse=rmNode.getLastNodeHeartBeatResponse();  if (getNextResponseId(remoteNodeStatus.getResponseId()) == lastNodeHeartbeatResponse.getResponseId()) {    LOG.info("Received duplicate heartbeat from node " + rmNode.getNodeAddress() + " responseId="+ remoteNodeStatus.getResponseId());    return lastNodeHeartbeatResponse;  } else   if (remoteNodeStatus.getResponseId() != lastNodeHeartbeatResponse.getResponseId()) {    String message="Too far behind rm response id:" + lastNodeHeartbeatResponse.getResponseId() + " nm response id:"+ remoteNodeStatus.getResponseId();
    String message="Node not found resyncing " + remoteNodeStatus.getNodeId();    LOG.info(message);    return YarnServerBuilderUtils.newNodeHeartbeatResponse(NodeAction.RESYNC,message);  }  this.nmLivelinessMonitor.receivedPing(nodeId);  this.decommissioningWatcher.update(rmNode,remoteNodeStatus);  NodeHeartbeatResponse lastNodeHeartbeatResponse=rmNode.getLastNodeHeartBeatResponse();  if (getNextResponseId(remoteNodeStatus.getResponseId()) == lastNodeHeartbeatResponse.getResponseId()) {    LOG.info("Received duplicate heartbeat from node " + rmNode.getNodeAddress() + " responseId="+ remoteNodeStatus.getResponseId());    return lastNodeHeartbeatResponse;  } else   if (remoteNodeStatus.getResponseId() != lastNodeHeartbeatResponse.getResponseId()) {    String message="Too far behind rm response id:" + lastNodeHeartbeatResponse.getResponseId() + " nm response id:"+ remoteNodeStatus.getResponseId();    LOG.info(message);    this.rmContext.getDispatcher().getEventHandler().handle(new RMNodeEvent(nodeId,RMNodeEventType.REBOOTING));    return YarnServerBuilderUtils.newNodeHeartbeatResponse(NodeAction.RESYNC,message);  }  if (rmNode.getState() == NodeState.DECOMMISSIONING && decommissioningWatcher.checkReadyToBeDecommissioned(rmNode.getNodeID())) {
    Map<ApplicationId,RMApp> rmApps=rmContext.getRMApps();    for (    Map.Entry<ApplicationId,AppCollectorData> entry : registeringCollectorsMap.entrySet()) {      ApplicationId appId=entry.getKey();      AppCollectorData collectorData=entry.getValue();      if (collectorData != null) {        if (!collectorData.isStamped()) {          collectorData.setRMIdentifier(ResourceManager.getClusterTimeStamp());          collectorData.setVersion(timelineCollectorVersion.getAndIncrement());        }        RMApp rmApp=rmApps.get(appId);        if (rmApp == null) {          LOG.warn("Cannot update collector info because application ID: " + appId + " is not found in RMContext!");        } else {synchronized (rmApp) {            AppCollectorData previousCollectorData=rmApp.getCollectorData();            if (AppCollectorData.happensBefore(previousCollectorData,collectorData)) {
@SuppressWarnings("unchecked") @Override public UnRegisterNodeManagerResponse unRegisterNodeManager(UnRegisterNodeManagerRequest request) throws YarnException, IOException {  UnRegisterNodeManagerResponse response=recordFactory.newRecordInstance(UnRegisterNodeManagerResponse.class);  NodeId nodeId=request.getNodeId();  RMNode rmNode=this.rmContext.getRMNodes().get(nodeId);  if (rmNode == null) {
private void updateNodeLabelsFromNMReport(Set<String> nodeLabels,NodeId nodeId) throws IOException {  try {    Map<NodeId,Set<String>> labelsUpdate=new HashMap<NodeId,Set<String>>();    labelsUpdate.put(nodeId,nodeLabels);    this.rmContext.getNodeLabelManager().replaceLabelsOnNode(labelsUpdate);    if (LOG.isDebugEnabled()) {
private void populateTokenSequenceNo(NodeHeartbeatRequest request,NodeHeartbeatResponse nodeHeartBeatResponse){  if (LOG.isDebugEnabled()) {
protected void handleWritingApplicationHistoryEvent(WritingApplicationHistoryEvent event){switch (event.getType()) {case APP_START:    WritingApplicationStartEvent wasEvent=(WritingApplicationStartEvent)event;  try {    writer.applicationStarted(wasEvent.getApplicationStartData());
    writer.applicationStarted(wasEvent.getApplicationStartData());    LOG.info("Stored the start data of application " + wasEvent.getApplicationId());  } catch (  IOException e) {    LOG.error("Error when storing the start data of application " + wasEvent.getApplicationId());  }break;case APP_FINISH:WritingApplicationFinishEvent wafEvent=(WritingApplicationFinishEvent)event;try {writer.applicationFinished(wafEvent.getApplicationFinishData());LOG.info("Stored the finish data of application " + wafEvent.getApplicationId());} catch (IOException e) {LOG.error("Error when storing the finish data of application " + wafEvent.getApplicationId());}break;case APP_ATTEMPT_START:WritingApplicationAttemptStartEvent waasEvent=(WritingApplicationAttemptStartEvent)event;try {writer.applicationAttemptStarted(waasEvent.getApplicationAttemptStartData());
  } catch (  IOException e) {    LOG.error("Error when storing the start data of application " + wasEvent.getApplicationId());  }break;case APP_FINISH:WritingApplicationFinishEvent wafEvent=(WritingApplicationFinishEvent)event;try {writer.applicationFinished(wafEvent.getApplicationFinishData());LOG.info("Stored the finish data of application " + wafEvent.getApplicationId());} catch (IOException e) {LOG.error("Error when storing the finish data of application " + wafEvent.getApplicationId());}break;case APP_ATTEMPT_START:WritingApplicationAttemptStartEvent waasEvent=(WritingApplicationAttemptStartEvent)event;try {writer.applicationAttemptStarted(waasEvent.getApplicationAttemptStartData());LOG.info("Stored the start data of application attempt " + waasEvent.getApplicationAttemptId());} catch (IOException e) {
case APP_FINISH:WritingApplicationFinishEvent wafEvent=(WritingApplicationFinishEvent)event;try {writer.applicationFinished(wafEvent.getApplicationFinishData());LOG.info("Stored the finish data of application " + wafEvent.getApplicationId());} catch (IOException e) {LOG.error("Error when storing the finish data of application " + wafEvent.getApplicationId());}break;case APP_ATTEMPT_START:WritingApplicationAttemptStartEvent waasEvent=(WritingApplicationAttemptStartEvent)event;try {writer.applicationAttemptStarted(waasEvent.getApplicationAttemptStartData());LOG.info("Stored the start data of application attempt " + waasEvent.getApplicationAttemptId());} catch (IOException e) {LOG.error("Error when storing the start data of application attempt " + waasEvent.getApplicationAttemptId());}break;case APP_ATTEMPT_FINISH:WritingApplicationAttemptFinishEvent waafEvent=(WritingApplicationAttemptFinishEvent)event;
writer.applicationFinished(wafEvent.getApplicationFinishData());LOG.info("Stored the finish data of application " + wafEvent.getApplicationId());} catch (IOException e) {LOG.error("Error when storing the finish data of application " + wafEvent.getApplicationId());}break;case APP_ATTEMPT_START:WritingApplicationAttemptStartEvent waasEvent=(WritingApplicationAttemptStartEvent)event;try {writer.applicationAttemptStarted(waasEvent.getApplicationAttemptStartData());LOG.info("Stored the start data of application attempt " + waasEvent.getApplicationAttemptId());} catch (IOException e) {LOG.error("Error when storing the start data of application attempt " + waasEvent.getApplicationAttemptId());}break;case APP_ATTEMPT_FINISH:WritingApplicationAttemptFinishEvent waafEvent=(WritingApplicationAttemptFinishEvent)event;try {writer.applicationAttemptFinished(waafEvent.getApplicationAttemptFinishData());
case APP_ATTEMPT_START:WritingApplicationAttemptStartEvent waasEvent=(WritingApplicationAttemptStartEvent)event;try {writer.applicationAttemptStarted(waasEvent.getApplicationAttemptStartData());LOG.info("Stored the start data of application attempt " + waasEvent.getApplicationAttemptId());} catch (IOException e) {LOG.error("Error when storing the start data of application attempt " + waasEvent.getApplicationAttemptId());}break;case APP_ATTEMPT_FINISH:WritingApplicationAttemptFinishEvent waafEvent=(WritingApplicationAttemptFinishEvent)event;try {writer.applicationAttemptFinished(waafEvent.getApplicationAttemptFinishData());LOG.info("Stored the finish data of application attempt " + waafEvent.getApplicationAttemptId());} catch (IOException e) {LOG.error("Error when storing the finish data of application attempt " + waafEvent.getApplicationAttemptId());}break;case CONTAINER_START:WritingContainerStartEvent wcsEvent=(WritingContainerStartEvent)event;
private void launch() throws IOException, YarnException {  connect();  ContainerId masterContainerID=masterContainer.getId();  ApplicationSubmissionContext applicationContext=application.getSubmissionContext();
private void launch() throws IOException, YarnException {  connect();  ContainerId masterContainerID=masterContainer.getId();  ApplicationSubmissionContext applicationContext=application.getSubmissionContext();  LOG.info("Setting up container " + masterContainer + " for AM "+ application.getAppAttemptId());  ContainerLaunchContext launchContext=createAMContainerLaunchContext(applicationContext,masterContainerID);  StartContainerRequest scRequest=StartContainerRequest.newInstance(launchContext,masterContainer.getContainerToken());  List<StartContainerRequest> list=new ArrayList<StartContainerRequest>();  list.add(scRequest);  StartContainersRequest allRequests=StartContainersRequest.newInstance(list);  StartContainersResponse response=containerMgrProxy.startContainers(allRequests);  if (response.getFailedRequests() != null && response.getFailedRequests().containsKey(masterContainerID)) {    Throwable t=response.getFailedRequests().get(masterContainerID).deSerialize();    parseAndThrowException(t);  } else {
      LOG.info("Launching master" + application.getAppAttemptId());      launch();      handler.handle(new RMAppAttemptEvent(application.getAppAttemptId(),RMAppAttemptEventType.LAUNCHED,System.currentTimeMillis()));    } catch (    Exception ie) {      onAMLaunchFailed(masterContainer.getId(),ie);    }  break;case CLEANUP:try {  LOG.info("Cleaning master " + application.getAppAttemptId());  cleanup();} catch (IOException ie) {  LOG.info("Error cleaning master ",ie);}catch (YarnException e) {  StringBuilder sb=new StringBuilder("Container ");  sb.append(masterContainer.getId().toString()).append(" is not handled by this NodeManager");  if (!e.getMessage().contains(sb.toString())) {
@SuppressWarnings("unchecked") protected void onAMLaunchFailed(ContainerId containerId,Exception ie){  String message="Error launching " + application.getAppAttemptId() + ". Got exception: "+ StringUtils.stringifyException(ie);
@Override public ResourceBlacklistRequest getBlacklistUpdates(){  ResourceBlacklistRequest ret;  List<String> blacklist=new ArrayList<>(blacklistNodes);  final int currentBlacklistSize=blacklist.size();  final double failureThreshold=this.blacklistDisableFailureThreshold * numberOfNodeManagerHosts;  if (currentBlacklistSize < failureThreshold) {
@Override public synchronized void run(){  try {    updateClusterState();    SubClusterHeartbeatRequest request=SubClusterHeartbeatRequest.newInstance(subClusterId,SubClusterState.SC_RUNNING,capability);    stateStoreService.subClusterHeartbeat(request);
private void registerAndInitializeHeartbeat(){  String clientRMAddress=getServiceAddress(rmContext.getClientRMService().getBindAddress());  String amRMAddress=getServiceAddress(rmContext.getApplicationMasterService().getBindAddress());  String rmAdminAddress=getServiceAddress(config.getSocketAddr(YarnConfiguration.RM_ADMIN_ADDRESS,YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,YarnConfiguration.DEFAULT_RM_ADMIN_PORT));  String webAppAddress=getServiceAddress(NetUtils.createSocketAddr(WebAppUtils.getRMWebAppURLWithScheme(config)));  SubClusterInfo subClusterInfo=SubClusterInfo.newInstance(subClusterId,amRMAddress,clientRMAddress,rmAdminAddress,webAppAddress,SubClusterState.SC_NEW,ResourceManager.getClusterTimeStamp(),"");  try {    registerSubCluster(SubClusterRegisterRequest.newInstance(subClusterInfo));
private void putEntity(TimelineEntity entity){  try {    if (LOG.isDebugEnabled()) {
private void putEntity(TimelineEntity entity,ApplicationId appId){  try {    if (LOG.isDebugEnabled()) {
private void putEntity(TimelineEntity entity,ApplicationId appId){  try {    if (LOG.isDebugEnabled()) {      LOG.debug("Publishing the entity " + entity + ", JSON-style content: "+ TimelineUtils.dumpTimelineRecordtoJSON(entity));    }    TimelineCollector timelineCollector=rmTimelineCollectorManager.get(appId);    if (timelineCollector != null) {      TimelineEntities entities=new TimelineEntities();      entities.addEntity(entity);      timelineCollector.putEntities(entities,UserGroupInformation.getCurrentUser());    } else {      LOG.debug("Cannot find active collector while publishing entity " + entity);    }  } catch (  IOException e) {    LOG.error("Error when publishing entity " + entity);    LOG.debug("Error when publishing entity {}",entity,e);  }catch (  Exception e) {
private void silentlyStopSchedulingMonitor(String name){  SchedulingMonitor mon=runningSchedulingMonitors.get(name);  try {    mon.stop();
@Override public Map<ApplicationAttemptId,Set<RMContainer>> selectCandidates(Map<ApplicationAttemptId,Set<RMContainer>> selectedCandidates,Resource clusterResource,Resource totalPreemptionAllowed){  Map<ApplicationAttemptId,Set<RMContainer>> curCandidates=new HashMap<>();  preemptableAmountCalculator.computeIdealAllocation(clusterResource,totalPreemptionAllowed);  CapacitySchedulerPreemptionUtils.deductPreemptableResourcesBasedSelectedCandidates(preemptionContext,selectedCandidates);  List<RMContainer> skippedAMContainerlist=new ArrayList<>();  for (  String queueName : preemptionContext.getLeafQueueNames()) {    if (preemptionContext.getQueueByPartition(queueName,RMNodeLabelsManager.NO_LABEL).preemptionDisabled) {
  Collection<FiCaSchedulerApp> apps=tq.leafQueue.getAllApplications();  if (apps.size() == 1) {    return;  }  PriorityQueue<TempAppPerPartition> orderedByPriority=createTempAppForResCalculation(tq,apps,clusterResource,perUserAMUsed);  TreeSet<TempAppPerPartition> orderedApps=calculateIdealAssignedResourcePerApp(clusterResource,tq,selectedCandidates,queueReassignableResource,orderedByPriority);  Resource maxIntraQueuePreemptable=Resources.multiply(tq.getGuaranteed(),maxAllowablePreemptLimit);  if (Resources.greaterThan(rc,clusterResource,maxIntraQueuePreemptable,tq.getActuallyToBePreempted())) {    Resources.subtractFrom(maxIntraQueuePreemptable,tq.getActuallyToBePreempted());  } else {    maxIntraQueuePreemptable=Resource.newInstance(0,0);  }  Resource preemptionLimit=Resources.min(rc,clusterResource,maxIntraQueuePreemptable,totalPreemptedResourceAllowed);  calculateToBePreemptedResourcePerApp(clusterResource,orderedApps,Resources.clone(preemptionLimit));  tq.addAllApps(orderedApps);  validateOutSameAppPriorityFromDemand(clusterResource,(TreeSet<TempAppPerPartition>)orderedApps,tq.getUsersPerPartition(),context.getIntraQueuePreemptionOrderPolicy());  if (LOG.isDebugEnabled()) {
    return;  }  PriorityQueue<TempAppPerPartition> orderedByPriority=createTempAppForResCalculation(tq,apps,clusterResource,perUserAMUsed);  TreeSet<TempAppPerPartition> orderedApps=calculateIdealAssignedResourcePerApp(clusterResource,tq,selectedCandidates,queueReassignableResource,orderedByPriority);  Resource maxIntraQueuePreemptable=Resources.multiply(tq.getGuaranteed(),maxAllowablePreemptLimit);  if (Resources.greaterThan(rc,clusterResource,maxIntraQueuePreemptable,tq.getActuallyToBePreempted())) {    Resources.subtractFrom(maxIntraQueuePreemptable,tq.getActuallyToBePreempted());  } else {    maxIntraQueuePreemptable=Resource.newInstance(0,0);  }  Resource preemptionLimit=Resources.min(rc,clusterResource,maxIntraQueuePreemptable,totalPreemptedResourceAllowed);  calculateToBePreemptedResourcePerApp(clusterResource,orderedApps,Resources.clone(preemptionLimit));  tq.addAllApps(orderedApps);  validateOutSameAppPriorityFromDemand(clusterResource,(TreeSet<TempAppPerPartition>)orderedApps,tq.getUsersPerPartition(),context.getIntraQueuePreemptionOrderPolicy());  if (LOG.isDebugEnabled()) {    LOG.debug("Queue Name:" + tq.queueName + ", partition:"+ tq.partition);    for (    TempAppPerPartition tmpApp : tq.getApps()) {
    used=(used == null) ? Resources.createResource(0,0) : used;    amUsed=(amUsed == null) ? Resources.createResource(0,0) : amUsed;    pending=(pending == null) ? Resources.createResource(0,0) : pending;    reserved=(reserved == null) ? Resources.createResource(0,0) : reserved;    HashSet<String> partitions=new HashSet<String>(app.getAppAttemptResourceUsage().getNodePartitionsSet());    partitions.addAll(app.getTotalPendingRequestsPerPartition().keySet());    TempAppPerPartition tmpApp=new TempAppPerPartition(app,Resources.clone(used),Resources.clone(amUsed),Resources.clone(reserved),Resources.clone(pending));    tmpApp.idealAssigned=Resources.createResource(0,0);    String userName=app.getUser();    TempUserPerPartition tmpUser=usersPerPartition.get(userName);    if (tmpUser == null) {      ResourceUsage userResourceUsage=tq.leafQueue.getUser(userName).getResourceUsage();      Resource userSpecificAmUsed=perUserAMUsed.get(userName);      amUsed=(userSpecificAmUsed == null) ? Resources.none() : userSpecificAmUsed;      tmpUser=new TempUserPerPartition(tq.leafQueue.getUser(userName),tq.queueName,Resources.clone(userResourceUsage.getUsed(partition)),Resources.clone(amUsed),Resources.clone(userResourceUsage.getReserved(partition)),Resources.none());
private void initializeUsageAndUserLimitForCompute(Resource clusterResource,String partition,LeafQueue leafQueue,Map<String,Resource> rollingResourceUsagePerUser){  for (  String user : leafQueue.getAllUsers()) {    rollingResourceUsagePerUser.put(user,Resources.clone(leafQueue.getUser(user).getResourceUsage().getUsed(partition)));
private void preemptFromLeastStarvedApp(LeafQueue leafQueue,FiCaSchedulerApp app,Map<ApplicationAttemptId,Set<RMContainer>> selectedCandidates,Map<ApplicationAttemptId,Set<RMContainer>> curCandidates,Resource clusterResource,Resource totalPreemptedResourceAllowed,Map<String,Resource> resToObtainByPartition,Map<String,Resource> rollingResourceUsagePerUser){  List<RMContainer> liveContainers=new ArrayList<>(app.getLiveContainers());  sortContainers(liveContainers);
private void preemptFromLeastStarvedApp(LeafQueue leafQueue,FiCaSchedulerApp app,Map<ApplicationAttemptId,Set<RMContainer>> selectedCandidates,Map<ApplicationAttemptId,Set<RMContainer>> curCandidates,Resource clusterResource,Resource totalPreemptedResourceAllowed,Map<String,Resource> resToObtainByPartition,Map<String,Resource> rollingResourceUsagePerUser){  List<RMContainer> liveContainers=new ArrayList<>(app.getLiveContainers());  sortContainers(liveContainers);  LOG.debug("totalPreemptedResourceAllowed for preemption at this" + " round is :{}",totalPreemptedResourceAllowed);  Resource rollingUsedResourcePerUser=rollingResourceUsagePerUser.get(app.getUser());  for (  RMContainer c : liveContainers) {    if (resToObtainByPartition.isEmpty()) {      return;    }    if (CapacitySchedulerPreemptionUtils.isContainerAlreadySelected(c,selectedCandidates)) {      continue;    }    if (null != preemptionContext.getKillableContainers() && preemptionContext.getKillableContainers().contains(c.getContainerId())) {      continue;    }    if (c.isAMContainer()) {      continue;    }    if (fifoPreemptionComputePlugin.skipContainerBasedOnIntraQueuePolicy(app,clusterResource,rollingUsedResourcePerUser,c)) {
private void calculateResToObtainByPartitionForLeafQueues(Set<String> leafQueueNames,Resource clusterResource){  for (  String queueName : leafQueueNames) {    if (context.getQueueByPartition(queueName,RMNodeLabelsManager.NO_LABEL).preemptionDisabled) {
    candidatesSelectionPolicies.add(new QueuePriorityContainerCandidateSelector(this));  }  boolean selectCandidatesForResevedContainers=config.getBoolean(CapacitySchedulerConfiguration.PREEMPTION_SELECT_CANDIDATES_FOR_RESERVED_CONTAINERS,CapacitySchedulerConfiguration.DEFAULT_PREEMPTION_SELECT_CANDIDATES_FOR_RESERVED_CONTAINERS);  if (selectCandidatesForResevedContainers) {    candidatesSelectionPolicies.add(new ReservedContainerCandidatesSelector(this));  }  boolean additionalPreemptionBasedOnReservedResource=config.getBoolean(CapacitySchedulerConfiguration.ADDITIONAL_RESOURCE_BALANCE_BASED_ON_RESERVED_CONTAINERS,CapacitySchedulerConfiguration.DEFAULT_ADDITIONAL_RESOURCE_BALANCE_BASED_ON_RESERVED_CONTAINERS);  candidatesSelectionPolicies.add(new FifoCandidatesSelector(this,additionalPreemptionBasedOnReservedResource,false));  boolean isPreemptionToBalanceRequired=config.getBoolean(CapacitySchedulerConfiguration.PREEMPTION_TO_BALANCE_QUEUES_BEYOND_GUARANTEED,CapacitySchedulerConfiguration.DEFAULT_PREEMPTION_TO_BALANCE_QUEUES_BEYOND_GUARANTEED);  long maximumKillWaitTimeForPreemptionToQueueBalance=config.getLong(CapacitySchedulerConfiguration.MAX_WAIT_BEFORE_KILL_FOR_QUEUE_BALANCE_PREEMPTION,CapacitySchedulerConfiguration.DEFAULT_MAX_WAIT_BEFORE_KILL_FOR_QUEUE_BALANCE_PREEMPTION);  if (isPreemptionToBalanceRequired) {    PreemptionCandidatesSelector selector=new FifoCandidatesSelector(this,false,true);    selector.setMaximumKillWaitTime(maximumKillWaitTimeForPreemptionToQueueBalance);    candidatesSelectionPolicies.add(selector);  }  boolean isIntraQueuePreemptionEnabled=config.getBoolean(CapacitySchedulerConfiguration.INTRAQUEUE_PREEMPTION_ENABLED,CapacitySchedulerConfiguration.DEFAULT_INTRAQUEUE_PREEMPTION_ENABLED);  if (isIntraQueuePreemptionEnabled) {    candidatesSelectionPolicies.add(new IntraQueueCandidatesSelector(this));
    for (    String partitionToLookAt : allPartitions) {      cloneQueues(root,Resources.clone(nlm.getResourceByLabel(partitionToLookAt,clusterResources)),partitionToLookAt);    }  }  this.leafQueueNames=ImmutableSet.copyOf(getLeafQueueNames(getQueueByPartition(CapacitySchedulerConfiguration.ROOT,RMNodeLabelsManager.NO_LABEL)));  Resource totalPreemptionAllowed=Resources.multiply(clusterResources,percentageClusterPreemptionAllowed);  partitionToUnderServedQueues.clear();  Map<ApplicationAttemptId,Set<RMContainer>> toPreempt=new HashMap<>();  Map<PreemptionCandidatesSelector,Map<ApplicationAttemptId,Set<RMContainer>>> toPreemptPerSelector=new HashMap<>();  ;  for (  PreemptionCandidatesSelector selector : candidatesSelectionPolicies) {    long startTime=0;    if (LOG.isDebugEnabled()) {      LOG.debug(MessageFormat.format("Trying to use {0} to select preemption candidates",selector.getClass().getName()));      startTime=clock.getTime();    }    Map<ApplicationAttemptId,Set<RMContainer>> curCandidates=selector.selectCandidates(toPreempt,clusterResources,totalPreemptionAllowed);    toPreemptPerSelector.putIfAbsent(selector,curCandidates);
  Map<PreemptionCandidatesSelector,Map<ApplicationAttemptId,Set<RMContainer>>> toPreemptPerSelector=new HashMap<>();  ;  for (  PreemptionCandidatesSelector selector : candidatesSelectionPolicies) {    long startTime=0;    if (LOG.isDebugEnabled()) {      LOG.debug(MessageFormat.format("Trying to use {0} to select preemption candidates",selector.getClass().getName()));      startTime=clock.getTime();    }    Map<ApplicationAttemptId,Set<RMContainer>> curCandidates=selector.selectCandidates(toPreempt,clusterResources,totalPreemptionAllowed);    toPreemptPerSelector.putIfAbsent(selector,curCandidates);    if (LOG.isDebugEnabled()) {      LOG.debug(MessageFormat.format("{0} uses {1} millisecond to run",selector.getClass().getName(),clock.getTime() - startTime));      int totalSelected=0;      int curSelected=0;      for (      Set<RMContainer> set : toPreempt.values()) {        totalSelected+=set.size();
    for (    String q2 : preemptionContext.getLeafQueueNames()) {      if (q1.compareTo(q2) < 0) {        TempQueuePerPartition tq1=preemptionContext.getQueueByPartition(q1,RMNodeLabelsManager.NO_LABEL);        TempQueuePerPartition tq2=preemptionContext.getQueueByPartition(q2,RMNodeLabelsManager.NO_LABEL);        List<TempQueuePerPartition> path1=getPathToRoot(tq1);        List<TempQueuePerPartition> path2=getPathToRoot(tq2);        int i=path1.size() - 1;        int j=path2.size() - 1;        while (path1.get(i).queueName.equals(path2.get(j).queueName)) {          i--;          j--;        }        int p1=path1.get(i).relativePriority;        int p2=path2.get(j).relativePriority;        if (p1 < p2) {          priorityDigraph.put(q2,q1,true);
        TempQueuePerPartition tq2=preemptionContext.getQueueByPartition(q2,RMNodeLabelsManager.NO_LABEL);        List<TempQueuePerPartition> path1=getPathToRoot(tq1);        List<TempQueuePerPartition> path2=getPathToRoot(tq2);        int i=path1.size() - 1;        int j=path2.size() - 1;        while (path1.get(i).queueName.equals(path2.get(j).queueName)) {          i--;          j--;        }        int p1=path1.get(i).relativePriority;        int p2=path2.get(j).relativePriority;        if (p1 < p2) {          priorityDigraph.put(q2,q1,true);          LOG.debug("- Added priority ordering edge: {} >> {}",q2,q1);        } else         if (p2 < p1) {          priorityDigraph.put(q1,q2,true);
      }    }  }  Collections.sort(reservedContainers,CONTAINER_CREATION_TIME_COMPARATOR);  long currentTime=System.currentTimeMillis();  for (  RMContainer reservedContainer : reservedContainers) {    if (currentTime - reservedContainer.getCreationTime() < minTimeout) {      continue;    }    FiCaSchedulerNode node=preemptionContext.getScheduler().getNode(reservedContainer.getReservedNode());    if (null == node) {      continue;    }    List<RMContainer> newlySelectedToBePreemptContainers=new ArrayList<>();    String demandingQueueName=reservedContainer.getQueueName();    boolean demandingQueueSatisfied=isQueueSatisfied(demandingQueueName,node.getPartition());    boolean canPreempt=false;    if (!demandingQueueSatisfied) {      canPreempt=canPreemptEnoughResourceForAsked(reservedContainer.getReservedResource(),demandingQueueName,node,false,newlySelectedToBePreemptContainers);    }    if (canPreempt) {
  Collections.sort(sortedRunningContainers,new Comparator<RMContainer>(){    @Override public int compare(    RMContainer o1,    RMContainer o2){      return -1 * o1.getContainerId().compareTo(o2.getContainerId());    }  });  boolean canAllocateReservedContainer=false;  Resource cur=Resources.add(available,node.getTotalKillableResources());  String partition=node.getPartition();  if (Resources.fitsIn(rc,reservedContainer.getReservedResource(),cur)) {    return null;  }  float amPreemptionCost=0f;  for (  RMContainer c : sortedRunningContainers) {    String containerQueueName=c.getQueueName();    if (killableContainers.containsKey(c.getContainerId())) {      continue;    }    if (c.isAMContainer()) {
@Override public void init(Configuration config,RMContext rmContext,ResourceScheduler scheduler){  this.conf=config;  this.context=rmContext;  this.scheduler=scheduler;  this.throwOnInvariantViolation=conf.getBoolean(InvariantsChecker.THROW_ON_VIOLATION,false);  this.monitoringInterval=conf.getLong(InvariantsChecker.INVARIANT_MONITOR_INTERVAL,1000L);
break;case REPLACE:clusterAttributes.putAll(newAttributesToBeAdded);replaceNodeToAttribute(nodeHost,attributePrefix,node.getAttributes(),attributes);node.replaceAttributes(attributes,attributePrefix);break;default:break;}logMsg.append(" NM = ").append(entry.getKey()).append(", attributes=[ ").append(StringUtils.join(entry.getValue().keySet(),",")).append("] ,");}LOG.debug("{}",logMsg);if (null != dispatcher && NodeAttribute.PREFIX_CENTRALIZED.equals(attributePrefix)) {dispatcher.getEventHandler().handle(new NodeAttributesStoreEvent(nodeAttributeMapping,op));}Map<String,Set<NodeAttribute>> newNodeToAttributesMap=new HashMap<String,Set<NodeAttribute>>();nodeAttributeMapping.forEach((k,v) -> {Host node=nodeCollections.get(k);newNodeToAttributesMap.put(k,node.attributes.keySet());});
public static void verifyCentralizedNodeLabelConfEnabled(String operation,boolean isCentralizedNodeLabelConfiguration) throws IOException {  if (!isCentralizedNodeLabelConfiguration) {    String msg=String.format("Error when invoke method=%s because " + "centralized node label configuration is not enabled.",operation);
        }        QueueMapping newMapping=validateAndGetAutoCreatedQueueMapping(queueManager,mapping);        if (newMapping == null) {          throw new IOException("mapping contains invalid or non-leaf queue " + mapping.getQueue());        }        newMappings.add(newMapping);      } else {        QueueMapping newMapping=validateAndGetQueueMapping(queueManager,queue,mapping);        newMappings.add(newMapping);      }    } else {      QueueMapping newMapping=validateAndGetAutoCreatedQueueMapping(queueManager,mapping);      if (newMapping != null) {        newMappings.add(newMapping);      } else {        newMappings.add(mapping);      }    }  }  if (newMappings.size() > 0) {    this.mappings=newMappings;
@Override public ApplicationPlacementContext getPlacementForApp(ApplicationSubmissionContext asc,String user) throws YarnException {  String queueName=asc.getQueue();  String applicationName=asc.getApplicationName();  if (mappings != null && mappings.size() > 0) {    try {      ApplicationPlacementContext mappedQueue=getAppPlacementContext(user,applicationName);      if (mappedQueue != null) {        if (queueName.equals(YarnConfiguration.DEFAULT_QUEUE_NAME) || queueName.equals(mappedQueue.getQueue()) || overrideWithQueueMappings) {
  if (!(scheduler instanceof CapacityScheduler)) {    throw new IOException("CSMappingPlacementRule can be only used with CapacityScheduler");  }  LOG.info("Initializing {} queue mapping manager.",getClass().getSimpleName());  CapacitySchedulerContext csContext=(CapacitySchedulerContext)scheduler;  queueManager=csContext.getCapacitySchedulerQueueManager();  CapacitySchedulerConfiguration conf=csContext.getConfiguration();  overrideWithQueueMappings=conf.getOverrideWithQueueMappings();  if (groups == null) {    groups=Groups.getUserToGroupsMappingService(conf);  }  MappingRuleValidationContext validationContext=buildValidationContext();  mappingRules=conf.getMappingRules();  for (  MappingRule rule : mappingRules) {    try {      rule.validate(validationContext);    } catch (    YarnException e) {
  CapacitySchedulerContext csContext=(CapacitySchedulerContext)scheduler;  queueManager=csContext.getCapacitySchedulerQueueManager();  CapacitySchedulerConfiguration conf=csContext.getConfiguration();  overrideWithQueueMappings=conf.getOverrideWithQueueMappings();  if (groups == null) {    groups=Groups.getUserToGroupsMappingService(conf);  }  MappingRuleValidationContext validationContext=buildValidationContext();  mappingRules=conf.getMappingRules();  for (  MappingRule rule : mappingRules) {    try {      rule.validate(validationContext);    } catch (    YarnException e) {      LOG.error("Error initializing queue mappings, rule '{}' " + "has encountered a validation error: {}",rule,e.getMessage());      if (failOnConfigError) {        throw new IOException(e);
@Override public ApplicationPlacementContext getPlacementForApp(ApplicationSubmissionContext asc,String user) throws YarnException {  String appQueue=asc.getQueue();  if (appQueue != null && !appQueue.equals(YarnConfiguration.DEFAULT_QUEUE_NAME) && !appQueue.equals(YarnConfiguration.DEFAULT_QUEUE_FULL_NAME) && !overrideWithQueueMappings) {
  VariableContext variables;  try {    variables=createVariableContext(asc,user);  } catch (  IOException e) {    LOG.error("Unable to setup variable context",e);    throw new YarnException(e);  }  for (  MappingRule rule : mappingRules) {    MappingRuleResult result=evaluateRule(rule,variables);switch (result.getResult()) {case PLACE_TO_DEFAULT:      return placeToDefault(asc,variables,rule);case PLACE:    return placeToQueue(asc,rule,result);case REJECT:  LOG.info("Rejecting application '{}', reason: Mapping rule '{}' " + " fallback action is set to REJECT.",asc.getApplicationName(),rule);throw new YarnException("Application submission have been rejected by" + " a mapping rule. Please see the logs for details");case SKIP:break;default:LOG.error("Invalid result '{}'",result);
private ApplicationPlacementContext placeToQueue(ApplicationSubmissionContext asc,MappingRule rule,MappingRuleResult result){
private ApplicationPlacementContext placeToDefault(ApplicationSubmissionContext asc,VariableContext variables,MappingRule rule) throws YarnException {  try {    String queueName=validateAndNormalizeQueue(variables.replacePathVariables("%default"));
@Override public void setConfig(Element conf){  createQueue=getCreateFlag(conf);  if (conf != null) {    defaultQueueName=conf.getAttribute("queue");    if (!isValidQueueName(defaultQueueName)) {
@Override public void setConfig(Boolean create){  createQueue=create;  defaultQueueName=assureRoot(YarnConfiguration.DEFAULT_QUEUE_NAME);
public static PlacementRule getPlacementRule(String ruleStr,Configuration conf) throws ClassNotFoundException {  Class<? extends PlacementRule> ruleClass=Class.forName(ruleStr).asSubclass(PlacementRule.class);
public static PlacementRule getPlacementRule(Class<? extends PlacementRule> ruleClass,Object initArg){
@Override public ApplicationPlacementContext getPlacementForApp(ApplicationSubmissionContext asc,String user) throws YarnException {  final Set<String> groupSet;  try {    groupSet=groupProvider.getGroupsSet(user);  } catch (  IOException ioe) {    throw new YarnException("Group resolution failed",ioe);  }  String parentQueue=null;  PlacementRule parentRule=getParentRule();  if (parentRule != null) {    LOG.debug("SecondaryGroupExisting rule: parent rule found: {}",parentRule.getName());    ApplicationPlacementContext parent=parentRule.getPlacementForApp(asc,user);    if (parent == null || getQueueManager().getQueue(parent.getQueue()) instanceof FSLeafQueue) {      LOG.debug("SecondaryGroupExisting rule: parent rule failed");      return null;    }    parentQueue=parent.getQueue();
@Override public ApplicationPlacementContext getPlacementForApp(ApplicationSubmissionContext asc,String user) throws YarnException {  String queueName=asc.getQueue();  if (!isValidQueueName(queueName)) {
private ApplicationPlacementContext getPlacementForUser(String user) throws IOException {  for (  QueueMapping mapping : mappings) {    if (mapping.getType().equals(MappingType.USER)) {      if (mapping.getSource().equals(CURRENT_USER_MAPPING)) {        if (mapping.getParentQueue() != null && mapping.getParentQueue().equals(PRIMARY_GROUP_MAPPING) && mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {
      if (mapping.getSource().equals(CURRENT_USER_MAPPING)) {        if (mapping.getParentQueue() != null && mapping.getParentQueue().equals(PRIMARY_GROUP_MAPPING) && mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "primary group current user mapping",user);          }          return getContextForGroupParent(user,mapping,getPrimaryGroup(user));        } else         if (mapping.getParentQueue() != null && mapping.getParentQueue().equals(SECONDARY_GROUP_MAPPING) && mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "secondary group current user mapping",user);          }          return getContextForGroupParent(user,mapping,getSecondaryGroup(user));        } else         if (mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user mapping",user);          }          return getPlacementContext(mapping,user);        } else         if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {
          }          return getContextForGroupParent(user,mapping,getPrimaryGroup(user));        } else         if (mapping.getParentQueue() != null && mapping.getParentQueue().equals(SECONDARY_GROUP_MAPPING) && mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "secondary group current user mapping",user);          }          return getContextForGroupParent(user,mapping,getSecondaryGroup(user));        } else         if (mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user mapping",user);          }          return getPlacementContext(mapping,user);        } else         if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {
          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "secondary group current user mapping",user);          }          return getContextForGroupParent(user,mapping,getSecondaryGroup(user));        } else         if (mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user mapping",user);          }          return getPlacementContext(mapping,user);        } else         if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "secondary group mapping",user);          }          return getPlacementContext(mapping,getSecondaryGroup(user));
 else         if (mapping.getQueue().equals(CURRENT_USER_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user mapping",user);          }          return getPlacementContext(mapping,user);        } else         if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "secondary group mapping",user);          }          return getPlacementContext(mapping,getSecondaryGroup(user));        } else {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user static mapping",user);
 else         if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "secondary group mapping",user);          }          return getPlacementContext(mapping,getSecondaryGroup(user));        } else {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user static mapping",user);          }          return getPlacementContext(mapping);        }      }      if (user.equals(mapping.getSource())) {        if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {
          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "secondary group mapping",user);          }          return getPlacementContext(mapping,getSecondaryGroup(user));        } else {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user static mapping",user);          }          return getPlacementContext(mapping);        }      }      if (user.equals(mapping.getSource())) {        if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "static user primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {
            LOG.debug("Creating placement context for user {} using " + "secondary group mapping",user);          }          return getPlacementContext(mapping,getSecondaryGroup(user));        } else {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "current user static mapping",user);          }          return getPlacementContext(mapping);        }      }      if (user.equals(mapping.getSource())) {        if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "static user primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          String secondaryGroup=getSecondaryGroup(user);          if (secondaryGroup != null) {            if (LOG.isDebugEnabled()) {
          }          return getPlacementContext(mapping);        }      }      if (user.equals(mapping.getSource())) {        if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "static user primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          String secondaryGroup=getSecondaryGroup(user);          if (secondaryGroup != null) {            if (LOG.isDebugEnabled()) {              LOG.debug("Creating placement context for user {} using " + "static user secondary group mapping",user);            }            return getPlacementContext(mapping,secondaryGroup);          } else {            if (LOG.isDebugEnabled()) {              LOG.debug("Wanted to create placement context for user {}" + " using static user secondary group mapping," + " but user has no secondary group!",user);
      }      if (user.equals(mapping.getSource())) {        if (mapping.getQueue().equals(PRIMARY_GROUP_MAPPING)) {          if (LOG.isDebugEnabled()) {            LOG.debug("Creating placement context for user {} using " + "static user primary group mapping",user);          }          return getPlacementContext(mapping,getPrimaryGroup(user));        } else         if (mapping.getQueue().equals(SECONDARY_GROUP_MAPPING)) {          String secondaryGroup=getSecondaryGroup(user);          if (secondaryGroup != null) {            if (LOG.isDebugEnabled()) {              LOG.debug("Creating placement context for user {} using " + "static user secondary group mapping",user);            }            return getPlacementContext(mapping,secondaryGroup);          } else {            if (LOG.isDebugEnabled()) {              LOG.debug("Wanted to create placement context for user {}" + " using static user secondary group mapping," + " but user has no secondary group!",user);            }            return null;
@Override public ApplicationPlacementContext getPlacementForApp(ApplicationSubmissionContext asc,String user) throws YarnException {  String queueName=asc.getQueue();  ApplicationId applicationId=asc.getApplicationId();  if (mappings != null && mappings.size() > 0) {    try {      ApplicationPlacementContext mappedQueue=getPlacementForUser(user);      if (mappedQueue != null) {        if (queueName.equals(YarnConfiguration.DEFAULT_QUEUE_NAME) || queueName.equals(mappedQueue.getQueue()) || overrideWithQueueMappings) {
@Override public ApplicationPlacementContext getPlacementForApp(ApplicationSubmissionContext asc,String user) throws YarnException {  String queueName;  String cleanUser=cleanName(user);  PlacementRule parentRule=getParentRule();  if (parentRule != null) {
public void start(Configuration conf){  this.hostsFilePath=conf.get(YarnConfiguration.RM_SUBMISSION_PREPROCESSOR_FILE_PATH,YarnConfiguration.DEFAULT_RM_SUBMISSION_PREPROCESSOR_FILE_PATH);  int refreshPeriod=conf.getInt(YarnConfiguration.RM_SUBMISSION_PREPROCESSOR_REFRESH_INTERVAL_MS,YarnConfiguration.DEFAULT_RM_SUBMISSION_PREPROCESSOR_REFRESH_INTERVAL_MS);
    } else {      FileInputStream fileInputStream=new FileInputStream(hostFile);      BufferedReader reader=null;      Map<String,Map<ContextProp,String>> tempHostCommands=new HashMap<>();      try {        reader=new BufferedReader(new InputStreamReader(fileInputStream,StandardCharsets.UTF_8));        String line;        while ((line=reader.readLine()) != null) {          String[] commands=line.split("[ \t\n\f\r]+");          if (commands != null && commands.length > 1) {            String host=commands[0].trim();            if (host.startsWith("#")) {              continue;            }            Map<ContextProp,String> cMap=null;            for (int i=1; i < commands.length; i++) {
        String line;        while ((line=reader.readLine()) != null) {          String[] commands=line.split("[ \t\n\f\r]+");          if (commands != null && commands.length > 1) {            String host=commands[0].trim();            if (host.startsWith("#")) {              continue;            }            Map<ContextProp,String> cMap=null;            for (int i=1; i < commands.length; i++) {              String[] cSplit=commands[i].split("=");              if (cSplit == null || cSplit.length != 2) {                LOG.error("No commands found for line [{}]",commands[i]);                continue;              }              if (cMap == null) {                cMap=new HashMap<>();
private boolean checkAndRemovePartialRecord(Path record) throws IOException {  if (record.getName().endsWith(".tmp")) {
@Override public synchronized void storeApplicationStateInternal(ApplicationId appId,ApplicationStateData appStateDataPB) throws Exception {  Path appDirPath=getAppDir(rmAppRoot,appId);  mkdirsWithRetries(appDirPath);  Path nodeCreatePath=getNodePath(appDirPath,appId.toString());
@Override public synchronized void updateApplicationStateInternal(ApplicationId appId,ApplicationStateData appStateDataPB) throws Exception {  Path appDirPath=getAppDir(rmAppRoot,appId);  Path nodeCreatePath=getNodePath(appDirPath,appId.toString());
@Override public synchronized void storeApplicationAttemptStateInternal(ApplicationAttemptId appAttemptId,ApplicationAttemptStateData attemptStateDataPB) throws Exception {  Path appDirPath=getAppDir(rmAppRoot,appAttemptId.getApplicationId());  Path nodeCreatePath=getNodePath(appDirPath,appAttemptId.toString());
@Override public synchronized void updateApplicationAttemptStateInternal(ApplicationAttemptId appAttemptId,ApplicationAttemptStateData attemptStateDataPB) throws Exception {  Path appDirPath=getAppDir(rmAppRoot,appAttemptId.getApplicationId());  Path nodeCreatePath=getNodePath(appDirPath,appAttemptId.toString());
@Override public synchronized void removeApplicationAttemptInternal(ApplicationAttemptId appAttemptId) throws Exception {  Path appDirPath=getAppDir(rmAppRoot,appAttemptId.getApplicationId());  Path nodeRemovePath=getNodePath(appDirPath,appAttemptId.toString());
@Override public synchronized void removeApplicationStateInternal(ApplicationStateData appState) throws Exception {  ApplicationId appId=appState.getApplicationSubmissionContext().getApplicationId();  Path nodeRemovePath=getAppDir(rmAppRoot,appId);
@Override public synchronized void removeRMDelegationTokenState(RMDelegationTokenIdentifier identifier) throws Exception {  Path nodeCreatePath=getNodePath(rmDTSecretManagerRoot,DELEGATION_TOKEN_PREFIX + identifier.getSequenceNumber());
private void storeOrUpdateRMDelegationTokenState(RMDelegationTokenIdentifier identifier,Long renewDate,boolean isUpdate) throws Exception {  Path nodeCreatePath=getNodePath(rmDTSecretManagerRoot,DELEGATION_TOKEN_PREFIX + identifier.getSequenceNumber());  RMDelegationTokenIdentifierData identifierData=new RMDelegationTokenIdentifierData(identifier,renewDate);  if (isUpdate) {
@Override public synchronized void storeRMDTMasterKeyState(DelegationKey masterKey) throws Exception {  Path nodeCreatePath=getNodePath(rmDTSecretManagerRoot,DELEGATION_KEY_PREFIX + masterKey.getKeyId());  ByteArrayOutputStream os=new ByteArrayOutputStream();  try (DataOutputStream fsOut=new DataOutputStream(os)){
@Override public synchronized void removeRMDTMasterKeyState(DelegationKey masterKey) throws Exception {  Path nodeCreatePath=getNodePath(rmDTSecretManagerRoot,DELEGATION_KEY_PREFIX + masterKey.getKeyId());
@Override protected void storeReservationState(ReservationAllocationStateProto reservationAllocation,String planName,String reservationIdName) throws Exception {  Path planCreatePath=getNodePath(reservationRoot,planName);  mkdirsWithRetries(planCreatePath);  Path reservationPath=getNodePath(planCreatePath,reservationIdName);
@Override protected void removeReservationState(String planName,String reservationIdName) throws Exception {  Path planCreatePath=getNodePath(reservationRoot,planName);  Path reservationPath=getNodePath(planCreatePath,reservationIdName);
@Override protected void startInternal() throws Exception {  Path storeRoot=createStorageDir();  Options options=new Options();  options.createIfMissing(false);
      String planReservationString=key.substring(RM_RESERVATION_KEY_PREFIX.length());      String[] parts=planReservationString.split(SEPARATOR);      if (parts.length != 2) {        LOG.warn("Incorrect reservation state key " + key);        continue;      }      String planName=parts[0];      String reservationName=parts[1];      ReservationAllocationStateProto allocationState=ReservationAllocationStateProto.parseFrom(entry.getValue());      if (!rmState.getReservationState().containsKey(planName)) {        rmState.getReservationState().put(planName,new HashMap<ReservationId,ReservationAllocationStateProto>());      }      ReservationId reservationId=ReservationId.parseReservationId(reservationName);      rmState.getReservationState().get(planName).put(reservationId,allocationState);      numReservations++;    }  } catch (  DBException e) {    throw new IOException(e);
private void loadRMDTSecretManagerState(RMState state) throws IOException {  int numKeys=loadRMDTSecretManagerKeys(state);
private void loadRMDTSecretManagerState(RMState state) throws IOException {  int numKeys=loadRMDTSecretManagerKeys(state);  LOG.info("Recovered " + numKeys + " RM delegation token master keys");  int numTokens=loadRMDTSecretManagerTokens(state);
  rmState.appState.put(appId,appState);  String attemptNodePrefix=getApplicationNodeKey(appId) + SEPARATOR;  while (iter.hasNext()) {    Entry<byte[],byte[]> entry=iter.peekNext();    String key=asString(entry.getKey());    if (!key.startsWith(attemptNodePrefix)) {      break;    }    String attemptId=key.substring(attemptNodePrefix.length());    if (attemptId.startsWith(ApplicationAttemptId.appAttemptIdStrPrefix)) {      ApplicationAttemptStateData attemptState=createAttemptState(attemptId,entry.getValue());      appState.attempts.put(attemptState.getAttemptId(),attemptState);    } else {      LOG.warn("Ignoring unknown application key: " + key);    }    iter.next();  }  int numAttempts=appState.attempts.size();
@Override protected void storeApplicationStateInternal(ApplicationId appId,ApplicationStateData appStateData) throws IOException {  String key=getApplicationNodeKey(appId);
@Override protected void storeApplicationAttemptStateInternal(ApplicationAttemptId attemptId,ApplicationAttemptStateData attemptStateData) throws IOException {  String key=getApplicationAttemptNodeKey(attemptId);
@Override public synchronized void removeApplicationAttemptInternal(ApplicationAttemptId attemptId) throws IOException {  String attemptKey=getApplicationAttemptNodeKey(attemptId);
@Override protected void storeReservationState(ReservationAllocationStateProto reservationAllocation,String planName,String reservationIdName) throws Exception {  try {    try (WriteBatch batch=db.createWriteBatch()){      String key=getReservationNodeKey(planName,reservationIdName);
@Override protected void removeReservationState(String planName,String reservationIdName) throws Exception {  try {    try (WriteBatch batch=db.createWriteBatch()){      String reservationKey=getReservationNodeKey(planName,reservationIdName);      batch.delete(bytes(reservationKey));
private void storeOrUpdateRMDT(RMDelegationTokenIdentifier tokenId,Long renewDate,boolean isUpdate) throws IOException {  String tokenKey=getRMDTTokenNodeKey(tokenId);  RMDelegationTokenIdentifierData tokenData=new RMDelegationTokenIdentifierData(tokenId,renewDate);
@Override protected void removeRMDelegationTokenState(RMDelegationTokenIdentifier tokenId) throws IOException {  String tokenKey=getRMDTTokenNodeKey(tokenId);
@Override protected void storeRMDTMasterKeyState(DelegationKey masterKey) throws IOException {  String dbKey=getRMDTMasterKeyNodeKey(masterKey);
@Override protected void removeRMDTMasterKeyState(DelegationKey masterKey) throws IOException {  String dbKey=getRMDTMasterKeyNodeKey(masterKey);
@Override public void deleteStore() throws IOException {  Path root=getStorageDir();
@Override public synchronized void removeApplication(ApplicationId removeAppId) throws IOException {  String appKey=getApplicationNodeKey(removeAppId);
@VisibleForTesting int getNumEntriesInDatabase() throws IOException {  int numEntries=0;  try (LeveldbIterator iter=new LeveldbIterator(db)){    iter.seekToFirst();    while (iter.hasNext()) {      Entry<byte[],byte[]> entry=iter.next();
@Override public synchronized void updateApplicationStateInternal(ApplicationId appId,ApplicationStateData appState) throws Exception {
@Override public synchronized void removeApplicationAttemptInternal(ApplicationAttemptId appAttemptId) throws Exception {  ApplicationStateData appState=state.getApplicationState().get(appAttemptId.getApplicationId());  ApplicationAttemptStateData attemptState=appState.attempts.remove(appAttemptId);
private void storeOrUpdateRMDT(RMDelegationTokenIdentifier rmDTIdentifier,Long renewDate,boolean isUpdate) throws Exception {  Map<RMDelegationTokenIdentifier,Long> rmDTState=state.rmSecretManagerState.getTokenState();  if (rmDTState.containsKey(rmDTIdentifier)) {    IOException e=new IOException("RMDelegationToken: " + rmDTIdentifier + "is already stored.");
@Override public synchronized void removeRMDelegationTokenState(RMDelegationTokenIdentifier rmDTIdentifier) throws Exception {  Map<RMDelegationTokenIdentifier,Long> rmDTState=state.rmSecretManagerState.getTokenState();  rmDTState.remove(rmDTIdentifier);
@Override protected synchronized void updateRMDelegationTokenState(RMDelegationTokenIdentifier rmDTIdentifier,Long renewDate) throws Exception {  removeRMDelegationTokenState(rmDTIdentifier);  storeOrUpdateRMDT(rmDTIdentifier,renewDate,true);
@Override public synchronized void storeRMDTMasterKeyState(DelegationKey delegationKey) throws Exception {  Set<DelegationKey> rmDTMasterKeyState=state.rmSecretManagerState.getMasterKeyState();  if (rmDTMasterKeyState.contains(delegationKey)) {    IOException e=new IOException("RMDTMasterKey with keyID: " + delegationKey.getKeyId() + " is already stored");
@Override public synchronized void removeRMDTMasterKeyState(DelegationKey delegationKey) throws Exception {
@Override protected synchronized void storeReservationState(ReservationAllocationStateProto reservationAllocation,String planName,String reservationIdName) throws Exception {
@Override protected synchronized void removeReservationState(String planName,String reservationIdName) throws Exception {
public void checkVersion() throws Exception {  Version loadedVersion=loadVersion();
protected void handleStoreEvent(RMStateStoreEvent event){  this.writeLock.lock();  try {
protected void handleStoreEvent(RMStateStoreEvent event){  this.writeLock.lock();  try {    LOG.debug("Processing event of type {}",event.getType());    final RMStateStoreState oldState=getRMStateStoreState();    this.stateMachine.doTransition(event.getType(),event);    if (oldState != getRMStateStoreState()) {
@SuppressWarnings("unchecked") private boolean notifyStoreOperationFailedInternal(Exception failureCause){  boolean isFenced=false;
public static RMStateStore getStore(Configuration conf){  Class<? extends RMStateStore> storeClass=conf.getClass(YarnConfiguration.RM_STORE,MemoryRMStateStore.class,RMStateStore.class);
  }  fencingNodePath=getNodePath(zkRootNodePath,FENCING_LOCK);  zkSessionTimeout=conf.getInt(YarnConfiguration.RM_ZK_TIMEOUT_MS,YarnConfiguration.DEFAULT_RM_ZK_TIMEOUT_MS);  zknodeLimit=conf.getInt(YarnConfiguration.RM_ZK_ZNODE_SIZE_LIMIT_BYTES,YarnConfiguration.DEFAULT_RM_ZK_ZNODE_SIZE_LIMIT_BYTES);  appIdNodeSplitIndex=conf.getInt(YarnConfiguration.ZK_APPID_NODE_SPLIT_INDEX,YarnConfiguration.DEFAULT_ZK_APPID_NODE_SPLIT_INDEX);  if (appIdNodeSplitIndex < 0 || appIdNodeSplitIndex > 4) {    LOG.info("Invalid value " + appIdNodeSplitIndex + " for config "+ YarnConfiguration.ZK_APPID_NODE_SPLIT_INDEX+ " specified. "+ "Resetting it to "+ YarnConfiguration.DEFAULT_ZK_APPID_NODE_SPLIT_INDEX);    appIdNodeSplitIndex=YarnConfiguration.DEFAULT_ZK_APPID_NODE_SPLIT_INDEX;  }  zkAcl=ZKCuratorManager.getZKAcls(conf);  if (HAUtil.isHAEnabled(conf)) {    String zkRootNodeAclConf=HAUtil.getConfValueForRMInstance(YarnConfiguration.ZK_RM_STATE_STORE_ROOT_NODE_ACL,conf);    if (zkRootNodeAclConf != null) {      zkRootNodeAclConf=ZKUtil.resolveConfIndirection(zkRootNodeAclConf);      try {        zkRootNodeAcl=ZKUtil.parseACLs(zkRootNodeAclConf);      } catch (      ZKUtil.BadAclFormatException bafe) {
private void loadReservationSystemState(RMState rmState) throws Exception {  List<String> planNodes=getChildren(reservationRoot);  for (  String planName : planNodes) {
private void loadReservationSystemState(RMState rmState) throws Exception {  List<String> planNodes=getChildren(reservationRoot);  for (  String planName : planNodes) {    LOG.debug("Loading plan from znode: {}",planName);    String planNodePath=getNodePath(reservationRoot,planName);    List<String> reservationNodes=getChildren(planNodePath);    for (    String reservationNodeName : reservationNodes) {      String reservationNodePath=getNodePath(planNodePath,reservationNodeName);
    if (tokenRoot == null) {      continue;    }    List<String> childNodes=getChildren(tokenRoot);    boolean dtNodeFound=false;    for (    String childNodeName : childNodes) {      if (childNodeName.startsWith(DELEGATION_TOKEN_PREFIX)) {        dtNodeFound=true;        String parentNodePath=getNodePath(tokenRoot,childNodeName);        if (splitIndex == 0) {          loadDelegationTokenFromNode(rmState,parentNodePath);        } else {          List<String> leafNodes=getChildren(parentNodePath);          for (          String leafNodeName : leafNodes) {            loadDelegationTokenFromNode(rmState,getNodePath(parentNodePath,leafNodeName));          }        }      } else       if (splitIndex == 0 && !(childNodeName.equals("1") || childNodeName.equals("2") || childNodeName.equals("3")|| childNodeName.equals("4"))) {
private void loadRMAppStateFromAppNode(RMState rmState,String appNodePath,String appIdStr) throws Exception {  byte[] appData=getData(appNodePath);
      continue;    }    List<String> childNodes=getChildren(appRoot);    boolean appNodeFound=false;    for (    String childNodeName : childNodes) {      if (childNodeName.startsWith(ApplicationId.appIdStrPrefix)) {        appNodeFound=true;        if (splitIndex == 0) {          loadRMAppStateFromAppNode(rmState,getNodePath(appRoot,childNodeName),childNodeName);        } else {          String parentNodePath=getNodePath(appRoot,childNodeName);          List<String> leafNodes=getChildren(parentNodePath);          for (          String leafNodeName : leafNodes) {            String appIdStr=childNodeName + leafNodeName;            loadRMAppStateFromAppNode(rmState,getNodePath(parentNodePath,leafNodeName),appIdStr);          }        }      } else       if (!childNodeName.equals(RM_APP_ROOT_HIERARCHIES)) {
@Override public synchronized void storeApplicationStateInternal(ApplicationId appId,ApplicationStateData appStateDataPB) throws Exception {  String nodeCreatePath=getLeafAppIdNodePath(appId.toString(),true);
    ZnodeSplitInfo alternatePathInfo=getAlternateAppPath(appId.toString());    if (alternatePathInfo != null) {      nodeUpdatePath=alternatePathInfo.path;    } else {      pathExists=false;      if (appIdNodeSplitIndex != 0) {        String rootNode=getSplitZnodeParent(nodeUpdatePath,appIdNodeSplitIndex);        if (!exists(rootNode)) {          zkManager.safeCreate(rootNode,null,zkAcl,CreateMode.PERSISTENT,zkAcl,fencingNodePath);        }      }    }  }  LOG.debug("Storing final state info for app: {} at: {}",appId,nodeUpdatePath);  byte[] appStateData=appStateDataPB.getProto().toByteArray();  if (pathExists) {    zkManager.safeSetData(nodeUpdatePath,appStateData,-1,zkAcl,fencingNodePath);  } else {    zkManager.safeCreate(nodeUpdatePath,appStateData,zkAcl,CreateMode.PERSISTENT,zkAcl,fencingNodePath);
    if (alternatePathInfo == null) {      if (operation == AppAttemptOp.REMOVE) {        return;      } else {        throw new YarnRuntimeException("Unexpected Exception. App node for " + "app " + appId + " not found");      }    } else {      appDirPath=alternatePathInfo.path;    }  }  String path=getNodePath(appDirPath,appAttemptId.toString());  byte[] attemptStateData=(attemptStateDataPB == null) ? null : attemptStateDataPB.getProto().toByteArray();  LOG.debug("{} info for attempt: {} at: {}",operation,appAttemptId,path);switch (operation) {case UPDATE:    if (exists(path)) {      zkManager.safeSetData(path,attemptStateData,-1,zkAcl,fencingNodePath);    } else {      zkManager.safeCreate(path,attemptStateData,zkAcl,CreateMode.PERSISTENT,zkAcl,fencingNodePath);
@Override protected synchronized void storeRMDelegationTokenState(RMDelegationTokenIdentifier rmDTIdentifier,Long renewDate) throws Exception {  String nodeCreatePath=getLeafDelegationTokenNodePath(rmDTIdentifier.getSequenceNumber(),true);
@Override protected synchronized void storeRMDelegationTokenState(RMDelegationTokenIdentifier rmDTIdentifier,Long renewDate) throws Exception {  String nodeCreatePath=getLeafDelegationTokenNodePath(rmDTIdentifier.getSequenceNumber(),true);  LOG.debug("Storing {}{}",DELEGATION_TOKEN_PREFIX,rmDTIdentifier.getSequenceNumber());  RMDelegationTokenIdentifierData identifierData=new RMDelegationTokenIdentifierData(rmDTIdentifier,renewDate);  ByteArrayOutputStream seqOs=new ByteArrayOutputStream();  try (DataOutputStream seqOut=new DataOutputStream(seqOs)){    SafeTransaction trx=zkManager.createTransaction(zkAcl,fencingNodePath);    trx.create(nodeCreatePath,identifierData.toByteArray(),zkAcl,CreateMode.PERSISTENT);    seqOut.writeInt(rmDTIdentifier.getSequenceNumber());
@Override protected synchronized void storeRMDTMasterKeyState(DelegationKey delegationKey) throws Exception {  String nodeCreatePath=getNodePath(dtMasterKeysRootPath,DELEGATION_KEY_PREFIX + delegationKey.getKeyId());
@Override protected synchronized void removeRMDTMasterKeyState(DelegationKey delegationKey) throws Exception {  String nodeRemovePath=getNodePath(dtMasterKeysRootPath,DELEGATION_KEY_PREFIX + delegationKey.getKeyId());
@Override protected synchronized void removeReservationState(String planName,String reservationIdName) throws Exception {  String planNodePath=getNodePath(reservationRoot,planName);  String reservationPath=getNodePath(planNodePath,reservationIdName);
private void addOrUpdateReservationState(ReservationAllocationStateProto reservationAllocation,String planName,String reservationIdName,SafeTransaction trx,boolean isUpdate) throws Exception {  String planCreatePath=getNodePath(reservationRoot,planName);  String reservationPath=getNodePath(planCreatePath,reservationIdName);  byte[] reservationData=reservationAllocation.toByteArray();  if (!exists(planCreatePath)) {
@Override public ReservationId getNewReservationId(){  writeLock.lock();  try {    ReservationId resId=ReservationId.newInstance(ResourceManager.getClusterTimeStamp(),resCounter.incrementAndGet());
protected Plan initializePlan(String planQueueName) throws YarnException {  String planQueuePath=getPlanQueuePath(planQueueName);  SharingPolicy adPolicy=getAdmissionPolicy(planQueuePath);  adPolicy.init(planQueuePath,getReservationSchedulerConfiguration());  Resource minAllocation=getMinAllocation();  Resource maxAllocation=getMaxAllocation();  ResourceCalculator rescCalc=getResourceCalculator();  Resource totCap=getPlanQueueCapacity(planQueueName);  Plan plan=new InMemoryPlan(getRootQueueMetrics(),adPolicy,getAgent(planQueuePath),totCap,planStepSize,rescCalc,minAllocation,maxAllocation,planQueueName,getReplanner(planQueuePath),getReservationSchedulerConfiguration().getMoveOnExpiry(planQueuePath),maxPeriodicity,rmContext);
protected Planner getReplanner(String planQueueName){  ReservationSchedulerConfiguration reservationConfig=getReservationSchedulerConfiguration();  String plannerClassName=reservationConfig.getReplanner(planQueueName);
protected ReservationAgent getAgent(String queueName){  ReservationSchedulerConfiguration reservationConfig=getReservationSchedulerConfiguration();  String agentClassName=reservationConfig.getReservationAgent(queueName);
protected SharingPolicy getAdmissionPolicy(String queueName){  ReservationSchedulerConfiguration reservationConfig=getReservationSchedulerConfiguration();  String admissionPolicyClassName=reservationConfig.getReservationAdmissionPolicy(queueName);
@Override public synchronized void synchronizePlan(Plan plan,boolean shouldReplan){  String planQueueName=plan.getQueueName();
      curReservationNames.remove(reservationId);    } else {      expired.add(reservationId);    }  }  cleanupExpiredQueues(planQueueName,plan.getMoveOnExpiry(),expired,defReservationQueue);  float totalAssignedCapacity=0f;  if (currentReservations != null) {    try {      setQueueEntitlement(planQueueName,defReservationQueue,0f,1.0f);    } catch (    YarnException e) {      LOG.warn("Exception while trying to release default queue capacity for plan: {}",planQueueName,e);    }    List<ReservationAllocation> sortedAllocations=sortByDelta(new ArrayList<ReservationAllocation>(currentReservations),now,plan);    for (    ReservationAllocation res : sortedAllocations) {      String currResId=res.getReservationId().toString();      if (curReservationNames.contains(currResId)) {        addReservationQueue(planQueueName,planQueue,currResId);
    try {      setQueueEntitlement(planQueueName,defReservationQueue,0f,1.0f);    } catch (    YarnException e) {      LOG.warn("Exception while trying to release default queue capacity for plan: {}",planQueueName,e);    }    List<ReservationAllocation> sortedAllocations=sortByDelta(new ArrayList<ReservationAllocation>(currentReservations),now,plan);    for (    ReservationAllocation res : sortedAllocations) {      String currResId=res.getReservationId().toString();      if (curReservationNames.contains(currResId)) {        addReservationQueue(planQueueName,planQueue,currResId);      }      Resource capToAssign=res.getResourcesAtTime(now);      float targetCapacity=0f;      if (planResources.getMemorySize() > 0 && planResources.getVirtualCores() > 0) {        if (shouldResize) {          capToAssign=calculateReservationToPlanProportion(plan.getResourceCalculator(),planResources,reservedResources,capToAssign);        }        targetCapacity=calculateReservationToPlanRatio(plan.getResourceCalculator(),clusterResources,planResources,capToAssign);
        addReservationQueue(planQueueName,planQueue,currResId);      }      Resource capToAssign=res.getResourcesAtTime(now);      float targetCapacity=0f;      if (planResources.getMemorySize() > 0 && planResources.getVirtualCores() > 0) {        if (shouldResize) {          capToAssign=calculateReservationToPlanProportion(plan.getResourceCalculator(),planResources,reservedResources,capToAssign);        }        targetCapacity=calculateReservationToPlanRatio(plan.getResourceCalculator(),clusterResources,planResources,capToAssign);      }      LOG.debug("Assigning capacity of {} to queue {} with target capacity {}",capToAssign,currResId,targetCapacity);      float maxCapacity=1.0f;      if (res.containsGangs()) {        maxCapacity=targetCapacity;      }      try {        setQueueEntitlement(planQueueName,currResId,targetCapacity,maxCapacity);      } catch (      YarnException e) {        LOG.warn("Exception while trying to size reservation for plan: {}",currResId,planQueueName,e);
@Override protected Queue getPlanQueue(String planQueueName){  Queue planQueue=fs.getQueueManager().getParentQueue(planQueueName,false);  if (planQueue == null) {
@Override public boolean addReservation(ReservationAllocation reservation,boolean isRecovering) throws PlanningException {  InMemoryReservationAllocation inMemReservation=(InMemoryReservationAllocation)reservation;  if (inMemReservation.getUser() == null) {    String errMsg="The specified Reservation with ID " + inMemReservation.getReservationId() + " is not mapped to any user";
  writeLock.lock();  try {    if (reservationTable.containsKey(inMemReservation.getReservationId())) {      String errMsg="The specified Reservation with ID " + inMemReservation.getReservationId() + " already exists";      LOG.error(errMsg);      throw new IllegalArgumentException(errMsg);    }    if (!isRecovering) {      policy.validate(this,inMemReservation);      reservation.setAcceptanceTimestamp(clock.getTime());      if (rmStateStore != null) {        rmStateStore.storeNewReservation(ReservationSystemUtil.buildStateProto(inMemReservation),getQueueName(),inMemReservation.getReservationId().toString());      }    }    ReservationInterval searchInterval=new ReservationInterval(inMemReservation.getStartTime(),inMemReservation.getEndTime());    Set<InMemoryReservationAllocation> reservations=currentReservations.get(searchInterval);    if (reservations == null) {      reservations=new HashSet<InMemoryReservationAllocation>();
      throw new IllegalArgumentException(errMsg);    }    if (!isRecovering) {      policy.validate(this,inMemReservation);      reservation.setAcceptanceTimestamp(clock.getTime());      if (rmStateStore != null) {        rmStateStore.storeNewReservation(ReservationSystemUtil.buildStateProto(inMemReservation),getQueueName(),inMemReservation.getReservationId().toString());      }    }    ReservationInterval searchInterval=new ReservationInterval(inMemReservation.getStartTime(),inMemReservation.getEndTime());    Set<InMemoryReservationAllocation> reservations=currentReservations.get(searchInterval);    if (reservations == null) {      reservations=new HashSet<InMemoryReservationAllocation>();    }    if (!reservations.add(inMemReservation)) {      LOG.error("Unable to add reservation: {} to plan.",inMemReservation.getReservationId());      return false;    }    currentReservations.put(searchInterval,reservations);    reservationTable.put(inMemReservation.getReservationId(),inMemReservation);
@Override public boolean updateReservation(ReservationAllocation reservation) throws PlanningException {  writeLock.lock();  boolean result=false;  try {    ReservationId resId=reservation.getReservationId();    ReservationAllocation currReservation=getReservationById(resId);    if (currReservation == null) {      String errMsg="The specified Reservation with ID " + resId + " does not exist in the plan";
  boolean result=false;  try {    ReservationId resId=reservation.getReservationId();    ReservationAllocation currReservation=getReservationById(resId);    if (currReservation == null) {      String errMsg="The specified Reservation with ID " + resId + " does not exist in the plan";      LOG.error(errMsg);      throw new IllegalArgumentException(errMsg);    }    policy.validate(this,reservation);    if (!removeReservation(currReservation)) {      LOG.error("Unable to replace reservation: {} from plan.",reservation.getReservationId());      return result;    }    try {      result=addReservation(reservation,false);    } catch (    PlanningException e) {
    ReservationId resId=reservation.getReservationId();    ReservationAllocation currReservation=getReservationById(resId);    if (currReservation == null) {      String errMsg="The specified Reservation with ID " + resId + " does not exist in the plan";      LOG.error(errMsg);      throw new IllegalArgumentException(errMsg);    }    policy.validate(this,reservation);    if (!removeReservation(currReservation)) {      LOG.error("Unable to replace reservation: {} from plan.",reservation.getReservationId());      return result;    }    try {      result=addReservation(reservation,false);    } catch (    PlanningException e) {      LOG.error("Unable to update reservation: {} from plan due to {}.",reservation.getReservationId(),e.getMessage());    }    if (result) {
      LOG.error(errMsg);      throw new IllegalArgumentException(errMsg);    }    policy.validate(this,reservation);    if (!removeReservation(currReservation)) {      LOG.error("Unable to replace reservation: {} from plan.",reservation.getReservationId());      return result;    }    try {      result=addReservation(reservation,false);    } catch (    PlanningException e) {      LOG.error("Unable to update reservation: {} from plan due to {}.",reservation.getReservationId(),e.getMessage());    }    if (result) {      LOG.info("Successfully updated reservation: {} in plan.",reservation.getReservationId());      return result;    } else {      addReservation(currReservation,false);
  Set<InMemoryReservationAllocation> reservations=currentReservations.get(searchInterval);  if (reservations != null) {    if (rmStateStore != null) {      rmStateStore.removeReservation(getQueueName(),reservation.getReservationId().toString());    }    if (!reservations.remove(reservation)) {      LOG.error("Unable to remove reservation: {} from plan.",reservation.getReservationId());      return false;    }    if (reservations.isEmpty()) {      currentReservations.remove(searchInterval);    }  } else {    String errMsg="The specified Reservation with ID " + reservation.getReservationId() + " does not exist in the plan";    LOG.error(errMsg);    throw new IllegalArgumentException(errMsg);  }  reservationTable.remove(reservation.getReservationId());  decrementAllocation(reservation);
@Override public boolean deleteReservation(ReservationId reservationID){  writeLock.lock();  try {    ReservationAllocation reservation=getReservationById(reservationID);    if (reservation == null) {      String errMsg="The specified Reservation with ID " + reservationID + " does not exist in the plan";
@Override public void archiveCompletedReservations(long tick){
@Override public boolean createReservation(ReservationId reservationId,String user,Plan plan,ReservationDefinition contract) throws PlanningException {
@Override public boolean createReservation(ReservationId reservationId,String user,Plan plan,ReservationDefinition contract) throws PlanningException {  LOG.info("placing the following ReservationRequest: " + contract);  try {    boolean res=planner.createReservation(reservationId,user,plan,contract);    if (res) {
@Override public boolean updateReservation(ReservationId reservationId,String user,Plan plan,ReservationDefinition contract) throws PlanningException {
@Override public boolean deleteReservation(ReservationId reservationId,String user,Plan plan) throws PlanningException {
@Override public void init(Configuration conf){  allocateLeft=conf.getBoolean(FAVOR_EARLY_ALLOCATION,DEFAULT_GREEDY_FAVOR_EARLY_ALLOCATION);  if (allocateLeft) {
@Override public boolean createReservation(ReservationId reservationId,String user,Plan plan,ReservationDefinition contract) throws PlanningException {
@Override public boolean createReservation(ReservationId reservationId,String user,Plan plan,ReservationDefinition contract) throws PlanningException {  LOG.info("placing the following ReservationRequest: " + contract);  try {    boolean res=planner.createReservation(reservationId,user,plan,contract);    if (res) {
@Override public boolean updateReservation(ReservationId reservationId,String user,Plan plan,ReservationDefinition contract) throws PlanningException {
@Override public boolean deleteReservation(ReservationId reservationId,String user,Plan plan) throws PlanningException {
public void setVcoresPerNode(String node,int vcores){  setInt(getNodePrefix(node) + VCORES,vcores);
public void setMemoryPerNode(String node,int memory){  setInt(getNodePrefix(node) + MEMORY,memory);
public void setOverCommitTimeoutPerNode(String node,int overCommitTimeout){  setInt(getNodePrefix(node) + OVERCOMMIT_TIMEOUT,overCommitTimeout);
      resourcesFile=tmp.getPath();    }  }  ObjectMapper mapper=new ObjectMapper();  Map data=mapper.readValue(new File(resourcesFile),Map.class);  Iterator iterator=data.entrySet().iterator();  while (iterator.hasNext()) {    Map.Entry entry=(Map.Entry)iterator.next();    String profileName=entry.getKey().toString();    if (profileName.isEmpty()) {      throw new IOException("Name of resource profile cannot be an empty string");    }    if (profileName.equals(MINIMUM_PROFILE) || profileName.equals(MAXIMUM_PROFILE)) {      throw new IOException(String.format("profile={%s, %s} is should not be specified " + "inside %s, they will be loaded from resource-types.xml",MINIMUM_PROFILE,MAXIMUM_PROFILE,sourceFile));    }    if (entry.getValue() instanceof Map) {      Map profileInfo=(Map)entry.getValue();      if (!profileInfo.containsKey(MEMORY) || !profileInfo.containsKey(VCORES)) {        throw new IOException("Illegal resource profile definition; profile '" + profileName + "' must contain '"+ MEMORY+ "' and '"+ VCORES+ "'");
    Map.Entry entry=(Map.Entry)iterator.next();    String profileName=entry.getKey().toString();    if (profileName.isEmpty()) {      throw new IOException("Name of resource profile cannot be an empty string");    }    if (profileName.equals(MINIMUM_PROFILE) || profileName.equals(MAXIMUM_PROFILE)) {      throw new IOException(String.format("profile={%s, %s} is should not be specified " + "inside %s, they will be loaded from resource-types.xml",MINIMUM_PROFILE,MAXIMUM_PROFILE,sourceFile));    }    if (entry.getValue() instanceof Map) {      Map profileInfo=(Map)entry.getValue();      if (!profileInfo.containsKey(MEMORY) || !profileInfo.containsKey(VCORES)) {        throw new IOException("Illegal resource profile definition; profile '" + profileName + "' must contain '"+ MEMORY+ "' and '"+ VCORES+ "'");      }      Resource resource=parseResource(profileInfo);      profiles.put(profileName,resource);      LOG.info("Added profile '" + profileName + "' with resources: "+ resource);    }  }  profiles.put(MINIMUM_PROFILE,ResourceUtils.getResourceTypesMinimumAllocation());  profiles.put(MAXIMUM_PROFILE,ResourceUtils.getResourceTypesMaximumAllocation());
@Override public void handle(RMAppEvent event){  this.writeLock.lock();  try {    ApplicationId appID=event.getApplicationId();
@Override public void recover(RMState state){  ApplicationStateData appState=state.getApplicationState().get(getApplicationId());  this.recoveredFinalState=appState.getState();  if (recoveredFinalState == null) {
private void processNodeUpdate(RMAppNodeUpdateType type,RMNode node){  NodeState nodeState=node.getState();  updatedNodes.put(node,RMAppNodeUpdateType.convertToNodeUpdateType(type));
static void appAdminClientCleanUp(RMAppImpl app){  try {    AppAdminClient client=AppAdminClient.createAppAdminClient(app.applicationType,app.conf);    int result=client.actionCleanUp(app.name,app.user);    if (result == 0) {
private int getDiagnosticsLimitKCOrThrow(final Configuration configuration){  try {    final int diagnosticsLimitKC=configuration.getInt(YarnConfiguration.APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC,YarnConfiguration.DEFAULT_APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC);    if (diagnosticsLimitKC <= 0) {      final String message=String.format(DIAGNOSTIC_LIMIT_CONFIG_ERROR_MESSAGE,YarnConfiguration.APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC,diagnosticsLimitKC);
@Override public void handle(RMAppAttemptEvent event){  this.writeLock.lock();  try {    ApplicationAttemptId appAttemptID=event.getApplicationAttemptId();
finalTrackingUrl=sanitizeTrackingUrl(unregisterEvent.getFinalTrackingUrl());finalStatus=unregisterEvent.getFinalApplicationStatus();break;case CONTAINER_FINISHED:RMAppAttemptContainerFinishedEvent finishEvent=(RMAppAttemptContainerFinishedEvent)event;diags.append(getAMContainerCrashedDiagnostics(finishEvent));exitStatus=finishEvent.getContainerStatus().getExitStatus();break;case KILL:break;case FAIL:diags.append(event.getDiagnosticMsg());break;case EXPIRE:diags.append(getAMExpiredDiagnostics(event));break;default:break;}AggregateAppResourceUsage resUsage=this.attemptMetrics.getAggregateAppResourceUsage();RMStateStore rmStore=rmContext.getStateStore();
@Override public void handle(RMContainerEvent event){
@VisibleForTesting protected void onInvalidStateTransition(RMContainerEventType rmContainerEventType,RMContainerState state){
public void handle(RMNodeEvent event){
private void handleReportedIncreasedContainers(List<Container> reportedIncreasedContainers){  for (  Container container : reportedIncreasedContainers) {    ContainerId containerId=container.getId();    if (containersToClean.contains(containerId)) {
private void handleContainerStatus(List<ContainerStatus> containerStatuses){  List<ContainerStatus> newlyLaunchedContainers=new ArrayList<ContainerStatus>();  List<ContainerStatus> newlyCompletedContainers=new ArrayList<ContainerStatus>();  List<Map.Entry<ApplicationId,ContainerStatus>> needUpdateContainers=new ArrayList<Map.Entry<ApplicationId,ContainerStatus>>();  int numRemoteRunningContainers=0;  for (  ContainerStatus remoteContainer : containerStatuses) {    ContainerId containerId=remoteContainer.getContainerId();    if (containersToClean.contains(containerId)) {
private void handleContainerStatus(List<ContainerStatus> containerStatuses){  List<ContainerStatus> newlyLaunchedContainers=new ArrayList<ContainerStatus>();  List<ContainerStatus> newlyCompletedContainers=new ArrayList<ContainerStatus>();  List<Map.Entry<ApplicationId,ContainerStatus>> needUpdateContainers=new ArrayList<Map.Entry<ApplicationId,ContainerStatus>>();  int numRemoteRunningContainers=0;  for (  ContainerStatus remoteContainer : containerStatuses) {    ContainerId containerId=remoteContainer.getContainerId();    if (containersToClean.contains(containerId)) {      LOG.info("Container " + containerId + " already scheduled for "+ "cleanup, no further processing");      continue;    }    ApplicationId containerAppId=containerId.getApplicationAttemptId().getApplicationId();    if (finishedApplications.contains(containerAppId)) {      LOG.info("Container " + containerId + " belongs to an application that is already killed,"+ " no further processing");      continue;    } else     if (!runningApplications.contains(containerAppId)) {
protected void containerLaunchedOnNode(ContainerId containerId,SchedulerNode node){  readLock.lock();  try {    SchedulerApplicationAttempt application=getCurrentAttemptForContainer(containerId);    if (application == null) {
protected void containerIncreasedOnNode(ContainerId containerId,SchedulerNode node,Container increasedContainerReportedByNM){  SchedulerApplicationAttempt application=getCurrentAttemptForContainer(containerId);  if (application == null) {
@Override public SchedulerAppReport getSchedulerAppInfo(ApplicationAttemptId appAttemptId){  SchedulerApplicationAttempt attempt=getApplicationAttempt(appAttemptId);  if (attempt == null) {
@Override public ApplicationResourceUsageReport getAppResourceUsageReport(ApplicationAttemptId appAttemptId){  SchedulerApplicationAttempt attempt=getApplicationAttempt(appAttemptId);  if (attempt == null) {
    for (    NMContainerStatus container : containerReports) {      ApplicationId appId=container.getContainerId().getApplicationAttemptId().getApplicationId();      RMApp rmApp=rmContext.getRMApps().get(appId);      if (rmApp == null) {        LOG.error("Skip recovering container " + container + " for unknown application.");        killOrphanContainerOnNode(nm,container);        continue;      }      SchedulerApplication<T> schedulerApp=applications.get(appId);      if (schedulerApp == null) {        LOG.info("Skip recovering container  " + container + " for unknown SchedulerApplication. "+ "Application current state is "+ rmApp.getState());        killOrphanContainerOnNode(nm,container);        continue;      }      LOG.info("Recovering container " + container);      SchedulerApplicationAttempt schedulerAttempt=schedulerApp.getCurrentAppAttempt();      if (!rmApp.getApplicationSubmissionContext().getKeepContainersAcrossApplicationAttempts()) {
          killOrphanContainerOnNode(nm,container);          continue;        }      }      Queue queue=schedulerApp.getQueue();      String queueName=queue instanceof CSQueue ? ((CSQueue)queue).getQueuePath() : queue.getQueueName();      RMContainer rmContainer=recoverAndCreateContainer(container,nm,queueName);      rmContainer.handle(new RMContainerRecoverEvent(container.getContainerId(),container));      SchedulerNode schedulerNode=nodeTracker.getNode(nm.getNodeID());      schedulerNode.recoverContainer(rmContainer);      Queue queueToRecover=schedulerAttempt.getQueue();      queueToRecover.recoverContainer(getClusterResource(),schedulerAttempt,rmContainer);      schedulerAttempt.recoverContainer(schedulerNode,rmContainer);      RMAppAttempt appAttempt=rmApp.getCurrentAppAttempt();      if (appAttempt != null) {        Container masterContainer=appAttempt.getMasterContainer();        if (masterContainer != null && masterContainer.getId().equals(rmContainer.getContainerId())) {
@VisibleForTesting @Private public void completedContainer(RMContainer rmContainer,ContainerStatus containerStatus,RMContainerEventType event){  if (rmContainer == null) {
protected void releaseContainers(List<ContainerId> containers,SchedulerApplicationAttempt attempt){  for (  ContainerId containerId : containers) {    RMContainer rmContainer=getRMContainer(containerId);    if (rmContainer == null) {      if (System.currentTimeMillis() - ResourceManager.getClusterTimeStamp() < nmExpireInterval) {
public void updateNodeResource(RMNode nm,ResourceOption resourceOption){  writeLock.lock();  try {    SchedulerNode node=getSchedulerNode(nm.getNodeID());    Resource newResource=resourceOption.getResource();    final int timeout=resourceOption.getOverCommitTimeout();    Resource oldResource=node.getTotalResource();    if (!oldResource.equals(newResource)) {      rmContext.getNodeLabelManager().updateNodeResource(nm.getNodeID(),newResource);
  for (  Container container : newlyIncreasedContainers) {    containerIncreasedOnNode(container.getId(),schedulerNode,container);  }  for (  Map.Entry<ApplicationId,ContainerStatus> c : updateExistContainers) {    SchedulerApplication<T> app=applications.get(c.getKey());    ContainerId containerId=c.getValue().getContainerId();    if (app == null || app.getCurrentAppAttempt() == null) {      continue;    }    RMContainer rmContainer=app.getCurrentAppAttempt().getRMContainer(containerId);    if (rmContainer == null) {      continue;    }    if (rmContainer.getExposedPorts() != null && rmContainer.getExposedPorts().size() > 0) {      continue;    }    String strExposedPorts=c.getValue().getExposedPorts();    if (null != strExposedPorts && !strExposedPorts.isEmpty()) {      Gson gson=new Gson();
private int updateCompletedContainers(List<ContainerStatus> completedContainers,Resource releasedResources,NodeId nodeId,SchedulerNode schedulerNode){  int releasedContainers=0;  List<ContainerId> untrackedContainerIdList=new ArrayList<ContainerId>();  for (  ContainerStatus completedContainer : completedContainers) {    ContainerId containerId=completedContainer.getContainerId();
protected void nodeUpdate(RMNode nm){
  LOG.debug("nodeUpdate: {} cluster capacity: {}",nm,getClusterResource());  SchedulerNode schedulerNode=getNode(nm.getNodeID());  List<ContainerStatus> completedContainers=updateNewContainerInfo(nm,schedulerNode);  if (schedulerNode != null) {    schedulerNode.notifyNodeUpdate();  }  Resource releasedResources=Resource.newInstance(0,0);  int releasedContainers=updateCompletedContainers(completedContainers,releasedResources,nm.getNodeID(),schedulerNode);  if (nm.getState() == NodeState.DECOMMISSIONING && schedulerNode != null && schedulerNode.getTotalResource().compareTo(schedulerNode.getAllocatedResource()) != 0) {    this.rmContext.getDispatcher().getEventHandler().handle(new RMNodeResourceUpdateEvent(nm.getNodeID(),ResourceOption.newInstance(schedulerNode.getAllocatedResource(),0)));  }  updateSchedulerHealthInformation(releasedResources,releasedContainers);  if (schedulerNode != null) {    updateNodeResourceUtilization(nm,schedulerNode);  }  if (schedulerNode != null) {    signalContainersIfOvercommitted(schedulerNode,true);  }  if (LOG.isDebugEnabled()) {
  if (kill) {    eventType=SchedulerEventType.MARK_CONTAINER_FOR_KILLABLE;    if (!schedulerNode.isOvercommitTimedOut()) {      return;    }  }  ResourceCalculator rc=getResourceCalculator();  Resource unallocated=Resource.newInstance(schedulerNode.getUnallocatedResource());  if (Resources.fitsIn(rc,ZERO_RESOURCE,unallocated)) {    return;  }  LOG.info("{} is overcommitted ({}), preempt/kill containers",schedulerNode.getNodeID(),unallocated);  for (  RMContainer container : schedulerNode.getContainersToKill()) {    LOG.info("Send {} to {} to free up {}",eventType,container.getContainerId(),container.getAllocatedResource());    ApplicationAttemptId appId=container.getApplicationAttemptId();    ContainerPreemptEvent event=new ContainerPreemptEvent(appId,container,eventType);    this.rmContext.getDispatcher().getEventHandler().handle(event);    Resources.addTo(unallocated,container.getAllocatedResource());
protected void handleContainerUpdates(SchedulerApplicationAttempt appAttempt,ContainerUpdates updates){  List<UpdateContainerRequest> promotionRequests=updates.getPromotionRequests();  if (promotionRequests != null && !promotionRequests.isEmpty()) {
protected void handleContainerUpdates(SchedulerApplicationAttempt appAttempt,ContainerUpdates updates){  List<UpdateContainerRequest> promotionRequests=updates.getPromotionRequests();  if (promotionRequests != null && !promotionRequests.isEmpty()) {    LOG.info("Promotion Update requests : " + promotionRequests);    handleIncreaseRequests(appAttempt,promotionRequests);  }  List<UpdateContainerRequest> increaseRequests=updates.getIncreaseRequests();  if (increaseRequests != null && !increaseRequests.isEmpty()) {    LOG.info("Resource increase requests : " + increaseRequests);    handleIncreaseRequests(appAttempt,increaseRequests);  }  List<UpdateContainerRequest> demotionRequests=updates.getDemotionRequests();  if (demotionRequests != null && !demotionRequests.isEmpty()) {    LOG.info("Demotion Update requests : " + demotionRequests);    handleDecreaseRequests(appAttempt,demotionRequests);  }  List<UpdateContainerRequest> decreaseRequests=updates.getDecreaseRequests();  if (decreaseRequests != null && !decreaseRequests.isEmpty()) {
protected void rollbackContainerUpdate(ContainerId containerId){  RMContainer rmContainer=getRMContainer(containerId);  if (rmContainer == null) {
public Resource getMinimumAllocation(){  Resource ret=ResourceUtils.getResourceTypesMinimumAllocation();
public Resource getMaximumAllocation(){  Resource ret=ResourceUtils.getResourceTypesMaximumAllocation();
@Lock({Queue.class,SchedulerApplicationAttempt.class}) @Override synchronized public void activateApplication(String user,ApplicationId applicationId){  Set<ApplicationId> userApps=usersApplications.get(user);  if (userApps == null) {    userApps=new HashSet<ApplicationId>();    usersApplications.put(user,userApps);    ++activeUsers;    metrics.incrActiveUsers();
public static void updateMetrics(ApplicationId applicationId,NodeType type,SchedulerNode node,RMContainer containerAllocated,String user,Queue queue){
public static ConfigurationMutationACLPolicy getPolicy(Configuration conf){  Class<? extends ConfigurationMutationACLPolicy> policyClass=conf.getClass(YarnConfiguration.RM_SCHEDULER_MUTATION_ACL_POLICY_CLASS,DefaultConfigurationMutationACLPolicy.class,ConfigurationMutationACLPolicy.class);
@SuppressWarnings("unchecked") public boolean canDelete(String queueName){  SchedulerQueue<T> queue=queueManager.getQueue(queueName);  if (queue == null) {
public static boolean isPlaceBlacklisted(SchedulerApplicationAttempt application,SchedulerNode node,Logger log){  if (application.isPlaceBlacklisted(node.getNodeName())) {
public void addRMContainer(ContainerId id,RMContainer rmContainer){  writeLock.lock();  try {    if (!getApplicationAttemptId().equals(rmContainer.getApplicationAttemptId()) && !liveContainers.containsKey(id)) {
private static void normalizeNodeLabelExpressionInRequest(ResourceRequest resReq,QueueInfo queueInfo){  String labelExp=resReq.getNodeLabelExpression();  if (LOG.isDebugEnabled()) {
private static void normalizeNodeLabelExpressionInRequest(ResourceRequest resReq,QueueInfo queueInfo){  String labelExp=resReq.getNodeLabelExpression();  if (LOG.isDebugEnabled()) {    LOG.debug("Requested Node Label Expression : " + labelExp);
private static boolean checkResource(ResourceInformation requestedRI,Resource availableResource){  final ResourceInformation availableRI=availableResource.getResourceInformation(requestedRI.getName());  long requestedResourceValue=requestedRI.getValue();  long availableResourceValue=availableRI.getValue();  int unitsRelation=UnitsConversionUtil.compareUnits(requestedRI.getUnits(),availableRI.getUnits());  if (LOG.isDebugEnabled()) {
private static boolean checkResource(ResourceInformation requestedRI,Resource availableResource){  final ResourceInformation availableRI=availableResource.getResourceInformation(requestedRI.getName());  long requestedResourceValue=requestedRI.getValue();  long availableResourceValue=availableRI.getValue();  int unitsRelation=UnitsConversionUtil.compareUnits(requestedRI.getUnits(),availableRI.getUnits());  if (LOG.isDebugEnabled()) {    LOG.debug("Requested resource information: " + requestedRI);
private static boolean checkResource(ResourceInformation requestedRI,Resource availableResource){  final ResourceInformation availableRI=availableResource.getResourceInformation(requestedRI.getName());  long requestedResourceValue=requestedRI.getValue();  long availableResourceValue=availableRI.getValue();  int unitsRelation=UnitsConversionUtil.compareUnits(requestedRI.getUnits(),availableRI.getUnits());  if (LOG.isDebugEnabled()) {    LOG.debug("Requested resource information: " + requestedRI);    LOG.debug("Available resource information: " + availableRI);
private static boolean checkResource(ResourceInformation requestedRI,Resource availableResource){  final ResourceInformation availableRI=availableResource.getResourceInformation(requestedRI.getName());  long requestedResourceValue=requestedRI.getValue();  long availableResourceValue=availableRI.getValue();  int unitsRelation=UnitsConversionUtil.compareUnits(requestedRI.getUnits(),availableRI.getUnits());  if (LOG.isDebugEnabled()) {    LOG.debug("Requested resource information: " + requestedRI);    LOG.debug("Available resource information: " + availableRI);    LOG.debug("Relation of units: " + unitsRelation);  }  if (unitsRelation < 0) {    availableResourceValue=UnitsConversionUtil.convert(availableRI.getUnits(),requestedRI.getUnits(),availableRI.getValue());  } else   if (unitsRelation > 0) {    requestedResourceValue=UnitsConversionUtil.convert(requestedRI.getUnits(),availableRI.getUnits(),requestedRI.getValue());  }  if (LOG.isDebugEnabled()) {    LOG.debug("Requested resource value after conversion: " + requestedResourceValue);
    return;  }  if (rmContext.getScheduler() instanceof CapacityScheduler) {    CapacityScheduler cs=(CapacityScheduler)rmContext.getScheduler();    if (!cs.isMultiNodePlacementEnabled()) {      int numNodes=rmContext.getRMNodes().size();      int newAppActivitiesMaxQueueLength;      int numAsyncSchedulerThreads=cs.getNumAsyncSchedulerThreads();      if (numAsyncSchedulerThreads > 0) {        newAppActivitiesMaxQueueLength=Math.max(configuredAppActivitiesMaxQueueLength,numNodes * numAsyncSchedulerThreads);      } else {        newAppActivitiesMaxQueueLength=Math.max(configuredAppActivitiesMaxQueueLength,(int)(numNodes * 1.2));      }      if (appActivitiesMaxQueueLength != newAppActivitiesMaxQueueLength) {        LOG.info("Update max queue length of app activities from {} to {}," + " configured={}, numNodes={}, numAsyncSchedulerThreads={}" + " when multi-node placement disabled.",appActivitiesMaxQueueLength,newAppActivitiesMaxQueueLength,configuredAppActivitiesMaxQueueLength,numNodes,numAsyncSchedulerThreads);        appActivitiesMaxQueueLength=newAppActivitiesMaxQueueLength;      }    } else     if (appActivitiesMaxQueueLength != configuredAppActivitiesMaxQueueLength) {
          }        }        Iterator<Map.Entry<ApplicationId,Queue<AppAllocation>>> iteApp=completedAppAllocations.entrySet().iterator();        while (iteApp.hasNext()) {          Map.Entry<ApplicationId,Queue<AppAllocation>> appAllocation=iteApp.next();          RMApp rmApp=rmContext.getRMApps().get(appAllocation.getKey());          if (rmApp == null || rmApp.getFinalApplicationStatus() != FinalApplicationStatus.UNDEFINED) {            iteApp.remove();          } else {            Iterator<AppAllocation> appActivitiesIt=appAllocation.getValue().iterator();            while (appActivitiesIt.hasNext()) {              if (curTS - appActivitiesIt.next().getTime() > appActivitiesTTL) {                appActivitiesIt.remove();              } else {                break;              }            }            if (appAllocation.getValue().isEmpty()) {              iteApp.remove();
        Iterator<Map.Entry<ApplicationId,Queue<AppAllocation>>> iteApp=completedAppAllocations.entrySet().iterator();        while (iteApp.hasNext()) {          Map.Entry<ApplicationId,Queue<AppAllocation>> appAllocation=iteApp.next();          RMApp rmApp=rmContext.getRMApps().get(appAllocation.getKey());          if (rmApp == null || rmApp.getFinalApplicationStatus() != FinalApplicationStatus.UNDEFINED) {            iteApp.remove();          } else {            Iterator<AppAllocation> appActivitiesIt=appAllocation.getValue().iterator();            while (appActivitiesIt.hasNext()) {              if (curTS - appActivitiesIt.next().getTime() > appActivitiesTTL) {                appActivitiesIt.remove();              } else {                break;              }            }            if (appAllocation.getValue().isEmpty()) {              iteApp.remove();
protected void updateConfigurableResourceRequirement(String queuePath,Resource clusterResource){  CapacitySchedulerConfiguration conf=csContext.getConfiguration();  Set<String> configuredNodelabels=conf.getConfiguredNodeLabels(queuePath);  for (  String label : configuredNodelabels) {    Resource minResource=getMinimumAbsoluteResource(queuePath,label);    Resource maxResource=getMaximumAbsoluteResource(queuePath,label);    LOG.debug("capacityConfigType is '{}' for queue {}",capacityConfigType,getQueuePath());    CapacityConfigType localType=checkConfigTypeIsAbsoluteResource(queuePath,label) ? CapacityConfigType.ABSOLUTE_RESOURCE : CapacityConfigType.PERCENTAGE;    if (this.capacityConfigType.equals(CapacityConfigType.NONE)) {      this.capacityConfigType=localType;
  try {    Resource currentLimitResource=getCurrentLimitResource(nodePartition,clusterResource,currentResourceLimits,schedulingMode);    Resource nowTotalUsed=queueUsage.getUsed(nodePartition);    Resource usedExceptKillable=nowTotalUsed;    if (hasChildQueues()) {      usedExceptKillable=Resources.subtract(nowTotalUsed,getTotalKillableResource(nodePartition));    }    currentResourceLimits.setHeadroom(Resources.subtract(currentLimitResource,usedExceptKillable));    if (Resources.greaterThanOrEqual(resourceCalculator,clusterResource,usedExceptKillable,currentLimitResource)) {      if (this.reservationsContinueLooking && Resources.greaterThan(resourceCalculator,clusterResource,resourceCouldBeUnreserved,Resources.none())) {        Resource newTotalWithoutReservedResource=Resources.subtract(usedExceptKillable,resourceCouldBeUnreserved);        if (Resources.lessThan(resourceCalculator,clusterResource,newTotalWithoutReservedResource,currentLimitResource)) {          if (LOG.isDebugEnabled()) {            LOG.debug("try to use reserved: " + getQueuePath() + " usedResources: "+ queueUsage.getUsed()+ ", clusterResources: "+ clusterResource+ ", reservedResources: "+ resourceCouldBeUnreserved+ ", capacity-without-reserved: "+ newTotalWithoutReservedResource+ ", maxLimitCapacity: "+ currentLimitResource);          }          return true;        }      }      if (LOG.isDebugEnabled()) {
    Resource usedExceptKillable=nowTotalUsed;    if (hasChildQueues()) {      usedExceptKillable=Resources.subtract(nowTotalUsed,getTotalKillableResource(nodePartition));    }    currentResourceLimits.setHeadroom(Resources.subtract(currentLimitResource,usedExceptKillable));    if (Resources.greaterThanOrEqual(resourceCalculator,clusterResource,usedExceptKillable,currentLimitResource)) {      if (this.reservationsContinueLooking && Resources.greaterThan(resourceCalculator,clusterResource,resourceCouldBeUnreserved,Resources.none())) {        Resource newTotalWithoutReservedResource=Resources.subtract(usedExceptKillable,resourceCouldBeUnreserved);        if (Resources.lessThan(resourceCalculator,clusterResource,newTotalWithoutReservedResource,currentLimitResource)) {          if (LOG.isDebugEnabled()) {            LOG.debug("try to use reserved: " + getQueuePath() + " usedResources: "+ queueUsage.getUsed()+ ", clusterResources: "+ clusterResource+ ", reservedResources: "+ resourceCouldBeUnreserved+ ", capacity-without-reserved: "+ newTotalWithoutReservedResource+ ", maxLimitCapacity: "+ currentLimitResource);          }          return true;        }      }      if (LOG.isDebugEnabled()) {        LOG.debug("Failed to assign to queue: " + getQueuePath() + " nodePatrition: "+ nodePartition+ ", usedResources: "+ queueUsage.getUsed(nodePartition)+ ", clusterResources: "+ clusterResource+ ", reservedResources: "+ resourceCouldBeUnreserved+ ", maxLimitCapacity: "+ currentLimitResource+ ", currTotalUsed:"+ usedExceptKillable);      }      return false;    }    if (LOG.isDebugEnabled()) {
  ContainerAllocationProposal<FiCaSchedulerApp,FiCaSchedulerNode> allocation=request.getFirstAllocatedOrReservedContainer();  SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> schedulerContainer=allocation.getAllocatedOrReservedContainer();  if (allocation.getAllocateFromReservedContainer() == null) {    Resource required=allocation.getAllocatedOrReservedResource();    Resource netAllocated=Resources.subtract(required,request.getTotalReleasedResource());    readLock.lock();    try {      String partition=schedulerContainer.getNodePartition();      Resource maxResourceLimit;      if (allocation.getSchedulingMode() == SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY) {        maxResourceLimit=getQueueMaxResource(partition);      } else {        maxResourceLimit=labelManager.getResourceByLabel(schedulerContainer.getNodePartition(),cluster);      }      if (!Resources.fitsIn(resourceCalculator,Resources.add(queueUsage.getUsed(partition),netAllocated),maxResourceLimit)) {        if (LOG.isDebugEnabled()) {
private boolean exceedQueueMaxParallelApps(AbstractCSQueue queue){  while (queue != null) {    if (queue.getNumRunnableApps() >= queue.getMaxParallelApps()) {
    FiCaSchedulerApp next=iter.next();    if (next == prev) {      continue;    }    if (checkRunnabilityWithUpdate(next)) {      LeafQueue nextQueue=next.getCSLeafQueue();      LOG.info("{} is now runnable in {}",next.getApplicationAttemptId(),nextQueue);      trackRunnableApp(next);      FiCaSchedulerApp appSched=next;      nextQueue.submitApplicationAttempt(next,next.getUser());      noLongerPendingApps.add(appSched);      if (noLongerPendingApps.size() >= maxRunnableApps) {        break;      }    }    prev=next;  }  for (  FiCaSchedulerApp appSched : noLongerPendingApps) {    if (!(appSched.getCSLeafQueue().removeNonRunnableApp(appSched))) {
      continue;    }    if (checkRunnabilityWithUpdate(next)) {      LeafQueue nextQueue=next.getCSLeafQueue();      LOG.info("{} is now runnable in {}",next.getApplicationAttemptId(),nextQueue);      trackRunnableApp(next);      FiCaSchedulerApp appSched=next;      nextQueue.submitApplicationAttempt(next,next.getUser());      noLongerPendingApps.add(appSched);      if (noLongerPendingApps.size() >= maxRunnableApps) {        break;      }    }    prev=next;  }  for (  FiCaSchedulerApp appSched : noLongerPendingApps) {    if (!(appSched.getCSLeafQueue().removeNonRunnableApp(appSched))) {      LOG.error("Can't make app runnable that does not already exist in queue" + " as non-runnable: {}. This should never happen.",appSched.getApplicationAttemptId());    }    if (!usersNonRunnableApps.remove(appSched.getUser(),appSched)) {
scheduleAsynchronously=this.conf.getScheduleAynschronously();asyncScheduleInterval=this.conf.getLong(ASYNC_SCHEDULER_INTERVAL,DEFAULT_ASYNC_SCHEDULER_INTERVAL);this.assignMultipleEnabled=this.conf.getAssignMultipleEnabled();this.maxAssignPerHeartbeat=this.conf.getMaxAssignPerHeartbeat();int maxAsyncSchedulingThreads=this.conf.getInt(CapacitySchedulerConfiguration.SCHEDULE_ASYNCHRONOUSLY_MAXIMUM_THREAD,1);maxAsyncSchedulingThreads=Math.max(maxAsyncSchedulingThreads,1);if (scheduleAsynchronously) {asyncSchedulerThreads=new ArrayList<>();for (int i=0; i < maxAsyncSchedulingThreads; i++) {asyncSchedulerThreads.add(new AsyncScheduleThread(this));}resourceCommitterService=new ResourceCommitterService(this);asyncMaxPendingBacklogs=this.conf.getInt(CapacitySchedulerConfiguration.SCHEDULE_ASYNCHRONOUSLY_MAXIMUM_PENDING_BACKLOGS,CapacitySchedulerConfiguration.DEFAULT_SCHEDULE_ASYNCHRONOUSLY_MAXIMUM_PENDING_BACKLOGS);}offswitchPerHeartbeatLimit=this.conf.getOffSwitchPerHeartbeatLimit();multiNodePlacementEnabled=this.conf.getMultiNodePlacementEnabled();if (rmContext.getMultiNodeSortingManager() != null) {
    CSQueue queue=getOrCreateQueueFromPlacementContext(applicationId,user,queueName,placementContext,true);    if (queue == null) {      if (!getConfiguration().shouldAppFailFast(getConfig())) {        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.KILL,"Application killed on recovery as it" + " was submitted to queue " + queueName + " which no longer exists after restart."));        return;      } else {        String queueErrorMsg="Queue named " + queueName + " missing "+ "during application recovery."+ " Queue removal during recovery is not presently "+ "supported by the capacity scheduler, please "+ "restart with all queues configured"+ " which were present before shutdown/restart.";        LOG.error(FATAL,queueErrorMsg);        throw new QueueInvalidException(queueErrorMsg);      }    }    if (!(queue instanceof LeafQueue)) {      if (!getConfiguration().shouldAppFailFast(getConfig())) {        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.KILL,"Application killed on recovery as it was " + "submitted to queue " + queueName + " which is no longer a leaf queue after restart."));        return;      } else {        String queueErrorMsg="Queue named " + queueName + " is no longer a leaf queue during application recovery."+ " Changing a leaf queue to a parent queue during recovery is"+ " not presently supported by the capacity scheduler. Please"+ " restart with leaf queues before shutdown/restart continuing"+ " as leaf queues.";
      }    }    if (!(queue instanceof LeafQueue)) {      if (!getConfiguration().shouldAppFailFast(getConfig())) {        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.KILL,"Application killed on recovery as it was " + "submitted to queue " + queueName + " which is no longer a leaf queue after restart."));        return;      } else {        String queueErrorMsg="Queue named " + queueName + " is no longer a leaf queue during application recovery."+ " Changing a leaf queue to a parent queue during recovery is"+ " not presently supported by the capacity scheduler. Please"+ " restart with leaf queues before shutdown/restart continuing"+ " as leaf queues.";        LOG.error(FATAL,queueErrorMsg);        throw new QueueInvalidException(queueErrorMsg);      }    }    if (queue.getState() == QueueState.STOPPED) {      ((LeafQueue)queue).recoverDrainingState();    }    try {      queue.submitApplication(applicationId,user,queueName);    } catch (    AccessControlException ace) {    }    queue.getMetrics().submitApp(user);    SchedulerApplication<FiCaSchedulerApp> application=new SchedulerApplication<FiCaSchedulerApp>(queue,user,priority);
    }    if (!(queue instanceof LeafQueue)) {      if (!getConfiguration().shouldAppFailFast(getConfig())) {        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.KILL,"Application killed on recovery as it was " + "submitted to queue " + queueName + " which is no longer a leaf queue after restart."));        return;      } else {        String queueErrorMsg="Queue named " + queueName + " is no longer a leaf queue during application recovery."+ " Changing a leaf queue to a parent queue during recovery is"+ " not presently supported by the capacity scheduler. Please"+ " restart with leaf queues before shutdown/restart continuing"+ " as leaf queues.";        LOG.error(FATAL,queueErrorMsg);        throw new QueueInvalidException(queueErrorMsg);      }    }    if (queue.getState() == QueueState.STOPPED) {      ((LeafQueue)queue).recoverDrainingState();    }    try {      queue.submitApplication(applicationId,user,queueName);    } catch (    AccessControlException ace) {    }    queue.getMetrics().submitApp(user);    SchedulerApplication<FiCaSchedulerApp> application=new SchedulerApplication<FiCaSchedulerApp>(queue,user,priority);
  CSQueue queue=getQueue(queueName);  if (queue == null) {    if (placementContext != null && placementContext.hasParentQueue()) {      try {        return autoCreateLeafQueue(placementContext);      } catch (      YarnException|IOException e) {        if (isRecovery) {          if (!getConfiguration().shouldAppFailFast(getConfig())) {            LOG.error("Could not auto-create leaf queue " + queueName + " due to : ",e);            this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.KILL,"Application killed on recovery" + " as it was submitted to queue " + queueName + " which could not be auto-created"));          } else {            String queueErrorMsg="Queue named " + queueName + " could not be "+ "auto-created during application recovery.";            LOG.error(FATAL,queueErrorMsg,e);            throw new QueueInvalidException(queueErrorMsg);          }        } else {
      return;    } else     if (queue instanceof AutoCreatedLeafQueue && queue.getParent() instanceof ManagedParentQueue) {      if (placementContext == null) {        String message="Application " + applicationId + " submission by user : "+ user+ " to specified queue : "+ queueName+ "  is prohibited. "+ "Verify automatic queue mapping for user exists in "+ QUEUE_MAPPING;        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.APP_REJECTED,message));        return;      } else       if (!queue.getParent().getQueueShortName().equals(placementContext.getParentQueue()) && !queue.getParent().getQueuePath().equals(placementContext.getParentQueue())) {        String message="Auto created Leaf queue " + placementContext.getQueue() + " "+ "already exists under queue : "+ queue.getParent().getQueueShortName()+ ". But Queue mapping configuration "+ CapacitySchedulerConfiguration.QUEUE_MAPPING+ " has been "+ "updated to a different parent queue : "+ placementContext.getParentQueue()+ " for the specified user : "+ user;        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.APP_REJECTED,message));        return;      }    }    try {      priority=workflowPriorityMappingsMgr.mapWorkflowPriorityForApp(applicationId,queue,user,priority);    } catch (    YarnException e) {      String message="Failed to submit application " + applicationId + " submitted by user "+ user+ " reason: "+ e.getMessage();      this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.APP_REJECTED,message));
        return;      } else       if (!queue.getParent().getQueueShortName().equals(placementContext.getParentQueue()) && !queue.getParent().getQueuePath().equals(placementContext.getParentQueue())) {        String message="Auto created Leaf queue " + placementContext.getQueue() + " "+ "already exists under queue : "+ queue.getParent().getQueueShortName()+ ". But Queue mapping configuration "+ CapacitySchedulerConfiguration.QUEUE_MAPPING+ " has been "+ "updated to a different parent queue : "+ placementContext.getParentQueue()+ " for the specified user : "+ user;        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.APP_REJECTED,message));        return;      }    }    try {      priority=workflowPriorityMappingsMgr.mapWorkflowPriorityForApp(applicationId,queue,user,priority);    } catch (    YarnException e) {      String message="Failed to submit application " + applicationId + " submitted by user "+ user+ " reason: "+ e.getMessage();      this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId,RMAppEventType.APP_REJECTED,message));      return;    }    try {      queue.submitApplication(applicationId,user,queueName);    } catch (    AccessControlException ace) {      LOG.info("Failed to submit application " + applicationId + " to queue "+ queueName+ " from user "+ user,ace);
  writeLock.lock();  try {    SchedulerApplication<FiCaSchedulerApp> application=applications.get(applicationAttemptId.getApplicationId());    if (application == null) {      LOG.warn("Application " + applicationAttemptId.getApplicationId() + " cannot be found in scheduler.");      return;    }    CSQueue queue=(CSQueue)application.getQueue();    FiCaSchedulerApp attempt=new FiCaSchedulerApp(applicationAttemptId,application.getUser(),queue,queue.getAbstractUsersManager(),rmContext,application.getPriority(),isAttemptRecovering,activitiesManager);    if (transferStateFromPreviousAttempt) {      attempt.transferStateFromPreviousAttempt(application.getCurrentAppAttempt());    }    application.setCurrentAppAttempt(attempt);    attempt.setPriority(application.getPriority());    maxRunningEnforcer.checkRunnabilityWithUpdate(attempt);    maxRunningEnforcer.trackApp(attempt);    queue.submitApplicationAttempt(attempt,application.getUser());
    SchedulerApplication<FiCaSchedulerApp> application=applications.get(applicationAttemptId.getApplicationId());    if (application == null) {      LOG.warn("Application " + applicationAttemptId.getApplicationId() + " cannot be found in scheduler.");      return;    }    CSQueue queue=(CSQueue)application.getQueue();    FiCaSchedulerApp attempt=new FiCaSchedulerApp(applicationAttemptId,application.getUser(),queue,queue.getAbstractUsersManager(),rmContext,application.getPriority(),isAttemptRecovering,activitiesManager);    if (transferStateFromPreviousAttempt) {      attempt.transferStateFromPreviousAttempt(application.getCurrentAppAttempt());    }    application.setCurrentAppAttempt(attempt);    attempt.setPriority(application.getPriority());    maxRunningEnforcer.checkRunnabilityWithUpdate(attempt);    maxRunningEnforcer.trackApp(attempt);    queue.submitApplicationAttempt(attempt,application.getUser());    LOG.info("Added Application Attempt " + applicationAttemptId + " to scheduler from user "+ application.getUser()+ " in queue "+ queue.getQueuePath());    if (isAttemptRecovering) {
private void doneApplicationAttempt(ApplicationAttemptId applicationAttemptId,RMAppAttemptState rmAppAttemptFinalState,boolean keepContainers){  writeLock.lock();  try {
private void doneApplicationAttempt(ApplicationAttemptId applicationAttemptId,RMAppAttemptState rmAppAttemptFinalState,boolean keepContainers){  writeLock.lock();  try {    LOG.info("Application Attempt " + applicationAttemptId + " is done."+ " finalState="+ rmAppAttemptFinalState);    FiCaSchedulerApp attempt=getApplicationAttempt(applicationAttemptId);    SchedulerApplication<FiCaSchedulerApp> application=applications.get(applicationAttemptId.getApplicationId());    if (application == null || attempt == null) {
    SchedulerApplication<FiCaSchedulerApp> application=applications.get(applicationAttemptId.getApplicationId());    if (application == null || attempt == null) {      LOG.info("Unknown application " + applicationAttemptId + " has completed!");      return;    }    for (    RMContainer rmContainer : attempt.getLiveContainers()) {      if (keepContainers && rmContainer.getState().equals(RMContainerState.RUNNING)) {        LOG.info("Skip killing " + rmContainer.getContainerId());        continue;      }      super.completedContainer(rmContainer,SchedulerUtils.createAbnormalContainerStatus(rmContainer.getContainerId(),SchedulerUtils.COMPLETED_APPLICATION),RMContainerEventType.KILL);    }    for (    RMContainer rmContainer : attempt.getReservedContainers()) {      super.completedContainer(rmContainer,SchedulerUtils.createAbnormalContainerStatus(rmContainer.getContainerId(),"Application Complete"),RMContainerEventType.KILL);    }    attempt.stop(rmAppAttemptFinalState);    Queue queue=attempt.getQueue();    CSQueue csQueue=(CSQueue)queue;    if (!(csQueue instanceof LeafQueue)) {
@Override @Lock(Lock.NoLock.class) public Allocation allocate(ApplicationAttemptId applicationAttemptId,List<ResourceRequest> ask,List<SchedulingRequest> schedulingRequests,List<ContainerId> release,List<String> blacklistAdditions,List<String> blacklistRemovals,ContainerUpdates updateRequests){  FiCaSchedulerApp application=getApplicationAttempt(applicationAttemptId);  if (application == null) {
  }  if (!application.getApplicationAttemptId().equals(applicationAttemptId)) {    LOG.error("Calling allocate on previous or removed " + "or non existent application attempt " + applicationAttemptId);    return EMPTY_ALLOCATION;  }  handleContainerUpdates(application,updateRequests);  releaseContainers(release,application);  LeafQueue updateDemandForQueue=null;  normalizeResourceRequests(ask);  normalizeSchedulingRequests(schedulingRequests);  Allocation allocation;  application.getWriteLock().lock();  try {    if (application.isStopped()) {      return EMPTY_ALLOCATION;    }    if (!ask.isEmpty() || (schedulingRequests != null && !schedulingRequests.isEmpty())) {      if (LOG.isDebugEnabled()) {
      return EMPTY_ALLOCATION;    }    if (!ask.isEmpty() || (schedulingRequests != null && !schedulingRequests.isEmpty())) {      if (LOG.isDebugEnabled()) {        LOG.debug("allocate: pre-update " + applicationAttemptId + " ask size ="+ ask.size());        application.showRequests();      }      if (application.updateResourceRequests(ask) || application.updateSchedulingRequests(schedulingRequests)) {        updateDemandForQueue=(LeafQueue)application.getQueue();      }      if (LOG.isDebugEnabled()) {        LOG.debug("allocate: post-update");        application.showRequests();      }    }    application.updateBlacklist(blacklistAdditions,blacklistRemovals);    allocation=application.getAllocation(getResourceCalculator(),getClusterResource(),getMinimumResourceCapability());  }  finally {    application.getWriteLock().unlock();  }  if (updateDemandForQueue != null && !application.isWaitingForAMContainer()) {
    int assignedContainers=0;    CandidateNodeSet<FiCaSchedulerNode> candidates=getCandidateNodeSet(node);    CSAssignment assignment=allocateContainersToNode(candidates,withNodeHeartbeat);    if (null != assignment && withNodeHeartbeat) {      if (assignment.getType() == NodeType.OFF_SWITCH) {        offswitchCount++;      }      if (Resources.greaterThan(calculator,getClusterResource(),assignment.getResource(),Resources.none())) {        assignedContainers++;      }      while (canAllocateMore(assignment,offswitchCount,assignedContainers)) {        assignment=allocateContainersToNode(candidates,true);        if (null != assignment && assignment.getType() == NodeType.OFF_SWITCH) {          offswitchCount++;        }        if (null != assignment && Resources.greaterThan(calculator,getClusterResource(),assignment.getResource(),Resources.none())) {          assignedContainers++;        }      }      if (offswitchCount >= offswitchPerHeartbeatLimit) {
private CSAssignment allocateContainerOnSingleNode(CandidateNodeSet<FiCaSchedulerNode> candidates,FiCaSchedulerNode node,boolean withNodeHeartbeat){
private CSAssignment allocateContainerOnSingleNode(CandidateNodeSet<FiCaSchedulerNode> candidates,FiCaSchedulerNode node,boolean withNodeHeartbeat){  LOG.debug("Trying to schedule on node: {}, available: {}",node.getNodeName(),node.getUnallocatedResource());  if (getNode(node.getNodeID()) != node) {
private void allocateFromReservedContainer(FiCaSchedulerNode node,boolean withNodeHeartbeat,RMContainer reservedContainer){  FiCaSchedulerApp reservedApplication=getCurrentAttemptForContainer(reservedContainer.getContainerId());  if (reservedApplication == null) {
@Override protected void completedContainerInternal(RMContainer rmContainer,ContainerStatus containerStatus,RMContainerEventType event){  Container container=rmContainer.getContainer();  ContainerId containerId=container.getId();  FiCaSchedulerApp application=getCurrentAttemptForContainer(container.getId());  ApplicationId appId=containerId.getApplicationAttemptId().getApplicationId();  if (application == null) {
@Override public void killReservedContainer(RMContainer container){
@Override public void markContainerForPreemption(ApplicationAttemptId aid,RMContainer cont){
public void markContainerForKillable(RMContainer killableContainer){  writeLock.lock();  try {
private void markContainerForNonKillable(RMContainer nonKillableContainer){  writeLock.lock();  try {
@Override public boolean checkAccess(UserGroupInformation callerUGI,QueueACL acl,String queueName){  CSQueue queue=getQueue(queueName);  if (queue == null) {
@Override public void removeQueue(String queueName) throws SchedulerDynamicEditException {  writeLock.lock();  try {
 catch (    AccessControlException e) {      throw new YarnException(e);    }    FiCaSchedulerApp app=application.getCurrentAppAttempt();    if (app != null) {      for (      RMContainer rmContainer : app.getLiveContainers()) {        source.detachContainer(getClusterResource(),app,rmContainer);        dest.attachContainer(getClusterResource(),app,rmContainer);      }      for (      RMContainer rmContainer : app.getReservedContainers()) {        source.detachContainer(getClusterResource(),app,rmContainer);        dest.attachContainer(getClusterResource(),app,rmContainer);      }      if (!app.isStopped()) {        source.finishApplicationAttempt(app,sourceQueueName);        dest.submitApplicationAttempt(app,user,true);      }      app.move(dest);    }    source.appFinished();
    SchedulerApplication<FiCaSchedulerApp> application=applications.get(applicationId);    if (application == null) {      throw new YarnException("Application '" + applicationId + "' is not present, hence could not change priority.");    }    RMApp rmApp=rmContext.getRMApps().get(applicationId);    appPriority=checkAndGetApplicationPriority(newPriority,user,rmApp.getQueue(),applicationId);    if (application.getPriority().equals(appPriority)) {      future.set(null);      return appPriority;    }    rmApp.getApplicationSubmissionContext().setPriority(appPriority);    ApplicationStateData appState=ApplicationStateData.newInstance(rmApp.getSubmitTime(),rmApp.getStartTime(),rmApp.getApplicationSubmissionContext(),rmApp.getUser(),rmApp.getCallerContext());    appState.setApplicationTimeouts(rmApp.getApplicationTimeouts());    appState.setLaunchTime(rmApp.getLaunchTime());    rmContext.getStateStore().updateApplicationStateSynchronously(appState,false,future);    LeafQueue queue=(LeafQueue)getQueue(rmApp.getQueue());    queue.updateApplicationPriority(application,appPriority);
      attemptId=request.getContainersToRelease().get(0).getSchedulerApplicationAttempt().getApplicationAttemptId();    }  }  LOG.debug("Try to commit allocation proposal={}",request);  boolean isSuccess=false;  if (attemptId != null) {    FiCaSchedulerApp app=getApplicationAttempt(attemptId);    if (app != null && attemptId.equals(app.getApplicationAttemptId())) {      if (app.accept(cluster,request,updatePending) && app.apply(cluster,request,updatePending)) {        long commitSuccess=System.nanoTime() - commitStart;        CapacitySchedulerMetrics.getMetrics().addCommitSuccess(commitSuccess);        LOG.info("Allocation proposal accepted");        isSuccess=true;      } else {        long commitFailed=System.nanoTime() - commitStart;        CapacitySchedulerMetrics.getMetrics().addCommitFailure(commitFailed);        LOG.info("Failed to accept allocation proposal");
public boolean moveReservedContainer(RMContainer toBeMovedContainer,FiCaSchedulerNode targetNode){  writeLock.lock();  try {
public boolean moveReservedContainer(RMContainer toBeMovedContainer,FiCaSchedulerNode targetNode){  writeLock.lock();  try {    LOG.debug("Trying to move container={} to node={}",toBeMovedContainer,targetNode.getNodeID());    FiCaSchedulerNode sourceNode=getNode(toBeMovedContainer.getNodeId());    if (null == sourceNode) {
  writeLock.lock();  try {    LOG.debug("Trying to move container={} to node={}",toBeMovedContainer,targetNode.getNodeID());    FiCaSchedulerNode sourceNode=getNode(toBeMovedContainer.getNodeId());    if (null == sourceNode) {      LOG.debug("Failed to move reservation, cannot find source node={}",toBeMovedContainer.getNodeId());      return false;    }    if (getNode(targetNode.getNodeID()) != targetNode) {      LOG.debug("Failed to move reservation, node updated or removed," + " moving cancelled.");      return false;    }    if (targetNode.getReservedContainer() != null) {      LOG.debug("Target node's reservation status changed," + " moving cancelled.");      return false;    }    FiCaSchedulerApp app=getApplicationAttempt(toBeMovedContainer.getApplicationAttemptId());    if (null == app) {
@Override public long getMaximumApplicationLifetime(String queueName){  CSQueue queue=getQueue(queueName);  if (queue == null || !(queue instanceof LeafQueue)) {    if (isAmbiguous(queueName)) {
public static void validateQueueHierarchy(CSQueueStore queues,CSQueueStore newQueues,CapacitySchedulerConfiguration newConf) throws IOException {  for (  CSQueue oldQueue : queues.getQueues()) {    if (!(AbstractAutoCreatedLeafQueue.class.isAssignableFrom(oldQueue.getClass()))) {      String queuePath=oldQueue.getQueuePath();      CSQueue newQueue=newQueues.get(queuePath);      String configPrefix=newConf.getQueuePrefix(oldQueue.getQueuePath());      String state=newConf.get(configPrefix + "state");      QueueState newQueueState=null;      if (state != null) {        try {          newQueueState=QueueState.valueOf(state);        } catch (        Exception ex) {          LOG.warn("Not a valid queue state for queue " + oldQueue.getQueuePath());        }      }      if (null == newQueue) {        if (oldQueue.getState() == QueueState.STOPPED || newQueueState == QueueState.STOPPED) {
        } catch (        Exception ex) {          LOG.warn("Not a valid queue state for queue " + oldQueue.getQueuePath());        }      }      if (null == newQueue) {        if (oldQueue.getState() == QueueState.STOPPED || newQueueState == QueueState.STOPPED) {          LOG.info("Deleting Queue " + queuePath + ", as it is not"+ " present in the modified capacity configuration xml");        } else {          throw new IOException(oldQueue.getQueuePath() + " cannot be" + " deleted from the capacity scheduler configuration, as the"+ " queue is not yet in stopped state. Current State : "+ oldQueue.getState());        }      } else       if (!oldQueue.getQueuePath().equals(newQueue.getQueuePath())) {        throw new IOException(queuePath + " is moved from:" + oldQueue.getQueuePath()+ " to:"+ newQueue.getQueuePath()+ " after refresh, which is not allowed.");      } else       if (oldQueue instanceof ParentQueue && !(oldQueue instanceof ManagedParentQueue) && newQueue instanceof ManagedParentQueue) {        throw new IOException("Can not convert parent queue: " + oldQueue.getQueuePath() + " to auto create enabled parent queue since "+ "it could have other pre-configured queues which is not "+ "supported");      } else       if (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue)) {        throw new IOException("Cannot convert auto create enabled parent queue: " + oldQueue.getQueuePath() + " to leaf queue. Please check "+ " parent queue's configuration "+ CapacitySchedulerConfiguration.AUTO_CREATE_CHILD_QUEUE_ENABLED+ " is set to true");      } else       if (oldQueue instanceof LeafQueue && newQueue instanceof ParentQueue) {        if (oldQueue.getState() == QueueState.STOPPED || newQueueState == QueueState.STOPPED) {
      }      if (null == newQueue) {        if (oldQueue.getState() == QueueState.STOPPED || newQueueState == QueueState.STOPPED) {          LOG.info("Deleting Queue " + queuePath + ", as it is not"+ " present in the modified capacity configuration xml");        } else {          throw new IOException(oldQueue.getQueuePath() + " cannot be" + " deleted from the capacity scheduler configuration, as the"+ " queue is not yet in stopped state. Current State : "+ oldQueue.getState());        }      } else       if (!oldQueue.getQueuePath().equals(newQueue.getQueuePath())) {        throw new IOException(queuePath + " is moved from:" + oldQueue.getQueuePath()+ " to:"+ newQueue.getQueuePath()+ " after refresh, which is not allowed.");      } else       if (oldQueue instanceof ParentQueue && !(oldQueue instanceof ManagedParentQueue) && newQueue instanceof ManagedParentQueue) {        throw new IOException("Can not convert parent queue: " + oldQueue.getQueuePath() + " to auto create enabled parent queue since "+ "it could have other pre-configured queues which is not "+ "supported");      } else       if (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue)) {        throw new IOException("Cannot convert auto create enabled parent queue: " + oldQueue.getQueuePath() + " to leaf queue. Please check "+ " parent queue's configuration "+ CapacitySchedulerConfiguration.AUTO_CREATE_CHILD_QUEUE_ENABLED+ " is set to true");      } else       if (oldQueue instanceof LeafQueue && newQueue instanceof ParentQueue) {        if (oldQueue.getState() == QueueState.STOPPED || newQueueState == QueueState.STOPPED) {          LOG.info("Converting the leaf queue: " + oldQueue.getQueuePath() + " to parent queue.");        } else {
public void setUserLimit(String queue,int userLimit){  setInt(getQueuePrefix(queue) + USER_LIMIT,userLimit);
public String[] getQueues(String queue){
public void setQueues(String queue,String[] subQueues){  set(getQueuePrefix(queue) + QUEUES,StringUtils.arrayToString(subQueues));
public void setReservable(String queue,boolean isReservable){  setBoolean(getQueuePrefix(queue) + IS_RESERVABLE,isReservable);
@Private protected AutoCreatedQueueManagementPolicy getAutoCreatedQueueManagementPolicyClass(String queueName){  String queueManagementPolicyClassName=getAutoCreatedQueueManagementPolicy(queueName);
    return Resources.none();  }  Resource resource=Resource.newInstance(0L,0);  Matcher matcher=RESOURCE_PATTERN.matcher(resourceString);  if (matcher.find()) {    String subGroup=matcher.group(0);    if (subGroup.trim().isEmpty()) {      return Resources.none();    }    subGroup=subGroup.substring(1,subGroup.length() - 1);    for (    String kvPair : subGroup.trim().split(",")) {      String[] splits=kvPair.split("=");      if (splits != null && splits.length > 1) {        updateResourceValuesFromConfig(resourceTypes,resource,splits);      }    }  }  if (resource.getMemorySize() == 0L) {    return Resources.none();  }  if (LOG.isDebugEnabled()) {
public void initializeQueues(CapacitySchedulerConfiguration conf) throws IOException {  root=parseQueue(this.csContext,conf,null,CapacitySchedulerConfiguration.ROOT,queues,queues,NOOP);  setQueueAcls(authorizer,appPriorityACLManager,queues);  labelManager.reinitializeQueueLabels(getQueueToLabels());  this.queueStateManager.initialize(this);
      queue=new ManagedParentQueue(csContext,queueName,parent,oldQueues.get(fullQueueName));    } else {      queue=new LeafQueue(csContext,queueName,parent,oldQueues.get(fullQueueName));      queue=hook.hook(queue);    }  } else {    if (isReservableQueue) {      throw new IllegalStateException("Only Leaf Queues can be reservable for " + fullQueueName);    }    ParentQueue parentQueue;    if (isAutoCreateEnabled) {      parentQueue=new ManagedParentQueue(csContext,queueName,parent,oldQueues.get(fullQueueName));    } else {      parentQueue=new ParentQueue(csContext,queueName,parent,oldQueues.get(fullQueueName));    }    queue=hook.hook(parentQueue);    List<CSQueue> childQueues=new ArrayList<>();    for (    String childQueueName : childQueueNames) {
    }    nodeLocalityDelay=schedConf.getNodeLocalityDelay();    rackLocalityAdditionalDelay=schedConf.getRackLocalityAdditionalDelay();    rackLocalityFullReset=schedConf.getRackLocalityFullReset();    this.minimumAllocationFactor=Resources.ratio(resourceCalculator,Resources.subtract(maximumAllocation,minimumAllocation),maximumAllocation);    StringBuilder aclsString=new StringBuilder();    for (    Map.Entry<AccessType,AccessControlList> e : acls.entrySet()) {      aclsString.append(e.getKey() + ":" + e.getValue().getAclString());    }    StringBuilder labelStrBuilder=new StringBuilder();    if (accessibleLabels != null) {      for (      String s : accessibleLabels) {        labelStrBuilder.append(s).append(",");      }    }    defaultAppPriorityPerQueue=Priority.newInstance(conf.getDefaultApplicationPriorityConfPerQueue(getQueuePath()));    int queueUL=Math.min(100,conf.getUserLimit(getQueuePath()));    for (    Entry<String,Float> e : getUserWeights().entrySet()) {      float val=e.getValue().floatValue();
public void validateSubmitApplication(ApplicationId applicationId,String userName,String queue) throws AccessControlException {  writeLock.lock();  try {    if (getState() != QueueState.RUNNING) {      String msg="Queue " + getQueuePath() + " is STOPPED. Cannot accept submission of application: "+ applicationId;
      throw new AccessControlException(msg);    }    if (getNumApplications() >= getMaxApplications()) {      String msg="Queue " + getQueuePath() + " already has "+ getNumApplications()+ " applications,"+ " cannot accept submission of application: "+ applicationId;      LOG.info(msg);      throw new AccessControlException(msg);    }    User user=usersManager.getUserAndAddIfAbsent(userName);    if (user.getTotalApplications() >= getMaxApplicationsPerUser()) {      String msg="Queue " + getQueuePath() + " already has "+ user.getTotalApplications()+ " applications from user "+ userName+ " cannot accept submission of application: "+ applicationId;      LOG.info(msg);      throw new AccessControlException(msg);    }  }  finally {    writeLock.unlock();  }  try {    getParent().validateSubmitApplication(applicationId,userName,queue);  } catch (  AccessControlException ace) {
public Resource getUserAMResourceLimitPerPartition(String nodePartition,String userName){  float userWeight=1.0f;  if (userName != null && getUser(userName) != null) {    userWeight=getUser(userName).getWeight();  }  readLock.lock();  try {    float effectiveUserLimit=Math.max(usersManager.getUserLimit() / 100.0f,1.0f / Math.max(getAbstractUsersManager().getNumActiveUsers(),1));    float preWeightedUserLimit=effectiveUserLimit;    effectiveUserLimit=Math.min(effectiveUserLimit * userWeight,1.0f);    Resource queuePartitionResource=getEffectiveCapacity(nodePartition);    Resource userAMLimit=Resources.multiplyAndNormalizeUp(resourceCalculator,queuePartitionResource,queueCapacities.getMaxAMResourcePercentage(nodePartition) * effectiveUserLimit * usersManager.getUserLimitFactor(),minimumAllocation);    userAMLimit=Resources.min(resourceCalculator,lastClusterResource,userAMLimit,Resources.clone(getAMResourceLimitPerPartition(nodePartition)));    Resource preWeighteduserAMLimit=Resources.multiplyAndNormalizeUp(resourceCalculator,queuePartitionResource,queueCapacities.getMaxAMResourcePercentage(nodePartition) * preWeightedUserLimit * usersManager.getUserLimitFactor(),minimumAllocation);    preWeighteduserAMLimit=Resources.min(resourceCalculator,lastClusterResource,preWeighteduserAMLimit,Resources.clone(getAMResourceLimitPerPartition(nodePartition)));    queueUsage.setUserAMLimit(nodePartition,preWeighteduserAMLimit);
protected void activateApplications(){  writeLock.lock();  try {    Map<String,Resource> userAmPartitionLimit=new HashMap<String,Resource>();    for (    String nodePartition : getNodeLabelsForQueue()) {      calculateAndGetAMResourceLimitPerPartition(nodePartition);    }    for (Iterator<FiCaSchedulerApp> fsApp=getPendingAppsOrderingPolicy().getAssignmentIterator(IteratorSelector.EMPTY_ITERATOR_SELECTOR); fsApp.hasNext(); ) {      FiCaSchedulerApp application=fsApp.next();      ApplicationId applicationId=application.getApplicationId();      String partitionName=application.getAppAMNodePartitionName();      Resource amLimit=getAMResourceLimitPerPartition(partitionName);      if (amLimit == null) {        amLimit=calculateAndGetAMResourceLimitPerPartition(partitionName);      }      Resource amIfStarted=Resources.add(application.getAMResource(partitionName),queueUsage.getAMUsed(partitionName));      if (LOG.isDebugEnabled()) {
    }    for (Iterator<FiCaSchedulerApp> fsApp=getPendingAppsOrderingPolicy().getAssignmentIterator(IteratorSelector.EMPTY_ITERATOR_SELECTOR); fsApp.hasNext(); ) {      FiCaSchedulerApp application=fsApp.next();      ApplicationId applicationId=application.getApplicationId();      String partitionName=application.getAppAMNodePartitionName();      Resource amLimit=getAMResourceLimitPerPartition(partitionName);      if (amLimit == null) {        amLimit=calculateAndGetAMResourceLimitPerPartition(partitionName);      }      Resource amIfStarted=Resources.add(application.getAMResource(partitionName),queueUsage.getAMUsed(partitionName));      if (LOG.isDebugEnabled()) {        LOG.debug("application " + application.getId() + " AMResource "+ application.getAMResource(partitionName)+ " maxAMResourcePerQueuePercent "+ maxAMResourcePerQueuePercent+ " amLimit "+ amLimit+ " lastClusterResource "+ lastClusterResource+ " amIfStarted "+ amIfStarted+ " AM node-partition name "+ partitionName);      }      if (!resourceCalculator.fitsIn(amIfStarted,amLimit)) {        if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual(resourceCalculator,lastClusterResource,queueUsage.getAMUsed(partitionName),Resources.none()))) {          LOG.warn("maximum-am-resource-percent is insufficient to start a" + " single application in queue, it is likely set too low." + " skipping enforcement to allow at least one application"+ " to start");        } else {          application.updateAMContainerDiagnostics(AMState.INACTIVATED,CSAMContainerLaunchDiagnosticsConstants.QUEUE_AM_RESOURCE_LIMIT_EXCEED);
      if (!resourceCalculator.fitsIn(amIfStarted,amLimit)) {        if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual(resourceCalculator,lastClusterResource,queueUsage.getAMUsed(partitionName),Resources.none()))) {          LOG.warn("maximum-am-resource-percent is insufficient to start a" + " single application in queue, it is likely set too low." + " skipping enforcement to allow at least one application"+ " to start");        } else {          application.updateAMContainerDiagnostics(AMState.INACTIVATED,CSAMContainerLaunchDiagnosticsConstants.QUEUE_AM_RESOURCE_LIMIT_EXCEED);          LOG.debug("Not activating application {} as  amIfStarted: {}" + " exceeds amLimit: {}",applicationId,amIfStarted,amLimit);          continue;        }      }      User user=getUser(application.getUser());      Resource userAMLimit=userAmPartitionLimit.get(partitionName);      if (userAMLimit == null) {        userAMLimit=getUserAMResourceLimitPerPartition(partitionName,application.getUser());        userAmPartitionLimit.put(partitionName,userAMLimit);      }      Resource userAmIfStarted=Resources.add(application.getAMResource(partitionName),user.getConsumedAMResources(partitionName));      if (!resourceCalculator.fitsIn(userAmIfStarted,userAMLimit)) {        if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual(resourceCalculator,lastClusterResource,queueUsage.getAMUsed(partitionName),Resources.none()))) {
      Resource userAMLimit=userAmPartitionLimit.get(partitionName);      if (userAMLimit == null) {        userAMLimit=getUserAMResourceLimitPerPartition(partitionName,application.getUser());        userAmPartitionLimit.put(partitionName,userAMLimit);      }      Resource userAmIfStarted=Resources.add(application.getAMResource(partitionName),user.getConsumedAMResources(partitionName));      if (!resourceCalculator.fitsIn(userAmIfStarted,userAMLimit)) {        if (getNumActiveApplications() < 1 || (Resources.lessThanOrEqual(resourceCalculator,lastClusterResource,queueUsage.getAMUsed(partitionName),Resources.none()))) {          LOG.warn("maximum-am-resource-percent is insufficient to start a" + " single application in queue for user, it is likely set too" + " low. skipping enforcement to allow at least one application"+ " to start");        } else {          application.updateAMContainerDiagnostics(AMState.INACTIVATED,CSAMContainerLaunchDiagnosticsConstants.USER_AM_RESOURCE_LIMIT_EXCEED);          LOG.debug("Not activating application {} for user: {} as" + " userAmIfStarted: {} exceeds userAmLimit: {}",applicationId,user,userAmIfStarted,userAMLimit);          continue;        }      }      user.activateApplication();      orderingPolicy.addSchedulableEntity(application);      application.updateAMContainerDiagnostics(AMState.ACTIVATED,null);
private void addApplicationAttempt(FiCaSchedulerApp application,User user){  writeLock.lock();  try {    applicationAttemptMap.put(application.getApplicationAttemptId(),application);    if (application.isRunnable()) {      runnableApps.add(application);
  try {    applicationAttemptMap.put(application.getApplicationAttemptId(),application);    if (application.isRunnable()) {      runnableApps.add(application);      LOG.debug("Adding runnable application: {}",application.getApplicationAttemptId());    } else {      nonRunnableApps.add(application);      LOG.info("Application attempt {} is not runnable," + " parallel limit reached",application.getApplicationAttemptId());      return;    }    user.submitApplication();    getPendingAppsOrderingPolicy().addSchedulableEntity(application);    if (Resources.greaterThan(resourceCalculator,lastClusterResource,lastClusterResource,Resources.none())) {      activateApplications();    } else {      application.updateAMContainerDiagnostics(AMState.INACTIVATED,CSAMContainerLaunchDiagnosticsConstants.CLUSTER_RESOURCE_EMPTY);
private void removeApplicationAttempt(FiCaSchedulerApp application,String userName){  writeLock.lock();  try {    User user=usersManager.getUserAndAddIfAbsent(userName);    boolean runnable=runnableApps.remove(application);    if (!runnable) {      if (!removeNonRunnableApp(application)) {
      if (!removeNonRunnableApp(application)) {        LOG.error("Given app to remove " + application + " does not exist in queue "+ getQueuePath());      }    }    String partitionName=application.getAppAMNodePartitionName();    boolean wasActive=orderingPolicy.removeSchedulableEntity(application);    if (!wasActive) {      pendingOrderingPolicy.removeSchedulableEntity(application);    } else {      queueUsage.decAMUsed(partitionName,application.getAMResource(partitionName));      user.getResourceUsage().decAMUsed(partitionName,application.getAMResource(partitionName));      metrics.decAMUsed(partitionName,application.getUser(),application.getAMResource(partitionName));    }    applicationAttemptMap.remove(application.getApplicationAttemptId());    user.finishApplication(wasActive);    if (user.getTotalApplications() == 0) {      usersManager.removeUser(application.getUser());    }    activateApplications();
@Override public CSAssignment assignContainers(Resource clusterResource,CandidateNodeSet<FiCaSchedulerNode> candidates,ResourceLimits currentResourceLimits,SchedulingMode schedulingMode){  updateCurrentResourceLimits(currentResourceLimits,clusterResource);  FiCaSchedulerNode node=CandidateNodeSetUtils.getSingleNode(candidates);  if (LOG.isDebugEnabled()) {
    }    CachedUserLimit cul=userLimits.get(application.getUser());    Resource cachedUserLimit=null;    if (cul != null) {      cachedUserLimit=cul.userLimit;    }    Resource userLimit=computeUserLimitAndSetHeadroom(application,clusterResource,candidates.getPartition(),schedulingMode,cachedUserLimit);    if (cul == null) {      cul=new CachedUserLimit(userLimit);      userLimits.put(application.getUser(),cul);    }    boolean userAssignable=true;    if (!cul.canAssign && Resources.fitsIn(appReserved,cul.reservation)) {      userAssignable=false;    } else {      userAssignable=canAssignToUser(clusterResource,application.getUser(),userLimit,application,candidates.getPartition(),currentResourceLimits);      if (!userAssignable && Resources.fitsIn(cul.reservation,appReserved)) {        cul.canAssign=false;
@Override public boolean accept(Resource cluster,ResourceCommitRequest<FiCaSchedulerApp,FiCaSchedulerNode> request){  ContainerAllocationProposal<FiCaSchedulerApp,FiCaSchedulerNode> allocation=request.getFirstAllocatedOrReservedContainer();  SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> schedulerContainer=allocation.getAllocatedOrReservedContainer();  if (allocation.getAllocateFromReservedContainer() == null) {    readLock.lock();    try {      FiCaSchedulerApp app=schedulerContainer.getSchedulerApplicationAttempt();      String username=app.getUser();      String p=schedulerContainer.getNodePartition();      Resource userLimit=computeUserLimitAndSetHeadroom(app,cluster,p,allocation.getSchedulingMode(),null);      User user=getUser(username);      if (user == null) {
  SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> schedulerContainer=allocation.getAllocatedOrReservedContainer();  if (allocation.getAllocateFromReservedContainer() == null) {    readLock.lock();    try {      FiCaSchedulerApp app=schedulerContainer.getSchedulerApplicationAttempt();      String username=app.getUser();      String p=schedulerContainer.getNodePartition();      Resource userLimit=computeUserLimitAndSetHeadroom(app,cluster,p,allocation.getSchedulingMode(),null);      User user=getUser(username);      if (user == null) {        LOG.debug("User {} has been removed!",username);        return false;      }      Resource usedResource=Resources.clone(user.getUsed(p));      Resources.subtractFrom(usedResource,request.getTotalReleasedResource());      if (Resources.greaterThan(resourceCalculator,cluster,usedResource,userLimit)) {
@Lock({LeafQueue.class}) Resource computeUserLimitAndSetHeadroom(FiCaSchedulerApp application,Resource clusterResource,String nodePartition,SchedulingMode schedulingMode,Resource userLimit){  String user=application.getUser();  User queueUser=getUser(user);  if (queueUser == null) {
@Private protected boolean canAssignToUser(Resource clusterResource,String userName,Resource limit,FiCaSchedulerApp application,String nodePartition,ResourceLimits currentResourceLimits){  readLock.lock();  try {    User user=getUser(userName);    if (user == null) {
  try {    User user=getUser(userName);    if (user == null) {      LOG.debug("User {} has been removed!",userName);      return false;    }    currentResourceLimits.setAmountNeededUnreserve(Resources.none());    if (Resources.greaterThan(resourceCalculator,clusterResource,user.getUsed(nodePartition),limit)) {      if (this.reservationsContinueLooking) {        if (Resources.lessThanOrEqual(resourceCalculator,clusterResource,Resources.subtract(user.getUsed(),application.getCurrentReservation()),limit)) {          if (LOG.isDebugEnabled()) {            LOG.debug("User " + userName + " in queue "+ getQueuePath()+ " will exceed limit based on reservations - "+ " consumed: "+ user.getUsed()+ " reserved: "+ application.getCurrentReservation()+ " limit: "+ limit);          }          Resource amountNeededToUnreserve=Resources.subtract(user.getUsed(nodePartition),limit);          currentResourceLimits.setAmountNeededUnreserve(amountNeededToUnreserve);          return true;        }      }      if (LOG.isDebugEnabled()) {
  try {    super.allocateResource(clusterResource,resource,nodePartition);    if (null != rmContainer && rmContainer.getNodeLabelExpression().equals(RMNodeLabelsManager.NO_LABEL) && !nodePartition.equals(RMNodeLabelsManager.NO_LABEL)) {      TreeSet<RMContainer> rmContainers=null;      if (null == (rmContainers=ignorePartitionExclusivityRMContainers.get(nodePartition))) {        rmContainers=new TreeSet<>();        ignorePartitionExclusivityRMContainers.put(nodePartition,rmContainers);      }      rmContainers.add(rmContainer);    }    String userName=application.getUser();    User user=usersManager.updateUserResourceUsage(userName,resource,nodePartition,true);    Resource partitionHeadroom=Resources.createResource(0,0);    if (metrics.getUserMetrics(userName) != null) {      partitionHeadroom=getHeadroom(user,cachedResourceLimitsForHeadroom.getLimit(),clusterResource,getResourceLimitForActiveUsers(userName,clusterResource,nodePartition,SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY),nodePartition);    }    metrics.setAvailableResourcesToUser(nodePartition,userName,partitionHeadroom);    if (LOG.isDebugEnabled()) {
  try {    super.releaseResource(clusterResource,resource,nodePartition);    if (null != rmContainer && rmContainer.getNodeLabelExpression().equals(RMNodeLabelsManager.NO_LABEL) && !nodePartition.equals(RMNodeLabelsManager.NO_LABEL)) {      if (ignorePartitionExclusivityRMContainers.containsKey(nodePartition)) {        Set<RMContainer> rmContainers=ignorePartitionExclusivityRMContainers.get(nodePartition);        rmContainers.remove(rmContainer);        if (rmContainers.isEmpty()) {          ignorePartitionExclusivityRMContainers.remove(nodePartition);        }      }    }    String userName=application.getUser();    User user=usersManager.updateUserResourceUsage(userName,resource,nodePartition,false);    Resource partitionHeadroom=Resources.createResource(0,0);    if (metrics.getUserMetrics(userName) != null) {      partitionHeadroom=getHeadroom(user,cachedResourceLimitsForHeadroom.getLimit(),clusterResource,getResourceLimitForActiveUsers(userName,clusterResource,nodePartition,SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY),nodePartition);    }    metrics.setAvailableResourcesToUser(nodePartition,userName,partitionHeadroom);    if (LOG.isDebugEnabled()) {
@Override public void attachContainer(Resource clusterResource,FiCaSchedulerApp application,RMContainer rmContainer){  if (application != null && rmContainer != null && rmContainer.getExecutionType() == ExecutionType.GUARANTEED) {    FiCaSchedulerNode node=scheduler.getNode(rmContainer.getContainer().getNodeId());    allocateResource(clusterResource,application,rmContainer.getContainer().getResource(),node.getPartition(),rmContainer);
@Override public void detachContainer(Resource clusterResource,FiCaSchedulerApp application,RMContainer rmContainer){  if (application != null && rmContainer != null && rmContainer.getExecutionType() == ExecutionType.GUARANTEED) {    FiCaSchedulerNode node=scheduler.getNode(rmContainer.getContainer().getNodeId());    releaseResource(clusterResource,application,rmContainer.getContainer().getResource(),node.getPartition(),rmContainer);
    validate(newlyParsedQueue);    shouldFailAutoCreationWhenGuaranteedCapacityExceeded=csContext.getConfiguration().getShouldFailAutoQueueCreationWhenGuaranteedCapacityExceeded(getQueuePath());    if (shouldFailAutoCreationWhenGuaranteedCapacityExceeded) {      float childCap=sumOfChildCapacities();      if (getCapacity() < childCap) {        throw new IOException("Total of Auto Created leaf queues guaranteed capacity : " + childCap + " exceeds Parent queue's "+ getQueuePath()+ " guaranteed capacity "+ getCapacity()+ ""+ ".Cannot enforce policy to auto"+ " create queues beyond parent queue's capacity");      }    }    leafQueueTemplate=initializeLeafQueueConfigs().build();    super.reinitialize(newlyParsedQueue,clusterResource);    for (    CSQueue res : this.getChildQueues()) {      res.reinitialize(res,clusterResource);    }    reinitializeQueueManagementPolicy();    final List<QueueManagementChange> queueManagementChanges=queueManagementPolicy.computeQueueManagementChanges();    validateAndApplyQueueManagementChanges(queueManagementChanges);    LOG.info("Reinitialized Managed Parent Queue: [{}] with capacity [{}]" + " with max capacity [{}]",queueName,super.getCapacity(),super.getMaximumCapacity());  } catch (  YarnException ye) {
    ParentQueue newlyParsedParentQueue=(ParentQueue)newlyParsedQueue;    setupQueueConfigs(clusterResource);    Map<String,CSQueue> currentChildQueues=getQueuesMap(childQueues);    Map<String,CSQueue> newChildQueues=getQueuesMap(newlyParsedParentQueue.childQueues);    for (    Map.Entry<String,CSQueue> e : newChildQueues.entrySet()) {      String newChildQueueName=e.getKey();      CSQueue newChildQueue=e.getValue();      CSQueue childQueue=currentChildQueues.get(newChildQueueName);      if (childQueue != null) {        if ((childQueue instanceof LeafQueue && newChildQueue instanceof ParentQueue) || (childQueue instanceof ParentQueue && newChildQueue instanceof LeafQueue)) {          newChildQueue.setParent(this);          currentChildQueues.put(newChildQueueName,newChildQueue);          CapacitySchedulerQueueManager queueManager=this.csContext.getCapacitySchedulerQueueManager();          queueManager.addQueue(newChildQueueName,newChildQueue);          continue;
    for (    Map.Entry<String,CSQueue> e : newChildQueues.entrySet()) {      String newChildQueueName=e.getKey();      CSQueue newChildQueue=e.getValue();      CSQueue childQueue=currentChildQueues.get(newChildQueueName);      if (childQueue != null) {        if ((childQueue instanceof LeafQueue && newChildQueue instanceof ParentQueue) || (childQueue instanceof ParentQueue && newChildQueue instanceof LeafQueue)) {          newChildQueue.setParent(this);          currentChildQueues.put(newChildQueueName,newChildQueue);          CapacitySchedulerQueueManager queueManager=this.csContext.getCapacitySchedulerQueueManager();          queueManager.addQueue(newChildQueueName,newChildQueue);          continue;        }        childQueue.reinitialize(newChildQueue,clusterResource);        LOG.info(getQueuePath() + ": re-configured queue: " + childQueue);      } else {        newChildQueue.setParent(this);
private void addApplication(ApplicationId applicationId,String user){  writeLock.lock();  try {    ++numApplications;
private void removeApplication(ApplicationId applicationId,String user){  writeLock.lock();  try {    --numApplications;
@Override public CSAssignment assignContainers(Resource clusterResource,CandidateNodeSet<FiCaSchedulerNode> candidates,ResourceLimits resourceLimits,SchedulingMode schedulingMode){  FiCaSchedulerNode node=CandidateNodeSetUtils.getSingleNode(candidates);  if (schedulingMode == SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY && !accessibleToPartition(candidates.getPartition())) {    if (LOG.isDebugEnabled()) {      long now=System.currentTimeMillis();      if (now - this.lastSkipQueueDebugLoggingTimestamp > 1000) {
  FiCaSchedulerNode node=CandidateNodeSetUtils.getSingleNode(candidates);  if (schedulingMode == SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY && !accessibleToPartition(candidates.getPartition())) {    if (LOG.isDebugEnabled()) {      long now=System.currentTimeMillis();      if (now - this.lastSkipQueueDebugLoggingTimestamp > 1000) {        LOG.debug("Skip this queue=" + getQueuePath() + ", because it is not able to access partition="+ candidates.getPartition());        this.lastSkipQueueDebugLoggingTimestamp=now;      }    }    ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager,node,getParentName(),getQueuePath(),ActivityState.REJECTED,ActivityDiagnosticConstant.QUEUE_NOT_ABLE_TO_ACCESS_PARTITION);    if (rootQueue) {      ActivitiesLogger.NODE.finishSkippedNodeAllocation(activitiesManager,node);    }    return CSAssignment.NULL_ASSIGNMENT;  }  if (!super.hasPendingResourceRequest(candidates.getPartition(),clusterResource,schedulingMode)) {    if (LOG.isDebugEnabled()) {      long now=System.currentTimeMillis();      if (now - this.lastSkipQueueDebugLoggingTimestamp > 1000) {
        ActivitiesLogger.NODE.finishSkippedNodeAllocation(activitiesManager,node);      }      break;    }    CSAssignment assignedToChild=assignContainersToChildQueues(clusterResource,candidates,resourceLimits,schedulingMode);    assignment.setType(assignedToChild.getType());    assignment.setRequestLocalityType(assignedToChild.getRequestLocalityType());    assignment.setExcessReservation(assignedToChild.getExcessReservation());    assignment.setContainersToKill(assignedToChild.getContainersToKill());    assignment.setFulfilledReservation(assignedToChild.isFulfilledReservation());    assignment.setFulfilledReservedContainer(assignedToChild.getFulfilledReservedContainer());    if (Resources.greaterThan(resourceCalculator,clusterResource,assignedToChild.getResource(),Resources.none())) {      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager,node,getParentName(),getQueuePath(),ActivityState.ACCEPTED,ActivityDiagnosticConstant.EMPTY);      boolean isReserved=assignedToChild.getAssignmentInformation().getReservationDetails() != null && !assignedToChild.getAssignmentInformation().getReservationDetails().isEmpty();      if (rootQueue) {        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,node,assignedToChild.getAssignmentInformation().getFirstAllocatedOrReservedContainerId(),isReserved ? AllocationState.RESERVED : AllocationState.ALLOCATED);      }      Resources.addTo(assignment.getResource(),assignedToChild.getResource());
      }      break;    }    CSAssignment assignedToChild=assignContainersToChildQueues(clusterResource,candidates,resourceLimits,schedulingMode);    assignment.setType(assignedToChild.getType());    assignment.setRequestLocalityType(assignedToChild.getRequestLocalityType());    assignment.setExcessReservation(assignedToChild.getExcessReservation());    assignment.setContainersToKill(assignedToChild.getContainersToKill());    assignment.setFulfilledReservation(assignedToChild.isFulfilledReservation());    assignment.setFulfilledReservedContainer(assignedToChild.getFulfilledReservedContainer());    if (Resources.greaterThan(resourceCalculator,clusterResource,assignedToChild.getResource(),Resources.none())) {      ActivitiesLogger.QUEUE.recordQueueActivity(activitiesManager,node,getParentName(),getQueuePath(),ActivityState.ACCEPTED,ActivityDiagnosticConstant.EMPTY);      boolean isReserved=assignedToChild.getAssignmentInformation().getReservationDetails() != null && !assignedToChild.getAssignmentInformation().getReservationDetails().isEmpty();      if (rootQueue) {        ActivitiesLogger.NODE.finishAllocatedNodeAllocation(activitiesManager,node,assignedToChild.getAssignmentInformation().getFirstAllocatedOrReservedContainerId(),isReserved ? AllocationState.RESERVED : AllocationState.ALLOCATED);      }      Resources.addTo(assignment.getResource(),assignedToChild.getResource());      Resources.addTo(assignment.getAssignmentInformation().getAllocated(),assignedToChild.getAssignmentInformation().getAllocated());
private CSAssignment assignContainersToChildQueues(Resource cluster,CandidateNodeSet<FiCaSchedulerNode> candidates,ResourceLimits limits,SchedulingMode schedulingMode){  CSAssignment assignment=CSAssignment.NULL_ASSIGNMENT;  printChildQueues();  for (Iterator<CSQueue> iter=sortAndGetChildrenAllocationIterator(candidates.getPartition()); iter.hasNext(); ) {    CSQueue childQueue=iter.next();
      LOG.debug("Assigned to queue: " + childQueue.getQueuePath() + " stats: "+ childQueue+ " --> "+ childAssignment.getResource()+ ", "+ childAssignment.getType());    }    if (Resources.greaterThan(resourceCalculator,cluster,childAssignment.getResource(),Resources.none())) {      assignment=childAssignment;      break;    } else     if (childAssignment.getSkippedType() == CSAssignment.SkippedType.QUEUE_LIMIT) {      if (assignment.getSkippedType() != CSAssignment.SkippedType.QUEUE_LIMIT) {        assignment=childAssignment;      }      Resource blockedHeadroom=null;      if (childQueue instanceof LeafQueue) {        blockedHeadroom=childLimits.getHeadroom();      } else {        blockedHeadroom=childLimits.getBlockedHeadroom();      }      Resource resourceToSubtract=Resources.max(resourceCalculator,cluster,blockedHeadroom,Resources.none());      limits.addBlockedHeadroom(resourceToSubtract);      if (LOG.isDebugEnabled()) {
private void internalReleaseResource(Resource clusterResource,FiCaSchedulerNode node,Resource releasedResource){  writeLock.lock();  try {    super.releaseResource(clusterResource,releasedResource,node.getPartition());
 else {    if (Resources.lessThan(rc,clusterResource,queueResourceQuotas.getEffectiveMinResource(label),configuredMinResources)) {      numeratorForMinRatio=queueResourceQuotas.getEffectiveMinResource(label);    }  }  Map<String,Float> effectiveMinRatioPerResource=getEffectiveMinRatioPerResource(configuredMinResources,numeratorForMinRatio);  for (  CSQueue childQueue : getChildQueues()) {    Resource minResource=childQueue.getQueueResourceQuotas().getConfiguredMinResource(label);    if (childQueue.getCapacityConfigType().equals(CapacityConfigType.ABSOLUTE_RESOURCE)) {      childQueue.getQueueResourceQuotas().setEffectiveMinResource(label,getMinResourceNormalized(childQueue.getQueuePath(),effectiveMinRatioPerResource,minResource));      Resource parentMaxRes=queueResourceQuotas.getConfiguredMaxResource(label);      if (parent != null && parentMaxRes.equals(Resources.none())) {        parentMaxRes=parent.getQueueResourceQuotas().getEffectiveMaxResource(label);      }      Resource childMaxResource=childQueue.getQueueResourceQuotas().getConfiguredMaxResource(label);      Resource effMaxResource=Resources.min(resourceCalculator,resourceByLabel,childMaxResource.equals(Resources.none()) ? parentMaxRes : childMaxResource,parentMaxRes);      childQueue.getQueueResourceQuotas().setEffectiveMaxResource(label,Resources.clone(effMaxResource));      deriveCapacityFromAbsoluteConfigurations(label,clusterResource,rc,childQueue);
private Resource getMinResourceNormalized(String name,Map<String,Float> effectiveMinRatio,Resource minResource){  Resource ret=Resource.newInstance(minResource);  int maxLength=ResourceUtils.getNumberOfCountableResourceTypes();  for (int i=0; i < maxLength; i++) {    ResourceInformation nResourceInformation=minResource.getResourceInformation(i);    Float ratio=effectiveMinRatio.get(nResourceInformation.getName());    if (ratio != null) {      ret.setResourceValue(i,(long)(nResourceInformation.getValue() * ratio.floatValue()));      if (LOG.isDebugEnabled()) {
  childQueue.getQueueCapacities().setAbsoluteCapacity(label,childQueue.getQueueCapacities().getCapacity(label) * getQueueCapacities().getAbsoluteCapacity(label));  childQueue.getQueueCapacities().setAbsoluteMaximumCapacity(label,childQueue.getQueueCapacities().getMaximumCapacity(label) * getQueueCapacities().getAbsoluteMaximumCapacity(label));  if (childQueue instanceof LeafQueue) {    LeafQueue leafQueue=(LeafQueue)childQueue;    CapacitySchedulerConfiguration conf=csContext.getConfiguration();    int maxApplications=conf.getMaximumApplicationsPerQueue(childQueue.getQueuePath());    if (maxApplications < 0) {      int maxGlobalPerQueueApps=conf.getGlobalMaximumApplicationsPerQueue();      if (maxGlobalPerQueueApps > 0) {        maxApplications=(int)(maxGlobalPerQueueApps * childQueue.getQueueCapacities().getAbsoluteCapacity(label));      } else {        maxApplications=(int)(conf.getMaximumSystemApplications() * childQueue.getQueueCapacities().getAbsoluteCapacity(label));      }    }    leafQueue.setMaxApplications(maxApplications);    int maxApplicationsPerUser=Math.min(maxApplications,(int)(maxApplications * (leafQueue.getUsersManager().getUserLimit() / 100.0f) * leafQueue.getUsersManager().getUserLimitFactor()));    leafQueue.setMaxApplicationsPerUser(maxApplicationsPerUser);
@Override public void attachContainer(Resource clusterResource,FiCaSchedulerApp application,RMContainer rmContainer){  if (application != null) {    FiCaSchedulerNode node=scheduler.getNode(rmContainer.getContainer().getNodeId());    allocateResource(clusterResource,rmContainer.getContainer().getResource(),node.getPartition());
@Override public void detachContainer(Resource clusterResource,FiCaSchedulerApp application,RMContainer rmContainer){  if (application != null) {    FiCaSchedulerNode node=scheduler.getNode(rmContainer.getContainer().getNodeId());    super.releaseResource(clusterResource,rmContainer.getContainer().getResource(),node.getPartition());
public void apply(Resource cluster,ResourceCommitRequest<FiCaSchedulerApp,FiCaSchedulerNode> request){  if (request.anythingAllocatedOrReserved()) {    ContainerAllocationProposal<FiCaSchedulerApp,FiCaSchedulerNode> allocation=request.getFirstAllocatedOrReservedContainer();    SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> schedulerContainer=allocation.getAllocatedOrReservedContainer();    if (allocation.getAllocateFromReservedContainer() == null) {      writeLock.lock();      try {        allocateResource(cluster,allocation.getAllocatedOrReservedResource(),schedulerContainer.getNodePartition());
  List<QueueManagementChange> queueManagementChanges=Collections.emptyList();  if (!parentQueue.shouldFailAutoCreationWhenGuaranteedCapacityExceeded()) {    AutoCreatedQueueManagementPolicy policyClazz=parentQueue.getAutoCreatedQueueManagementPolicy();    long startTime=0;    try {      startTime=clock.getTime();      queueManagementChanges=policyClazz.computeQueueManagementChanges();      if (queueManagementChanges.size() > 0) {        QueueManagementChangeEvent queueManagementChangeEvent=new QueueManagementChangeEvent(parentQueue,queueManagementChanges);        scheduler.getRMContext().getDispatcher().getEventHandler().handle(queueManagementChangeEvent);      }      if (LOG.isDebugEnabled()) {        LOG.debug("{} uses {} millisecond" + " to run",policyClazz.getClass().getName(),clock.getTime() - startTime);        if (queueManagementChanges.size() > 0) {          LOG.debug(" Updated queue management changes for parent queue" + " " + "{}: [{}]",parentQueue.getQueuePath(),queueManagementChanges.size() < 25 ? queueManagementChanges.toString() : queueManagementChanges.size());        }      }    } catch (    YarnException e) {
    AutoCreatedQueueManagementPolicy policyClazz=parentQueue.getAutoCreatedQueueManagementPolicy();    long startTime=0;    try {      startTime=clock.getTime();      queueManagementChanges=policyClazz.computeQueueManagementChanges();      if (queueManagementChanges.size() > 0) {        QueueManagementChangeEvent queueManagementChangeEvent=new QueueManagementChangeEvent(parentQueue,queueManagementChanges);        scheduler.getRMContext().getDispatcher().getEventHandler().handle(queueManagementChangeEvent);      }      if (LOG.isDebugEnabled()) {        LOG.debug("{} uses {} millisecond" + " to run",policyClazz.getClass().getName(),clock.getTime() - startTime);        if (queueManagementChanges.size() > 0) {          LOG.debug(" Updated queue management changes for parent queue" + " " + "{}: [{}]",parentQueue.getQueuePath(),queueManagementChanges.size() < 25 ? queueManagementChanges.toString() : queueManagementChanges.size());        }      }    } catch (    YarnException e) {      LOG.error("Could not compute child queue management updates for parent " + "queue " + parentQueue.getQueuePath(),e);    }  } else {
  Resource consumed=Resources.multiplyAndNormalizeUp(resourceCalculator,partitionResource,getUsageRatio(nodePartition),lQueue.getMinimumAllocation());  Resource currentCapacity=Resources.lessThan(resourceCalculator,partitionResource,consumed,queueCapacity) ? queueCapacity : Resources.add(consumed,required);  float usersSummedByWeight=activeUsersTimesWeights;  Resource resourceUsed=Resources.add(totalResUsageForActiveUsers.getUsed(nodePartition),required);  if (!activeUser) {    resourceUsed=currentCapacity;    usersSummedByWeight=allUsersTimesWeights;  }  Resource userLimitResource=Resources.max(resourceCalculator,partitionResource,Resources.divideAndCeil(resourceCalculator,resourceUsed,usersSummedByWeight),Resources.divideAndCeil(resourceCalculator,Resources.multiplyAndRoundDown(currentCapacity,getUserLimit()),100));  Resource maxUserLimit=Resources.none();  if (schedulingMode == SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY) {    maxUserLimit=Resources.multiplyAndRoundDown(queueCapacity,getUserLimitFactor());  } else   if (schedulingMode == SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {    maxUserLimit=partitionResource;  }  userLimitResource=Resources.roundUp(resourceCalculator,Resources.min(resourceCalculator,partitionResource,userLimitResource,maxUserLimit),lQueue.getMinimumAllocation());  if (LOG.isDebugEnabled()) {
private void updateNonActiveUsersResourceUsage(String userName){  this.writeLock.lock();  try {    User user=getUser(userName);    if (user == null)     return;    ResourceUsage resourceUsage=user.getResourceUsage();    if (activeUsersSet.contains(userName)) {      activeUsersSet.remove(userName);      nonActiveUsersSet.add(userName);      activeUsersTimesWeights=sumActiveUsersTimesWeights();      for (      String partition : resourceUsage.getNodePartitionsSet()) {        totalResUsageForActiveUsers.decUsed(partition,resourceUsage.getUsed(partition));        totalResUsageForNonActiveUsers.incUsed(partition,resourceUsage.getUsed(partition));        if (LOG.isDebugEnabled()) {
@VisibleForTesting public void initialize(CapacityScheduler scheduler) throws IOException {  this.scheduler=scheduler;  this.conf=scheduler.getConfiguration();  boolean overrideWithWorkflowPriorityMappings=conf.getOverrideWithWorkflowPriorityMappings();
protected CSAssignment getCSAssignmentFromAllocateResult(Resource clusterResource,ContainerAllocation result,RMContainer rmContainer,FiCaSchedulerNode node){  CSAssignment.SkippedType skipped=(result.getAllocationState() == AllocationState.APP_SKIPPED) ? CSAssignment.SkippedType.OTHER : CSAssignment.SkippedType.NONE;  CSAssignment assignment=new CSAssignment(skipped);  assignment.setApplication(application);  assignment.setExcessReservation(result.getContainerToBeUnreserved());  assignment.setRequestLocalityType(result.requestLocalityType);  if (Resources.greaterThan(rc,clusterResource,result.getResourceToBeAllocated(),Resources.none())) {    Resource allocatedResource=result.getResourceToBeAllocated();    RMContainer updatedContainer=result.getUpdatedContainer();    assignment.setResource(allocatedResource);    assignment.setType(result.getContainerNodeType());    if (result.getAllocationState() == AllocationState.RESERVED) {      if (LOG.isDebugEnabled()) {
    Resource allocatedResource=result.getResourceToBeAllocated();    RMContainer updatedContainer=result.getUpdatedContainer();    assignment.setResource(allocatedResource);    assignment.setType(result.getContainerNodeType());    if (result.getAllocationState() == AllocationState.RESERVED) {      if (LOG.isDebugEnabled()) {        LOG.debug("Reserved container " + " application=" + application.getApplicationId() + " resource="+ allocatedResource+ " queue="+ appInfo.getQueueName()+ " cluster="+ clusterResource);      }      assignment.getAssignmentInformation().addReservationDetails(updatedContainer,application.getCSLeafQueue().getQueuePath());      assignment.getAssignmentInformation().incrReservations();      Resources.addTo(assignment.getAssignmentInformation().getReserved(),allocatedResource);      if (rmContainer != null) {        ActivitiesLogger.APP.finishSkippedAppAllocationRecording(activitiesManager,application.getApplicationId(),ActivityState.SKIPPED,ActivityDiagnosticConstant.EMPTY);      } else {        ActivitiesLogger.APP.finishAllocatedAppAllocationRecording(activitiesManager,application.getApplicationId(),updatedContainer.getContainerId(),ActivityState.RESERVED,ActivityDiagnosticConstant.EMPTY);      }    } else     if (result.getAllocationState() == AllocationState.ALLOCATED) {
  }  if (schedulingMode == SchedulingMode.IGNORE_PARTITION_EXCLUSIVITY) {    if (application.isWaitingForAMContainer()) {      LOG.debug("Skip allocating AM container to app_attempt={}," + " don't allow to allocate AM container in non-exclusive mode",application.getApplicationAttemptId());      application.updateAppSkipNodeDiagnostics("Skipping assigning to Node in Ignore Exclusivity mode. ");      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(activitiesManager,node,application,schedulerKey,ActivityDiagnosticConstant.REQUEST_SKIPPED_IN_IGNORE_EXCLUSIVITY_MODE,ActivityLevel.REQUEST);      return ContainerAllocation.APP_SKIPPED;    }  }  Optional<DiagnosticsCollector> dcOpt=activitiesManager == null ? Optional.empty() : activitiesManager.getOptionalDiagnosticsCollector();  if (!appInfo.precheckNode(schedulerKey,node,schedulingMode,dcOpt)) {    ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(activitiesManager,node,application,schedulerKey,ActivityDiagnosticConstant.NODE_DO_NOT_MATCH_PARTITION_OR_PLACEMENT_CONSTRAINTS + ActivitiesManager.getDiagnostics(dcOpt),ActivityLevel.NODE);    return ContainerAllocation.PRIORITY_SKIPPED;  }  if (!application.getCSLeafQueue().getReservationContinueLooking()) {    if (!shouldAllocOrReserveNewContainer(schedulerKey,required)) {      LOG.debug("doesn't need containers based on reservation algo!");      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(activitiesManager,node,application,schedulerKey,ActivityDiagnosticConstant.REQUEST_SKIPPED_BECAUSE_OF_RESERVATION,ActivityLevel.REQUEST);      return ContainerAllocation.PRIORITY_SKIPPED;
  if (!application.getCSLeafQueue().getReservationContinueLooking()) {    if (!shouldAllocOrReserveNewContainer(schedulerKey,required)) {      LOG.debug("doesn't need containers based on reservation algo!");      ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(activitiesManager,node,application,schedulerKey,ActivityDiagnosticConstant.REQUEST_SKIPPED_BECAUSE_OF_RESERVATION,ActivityLevel.REQUEST);      return ContainerAllocation.PRIORITY_SKIPPED;    }  }  if (!checkHeadroom(clusterResource,resourceLimits,required,node.getPartition())) {    LOG.debug("cannot allocate required resource={} because of headroom",required);    ActivitiesLogger.APP.recordAppActivityWithoutAllocation(activitiesManager,node,application,schedulerKey,ActivityDiagnosticConstant.QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM,ActivityState.REJECTED,ActivityLevel.REQUEST);    return ContainerAllocation.QUEUE_SKIPPED;  }  int missedNonPartitionedRequestSchedulingOpportunity=0;  AppPlacementAllocator appPlacementAllocator=appInfo.getAppPlacementAllocator(schedulerKey);  if (null == appPlacementAllocator) {    ActivitiesLogger.APP.recordSkippedAppActivityWithoutAllocation(activitiesManager,node,application,schedulerKey,ActivityDiagnosticConstant.REQUEST_SKIPPED_BECAUSE_NULL_ANY_REQUEST,ActivityLevel.REQUEST);    return ContainerAllocation.PRIORITY_SKIPPED;  }  String requestPartition=appPlacementAllocator.getPrimaryRequestedNodePartition();
private ContainerAllocation assignContainer(Resource clusterResource,FiCaSchedulerNode node,SchedulerRequestKey schedulerKey,PendingAsk pendingAsk,NodeType type,RMContainer rmContainer,SchedulingMode schedulingMode,ResourceLimits currentResoureLimits){  if (LOG.isDebugEnabled()) {
boolean shouldAllocOrReserveNewContainer(SchedulerRequestKey schedulerKey,Resource required){  int requiredContainers=application.getOutstandingAsksCount(schedulerKey);  int reservedContainers=application.getNumReservedContainers(schedulerKey);  int starvation=0;  if (reservedContainers > 0) {    float nodeFactor=Resources.ratio(rc,required,application.getCSLeafQueue().getMaximumAllocation());    starvation=(int)((application.getReReservations(schedulerKey) / (float)reservedContainers) * (1.0f - (Math.min(nodeFactor,application.getCSLeafQueue().getMinimumAllocationFactor()))));    if (LOG.isDebugEnabled()) {
@Override public CSAssignment assignContainers(Resource clusterResource,CandidateNodeSet<FiCaSchedulerNode> candidates,SchedulingMode schedulingMode,ResourceLimits resourceLimits,RMContainer reservedContainer){  FiCaSchedulerNode node=CandidateNodeSetUtils.getSingleNode(candidates);  if (reservedContainer == null) {    if (!application.hasPendingResourceRequest(candidates.getPartition(),schedulingMode)) {      if (LOG.isDebugEnabled()) {
      String pathName=path.getName();      return pathName.startsWith(YarnConfiguration.CS_CONFIGURATION_FILE) && !pathName.endsWith(TMP);    }  };  Configuration conf=new Configuration(fsConf);  String schedulerConfPathStr=conf.get(YarnConfiguration.SCHEDULER_CONFIGURATION_FS_PATH);  if (schedulerConfPathStr == null || schedulerConfPathStr.isEmpty()) {    throw new IOException(YarnConfiguration.SCHEDULER_CONFIGURATION_FS_PATH + " must be set");  }  this.schedulerConfDir=new Path(schedulerConfPathStr);  String scheme=schedulerConfDir.toUri().getScheme();  if (scheme == null) {    scheme=FileSystem.getDefaultUri(conf).getScheme();  }  if (scheme != null) {    String disableCacheName=String.format("fs.%s.impl.disable.cache",scheme);    conf.setBoolean(disableCacheName,true);  }  this.fileSystem=this.schedulerConfDir.getFileSystem(conf);
      return pathName.startsWith(YarnConfiguration.CS_CONFIGURATION_FILE) && !pathName.endsWith(TMP);    }  };  Configuration conf=new Configuration(fsConf);  String schedulerConfPathStr=conf.get(YarnConfiguration.SCHEDULER_CONFIGURATION_FS_PATH);  if (schedulerConfPathStr == null || schedulerConfPathStr.isEmpty()) {    throw new IOException(YarnConfiguration.SCHEDULER_CONFIGURATION_FS_PATH + " must be set");  }  this.schedulerConfDir=new Path(schedulerConfPathStr);  String scheme=schedulerConfDir.toUri().getScheme();  if (scheme == null) {    scheme=FileSystem.getDefaultUri(conf).getScheme();  }  if (scheme != null) {    String disableCacheName=String.format("fs.%s.impl.disable.cache",scheme);    conf.setBoolean(disableCacheName,true);  }  this.fileSystem=this.schedulerConfDir.getFileSystem(conf);  this.maxVersion=conf.getInt(YarnConfiguration.SCHEDULER_CONFIGURATION_FS_MAX_VERSION,YarnConfiguration.DEFAULT_SCHEDULER_CONFIGURATION_FS_MAX_VERSION);
private void finalizeFileSystemFile() throws IOException {  Path finalConfigPath=getFinalConfigPath(tempConfigPath);  fileSystem.rename(tempConfigPath,finalConfigPath);
private void removeTmpConfigFile() throws IOException {  fileSystem.delete(tempConfigPath,true);
@VisibleForTesting private Path writeTmpConfig(Configuration vSchedConf) throws IOException {  long start=Time.monotonicNow();  String tempSchedulerConfigFile=YarnConfiguration.CS_CONFIGURATION_FILE + "." + System.currentTimeMillis()+ TMP;  Path tempSchedulerConfigPath=new Path(this.schedulerConfDir,tempSchedulerConfigFile);  try (FSDataOutputStream outputStream=fileSystem.create(tempSchedulerConfigPath)){    cleanConfigurationFile();    vSchedConf.writeXml(outputStream);
public void checkVersion() throws Exception {  Version loadedVersion=getConfStoreVersion();  Version currentVersion=getCurrentVersion();
public static YarnConfigurationStore getStore(Configuration conf){  String store=conf.get(YarnConfiguration.SCHEDULER_CONFIGURATION_STORE_CLASS,YarnConfiguration.MEMORY_CONFIGURATION_STORE);switch (store) {case YarnConfiguration.MEMORY_CONFIGURATION_STORE:    return new InMemoryConfigurationStore();case YarnConfiguration.LEVELDB_CONFIGURATION_STORE:  return new LeveldbConfigurationStore();case YarnConfiguration.ZK_CONFIGURATION_STORE:return new ZKConfigurationStore();case YarnConfiguration.FS_CONFIGURATION_STORE:return new FSSchedulerConfigurationStore();default:Class<? extends YarnConfigurationStore> storeClass=conf.getClass(YarnConfiguration.SCHEDULER_CONFIGURATION_STORE_CLASS,InMemoryConfigurationStore.class,YarnConfigurationStore.class);
private void initializeLeafQueueTemplate(ManagedParentQueue parentQueue) throws IOException {  leafQueueTemplate=parentQueue.getLeafQueueTemplate();  leafQueueTemplateCapacities=leafQueueTemplate.getQueueCapacities();  Set<String> parentQueueLabels=parentQueue.getNodeLabelsForQueue();  for (  String nodeLabel : leafQueueTemplateCapacities.getExistingNodeLabels()) {    if (!parentQueueLabels.contains(nodeLabel)) {
@Override public List<QueueManagementChange> computeQueueManagementChanges() throws SchedulerDynamicEditException {  updateLeafQueueState();  readLock.lock();  try {    List<QueueManagementChange> queueManagementChanges=new ArrayList<>();    List<FiCaSchedulerApp> pendingApps=getSortedPendingApplications();    Map<String,QueueCapacities> leafQueueEntitlements=new HashMap<>();    for (    String nodeLabel : leafQueueTemplateNodeLabels) {      float parentAbsoluteCapacity=managedParentQueue.getQueueCapacities().getAbsoluteCapacity(nodeLabel);      float leafQueueTemplateAbsoluteCapacity=leafQueueTemplateCapacities.getAbsoluteCapacity(nodeLabel);      Map<String,QueueCapacities> deactivatedLeafQueues=deactivateLeafQueuesIfInActive(managedParentQueue,nodeLabel,leafQueueEntitlements);      if (LOG.isDebugEnabled()) {        if (deactivatedLeafQueues.size() > 0) {
  try {    List<QueueManagementChange> queueManagementChanges=new ArrayList<>();    List<FiCaSchedulerApp> pendingApps=getSortedPendingApplications();    Map<String,QueueCapacities> leafQueueEntitlements=new HashMap<>();    for (    String nodeLabel : leafQueueTemplateNodeLabels) {      float parentAbsoluteCapacity=managedParentQueue.getQueueCapacities().getAbsoluteCapacity(nodeLabel);      float leafQueueTemplateAbsoluteCapacity=leafQueueTemplateCapacities.getAbsoluteCapacity(nodeLabel);      Map<String,QueueCapacities> deactivatedLeafQueues=deactivateLeafQueuesIfInActive(managedParentQueue,nodeLabel,leafQueueEntitlements);      if (LOG.isDebugEnabled()) {        if (deactivatedLeafQueues.size() > 0) {          LOG.debug("Parent queue = {},  " + ", nodeLabel = {}, deactivated leaf queues = [{}] ",managedParentQueue.getQueuePath(),nodeLabel,deactivatedLeafQueues.size() > 25 ? deactivatedLeafQueues.size() : deactivatedLeafQueues);        }      }      float deactivatedCapacity=getTotalDeactivatedCapacity(deactivatedLeafQueues,nodeLabel);      float sumOfChildQueueActivatedCapacity=parentQueueState.getAbsoluteActivatedChildQueueCapacity(nodeLabel);      float availableCapacity=parentAbsoluteCapacity - sumOfChildQueueActivatedCapacity + deactivatedCapacity + EPSILON;      if (LOG.isDebugEnabled()) {
      float parentAbsoluteCapacity=managedParentQueue.getQueueCapacities().getAbsoluteCapacity(nodeLabel);      float leafQueueTemplateAbsoluteCapacity=leafQueueTemplateCapacities.getAbsoluteCapacity(nodeLabel);      Map<String,QueueCapacities> deactivatedLeafQueues=deactivateLeafQueuesIfInActive(managedParentQueue,nodeLabel,leafQueueEntitlements);      if (LOG.isDebugEnabled()) {        if (deactivatedLeafQueues.size() > 0) {          LOG.debug("Parent queue = {},  " + ", nodeLabel = {}, deactivated leaf queues = [{}] ",managedParentQueue.getQueuePath(),nodeLabel,deactivatedLeafQueues.size() > 25 ? deactivatedLeafQueues.size() : deactivatedLeafQueues);        }      }      float deactivatedCapacity=getTotalDeactivatedCapacity(deactivatedLeafQueues,nodeLabel);      float sumOfChildQueueActivatedCapacity=parentQueueState.getAbsoluteActivatedChildQueueCapacity(nodeLabel);      float availableCapacity=parentAbsoluteCapacity - sumOfChildQueueActivatedCapacity + deactivatedCapacity + EPSILON;      if (LOG.isDebugEnabled()) {        LOG.debug("Parent queue = " + managedParentQueue.getQueuePath() + ", nodeLabel = "+ nodeLabel+ ", absCapacity = "+ parentAbsoluteCapacity+ ", leafQueueAbsoluteCapacity = "+ leafQueueTemplateAbsoluteCapacity+ ", deactivatedCapacity = "+ deactivatedCapacity+ " , absChildActivatedCapacity = "+ sumOfChildQueueActivatedCapacity+ ", availableCapacity = "+ availableCapacity);      }      if (availableCapacity >= leafQueueTemplateAbsoluteCapacity) {        if (pendingApps.size() > 0) {          int maxLeafQueuesTobeActivated=getMaxLeavesToBeActivated(availableCapacity,leafQueueTemplateAbsoluteCapacity,pendingApps.size());          if (LOG.isDebugEnabled()) {
  writeLock.lock();  try {    Set<String> newPartitions=new HashSet<>();    Set<String> newQueues=new HashSet<>();    for (    CSQueue newQueue : managedParentQueue.getChildQueues()) {      if (newQueue instanceof LeafQueue) {        for (        String nodeLabel : leafQueueTemplateNodeLabels) {          leafQueueState.createLeafQueueStateIfNotExists((LeafQueue)newQueue,nodeLabel);          newPartitions.add(nodeLabel);        }        newQueues.add(newQueue.getQueuePath());      }    }    for (Iterator<Map.Entry<String,Map<String,LeafQueueStatePerPartition>>> itr=leafQueueState.getLeafQueueStateMap().entrySet().iterator(); itr.hasNext(); ) {      Map.Entry<String,Map<String,LeafQueueStatePerPartition>> e=itr.next();      String partition=e.getKey();      if (!newPartitions.contains(partition)) {        itr.remove();
          leafQueueState.createLeafQueueStateIfNotExists((LeafQueue)newQueue,nodeLabel);          newPartitions.add(nodeLabel);        }        newQueues.add(newQueue.getQueuePath());      }    }    for (Iterator<Map.Entry<String,Map<String,LeafQueueStatePerPartition>>> itr=leafQueueState.getLeafQueueStateMap().entrySet().iterator(); itr.hasNext(); ) {      Map.Entry<String,Map<String,LeafQueueStatePerPartition>> e=itr.next();      String partition=e.getKey();      if (!newPartitions.contains(partition)) {        itr.remove();        LOG.info(managedParentQueue.getQueuePath() + " : Removed partition " + partition+ " from leaf queue "+ "state");      } else {        Map<String,LeafQueueStatePerPartition> queues=e.getValue();        for (Iterator<Map.Entry<String,LeafQueueStatePerPartition>> queueItr=queues.entrySet().iterator(); queueItr.hasNext(); ) {          String queue=queueItr.next().getKey();          if (!newQueues.contains(queue)) {            queueItr.remove();
  try {    for (    QueueManagementChange queueManagementChange : queueManagementChanges) {      AutoCreatedLeafQueueConfig updatedQueueTemplate=queueManagementChange.getUpdatedQueueTemplate();      CSQueue queue=queueManagementChange.getQueue();      if (!(queue instanceof AutoCreatedLeafQueue)) {        throw new SchedulerDynamicEditException("Expected queue management change for AutoCreatedLeafQueue. " + "Found " + queue.getClass().getName());      }      AutoCreatedLeafQueue leafQueue=(AutoCreatedLeafQueue)queue;      for (      String nodeLabel : updatedQueueTemplate.getQueueCapacities().getExistingNodeLabels()) {        if (updatedQueueTemplate.getQueueCapacities().getCapacity(nodeLabel) > 0) {          if (isActive(leafQueue,nodeLabel)) {            LOG.debug("Queue is already active. Skipping activation : {}",leafQueue.getQueuePath());          } else {            activate(leafQueue,nodeLabel);          }        } else {          if (!isActive(leafQueue,nodeLabel)) {
private boolean anyContainerInFinalState(ResourceCommitRequest<FiCaSchedulerApp,FiCaSchedulerNode> request){  for (  SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> c : request.getContainersToRelease()) {    if (rmContainerInFinalState(c.getRmContainer())) {
    if (rmContainerInFinalState(c.getRmContainer())) {      LOG.debug("To-release container={} is in final state",c.getRmContainer());      return true;    }  }  for (  ContainerAllocationProposal<FiCaSchedulerApp,FiCaSchedulerNode> c : request.getContainersToAllocate()) {    for (    SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> r : c.getToRelease()) {      if (rmContainerInFinalState(r.getRmContainer())) {        LOG.debug("To-release container={}, for to a new allocated" + " container, is in final state",r.getRmContainer());        return true;      }    }    if (null != c.getAllocateFromReservedContainer()) {      if (rmContainerInFinalState(c.getAllocateFromReservedContainer().getRmContainer())) {        LOG.debug("Allocate from reserved container {} is in final state",c.getAllocateFromReservedContainer().getRmContainer());        return true;      }    }  }  for (  ContainerAllocationProposal<FiCaSchedulerApp,FiCaSchedulerNode> c : request.getContainersToReserve()) {    for (    SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> r : c.getToRelease()) {      if (rmContainerInFinalState(r.getRmContainer())) {
      return false;    }  }  if (allocation.getAllocateFromReservedContainer() != null && reservedContainerOnNode == null) {    if (LOG.isDebugEnabled()) {      LOG.debug("Try to allocate from reserved container " + allocation.getAllocateFromReservedContainer().getRmContainer().getContainerId() + ", but node is not reserved");    }    return false;  }  Resource availableResource=Resources.clone(schedulerContainer.getSchedulerNode().getUnallocatedResource());  if (allocation.getToRelease() != null && !allocation.getToRelease().isEmpty()) {    for (    SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> releaseContainer : allocation.getToRelease()) {      if (releaseContainer.getRmContainer().getState() == RMContainerState.RESERVED && releaseContainer.getRmContainer() != releaseContainer.getSchedulerNode().getReservedContainer()) {        if (LOG.isDebugEnabled()) {          LOG.debug("Failed to accept this proposal because " + "it tries to release an outdated reserved container " + releaseContainer.getRmContainer().getContainerId() + " on node "+ releaseContainer.getSchedulerNode().getNodeID()+ " whose reserved container is "+ releaseContainer.getSchedulerNode().getReservedContainer());        }        return false;      }      if (releaseContainer.getRmContainer().getState() != RMContainerState.RESERVED && releaseContainer.getSchedulerNode() == schedulerContainer.getSchedulerNode()) {        Resources.addTo(availableResource,releaseContainer.getRmContainer().getAllocatedResource());      }    }  }  if (!Resources.fitsIn(rc,allocation.getAllocatedOrReservedResource(),availableResource)) {
  readLock.lock();  try {    if (anyContainerInFinalState(request)) {      return false;    }    if (request.anythingAllocatedOrReserved()) {      ContainerAllocationProposal<FiCaSchedulerApp,FiCaSchedulerNode> allocation=request.getFirstAllocatedOrReservedContainer();      SchedulerContainer<FiCaSchedulerApp,FiCaSchedulerNode> schedulerContainer=allocation.getAllocatedOrReservedContainer();      if (schedulerContainer.getSchedulerNode().getRMNode().getState() != NodeState.RUNNING) {        if (LOG.isDebugEnabled()) {          LOG.debug("Failed to accept this proposal because node " + schedulerContainer.getSchedulerNode().getNodeID() + " is in "+ schedulerContainer.getSchedulerNode().getRMNode().getState()+ " state (not RUNNING)");        }        return false;      }      if (schedulerContainer.isAllocated()) {        containerRequest=schedulerContainer.getRmContainer().getContainerRequest();        if (checkPending && !appSchedulingInfo.checkAllocation(allocation.getAllocationLocalityType(),schedulerContainer.getSchedulerNode(),schedulerContainer.getSchedulerRequestKey())) {          if (LOG.isDebugEnabled()) {
      if (schedulerContainer.getSchedulerNode().getRMNode().getState() != NodeState.RUNNING) {        if (LOG.isDebugEnabled()) {          LOG.debug("Failed to accept this proposal because node " + schedulerContainer.getSchedulerNode().getNodeID() + " is in "+ schedulerContainer.getSchedulerNode().getRMNode().getState()+ " state (not RUNNING)");        }        return false;      }      if (schedulerContainer.isAllocated()) {        containerRequest=schedulerContainer.getRmContainer().getContainerRequest();        if (checkPending && !appSchedulingInfo.checkAllocation(allocation.getAllocationLocalityType(),schedulerContainer.getSchedulerNode(),schedulerContainer.getSchedulerRequestKey())) {          if (LOG.isDebugEnabled()) {            LOG.debug("No pending resource for: nodeType=" + allocation.getAllocationLocalityType() + ", node="+ schedulerContainer.getSchedulerNode()+ ", requestKey="+ schedulerContainer.getSchedulerRequestKey()+ ", application="+ getApplicationAttemptId());          }          return false;        }        if (!commonCheckContainerAllocation(allocation,schedulerContainer)) {          return false;        }      } else {        if (schedulerContainer.getRmContainer().getState() == RMContainerState.RESERVED) {          if (schedulerContainer.getRmContainer() != schedulerContainer.getSchedulerNode().getReservedContainer()) {
        if (allocation.getAllocateFromReservedContainer() != null) {          RMContainer reservedContainer=allocation.getAllocateFromReservedContainer().getRmContainer();          unreserve(schedulerContainer.getSchedulerRequestKey(),schedulerContainer.getSchedulerNode(),reservedContainer);        }        addToNewlyAllocatedContainers(schedulerContainer.getSchedulerNode(),rmContainer);        liveContainers.put(containerId,rmContainer);        if (updatePending) {          ContainerRequest containerRequest=appSchedulingInfo.allocate(allocation.getAllocationLocalityType(),schedulerContainer.getSchedulerNode(),schedulerContainer.getSchedulerRequestKey(),schedulerContainer.getRmContainer());          ((RMContainerImpl)rmContainer).setContainerRequest(containerRequest);          if (containerRequest != null && containerRequest.getSchedulingRequest() != null) {            ((RMContainerImpl)rmContainer).setAllocationTags(containerRequest.getSchedulingRequest().getAllocationTags());          }        } else {          AppSchedulingInfo.updateMetrics(getApplicationId(),allocation.getAllocationLocalityType(),schedulerContainer.getSchedulerNode(),schedulerContainer.getRmContainer(),getUser(),getQueue());        }        attemptResourceUsage.incUsed(schedulerContainer.getNodePartition(),allocation.getAllocatedOrReservedResource());        rmContainer.handle(new RMContainerEvent(containerId,RMContainerEventType.START));        schedulerContainer.getSchedulerNode().allocateContainer(rmContainer);
          ContainerRequest containerRequest=appSchedulingInfo.allocate(allocation.getAllocationLocalityType(),schedulerContainer.getSchedulerNode(),schedulerContainer.getSchedulerRequestKey(),schedulerContainer.getRmContainer());          ((RMContainerImpl)rmContainer).setContainerRequest(containerRequest);          if (containerRequest != null && containerRequest.getSchedulingRequest() != null) {            ((RMContainerImpl)rmContainer).setAllocationTags(containerRequest.getSchedulingRequest().getAllocationTags());          }        } else {          AppSchedulingInfo.updateMetrics(getApplicationId(),allocation.getAllocationLocalityType(),schedulerContainer.getSchedulerNode(),schedulerContainer.getRmContainer(),getUser(),getQueue());        }        attemptResourceUsage.incUsed(schedulerContainer.getNodePartition(),allocation.getAllocatedOrReservedResource());        rmContainer.handle(new RMContainerEvent(containerId,RMContainerEventType.START));        schedulerContainer.getSchedulerNode().allocateContainer(rmContainer);        incNumAllocatedContainers(allocation.getAllocationLocalityType(),allocation.getRequestLocalityType());        if (LOG.isDebugEnabled()) {          LOG.debug("allocate: applicationAttemptId=" + containerId.getApplicationAttemptId() + " container="+ containerId+ " host="+ rmContainer.getAllocatedNode().getHost()+ " type="+ allocation.getAllocationLocalityType());        }        String partition=schedulerContainer.getSchedulerNode().getPartition();        if (partition != null && partition.isEmpty()) {          partition=null;
@VisibleForTesting public NodeId getNodeIdToUnreserve(SchedulerRequestKey schedulerKey,Resource resourceNeedUnreserve,ResourceCalculator resourceCalculator){  Map<NodeId,RMContainer> reservedContainers=this.reservedContainers.get(schedulerKey);  if ((reservedContainers != null) && (!reservedContainers.isEmpty())) {    for (    Map.Entry<NodeId,RMContainer> entry : reservedContainers.entrySet()) {      NodeId nodeId=entry.getKey();      RMContainer reservedContainer=entry.getValue();      Resource reservedResource=reservedContainer.getReservedResource();      if (Resources.fitsIn(resourceCalculator,resourceNeedUnreserve,reservedResource)) {
      LOG.debug("Failed to move reservation, two nodes are in" + " different partition");      return false;    }    Map<NodeId,RMContainer> map=reservedContainers.get(reservedContainer.getReservedSchedulerKey());    if (null == map) {      LOG.debug("Cannot find reserved container map.");      return false;    }    if (sourceNode.getReservedContainer() != reservedContainer) {      LOG.debug("To-be-moved container already updated.");      return false;    }synchronized (targetNode) {      if (targetNode.getReservedContainer() != null) {        LOG.debug("Target node is already occupied before moving");      }      try {        targetNode.reserveResource(this,reservedContainer.getReservedSchedulerKey(),reservedContainer);      } catch (      IllegalStateException e) {
protected synchronized void allocateContainer(RMContainer rmContainer,boolean launchedOnNode){  super.allocateContainer(rmContainer,launchedOnNode);  final Container container=rmContainer.getContainer();
  Map<String,PlacementConstraint> constraintsForApp=new HashMap<>();  readLock.lock();  try {    if (appConstraints.get(appId) != null) {      LOG.warn("Application {} has already been registered.",appId);      return;    }    for (    Map.Entry<Set<String>,PlacementConstraint> entry : constraintMap.entrySet()) {      Set<String> sourceTags=entry.getKey();      PlacementConstraint constraint=entry.getValue();      if (validateConstraint(sourceTags,constraint)) {        String sourceTag=getValidSourceTag(sourceTags);        constraintsForApp.put(sourceTag,constraint);      }    }  }  finally {    readLock.unlock();  }  if (constraintsForApp.isEmpty()) {
@Override public void addConstraint(ApplicationId appId,Set<String> sourceTags,PlacementConstraint placementConstraint,boolean replace){  writeLock.lock();  try {    Map<String,PlacementConstraint> constraintsForApp=appConstraints.get(appId);    if (constraintsForApp == null) {
private void addConstraintToMap(Map<String,PlacementConstraint> constraintMap,Set<String> sourceTags,PlacementConstraint placementConstraint,boolean replace){  if (validateConstraint(sourceTags,placementConstraint)) {    String sourceTag=getValidSourceTag(sourceTags);    if (constraintMap.get(sourceTag) == null || replace) {      if (replace) {
@Override public Map<Set<String>,PlacementConstraint> getConstraints(ApplicationId appId){  readLock.lock();  try {    if (appConstraints.get(appId) == null) {
private static boolean getNodeConstraintEvaluatedResult(SchedulerNode schedulerNode,NodeAttributeOpCode opCode,NodeAttribute requestAttribute){  if (schedulerNode.getNodeAttributes() == null || !schedulerNode.getNodeAttributes().contains(requestAttribute)) {    if (opCode == NodeAttributeOpCode.NE) {
      LOG.debug("Incoming requestAttribute:{} is not present in {}," + " however opcode is NE. Hence accept this node.",requestAttribute,schedulerNode.getNodeID());      return true;    }    LOG.debug("Incoming requestAttribute:{} is not present in {}," + " skip such node.",requestAttribute,schedulerNode.getNodeID());    return false;  }  boolean found=false;  for (Iterator<NodeAttribute> it=schedulerNode.getNodeAttributes().iterator(); it.hasNext(); ) {    NodeAttribute nodeAttribute=it.next();    if (LOG.isDebugEnabled()) {      LOG.debug("Starting to compare Incoming requestAttribute :" + requestAttribute + " with requestAttribute value= "+ requestAttribute.getAttributeValue()+ ", stored nodeAttribute value="+ nodeAttribute.getAttributeValue());    }    if (requestAttribute.equals(nodeAttribute)) {      if (isOpCodeMatches(requestAttribute,nodeAttribute,opCode)) {        LOG.debug("Incoming requestAttribute:{} matches with node:{}",requestAttribute,schedulerNode.getNodeID());        found=true;        return found;      }    }  }  if (!found) {
private static boolean canSatisfyConstraints(ApplicationId appId,PlacementConstraint constraint,SchedulerNode node,AllocationTagsManager atm,Optional<DiagnosticsCollector> dcOpt) throws InvalidAllocationTagsQueryException {  if (constraint == null) {
private static NodeAttribute getNodeConstraintFromRequest(String attrKey,String attrString){  NodeAttribute nodeAttribute=null;
private void processPlacementConstraints(ApplicationId applicationId,Map<Set<String>,PlacementConstraint> appPlacementConstraints){  if (appPlacementConstraints != null && !appPlacementConstraints.isEmpty()) {
  ConstraintPlacementAlgorithm algorithm=null;  if (instances != null && !instances.isEmpty()) {    algorithm=instances.get(0);  } else {    algorithm=new DefaultPlacementAlgorithm();  }  LOG.info("Placement Algorithm [{}]",algorithm.getClass().getName());  String iteratorName=((RMContextImpl)amsContext).getYarnConfiguration().get(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_ALGORITHM_ITERATOR,BatchedRequests.IteratorType.SERIAL.name());  LOG.info("Placement Algorithm Iterator[{}]",iteratorName);  try {    iteratorType=BatchedRequests.IteratorType.valueOf(iteratorName);  } catch (  IllegalArgumentException e) {    throw new YarnRuntimeException("Could not instantiate Placement Algorithm Iterator: ",e);  }  int algoPSize=((RMContextImpl)amsContext).getYarnConfiguration().getInt(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE,YarnConfiguration.DEFAULT_RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE);  this.placementDispatcher=new PlacementDispatcher();  this.placementDispatcher.init(((RMContextImpl)amsContext),algorithm,algoPSize);
  } else {    algorithm=new DefaultPlacementAlgorithm();  }  LOG.info("Placement Algorithm [{}]",algorithm.getClass().getName());  String iteratorName=((RMContextImpl)amsContext).getYarnConfiguration().get(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_ALGORITHM_ITERATOR,BatchedRequests.IteratorType.SERIAL.name());  LOG.info("Placement Algorithm Iterator[{}]",iteratorName);  try {    iteratorType=BatchedRequests.IteratorType.valueOf(iteratorName);  } catch (  IllegalArgumentException e) {    throw new YarnRuntimeException("Could not instantiate Placement Algorithm Iterator: ",e);  }  int algoPSize=((RMContextImpl)amsContext).getYarnConfiguration().getInt(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE,YarnConfiguration.DEFAULT_RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE);  this.placementDispatcher=new PlacementDispatcher();  this.placementDispatcher.init(((RMContextImpl)amsContext),algorithm,algoPSize);  LOG.info("Planning Algorithm pool size [{}]",algoPSize);  int schedPSize=((RMContextImpl)amsContext).getYarnConfiguration().getInt(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_SCHEDULER_POOL_SIZE,YarnConfiguration.DEFAULT_RM_PLACEMENT_CONSTRAINTS_SCHEDULER_POOL_SIZE);  this.schedulingThreadPool=Executors.newFixedThreadPool(schedPSize);
void dispatch(final BatchedRequests batchedRequests){  final ConstraintPlacementAlgorithmOutputCollector collector=this;  Runnable placingTask=() -> {
@Override public void collect(ConstraintPlacementAlgorithmOutput placement){  if (!placement.getPlacedRequests().isEmpty()) {    List<PlacedSchedulingRequest> processed=placedRequests.computeIfAbsent(placement.getApplicationId(),k -> new ArrayList<>());synchronized (processed) {
@Override public List<Container> allocateContainers(ResourceBlacklistRequest blackList,List<ResourceRequest> oppResourceReqs,ApplicationAttemptId applicationAttemptId,OpportunisticContainerContext opportContext,long rmIdentifier,String appSubmitter) throws YarnException {  updateBlacklist(blackList,opportContext);  opportContext.addToOutstandingReqs(oppResourceReqs);  Set<String> nodeBlackList=new HashSet<>(opportContext.getBlacklist());  List<Container> allocatedContainers=new ArrayList<>();  int maxAllocationsPerAMHeartbeat=getMaxAllocationsPerAMHeartbeat();  List<Map<Resource,List<Allocation>>> allocations=new ArrayList<>();  for (  SchedulerRequestKey schedulerKey : opportContext.getOutstandingOpReqs().descendingKeySet()) {    int remAllocs=-1;    if (maxAllocationsPerAMHeartbeat > 0) {      remAllocs=maxAllocationsPerAMHeartbeat - getTotalAllocations(allocations);      if (remAllocs <= 0) {
@SuppressWarnings("checkstyle:parameternumber") private List<Container> allocateNodeLocal(EnrichedResourceRequest enrichedAsk,String nodeLocation,int toAllocate,long rmIdentifier,AllocationParams appParams,ContainerIdGenerator idCounter,Set<String> blacklist,ApplicationAttemptId id,String userName,Map<Resource,List<Allocation>> allocations) throws YarnException {  List<Container> allocatedContainers=new ArrayList<>();  while (toAllocate > 0) {    RMNode node=nodeQueueLoadMonitor.selectLocalNode(nodeLocation,blacklist);    if (node != null) {      toAllocate--;      Container container=createContainer(rmIdentifier,appParams,idCounter,id,userName,allocations,nodeLocation,enrichedAsk.getRequest(),convertToRemoteNode(node));      allocatedContainers.add(container);
@SuppressWarnings("checkstyle:parameternumber") private List<Container> allocateRackLocal(EnrichedResourceRequest enrichedAsk,String rackLocation,int toAllocate,long rmIdentifier,AllocationParams appParams,ContainerIdGenerator idCounter,Set<String> blacklist,ApplicationAttemptId id,String userName,Map<Resource,List<Allocation>> allocations) throws YarnException {  List<Container> allocatedContainers=new ArrayList<>();  while (toAllocate > 0) {    RMNode node=nodeQueueLoadMonitor.selectRackLocalNode(rackLocation,blacklist);    if (node != null) {      toAllocate--;      Container container=createContainer(rmIdentifier,appParams,idCounter,id,userName,allocations,rackLocation,enrichedAsk.getRequest(),convertToRemoteNode(node));      allocatedContainers.add(container);      metrics.incrRackLocalOppContainers();
@SuppressWarnings("checkstyle:parameternumber") private List<Container> allocateAny(EnrichedResourceRequest enrichedAsk,int toAllocate,long rmIdentifier,AllocationParams appParams,ContainerIdGenerator idCounter,Set<String> blacklist,ApplicationAttemptId id,String userName,Map<Resource,List<Allocation>> allocations) throws YarnException {  List<Container> allocatedContainers=new ArrayList<>();  while (toAllocate > 0) {    RMNode node=nodeQueueLoadMonitor.selectAnyNode(blacklist);    if (node != null) {      toAllocate--;      Container container=createContainer(rmIdentifier,appParams,idCounter,id,userName,allocations,ResourceRequest.ANY,enrichedAsk.getRequest(),convertToRemoteNode(node));      allocatedContainers.add(container);      metrics.incrOffSwitchOppContainers();
@Override public void updateNode(RMNode rmNode){
protected void onNewNodeAdded(RMNode rmNode,OpportunisticContainersStatus status){  int opportQueueCapacity=status.getOpportQueueCapacity();  int estimatedQueueWaitTime=status.getEstimatedQueueWaitTime();  int waitQueueLength=status.getWaitQueueLength();  if (rmNode.getState() != NodeState.DECOMMISSIONING && (estimatedQueueWaitTime != -1 || comparator == LoadComparator.QUEUE_LENGTH)) {    this.clusterNodes.put(rmNode.getNodeID(),new ClusterNode(rmNode.getNodeID()).setQueueWaitTime(estimatedQueueWaitTime).setQueueLength(waitQueueLength).setNodeLabels(rmNode.getNodeLabels()).setQueueCapacity(opportQueueCapacity));
protected void onExistingNodeUpdated(RMNode rmNode,ClusterNode clusterNode,OpportunisticContainersStatus status){  int estimatedQueueWaitTime=status.getEstimatedQueueWaitTime();  int waitQueueLength=status.getWaitQueueLength();  if (rmNode.getState() != NodeState.DECOMMISSIONING && (estimatedQueueWaitTime != -1 || comparator == LoadComparator.QUEUE_LENGTH)) {    clusterNode.setQueueWaitTime(estimatedQueueWaitTime).setQueueLength(waitQueueLength).setNodeLabels(rmNode.getNodeLabels()).updateTimestamp();
@Override public void updateNodeResource(RMNode rmNode,ResourceOption resourceOption){
  this.allocFile=getAllocationFile(conf);  if (this.allocFile != null) {    this.fs=allocFile.getFileSystem(conf);    reloadThread=new Thread(() -> {      while (running) {        try {synchronized (this) {            reloadListener.onCheck();          }          long time=clock.getTime();          long lastModified=fs.getFileStatus(allocFile).getModificationTime();          if (lastModified > lastSuccessfulReload && time > lastModified + ALLOC_RELOAD_WAIT_MS) {            try {              reloadAllocations();            } catch (            Exception ex) {              if (!lastReloadAttemptFailed) {
            reloadListener.onCheck();          }          long time=clock.getTime();          long lastModified=fs.getFileStatus(allocFile).getModificationTime();          if (lastModified > lastSuccessfulReload && time > lastModified + ALLOC_RELOAD_WAIT_MS) {            try {              reloadAllocations();            } catch (            Exception ex) {              if (!lastReloadAttemptFailed) {                LOG.error("Failed to reload fair scheduler config file - " + "will use existing allocations.",ex);              }              lastReloadAttemptFailed=true;            }          } else           if (lastModified == 0l) {            if (!lastReloadAttemptFailed) {              LOG.warn("Failed to reload fair scheduler config file because" + " last modified returned 0. File exists: " + fs.exists(allocFile));            }            lastReloadAttemptFailed=true;          }        } catch (        IOException e) {
void containerCompleted(RMContainer rmContainer,ContainerStatus containerStatus,RMContainerEventType event){  writeLock.lock();  try {    Container container=rmContainer.getContainer();    ContainerId containerId=container.getId();    if (liveContainers.remove(containerId) == null) {
@Override public Resource getHeadroom(){  final FSQueue fsQueue=getQueue();  SchedulingPolicy policy=fsQueue.getPolicy();  Resource queueFairShare=fsQueue.getFairShare();  Resource queueUsage=fsQueue.getResourceUsage();  Resource clusterResource=this.scheduler.getClusterResource();  Resource clusterUsage=this.scheduler.getRootQueueMetrics().getAllocatedResources();  Resource clusterAvailableResources=Resources.subtract(clusterResource,clusterUsage);  subtractResourcesOnBlacklistedNodes(clusterAvailableResources);  Resource queueMaxAvailableResources=Resources.subtract(fsQueue.getMaxShare(),queueUsage);  Resource maxAvailableResource=Resources.componentwiseMin(clusterAvailableResources,queueMaxAvailableResources);  Resource headroom=policy.getHeadroom(queueFairShare,queueUsage,maxAvailableResource);
        this.resetAllowedLocalityLevel(schedulerKey,type);      }    }    if (getOutstandingAsksCount(schedulerKey) <= 0) {      return null;    }    container=reservedContainer;    if (container == null) {      container=createContainer(node,pendingAsk.getPerAllocationResource(),schedulerKey);    }    rmContainer=new RMContainerImpl(container,schedulerKey,getApplicationAttemptId(),node.getNodeID(),appSchedulingInfo.getUser(),rmContext);    ((RMContainerImpl)rmContainer).setQueueName(this.getQueueName());    addToNewlyAllocatedContainers(node,rmContainer);    liveContainers.put(container.getId(),rmContainer);    ContainerRequest containerRequest=appSchedulingInfo.allocate(type,node,schedulerKey,rmContainer);    this.attemptResourceUsage.incUsed(container.getResource());    getQueue().incUsedResource(container.getResource());    ((RMContainerImpl)rmContainer).setContainerRequest(containerRequest);    rmContainer.handle(new RMContainerEvent(container.getId(),RMContainerEventType.START));
private boolean reserve(Resource perAllocationResource,FSSchedulerNode node,Container reservedContainer,NodeType type,SchedulerRequestKey schedulerKey){  RMContainer nodeReservedContainer=node.getReservedContainer();  boolean reservableForThisApp=nodeReservedContainer == null || nodeReservedContainer.getApplicationAttemptId().equals(getApplicationAttemptId());  if (reservableForThisApp && !reservationExceedsThreshold(node,type)) {
private boolean reservationExceedsThreshold(FSSchedulerNode node,NodeType type){  if (type != NodeType.NODE_LOCAL) {    int existingReservations=getNumReservations(node.getRackName(),type == NodeType.OFF_SWITCH);    int totalAvailNodes=(type == NodeType.OFF_SWITCH) ? scheduler.getNumClusterNodes() : scheduler.getNumNodesInRack(node.getRackName());    int numAllowedReservations=(int)Math.ceil(totalAvailNodes * scheduler.getReservableNodesRatio());    if (existingReservations >= numAllowedReservations) {      DecimalFormat df=new DecimalFormat();      df.setMaximumFractionDigits(2);      if (LOG.isDebugEnabled()) {
  if (Resources.fitsIn(capability,available)) {    RMContainer allocatedContainer=allocate(type,node,schedulerKey,pendingAsk,reservedContainer);    if (allocatedContainer == null) {      if (reserved) {        unreserve(schedulerKey,node);      }      LOG.debug("Resource ask {} fits in available node resources {}," + " but no container was allocated",capability,available);      return Resources.none();    }    if (reserved) {      unreserve(schedulerKey,node);    }    node.allocateContainer(allocatedContainer);    if (!isAmRunning() && !getUnmanagedAM()) {      setAMResource(capability);      getQueue().addAMResourceUsage(capability);      setAmRunning(true);    }    return capability;
      return Resources.none();    }    if (reserved) {      unreserve(schedulerKey,node);    }    node.allocateContainer(allocatedContainer);    if (!isAmRunning() && !getUnmanagedAM()) {      setAMResource(capability);      getQueue().addAMResourceUsage(capability);      setAmRunning(true);    }    return capability;  }  LOG.debug("Resource request: {} exceeds the available" + " resources of the node.",capability);  if (isReservable(capability) && !node.isPreemptedForApp(this) && reserve(pendingAsk.getPerAllocationResource(),node,reservedContainer,type,schedulerKey)) {    updateAMDiagnosticMsg(capability," exceeds the available resources of " + "the node and the request is reserved)");    LOG.debug("{}'s resource request is reserved.",getName());    return FairScheduler.CONTAINER_RESERVED;  } else {
boolean assignReservedContainer(FSSchedulerNode node){  RMContainer rmContainer=node.getReservedContainer();  SchedulerRequestKey reservedSchedulerKey=rmContainer.getReservedSchedulerKey();  if (!isValidReservation(node)) {
@Override public Resource assignContainer(FSSchedulerNode node){  if (isOverAMShareLimit()) {    PendingAsk amAsk=appSchedulingInfo.getNextPendingAsk();    updateAMDiagnosticMsg(amAsk.getPerAllocationResource()," exceeds maximum AM resource allowed).");    if (LOG.isDebugEnabled()) {
@Override public Resource assignContainer(FSSchedulerNode node){  Resource assigned=none();  if (LOG.isDebugEnabled()) {
@Override public void updateDemand(){  writeLock.lock();  try {    demand=Resources.createResource(0);    for (    FSQueue childQueue : childQueues) {      childQueue.updateDemand();      Resource toAdd=childQueue.getDemand();      demand=Resources.add(demand,toAdd);      if (LOG.isDebugEnabled()) {
@Override public Resource assignContainer(FSSchedulerNode node){  Resource assigned=Resources.none();  if (!assignContainerPreCheck(node)) {    if (LOG.isDebugEnabled()) {
private List<RMContainer> identifyContainersToPreempt(FSAppAttempt starvedApp){  List<RMContainer> containersToPreempt=new ArrayList<>();  for (  ResourceRequest rr : starvedApp.getStarvedResourceRequests()) {    List<FSSchedulerNode> potentialNodes=scheduler.getNodeTracker().getNodesByResourceName(rr.getResourceName());    for (int i=0; i < rr.getNumContainers(); i++) {      PreemptableContainers bestContainers=getBestPreemptableContainers(rr,potentialNodes);      if (bestContainers != null) {        List<RMContainer> containers=bestContainers.getAllContainers();        if (containers.size() > 0) {          containersToPreempt.addAll(containers);          trackPreemptionsAgainstNode(containers,starvedApp);          for (          RMContainer container : containers) {            FSAppAttempt app=scheduler.getSchedulerApp(container.getApplicationAttemptId());
private PreemptableContainers identifyContainersToPreemptOnNode(Resource request,FSSchedulerNode node,int maxAMContainers){  PreemptableContainers preemptableContainers=new PreemptableContainers(maxAMContainers);  List<RMContainer> containersToCheck=node.getRunningContainersWithAMsAtTheEnd();  containersToCheck.removeAll(node.getContainersForPreemption());  Resource potential=Resources.subtractFromNonNegative(Resources.clone(node.getUnallocatedResource()),node.getTotalReserved());  for (  RMContainer container : containersToCheck) {    FSAppAttempt app=scheduler.getSchedulerApp(container.getApplicationAttemptId());    if (app == null) {
@Override public void setFairShare(Resource fairShare){  this.fairShare=fairShare;  metrics.setFairShare(fairShare);
boolean assignContainerPreCheck(FSSchedulerNode node){  if (node.getReservedContainer() != null) {
boolean fitsInMaxShare(Resource additionalResource){  Resource usagePlusAddition=Resources.add(getResourceUsage(),additionalResource);  if (!Resources.fitsIn(usagePlusAddition,getMaxShare())) {    if (LOG.isDebugEnabled()) {
@Override protected synchronized void allocateContainer(RMContainer rmContainer,boolean launchedOnNode){  super.allocateContainer(rmContainer,launchedOnNode);  if (LOG.isDebugEnabled()) {    final Container container=rmContainer.getContainer();
  if (LOG.isDebugEnabled()) {    final Container container=rmContainer.getContainer();    LOG.debug("Assigned container " + container.getId() + " of capacity "+ container.getResource()+ " on host "+ getRMNode().getNodeAddress()+ ", which has "+ getNumContainers()+ " containers, "+ getAllocatedResource()+ " used and "+ getUnallocatedResource()+ " available after allocation");  }  Resource allocated=rmContainer.getAllocatedResource();  if (!Resources.isNone(allocated)) {    FSAppAttempt app=appIdToAppMap.get(rmContainer.getApplicationAttemptId());    if (app != null) {      Resource reserved=resourcesPreemptedForApp.get(app);      Resource fulfilled=Resources.componentwiseMin(reserved,allocated);      Resources.subtractFrom(reserved,fulfilled);      Resources.subtractFrom(totalResourcesPreempted,fulfilled);      if (Resources.isNone(reserved)) {        resourcesPreemptedForApp.remove(app);        appIdToAppMap.remove(rmContainer.getApplicationAttemptId());      }    }  } else {
private void dumpSchedulerState(){  FSQueue rootQueue=queueMgr.getRootQueue();  Resource clusterResource=getClusterResource();
private void dumpSchedulerState(){  FSQueue rootQueue=queueMgr.getRootQueue();  Resource clusterResource=getClusterResource();  STATE_DUMP_LOG.debug("FairScheduler state: Cluster Capacity: " + clusterResource + "  Allocations: "+ rootMetrics.getAllocatedResources()+ "  Availability: "+ Resource.newInstance(rootMetrics.getAvailableMB(),rootMetrics.getAvailableVirtualCores())+ "  Demand: "+ rootQueue.getDemand());
      if (!isAppRecovering) {        rejectApplicationWithMessage(applicationId,queueName + " is not a leaf queue");        return;      }      queueName="root.recovery";      queue=queueMgr.getLeafQueue(queueName,true,applicationId);    }    if (!isAppRecovering) {      UserGroupInformation userUgi=UserGroupInformation.createRemoteUser(user);      if (!queue.hasAccess(QueueACL.SUBMIT_APPLICATIONS,userUgi) && !queue.hasAccess(QueueACL.ADMINISTER_QUEUE,userUgi)) {        String msg="User " + user + " does not have permission to submit "+ applicationId+ " to queue "+ queueName;        rejectApplicationWithMessage(applicationId,msg);        queue.removeAssignedApp(applicationId);        return;      }    }    RMApp rmApp=rmContext.getRMApps().get(applicationId);    if (rmApp != null) {      rmApp.setQueue(queueName);
        rejectApplicationWithMessage(applicationId,msg);        queue.removeAssignedApp(applicationId);        return;      }    }    RMApp rmApp=rmContext.getRMApps().get(applicationId);    if (rmApp != null) {      rmApp.setQueue(queueName);    } else {      LOG.error("Couldn't find RM app for " + applicationId + " to set queue name on");    }    if (!isAppRecovering && rmApp != null && rmApp.getAMResourceRequests() != null) {      List<MaxResourceValidationResult> invalidAMResourceRequests=validateResourceRequests(rmApp.getAMResourceRequests(),queue);      if (!invalidAMResourceRequests.isEmpty()) {        String msg=String.format("Cannot submit application %s to queue %s because " + "it has zero amount of resource for a requested " + "resource! Invalid requested AM resources: %s, "+ "maximum queue resources: %s",applicationId,queueName,invalidAMResourceRequests,queue.getMaxShare());        rejectApplicationWithMessage(applicationId,msg);        queue.removeAssignedApp(applicationId);        return;
        return;      }    }    RMApp rmApp=rmContext.getRMApps().get(applicationId);    if (rmApp != null) {      rmApp.setQueue(queueName);    } else {      LOG.error("Couldn't find RM app for " + applicationId + " to set queue name on");    }    if (!isAppRecovering && rmApp != null && rmApp.getAMResourceRequests() != null) {      List<MaxResourceValidationResult> invalidAMResourceRequests=validateResourceRequests(rmApp.getAMResourceRequests(),queue);      if (!invalidAMResourceRequests.isEmpty()) {        String msg=String.format("Cannot submit application %s to queue %s because " + "it has zero amount of resource for a requested " + "resource! Invalid requested AM resources: %s, "+ "maximum queue resources: %s",applicationId,queueName,invalidAMResourceRequests,queue.getMaxShare());        rejectApplicationWithMessage(applicationId,msg);        queue.removeAssignedApp(applicationId);        return;      }    }    SchedulerApplication<FSAppAttempt> application=new SchedulerApplication<>(queue,user);    applications.put(applicationId,application);
  try {    SchedulerApplication<FSAppAttempt> application=applications.get(applicationAttemptId.getApplicationId());    String user=application.getUser();    FSLeafQueue queue=(FSLeafQueue)application.getQueue();    FSAppAttempt attempt=new FSAppAttempt(this,applicationAttemptId,user,queue,new ActiveUsersManager(getRootQueueMetrics()),rmContext);    if (transferStateFromPreviousAttempt) {      attempt.transferStateFromPreviousAttempt(application.getCurrentAppAttempt());    }    application.setCurrentAppAttempt(attempt);    boolean runnable=maxRunningEnforcer.canAppBeRunnable(queue,attempt);    queue.addApp(attempt,runnable);    if (runnable) {      maxRunningEnforcer.trackRunnableApp(attempt);    } else {      maxRunningEnforcer.trackNonRunnableApp(attempt);    }    queue.getMetrics().submitAppAttempt(user);
    String user=application.getUser();    FSLeafQueue queue=(FSLeafQueue)application.getQueue();    FSAppAttempt attempt=new FSAppAttempt(this,applicationAttemptId,user,queue,new ActiveUsersManager(getRootQueueMetrics()),rmContext);    if (transferStateFromPreviousAttempt) {      attempt.transferStateFromPreviousAttempt(application.getCurrentAppAttempt());    }    application.setCurrentAppAttempt(attempt);    boolean runnable=maxRunningEnforcer.canAppBeRunnable(queue,attempt);    queue.addApp(attempt,runnable);    if (runnable) {      maxRunningEnforcer.trackRunnableApp(attempt);    } else {      maxRunningEnforcer.trackNonRunnableApp(attempt);    }    queue.getMetrics().submitAppAttempt(user);    LOG.info("Added Application Attempt " + applicationAttemptId + " to scheduler from user: "+ user);    if (isAttemptRecovering) {
private void removeApplicationAttempt(ApplicationAttemptId applicationAttemptId,RMAppAttemptState rmAppAttemptFinalState,boolean keepContainers){  writeLock.lock();  try {
private void removeApplicationAttempt(ApplicationAttemptId applicationAttemptId,RMAppAttemptState rmAppAttemptFinalState,boolean keepContainers){  writeLock.lock();  try {    LOG.info("Application " + applicationAttemptId + " is done. finalState="+ rmAppAttemptFinalState);    FSAppAttempt attempt=getApplicationAttempt(applicationAttemptId);    if (attempt == null) {
@Override protected void completedContainerInternal(RMContainer rmContainer,ContainerStatus containerStatus,RMContainerEventType event){  writeLock.lock();  try {    Container container=rmContainer.getContainer();    FSAppAttempt application=getCurrentAttemptForContainer(container.getId());    ApplicationId appId=container.getId().getApplicationAttemptId().getApplicationId();    if (application == null) {
@Override protected void completedContainerInternal(RMContainer rmContainer,ContainerStatus containerStatus,RMContainerEventType event){  writeLock.lock();  try {    Container container=rmContainer.getContainer();    FSAppAttempt application=getCurrentAttemptForContainer(container.getId());    ApplicationId appId=container.getId().getApplicationAttemptId().getApplicationId();    if (application == null) {      LOG.info("Container " + container + " of finished application "+ appId+ " completed with event "+ event);      return;    }    NodeId nodeID=container.getNodeId();    FSSchedulerNode node=getFSSchedulerNode(nodeID);    if (rmContainer.getState() == RMContainerState.RESERVED) {      if (node != null) {        application.unreserve(rmContainer.getReservedSchedulerKey(),node);      } else {
    if (application == null) {      LOG.info("Container " + container + " of finished application "+ appId+ " completed with event "+ event);      return;    }    NodeId nodeID=container.getNodeId();    FSSchedulerNode node=getFSSchedulerNode(nodeID);    if (rmContainer.getState() == RMContainerState.RESERVED) {      if (node != null) {        application.unreserve(rmContainer.getReservedSchedulerKey(),node);      } else {        LOG.debug("Skipping unreserve on removed node: {}",nodeID);      }    } else {      application.containerCompleted(rmContainer,containerStatus,event);      if (node != null) {        node.releaseContainer(rmContainer.getContainerId(),false);      } else {
    }    NodeId nodeID=container.getNodeId();    FSSchedulerNode node=getFSSchedulerNode(nodeID);    if (rmContainer.getState() == RMContainerState.RESERVED) {      if (node != null) {        application.unreserve(rmContainer.getReservedSchedulerKey(),node);      } else {        LOG.debug("Skipping unreserve on removed node: {}",nodeID);      }    } else {      application.containerCompleted(rmContainer,containerStatus,event);      if (node != null) {        node.releaseContainer(rmContainer.getContainerId(),false);      } else {        LOG.debug("Skipping container release on removed node: {}",nodeID);      }      updateRootQueueMetrics();    }    if (LOG.isDebugEnabled()) {
private void addNode(List<NMContainerStatus> containerReports,RMNode node){  writeLock.lock();  try {    FSSchedulerNode schedulerNode=new FSSchedulerNode(node,usePortForNodeName);    nodeTracker.addNode(schedulerNode);    triggerUpdate();    Resource clusterResource=getClusterResource();    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);    queueMgr.getRootQueue().recomputeSteadyShares();
private void removeNode(RMNode rmNode){  writeLock.lock();  try {    NodeId nodeId=rmNode.getNodeID();    FSSchedulerNode node=nodeTracker.getNode(nodeId);    if (node == null) {
      LOG.error("Attempting to remove non-existent node " + nodeId);      return;    }    List<RMContainer> runningContainers=node.getCopiedListOfRunningContainers();    for (    RMContainer container : runningContainers) {      super.completedContainer(container,SchedulerUtils.createAbnormalContainerStatus(container.getContainerId(),SchedulerUtils.LOST_CONTAINER),RMContainerEventType.KILL);      node.releaseContainer(container.getContainerId(),true);    }    RMContainer reservedContainer=node.getReservedContainer();    if (reservedContainer != null) {      super.completedContainer(reservedContainer,SchedulerUtils.createAbnormalContainerStatus(reservedContainer.getContainerId(),SchedulerUtils.LOST_CONTAINER),RMContainerEventType.KILL);    }    nodeTracker.removeNode(nodeId);    Resource clusterResource=getClusterResource();    queueMgr.getRootQueue().setSteadyFairShare(clusterResource);    queueMgr.getRootQueue().recomputeSteadyShares();    updateRootQueueMetrics();    triggerUpdate();
@VisibleForTesting @Override public void killContainer(RMContainer container){  ContainerStatus status=SchedulerUtils.createKilledContainerStatus(container.getContainerId(),"Killed by RM to simulate an AM container failure");
@Override public Allocation allocate(ApplicationAttemptId appAttemptId,List<ResourceRequest> ask,List<SchedulingRequest> schedulingRequests,List<ContainerId> release,List<String> blacklistAdditions,List<String> blacklistRemovals,ContainerUpdates updateRequests){  FSAppAttempt application=getSchedulerApp(appAttemptId);  if (application == null) {
    LOG.error("Calling allocate on previous or removed " + "or non existent application attempt " + appAttemptId);    return EMPTY_ALLOCATION;  }  ApplicationId applicationId=application.getApplicationId();  FSLeafQueue queue=application.getQueue();  List<MaxResourceValidationResult> invalidAsks=validateResourceRequests(ask,queue);  if (!invalidAsks.isEmpty()) {    throw new SchedulerInvalidResoureRequestException(String.format("Resource request is invalid for application %s because queue %s " + "has 0 amount of resource for a resource type! " + "Validation result: %s",applicationId,queue.getName(),invalidAsks));  }  handleContainerUpdates(application,updateRequests);  normalizeResourceRequests(ask,queue.getName());  application.recordContainerRequestTime(getClock().getTime());  releaseContainers(release,application);  ReentrantReadWriteLock.WriteLock lock=application.getWriteLock();  lock.lock();  try {    if (!ask.isEmpty()) {
  }  handleContainerUpdates(application,updateRequests);  normalizeResourceRequests(ask,queue.getName());  application.recordContainerRequestTime(getClock().getTime());  releaseContainers(release,application);  ReentrantReadWriteLock.WriteLock lock=application.getWriteLock();  lock.lock();  try {    if (!ask.isEmpty()) {      if (LOG.isDebugEnabled()) {        LOG.debug("allocate: pre-update" + " applicationAttemptId=" + appAttemptId + " application="+ application.getApplicationId());      }      application.showRequests();      application.updateResourceRequests(ask);      application.showRequests();    }  }  finally {    lock.unlock();
    final NodeId nodeID=(node != null ? node.getNodeID() : null);    if (!nodeTracker.exists(nodeID)) {      LOG.info("Skipping scheduling as the node " + nodeID + " has been removed");      return;    }    assignPreemptedContainers(node);    FSAppAttempt reservedAppSchedulable=node.getReservedAppSchedulable();    boolean validReservation=false;    if (reservedAppSchedulable != null) {      validReservation=reservedAppSchedulable.assignReservedContainer(node);    }    if (!validReservation) {      int assignedContainers=0;      Resource assignedResource=Resources.clone(Resources.none());      Resource maxResourcesToAssign=Resources.multiply(node.getUnallocatedResource(),0.5f);      while (node.getReservedContainer() == null) {        Resource assignment=queueMgr.getRootQueue().assignContainer(node);
private void rejectApplicationWithMessage(ApplicationId applicationId,String msg){
@Override public boolean checkAccess(UserGroupInformation callerUGI,QueueACL acl,String queueName){  readLock.lock();  try {    FSQueue queue=getQueueManager().getQueue(queueName);    if (queue == null) {
  List<FSAppAttempt> noLongerPendingApps=new ArrayList<FSAppAttempt>();  while (iter.hasNext()) {    FSAppAttempt next=iter.next();    if (next == prev) {      continue;    }    if (canAppBeRunnable(next.getQueue(),next)) {      trackRunnableApp(next);      FSAppAttempt appSched=next;      next.getQueue().addApp(appSched,true);      noLongerPendingApps.add(appSched);      if (noLongerPendingApps.size() >= maxRunnableApps) {        break;      }    }    prev=next;  }  for (  FSAppAttempt appSched : noLongerPendingApps) {    if (!appSched.getQueue().removeNonRunnableApp(appSched)) {
    FSAppAttempt next=iter.next();    if (next == prev) {      continue;    }    if (canAppBeRunnable(next.getQueue(),next)) {      trackRunnableApp(next);      FSAppAttempt appSched=next;      next.getQueue().addApp(appSched,true);      noLongerPendingApps.add(appSched);      if (noLongerPendingApps.size() >= maxRunnableApps) {        break;      }    }    prev=next;  }  for (  FSAppAttempt appSched : noLongerPendingApps) {    if (!appSched.getQueue().removeNonRunnableApp(appSched)) {      LOG.error("Can't make app runnable that does not already exist in queue" + " as non-runnable: " + appSched + ". This should never happen.");    }    if (!usersNonRunnableApps.remove(appSched.getUser(),appSched)) {
private FSQueue createNewQueues(FSQueueType queueType,FSParentQueue topParent,List<String> newQueueNames){  AllocationConfiguration queueConf=scheduler.getAllocationConfiguration();  Iterator<String> i=newQueueNames.iterator();  FSParentQueue parent=topParent;  FSQueue queue=null;  while (i.hasNext()) {    FSParentQueue newParent=null;    String queueName=i.next();    SchedulingPolicy childPolicy=scheduler.getAllocationConfiguration().getSchedulingPolicy(queueName);    if (!parent.getPolicy().isChildPolicyAllowed(childPolicy)) {
  FSParentQueue parent=topParent;  FSQueue queue=null;  while (i.hasNext()) {    FSParentQueue newParent=null;    String queueName=i.next();    SchedulingPolicy childPolicy=scheduler.getAllocationConfiguration().getSchedulingPolicy(queueName);    if (!parent.getPolicy().isChildPolicyAllowed(childPolicy)) {      LOG.error("Can't create queue '" + queueName + "',"+ "the child scheduling policy is not allowed by parent queue!");      return null;    }    if (!i.hasNext() && (queueType != FSQueueType.PARENT)) {      FSLeafQueue leafQueue=new FSLeafQueue(queueName,scheduler,parent);      leafQueues.add(leafQueue);      queue=leafQueue;    } else {      if (childPolicy instanceof FifoPolicy) {
private static PlacementRule getParentRule(Element parent,FairScheduler fs) throws AllocationConfigurationException {
  siteConf.set(YarnConfiguration.FS_BASED_RM_CONF_STORE,outputDir);  ConfigurationProvider provider=new FileSystemBasedConfigurationProvider();  provider.init(siteConf);  rmContext.setConfigurationProvider(provider);  RMNodeLabelsManager mgr=new RMNodeLabelsManager();  mgr.init(siteConf);  rmContext.setNodeLabelManager(mgr);  try (CapacityScheduler cs=new CapacityScheduler()){    cs.setConf(siteConf);    cs.setRMContext(rmContext);    cs.serviceInit(csConfig);    cs.serviceStart();    LOG.info("Capacity scheduler was successfully started");    cs.serviceStop();  } catch (  Exception e) {
public void printDryRunResults(){  LOG.info("");  LOG.info("Results of dry run:");  LOG.info("");  int noOfErrors=errors.size();  int noOfWarnings=warnings.size();
public void printDryRunResults(){  LOG.info("");  LOG.info("Results of dry run:");  LOG.info("");  int noOfErrors=errors.size();  int noOfWarnings=warnings.size();  LOG.info("Number of errors: {}",noOfErrors);
static void logAndStdErr(Throwable t,String msg){
static void logAndStdErr(Throwable t,String msg){  LOG.debug("Stack trace",t);
private void loadConversionRules(String rulesFile) throws IOException {  if (rulesFile != null) {
private void setActionForProperty(String property){  String action=properties.getProperty(property);  if (action == null) {
@Override public boolean isChildPolicyAllowed(SchedulingPolicy childPolicy){  if (childPolicy instanceof DominantResourceFairnessPolicy) {
    if (isStopped) {      return null;    }    if (getOutstandingAsksCount(schedulerKey) <= 0) {      return null;    }    RMContainer rmContainer=new RMContainerImpl(container,schedulerKey,this.getApplicationAttemptId(),node.getNodeID(),appSchedulingInfo.getUser(),this.rmContext,node.getPartition());    ((RMContainerImpl)rmContainer).setQueueName(this.getQueueName());    updateAMContainerDiagnostics(AMState.ASSIGNED,null);    addToNewlyAllocatedContainers(node,rmContainer);    ContainerId containerId=container.getId();    liveContainers.put(containerId,rmContainer);    ContainerRequest containerRequest=appSchedulingInfo.allocate(type,node,schedulerKey,rmContainer);    attemptResourceUsage.incUsed(node.getPartition(),container.getResource());    ((RMContainerImpl)rmContainer).setContainerRequest(containerRequest);    rmContainer.handle(new RMContainerEvent(containerId,RMContainerEventType.START));    if (LOG.isDebugEnabled()) {
@Override public Allocation allocate(ApplicationAttemptId applicationAttemptId,List<ResourceRequest> ask,List<SchedulingRequest> schedulingRequests,List<ContainerId> release,List<String> blacklistAdditions,List<String> blacklistRemovals,ContainerUpdates updateRequests){  FifoAppAttempt application=getApplicationAttempt(applicationAttemptId);  if (application == null) {
@Override public Allocation allocate(ApplicationAttemptId applicationAttemptId,List<ResourceRequest> ask,List<SchedulingRequest> schedulingRequests,List<ContainerId> release,List<String> blacklistAdditions,List<String> blacklistRemovals,ContainerUpdates updateRequests){  FifoAppAttempt application=getApplicationAttempt(applicationAttemptId);  if (application == null) {    LOG.error("Calling allocate on removed or non existent application " + applicationAttemptId.getApplicationId());    return EMPTY_ALLOCATION;  }  if (!application.getApplicationAttemptId().equals(applicationAttemptId)) {    LOG.error("Calling allocate on previous or removed " + "or non existent application attempt " + applicationAttemptId);    return EMPTY_ALLOCATION;  }  normalizeResourceRequests(ask);  releaseContainers(release,application);synchronized (application) {    if (application.isStopped()) {      LOG.info("Calling allocate on a stopped " + "application " + applicationAttemptId);      return EMPTY_ALLOCATION;    }    if (!ask.isEmpty()) {
    LOG.error("Calling allocate on removed or non existent application " + applicationAttemptId.getApplicationId());    return EMPTY_ALLOCATION;  }  if (!application.getApplicationAttemptId().equals(applicationAttemptId)) {    LOG.error("Calling allocate on previous or removed " + "or non existent application attempt " + applicationAttemptId);    return EMPTY_ALLOCATION;  }  normalizeResourceRequests(ask);  releaseContainers(release,application);synchronized (application) {    if (application.isStopped()) {      LOG.info("Calling allocate on a stopped " + "application " + applicationAttemptId);      return EMPTY_ALLOCATION;    }    if (!ask.isEmpty()) {      LOG.debug("allocate: pre-update" + " applicationId=" + applicationAttemptId + " application="+ application);      application.showRequests();      application.updateResourceRequests(ask);
  }  if (!application.getApplicationAttemptId().equals(applicationAttemptId)) {    LOG.error("Calling allocate on previous or removed " + "or non existent application attempt " + applicationAttemptId);    return EMPTY_ALLOCATION;  }  normalizeResourceRequests(ask);  releaseContainers(release,application);synchronized (application) {    if (application.isStopped()) {      LOG.info("Calling allocate on a stopped " + "application " + applicationAttemptId);      return EMPTY_ALLOCATION;    }    if (!ask.isEmpty()) {      LOG.debug("allocate: pre-update" + " applicationId=" + applicationAttemptId + " application="+ application);      application.showRequests();      application.updateResourceRequests(ask);      LOG.debug("allocate: post-update" + " applicationId=" + applicationAttemptId + " application="+ application);      application.showRequests();
@VisibleForTesting public synchronized void addApplication(ApplicationId applicationId,String queue,String user,boolean isAppRecovering){  SchedulerApplication<FifoAppAttempt> application=new SchedulerApplication<>(DEFAULT_QUEUE,user);  applications.put(applicationId,application);  metrics.submitApp(user);
@VisibleForTesting public synchronized void addApplication(ApplicationId applicationId,String queue,String user,boolean isAppRecovering){  SchedulerApplication<FifoAppAttempt> application=new SchedulerApplication<>(DEFAULT_QUEUE,user);  applications.put(applicationId,application);  metrics.submitApp(user);  LOG.info("Accepted application " + applicationId + " from user: "+ user+ ", currently num of applications: "+ applications.size());  if (isAppRecovering) {
private int assignContainer(FiCaSchedulerNode node,FifoAppAttempt application,SchedulerRequestKey schedulerKey,int assignableContainers,Resource capability,NodeType type){
}break;case APP_ADDED:{AppAddedSchedulerEvent appAddedEvent=(AppAddedSchedulerEvent)event;addApplication(appAddedEvent.getApplicationId(),appAddedEvent.getQueue(),appAddedEvent.getUser(),appAddedEvent.getIsAppRecovering());}break;case APP_REMOVED:{AppRemovedSchedulerEvent appRemovedEvent=(AppRemovedSchedulerEvent)event;doneApplication(appRemovedEvent.getApplicationID(),appRemovedEvent.getFinalState());}break;case APP_ATTEMPT_ADDED:{AppAttemptAddedSchedulerEvent appAttemptAddedEvent=(AppAttemptAddedSchedulerEvent)event;addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),appAttemptAddedEvent.getTransferStateFromPreviousAttempt(),appAttemptAddedEvent.getIsAttemptRecovering());}break;case APP_ATTEMPT_REMOVED:{AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent=(AppAttemptRemovedSchedulerEvent)event;
@Lock(FifoScheduler.class) @Override protected synchronized void completedContainerInternal(RMContainer rmContainer,ContainerStatus containerStatus,RMContainerEventType event){  Container container=rmContainer.getContainer();  FifoAppAttempt application=getCurrentAttemptForContainer(container.getId());  ApplicationId appId=container.getId().getApplicationAttemptId().getApplicationId();  FiCaSchedulerNode node=(FiCaSchedulerNode)getNode(container.getNodeId());  if (application == null) {
@VisibleForTesting @Override public void killContainer(RMContainer container){  ContainerStatus status=SchedulerUtils.createKilledContainerStatus(container.getContainerId(),"Killed by RM to simulate an AM container failure");
public void initialize(AppSchedulingInfo appSchedulingInfo,SchedulerRequestKey schedulerRequestKey,RMContext rmContext){  this.appSchedulingInfo=appSchedulingInfo;  this.rmContext=rmContext;  this.schedulerRequestKey=schedulerRequestKey;  multiNodeSortPolicyName=appSchedulingInfo.getApplicationSchedulingEnvs().get(ApplicationSchedulingConfig.ENV_MULTI_NODE_SORTING_POLICY_CLASS);  multiNodeSortingManager=(MultiNodeSortingManager<N>)rmContext.getMultiNodeSortingManager();  if (LOG.isDebugEnabled()) {
@Override public boolean precheckNode(SchedulerNode schedulerNode,SchedulingMode schedulingMode,Optional<DiagnosticsCollector> dcOpt){
@Override public void showRequests(){  for (  ResourceRequest request : resourceRequestMap.values()) {    if (request.getNumContainers() > 0) {
public void registerMultiNodePolicyNames(boolean isMultiNodePlacementEnabled,Set<MultiNodePolicySpec> multiNodePlacementPolicies){  this.policySpecs.addAll(multiNodePlacementPolicies);  this.multiNodePlacementEnabled=isMultiNodePlacementEnabled;
    if (recoverContainer) {      newNumAllocations=existingNumAllocations + 1;    } else {      newNumAllocations=sizing.getNumAllocations();    }    sizing.setNumAllocations(existingNumAllocations);    if (!schedulingRequest.equals(newSchedulingRequest)) {      sizing.setNumAllocations(newNumAllocations);      throw new SchedulerInvalidResoureRequestException("Invalid updated SchedulingRequest added to scheduler, " + " we only allows changing numAllocations for the updated " + "SchedulingRequest. Old=" + schedulingRequest.toString() + " new="+ newSchedulingRequest.toString()+ ", if any fields need to be updated, please cancel the "+ "old request (by setting numAllocations to 0) and send a "+ "SchedulingRequest with different combination of "+ "priority/allocationId");    } else {      if (newNumAllocations == existingNumAllocations) {        return null;      }    }    sizing.setNumAllocations(newNumAllocations);    if (newNumAllocations < 0) {      throw new SchedulerInvalidResoureRequestException("numAllocation in ResourceSizing field must be >= 0, " + "updating schedulingRequest failed.");    }    PendingAskUpdateResult updateResult=new PendingAskUpdateResult(new PendingAsk(schedulingRequest.getResourceSizing()),new PendingAsk(newSchedulingRequest.getResourceSizing()),targetNodePartition,targetNodePartition);
@Override public void showRequests(){  readLock.lock();  try {    if (schedulingRequest != null) {
public void applicationMasterFinished(ApplicationAttemptId appAttemptId){  this.writeLock.lock();  try {
public Token<AMRMTokenIdentifier> createAndGetAMRMToken(ApplicationAttemptId appAttemptId){  this.writeLock.lock();  try {
public void addPersistedPassword(Token<AMRMTokenIdentifier> token) throws IOException {  this.writeLock.lock();  try {    AMRMTokenIdentifier identifier=token.decodeIdentifier();
@Override public byte[] retrievePassword(AMRMTokenIdentifier identifier) throws InvalidToken {  this.readLock.lock();  try {    ApplicationAttemptId applicationAttemptId=identifier.getApplicationAttemptId();
@Override @Private protected byte[] createPassword(AMRMTokenIdentifier identifier){  this.readLock.lock();  try {    ApplicationAttemptId applicationAttemptId=identifier.getApplicationAttemptId();
  serviceStateLock.writeLock().lock();  try {    isServiceStarted=false;    this.renewerService.shutdown();  }  finally {    serviceStateLock.writeLock().unlock();  }  dtCancelThread.interrupt();  try {    dtCancelThread.join(1000);  } catch (  InterruptedException e) {    e.printStackTrace();  }  if (tokenKeepAliveEnabled && delayedRemovalThread != null) {    delayedRemovalThread.interrupt();    try {      delayedRemovalThread.join(1000);
private void handleAppSubmitEvent(AbstractDelegationTokenRenewerAppEvent evt) throws IOException, InterruptedException {  ApplicationId applicationId=evt.getApplicationId();  Credentials ts=evt.getCredentials();  boolean shouldCancelAtEnd=evt.shouldCancelAtEnd();  if (ts == null) {    return;  }  LOG.debug("Registering tokens for renewal for: appId = {}",applicationId);  Collection<Token<?>> tokens=ts.getAllTokens();  long now=System.currentTimeMillis();  appTokens.put(applicationId,Collections.synchronizedSet(new HashSet<DelegationTokenToRenew>()));  Set<DelegationTokenToRenew> tokenList=new HashSet<DelegationTokenToRenew>();  boolean hasHdfsToken=false;  for (  Token<?> token : tokens) {    if (token.isManaged()) {      if (token.getKind().equals(HDFS_DELEGATION_KIND)) {
  long now=System.currentTimeMillis();  appTokens.put(applicationId,Collections.synchronizedSet(new HashSet<DelegationTokenToRenew>()));  Set<DelegationTokenToRenew> tokenList=new HashSet<DelegationTokenToRenew>();  boolean hasHdfsToken=false;  for (  Token<?> token : tokens) {    if (token.isManaged()) {      if (token.getKind().equals(HDFS_DELEGATION_KIND)) {        LOG.info(applicationId + " found existing hdfs token " + token);        hasHdfsToken=true;      }      if (skipTokenRenewal(token)) {        continue;      }      DelegationTokenToRenew dttr=allTokens.get(token);      if (dttr == null) {        Configuration tokenConf;        if (evt.tokenConf != null) {
      }      DelegationTokenToRenew dttr=allTokens.get(token);      if (dttr == null) {        Configuration tokenConf;        if (evt.tokenConf != null) {          tokenConf=evt.tokenConf;          LOG.info("Using app provided token conf for renewal," + " number of configs = " + tokenConf.size());          if (LOG.isDebugEnabled()) {            for (Iterator<Map.Entry<String,String>> itor=tokenConf.iterator(); itor.hasNext(); ) {              Map.Entry<String,String> entry=itor.next();              LOG.debug("Token conf key is {} and value is {}",entry.getKey(),entry.getValue());            }          }        } else {          tokenConf=getConfig();        }        dttr=new DelegationTokenToRenew(Arrays.asList(applicationId),token,tokenConf,now,shouldCancelAtEnd,evt.getUser());        try {          renewToken(dttr);
@VisibleForTesting protected void setTimerForTokenRenewal(DelegationTokenToRenew token) throws IOException {  long expiresIn=token.expirationDate - System.currentTimeMillis();  if (expiresIn <= 0) {
synchronized (dttr.referringAppIds) {      applicationIds=new HashSet<>(dttr.referringAppIds);      dttr.referringAppIds.clear();    }    for (    ApplicationId appId : applicationIds) {      Set<DelegationTokenToRenew> tokenSet=appTokens.get(appId);      if (tokenSet == null || tokenSet.isEmpty()) {        continue;      }      Iterator<DelegationTokenToRenew> iter=tokenSet.iterator();synchronized (tokenSet) {        while (iter.hasNext()) {          DelegationTokenToRenew t=iter.next();          if (t.token.getKind().equals(HDFS_DELEGATION_KIND)) {            iter.remove();            allTokens.remove(t.token);            t.cancelTimer();
private void removeFailedDelegationToken(DelegationTokenToRenew t){  Collection<ApplicationId> applicationIds=t.referringAppIds;synchronized (applicationIds) {
private void removeApplicationFromRenewal(ApplicationId applicationId){  rmContext.getSystemCredentialsForApps().remove(applicationId);  Set<DelegationTokenToRenew> tokens=appTokens.remove(applicationId);  if (tokens != null && !tokens.isEmpty()) {synchronized (tokens) {      Iterator<DelegationTokenToRenew> it=tokens.iterator();      while (it.hasNext()) {        DelegationTokenToRenew dttr=it.next();        if (LOG.isDebugEnabled()) {
private TimerTask getTimerTask(AbstractDelegationTokenRenewerAppEvent evt){  return new TimerTask(){    @Override public void run(){
public void clearNodeSetForAttempt(ApplicationAttemptId attemptId){  super.writeLock.lock();  try {    HashSet<NodeId> nodeSet=this.appAttemptToNodeKeyMap.get(attemptId);    if (nodeSet != null) {
public NMToken createAndGetNMToken(String applicationSubmitter,ApplicationAttemptId appAttemptId,Container container){  this.writeLock.lock();  try {    HashSet<NodeId> nodeSet=this.appAttemptToNodeKeyMap.get(appAttemptId);    NMToken nmToken=null;    if (nodeSet != null) {      if (!nodeSet.contains(container.getNodeId())) {
@Override protected void storeNewMasterKey(DelegationKey newKey){  try {
@Override protected void removeStoredMasterKey(DelegationKey key){  try {
@Override protected void storeNewToken(RMDelegationTokenIdentifier identifier,long renewDate){  try {
@Override protected void updateStoredToken(RMDelegationTokenIdentifier id,long renewDate){  try {
@Override protected void removeStoredToken(RMDelegationTokenIdentifier ident) throws IOException {  try {
private void initCsiAdaptorCache(final Map<String,CsiAdaptorProtocol> adaptorMap,Configuration conf) throws IOException, YarnException {  LOG.info("Initializing cache for csi-driver-adaptors");  String[] addresses=conf.getStrings(YarnConfiguration.NM_CSI_ADAPTOR_ADDRESSES);  if (addresses != null && addresses.length > 0) {    for (    String addr : addresses) {
private void initCsiAdaptorCache(final Map<String,CsiAdaptorProtocol> adaptorMap,Configuration conf) throws IOException, YarnException {  LOG.info("Initializing cache for csi-driver-adaptors");  String[] addresses=conf.getStrings(YarnConfiguration.NM_CSI_ADAPTOR_ADDRESSES);  if (addresses != null && addresses.length > 0) {    for (    String addr : addresses) {      LOG.info("Found csi-driver-adaptor socket address: " + addr);      InetSocketAddress address=NetUtils.createSocketAddr(addr);      YarnRPC rpc=YarnRPC.create(conf);      UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();      CsiAdaptorProtocol adaptorClient=NMProxy.createNMProxy(conf,CsiAdaptorProtocol.class,currentUser,rpc,address);
  String[] addresses=conf.getStrings(YarnConfiguration.NM_CSI_ADAPTOR_ADDRESSES);  if (addresses != null && addresses.length > 0) {    for (    String addr : addresses) {      LOG.info("Found csi-driver-adaptor socket address: " + addr);      InetSocketAddress address=NetUtils.createSocketAddr(addr);      YarnRPC rpc=YarnRPC.create(conf);      UserGroupInformation currentUser=UserGroupInformation.getCurrentUser();      CsiAdaptorProtocol adaptorClient=NMProxy.createNMProxy(conf,CsiAdaptorProtocol.class,currentUser,rpc,address);      LOG.info("Retrieving info from csi-driver-adaptor on address " + addr);      GetPluginInfoResponse response=adaptorClient.getPluginInfo(GetPluginInfoRequest.newInstance());      if (!Strings.isNullOrEmpty(response.getDriverName())) {        String driverName=response.getDriverName();        if (adaptorMap.containsKey(driverName)) {          throw new YarnException("Duplicate driver adaptor found," + " driver name: " + driverName);        }        adaptorMap.put(driverName,adaptorClient);
@Override public ScheduledFuture<VolumeProvisioningResults> schedule(VolumeProvisioningTask volumeProvisioningTask,int delaySecond){
  this.writeLock.lock();  try {    VolumeId volumeId=event.getVolumeId();    if (volumeId == null) {      LOG.warn("Unexpected volume event received, event type is " + event.getType().name() + ", but the volumeId is null.");      return;    }    LOG.info("Processing volume event, type=" + event.getType().name() + ", volumeId="+ volumeId.toString());    VolumeState oldState=null;    VolumeState newState=null;    try {      oldState=stateMachine.getCurrentState();      newState=stateMachine.doTransition(event.getType(),event);    } catch (    InvalidStateTransitionException e) {      LOG.warn("Can't handle this event at current state: Current: [" + oldState + "], eventType: ["+ event.getType()+ "],"+ " volumeId: ["+ volumeId+ "]",e);    }    if (newState != null && oldState != newState) {
      ApplicationId appId=null;      ApplicationAttemptId appAttemptId=null;      ContainerId containerId=null;switch (type) {case "app":        try {          appId=Apps.toAppID(parts[3]);        } catch (        YarnRuntimeException|NumberFormatException e) {          LOG.debug("Error parsing {} as an ApplicationId",parts[3],e);          return redirectPath;        }      if (!context.getRMApps().containsKey(appId)) {        redirectPath=pjoin(ahsPageURLPrefix,"app",appId);      }    break;case "appattempt":  try {    appAttemptId=ApplicationAttemptId.fromString(parts[3]);  } catch (  IllegalArgumentException e) {
          LOG.debug("Error parsing {} as an ApplicationId",parts[3],e);          return redirectPath;        }      if (!context.getRMApps().containsKey(appId)) {        redirectPath=pjoin(ahsPageURLPrefix,"app",appId);      }    break;case "appattempt":  try {    appAttemptId=ApplicationAttemptId.fromString(parts[3]);  } catch (  IllegalArgumentException e) {    LOG.debug("Error parsing {} as an ApplicationAttemptId",parts[3],e);    return redirectPath;  }if (!context.getRMApps().containsKey(appAttemptId.getApplicationId())) {  redirectPath=pjoin(ahsPageURLPrefix,"appattempt",appAttemptId);}break;case "container":try {containerId=ContainerId.fromString(parts[3]);
@POST @Path(RMWSConsts.NODE_RESOURCE) @Consumes({MediaType.APPLICATION_JSON,MediaType.APPLICATION_XML}) @Produces({MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8,MediaType.APPLICATION_XML + "; " + JettyUtils.UTF_8}) public ResourceInfo updateNodeResource(@Context HttpServletRequest hsr,@PathParam(RMWSConsts.NODEID) String nodeId,ResourceOptionInfo resourceOption) throws AuthorizationException {  UserGroupInformation callerUGI=getCallerUserGroupInformation(hsr,true);  initForWritableEndpoints(callerUGI,false);  RMNode rmNode=getRMNode(nodeId);  Map<NodeId,ResourceOption> nodeResourceMap=Collections.singletonMap(rmNode.getNodeID(),resourceOption.getResourceOption());  UpdateNodeResourceRequest updateRequest=UpdateNodeResourceRequest.newInstance(nodeResourceMap);  try {    RMContext rmContext=this.rm.getRMContext();    AdminService admin=rmContext.getRMAdminService();    admin.updateNodeResource(updateRequest);  } catch (  YarnException e) {    String message="Failed to update the node resource " + rmNode.getNodeID() + ".";    LOG.error(message,e);    throw new YarnRuntimeException(message,e);  }catch (  IOException e) {
  int limitNum=-1;  if (limit != null) {    try {      limitNum=Integer.parseInt(limit);      if (limitNum <= 0) {        return new AppActivitiesInfo("limit must be greater than 0!",appId);      }    } catch (    NumberFormatException e) {      return new AppActivitiesInfo("limit must be integer!",appId);    }  }  double maxTime=3.0;  if (time != null) {    if (time.contains(".")) {      maxTime=Double.parseDouble(time);    } else {      maxTime=Double.parseDouble(time + ".0");    }  }  ApplicationId applicationId;
    resp=callerUGI.doAs(new PrivilegedExceptionAction<RenewDelegationTokenResponse>(){      @Override public RenewDelegationTokenResponse run() throws YarnException {        return rm.getClientRMService().renewDelegationToken(req);      }    });  } catch (  UndeclaredThrowableException ue) {    if (ue.getCause() instanceof YarnException) {      if (ue.getCause().getCause() instanceof InvalidToken) {        throw new BadRequestException(ue.getCause().getCause().getMessage());      } else       if (ue.getCause().getCause() instanceof org.apache.hadoop.security.AccessControlException) {        return Response.status(Status.FORBIDDEN).entity(ue.getCause().getCause().getMessage()).build();      }      LOG.info("Renew delegation token request failed",ue);      throw ue;    }    LOG.info("Renew delegation token request failed",ue);    throw ue;  }catch (  Exception e) {
    return Response.status(Status.FORBIDDEN).entity(ye.getMessage()).build();  }  Token<RMDelegationTokenIdentifier> token=extractToken(hsr);  org.apache.hadoop.yarn.api.records.Token dToken=BuilderUtils.newDelegationToken(token.getIdentifier(),token.getKind().toString(),token.getPassword(),token.getService().toString());  final CancelDelegationTokenRequest req=CancelDelegationTokenRequest.newInstance(dToken);  try {    callerUGI.doAs(new PrivilegedExceptionAction<CancelDelegationTokenResponse>(){      @Override public CancelDelegationTokenResponse run() throws IOException, YarnException {        return rm.getClientRMService().cancelDelegationToken(req);      }    });  } catch (  UndeclaredThrowableException ue) {    if (ue.getCause() instanceof YarnException) {      if (ue.getCause().getCause() instanceof InvalidToken) {        throw new BadRequestException(ue.getCause().getCause().getMessage());      } else       if (ue.getCause().getCause() instanceof org.apache.hadoop.security.AccessControlException) {        return Response.status(Status.FORBIDDEN).entity(ue.getCause().getCause().getMessage()).build();
  Token<RMDelegationTokenIdentifier> token=extractToken(hsr);  org.apache.hadoop.yarn.api.records.Token dToken=BuilderUtils.newDelegationToken(token.getIdentifier(),token.getKind().toString(),token.getPassword(),token.getService().toString());  final CancelDelegationTokenRequest req=CancelDelegationTokenRequest.newInstance(dToken);  try {    callerUGI.doAs(new PrivilegedExceptionAction<CancelDelegationTokenResponse>(){      @Override public CancelDelegationTokenResponse run() throws IOException, YarnException {        return rm.getClientRMService().cancelDelegationToken(req);      }    });  } catch (  UndeclaredThrowableException ue) {    if (ue.getCause() instanceof YarnException) {      if (ue.getCause().getCause() instanceof InvalidToken) {        throw new BadRequestException(ue.getCause().getCause().getMessage());      } else       if (ue.getCause().getCause() instanceof org.apache.hadoop.security.AccessControlException) {        return Response.status(Status.FORBIDDEN).entity(ue.getCause().getCause().getMessage()).build();      }      LOG.info("Renew delegation token request failed",ue);
  UserGroupInformation callerUGI=getCallerUserGroupInformation(hsr,true);  initForWritableEndpoints(callerUGI,true);  ResourceScheduler scheduler=rm.getResourceScheduler();  if (scheduler instanceof MutableConfScheduler && ((MutableConfScheduler)scheduler).isConfigurationMutable()) {    try {      MutableConfigurationProvider mutableConfigurationProvider=((MutableConfScheduler)scheduler).getMutableConfProvider();      mutableConfigurationProvider.formatConfigurationInStore(conf);      try {        rm.getRMContext().getRMAdminService().refreshQueues();      } catch (      IOException|YarnException e) {        LOG.error("Exception thrown when formatting configuration.",e);        mutableConfigurationProvider.revertToOldConfig(conf);        throw e;      }      return Response.status(Status.OK).entity("Configuration under " + "store successfully formatted.").build();    } catch (    Exception e) {
      callerUGI.doAs(new PrivilegedExceptionAction<Void>(){        @Override public Void run() throws Exception {          MutableConfigurationProvider provider=((MutableConfScheduler)scheduler).getMutableConfProvider();          if (!provider.getAclMutationPolicy().isMutationAllowed(callerUGI,mutationInfo)) {            throw new org.apache.hadoop.security.AccessControlException("User" + " is not admin of all modified queues.");          }          LogMutation logMutation=provider.logAndApplyMutation(callerUGI,mutationInfo);          try {            rm.getRMContext().getRMAdminService().refreshQueues();          } catch (          IOException|YarnException e) {            provider.confirmPendingMutation(logMutation,false);            throw e;          }          provider.confirmPendingMutation(logMutation,true);          return null;        }      });    } catch (    IOException e) {
public synchronized void addTask(Task task){  SchedulerRequestKey schedulerKey=task.getSchedulerKey();  Map<String,ResourceRequest> requests=this.requests.get(schedulerKey);  if (requests == null) {    requests=new HashMap<String,ResourceRequest>();    this.requests.put(schedulerKey,requests);
public synchronized List<Container> getResources() throws IOException {  if (LOG.isDebugEnabled()) {    LOG.debug("getResources begin: application={} #ask={}",applicationId,ask.size());    for (    ResourceRequest request : ask) {
private synchronized void assign(SchedulerRequestKey schedulerKey,NodeType type,List<Container> containers) throws IOException, YarnException {  for (Iterator<Container> i=containers.iterator(); i.hasNext(); ) {    Container container=i.next();    String host=container.getNodeId().toString();    if (Resources.equals(requestSpec.get(schedulerKey),container.getResource())) {      for (Iterator<Task> t=tasks.get(schedulerKey).iterator(); t.hasNext(); ) {        Task task=t.next();        if (task.getState() == State.PENDING && task.canSchedule(type,host)) {          NodeManager nodeManager=getNodeManager(host);          task.start(nodeManager,container.getId());          i.remove();          Resources.addTo(used,container.getResource());
private void updateResourceRequests(Map<String,ResourceRequest> requests,NodeType type,Task task){  if (type == NodeType.NODE_LOCAL) {    for (    String host : task.getHosts()) {      if (LOG.isDebugEnabled()) {
private void updateResourceRequest(ResourceRequest request){  request.setNumContainers(request.getNumContainers() - 1);  ask.remove(request);  ask.add(ResourceRequest.clone(request));
  driver.put(testCapacityDRConf,EnumSet.of(YarnServiceProtos.SchedulerResourceTypes.CPU,YarnServiceProtos.SchedulerResourceTypes.MEMORY));  driver.put(testCapacityDefConf,EnumSet.of(YarnServiceProtos.SchedulerResourceTypes.MEMORY));  driver.put(testFairDefConf,EnumSet.of(YarnServiceProtos.SchedulerResourceTypes.MEMORY,YarnServiceProtos.SchedulerResourceTypes.CPU));  for (  Map.Entry<YarnConfiguration,EnumSet<YarnServiceProtos.SchedulerResourceTypes>> entry : driver.entrySet()) {    EnumSet<YarnServiceProtos.SchedulerResourceTypes> expectedValue=entry.getValue();    MockRM rm=new MockRM(entry.getKey());    rm.start();    MockNM nm1=rm.registerNode(DEFAULT_HOST + ":" + DEFAULT_PORT,6 * GB);    RMApp app1=MockRMAppSubmitter.submitWithMemory(2048,rm);    Thread.sleep(1000);    nm1.nodeHeartbeat(true);    RMAppAttempt attempt1=app1.getCurrentAppAttempt();    MockAM am1=rm.sendAMLaunched(attempt1.getAppAttemptId());    RegisterApplicationMasterResponse resp=am1.registerAppAttempt();    EnumSet<YarnServiceProtos.SchedulerResourceTypes> types=resp.getSchedulerResourceTypes();
public void waitForContainerToComplete(RMAppAttempt attempt,NMContainerStatus completedContainer) throws InterruptedException {  drainEventsImplicitly();  int timeWaiting=0;  while (timeWaiting < TIMEOUT_MS_FOR_CONTAINER_AND_NODE) {    List<ContainerStatus> containers=attempt.getJustFinishedContainers();
      nm.nodeHeartbeat(true);    }    drainEventsImplicitly();    container=getResourceScheduler().getRMContainer(containerId);    LOG.info("Waiting for container " + containerId + " to be "+ containerState+ ", container is null right now.");    Thread.sleep(WAIT_MS_PER_LOOP);    timeWaiting+=WAIT_MS_PER_LOOP;  }  while (!containerState.equals(container.getState())) {    if (timeWaiting >= timeoutMsecs) {      return false;    }    LOG.info("Container : " + containerId + " State is : "+ container.getState()+ " Waiting for state : "+ containerState);    for (    MockNM nm : nms) {      nm.nodeHeartbeat(true);    }    drainEventsImplicitly();    Thread.sleep(WAIT_MS_PER_LOOP);    timeWaiting+=WAIT_MS_PER_LOOP;
@SuppressWarnings("rawtypes") private static void waitForSchedulerAppAttemptAdded(ApplicationAttemptId attemptId,MockRM rm) throws InterruptedException {  int tick=0;  rm.drainEventsImplicitly();  while (null == ((AbstractYarnScheduler)rm.getResourceScheduler()).getApplicationAttempt(attemptId) && tick < 50) {    Thread.sleep(100);    if (tick % 10 == 0) {
public static MockAM launchAM(RMApp app,MockRM rm,MockNM nm) throws Exception {  rm.drainEventsImplicitly();  RMAppAttempt attempt=waitForAttemptScheduled(app,rm);
public static MockAM launchUAM(RMApp app,MockRM rm,MockNM nm) throws Exception {  rm.drainEventsImplicitly();  rm.waitForState(app.getApplicationId(),RMAppState.ACCEPTED);  RMAppAttempt attempt=app.getCurrentAppAttempt();  waitForSchedulerAppAttemptAdded(attempt.getAppAttemptId(),rm);
      throw RPCUtil.getRemoteException(e);    }    ContainerId containerID=tokenId.getContainerID();    ApplicationId applicationId=containerID.getApplicationAttemptId().getApplicationId();    List<Container> applicationContainers=containers.get(applicationId);    if (applicationContainers == null) {      applicationContainers=new ArrayList<Container>();      containers.put(applicationId,applicationContainers);    }    for (    Container container : applicationContainers) {      if (container.getId().compareTo(containerID) == 0) {        throw new IllegalStateException("Container " + containerID + " already setup on node "+ containerManagerAddress);      }    }    Container container=BuilderUtils.newContainer(containerID,this.nodeId,nodeHttpAddress,tokenId.getResource(),null,null);    ContainerStatus containerStatus=BuilderUtils.newContainerStatus(container.getId(),ContainerState.NEW,"",-1000,container.getResource());    applicationContainers.add(container);    containerStatusMap.put(container,containerStatus);    Resources.subtractFrom(available,tokenId.getResource());
      }    }    try {      heartbeat();    } catch (    IOException ioe) {      throw RPCUtil.getRemoteException(ioe);    }    int ctr=0;    Container container=null;    for (Iterator<Container> i=applicationContainers.iterator(); i.hasNext(); ) {      container=i.next();      if (container.getId().compareTo(containerID) == 0) {        i.remove();        ++ctr;      }    }    if (ctr != 1) {      throw new IllegalStateException("Container " + containerID + " stopped "+ ctr+ " times!");    }    Resources.addTo(available,container.getResource());    Resources.subtractFrom(used,container.getResource());
private void handleAdministerException(Exception e,String user,String queue,String operation){
private void waitForLaunchedState(RMAppAttempt attempt) throws InterruptedException {  int waitCount=0;  while (attempt.getAppAttemptState() != RMAppAttemptState.LAUNCHED && waitCount++ < 40) {
  Container mockContainer=mock(Container.class);  NodeId mockNodeId=mock(NodeId.class);  String host="127.0.0.1";  when(mockNodeId.getHost()).thenReturn(host);  when(mockContainer.getNodeId()).thenReturn(mockNodeId);  when(mockRMAppAttempt.getMasterContainer()).thenReturn(mockContainer);  when(app.getCurrentAppAttempt()).thenReturn(mockRMAppAttempt);  Map<String,Long> resourceSecondsMap=new HashMap<>();  resourceSecondsMap.put(ResourceInformation.MEMORY_MB.getName(),16384L);  resourceSecondsMap.put(ResourceInformation.VCORES.getName(),64L);  RMAppMetrics metrics=new RMAppMetrics(Resource.newInstance(1234,56),10,1,resourceSecondsMap,new HashMap<>(),1234);  when(app.getRMAppMetrics()).thenReturn(metrics);  when(app.getDiagnostics()).thenReturn(new StringBuilder("Multiline\n\n\r\rDiagnostics=Diagn,ostic"));  RMAppManager.ApplicationSummary.SummaryBuilder summary=new RMAppManager.ApplicationSummary().createAppSummary(app);  String msg=summary.toString();
  final GetApplicationReportRequest appReportRequest=recordFactory.newRecordInstance(GetApplicationReportRequest.class);  appReportRequest.setApplicationId(applicationId);  final KillApplicationRequest finishAppRequest=recordFactory.newRecordInstance(KillApplicationRequest.class);  finishAppRequest.setApplicationId(applicationId);  ApplicationClientProtocol enemyRmClient=getRMClientForUser(ENEMY);  ApplicationReport appReport=enemyRmClient.getApplicationReport(appReportRequest).getApplicationReport();  verifyEnemyAppReport(appReport);  List<ApplicationReport> appReports=enemyRmClient.getApplications(recordFactory.newRecordInstance(GetApplicationsRequest.class)).getApplicationList();  Assert.assertEquals("App view by enemy should list the apps!!",4,appReports.size());  for (  ApplicationReport report : appReports) {    verifyEnemyAppReport(report);  }  try {    enemyRmClient.forceKillApplication(finishAppRequest);    Assert.fail("App killing by the enemy should fail!!");  } catch (  YarnException e) {
  MockRM rm=new MockRM();  rm.start();  MockNM nm1=rm.registerNode("127.0.0.1:1234",5000);  RMApp app=MockRMAppSubmitter.submitWithMemory(2000,rm);  nm1.nodeHeartbeat(true);  RMAppAttempt attempt=app.getCurrentAppAttempt();  MockAM am=rm.sendAMLaunched(attempt.getAppAttemptId());  am.registerAppAttempt();  int request=2;  am.allocate("127.0.0.1",1000,request,new ArrayList<ContainerId>());  nm1.nodeHeartbeat(true);  List<Container> conts=am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers();  int contReceived=conts.size();  int waitCount=0;  while (contReceived < request && waitCount++ < 200) {
    Thread.sleep(100);    conts=am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers();    contReceived+=conts.size();    nm1.nodeHeartbeat(true);  }  Assert.assertEquals(request,contReceived);  am.unregisterAppAttempt();  NodeHeartbeatResponse resp=nm1.nodeHeartbeat(attempt.getAppAttemptId(),1,ContainerState.COMPLETE);  rm.waitForState(am.getApplicationAttemptId(),RMAppAttemptState.FINISHED);  resp=nm1.nodeHeartbeat(true);  List<ContainerId> containersToCleanup=resp.getContainersToCleanup();  List<ApplicationId> appsToCleanup=resp.getApplicationsToCleanup();  int numCleanedContainers=containersToCleanup.size();  int numCleanedApps=appsToCleanup.size();  waitCount=0;  while ((numCleanedContainers < 2 || numCleanedApps < 1) && waitCount++ < 200) {
  rm.start();  MockNM nm1=rm.registerNode("127.0.0.1:1234",5000);  RMApp app=MockRMAppSubmitter.submitWithMemory(2000,rm);  nm1.nodeHeartbeat(true);  RMAppAttempt attempt=app.getCurrentAppAttempt();  MockAM am=rm.sendAMLaunched(attempt.getAppAttemptId());  am.registerAppAttempt();  int request=2;  am.allocate("127.0.0.1",1000,request,new ArrayList<ContainerId>());  rm.drainEvents();  nm1.nodeHeartbeat(true);  List<Container> conts=am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers();  int contReceived=conts.size();  int waitCount=0;  while (contReceived < request && waitCount++ < 200) {
protected void waitForContainerCleanup(MockRM rm,MockNM nm,NodeHeartbeatResponse resp) throws Exception {  int waitCount=0, cleanedConts=0;  List<ContainerId> contsToClean;  do {    rm.drainEvents();    contsToClean=resp.getContainersToCleanup();    cleanedConts+=contsToClean.size();    if (cleanedConts >= 1) {      break;    }    Thread.sleep(100);    resp=nm.nodeHeartbeat(true);  } while (waitCount++ < 200);  if (contsToClean.isEmpty()) {    LOG.error("Failed to get any containers to cleanup");  } else {
    protected ClientRMService createClientRMService(){      return new ClientRMService(this.rmContext,scheduler,this.rmAppManager,this.applicationACLsManager,this.queueACLsManager,this.getRMContext().getRMDelegationTokenSecretManager());    }  };  rm.start();  int nodeMemory=1024;  MockNM nm1=rm.registerNode("host1:1234",nodeMemory);  rm.sendNodeStarted(nm1);  nm1.nodeHeartbeat(true);  rm.waitForState(nm1.getNodeId(),NodeState.RUNNING);  Integer decommissioningTimeout=600;  rm.sendNodeGracefulDecommission(nm1,decommissioningTimeout);  rm.waitForState(nm1.getNodeId(),NodeState.DECOMMISSIONING);  Configuration conf=new Configuration();  YarnRPC rpc=YarnRPC.create(conf);  InetSocketAddress rmAddress=rm.getClientRMService().getBindAddress();
    }  };  rm.start();  NodeLabel labelX=NodeLabel.newInstance("x",false);  NodeLabel labelY=NodeLabel.newInstance("y");  RMNodeLabelsManager labelsMgr=rm.getRMContext().getNodeLabelManager();  labelsMgr.addToCluserNodeLabels(ImmutableSet.of(labelX,labelY));  NodeId node1=NodeId.newInstance("host1",1234);  NodeId node2=NodeId.newInstance("host2",1234);  Map<NodeId,Set<String>> map=new HashMap<NodeId,Set<String>>();  map.put(node1,ImmutableSet.of("x"));  map.put(node2,ImmutableSet.of("y"));  labelsMgr.replaceLabelsOnNode(map);  Configuration conf=new Configuration();  YarnRPC rpc=YarnRPC.create(conf);  InetSocketAddress rmAddress=rm.getClientRMService().getBindAddress();
    }  };  rm.start();  NodeAttributesManager mgr=rm.getRMContext().getNodeAttributesManager();  NodeId host1=NodeId.newInstance("host1",0);  NodeId host2=NodeId.newInstance("host2",0);  NodeAttribute gpu=NodeAttribute.newInstance(NodeAttribute.PREFIX_CENTRALIZED,"GPU",NodeAttributeType.STRING,"nvida");  NodeAttribute os=NodeAttribute.newInstance(NodeAttribute.PREFIX_CENTRALIZED,"OS",NodeAttributeType.STRING,"windows64");  NodeAttribute docker=NodeAttribute.newInstance(NodeAttribute.PREFIX_DISTRIBUTED,"DOCKER",NodeAttributeType.STRING,"docker0");  Map<String,Set<NodeAttribute>> nodes=new HashMap<>();  nodes.put(host1.getHost(),ImmutableSet.of(gpu,os));  nodes.put(host2.getHost(),ImmutableSet.of(docker));  mgr.addNodeAttributes(nodes);  Configuration conf=new Configuration();  YarnRPC rpc=YarnRPC.create(conf);  InetSocketAddress rmAddress=rm.getClientRMService().getBindAddress();
  yarnConf.setClass(CapacitySchedulerConfiguration.RESOURCE_CALCULATOR_CLASS,DominantResourceCalculator.class,ResourceCalculator.class);  MockRM rm=new MockRM(yarnConf){    protected ClientRMService createClientRMService(){      return new ClientRMService(this.rmContext,scheduler,this.rmAppManager,this.applicationACLsManager,this.queueACLsManager,this.getRMContext().getRMDelegationTokenSecretManager());    }  };  rm.start();  Resource resource=BuilderUtils.newResource(1024,1);  resource.setResourceInformation("memory-mb",ResourceInformation.newInstance("memory-mb","G",1024));  resource.setResourceInformation("resource1",ResourceInformation.newInstance("resource1","T",1));  resource.setResourceInformation("resource2",ResourceInformation.newInstance("resource2","M",1));  MockNM node=rm.registerNode("host1:1234",resource);  node.nodeHeartbeat(true);  Configuration conf=new Configuration();  YarnRPC rpc=YarnRPC.create(conf);  InetSocketAddress rmAddress=rm.getClientRMService().getBindAddress();
@Test public void testDelegationToken() throws IOException, InterruptedException {  final YarnConfiguration conf=new YarnConfiguration();  conf.set(YarnConfiguration.RM_PRINCIPAL,"testuser/localhost@apache.org");  conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,"kerberos");  UserGroupInformation.setConfiguration(conf);  ResourceScheduler scheduler=createMockScheduler(conf);  long initialInterval=10000l;  long maxLifetime=20000l;  long renewInterval=10000l;  RMDelegationTokenSecretManager rmDtSecretManager=createRMDelegationTokenSecretManager(initialInterval,maxLifetime,renewInterval);  rmDtSecretManager.startThreads();
  long maxLifetime=20000l;  long renewInterval=10000l;  RMDelegationTokenSecretManager rmDtSecretManager=createRMDelegationTokenSecretManager(initialInterval,maxLifetime,renewInterval);  rmDtSecretManager.startThreads();  LOG.info("Creating DelegationTokenSecretManager with initialInterval: " + initialInterval + ", maxLifetime: "+ maxLifetime+ ", renewInterval: "+ renewInterval);  final ClientRMService clientRMService=new ClientRMServiceForTest(conf,scheduler,rmDtSecretManager);  clientRMService.init(conf);  clientRMService.start();  ApplicationClientProtocol clientRMWithDT=null;  try {    UserGroupInformation loggedInUser=UserGroupInformation.createRemoteUser("testrenewer@APACHE.ORG");    Assert.assertEquals("testrenewer",loggedInUser.getShortUserName());    loggedInUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);    org.apache.hadoop.yarn.api.records.Token token=getDelegationToken(loggedInUser,clientRMService,loggedInUser.getShortUserName());    long tokenFetchTime=System.currentTimeMillis();
    org.apache.hadoop.yarn.api.records.Token token=getDelegationToken(loggedInUser,clientRMService,loggedInUser.getShortUserName());    long tokenFetchTime=System.currentTimeMillis();    LOG.info("Got delegation token at: " + tokenFetchTime);    clientRMWithDT=getClientRMProtocolWithDT(token,clientRMService.getBindAddress(),"loginuser1",conf);    GetNewApplicationRequest request=Records.newRecord(GetNewApplicationRequest.class);    try {      clientRMWithDT.getNewApplication(request);    } catch (    IOException e) {      fail("Unexpected exception" + e);    }catch (    YarnException e) {      fail("Unexpected exception" + e);    }    while (System.currentTimeMillis() < tokenFetchTime + initialInterval / 2) {      Thread.sleep(500l);    }    long nextExpTime=renewDelegationToken(loggedInUser,clientRMService,token);    long renewalTime=System.currentTimeMillis();
    }    while (System.currentTimeMillis() < tokenFetchTime + initialInterval / 2) {      Thread.sleep(500l);    }    long nextExpTime=renewDelegationToken(loggedInUser,clientRMService,token);    long renewalTime=System.currentTimeMillis();    LOG.info("Renewed token at: " + renewalTime + ", NextExpiryTime: "+ nextExpTime);    while (System.currentTimeMillis() > tokenFetchTime + initialInterval && System.currentTimeMillis() < nextExpTime) {      Thread.sleep(500l);    }    Thread.sleep(50l);    try {      clientRMWithDT.getNewApplication(request);    } catch (    IOException e) {      fail("Unexpected exception" + e);    }catch (    YarnException e) {      fail("Unexpected exception" + e);    }    while (System.currentTimeMillis() < renewalTime + renewInterval) {
      clientRMWithDT.getNewApplication(request);      fail("Should not have succeeded with an expired token");    } catch (    Exception e) {      assertEquals(InvalidToken.class.getName(),e.getClass().getName());      assertTrue(e.getMessage().contains("is expired"));    }    if (clientRMWithDT != null) {      RPC.stopProxy(clientRMWithDT);      clientRMWithDT=null;    }    token=getDelegationToken(loggedInUser,clientRMService,loggedInUser.getShortUserName());    tokenFetchTime=System.currentTimeMillis();    LOG.info("Got delegation token at: " + tokenFetchTime);    clientRMWithDT=getClientRMProtocolWithDT(token,clientRMService.getBindAddress(),"loginuser2",conf);    request=Records.newRecord(GetNewApplicationRequest.class);    try {      clientRMWithDT.getNewApplication(request);
  MockNM nm1=rm.registerNode("h1:1234",5120);  MockNM nm2=rm.registerNode("h2:5678",10240);  RMApp app=MockRMAppSubmitter.submitWithMemory(2000,rm);  nm1.nodeHeartbeat(true);  RMAppAttempt attempt=app.getCurrentAppAttempt();  MockAM am=rm.sendAMLaunched(attempt.getAppAttemptId());  am.registerAppAttempt();  int request=13;  am.allocate("h1",1000,request,new ArrayList<ContainerId>());  List<Container> conts=am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers();  int contReceived=conts.size();  while (contReceived < 3) {    nm1.nodeHeartbeat(true);    conts.addAll(am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers());    contReceived=conts.size();
  List<Container> conts=am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers();  int contReceived=conts.size();  while (contReceived < 3) {    nm1.nodeHeartbeat(true);    conts.addAll(am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers());    contReceived=conts.size();    LOG.info("Got " + contReceived + " containers. Waiting to get "+ 3);    Thread.sleep(WAIT_SLEEP_MS);  }  Assert.assertEquals(3,conts.size());  conts=am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers();  contReceived=conts.size();  while (contReceived < 10) {    nm2.nodeHeartbeat(true);    conts.addAll(am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers());    contReceived=conts.size();
    allocateContainersAndValidateNMTokens(am,containersReceivedForNM1,4,nmTokens,nm1);    Assert.assertEquals(1,nmTokens.size());    MockNM nm2=rm.registerNode("h2:1234",10000);    nm2.nodeHeartbeat(true);    ArrayList<Container> containersReceivedForNM2=new ArrayList<Container>();    response=am.allocate("h2",1000,2,releaseContainerList);    Assert.assertEquals(0,response.getAllocatedContainers().size());    allocateContainersAndValidateNMTokens(am,containersReceivedForNM2,2,nmTokens,nm2);    Assert.assertEquals(2,nmTokens.size());    nm2=rm.registerNode("h2:1234",10000);    Map<NodeId,RMNode> nodes=rm.getRMContext().getRMNodes();    while (nodes.get(nm2.getNodeId()).getLastNodeHeartBeatResponse().getResponseId() > 0) {      Thread.sleep(WAIT_SLEEP_MS);    }    int interval=40;    while (nmTokenSecretManager.isApplicationAttemptNMTokenPresent(attempt.getAppAttemptId(),nm2.getNodeId()) && interval-- > 0) {
@Test(timeout=60000) public void testRMRestartWaitForPreviousSucceededAttempt() throws Exception {  conf.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS,2);  MemoryRMStateStore memStore=new MockMemoryRMStateStore(){    int count=0;    @Override public void updateApplicationStateInternal(    ApplicationId appId,    ApplicationStateData appStateData) throws Exception {      if (count == 1) {
@Test public void testFailoverAndSubmitReservation() throws Exception {  startRMs();  addNodeCapacityToPlan(rm1,102400,100);  explicitFailover();  addNodeCapacityToPlan(rm2,102400,100);  ClientRMService clientService=rm2.getClientRMService();  ReservationId reservationID=getNewReservation(clientService).getReservationId();  ReservationSubmissionRequest request=createReservationSubmissionRequest(reservationID);  ReservationSubmissionResponse response=null;  try {    response=clientService.submitReservation(request);  } catch (  Exception e) {    Assert.fail(e.getMessage());  }  Assert.assertNotNull(response);  Assert.assertNotNull(reservationID);
  try {    response=clientService.submitReservation(request);  } catch (  Exception e) {    Assert.fail(e.getMessage());  }  Assert.assertNotNull(response);  Assert.assertNotNull(resID1);  LOG.info("Submit reservation response: " + resID1);  ReservationId resID2=getNewReservation(clientService).getReservationId();  request.setReservationId(resID2);  try {    response=clientService.submitReservation(request);  } catch (  Exception e) {    Assert.fail(e.getMessage());  }  Assert.assertNotNull(response);  Assert.assertNotNull(resID2);
  ReservationId resID2=getNewReservation(clientService).getReservationId();  request.setReservationId(resID2);  try {    response=clientService.submitReservation(request);  } catch (  Exception e) {    Assert.fail(e.getMessage());  }  Assert.assertNotNull(response);  Assert.assertNotNull(resID2);  LOG.info("Submit reservation response: " + resID2);  ReservationId resID3=getNewReservation(clientService).getReservationId();  request.setReservationId(resID3);  try {    response=clientService.submitReservation(request);  } catch (  Exception e) {    Assert.fail(e.getMessage());
  if (rm.getResourceScheduler().getClass() == FairScheduler.class) {    fs=(FairScheduler)rm.getResourceScheduler();  }  rm.start();  MockNM nm1=rm.registerNode("h1:1234",5000);  RMApp app=MockRMAppSubmitter.submitWithMemory(2000,rm);  nm1.nodeHeartbeat(true);  RMAppAttempt attempt=app.getCurrentAppAttempt();  MockAM am=rm.sendAMLaunched(attempt.getAppAttemptId());  am.registerAppAttempt();  final int request=2;  am.allocate("h1",1000,request,new ArrayList<ContainerId>());  nm1.nodeHeartbeat(true);  List<Container> conts=new ArrayList<>(request);  int waitCount=0;  while (conts.size() < request && waitCount++ < 200) {
  int waitCount=0;  while (conts.size() < request && waitCount++ < 200) {    LOG.info("Got " + conts.size() + " containers. Waiting to get "+ request);    Thread.sleep(100);    List<Container> allocation=am.allocate(new ArrayList<ResourceRequest>(),new ArrayList<ContainerId>()).getAllocatedContainers();    conts.addAll(allocation);    if (fs != null) {      nm1.nodeHeartbeat(true);    }  }  Assert.assertEquals(request,conts.size());  for (  Container container : conts) {    rm.signalToContainer(container.getId(),SignalContainerCommand.OUTPUT_THREAD_DUMP);  }  NodeHeartbeatResponse resp;  List<SignalContainerRequest> contsToSignal;  int signaledConts=0;  waitCount=0;
      doAnswer(new Answer<Integer>(){        @Override public Integer answer(        InvocationOnMock invocation) throws Throwable {          return cId.compareTo(((RMContainer)invocation.getArguments()[0]).getContainerId());        }      }).when(rmc).compareTo(any(RMContainer.class));      if (containerId == 1) {        when(rmc.isAMContainer()).thenReturn(true);        when(app.getAMResource(label)).thenReturn(res);        when(app.getAppAMNodePartitionName()).thenReturn(label);      }      if (reserved) {        reservedContainers.add(rmc);        when(rmc.getReservedResource()).thenReturn(res);      } else {        liveContainers.add(rmc);      }      addContainerToSchedulerNode(host,rmc,reserved);      String partition=null;
  HashMap<String,HashMap<String,HashMap<String,ResourceUsage>>> userResourceUsagePerLabel=new HashMap<>();  LeafQueue queue=null;  int mulp=-1;  for (  String a : appsConfig.split(";")) {    String[] strs=a.split("\t");    String queueName=strs[0];    if (mulp <= 0 && strs.length > 2 && strs[2] != null) {      mulp=100 / (new Integer(strs[2]).intValue());    }    List<RMContainer> liveContainers=new ArrayList<RMContainer>();    List<RMContainer> reservedContainers=new ArrayList<RMContainer>();    ApplicationId appId=ApplicationId.newInstance(0L,id);    ApplicationAttemptId appAttemptId=ApplicationAttemptId.newInstance(appId,1);    FiCaSchedulerApp app=mock(FiCaSchedulerApp.class);    when(app.getAMResource(anyString())).thenReturn(Resources.createResource(0,0));    mockContainers(strs[1],app,appAttemptId,queueName,reservedContainers,liveContainers);
      userResourceUsage.put(app.getUser(),usage);    }    usage.incAMUsed(app.getAMResource(label));    usage.incUsed(app.getAppAttemptResourceUsage().getUsed(label));    id++;  }  for (  String label : userResourceUsagePerLabel.keySet()) {    for (    String queueName : userMap.keySet()) {      queue=(LeafQueue)nameToCSQueues.get(queueName);      Resource totResoucePerPartition=partitionToResource.get("");      Resource capacity=Resources.multiply(totResoucePerPartition,queue.getQueueCapacities().getAbsoluteCapacity());      HashSet<String> users=userMap.get(queue.getQueueName());      if (users == null) {        users=userMap.get(queue.getQueuePath());      }      when(queue.getAllUsers()).thenReturn(users);      Resource userLimit;      if (mulp > 0) {
    FiCaSchedulerNode sn=mock(FiCaSchedulerNode.class);    when(sn.getNodeID()).thenReturn(nodeId);    when(sn.getPartition()).thenReturn(partition);    Resource totalRes=Resources.createResource(0);    if (arr.length > 1) {      String res=arr[1];      if (res.contains("res=")) {        String resSring=res.substring(res.indexOf("res=") + "res=".length());        totalRes=parseResourceFromString(resSring);      }    }    when(sn.getTotalResource()).thenReturn(totalRes);    when(sn.getUnallocatedResource()).thenReturn(Resources.clone(totalRes));    when(sn.getTotalKillableResources()).thenReturn(Resources.none());    List<RMContainer> liveContainers=new ArrayList<>();    when(sn.getCopiedListOfRunningContainers()).thenReturn(liveContainers);    nodeIdToSchedulerNodes.put(nodeId,sn);
private void mockNodeLabelsManager(String nodeLabelsConfigStr) throws IOException {  String[] partitionConfigArr=nodeLabelsConfigStr.split(";");  clusterResource=Resources.createResource(0);  for (  String p : partitionConfigArr) {    String partitionName=p.substring(0,p.indexOf("="));    Resource res=parseResourceFromString(p.substring(p.indexOf("=") + 1,p.indexOf(",")));    boolean exclusivity=Boolean.valueOf(p.substring(p.indexOf(",") + 1,p.length()));    when(nlm.getResourceByLabel(eq(partitionName),any(Resource.class))).thenReturn(res);    when(nlm.isExclusiveNodeLabel(eq(partitionName))).thenReturn(exclusivity);    partitionToResource.put(partitionName,res);
private void setupQueue(CSQueue queue,String q,String[] queueExprArray,int idx){
    queuePath=ROOT;  }  String queueName=getQueueName(q);  when(queue.getQueueName()).thenReturn(queueName);  ParentQueue parentQueue=getParentQueue(queueExprArray,idx,myLevel);  if (null != parentQueue) {    when(queue.getParent()).thenReturn(parentQueue);    parentQueue.getChildQueues().add(queue);    queuePath=parentQueue.getQueuePath() + "." + queueName;  }  when(queue.getQueuePath()).thenReturn(queuePath);  QueueCapacities qc=new QueueCapacities(0 == myLevel);  ResourceUsage ru=new ResourceUsage();  QueueResourceQuotas qr=new QueueResourceQuotas();  when(queue.getQueueCapacities()).thenReturn(qc);  when(queue.getQueueResourceUsage()).thenReturn(ru);  when(queue.getQueueResourceQuotas()).thenReturn(qr);
    qr.setEffectiveMaxResource(parseResourceFromString(values[1].trim()));    qr.setEffectiveMinResource(parseResourceFromString(values[0].trim()));    qr.setEffectiveMaxResource(partitionName,parseResourceFromString(values[1].trim()));    qr.setEffectiveMinResource(partitionName,parseResourceFromString(values[0].trim()));    when(queue.getUsedCapacity()).thenReturn(used);    when(queue.getEffectiveCapacity(partitionName)).thenReturn(parseResourceFromString(values[0].trim()));    when(queue.getEffectiveMaxCapacity(partitionName)).thenReturn(parseResourceFromString(values[1].trim()));    ru.setPending(partitionName,pending);    Resource reserved=Resources.none();    if (values.length == 5) {      reserved=parseResourceFromString(values[4].trim());      ru.setReserved(partitionName,reserved);    }    if (!isParent(queueExprArray,idx)) {      LeafQueue lq=(LeafQueue)queue;      when(lq.getTotalPendingResourcesConsideringUserLimit(isA(Resource.class),isA(String.class),eq(false))).thenReturn(pending);
@Test public void testPeriodicCapacity(){  int[] alloc={10,7,5,2,0};  long[] timeSteps={0L,5L,10L,15L,19L};  RLESparseResourceAllocation rleSparseVector=ReservationSystemTestUtil.generateRLESparseResourceAllocation(alloc,timeSteps);  PeriodicRLESparseResourceAllocation periodicVector=new PeriodicRLESparseResourceAllocation(rleSparseVector,20L);
@Test public void testMaxPeriodicCapacity(){  int[] alloc={2,5,7,10,3,4,6,8};  long[] timeSteps={0L,1L,2L,3L,4L,5L,6L,7L};  RLESparseResourceAllocation rleSparseVector=ReservationSystemTestUtil.generateRLESparseResourceAllocation(alloc,timeSteps);  PeriodicRLESparseResourceAllocation periodicVector=new PeriodicRLESparseResourceAllocation(rleSparseVector,8L);
@Test public void testPartialRemoval(){  ResourceCalculator resCalc=new DefaultResourceCalculator();  RLESparseResourceAllocation rleSparseVector=new RLESparseResourceAllocation(resCalc);  ReservationInterval riAdd=new ReservationInterval(10,20);  Resource rr=Resource.newInstance(1024 * 100,100);  ReservationInterval riAdd2=new ReservationInterval(20,30);  Resource rr2=Resource.newInstance(1024 * 200,200);  ReservationInterval riRemove=new ReservationInterval(12,25);
@Test public void testPartialRemoval(){  ResourceCalculator resCalc=new DefaultResourceCalculator();  RLESparseResourceAllocation rleSparseVector=new RLESparseResourceAllocation(resCalc);  ReservationInterval riAdd=new ReservationInterval(10,20);  Resource rr=Resource.newInstance(1024 * 100,100);  ReservationInterval riAdd2=new ReservationInterval(20,30);  Resource rr2=Resource.newInstance(1024 * 200,200);  ReservationInterval riRemove=new ReservationInterval(12,25);  LOG.info(rleSparseVector.toString());  rleSparseVector.addInterval(riAdd,rr);  rleSparseVector.addInterval(riAdd2,rr2);
@Test public void testPartialRemoval(){  ResourceCalculator resCalc=new DefaultResourceCalculator();  RLESparseResourceAllocation rleSparseVector=new RLESparseResourceAllocation(resCalc);  ReservationInterval riAdd=new ReservationInterval(10,20);  Resource rr=Resource.newInstance(1024 * 100,100);  ReservationInterval riAdd2=new ReservationInterval(20,30);  Resource rr2=Resource.newInstance(1024 * 200,200);  ReservationInterval riRemove=new ReservationInterval(12,25);  LOG.info(rleSparseVector.toString());  rleSparseVector.addInterval(riAdd,rr);  rleSparseVector.addInterval(riAdd2,rr2);  LOG.info(rleSparseVector.toString());  rleSparseVector.removeInterval(riRemove,rr);
  Resource rr2=Resource.newInstance(1024 * 200,200);  ReservationInterval riRemove=new ReservationInterval(12,25);  LOG.info(rleSparseVector.toString());  rleSparseVector.addInterval(riAdd,rr);  rleSparseVector.addInterval(riAdd2,rr2);  LOG.info(rleSparseVector.toString());  rleSparseVector.removeInterval(riRemove,rr);  LOG.info(rleSparseVector.toString());  Assert.assertEquals(102400,rleSparseVector.getCapacityAtTime(10).getMemorySize());  Assert.assertEquals(0,rleSparseVector.getCapacityAtTime(13).getMemorySize());  Assert.assertEquals(0,rleSparseVector.getCapacityAtTime(19).getMemorySize());  Assert.assertEquals(102400,rleSparseVector.getCapacityAtTime(21).getMemorySize());  Assert.assertEquals(2 * 102400,rleSparseVector.getCapacityAtTime(26).getMemorySize());  ReservationInterval riRemove2=new ReservationInterval(9,13);  rleSparseVector.removeInterval(riRemove2,rr);
@Test public void testZeroAllocation(){  ResourceCalculator resCalc=new DefaultResourceCalculator();  RLESparseResourceAllocation rleSparseVector=new RLESparseResourceAllocation(resCalc);  rleSparseVector.addInterval(new ReservationInterval(0,Long.MAX_VALUE),Resource.newInstance(0,0));
@Test public void testMaxPeriodicCapacity(){  long[] timeSteps={0L,1L,2L,3L,4L,5L,6L,7L};  int[] alloc={2,5,7,10,3,4,6,8};  RLESparseResourceAllocation rleSparseVector=ReservationSystemTestUtil.generateRLESparseResourceAllocation(alloc,timeSteps);
@Test public void testGetMinimumCapacityInInterval(){  long[] timeSteps={0L,1L,2L,3L,4L,5L,6L,7L};  int[] alloc={2,5,7,10,3,4,0,8};  RLESparseResourceAllocation rleSparseVector=ReservationSystemTestUtil.generateRLESparseResourceAllocation(alloc,timeSteps);
    Assert.fail();  } catch (  YarnException e) {    Assert.assertNull(plan);    String message=e.getMessage();    Assert.assertTrue(message.startsWith("Invalid period "));    LOG.info(message);  }  request=createSimpleReservationSubmissionRequest(1,1,1,50,3,"10");  plan=null;  try {    plan=rrValidator.validateReservationSubmissionRequest(rSystem,request,ReservationSystemTestUtil.getNewReservationId());    Assert.fail();  } catch (  YarnException e) {    Assert.assertNull(plan);    String message=e.getMessage();    Assert.assertTrue(message.startsWith("Duration of the requested reservation:"));
    Assert.fail();  } catch (  YarnException e) {    Assert.assertNull(plan);    String message=e.getMessage();    Assert.assertTrue(message.startsWith("Invalid period "));    LOG.info(message);  }  request=createSimpleReservationUpdateRequest(1,1,1,50,3,"10");  plan=null;  try {    plan=rrValidator.validateReservationUpdateRequest(rSystem,request);    Assert.fail();  } catch (  YarnException e) {    Assert.assertNull(plan);    String message=e.getMessage();    Assert.assertTrue(message.startsWith("Duration of the requested reservation:"));
@Before public void setup() throws Exception {  long seed=rand.nextLong();  rand.setSeed(seed);
@Before public void setup() throws Exception {  long seed=rand.nextLong();  rand.setSeed(seed);
@Before public void setup() throws Exception {  long seed=rand.nextLong();  rand.setSeed(seed);
private void logAssertingMessage(MetricsSource source){  String queueName=((QueueMetrics)source).queueName;  Map<String,QueueMetrics> users=((QueueMetrics)source).users;  if (LOG.isDebugEnabled()) {
private void logAssertingMessage(MetricsSource source){  String queueName=((QueueMetrics)source).queueName;  Map<String,QueueMetrics> users=((QueueMetrics)source).users;  if (LOG.isDebugEnabled()) {
private void waitForLaunchedState(RMAppAttempt attempt) throws InterruptedException {  int waitCount=0;  while (attempt.getAppAttemptState() != RMAppAttemptState.LAUNCHED && waitCount++ < 20) {
  YarnConfiguration conf=new YarnConfiguration();  CapacitySchedulerContext csContext=mock(CapacitySchedulerContext.class);  when(csContext.getConfiguration()).thenReturn(csConf);  when(csContext.getConf()).thenReturn(conf);  when(csContext.getMinimumResourceCapability()).thenReturn(Resources.createResource(GB,1));  when(csContext.getMaximumResourceCapability()).thenReturn(Resources.createResource(16 * GB,16));  when(csContext.getResourceCalculator()).thenReturn(resourceCalculator);  when(csContext.getRMContext()).thenReturn(rmContext);  when(csContext.getPreemptionManager()).thenReturn(new PreemptionManager());  Resource clusterResource=Resources.createResource(100 * 16 * GB,100 * 16);  when(csContext.getClusterResource()).thenReturn(clusterResource);  CSQueueStore queues=new CSQueueStore();  CSQueue root=CapacitySchedulerQueueManager.parseQueue(csContext,csConf,null,"root",queues,queues,TestUtils.spyHook);  root.updateClusterResource(clusterResource,new ResourceLimits(clusterResource));  LeafQueue queue=(LeafQueue)queues.get(A);
  nodeUpdate(nm_0);  nodeUpdate(nm_1);  application_0.schedule();  checkApplicationResourceUsage(1 * GB,application_0);  application_1.schedule();  checkApplicationResourceUsage(3 * GB,application_1);  checkNodeResourceUsage(4 * GB,nm_0);  checkNodeResourceUsage(0 * GB,nm_1);  LOG.info("Adding new tasks...");  Task task_1_1=new Task(application_1,priority_0,new String[]{ResourceRequest.ANY});  application_1.addTask(task_1_1);  application_1.schedule();  Task task_0_1=new Task(application_0,priority_0,new String[]{host_0,host_1});  application_0.addTask(task_0_1);  application_0.schedule();
  application_0.schedule();  checkApplicationResourceUsage(1 * GB,application_0);  application_1.schedule();  checkApplicationResourceUsage(3 * GB,application_1);  checkNodeResourceUsage(4 * GB,nm_0);  checkNodeResourceUsage(0 * GB,nm_1);  LOG.info("Adding new tasks...");  Task task_1_1=new Task(application_1,priority_0,new String[]{ResourceRequest.ANY});  application_1.addTask(task_1_1);  application_1.schedule();  Task task_0_1=new Task(application_0,priority_0,new String[]{host_0,host_1});  application_0.addTask(task_0_1);  application_0.schedule();  LOG.info("Sending hb from " + nm_0.getHostName());  nodeUpdate(nm_0);
  conf.setClass(YarnConfiguration.RM_SCHEDULER,CapacityScheduler.class,ResourceScheduler.class);  MyContainerManager containerManager=new MyContainerManager();  final MockRMWithAMS rm=new MockRMWithAMS(conf,containerManager);  rm.start();  MockNM nm1=rm.registerNode("localhost:1234",5120);  Map<ApplicationAccessType,String> acls=new HashMap<ApplicationAccessType,String>(2);  acls.put(ApplicationAccessType.VIEW_APP,"*");  MockRMAppSubmissionData data=MockRMAppSubmissionData.Builder.createWithMemory(1024,rm).withAppName("appname").withUser("appuser").withAcls(acls).build();  RMApp app=MockRMAppSubmitter.submit(rm,data);  nm1.nodeHeartbeat(true);  RMAppAttempt attempt=app.getCurrentAppAttempt();  ApplicationAttemptId applicationAttemptId=attempt.getAppAttemptId();  int msecToWait=10000;  int msecToSleep=100;  while (attempt.getAppAttemptState() != RMAppAttemptState.LAUNCHED && msecToWait > 0) {
  String b1QTobeDeleted="b1";  LeafQueue csB1Queue=Mockito.spy((LeafQueue)queues.get(b1QTobeDeleted));  when(csB1Queue.getState()).thenReturn(QueueState.DRAINING).thenReturn(QueueState.STOPPED);  cs.getCapacitySchedulerQueueManager().addQueue(b1QTobeDeleted,csB1Queue);  conf=new CapacitySchedulerConfiguration();  setupQueueConfigurationWithOutB1(conf);  try {    cs.reinitialize(conf,mockContext);    fail("Expected to throw exception when refresh queue tries to delete a" + " queue with running apps");  } catch (  IOException e) {  }  conf=new CapacitySchedulerConfiguration();  setupQueueConfigurationWithOutB1(conf);  try {    cs.reinitialize(conf,mockContext);  } catch (  IOException e) {
  conf.setQueues(AGROUP,new String[]{"f"});  conf.setCapacity(AGROUP_A,100f);  conf.setUserLimitFactor(AGROUP_A,100.0f);  conf.setUserLimitFactor(C,1.0f);  conf.setAutoCreateChildQueueEnabled(C,true);  conf.setAutoCreatedLeafQueueConfigCapacity(C,50.0f);  conf.setAutoCreatedLeafQueueConfigMaxCapacity(C,100.0f);  conf.setAutoCreatedLeafQueueConfigUserLimit(C,100);  conf.setAutoCreatedLeafQueueConfigUserLimitFactor(C,3.0f);  conf.setAutoCreatedLeafQueueTemplateCapacityByLabel(C,NODEL_LABEL_GPU,NODE_LABEL_GPU_TEMPLATE_CAPACITY);  conf.setAutoCreatedLeafQueueTemplateMaxCapacity(C,NODEL_LABEL_GPU,100.0f);  conf.setAutoCreatedLeafQueueTemplateCapacityByLabel(C,NODEL_LABEL_SSD,NODEL_LABEL_SSD_TEMPLATE_CAPACITY);  conf.setAutoCreatedLeafQueueTemplateMaxCapacity(C,NODEL_LABEL_SSD,100.0f);  conf.setDefaultNodeLabelExpression(C,NODEL_LABEL_GPU);  conf.setAutoCreatedLeafQueueConfigDefaultNodeLabelExpression(C,NODEL_LABEL_SSD);
  conf.setAutoCreatedLeafQueueConfigCapacity(D,10.0f);  conf.setAutoCreatedLeafQueueConfigMaxCapacity(D,100.0f);  conf.setAutoCreatedLeafQueueConfigUserLimit(D,3);  conf.setAutoCreatedLeafQueueConfigUserLimitFactor(D,100);  conf.set(CapacitySchedulerConfiguration.PREFIX + C + DOT+ CapacitySchedulerConfiguration.AUTO_CREATED_LEAF_QUEUE_TEMPLATE_PREFIX+ DOT+ CapacitySchedulerConfiguration.ORDERING_POLICY,FAIR_APP_ORDERING_POLICY);  accessibleNodeLabelsOnC.add(NODEL_LABEL_GPU);  accessibleNodeLabelsOnC.add(NODEL_LABEL_SSD);  accessibleNodeLabelsOnC.add(NO_LABEL);  conf.setAccessibleNodeLabels(C,accessibleNodeLabelsOnC);  conf.setAccessibleNodeLabels(ROOT,accessibleNodeLabelsOnC);  conf.setCapacityByLabel(ROOT,NODEL_LABEL_GPU,100f);  conf.setCapacityByLabel(ROOT,NODEL_LABEL_SSD,100f);  conf.setAccessibleNodeLabels(C,accessibleNodeLabelsOnC);  conf.setCapacityByLabel(C,NODEL_LABEL_GPU,100f);  conf.setCapacityByLabel(C,NODEL_LABEL_SSD,100f);
protected void validateDeactivatedQueueEntitlement(CSQueue parentQueue,String leafQueueName,Map<String,Float> expectedTotalChildQueueAbsCapacity,List<QueueManagementChange> queueManagementChanges) throws SchedulerDynamicEditException {  QueueEntitlement expectedEntitlement=new QueueEntitlement(0.0f,1.0f);  ManagedParentQueue autoCreateEnabledParentQueue=(ManagedParentQueue)parentQueue;  AutoCreatedLeafQueue leafQueue=(AutoCreatedLeafQueue)cs.getQueue(leafQueueName);  GuaranteedOrZeroCapacityOverTimePolicy policy=(GuaranteedOrZeroCapacityOverTimePolicy)autoCreateEnabledParentQueue.getAutoCreatedQueueManagementPolicy();  Map<String,QueueEntitlement> expectedEntitlements=new HashMap<>();  for (  String label : accessibleNodeLabelsOnC) {
@Test(timeout=60000) public void testExcessReservationThanNodeManagerCapacity() throws Exception {  @SuppressWarnings("resource") MockRM rm=new MockRM(conf);  rm.start();  MockNM nm1=rm.registerNode("127.0.0.1:1234",2 * GB,4);  MockNM nm2=rm.registerNode("127.0.0.1:2234",3 * GB,4);  nm1.nodeHeartbeat(true);  nm2.nodeHeartbeat(true);  int waitCount=20;  int size=rm.getRMContext().getRMNodes().size();  while ((size=rm.getRMContext().getRMNodes().size()) != 2 && waitCount-- > 0) {
  Resource clusterResource=Resources.createResource(numNodes * (8 * GB),numNodes * 100);  when(csContext.getNumClusterNodes()).thenReturn(numNodes);  when(csContext.getClusterResource()).thenReturn(clusterResource);  root.updateClusterResource(clusterResource,new ResourceLimits(clusterResource));  Priority priority=TestUtils.createMockPriority(1);  app0.updateResourceRequests(Collections.singletonList(TestUtils.createResourceRequest(ResourceRequest.ANY,1 * GB,40,10,true,priority,recordFactory,NO_LABEL)));  app2.updateResourceRequests(Collections.singletonList(TestUtils.createResourceRequest(ResourceRequest.ANY,2 * GB,10,10,true,priority,recordFactory,NO_LABEL)));  b.setUserLimit(50);  b.setUserLimitFactor(2);  User queueUser0=b.getUser(user0);  User queueUser1=b.getUser(user1);  assertEquals("There should 2 active users!",2,b.getAbstractUsersManager().getNumActiveUsers());  CSAssignment assign;  do {    assign=b.assignContainers(clusterResource,node0,new ResourceLimits(clusterResource),SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY);
public static FiCaSchedulerNode getMockNode(String host,String rack,int port,int memory,int vcores){  NodeId nodeId=NodeId.newInstance(host,port);  RMNode rmNode=mock(RMNode.class);  when(rmNode.getNodeID()).thenReturn(nodeId);  when(rmNode.getTotalCapability()).thenReturn(Resources.createResource(memory,vcores));  when(rmNode.getNodeAddress()).thenReturn(host + ":" + port);  when(rmNode.getHostName()).thenReturn(host);  when(rmNode.getRackName()).thenReturn(rack);  when(rmNode.getState()).thenReturn(NodeState.RUNNING);  FiCaSchedulerNode node=spy(new FiCaSchedulerNode(rmNode,false));
private static void printTags(Collection<MockNM> nodes,AllocationTagsManager atm){  for (  MockNM nm : nodes) {    Map<String,Long> nmTags=atm.getAllocationTagsWithCount(nm.getNodeId());    StringBuffer sb=new StringBuffer();    if (nmTags != null) {      nmTags.forEach((tag,count) -> sb.append(tag + "(" + count+ "),"));
@Test public void testRoundRobinSimpleAllocation() throws Exception {  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(1,ResourceRequest.ANY,1),createResourceRequest(2,ResourceRequest.ANY,1),createResourceRequest(3,ResourceRequest.ANY,1));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,3));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");
@Test public void testNodeLocalAllocation() throws Exception {  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(1,ResourceRequest.ANY,1),createResourceRequest(2,"/r1",1),createResourceRequest(2,"h1",1),createResourceRequest(2,ResourceRequest.ANY,1),createResourceRequest(3,"/r1",1),createResourceRequest(3,"h1",1),createResourceRequest(3,ResourceRequest.ANY,1));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");
@Test public void testNodeLocalAllocationSameSchedulerKey() throws Exception {  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(2,"/r1",2),createResourceRequest(2,"h1",2),createResourceRequest(2,ResourceRequest.ANY,2));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");
@Test public void testOffSwitchAllocationWhenNoNodeOrRack() throws Exception {  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(2,"/r3",2),createResourceRequest(2,"h6",2),createResourceRequest(2,ResourceRequest.ANY,2));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  NodeQueueLoadMonitor selector=createNodeQueueLoadMonitor(Arrays.asList("h1","h2","h3","h4"),Arrays.asList("/r2","/r1","/r2","/r1"),Arrays.asList(4,4,4,4),Arrays.asList(5,5,5,5));  allocator.setNodeQueueLoadMonitor(selector);  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");
@Test public void testMaxAllocationsPerAMHeartbeat() throws Exception {  allocator.setMaxAllocationsPerAMHeartbeat(2);  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(2,"/r3",3),createResourceRequest(2,"h6",3),createResourceRequest(2,ResourceRequest.ANY,3));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");
@Test public void testMaxAllocationsPerAMHeartbeat() throws Exception {  allocator.setMaxAllocationsPerAMHeartbeat(2);  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(2,"/r3",3),createResourceRequest(2,"h6",3),createResourceRequest(2,ResourceRequest.ANY,3));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");  LOG.info("Containers: {}",containers);  assertEquals(2,containers.size());  containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,new ArrayList<>(),appAttId,oppCntxt,1L,"user");
@Test public void testMaxAllocationsPerAMHeartbeatDifferentSchedKey() throws Exception {  allocator.setMaxAllocationsPerAMHeartbeat(2);  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(1,ResourceRequest.ANY,1),createResourceRequest(2,"h6",2),createResourceRequest(3,"/r3",2));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");
@Test public void testMaxAllocationsPerAMHeartbeatDifferentSchedKey() throws Exception {  allocator.setMaxAllocationsPerAMHeartbeat(2);  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(1,ResourceRequest.ANY,1),createResourceRequest(2,"h6",2),createResourceRequest(3,"/r3",2));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");  LOG.info("Containers: {}",containers);  assertEquals(2,containers.size());  containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,new ArrayList<>(),appAttId,oppCntxt,1L,"user");
@Test public void testMaxAllocationsPerAMHeartbeatDifferentSchedKey() throws Exception {  allocator.setMaxAllocationsPerAMHeartbeat(2);  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(1,ResourceRequest.ANY,1),createResourceRequest(2,"h6",2),createResourceRequest(3,"/r3",2));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");  LOG.info("Containers: {}",containers);  assertEquals(2,containers.size());  containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,new ArrayList<>(),appAttId,oppCntxt,1L,"user");  LOG.info("Containers: {}",containers);  assertEquals(2,containers.size());  containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,new ArrayList<>(),appAttId,oppCntxt,1L,"user");
@Test public void testAllocationLatencyMetrics() throws Exception {  oppCntxt=spy(oppCntxt);  OpportunisticSchedulerMetrics metrics=mock(OpportunisticSchedulerMetrics.class);  when(oppCntxt.getOppSchedulerMetrics()).thenReturn(metrics);  List<ResourceRequest> reqs=Arrays.asList(createResourceRequest(2,"/r3",2),createResourceRequest(2,"h6",2),createResourceRequest(2,ResourceRequest.ANY,2));  ApplicationAttemptId appAttId=ApplicationAttemptId.newInstance(ApplicationId.newInstance(0L,1),1);  allocator.setNodeQueueLoadMonitor(createNodeQueueLoadMonitor(3,2,5));  List<Container> containers=allocator.allocateContainers(EMPTY_BLACKLIST_REQUEST,reqs,appAttId,oppCntxt,1L,"user");
@Test public void testConvertFSConfigurationDefaults() throws Exception {  setupFSConfigConversionFiles(true);  ArgumentCaptor<FSConfigToCSConfigConverterParams> conversionParams=ArgumentCaptor.forClass(FSConfigToCSConfigConverterParams.class);  FSConfigToCSConfigArgumentHandler argumentHandler=createArgumentHandler();  String[] args=getArgumentsAsArrayWithDefaults("-f",FSConfigConverterTestCommons.FS_ALLOC_FILE,"-r",FSConfigConverterTestCommons.CONVERSION_RULES_FILE);  argumentHandler.parseAndConvert(args);  verify(mockConverter).convert(conversionParams.capture());  FSConfigToCSConfigConverterParams params=conversionParams.getValue();
@Test public void testConvertFSConfigurationWithConsoleParam() throws Exception {  setupFSConfigConversionFiles(true);  ArgumentCaptor<FSConfigToCSConfigConverterParams> conversionParams=ArgumentCaptor.forClass(FSConfigToCSConfigConverterParams.class);  FSConfigToCSConfigArgumentHandler argumentHandler=createArgumentHandler();  String[] args=getArgumentsAsArrayWithDefaults("-f",FSConfigConverterTestCommons.FS_ALLOC_FILE,"-r",FSConfigConverterTestCommons.CONVERSION_RULES_FILE,"-p");  argumentHandler.parseAndConvert(args);  verify(mockConverter).convert(conversionParams.capture());  FSConfigToCSConfigConverterParams params=conversionParams.getValue();
@Test public void testConvertFSConfigurationClusterResource() throws Exception {  setupFSConfigConversionFiles(true);  ArgumentCaptor<FSConfigToCSConfigConverterParams> conversionParams=ArgumentCaptor.forClass(FSConfigToCSConfigConverterParams.class);  FSConfigToCSConfigArgumentHandler argumentHandler=createArgumentHandler();  String[] args=getArgumentsAsArrayWithDefaults("-f",FSConfigConverterTestCommons.FS_ALLOC_FILE,"-r",FSConfigConverterTestCommons.CONVERSION_RULES_FILE,"-p","-c","vcores=20, memory-mb=240");  argumentHandler.parseAndConvert(args);  verify(mockConverter).convert(conversionParams.capture());  FSConfigToCSConfigConverterParams params=conversionParams.getValue();
  Assert.assertEquals(nm1.getNodeId(),allocated1.get(0).getNodeId());  report_nm1=rm.getResourceScheduler().getNodeReport(nm1.getNodeId());  Assert.assertEquals(0,report_nm1.getAvailableResource().getMemorySize());  Assert.assertEquals(4 * GB,report_nm1.getUsedResource().getMemorySize());  Container c1=allocated1.get(0);  Assert.assertEquals(2 * GB,c1.getResource().getMemorySize());  Map<NodeId,ResourceOption> nodeResourceMap=new HashMap<NodeId,ResourceOption>();  nodeResourceMap.put(nm1.getNodeId(),ResourceOption.newInstance(Resource.newInstance(2 * GB,1),-1));  UpdateNodeResourceRequest request=UpdateNodeResourceRequest.newInstance(nodeResourceMap);  rm.getAdminService().updateNodeResource(request);  waitCount=0;  while (waitCount++ != 20) {    report_nm1=rm.getResourceScheduler().getNodeReport(nm1.getNodeId());    if (null != report_nm1 && report_nm1.getAvailableResource().getMemorySize() != 0) {      break;
  UpdateNodeResourceRequest request=UpdateNodeResourceRequest.newInstance(nodeResourceMap);  rm.getAdminService().updateNodeResource(request);  waitCount=0;  while (waitCount++ != 20) {    report_nm1=rm.getResourceScheduler().getNodeReport(nm1.getNodeId());    if (null != report_nm1 && report_nm1.getAvailableResource().getMemorySize() != 0) {      break;    }    LOG.info("Waiting for RMNodeResourceUpdateEvent to be handled... Tried " + waitCount + " times already..");    Thread.sleep(1000);  }  report_nm1=rm.getResourceScheduler().getNodeReport(nm1.getNodeId());  Assert.assertEquals(4 * GB,report_nm1.getUsedResource().getMemorySize());  Assert.assertEquals(-2 * GB,report_nm1.getAvailableResource().getMemorySize());  ContainerStatus containerStatus=BuilderUtils.newContainerStatus(c1.getId(),ContainerState.COMPLETE,"",0,c1.getResource());  nm1.containerStatus(containerStatus);  waitCount=0;
@Test(timeout=60000) public void testDTRenewal() throws Exception {  MyFS dfs=(MyFS)FileSystem.get(conf);
@Test(timeout=60000) public void testDTRenewal() throws Exception {  MyFS dfs=(MyFS)FileSystem.get(conf);  LOG.info("dfs=" + (Object)dfs.hashCode() + ";conf="+ conf.hashCode());  MyToken token1, token2, token3;  token1=dfs.getDelegationToken("user1");  token2=dfs.getDelegationToken("user2");  token3=dfs.getDelegationToken("user3");  Renewer.tokenToRenewIn2Sec=token1;
  delegationTokenRenewer.addApplicationAsync(applicationId_0,ts,true,"user",new Configuration());  waitForEventsToGetProcessed(delegationTokenRenewer);  int numberOfExpectedRenewals=3 + 1;  int attempts=10;  while (attempts-- > 0) {    try {      Thread.sleep(3 * 1000);    } catch (    InterruptedException e) {    }    if (Renewer.counter == numberOfExpectedRenewals)     break;  }  LOG.info("dfs=" + dfs.hashCode() + ";Counter = "+ Renewer.counter+ ";t="+ Renewer.lastRenewed);  assertEquals("renew wasn't called as many times as expected(4):",numberOfExpectedRenewals,Renewer.counter);  assertEquals("most recently renewed token mismatch",Renewer.lastRenewed,token1);  ts=new Credentials();  MyToken token4=dfs.getDelegationToken("user4");  Renewer.tokenToRenewIn2Sec=token4;
  assertEquals("renew wasn't called as many times as expected(4):",numberOfExpectedRenewals,Renewer.counter);  assertEquals("most recently renewed token mismatch",Renewer.lastRenewed,token1);  ts=new Credentials();  MyToken token4=dfs.getDelegationToken("user4");  Renewer.tokenToRenewIn2Sec=token4;  LOG.info("token=" + token4 + " should be renewed for 2 secs");  String nn4=DelegationTokenRenewer.SCHEME + "://host4:0";  ts.addToken(new Text(nn4),token4);  ApplicationId applicationId_1=BuilderUtils.newApplicationId(0,1);  delegationTokenRenewer.addApplicationAsync(applicationId_1,ts,true,"user",new Configuration());  waitForEventsToGetProcessed(delegationTokenRenewer);  delegationTokenRenewer.applicationFinished(applicationId_1);  waitForEventsToGetProcessed(delegationTokenRenewer);  numberOfExpectedRenewals=Renewer.counter;  try {
@Test(timeout=60000) public void testAppRejectionWithCancelledDelegationToken() throws Exception {  MyFS dfs=(MyFS)FileSystem.get(conf);
@Test(timeout=60000) public void testAppTokenWithNonRenewer() throws Exception {  MyFS dfs=(MyFS)FileSystem.get(conf);
@Test(timeout=60000) public void testDTRenewalWithNoCancel() throws Exception {  MyFS dfs=(MyFS)FileSystem.get(conf);
@Test(timeout=60000) public void testDTRenewalWithNoCancel() throws Exception {  MyFS dfs=(MyFS)FileSystem.get(conf);  LOG.info("dfs=" + (Object)dfs.hashCode() + ";conf="+ conf.hashCode());  Credentials ts=new Credentials();  MyToken token1=dfs.getDelegationToken("user1");  Renewer.tokenToRenewIn2Sec=token1;
  lconf.setBoolean(YarnConfiguration.RM_DELEGATION_TOKEN_ALWAYS_CANCEL,true);  DelegationTokenRenewer localDtr=createNewDelegationTokenRenewer(lconf,counter);  RMContext mockContext=mock(RMContext.class);  when(mockContext.getSystemCredentialsForApps()).thenReturn(new ConcurrentHashMap<ApplicationId,SystemCredentialsForAppsProto>());  ClientRMService mockClientRMService=mock(ClientRMService.class);  when(mockContext.getClientRMService()).thenReturn(mockClientRMService);  when(mockContext.getDelegationTokenRenewer()).thenReturn(localDtr);  when(mockContext.getDispatcher()).thenReturn(dispatcher);  InetSocketAddress sockAddr=InetSocketAddress.createUnresolved("localhost",1234);  when(mockClientRMService.getBindAddress()).thenReturn(sockAddr);  localDtr.setDelegationTokenRenewerPoolTracker(false);  localDtr.setRMContext(mockContext);  localDtr.init(lconf);  localDtr.start();  MyFS dfs=(MyFS)FileSystem.get(lconf);
  ClientRMService mockClientRMService=mock(ClientRMService.class);  when(mockContext.getClientRMService()).thenReturn(mockClientRMService);  when(mockContext.getDelegationTokenRenewer()).thenReturn(localDtr);  when(mockContext.getDispatcher()).thenReturn(dispatcher);  InetSocketAddress sockAddr=InetSocketAddress.createUnresolved("localhost",1234);  when(mockClientRMService.getBindAddress()).thenReturn(sockAddr);  localDtr.setDelegationTokenRenewerPoolTracker(false);  localDtr.setRMContext(mockContext);  localDtr.init(lconf);  localDtr.start();  MyFS dfs=(MyFS)FileSystem.get(lconf);  LOG.info("dfs=" + (Object)dfs.hashCode() + ";conf="+ lconf.hashCode());  Credentials ts=new Credentials();  MyToken token1=dfs.getDelegationToken("user1");  Renewer.tokenToRenewIn2Sec=token1;
  lconf.setLong(YarnConfiguration.RM_DELAYED_DELEGATION_TOKEN_REMOVAL_INTERVAL_MS,1000l);  DelegationTokenRenewer localDtr=createNewDelegationTokenRenewer(lconf,counter);  RMContext mockContext=mock(RMContext.class);  when(mockContext.getSystemCredentialsForApps()).thenReturn(new ConcurrentHashMap<ApplicationId,SystemCredentialsForAppsProto>());  ClientRMService mockClientRMService=mock(ClientRMService.class);  when(mockContext.getClientRMService()).thenReturn(mockClientRMService);  when(mockContext.getDelegationTokenRenewer()).thenReturn(localDtr);  when(mockContext.getDispatcher()).thenReturn(dispatcher);  InetSocketAddress sockAddr=InetSocketAddress.createUnresolved("localhost",1234);  when(mockClientRMService.getBindAddress()).thenReturn(sockAddr);  localDtr.setDelegationTokenRenewerPoolTracker(false);  localDtr.setRMContext(mockContext);  localDtr.init(lconf);  localDtr.start();  MyFS dfs=(MyFS)FileSystem.get(lconf);
  lconf.setLong(YarnConfiguration.RM_DELAYED_DELEGATION_TOKEN_REMOVAL_INTERVAL_MS,1000l);  DelegationTokenRenewer localDtr=createNewDelegationTokenRenewer(conf,counter);  RMContext mockContext=mock(RMContext.class);  when(mockContext.getSystemCredentialsForApps()).thenReturn(new ConcurrentHashMap<ApplicationId,SystemCredentialsForAppsProto>());  ClientRMService mockClientRMService=mock(ClientRMService.class);  when(mockContext.getClientRMService()).thenReturn(mockClientRMService);  when(mockContext.getDelegationTokenRenewer()).thenReturn(localDtr);  when(mockContext.getDispatcher()).thenReturn(dispatcher);  InetSocketAddress sockAddr=InetSocketAddress.createUnresolved("localhost",1234);  when(mockClientRMService.getBindAddress()).thenReturn(sockAddr);  localDtr.setDelegationTokenRenewerPoolTracker(false);  localDtr.setRMContext(mockContext);  localDtr.init(lconf);  localDtr.start();  MyFS dfs=(MyFS)FileSystem.get(lconf);
private DelegationTokenRenewer createNewDelegationTokenRenewerForTimeout(Configuration config,final AtomicInteger renewerCounter,final AtomicBoolean renewDelay){  DelegationTokenRenewer renew=new DelegationTokenRenewer(){    @Override protected ThreadPoolExecutor createNewThreadPoolService(    Configuration configuration){      ThreadPoolExecutor pool=new ThreadPoolExecutor(5,5,3L,TimeUnit.SECONDS,new LinkedBlockingQueue<Runnable>()){        @Override public Future<?> submit(        Runnable r){          renewerCounter.incrementAndGet();          return super.submit(r);        }      };      return pool;    }    @Override protected void renewToken(    final DelegationTokenToRenew dttr) throws IOException {      try {        if (renewDelay.get()) {          Thread.sleep(config.getTimeDuration(YarnConfiguration.RM_DT_RENEWER_THREAD_TIMEOUT,YarnConfiguration.DEFAULT_RM_DT_RENEWER_THREAD_TIMEOUT,TimeUnit.MILLISECONDS) * 2);        }        super.renewToken(dttr);      } catch (      InterruptedException e) {
public void verifyClusterSchedulerFifo(JSONObject json) throws JSONException, Exception {  assertEquals("incorrect number of elements in: " + json,1,json.length());  JSONObject info=json.getJSONObject("scheduler");  assertEquals("incorrect number of elements in: " + info,1,info.length());  info=info.getJSONObject("schedulerInfo");
@Test public void testUpdateQueue() throws Exception {  WebResource r=resource();  ClientResponse response;  SchedConfUpdateInfo updateInfo=new SchedConfUpdateInfo();  Map<String,String> updateParam=new HashMap<>();  updateParam.put(CapacitySchedulerConfiguration.MAXIMUM_AM_RESOURCE_SUFFIX,"0.2");  QueueConfigInfo aUpdateInfo=new QueueConfigInfo("root.a",updateParam);  updateInfo.getUpdateQueueInfo().add(aUpdateInfo);  CapacityScheduler cs=(CapacityScheduler)rm.getResourceScheduler();  assertEquals(CapacitySchedulerConfiguration.DEFAULT_MAXIMUM_APPLICATIONMASTERS_RESOURCE_PERCENT,cs.getConfiguration().getMaximumApplicationMasterResourcePerQueuePercent("root.a"),0.001f);  response=r.path("ws").path("v1").path("cluster").path("scheduler-conf").queryParam("user.name",userName).accept(MediaType.APPLICATION_JSON).entity(YarnWebServiceUtils.toJson(updateInfo,SchedConfUpdateInfo.class),MediaType.APPLICATION_JSON).put(ClientResponse.class);
private void logResponse(){  String responseStr=response.getEntity(String.class);
private void logResponse(){  String responseStr=response.getEntity(String.class);  LOG.info("Raw response from service URL {}: {}",path.toString(),responseStr);
private void logResponse(Document doc){  String responseStr=response.getEntity(String.class);
private void logResponse(Document doc){  String responseStr=response.getEntity(String.class);  LOG.info("Raw response from service URL {}: {}",path.toString(),responseStr);
@Public @Unstable public static void logAndThrowException(String errMsg,Throwable t) throws YarnException {  if (t != null) {
@Override public GetNewApplicationResponse getNewApplication(GetNewApplicationRequest request) throws YarnException, IOException {  long startTime=clock.getTime();  Map<SubClusterId,SubClusterInfo> subClustersActive=federationFacade.getSubClusters(true);  for (int i=0; i < numSubmitRetries; ++i) {    SubClusterId subClusterId=getRandomActiveSubCluster(subClustersActive);
    LOG.debug("getNewApplication try #{} on SubCluster {}",i,subClusterId);    ApplicationClientProtocol clientRMProxy=getClientRMProxyForSubCluster(subClusterId);    GetNewApplicationResponse response=null;    try {      response=clientRMProxy.getNewApplication(request);    } catch (    Exception e) {      LOG.warn("Unable to create a new ApplicationId in SubCluster " + subClusterId.getId(),e);    }    if (response != null) {      long stopTime=clock.getTime();      routerMetrics.succeededAppsCreated(stopTime - startTime);      return response;    } else {      subClustersActive.remove(subClusterId);    }  }  routerMetrics.incrAppsFailedCreated();  String errMsg="Fail to create a new application.";
    LOG.info("submitApplication appId" + applicationId + " try #"+ i+ " on SubCluster "+ subClusterId);    ApplicationHomeSubCluster appHomeSubCluster=ApplicationHomeSubCluster.newInstance(applicationId,subClusterId);    if (i == 0) {      try {        subClusterId=federationFacade.addApplicationHomeSubCluster(appHomeSubCluster);      } catch (      YarnException e) {        routerMetrics.incrAppsFailedSubmitted();        String message="Unable to insert the ApplicationId " + applicationId + " into the FederationStateStore";        RouterServerUtil.logAndThrowException(message,e);      }    } else {      try {        federationFacade.updateApplicationHomeSubCluster(appHomeSubCluster);      } catch (      YarnException e) {        String message="Unable to update the ApplicationId " + applicationId + " into the FederationStateStore";        SubClusterId subClusterIdInStateStore=federationFacade.getApplicationHomeSubCluster(applicationId);
    } else {      try {        federationFacade.updateApplicationHomeSubCluster(appHomeSubCluster);      } catch (      YarnException e) {        String message="Unable to update the ApplicationId " + applicationId + " into the FederationStateStore";        SubClusterId subClusterIdInStateStore=federationFacade.getApplicationHomeSubCluster(applicationId);        if (subClusterId == subClusterIdInStateStore) {          LOG.info("Application " + applicationId + " already submitted on SubCluster "+ subClusterId);        } else {          routerMetrics.incrAppsFailedSubmitted();          RouterServerUtil.logAndThrowException(message,e);        }      }    }    ApplicationClientProtocol clientRMProxy=getClientRMProxyForSubCluster(subClusterId);    SubmitApplicationResponse response=null;    try {      response=clientRMProxy.submitApplication(request);
@Override public KillApplicationResponse forceKillApplication(KillApplicationRequest request) throws YarnException, IOException {  long startTime=clock.getTime();  if (request == null || request.getApplicationId() == null) {    routerMetrics.incrAppsFailedKilled();    RouterServerUtil.logAndThrowException("Missing forceKillApplication request or ApplicationId.",null);  }  ApplicationId applicationId=request.getApplicationId();  SubClusterId subClusterId=null;  try {    subClusterId=federationFacade.getApplicationHomeSubCluster(request.getApplicationId());  } catch (  YarnException e) {    routerMetrics.incrAppsFailedKilled();    RouterServerUtil.logAndThrowException("Application " + applicationId + " does not exist in FederationStateStore",e);  }  ApplicationClientProtocol clientRMProxy=getClientRMProxyForSubCluster(subClusterId);  KillApplicationResponse response=null;  try {
    RouterServerUtil.logAndThrowException("Missing forceKillApplication request or ApplicationId.",null);  }  ApplicationId applicationId=request.getApplicationId();  SubClusterId subClusterId=null;  try {    subClusterId=federationFacade.getApplicationHomeSubCluster(request.getApplicationId());  } catch (  YarnException e) {    routerMetrics.incrAppsFailedKilled();    RouterServerUtil.logAndThrowException("Application " + applicationId + " does not exist in FederationStateStore",e);  }  ApplicationClientProtocol clientRMProxy=getClientRMProxyForSubCluster(subClusterId);  KillApplicationResponse response=null;  try {    LOG.info("forceKillApplication " + applicationId + " on SubCluster "+ subClusterId);    response=clientRMProxy.forceKillApplication(request);  } catch (  Exception e) {    routerMetrics.incrAppsFailedKilled();
  SubClusterId subClusterId=null;  try {    subClusterId=federationFacade.getApplicationHomeSubCluster(request.getApplicationId());  } catch (  YarnException e) {    routerMetrics.incrAppsFailedKilled();    RouterServerUtil.logAndThrowException("Application " + applicationId + " does not exist in FederationStateStore",e);  }  ApplicationClientProtocol clientRMProxy=getClientRMProxyForSubCluster(subClusterId);  KillApplicationResponse response=null;  try {    LOG.info("forceKillApplication " + applicationId + " on SubCluster "+ subClusterId);    response=clientRMProxy.forceKillApplication(request);  } catch (  Exception e) {    routerMetrics.incrAppsFailedKilled();    LOG.error("Unable to kill the application report for " + request.getApplicationId() + "to SubCluster "+ subClusterId.getId(),e);    throw e;
  if (request == null || request.getApplicationId() == null) {    routerMetrics.incrAppsFailedRetrieved();    RouterServerUtil.logAndThrowException("Missing getApplicationReport request or applicationId information.",null);  }  SubClusterId subClusterId=null;  try {    subClusterId=federationFacade.getApplicationHomeSubCluster(request.getApplicationId());  } catch (  YarnException e) {    routerMetrics.incrAppsFailedRetrieved();    RouterServerUtil.logAndThrowException("Application " + request.getApplicationId() + " does not exist in FederationStateStore",e);  }  ApplicationClientProtocol clientRMProxy=getClientRMProxyForSubCluster(subClusterId);  GetApplicationReportResponse response=null;  try {    response=clientRMProxy.getApplicationReport(request);  } catch (  Exception e) {    routerMetrics.incrAppsFailedRetrieved();
  }  SubClusterId subClusterId=null;  try {    subClusterId=federationFacade.getApplicationHomeSubCluster(request.getApplicationId());  } catch (  YarnException e) {    routerMetrics.incrAppsFailedRetrieved();    RouterServerUtil.logAndThrowException("Application " + request.getApplicationId() + " does not exist in FederationStateStore",e);  }  ApplicationClientProtocol clientRMProxy=getClientRMProxyForSubCluster(subClusterId);  GetApplicationReportResponse response=null;  try {    response=clientRMProxy.getApplicationReport(request);  } catch (  Exception e) {    routerMetrics.incrAppsFailedRetrieved();    LOG.error("Unable to get the application report for " + request.getApplicationId() + "to SubCluster "+ subClusterId.getId(),e);    throw e;  }  if (response == null) {
        ApplicationClientProtocol protocol=getClientRMProxyForSubCluster(subClusterId);        Method method=ApplicationClientProtocol.class.getMethod(request.getMethodName(),request.getTypes());        return method.invoke(protocol,request.getParams());      }    });  }  Map<SubClusterId,R> results=new TreeMap<>();  try {    futures.addAll(executorService.invokeAll(callables));    for (int i=0; i < futures.size(); i++) {      SubClusterId subClusterId=clusterIds.get(i);      try {        Future<Object> future=futures.get(i);        Object result=future.get();        results.put(subClusterId,clazz.cast(result));      } catch (      ExecutionException ex) {        Throwable cause=ex.getCause();
private RequestInterceptorChainWrapper initializePipeline(String user){synchronized (this.userPipelineMap) {    if (this.userPipelineMap.containsKey(user)) {
private RequestInterceptorChainWrapper initializePipeline(String user){synchronized (this.userPipelineMap) {    if (this.userPipelineMap.containsKey(user)) {
      Map<SubClusterId,SubClusterInfo> subClustersInfo=facade.getSubClusters(true);      List<SubClusterInfo> subclusters=new ArrayList<>();      subclusters.addAll(subClustersInfo.values());      Comparator<? super SubClusterInfo> cmp=new Comparator<SubClusterInfo>(){        @Override public int compare(        SubClusterInfo o1,        SubClusterInfo o2){          return o1.getSubClusterId().compareTo(o2.getSubClusterId());        }      };      Collections.sort(subclusters,cmp);      for (      SubClusterInfo subcluster : subclusters) {        SubClusterId subClusterId=subcluster.getSubClusterId();        String webAppAddress=subcluster.getRMWebServiceAddress();        String capability=subcluster.getCapability();        ClusterMetricsInfo subClusterInfo=getClusterMetricsInfo(capability);        tbody.tr().td().a("//" + webAppAddress,subClusterId.toString()).__().td(Integer.toString(subClusterInfo.getAppsSubmitted())).td(Integer.toString(subClusterInfo.getAppsPending())).td(Integer.toString(subClusterInfo.getAppsRunning())).td(Integer.toString(subClusterInfo.getAppsFailed())).td(Integer.toString(subClusterInfo.getAppsKilled())).td(Integer.toString(subClusterInfo.getAppsCompleted())).td(Integer.toString(subClusterInfo.getContainersAllocated())).td(Integer.toString(subClusterInfo.getReservedContainers())).td(Integer.toString(subClusterInfo.getPendingContainers())).td(StringUtils.byteDesc(subClusterInfo.getAvailableMB() * BYTES_IN_MB)).td(StringUtils.byteDesc(subClusterInfo.getAllocatedMB() * BYTES_IN_MB)).td(StringUtils.byteDesc(subClusterInfo.getReservedMB() * BYTES_IN_MB)).td(StringUtils.byteDesc(subClusterInfo.getTotalMB() * BYTES_IN_MB)).td(Long.toString(subClusterInfo.getAvailableVirtualCores())).td(Long.toString(subClusterInfo.getAllocatedVirtualCores())).td(Long.toString(subClusterInfo.getReservedVirtualCores())).td(Long.toString(subClusterInfo.getTotalVirtualCores())).td(Integer.toString(subClusterInfo.getActiveNodes())).td(Integer.toString(subClusterInfo.getLostNodes())).td(Integer.toString(subClusterInfo.getDecommissionedNodes())).td(Integer.toString(subClusterInfo.getUnhealthyNodes())).td(Integer.toString(subClusterInfo.getRebootedNodes())).td(Integer.toString(subClusterInfo.getTotalNodes())).__();      }    } catch (    YarnException e) {
  ApplicationId applicationId=null;  try {    applicationId=ApplicationId.fromString(newApp.getApplicationId());  } catch (  IllegalArgumentException e) {    routerMetrics.incrAppsFailedSubmitted();    return Response.status(Status.BAD_REQUEST).entity(e.getLocalizedMessage()).build();  }  List<SubClusterId> blacklist=new ArrayList<SubClusterId>();  for (int i=0; i < numSubmitRetries; ++i) {    ApplicationSubmissionContext context=RMWebAppUtil.createAppSubmissionContext(newApp,this.getConf());    SubClusterId subClusterId=null;    try {      subClusterId=policyFacade.getHomeSubcluster(context,blacklist);    } catch (    YarnException e) {      routerMetrics.incrAppsFailedSubmitted();      return Response.status(Status.SERVICE_UNAVAILABLE).entity(e.getLocalizedMessage()).build();
    ApplicationHomeSubCluster appHomeSubCluster=ApplicationHomeSubCluster.newInstance(applicationId,subClusterId);    if (i == 0) {      try {        subClusterId=federationFacade.addApplicationHomeSubCluster(appHomeSubCluster);      } catch (      YarnException e) {        routerMetrics.incrAppsFailedSubmitted();        String errMsg="Unable to insert the ApplicationId " + applicationId + " into the FederationStateStore";        return Response.status(Status.SERVICE_UNAVAILABLE).entity(errMsg + " " + e.getLocalizedMessage()).build();      }    } else {      try {        federationFacade.updateApplicationHomeSubCluster(appHomeSubCluster);      } catch (      YarnException e) {        String errMsg="Unable to update the ApplicationId " + applicationId + " into the FederationStateStore";        SubClusterId subClusterIdInStateStore;        try {
        String errMsg="Unable to update the ApplicationId " + applicationId + " into the FederationStateStore";        SubClusterId subClusterIdInStateStore;        try {          subClusterIdInStateStore=federationFacade.getApplicationHomeSubCluster(applicationId);        } catch (        YarnException e1) {          routerMetrics.incrAppsFailedSubmitted();          return Response.status(Status.SERVICE_UNAVAILABLE).entity(e1.getLocalizedMessage()).build();        }        if (subClusterId == subClusterIdInStateStore) {          LOG.info("Application {} already submitted on SubCluster {}",applicationId,subClusterId);        } else {          routerMetrics.incrAppsFailedSubmitted();          return Response.status(Status.SERVICE_UNAVAILABLE).entity(errMsg).build();        }      }    }    SubClusterInfo subClusterInfo;    try {      subClusterInfo=federationFacade.getSubCluster(subClusterId);
        }        if (subClusterId == subClusterIdInStateStore) {          LOG.info("Application {} already submitted on SubCluster {}",applicationId,subClusterId);        } else {          routerMetrics.incrAppsFailedSubmitted();          return Response.status(Status.SERVICE_UNAVAILABLE).entity(errMsg).build();        }      }    }    SubClusterInfo subClusterInfo;    try {      subClusterInfo=federationFacade.getSubCluster(subClusterId);    } catch (    YarnException e) {      routerMetrics.incrAppsFailedSubmitted();      return Response.status(Status.SERVICE_UNAVAILABLE).entity(e.getLocalizedMessage()).build();    }    Response response=null;    try {      response=getOrCreateInterceptorForSubCluster(subClusterId,subClusterInfo.getRMWebServiceAddress()).submitApplication(newApp,hsr);    } catch (    Exception e) {
  Map<SubClusterId,SubClusterInfo> subClustersActive=null;  try {    subClustersActive=federationFacade.getSubClusters(true);  } catch (  YarnException e) {    routerMetrics.incrMultipleAppsFailedRetrieved();    return null;  }  CompletionService<AppsInfo> compSvc=new ExecutorCompletionService<>(this.threadpool);  final HttpServletRequest hsrCopy=clone(hsr);  for (  final SubClusterInfo info : subClustersActive.values()) {    compSvc.submit(new Callable<AppsInfo>(){      @Override public AppsInfo call(){        DefaultRequestInterceptorREST interceptor=getOrCreateInterceptorForSubCluster(info.getSubClusterId(),info.getRMWebServiceAddress());        AppsInfo rmApps=interceptor.getApps(hsrCopy,stateQuery,statesQuery,finalStatusQuery,userQuery,queueQuery,count,startedBegin,startedEnd,finishBegin,finishEnd,applicationTypes,applicationTags,name,unselectedFields);        if (rmApps == null) {          routerMetrics.incrMultipleAppsFailedRetrieved();
  final Map<SubClusterId,SubClusterInfo> subClustersActive;  try {    subClustersActive=getActiveSubclusters();  } catch (  Exception e) {    LOG.error("Cannot get nodes: {}",e.getMessage());    return new NodesInfo();  }  CompletionService<NodesInfo> compSvc=new ExecutorCompletionService<NodesInfo>(this.threadpool);  for (  final SubClusterInfo info : subClustersActive.values()) {    compSvc.submit(new Callable<NodesInfo>(){      @Override public NodesInfo call(){        DefaultRequestInterceptorREST interceptor=getOrCreateInterceptorForSubCluster(info.getSubClusterId(),info.getRMWebServiceAddress());        try {          NodesInfo nodesInfo=interceptor.getNodes(states);          return nodesInfo;        } catch (        Exception e) {
  final Map<SubClusterId,SubClusterInfo> subClustersActive;  try {    subClustersActive=getActiveSubclusters();  } catch (  Exception e) {    LOG.error(e.getLocalizedMessage());    return metrics;  }  CompletionService<ClusterMetricsInfo> compSvc=new ExecutorCompletionService<ClusterMetricsInfo>(this.threadpool);  for (  final SubClusterInfo info : subClustersActive.values()) {    compSvc.submit(new Callable<ClusterMetricsInfo>(){      @Override public ClusterMetricsInfo call(){        DefaultRequestInterceptorREST interceptor=getOrCreateInterceptorForSubCluster(info.getSubClusterId(),info.getRMWebServiceAddress());        try {          ClusterMetricsInfo metrics=interceptor.getClusterMetricsInfo();          return metrics;        } catch (        Exception e) {
  }  try {    return callerUGI.doAs(new PrivilegedExceptionAction<T>(){      @SuppressWarnings("unchecked") @Override public T run(){        Map<String,String[]> paramMap=null;        if (hsr != null) {          paramMap=hsr.getParameterMap();        } else         if (additionalParam != null) {          paramMap=additionalParam;        }        ClientResponse response=RouterWebServiceUtil.invokeRMWebService(webApp,targetPath,method,(hsr == null) ? null : hsr.getPathInfo(),paramMap,formParam,getMediaTypeFromHttpServletRequest(hsr,returnType),conf);        if (Response.class.equals(returnType)) {          return (T)RouterWebServiceUtil.clientResponseToResponse(response);        }        if (response.getStatus() == SC_OK) {          return response.getEntity(returnType);        }        if (response.getStatus() == SC_NO_CONTENT) {          try {
private RequestInterceptorChainWrapper initializePipeline(String user){synchronized (this.userPipelineMap) {    if (this.userPipelineMap.containsKey(user)) {
  super.setUpConfig();  interceptor=new TestableFederationClientInterceptor();  stateStore=new MemoryFederationStateStore();  stateStore.init(this.getConf());  FederationStateStoreFacade.getInstance().reinitialize(stateStore,getConf());  stateStoreUtil=new FederationStateStoreTestUtil(stateStore);  interceptor.setConf(this.getConf());  interceptor.init(user);  subClusters=new ArrayList<SubClusterId>();  try {    for (int i=0; i < NUM_SUBCLUSTER; i++) {      SubClusterId sc=SubClusterId.newInstance(Integer.toString(i));      stateStoreUtil.registerSubCluster(sc);      subClusters.add(sc);    }  } catch (  YarnException e) {
@Test public void testClientPipelineConcurrent() throws InterruptedException {  final String user="test1";class ClientTestThread extends Thread {    private ClientRequestInterceptor interceptor;    @Override public void run(){      try {        interceptor=pipeline();      } catch (      IOException|InterruptedException e) {        e.printStackTrace();      }    }    private ClientRequestInterceptor pipeline() throws IOException, InterruptedException {      return UserGroupInformation.createRemoteUser(user).doAs(new PrivilegedExceptionAction<ClientRequestInterceptor>(){        @Override public ClientRequestInterceptor run() throws Exception {          RequestInterceptorChainWrapper wrapper=getRouterClientRMService().getInterceptorChain();          ClientRequestInterceptor interceptor=wrapper.getRootInterceptor();          Assert.assertNotNull(interceptor);
@Test public void testRMAdminPipelineConcurrent() throws InterruptedException {  final String user="test1";class ClientTestThread extends Thread {    private RMAdminRequestInterceptor interceptor;    @Override public void run(){      try {        interceptor=pipeline();      } catch (      IOException|InterruptedException e) {        e.printStackTrace();      }    }    private RMAdminRequestInterceptor pipeline() throws IOException, InterruptedException {      return UserGroupInformation.createRemoteUser(user).doAs(new PrivilegedExceptionAction<RMAdminRequestInterceptor>(){        @Override public RMAdminRequestInterceptor run() throws Exception {          RequestInterceptorChainWrapper wrapper=getRouterRMAdminService().getInterceptorChain();          RMAdminRequestInterceptor interceptor=wrapper.getRootInterceptor();          Assert.assertNotNull(interceptor);
@Override public Response submitApplication(ApplicationSubmissionContextInfo newApp,HttpServletRequest hsr) throws AuthorizationException, IOException, InterruptedException {  validateRunning();  ApplicationId appId=ApplicationId.fromString(newApp.getApplicationId());
  super.setUpConfig();  interceptor=new TestableFederationInterceptorREST();  stateStore=new MemoryFederationStateStore();  stateStore.init(this.getConf());  FederationStateStoreFacade.getInstance().reinitialize(stateStore,this.getConf());  stateStoreUtil=new FederationStateStoreTestUtil(stateStore);  interceptor.setConf(this.getConf());  interceptor.init(user);  subClusters=new ArrayList<>();  try {    for (int i=0; i < NUM_SUBCLUSTER; i++) {      SubClusterId sc=SubClusterId.newInstance(Integer.toString(i));      stateStoreUtil.registerSubCluster(sc);      subClusters.add(sc);    }  } catch (  YarnException e) {
private void setUpClusterMetrics(ClusterMetricsInfo metrics,long seed){
@Test public void testWebPipelineConcurrent() throws InterruptedException {  final String user="test1";class ClientTestThread extends Thread {    private RESTRequestInterceptor interceptor;    @Override public void run(){      try {        interceptor=pipeline();      } catch (      IOException|InterruptedException e) {        e.printStackTrace();      }    }    private RESTRequestInterceptor pipeline() throws IOException, InterruptedException {      return UserGroupInformation.createRemoteUser(user).doAs(new PrivilegedExceptionAction<RESTRequestInterceptor>(){        @Override public RESTRequestInterceptor run() throws Exception {          RequestInterceptorChainWrapper wrapper=getInterceptorChain(user);          RESTRequestInterceptor interceptor=wrapper.getRootInterceptor();          Assert.assertNotNull(interceptor);
private void removeGlobalCleanerPidFile(){  try {    FileSystem fs=FileSystem.get(this.conf);    String root=conf.get(YarnConfiguration.SHARED_CACHE_ROOT,YarnConfiguration.DEFAULT_SHARED_CACHE_ROOT);    Path pidPath=new Path(root,GLOBAL_CLEANER_PID);    fs.delete(pidPath,false);
void process(){  metrics.reportCleaningStart();  try {    String pattern=SharedCacheUtil.getCacheEntryGlobPattern(nestedLevel);    FileStatus[] resources=fs.globStatus(new Path(root,pattern));    int numResources=resources == null ? 0 : resources.length;
    LOG.info("Processing " + numResources + " resources in the shared cache");    long beginMs=System.currentTimeMillis();    if (resources != null) {      for (      FileStatus resource : resources) {        if (Thread.currentThread().isInterrupted()) {          LOG.warn("The cleaner task was interrupted. Aborting.");          break;        }        if (resource.isDirectory()) {          processSingleResource(resource);        } else {          LOG.warn("Invalid file at path " + resource.getPath().toString() + " when a directory was expected");        }        if (sleepTime > 0) {          Thread.sleep(sleepTime);        }      }    }    long endMs=System.currentTimeMillis();    long durationMs=endMs - beginMs;
    if (resources != null) {      for (      FileStatus resource : resources) {        if (Thread.currentThread().isInterrupted()) {          LOG.warn("The cleaner task was interrupted. Aborting.");          break;        }        if (resource.isDirectory()) {          processSingleResource(resource);        } else {          LOG.warn("Invalid file at path " + resource.getPath().toString() + " when a directory was expected");        }        if (sleepTime > 0) {          Thread.sleep(sleepTime);        }      }    }    long endMs=System.currentTimeMillis();    long durationMs=endMs - beginMs;    LOG.info("Processed " + numResources + " resource(s) in "+ durationMs+ " ms.");  } catch (  IOException e1) {
void processSingleResource(FileStatus resource){  Path path=resource.getPath();  ResourceStatus resourceStatus=ResourceStatus.INIT;  if (path.toString().endsWith(RENAMED_SUFFIX)) {
void processSingleResource(FileStatus resource){  Path path=resource.getPath();  ResourceStatus resourceStatus=ResourceStatus.INIT;  if (path.toString().endsWith(RENAMED_SUFFIX)) {    LOG.info("Found a renamed directory that was left undeleted at " + path.toString() + ". Deleting.");    try {      if (fs.delete(path,true)) {        resourceStatus=ResourceStatus.DELETED;      }    } catch (    IOException e) {      LOG.error("Error while processing a shared cache resource: " + path,e);    }  } else {    String key=path.getName();    try {      store.cleanResourceReferences(key);    } catch (    YarnException e) {
      }    } catch (    IOException e) {      LOG.error("Error while processing a shared cache resource: " + path,e);    }  } else {    String key=path.getName();    try {      store.cleanResourceReferences(key);    } catch (    YarnException e) {      LOG.error("Exception thrown while removing dead appIds.",e);    }    if (store.isResourceEvictable(key,resource)) {      try {        if (store.removeResource(key)) {          boolean deleted=removeResourceFromCacheFileSystem(path);          if (deleted) {            resourceStatus=ResourceStatus.DELETED;          } else {
  } else {    String key=path.getName();    try {      store.cleanResourceReferences(key);    } catch (    YarnException e) {      LOG.error("Exception thrown while removing dead appIds.",e);    }    if (store.isResourceEvictable(key,resource)) {      try {        if (store.removeResource(key)) {          boolean deleted=removeResourceFromCacheFileSystem(path);          if (deleted) {            resourceStatus=ResourceStatus.DELETED;          } else {            LOG.error("Failed to remove path from the file system." + " Skipping this resource: " + path);            resourceStatus=ResourceStatus.ERROR;
private boolean removeResourceFromCacheFileSystem(Path path) throws IOException {  Path renamedPath=new Path(path.toString() + RENAMED_SUFFIX);  if (fs.rename(path,renamedPath)) {
private void bootstrap(Configuration conf) throws IOException {  Map<String,String> initialCachedResources=getInitialCachedResources(FileSystem.get(conf),conf);
  FileStatus[] entries=fs.globStatus(new Path(root,pattern));  int numEntries=entries == null ? 0 : entries.length;  LOG.info("Found " + numEntries + " files: processing for one resource per "+ "key");  Map<String,String> initialCachedEntries=new HashMap<String,String>();  if (entries != null) {    for (    FileStatus entry : entries) {      Path file=entry.getPath();      String fileName=file.getName();      if (entry.isFile()) {        Path parent=file.getParent();        if (parent != null) {          String key=parent.getName();          if (initialCachedEntries.containsKey(key)) {            LOG.warn("Key " + key + " is already mapped to file "+ initialCachedEntries.get(key)+ "; file "+ fileName+ " will not be added");          } else {
@Override protected void serviceStart() throws Exception {  SCMWebApp scmWebApp=new SCMWebApp(scm);  this.webApp=WebApps.$for("sharedcache").at(bindAddress).start(scmWebApp);
private void waitForContainerToFinishOnNM(ContainerId containerId) throws InterruptedException {  Context nmContext=yarnCluster.getNodeManager(0).getNMContext();  final int timeout=4 * 60 * 1000;  Container waitContainer=nmContext.getContainers().get(containerId);  if (waitContainer != null) {    try {
private void testDirsFailures(boolean localORLogDirs) throws IOException {  String dirType=localORLogDirs ? "local" : "log";  String dirsProperty=localORLogDirs ? YarnConfiguration.NM_LOCAL_DIRS : YarnConfiguration.NM_LOG_DIRS;  Configuration conf=new Configuration();  conf.setLong(YarnConfiguration.NM_DISK_HEALTH_CHECK_INTERVAL_MS,TOO_HIGH_DISK_HEALTH_CHECK_INTERVAL);  conf.setFloat(YarnConfiguration.NM_MIN_HEALTHY_DISKS_FRACTION,0.60F);  if (yarnCluster != null) {    yarnCluster.stop();    FileUtil.fullyDelete(localFSDirBase);    localFSDirBase.mkdirs();  }  LOG.info("Starting up YARN cluster");  yarnCluster=new MiniYARNCluster(TestDiskFailures.class.getName(),1,numLocalDirs,numLogDirs);  yarnCluster.init(conf);  yarnCluster.start();  NodeManager nm=yarnCluster.getNodeManager(0);
private void verifyDisksHealth(boolean localORLogDirs,String expectedDirs,boolean isHealthy){  dirsHandler.checkDirs();  List<String> list=localORLogDirs ? dirsHandler.getLocalDirs() : dirsHandler.getLogDirs();  String seenDirs=StringUtils.join(",",list);
private void verifyDisksHealth(boolean localORLogDirs,String expectedDirs,boolean isHealthy){  dirsHandler.checkDirs();  List<String> list=localORLogDirs ? dirsHandler.getLocalDirs() : dirsHandler.getLogDirs();  String seenDirs=StringUtils.join(",",list);  LOG.info("ExpectedDirs=" + expectedDirs);
@Override protected void serviceInit(Configuration conf) throws Exception {  metrics=EntityGroupFSTimelineStoreMetrics.create();  summaryStore=createSummaryStore();  addService(summaryStore);  long logRetainSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);  logRetainMillis=logRetainSecs * 1000;
@Override protected void serviceInit(Configuration conf) throws Exception {  metrics=EntityGroupFSTimelineStoreMetrics.create();  summaryStore=createSummaryStore();  addService(summaryStore);  long logRetainSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);  logRetainMillis=logRetainSecs * 1000;  LOG.info("Cleaner set to delete logs older than {} seconds",logRetainSecs);  long unknownActiveSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT);  unknownActiveMillis=unknownActiveSecs * 1000;
@Override protected void serviceInit(Configuration conf) throws Exception {  metrics=EntityGroupFSTimelineStoreMetrics.create();  summaryStore=createSummaryStore();  addService(summaryStore);  long logRetainSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);  logRetainMillis=logRetainSecs * 1000;  LOG.info("Cleaner set to delete logs older than {} seconds",logRetainSecs);  long unknownActiveSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT);  unknownActiveMillis=unknownActiveSecs * 1000;  LOG.info("Unknown apps will be treated as complete after {} seconds",unknownActiveSecs);  appCacheMaxSize=conf.getInt(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT);
  metrics=EntityGroupFSTimelineStoreMetrics.create();  summaryStore=createSummaryStore();  addService(summaryStore);  long logRetainSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);  logRetainMillis=logRetainSecs * 1000;  LOG.info("Cleaner set to delete logs older than {} seconds",logRetainSecs);  long unknownActiveSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT);  unknownActiveMillis=unknownActiveSecs * 1000;  LOG.info("Unknown apps will be treated as complete after {} seconds",unknownActiveSecs);  appCacheMaxSize=conf.getInt(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT);  LOG.info("Application cache size is {}",appCacheMaxSize);  cachedLogs=Collections.synchronizedMap(new LinkedHashMap<TimelineEntityGroupId,EntityCacheItem>(appCacheMaxSize + 1,0.75f,true){    @Override protected boolean removeEldestEntry(    Map.Entry<TimelineEntityGroupId,EntityCacheItem> eldest){      if (super.size() > appCacheMaxSize) {        TimelineEntityGroupId groupId=eldest.getKey();
  addService(summaryStore);  long logRetainSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT);  logRetainMillis=logRetainSecs * 1000;  LOG.info("Cleaner set to delete logs older than {} seconds",logRetainSecs);  long unknownActiveSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT);  unknownActiveMillis=unknownActiveSecs * 1000;  LOG.info("Unknown apps will be treated as complete after {} seconds",unknownActiveSecs);  appCacheMaxSize=conf.getInt(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT);  LOG.info("Application cache size is {}",appCacheMaxSize);  cachedLogs=Collections.synchronizedMap(new LinkedHashMap<TimelineEntityGroupId,EntityCacheItem>(appCacheMaxSize + 1,0.75f,true){    @Override protected boolean removeEldestEntry(    Map.Entry<TimelineEntityGroupId,EntityCacheItem> eldest){      if (super.size() > appCacheMaxSize) {        TimelineEntityGroupId groupId=eldest.getKey();        LOG.debug("Evicting {} due to space limitations",groupId);        EntityCacheItem cacheItem=eldest.getValue();
  Collection<String> pluginNames=conf.getTrimmedStringCollection(YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_CLASSES);  String pluginClasspath=conf.getTrimmed(YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_CLASSPATH);  String[] systemClasses=conf.getTrimmedStrings(YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_SYSTEM_CLASSES);  List<TimelineEntityGroupPlugin> pluginList=new LinkedList<TimelineEntityGroupPlugin>();  ClassLoader customClassLoader=null;  if (pluginClasspath != null && pluginClasspath.length() > 0) {    try {      customClassLoader=createPluginClassLoader(pluginClasspath,systemClasses);    } catch (    IOException ioe) {      LOG.warn("Error loading classloader",ioe);    }  }  for (  final String name : pluginNames) {    LOG.debug("Trying to load plugin class {}",name);    TimelineEntityGroupPlugin cacheIdPlugin=null;    try {      if (customClassLoader != null) {
  summaryTdm.init(conf);  addService(summaryTdm);  super.serviceStart();  if (!fs.exists(activeRootPath)) {    fs.mkdirs(activeRootPath);    fs.setPermission(activeRootPath,ACTIVE_DIR_PERMISSION);  }  if (!fs.exists(doneRootPath)) {    fs.mkdirs(doneRootPath);    fs.setPermission(doneRootPath,DONE_DIR_PERMISSION);  }  objMapper=new ObjectMapper();  objMapper.setAnnotationIntrospector(new JaxbAnnotationIntrospector(TypeFactory.defaultInstance()));  jsonFactory=new MappingJsonFactory(objMapper);  final long scanIntervalSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS_DEFAULT);  final long cleanerIntervalSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS_DEFAULT);  final int numThreads=conf.getInt(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS_DEFAULT);
  addService(summaryTdm);  super.serviceStart();  if (!fs.exists(activeRootPath)) {    fs.mkdirs(activeRootPath);    fs.setPermission(activeRootPath,ACTIVE_DIR_PERMISSION);  }  if (!fs.exists(doneRootPath)) {    fs.mkdirs(doneRootPath);    fs.setPermission(doneRootPath,DONE_DIR_PERMISSION);  }  objMapper=new ObjectMapper();  objMapper.setAnnotationIntrospector(new JaxbAnnotationIntrospector(TypeFactory.defaultInstance()));  jsonFactory=new MappingJsonFactory(objMapper);  final long scanIntervalSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS_DEFAULT);  final long cleanerIntervalSecs=conf.getLong(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS_DEFAULT);  final int numThreads=conf.getInt(YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS,YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS_DEFAULT);  LOG.info("Scanning active directory {} every {} seconds",activeRootPath,scanIntervalSecs);
int scanActiveLogs(Path dir) throws IOException {  RemoteIterator<FileStatus> iter=list(dir);  int logsToScanCount=0;  while (iter.hasNext()) {    FileStatus stat=iter.next();    String name=stat.getPath().getName();    ApplicationId appId=parseApplicationId(name);    if (appId != null) {
private AppLogs getAndSetAppLogs(ApplicationId applicationId) throws IOException {
    if (fs.exists(appDirPath)) {      appState=AppState.COMPLETED;    } else {      appDirPath=getActiveAppPath(applicationId);      if (fs.exists(appDirPath)) {        appState=AppState.ACTIVE;      } else {        RemoteIterator<FileStatus> iter=list(activeRootPath);        while (iter.hasNext()) {          Path child=new Path(iter.next().getPath().getName(),applicationId.toString());          appDirPath=new Path(activeRootPath,child);          if (fs.exists(appDirPath)) {            appState=AppState.ACTIVE;            break;          }        }      }    }    if (appState != AppState.UNKNOWN) {
private void deleteDir(Path path){  try {
private static boolean shouldCleanAppLogDir(Path appLogPath,long now,FileSystem fs,long logRetainMillis) throws IOException {  RemoteIterator<FileStatus> iter=fs.listStatusIterator(appLogPath);  while (iter.hasNext()) {    FileStatus stat=iter.next();    if (now - stat.getModificationTime() <= logRetainMillis) {
private List<TimelineStore> getTimelineStoresFromCacheIds(Set<TimelineEntityGroupId> groupIds,String entityType,List<EntityCacheItem> cacheItems) throws IOException {  List<TimelineStore> stores=new LinkedList<TimelineStore>();  for (  TimelineEntityGroupId groupId : groupIds) {    TimelineStore storeForId=getCachedStore(groupId,cacheItems);    if (storeForId != null) {
protected List<TimelineStore> getTimelineStoresForRead(String entityId,String entityType,List<EntityCacheItem> cacheItems) throws IOException {  Set<TimelineEntityGroupId> groupIds=new HashSet<TimelineEntityGroupId>();  for (  TimelineEntityGroupPlugin cacheIdPlugin : cacheIdPlugins) {
private List<TimelineStore> getTimelineStoresForRead(String entityType,NameValuePair primaryFilter,Collection<NameValuePair> secondaryFilters,List<EntityCacheItem> cacheItems) throws IOException {  Set<TimelineEntityGroupId> groupIds=new HashSet<TimelineEntityGroupId>();  for (  TimelineEntityGroupPlugin cacheIdPlugin : cacheIdPlugins) {    Set<TimelineEntityGroupId> idsFromPlugin=cacheIdPlugin.getTimelineEntityGroupId(entityType,primaryFilter,secondaryFilters);    if (idsFromPlugin != null) {
private TimelineStore getCachedStore(TimelineEntityGroupId groupId,List<EntityCacheItem> cacheItems) throws IOException {  EntityCacheItem cacheItem;synchronized (this.cachedLogs) {    cacheItem=this.cachedLogs.get(groupId);    if (cacheItem == null) {
private TimelineStore getCachedStore(TimelineEntityGroupId groupId,List<EntityCacheItem> cacheItems) throws IOException {  EntityCacheItem cacheItem;synchronized (this.cachedLogs) {    cacheItem=this.cachedLogs.get(groupId);    if (cacheItem == null) {      LOG.debug("Set up new cache item for id {}",groupId);      cacheItem=new EntityCacheItem(groupId,getConfig());      AppLogs appLogs=getAndSetAppLogs(groupId.getApplicationId());      if (appLogs != null) {
synchronized (this.cachedLogs) {    cacheItem=this.cachedLogs.get(groupId);    if (cacheItem == null) {      LOG.debug("Set up new cache item for id {}",groupId);      cacheItem=new EntityCacheItem(groupId,getConfig());      AppLogs appLogs=getAndSetAppLogs(groupId.getApplicationId());      if (appLogs != null) {        LOG.debug("Set applogs {} for group id {}",appLogs,groupId);        cacheItem.setAppLogs(appLogs);        this.cachedLogs.put(groupId,cacheItem);      } else {        LOG.warn("AppLogs for groupId {} is set to null!",groupId);      }    }  }  TimelineStore store=null;  if (cacheItem.getAppLogs() != null) {    AppLogs appLogs=cacheItem.getAppLogs();
@Override public TimelineEntities getEntities(String entityType,Long limit,Long windowStart,Long windowEnd,String fromId,Long fromTs,NameValuePair primaryFilter,Collection<NameValuePair> secondaryFilters,EnumSet<Field> fieldsToRetrieve,CheckAcl checkAcl) throws IOException {
@Override public TimelineEntities getEntities(String entityType,Long limit,Long windowStart,Long windowEnd,String fromId,Long fromTs,NameValuePair primaryFilter,Collection<NameValuePair> secondaryFilters,EnumSet<Field> fieldsToRetrieve,CheckAcl checkAcl) throws IOException {  LOG.debug("getEntities type={} primary={}",entityType,primaryFilter);  List<EntityCacheItem> relatedCacheItems=new ArrayList<>();  List<TimelineStore> stores=getTimelineStoresForRead(entityType,primaryFilter,secondaryFilters,relatedCacheItems);  TimelineEntities returnEntities=new TimelineEntities();  for (  TimelineStore store : stores) {
@Override public TimelineEntity getEntity(String entityId,String entityType,EnumSet<Field> fieldsToRetrieve) throws IOException {
@Override public TimelineEntity getEntity(String entityId,String entityType,EnumSet<Field> fieldsToRetrieve) throws IOException {  LOG.debug("getEntity type={} id={}",entityType,entityId);  List<EntityCacheItem> relatedCacheItems=new ArrayList<>();  List<TimelineStore> stores=getTimelineStoresForRead(entityId,entityType,relatedCacheItems);  for (  TimelineStore store : stores) {
@Override public TimelineEvents getEntityTimelines(String entityType,SortedSet<String> entityIds,Long limit,Long windowStart,Long windowEnd,Set<String> eventTypes) throws IOException {
public long parseForStore(TimelineDataManager tdm,Path appDirPath,boolean appCompleted,JsonFactory jsonFactory,ObjectMapper objMapper,FileSystem fs) throws IOException {
public long parseForStore(TimelineDataManager tdm,Path appDirPath,boolean appCompleted,JsonFactory jsonFactory,ObjectMapper objMapper,FileSystem fs) throws IOException {  LOG.debug("Parsing for log dir {} on attempt {}",appDirPath,attemptDirName);  Path logPath=getPath(appDirPath);  FileStatus status=fs.getFileStatus(logPath);  long numParsed=0;  if (status != null) {    long startTime=Time.monotonicNow();    try {
public long parseForStore(TimelineDataManager tdm,Path appDirPath,boolean appCompleted,JsonFactory jsonFactory,ObjectMapper objMapper,FileSystem fs) throws IOException {  LOG.debug("Parsing for log dir {} on attempt {}",appDirPath,attemptDirName);  Path logPath=getPath(appDirPath);  FileStatus status=fs.getFileStatus(logPath);  long numParsed=0;  if (status != null) {    long startTime=Time.monotonicNow();    try {      LOG.debug("Parsing {} at offset {}",logPath,offset);      long count=parsePath(tdm,logPath,appCompleted,jsonFactory,objMapper,fs);
  ArrayList<TimelineEntity> entityList=new ArrayList<TimelineEntity>(1);  long bytesParsed;  long bytesParsedLastBatch=0;  boolean postError=false;  try {    MappingIterator<TimelineEntity> iter=objMapper.readValues(parser,TimelineEntity.class);    while (iter.hasNext()) {      TimelineEntity entity=iter.next();      String etype=entity.getEntityType();      String eid=entity.getEntityId();      LOG.trace("Read entity {}",etype);      ++count;      bytesParsed=parser.getCurrentLocation().getCharOffset() + 1;      LOG.trace("Parser now at offset {}",bytesParsed);      try {
@Override public void createTimelineSchema(String[] args){  try {    Configuration conf=new YarnConfiguration();
@Override public void serviceInit(Configuration conf) throws Exception {  DocumentStoreVendor storeType=DocumentStoreUtils.getStoreVendor(conf);
@Override public void serviceInit(Configuration conf) throws Exception {  storeType=DocumentStoreUtils.getStoreVendor(conf);
@Override public TimelineWriteResponse write(TimelineCollectorContext context,TimelineEntities data,UserGroupInformation callerUgi){
public TimelineEntityDocument readDocument(TimelineReaderContext context) throws IOException {
public List<TimelineEntityDocument> readDocuments(TimelineReaderContext context,long documentsSize) throws IOException {  List<TimelineEntityDocument> entityDocs=new ArrayList<>();
public Set<String> fetchEntityTypes(TimelineReaderContext context){
@Override public Set<String> fetchEntityTypes(String collectionName,TimelineReaderContext context){  StringBuilder queryStrBuilder=new StringBuilder();  queryStrBuilder.append(String.format(SELECT_DISTINCT_TYPES_FROM_COLLECTION,collectionName));  String sqlQuery=addPredicates(context,collectionName,queryStrBuilder);
private List<TimelineDoc> queryDocuments(String collectionName,TimelineReaderContext context,final Class<TimelineDoc> docClass,final long maxDocumentsSize){  final String sqlQuery=buildQueryWithPredicates(context,collectionName,maxDocumentsSize);
  if (!DocumentStoreUtils.isNullOrEmpty(context.getFlowName())) {    hasPredicate=true;    queryStrBuilder.append(AND_OPERATOR).append(String.format(CONTAINS_FUNC_FOR_ID,context.getFlowName()));  }  if (!DocumentStoreUtils.isNullOrEmpty(context.getAppId())) {    hasPredicate=true;    queryStrBuilder.append(AND_OPERATOR).append(String.format(CONTAINS_FUNC_FOR_ID,context.getAppId()));  }  if (!DocumentStoreUtils.isNullOrEmpty(context.getEntityId())) {    hasPredicate=true;    queryStrBuilder.append(AND_OPERATOR).append(String.format(CONTAINS_FUNC_FOR_ID,context.getEntityId()));  }  if (context.getFlowRunId() != null) {    hasPredicate=true;    queryStrBuilder.append(AND_OPERATOR).append(String.format(CONTAINS_FUNC_FOR_ID,context.getFlowRunId()));  }  if (!DocumentStoreUtils.isNullOrEmpty(context.getEntityType())) {    hasPredicate=true;    queryStrBuilder.append(AND_OPERATOR).append(String.format(CONTAINS_FUNC_FOR_TYPE,context.getEntityType()));
@Override public void createDatabase(){  Observable<ResourceResponse<Database>> databaseReadObs=client.readDatabase(String.format(DATABASE_LINK,databaseName),null);  Observable<ResourceResponse<Database>> databaseExistenceObs=databaseReadObs.doOnNext(databaseResourceResponse -> LOG.info("Database {} already exists.",databaseName)).onErrorResumeNext(throwable -> {    if (throwable instanceof DocumentClientException) {      DocumentClientException de=(DocumentClientException)throwable;      if (de.getStatusCode() == 404) {
@Override public void createCollection(final String collectionName){
@Override public void createCollection(final String collectionName){  LOG.info("Creating Timeline Collection : {} for Database : {}",collectionName,databaseName);  client.queryCollections(String.format(DATABASE_LINK,databaseName),new SqlQuerySpec(QUERY_COLLECTION_IF_EXISTS,new SqlParameterCollection(new SqlParameter(ID,collectionName))),null).single().flatMap((Func1<FeedResponse<DocumentCollection>,Observable<?>>)page -> {    if (page.getResults().isEmpty()) {      DocumentCollection collection=new DocumentCollection();      collection.setId(collectionName);
@Override public void writeDocument(final TimelineDoc timelineDoc,final CollectionType collectionType){
  o.setArgName("subApplicationTableName");  o.setRequired(false);  options.addOption(o);  o=new Option(SUB_APP_METRICS_TTL_OPTION_SHORT,"subApplicationMetricsTTL",true,"TTL for metrics column family");  o.setArgName("subApplicationMetricsTTL");  o.setRequired(false);  options.addOption(o);  o=new Option(SKIP_EXISTING_TABLE_OPTION_SHORT,"skipExistingTable",false,"skip existing Hbase tables and continue to create new tables");  o.setRequired(false);  options.addOption(o);  CommandLineParser parser=new PosixParser();  CommandLine commandLine=null;  try {    commandLine=parser.parse(options,args);  } catch (  Exception e) {
@Override protected void serviceInit(Configuration conf) throws Exception {  super.serviceInit(conf);  Configuration hbaseConf=HBaseTimelineStorageUtils.getTimelineServiceHBaseConf(conf);  conn=ConnectionFactory.createConnection(hbaseConf);  entityTable=new EntityTableRW().getTableMutator(hbaseConf,conn);  appToFlowTable=new AppToFlowTableRW().getTableMutator(hbaseConf,conn);  applicationTable=new ApplicationTableRW().getTableMutator(hbaseConf,conn);  flowRunTable=new FlowRunTableRW().getTableMutator(hbaseConf,conn);  flowActivityTable=new FlowActivityTableRW().getTableMutator(hbaseConf,conn);  subApplicationTable=new SubApplicationTableRW().getTableMutator(hbaseConf,conn);  domainTable=new DomainTableRW().getTableMutator(hbaseConf,conn);  UserGroupInformation ugi=UserGroupInformation.isSecurityEnabled() ? UserGroupInformation.getLoginUser() : UserGroupInformation.getCurrentUser();  storageMonitor=new HBaseStorageMonitor(conf);
        K converterColumnKey=null;        if (columnPrefixBytes == null) {          LOG.debug("null prefix was specified; returning all columns");          try {            converterColumnKey=keyConverter.decode(entry.getKey());          } catch (          IllegalArgumentException iae) {            LOG.error("Illegal column found, skipping this column.",iae);            continue;          }        } else {          byte[][] columnNameParts=Separator.QUALIFIERS.split(entry.getKey(),2);          byte[] actualColumnPrefixBytes=columnNameParts[0];          if (Bytes.equals(columnPrefixBytes,actualColumnPrefixBytes) && columnNameParts.length == 2) {            try {              converterColumnKey=keyConverter.decode(columnNameParts[1]);            } catch (            IllegalArgumentException iae) {
        K converterColumnKey=null;        if (columnPrefixBytes == null) {          try {            converterColumnKey=keyConverter.decode(columnKey);          } catch (          IllegalArgumentException iae) {            LOG.error("Illegal column found, skipping this column.",iae);            continue;          }        } else {          byte[][] columnNameParts=Separator.QUALIFIERS.split(columnKey,2);          if (columnNameParts.length > 0) {            byte[] actualColumnPrefixBytes=columnNameParts[0];            if (Bytes.equals(columnPrefixBytes,actualColumnPrefixBytes) && columnNameParts.length == 2) {              try {                converterColumnKey=keyConverter.decode(columnNameParts[1]);              } catch (              IllegalArgumentException iae) {
public void createTable(Admin admin,Configuration hbaseConf) throws IOException {  TableName table=getTableName(hbaseConf);  if (admin.tableExists(table)) {    throw new IOException("Table " + table.getNameAsString() + " already exists.");  }  HTableDescriptor flowRunTableDescp=new HTableDescriptor(table);  HColumnDescriptor infoCF=new HColumnDescriptor(FlowRunColumnFamily.INFO.getBytes());  infoCF.setBloomFilterType(BloomType.ROWCOL);  flowRunTableDescp.addFamily(infoCF);  infoCF.setMinVersions(1);  infoCF.setMaxVersions(DEFAULT_METRICS_MAX_VERSIONS);  String coprocessorJarPathStr=hbaseConf.get(YarnConfiguration.FLOW_RUN_COPROCESSOR_JAR_HDFS_LOCATION,YarnConfiguration.DEFAULT_HDFS_LOCATION_FLOW_RUN_COPROCESSOR_JAR);  Path coprocessorJarPath=new Path(coprocessorJarPathStr);  LOG.info("CoprocessorJarPath=" + coprocessorJarPath.toString());  flowRunTableDescp.addCoprocessor("org.apache.hadoop.yarn.server.timelineservice.storage." + "flow.FlowRunCoprocessor",coprocessorJarPath,Coprocessor.PRIORITY_USER,null);  admin.createTable(flowRunTableDescp);
public Set<String> readEntityTypes(Configuration hbaseConf,Connection conn) throws IOException {  validateParams();  augmentParams(hbaseConf,conn);  Set<String> types=new TreeSet<>();  TimelineReaderContext context=getContext();  EntityRowKeyPrefix prefix=new EntityRowKeyPrefix(context.getClusterId(),context.getUserId(),context.getFlowName(),context.getFlowRunId(),context.getAppId());  byte[] currRowKey=prefix.getRowKeyPrefix();  byte[] nextRowKey=prefix.getRowKeyPrefix();  nextRowKey[nextRowKey.length - 1]++;  FilterList typeFilterList=new FilterList();  typeFilterList.addFilter(new FirstKeyOnlyFilter());  typeFilterList.addFilter(new KeyOnlyFilter());  typeFilterList.addFilter(new PageFilter(1));
  typeFilterList.addFilter(new FirstKeyOnlyFilter());  typeFilterList.addFilter(new KeyOnlyFilter());  typeFilterList.addFilter(new PageFilter(1));  LOG.debug("FilterList created for scan is - {}",typeFilterList);  int counter=0;  while (true) {    try (ResultScanner results=getResult(hbaseConf,conn,typeFilterList,currRowKey,nextRowKey)){      TimelineEntity entity=parseEntityForType(results.next());      if (entity == null) {        break;      }      ++counter;      if (!types.add(entity.getType())) {        LOG.warn("Failed to add type " + entity.getType() + " to the result set because there is a duplicated copy. ");      }      String currType=entity.getType();      if (LOG.isDebugEnabled()) {
  typeFilterList.addFilter(new KeyOnlyFilter());  typeFilterList.addFilter(new PageFilter(1));  LOG.debug("FilterList created for scan is - {}",typeFilterList);  int counter=0;  while (true) {    try (ResultScanner results=getResult(hbaseConf,conn,typeFilterList,currRowKey,nextRowKey)){      TimelineEntity entity=parseEntityForType(results.next());      if (entity == null) {        break;      }      ++counter;      if (!types.add(entity.getType())) {        LOG.warn("Failed to add type " + entity.getType() + " to the result set because there is a duplicated copy. ");      }      String currType=entity.getType();      if (LOG.isDebugEnabled()) {        LOG.debug("Current row key: " + Arrays.toString(currRowKey));
public TimelineEntity readEntity(Configuration hbaseConf,Connection conn) throws IOException {  validateParams();  augmentParams(hbaseConf,conn);  FilterList filterList=constructFilterListBasedOnFields(new HashSet<>(0));  if (filterList != null) {
public Set<TimelineEntity> readEntities(Configuration hbaseConf,Connection conn) throws IOException {  validateParams();  augmentParams(hbaseConf,conn);  Set<TimelineEntity> entities=new LinkedHashSet<>();  FilterList filterList=createFilterList();  if (filterList != null) {
@Override public InternalScanner preFlush(ObserverContext<RegionCoprocessorEnvironment> c,Store store,InternalScanner scanner) throws IOException {  if (LOG.isDebugEnabled()) {    if (store != null) {
@Override public void postFlush(ObserverContext<RegionCoprocessorEnvironment> c,Store store,StoreFile resultFile){  if (LOG.isDebugEnabled()) {    if (store != null) {
@Override public InternalScanner preCompact(ObserverContext<RegionCoprocessorEnvironment> e,Store store,InternalScanner scanner,ScanType scanType,CompactionRequest request) throws IOException {  FlowScannerOperation requestOp=FlowScannerOperation.MINOR_COMPACTION;  if (request != null) {    requestOp=(request.isMajor() ? FlowScannerOperation.MAJOR_COMPACTION : FlowScannerOperation.MINOR_COMPACTION);
    byte[] currentColumnQualifier=CellUtil.cloneQualifier(cell);    if (previousColumnQualifier == null) {      previousColumnQualifier=currentColumnQualifier;    }    converter=getValueConverter(currentColumnQualifier);    if (comp.compare(previousColumnQualifier,currentColumnQualifier) != 0) {      addedCnt+=emitCells(cells,currentColumnCells,currentAggOp,converter,currentTimestamp);      resetState(currentColumnCells,alreadySeenAggDim);      previousColumnQualifier=currentColumnQualifier;      currentAggOp=getCurrentAggOp(cell);      converter=getValueConverter(currentColumnQualifier);    }    collectCells(currentColumnCells,currentAggOp,cell,alreadySeenAggDim,converter,scannerContext);    nextCell(scannerContext);  }  if ((!currentColumnCells.isEmpty()) && ((limit <= 0 || addedCnt < limit))) {    addedCnt+=emitCells(cells,currentColumnCells,currentAggOp,converter,currentTimestamp);    if (LOG.isDebugEnabled()) {
        sum=converter.add(sum,currentValue);        summationDone=true;        if (LOG.isTraceEnabled()) {          LOG.trace("MAJOR COMPACTION loop sum= " + sum + " discarding now: "+ " qualifier="+ Bytes.toString(CellUtil.cloneQualifier(cell))+ " value="+ converter.decodeValue(CellUtil.cloneValue(cell))+ " timestamp="+ cell.getTimestamp()+ " "+ this.action);        }      } else {        finalCells.add(cell);      }    }  }  if (summationDone) {    Cell anyCell=currentColumnCells.first();    List<Tag> tags=new ArrayList<Tag>();    Tag t=HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    t=HBaseTimelineServerUtils.createTag(AggregationCompactionDimension.APPLICATION_ID.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    byte[] tagByteArray=HBaseTimelineServerUtils.convertTagListToByteArray(tags);    Cell sumCell=HBaseTimelineServerUtils.createNewCell(CellUtil.cloneRow(anyCell),CellUtil.cloneFamily(anyCell),CellUtil.cloneQualifier(anyCell),TimestampGenerator.getSupplementedTimestamp(System.currentTimeMillis(),FLOW_APP_ID),converter.encodeValue(sum),tagByteArray);
        if (LOG.isTraceEnabled()) {          LOG.trace("MAJOR COMPACTION loop sum= " + sum + " discarding now: "+ " qualifier="+ Bytes.toString(CellUtil.cloneQualifier(cell))+ " value="+ converter.decodeValue(CellUtil.cloneValue(cell))+ " timestamp="+ cell.getTimestamp()+ " "+ this.action);        }      } else {        finalCells.add(cell);      }    }  }  if (summationDone) {    Cell anyCell=currentColumnCells.first();    List<Tag> tags=new ArrayList<Tag>();    Tag t=HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    t=HBaseTimelineServerUtils.createTag(AggregationCompactionDimension.APPLICATION_ID.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    byte[] tagByteArray=HBaseTimelineServerUtils.convertTagListToByteArray(tags);    Cell sumCell=HBaseTimelineServerUtils.createNewCell(CellUtil.cloneRow(anyCell),CellUtil.cloneFamily(anyCell),CellUtil.cloneQualifier(anyCell),TimestampGenerator.getSupplementedTimestamp(System.currentTimeMillis(),FLOW_APP_ID),converter.encodeValue(sum),tagByteArray);    finalCells.add(sumCell);    if (LOG.isTraceEnabled()) {
    return currentCell;  }  try {    Number previouslyChosenCellValue=(Number)converter.decodeValue(CellUtil.cloneValue(previouslyChosenCell));    Number currentCellValue=(Number)converter.decodeValue(CellUtil.cloneValue(currentCell));switch (currentAggOp) {case GLOBAL_MIN:      if (converter.compare(currentCellValue,previouslyChosenCellValue) < 0) {        return currentCell;      } else {        return previouslyChosenCell;      }case GLOBAL_MAX:    if (converter.compare(currentCellValue,previouslyChosenCellValue) > 0) {      return currentCell;    } else {      return previouslyChosenCell;    }default:  return currentCell;}} catch (IllegalArgumentException iae) {
@Override public InternalScanner preFlush(ObserverContext<RegionCoprocessorEnvironment> c,Store store,InternalScanner scanner,FlushLifeCycleTracker cycleTracker) throws IOException {  if (LOG.isDebugEnabled()) {    if (store != null) {
@Override public void postFlush(ObserverContext<RegionCoprocessorEnvironment> c,Store store,StoreFile resultFile,FlushLifeCycleTracker tracker){  if (LOG.isDebugEnabled()) {    if (store != null) {
@Override public InternalScanner preCompact(ObserverContext<RegionCoprocessorEnvironment> e,Store store,InternalScanner scanner,ScanType scanType,CompactionLifeCycleTracker tracker,CompactionRequest request) throws IOException {  FlowScannerOperation requestOp=FlowScannerOperation.MINOR_COMPACTION;  if (request != null) {    requestOp=(request.isMajor() ? FlowScannerOperation.MAJOR_COMPACTION : FlowScannerOperation.MINOR_COMPACTION);
    byte[] currentColumnQualifier=CellUtil.cloneQualifier(cell);    if (previousColumnQualifier == null) {      previousColumnQualifier=currentColumnQualifier;    }    converter=getValueConverter(currentColumnQualifier);    if (comp.compare(previousColumnQualifier,currentColumnQualifier) != 0) {      addedCnt+=emitCells(cells,currentColumnCells,currentAggOp,converter,currentTimestamp);      resetState(currentColumnCells,alreadySeenAggDim);      previousColumnQualifier=currentColumnQualifier;      currentAggOp=getCurrentAggOp(cell);      converter=getValueConverter(currentColumnQualifier);    }    collectCells(currentColumnCells,currentAggOp,cell,alreadySeenAggDim,converter,scannerContext);    nextCell(scannerContext);  }  if ((!currentColumnCells.isEmpty()) && ((limit <= 0 || addedCnt < limit))) {    addedCnt+=emitCells(cells,currentColumnCells,currentAggOp,converter,currentTimestamp);    if (LOG.isDebugEnabled()) {
        summationDone=true;        if (LOG.isTraceEnabled()) {          LOG.trace("MAJOR COMPACTION loop sum= " + sum + " discarding now: "+ " qualifier="+ Bytes.toString(CellUtil.cloneQualifier(cell))+ " value="+ converter.decodeValue(CellUtil.cloneValue(cell))+ " timestamp="+ cell.getTimestamp()+ " "+ this.action);        }      } else {        finalCells.add(cell);      }    }  }  if (summationDone) {    Cell anyCell=currentColumnCells.first();    List<Tag> tags=new ArrayList<Tag>();    Tag t=HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    t=HBaseTimelineServerUtils.createTag(AggregationCompactionDimension.APPLICATION_ID.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    byte[] tagByteArray=HBaseTimelineServerUtils.convertTagListToByteArray(tags);    Cell sumCell=HBaseTimelineServerUtils.createNewCell(CellUtil.cloneRow(anyCell),CellUtil.cloneFamily(anyCell),CellUtil.cloneQualifier(anyCell),TimestampGenerator.getSupplementedTimestamp(System.currentTimeMillis(),FLOW_APP_ID),converter.encodeValue(sum),tagByteArray);    finalCells.add(sumCell);
          LOG.trace("MAJOR COMPACTION loop sum= " + sum + " discarding now: "+ " qualifier="+ Bytes.toString(CellUtil.cloneQualifier(cell))+ " value="+ converter.decodeValue(CellUtil.cloneValue(cell))+ " timestamp="+ cell.getTimestamp()+ " "+ this.action);        }      } else {        finalCells.add(cell);      }    }  }  if (summationDone) {    Cell anyCell=currentColumnCells.first();    List<Tag> tags=new ArrayList<Tag>();    Tag t=HBaseTimelineServerUtils.createTag(AggregationOperation.SUM_FINAL.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    t=HBaseTimelineServerUtils.createTag(AggregationCompactionDimension.APPLICATION_ID.getTagType(),Bytes.toBytes(FLOW_APP_ID));    tags.add(t);    byte[] tagByteArray=HBaseTimelineServerUtils.convertTagListToByteArray(tags);    Cell sumCell=HBaseTimelineServerUtils.createNewCell(CellUtil.cloneRow(anyCell),CellUtil.cloneFamily(anyCell),CellUtil.cloneQualifier(anyCell),TimestampGenerator.getSupplementedTimestamp(System.currentTimeMillis(),FLOW_APP_ID),converter.encodeValue(sum),tagByteArray);    finalCells.add(sumCell);    if (LOG.isTraceEnabled()) {      LOG.trace("MAJOR COMPACTION final sum= " + sum + " for "+ Bytes.toString(CellUtil.cloneQualifier(sumCell))+ " "+ this.action);
    return currentCell;  }  try {    Number previouslyChosenCellValue=(Number)converter.decodeValue(CellUtil.cloneValue(previouslyChosenCell));    Number currentCellValue=(Number)converter.decodeValue(CellUtil.cloneValue(currentCell));switch (currentAggOp) {case GLOBAL_MIN:      if (converter.compare(currentCellValue,previouslyChosenCellValue) < 0) {        return currentCell;      } else {        return previouslyChosenCell;      }case GLOBAL_MAX:    if (converter.compare(currentCellValue,previouslyChosenCellValue) > 0) {      return currentCell;    } else {      return previouslyChosenCell;    }default:  return currentCell;}} catch (IllegalArgumentException iae) {
  if (host == null || host.isEmpty()) {    bindAddress=conf.get(YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST,YarnConfiguration.DEFAULT_TIMELINE_SERVICE_BIND_HOST) + ":" + startPort;  } else {    bindAddress=host + ":" + startPort;  }  try {    HttpServer2.Builder builder=new HttpServer2.Builder().setName("timeline").setConf(conf).addEndpoint(URI.create((YarnConfiguration.useHttps(conf) ? "https://" : "http://") + bindAddress));    if (portRanges != null && !portRanges.isEmpty()) {      builder.setPortRanges(portRanges);    }    if (YarnConfiguration.useHttps(conf)) {      builder=WebAppUtils.loadSslConfiguration(builder,conf);    }    timelineRestServer=builder.build();    timelineRestServer.addJerseyResourcePackage(TimelineCollectorWebService.class.getPackage().getName() + ";" + GenericExceptionHandler.class.getPackage().getName()+ ";"+ YarnJacksonJaxbJsonProvider.class.getPackage().getName(),"/*");    timelineRestServer.setAttribute(COLLECTOR_MANAGER_ATTR_KEY,this);    timelineRestServer.start();  } catch (  Exception e) {
private void reportNewCollectorInfoToNM(ApplicationId appId,org.apache.hadoop.yarn.api.records.Token token) throws YarnException, IOException {  ReportNewCollectorInfoRequest request=ReportNewCollectorInfoRequest.newInstance(appId,this.timelineRestServerBindAddress,token);
private void updateTimelineCollectorContext(ApplicationId appId,TimelineCollector collector) throws YarnException, IOException {  GetTimelineCollectorContextRequest request=GetTimelineCollectorContextRequest.newInstance(appId);
private void updateTimelineCollectorContext(ApplicationId appId,TimelineCollector collector) throws YarnException, IOException {  GetTimelineCollectorContextRequest request=GetTimelineCollectorContextRequest.newInstance(appId);  LOG.info("Get timeline collector context for " + appId);  GetTimelineCollectorContextResponse response=getNMCollectorService().getTimelineCollectorContext(request);  String userId=response.getUserId();  if (userId != null && !userId.isEmpty()) {
  GetTimelineCollectorContextResponse response=getNMCollectorService().getTimelineCollectorContext(request);  String userId=response.getUserId();  if (userId != null && !userId.isEmpty()) {    LOG.debug("Setting the user in the context: {}",userId);    collector.getTimelineEntityContext().setUserId(userId);  }  String flowName=response.getFlowName();  if (flowName != null && !flowName.isEmpty()) {    LOG.debug("Setting the flow name: {}",flowName);    collector.getTimelineEntityContext().setFlowName(flowName);  }  String flowVersion=response.getFlowVersion();  if (flowVersion != null && !flowVersion.isEmpty()) {    LOG.debug("Setting the flow version: {}",flowVersion);    collector.getTimelineEntityContext().setFlowVersion(flowVersion);  }  long flowRunId=response.getFlowRunId();  if (flowRunId != 0L) {
@VisibleForTesting protected CollectorNodemanagerProtocol getNMCollectorService(){  if (nmCollectorService == null) {synchronized (this) {      if (nmCollectorService == null) {        Configuration conf=getConfig();        InetSocketAddress nmCollectorServiceAddress=conf.getSocketAddr(YarnConfiguration.NM_BIND_HOST,YarnConfiguration.NM_COLLECTOR_SERVICE_ADDRESS,YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS,YarnConfiguration.DEFAULT_NM_COLLECTOR_SERVICE_PORT);
@VisibleForTesting protected Future removeApplicationCollector(final ContainerId containerId){  final ApplicationId appId=containerId.getApplicationAttemptId().getApplicationId();  return scheduler.schedule(new Runnable(){    public void run(){      boolean shouldRemoveApplication=false;synchronized (appIdToContainerId) {        Set<ContainerId> masterContainers=appIdToContainerId.get(appId);        if (masterContainers == null) {
public TimelineWriteResponse putEntities(TimelineEntities entities,UserGroupInformation callerUgi) throws IOException {
public TimelineWriteResponse putDomain(TimelineDomain domain,UserGroupInformation callerUgi) throws IOException {
public void putEntitiesAsync(TimelineEntities entities,UserGroupInformation callerUgi) throws IOException {
private TimelineWriter createTimelineWriter(final Configuration conf){  String timelineWriterClassName=conf.get(YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS,YarnConfiguration.DEFAULT_TIMELINE_SERVICE_WRITER_CLASS);
public TimelineCollector putIfAbsent(ApplicationId appId,TimelineCollector collector){  TimelineCollector collectorInTable=null;synchronized (collectors) {    collectorInTable=collectors.get(appId);    if (collectorInTable == null) {      try {        collector.init(getConfig());        collector.setWriter(writer);        collector.start();        collectors.put(appId,collector);
  TimelineCollector collectorInTable=null;synchronized (collectors) {    collectorInTable=collectors.get(appId);    if (collectorInTable == null) {      try {        collector.init(getConfig());        collector.setWriter(writer);        collector.start();        collectors.put(appId,collector);        LOG.info("the collector for " + appId + " was added");        collectorInTable=collector;        postPut(appId,collectorInTable);      } catch (      Exception e) {        throw new YarnRuntimeException(e);      }    } else {
public boolean remove(ApplicationId appId){  TimelineCollector collector=collectors.remove(appId);  if (collector == null) {
@PUT @Path("/entities") @Consumes({MediaType.APPLICATION_JSON}) public Response putEntities(@Context HttpServletRequest req,@Context HttpServletResponse res,@QueryParam("async") String async,@QueryParam("subappwrite") String isSubAppEntities,@QueryParam("appid") String appId,TimelineEntities entities){  init(res);  UserGroupInformation callerUgi=getUser(req);  boolean isAsync=async != null && async.trim().equalsIgnoreCase("true");  if (callerUgi == null) {    String msg="The owner of the posted timeline entities is not set";
  UserGroupInformation callerUgi=getUser(req);  boolean isAsync=async != null && async.trim().equalsIgnoreCase("true");  if (callerUgi == null) {    String msg="The owner of the posted timeline entities is not set";    LOG.error(msg);    throw new ForbiddenException(msg);  }  long startTime=Time.monotonicNow();  boolean succeeded=false;  try {    ApplicationId appID=parseApplicationId(appId);    if (appID == null) {      return Response.status(Response.Status.BAD_REQUEST).build();    }    NodeTimelineCollectorManager collectorManager=(NodeTimelineCollectorManager)context.getAttribute(NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);    TimelineCollector collector=collectorManager.get(appID);    if (collector == null) {
    if (appID == null) {      return Response.status(Response.Status.BAD_REQUEST).build();    }    NodeTimelineCollectorManager collectorManager=(NodeTimelineCollectorManager)context.getAttribute(NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);    TimelineCollector collector=collectorManager.get(appID);    if (collector == null) {      LOG.error("Application: " + appId + " is not found");      throw new NotFoundException("Application: " + appId + " is not found");    }    if (isAsync) {      collector.putEntitiesAsync(processTimelineEntities(entities,appId,Boolean.valueOf(isSubAppEntities)),callerUgi);    } else {      collector.putEntities(processTimelineEntities(entities,appId,Boolean.valueOf(isSubAppEntities)),callerUgi);    }    succeeded=true;    return Response.ok().build();  } catch (  NotFoundException|ForbiddenException e) {    throw new WebApplicationException(e,Response.Status.INTERNAL_SERVER_ERROR);
    NodeTimelineCollectorManager collectorManager=(NodeTimelineCollectorManager)context.getAttribute(NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);    TimelineCollector collector=collectorManager.get(appID);    if (collector == null) {      LOG.error("Application: " + appId + " is not found");      throw new NotFoundException("Application: " + appId + " is not found");    }    if (isAsync) {      collector.putEntitiesAsync(processTimelineEntities(entities,appId,Boolean.valueOf(isSubAppEntities)),callerUgi);    } else {      collector.putEntities(processTimelineEntities(entities,appId,Boolean.valueOf(isSubAppEntities)),callerUgi);    }    succeeded=true;    return Response.ok().build();  } catch (  NotFoundException|ForbiddenException e) {    throw new WebApplicationException(e,Response.Status.INTERNAL_SERVER_ERROR);  }catch (  IOException e) {    LOG.error("Error putting entities",e);
@PUT @Path("/domain") @Consumes({MediaType.APPLICATION_JSON}) public Response putDomain(@Context HttpServletRequest req,@Context HttpServletResponse res,@QueryParam("appid") String appId,TimelineDomain domain){  init(res);  UserGroupInformation callerUgi=getUser(req);  if (callerUgi == null) {    String msg="The owner of the posted timeline entities is not set";
  }  try {    ApplicationId appID=parseApplicationId(appId);    if (appID == null) {      return Response.status(Response.Status.BAD_REQUEST).build();    }    NodeTimelineCollectorManager collectorManager=(NodeTimelineCollectorManager)context.getAttribute(NodeTimelineCollectorManager.COLLECTOR_MANAGER_ATTR_KEY);    TimelineCollector collector=collectorManager.get(appID);    if (collector == null) {      LOG.error("Application: " + appId + " is not found");      throw new NotFoundException("Application: " + appId + " is not found");    }    domain.setOwner(callerUgi.getShortUserName());    collector.putDomain(domain,callerUgi);    return Response.ok().build();  } catch (  NotFoundException e) {    throw new WebApplicationException(e,Response.Status.INTERNAL_SERVER_ERROR);  }catch (  IOException e) {
private TimelineReader createTimelineReaderStore(final Configuration conf){  String timelineReaderClassName=conf.get(YarnConfiguration.TIMELINE_SERVICE_READER_CLASS,YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READER_CLASS);
  if (host == null || host.isEmpty()) {    hostProperty=YarnConfiguration.TIMELINE_SERVICE_BIND_HOST;  }  String bindAddress=WebAppUtils.getWebAppBindURL(conf,hostProperty,webAppURLWithoutScheme);  LOG.info("Instantiating TimelineReaderWebApp at " + bindAddress);  try {    String httpScheme=WebAppUtils.getHttpSchemePrefix(conf);    HttpServer2.Builder builder=new HttpServer2.Builder().setName("timeline").setConf(conf).addEndpoint(URI.create(httpScheme + bindAddress));    if (httpScheme.equals(WebAppUtils.HTTPS_PREFIX)) {      WebAppUtils.loadSslConfiguration(builder,conf);    }    readerWebServer=builder.build();    readerWebServer.addJerseyResourcePackage(TimelineReaderWebServices.class.getPackage().getName() + ";" + GenericExceptionHandler.class.getPackage().getName()+ ";"+ YarnJacksonJaxbJsonProvider.class.getPackage().getName()+ ";"+ LogWebService.class.getPackage().getName(),"/*");    readerWebServer.setAttribute(TIMELINE_READER_MANAGER_ATTR,timelineReaderManager);    readerWebServer.start();  } catch (  Exception e) {    String msg="TimelineReaderWebApp failed to start.";
private static void handleException(Exception e,String url,long startTime,String invalidNumMsg) throws BadRequestException, WebApplicationException {  long endTime=Time.monotonicNow();
  long endTime=Time.monotonicNow();  LOG.info("Processed URL " + url + " but encountered exception (Took "+ (endTime - startTime)+ " ms.)");  if (e instanceof NumberFormatException) {    throw new BadRequestException(invalidNumMsg + " is not a numeric value.");  } else   if (e instanceof IllegalArgumentException) {    throw new BadRequestException(e.getMessage() == null ? "Requested Invalid Field." : e.getMessage());  } else   if (e instanceof NotFoundException) {    throw (NotFoundException)e;  } else   if (e instanceof TimelineParseException) {    throw new BadRequestException(e.getMessage() == null ? "Filter Parsing failed." : e.getMessage());  } else   if (e instanceof BadRequestException) {    throw (BadRequestException)e;  } else   if (e instanceof ForbiddenException) {    throw (ForbiddenException)e;  } else {
@GET @Path("/app-uid/{uid}/entities/{entitytype}") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getEntities(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("uid") String uId,@PathParam("entitytype") String entityType,@QueryParam("limit") String limit,@QueryParam("createdtimestart") String createdTimeStart,@QueryParam("createdtimeend") String createdTimeEnd,@QueryParam("relatesto") String relatesTo,@QueryParam("isrelatedto") String isRelatedTo,@QueryParam("infofilters") String infofilters,@QueryParam("conffilters") String conffilters,@QueryParam("metricfilters") String metricfilters,@QueryParam("eventfilters") String eventfilters,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd,@QueryParam("fromid") String fromId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    TimelineReaderContext context=TimelineUIDConverter.APPLICATION_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    context.setEntityType(TimelineReaderWebServicesUtils.parseStr(entityType));    entities=timelineReaderManager.getEntities(context,TimelineReaderWebServicesUtils.createTimelineEntityFilters(limit,createdTimeStart,createdTimeEnd,relatesTo,isRelatedTo,infofilters,conffilters,metricfilters,eventfilters,fromId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForGenericEntities(entities,callerUGI,entityType);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either limit or createdtime start/end or metricslimit or metricstime" + " start/end or fromid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
@GET @Path("/clusters/{clusterid}/apps/{appid}/entities/{entitytype}") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getEntities(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("appid") String appId,@PathParam("entitytype") String entityType,@QueryParam("userid") String userId,@QueryParam("flowname") String flowName,@QueryParam("flowrunid") String flowRunId,@QueryParam("limit") String limit,@QueryParam("createdtimestart") String createdTimeStart,@QueryParam("createdtimeend") String createdTimeEnd,@QueryParam("relatesto") String relatesTo,@QueryParam("isrelatedto") String isRelatedTo,@QueryParam("infofilters") String infofilters,@QueryParam("conffilters") String conffilters,@QueryParam("metricfilters") String metricfilters,@QueryParam("eventfilters") String eventfilters,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd,@QueryParam("fromid") String fromId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    TimelineReaderContext context=TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,appId,entityType,null,null);    entities=timelineReaderManager.getEntities(context,TimelineReaderWebServicesUtils.createTimelineEntityFilters(limit,createdTimeStart,createdTimeEnd,relatesTo,isRelatedTo,infofilters,conffilters,metricfilters,eventfilters,fromId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForGenericEntities(entities,callerUGI,entityType);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either flowrunid or limit or createdtime start/end or metricslimit" + " or metricstime start/end or fromid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
@GET @Path("/entity-uid/{uid}/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public TimelineEntity getEntity(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("uid") String uId,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    TimelineReaderContext context=TimelineUIDConverter.GENERIC_ENTITY_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForGenericEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either metricslimit or metricstime" + " start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
  TimelineEntity entity=null;  try {    TimelineReaderContext context=TimelineUIDConverter.GENERIC_ENTITY_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForGenericEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either metricslimit or metricstime" + " start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);    LOG.info("Processed URL " + url + " (Took "+ latency+ " ms.)");  }  if (entity == null) {
@GET @Path("/clusters/{clusterid}/apps/{appid}/entities/{entitytype}/{entityid}/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public TimelineEntity getEntity(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("appid") String appId,@PathParam("entitytype") String entityType,@PathParam("entityid") String entityId,@QueryParam("userid") String userId,@QueryParam("flowname") String flowName,@QueryParam("flowrunid") String flowRunId,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd,@QueryParam("entityidprefix") String entityIdPrefix){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  LOG.info("Received URL " + url + " from user "+ TimelineReaderWebServicesUtils.getUserName(callerUGI));  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    entity=timelineReaderManager.getEntity(TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,appId,entityType,entityIdPrefix,entityId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForGenericEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either flowrunid or metricslimit or" + " metricstime start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    entity=timelineReaderManager.getEntity(TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,appId,entityType,entityIdPrefix,entityId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForGenericEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either flowrunid or metricslimit or" + " metricstime start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);    LOG.info("Processed URL " + url + " (Took "+ latency+ " ms.)");  }  if (entity == null) {
@GET @Path("/run-uid/{uid}/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public TimelineEntity getFlowRun(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("uid") String uId,@QueryParam("metricstoretrieve") String metricsToRetrieve){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    TimelineReaderContext context=TimelineUIDConverter.FLOWRUN_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    checkAccess(timelineReaderManager,callerUGI,context.getUserId());    context.setEntityType(TimelineEntityType.YARN_FLOW_RUN.toString());    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(null,metricsToRetrieve,null,null,null,null));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"flowrunid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
  try {    TimelineReaderContext context=TimelineUIDConverter.FLOWRUN_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    checkAccess(timelineReaderManager,callerUGI,context.getUserId());    context.setEntityType(TimelineEntityType.YARN_FLOW_RUN.toString());    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(null,metricsToRetrieve,null,null,null,null));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"flowrunid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);    LOG.info("Processed URL " + url + " (Took "+ latency+ " ms.)");  }  if (entity == null) {
@GET @Path("/clusters/{clusterid}/users/{userid}/flows/{flowname}/" + "runs/{flowrunid}/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public TimelineEntity getFlowRun(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("userid") String userId,@PathParam("flowname") String flowName,@PathParam("flowrunid") String flowRunId,@QueryParam("metricstoretrieve") String metricsToRetrieve){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    TimelineReaderContext context=TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,null,TimelineEntityType.YARN_FLOW_RUN.toString(),null,null);    checkAccess(timelineReaderManager,callerUGI,context.getUserId());    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(null,metricsToRetrieve,null,null,null,null));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"flowrunid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    TimelineReaderContext context=TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,null,TimelineEntityType.YARN_FLOW_RUN.toString(),null,null);    checkAccess(timelineReaderManager,callerUGI,context.getUserId());    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(null,metricsToRetrieve,null,null,null,null));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"flowrunid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);    LOG.info("Processed URL " + url + " (Took "+ latency+ " ms.)");  }  if (entity == null) {
@GET @Path("/flow-uid/{uid}/runs/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getFlowRuns(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("uid") String uId,@QueryParam("limit") String limit,@QueryParam("createdtimestart") String createdTimeStart,@QueryParam("createdtimeend") String createdTimeEnd,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("fromid") String fromId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    TimelineReaderContext context=TimelineUIDConverter.FLOW_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    checkAccess(timelineReaderManager,callerUGI,context.getUserId());    context.setEntityType(TimelineEntityType.YARN_FLOW_RUN.toString());    entities=timelineReaderManager.getEntities(context,TimelineReaderWebServicesUtils.createTimelineEntityFilters(limit,createdTimeStart,createdTimeEnd,null,null,null,null,null,null,fromId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(null,metricsToRetrieve,fields,null,null,null));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"createdTime start/end or limit or fromId");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
@GET @Path("/clusters/{clusterid}/users/{userid}/flows/{flowname}/runs/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getFlowRuns(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("userid") String userId,@PathParam("flowname") String flowName,@QueryParam("limit") String limit,@QueryParam("createdtimestart") String createdTimeStart,@QueryParam("createdtimeend") String createdTimeEnd,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("fromid") String fromId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    TimelineReaderContext timelineReaderContext=TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,null,null,TimelineEntityType.YARN_FLOW_RUN.toString(),null,null);    checkAccess(timelineReaderManager,callerUGI,timelineReaderContext.getUserId());    entities=timelineReaderManager.getEntities(timelineReaderContext,TimelineReaderWebServicesUtils.createTimelineEntityFilters(limit,createdTimeStart,createdTimeEnd,null,null,null,null,null,null,fromId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(null,metricsToRetrieve,fields,null,null,null));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"createdTime start/end or limit or fromId");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
@GET @Path("/clusters/{clusterid}/flows/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getFlows(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@QueryParam("limit") String limit,@QueryParam("daterange") String dateRange,@QueryParam("fromid") String fromId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    DateRange range=parseDateRange(dateRange);    TimelineEntityFilters entityFilters=TimelineReaderWebServicesUtils.createTimelineEntityFilters(limit,range.dateStart,range.dateEnd,null,null,null,null,null,null,fromId);    entities=timelineReaderManager.getEntities(TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,null,null,null,null,TimelineEntityType.YARN_FLOW_ACTIVITY.toString(),null,null),entityFilters,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(null,null,null,null,null,null));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"limit");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
@GET @Path("/app-uid/{uid}/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public TimelineEntity getApp(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("uid") String uId,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    TimelineReaderContext context=TimelineUIDConverter.APPLICATION_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    context.setEntityType(TimelineEntityType.YARN_APPLICATION.toString());    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForAppEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either metricslimit or metricstime" + " start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
  try {    TimelineReaderContext context=TimelineUIDConverter.APPLICATION_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    context.setEntityType(TimelineEntityType.YARN_APPLICATION.toString());    entity=timelineReaderManager.getEntity(context,TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForAppEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either metricslimit or metricstime" + " start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);    LOG.info("Processed URL " + url + " (Took "+ latency+ " ms.)");  }  if (entity == null) {
@GET @Path("/clusters/{clusterid}/apps/{appid}/") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public TimelineEntity getApp(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("appid") String appId,@QueryParam("flowname") String flowName,@QueryParam("flowrunid") String flowRunId,@QueryParam("userid") String userId,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  LOG.info("Received URL " + url + " from user "+ TimelineReaderWebServicesUtils.getUserName(callerUGI));  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    entity=timelineReaderManager.getEntity(TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,appId,TimelineEntityType.YARN_APPLICATION.toString(),null,null),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForAppEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either flowrunid or metricslimit or" + " metricstime start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  TimelineEntity entity=null;  try {    entity=timelineReaderManager.getEntity(TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,appId,TimelineEntityType.YARN_APPLICATION.toString(),null,null),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForAppEntity(entity,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either flowrunid or metricslimit or" + " metricstime start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);    LOG.info("Processed URL " + url + " (Took "+ latency+ " ms.)");  }  if (entity == null) {
@GET @Path("/run-uid/{uid}/apps") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getFlowRunApps(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("uid") String uId,@QueryParam("limit") String limit,@QueryParam("createdtimestart") String createdTimeStart,@QueryParam("createdtimeend") String createdTimeEnd,@QueryParam("relatesto") String relatesTo,@QueryParam("isrelatedto") String isRelatedTo,@QueryParam("infofilters") String infofilters,@QueryParam("conffilters") String conffilters,@QueryParam("metricfilters") String metricfilters,@QueryParam("eventfilters") String eventfilters,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd,@QueryParam("fromid") String fromId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    TimelineReaderContext context=TimelineUIDConverter.FLOWRUN_UID.decodeUID(uId);    if (context == null) {      throw new BadRequestException("Incorrect UID " + uId);    }    checkAccess(timelineReaderManager,callerUGI,context.getUserId());    context.setEntityType(TimelineEntityType.YARN_APPLICATION.toString());    entities=timelineReaderManager.getEntities(context,TimelineReaderWebServicesUtils.createTimelineEntityFilters(limit,createdTimeStart,createdTimeEnd,relatesTo,isRelatedTo,infofilters,conffilters,metricfilters,eventfilters,fromId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either limit or createdtime start/end or metricslimit or" + " metricstime start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
@GET @Path("/clusters/{clusterid}/apps/{appid}/entity-types") @Produces(MediaType.APPLICATION_JSON) public Set<String> getEntityTypes(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("appid") String appId,@QueryParam("flowname") String flowName,@QueryParam("flowrunid") String flowRunId,@QueryParam("userid") String userId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<String> results=null;  try {    TimelineReaderContext context=TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,userId,flowName,flowRunId,appId,null,null,null);    results=timelineReaderManager.getEntityTypes(context);    checkAccess(getTimelineReaderManager(),callerUGI,context.getUserId());    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"flowrunid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntityTypesLatency(latency,succeeded);
@GET @Path("/clusters/{clusterid}/users/{userid}/entities/{entitytype}") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getSubAppEntities(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("userid") String userId,@PathParam("entitytype") String entityType,@QueryParam("limit") String limit,@QueryParam("createdtimestart") String createdTimeStart,@QueryParam("createdtimeend") String createdTimeEnd,@QueryParam("relatesto") String relatesTo,@QueryParam("isrelatedto") String isRelatedTo,@QueryParam("infofilters") String infofilters,@QueryParam("conffilters") String conffilters,@QueryParam("metricfilters") String metricfilters,@QueryParam("eventfilters") String eventfilters,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd,@QueryParam("fromid") String fromId){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    TimelineReaderContext context=TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,null,null,null,null,entityType,null,null,userId);    entities=timelineReaderManager.getEntities(context,TimelineReaderWebServicesUtils.createTimelineEntityFilters(limit,createdTimeStart,createdTimeEnd,relatesTo,isRelatedTo,infofilters,conffilters,metricfilters,eventfilters,fromId),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForSubAppEntities(entities,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either limit or createdtime start/end or metricslimit or metricstime" + " start/end or fromid");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
@GET @Path("/clusters/{clusterid}/users/{userid}/entities/{entitytype}/{entityid}") @Produces(MediaType.APPLICATION_JSON + "; " + JettyUtils.UTF_8) public Set<TimelineEntity> getSubAppEntities(@Context HttpServletRequest req,@Context HttpServletResponse res,@PathParam("clusterid") String clusterId,@PathParam("userid") String userId,@PathParam("entitytype") String entityType,@PathParam("entityid") String entityId,@QueryParam("confstoretrieve") String confsToRetrieve,@QueryParam("metricstoretrieve") String metricsToRetrieve,@QueryParam("fields") String fields,@QueryParam("metricslimit") String metricsLimit,@QueryParam("metricstimestart") String metricsTimeStart,@QueryParam("metricstimeend") String metricsTimeEnd,@QueryParam("entityidprefix") String entityIdPrefix){  String url=req.getRequestURI() + (req.getQueryString() == null ? "" : QUERY_STRING_SEP + req.getQueryString());  UserGroupInformation callerUGI=TimelineReaderWebServicesUtils.getUser(req);
  long startTime=Time.monotonicNow();  boolean succeeded=false;  init(res);  TimelineReaderManager timelineReaderManager=getTimelineReaderManager();  Set<TimelineEntity> entities=null;  try {    TimelineReaderContext context=TimelineReaderWebServicesUtils.createTimelineReaderContext(clusterId,null,null,null,null,entityType,entityIdPrefix,entityId,userId);    entities=timelineReaderManager.getEntities(context,new TimelineEntityFilters.Builder().build(),TimelineReaderWebServicesUtils.createTimelineDataToRetrieve(confsToRetrieve,metricsToRetrieve,fields,metricsLimit,metricsTimeStart,metricsTimeEnd));    checkAccessForSubAppEntities(entities,callerUGI);    succeeded=true;  } catch (  Exception e) {    handleException(e,url,startTime,"Either metricslimit or metricstime" + " start/end");  } finally {    long latency=Time.monotonicNow() - startTime;    METRICS.addGetEntitiesLatency(latency,succeeded);
static boolean validateAuthUserWithEntityUser(TimelineReaderManager readerManager,UserGroupInformation ugi,String entityUser){  String authUser=TimelineReaderWebServicesUtils.getUserName(ugi);  String requestedUser=TimelineReaderWebServicesUtils.parseStr(entityUser);
    isWhitelistReadAuthEnabled=YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READ_AUTH_ENABLED;  } else {    isWhitelistReadAuthEnabled=Boolean.valueOf(isWhitelistReadAuthEnabledStr);  }  if (isWhitelistReadAuthEnabled) {    String listAllowedUsers=conf.getInitParameter(YarnConfiguration.TIMELINE_SERVICE_READ_ALLOWED_USERS);    if (StringUtils.isEmpty(listAllowedUsers)) {      listAllowedUsers=YarnConfiguration.DEFAULT_TIMELINE_SERVICE_READ_ALLOWED_USERS;    }    LOG.info("listAllowedUsers=" + listAllowedUsers);    allowedUsersAclList=new AccessControlList(listAllowedUsers);    LOG.info("allowedUsersAclList=" + allowedUsersAclList.getUsers());    String adminAclListStr=conf.getInitParameter(YarnConfiguration.YARN_ADMIN_ACL);    if (StringUtils.isEmpty(adminAclListStr)) {      adminAclListStr=TimelineReaderWhitelistAuthorizationFilter.EMPTY_STRING;      LOG.info("adminAclList not set, hence setting it to \"\"");    }    adminAclList=new AccessControlList(adminAclListStr);
@VisibleForTesting int createTimelineSchema(String[] args,Configuration conf) throws Exception {  String schemaCreatorClassName=conf.get(YarnConfiguration.TIMELINE_SERVICE_SCHEMA_CREATOR_CLASS,YarnConfiguration.DEFAULT_TIMELINE_SERVICE_SCHEMA_CREATOR_CLASS);
  SubjectPublicKeyInfo subPubKeyInfo=SubjectPublicKeyInfo.getInstance(publicKey.getEncoded());  X509v3CertificateBuilder certBuilder=new X509v3CertificateBuilder(issuer,new BigInteger(64,srand),from,to,subject,subPubKeyInfo);  AlgorithmIdentifier digAlgId=new DefaultDigestAlgorithmIdentifierFinder().find(SIG_ALG_ID);  ContentSigner contentSigner;  try {    contentSigner=new BcRSAContentSignerBuilder(SIG_ALG_ID,digAlgId).build(PrivateKeyFactory.createKey(privateKey.getEncoded()));  } catch (  OperatorCreationException oce) {    throw new GeneralSecurityException(oce);  }  if (isCa) {    certBuilder.addExtension(Extension.basicConstraints,true,new BasicConstraints(0));  } else {    certBuilder.addExtension(Extension.basicConstraints,true,new BasicConstraints(false));    certBuilder.addExtension(Extension.authorityKeyIdentifier,false,new JcaX509ExtensionUtils().createAuthorityKeyIdentifier(caCert));  }  X509CertificateHolder certHolder=certBuilder.build(contentSigner);  X509Certificate cert=new JcaX509CertificateConverter().setProvider("BC").getCertificate(certHolder);
private void createCACertAndKeyPair() throws GeneralSecurityException, IOException {  Date from=new Date();  Date to=new GregorianCalendar(2037,Calendar.DECEMBER,31).getTime();  KeyPairGenerator keyGen=KeyPairGenerator.getInstance("RSA");  keyGen.initialize(2048);  caKeyPair=keyGen.genKeyPair();  String subject="OU=YARN-" + UUID.randomUUID();  caCert=createCert(true,subject,subject,from,to,caKeyPair.getPublic(),caKeyPair.getPrivate());
public static void sendRedirect(HttpServletRequest request,HttpServletResponse response,String target) throws IOException {
  } else {    LOG.warn("Unrecognized attribute value for " + CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION + " of "+ auth);  }  String proxy=WebAppUtils.getProxyHostAndPort(conf);  String[] proxyParts=proxy.split(":");  proxyHost=proxyParts[0];  fetcher=new AppReportFetcher(conf);  bindAddress=conf.get(YarnConfiguration.PROXY_ADDRESS);  if (bindAddress == null || bindAddress.isEmpty()) {    throw new YarnRuntimeException(YarnConfiguration.PROXY_ADDRESS + " is not set so the proxy will not run.");  }  String[] parts=StringUtils.split(bindAddress,':');  port=0;  if (parts.length == 2) {    bindAddress=parts[0];    port=Integer.parseInt(parts[1]);  }  String bindHost=conf.getTrimmed(YarnConfiguration.PROXY_BIND_HOST,null);
  }  String proxy=WebAppUtils.getProxyHostAndPort(conf);  String[] proxyParts=proxy.split(":");  proxyHost=proxyParts[0];  fetcher=new AppReportFetcher(conf);  bindAddress=conf.get(YarnConfiguration.PROXY_ADDRESS);  if (bindAddress == null || bindAddress.isEmpty()) {    throw new YarnRuntimeException(YarnConfiguration.PROXY_ADDRESS + " is not set so the proxy will not run.");  }  String[] parts=StringUtils.split(bindAddress,':');  port=0;  if (parts.length == 2) {    bindAddress=parts[0];    port=Integer.parseInt(parts[1]);  }  String bindHost=conf.getTrimmed(YarnConfiguration.PROXY_BIND_HOST,null);  if (bindHost != null) {    LOG.debug("{} is set, will be used to run proxy.",YarnConfiguration.PROXY_BIND_HOST);
    boolean checkUser=securityEnabled && (!userWasWarned || !userApproved);    FetchedAppReport fetchedAppReport;    try {      fetchedAppReport=getFetchedAppReport(id);    } catch (    ApplicationNotFoundException e) {      fetchedAppReport=null;    }    ApplicationReport applicationReport=null;    if (fetchedAppReport != null) {      applicationReport=fetchedAppReport.getApplicationReport();    }    if (applicationReport == null) {      LOG.warn("{} attempting to access {} that was not found",remoteUser,id);      URI toFetch=ProxyUriUtils.getUriFromTrackingPlugins(id,this.trackingUriPlugins);      if (toFetch != null) {        ProxyUtils.sendRedirect(req,resp,toFetch.toString());        return;
    } catch (    ApplicationNotFoundException e) {      fetchedAppReport=null;    }    ApplicationReport applicationReport=null;    if (fetchedAppReport != null) {      applicationReport=fetchedAppReport.getApplicationReport();    }    if (applicationReport == null) {      LOG.warn("{} attempting to access {} that was not found",remoteUser,id);      URI toFetch=ProxyUriUtils.getUriFromTrackingPlugins(id,this.trackingUriPlugins);      if (toFetch != null) {        ProxyUtils.sendRedirect(req,resp,toFetch.toString());        return;      }      notFound(resp,"Application " + appId + " could not be found "+ "in RM or history server");      return;    }    URI trackingUri=getTrackingUri(req,resp,id,applicationReport.getOriginalTrackingUrl(),fetchedAppReport.getAppReportSource());    if (trackingUri == null) {
protected Set<String> getProxyAddresses() throws ServletException {  long now=Time.monotonicNow();synchronized (this) {    if (proxyAddresses == null || (lastUpdate + updateInterval) <= now) {      proxyAddresses=new HashSet<>();      for (      String proxyHost : proxyHosts) {        try {          for (          InetAddress add : InetAddress.getAllByName(proxyHost)) {
@Override public void doFilter(ServletRequest req,ServletResponse resp,FilterChain chain) throws IOException, ServletException {  ProxyUtils.rejectNonHttpRequests(req);  HttpServletRequest httpReq=(HttpServletRequest)req;  HttpServletResponse httpResp=(HttpServletResponse)resp;
      insertPoint+=PROXY_PATH.length();      redirect.insert(insertPoint,"/redirect");    }    String queryString=httpReq.getQueryString();    if (queryString != null && !queryString.isEmpty()) {      redirect.append("?");      redirect.append(queryString);    }    ProxyUtils.sendRedirect(httpReq,httpResp,redirect.toString());  } else {    String user=null;    if (httpReq.getCookies() != null) {      for (      Cookie c : httpReq.getCookies()) {        if (WebAppProxyServlet.PROXY_USER_COOKIE_NAME.equals(c.getName())) {          user=c.getValue();          break;        }      }    }    if (user == null) {
@BeforeClass public static void start() throws Exception {  server=new Server(0);  ((QueuedThreadPool)server.getThreadPool()).setMaxThreads(20);  ServletContextHandler context=new ServletContextHandler();  context.setContextPath("/foo");  server.setHandler(context);  context.addServlet(new ServletHolder(TestServlet.class),"/bar");  ((ServerConnector)server.getConnectors()[0]).setHost("localhost");  server.start();  originalPort=((ServerConnector)server.getConnectors()[0]).getLocalPort();
    proxyConn=(HttpURLConnection)url.openConnection();    proxyConn.connect();    assertEquals(HttpURLConnection.HTTP_OK,proxyConn.getResponseCode());    String s=readInputStream(proxyConn.getInputStream());    assertTrue(s.contains("to continue to an Application Master web interface owned by"));    assertTrue(s.contains("WARNING: The following page may not be safe!"));    appReportFetcher.answer=3;    proxyConn=(HttpURLConnection)url.openConnection();    proxyConn.setRequestProperty("Cookie","checked_application_0_0000=true");    proxyConn.connect();    assertEquals(HttpURLConnection.HTTP_OK,proxyConn.getResponseCode());    appReportFetcher.answer=5;    URL clientUrl=new URL("http://localhost:" + proxyPort + "/proxy/application_00_0/test/tez?x=y&h=p");    proxyConn=(HttpURLConnection)clientUrl.openConnection();    proxyConn.connect();
