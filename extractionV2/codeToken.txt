public
void
simpleReadAfterWrite
(
final
FileSystem
fs
)
throws
IOException
{
private
File
createDir
(
String
dirPath
)
throws
IOException
{
File
dir
=
new
File
(
dirPath
)
;
if
(
!
dir
.
exists
(
)
)
{
try
{
InputStream
inputStream
=
this
.
store
.
retrieveBlock
(
this
.
key
,
this
.
readBuffer
.
getStart
(
)
,
this
.
readBuffer
.
getEnd
(
)
)
;
IOUtils
.
readFully
(
inputStream
,
this
.
readBuffer
.
getBuffer
(
)
,
0
,
readBuffer
.
getBuffer
(
)
.
length
)
;
inputStream
.
close
(
)
;
this
.
readBuffer
.
setStatus
(
CosNInputStream
.
ReadBuffer
.
SUCCESS
)
;
break
;
}
catch
(
IOException
e
)
{
this
.
readBuffer
.
setStatus
(
CosNInputStream
.
ReadBuffer
.
ERROR
)
;
LOG
.
warn
(
+
String
.
valueOf
(
this
.
readBuffer
.
getStart
(
)
)
+
+
this
.
readBuffer
.
getEnd
(
)
)
;
try
{
retryAction
=
this
.
retryPolicy
.
shouldRetry
(
e
,
retries
++
,
0
,
true
)
;
if
(
retryAction
.
action
==
RetryPolicy
.
RetryAction
.
RetryDecision
.
RETRY
)
{
Thread
.
sleep
(
retryAction
.
delayMillis
)
;
}
}
catch
(
Exception
e1
)
{
String
errMsg
=
String
.
format
(
+
,
this
.
retryPolicy
.
toString
(
)
,
String
.
valueOf
(
this
.
readBuffer
.
getStart
(
)
)
,
String
.
valueOf
(
this
.
readBuffer
.
getEnd
(
)
)
)
;
try
{
String
userName
=
System
.
getProperty
(
)
;
String
command
=
+
userName
;
if
(
!
getOwnerId
)
{
command
=
+
userName
;
}
Process
child
=
Runtime
.
getRuntime
(
)
.
exec
(
command
)
;
child
.
waitFor
(
)
;
InputStream
in
=
child
.
getInputStream
(
)
;
StringBuilder
strBuffer
=
new
StringBuilder
(
)
;
int
c
;
while
(
(
c
=
in
.
read
(
)
)
!=
-
1
)
{
strBuffer
.
append
(
(
char
)
c
)
;
}
in
.
close
(
)
;
ownerInfoId
=
strBuffer
.
toString
(
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
@
Override
public
boolean
delete
(
Path
f
,
boolean
recursive
)
throws
IOException
{
if
(
!
key
.
endsWith
(
PATH_DELIMITER
)
)
{
key
+=
PATH_DELIMITER
;
}
if
(
!
recursive
&&
listStatus
(
f
)
.
length
>
0
)
{
String
errMsg
=
String
.
format
(
+
,
f
)
;
throw
new
IOException
(
errMsg
)
;
}
createParent
(
f
)
;
String
priorLastKey
=
null
;
do
{
PartialListing
listing
=
store
.
list
(
key
,
Constants
.
COS_MAX_LISTING_LENGTH
,
priorLastKey
,
true
)
;
for
(
FileMetadata
file
:
listing
.
getFiles
(
)
)
{
store
.
delete
(
file
.
getKey
(
)
)
;
}
for
(
FileMetadata
commonPrefix
:
listing
.
getCommonPrefixes
(
)
)
{
store
.
delete
(
commonPrefix
.
getKey
(
)
)
;
}
priorLastKey
=
listing
.
getPriorLastKey
(
)
;
}
while
(
priorLastKey
!=
null
)
;
}
if
(
!
recursive
&&
listStatus
(
f
)
.
length
>
0
)
{
String
errMsg
=
String
.
format
(
+
,
f
)
;
throw
new
IOException
(
errMsg
)
;
}
createParent
(
f
)
;
String
priorLastKey
=
null
;
do
{
PartialListing
listing
=
store
.
list
(
key
,
Constants
.
COS_MAX_LISTING_LENGTH
,
priorLastKey
,
true
)
;
for
(
FileMetadata
file
:
listing
.
getFiles
(
)
)
{
store
.
delete
(
file
.
getKey
(
)
)
;
}
for
(
FileMetadata
commonPrefix
:
listing
.
getCommonPrefixes
(
)
)
{
store
.
delete
(
commonPrefix
.
getKey
(
)
)
;
}
priorLastKey
=
listing
.
getPriorLastKey
(
)
;
}
while
(
priorLastKey
!=
null
)
;
try
{
store
.
delete
(
key
)
;
}
LOG
.
debug
(
+
,
f
)
;
FileMetadata
meta
=
store
.
retrieveMetadata
(
key
)
;
if
(
meta
!=
null
)
{
if
(
meta
.
isFile
(
)
)
{
LOG
.
debug
(
,
f
,
key
)
;
return
newFile
(
meta
,
absolutePath
)
;
}
else
{
LOG
.
debug
(
,
f
,
key
)
;
return
newDirectory
(
meta
,
absolutePath
)
;
}
}
if
(
!
key
.
endsWith
(
PATH_DELIMITER
)
)
{
key
+=
PATH_DELIMITER
;
}
LOG
.
debug
(
,
key
)
;
PartialListing
listing
=
store
.
list
(
key
,
1
)
;
if
(
listing
.
getFiles
(
)
.
length
>
0
||
listing
.
getCommonPrefixes
(
)
.
length
>
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
key
=
pathToKey
(
absolutePath
)
;
if
(
key
.
length
(
)
>
0
)
{
FileStatus
fileStatus
=
this
.
getFileStatus
(
f
)
;
if
(
fileStatus
.
isFile
(
)
)
{
return
new
FileStatus
[
]
{
fileStatus
}
;
}
}
if
(
!
key
.
endsWith
(
PATH_DELIMITER
)
)
{
key
+=
PATH_DELIMITER
;
}
URI
pathUri
=
absolutePath
.
toUri
(
)
;
Set
<
FileStatus
>
status
=
new
TreeSet
<
>
(
)
;
String
priorLastKey
=
null
;
do
{
PartialListing
listing
=
store
.
list
(
key
,
Constants
.
COS_MAX_LISTING_LENGTH
,
priorLastKey
,
false
)
;
for
(
FileMetadata
fileMetadata
:
listing
.
getFiles
(
)
)
{
Path
subPath
=
keyToPath
(
fileMetadata
.
getKey
(
)
)
;
if
(
fileMetadata
.
getKey
(
)
.
equals
(
key
)
)
{
List
<
Path
>
paths
=
new
ArrayList
<
>
(
)
;
do
{
paths
.
add
(
absolutePath
)
;
absolutePath
=
absolutePath
.
getParent
(
)
;
}
while
(
absolutePath
!=
null
)
;
for
(
Path
path
:
paths
)
{
if
(
path
.
equals
(
new
Path
(
CosNFileSystem
.
PATH_DELIMITER
)
)
)
{
break
;
}
try
{
FileStatus
fileStatus
=
getFileStatus
(
path
)
;
if
(
fileStatus
.
isFile
(
)
)
{
throw
new
FileAlreadyExistsException
(
String
.
format
(
+
,
f
)
)
;
}
if
(
fileStatus
.
isDirectory
(
)
)
{
break
;
}
}
catch
(
FileNotFoundException
e
)
{
@
Override
public
boolean
rename
(
Path
src
,
Path
dst
)
throws
IOException
{
LOG
.
debug
(
+
,
dstParentPath
)
;
}
if
(
null
!=
dstParentPath
)
{
LOG
.
debug
(
+
,
src
,
dst
)
;
throw
new
IOException
(
String
.
format
(
+
,
src
,
dst
)
)
;
}
FileStatus
dstFileStatus
;
try
{
dstFileStatus
=
this
.
getFileStatus
(
dst
)
;
if
(
dstFileStatus
.
isFile
(
)
)
{
throw
new
FileAlreadyExistsException
(
String
.
format
(
,
dstFileStatus
.
getPath
(
)
)
)
;
}
else
{
dst
=
new
Path
(
dst
,
src
.
getName
(
)
)
;
FileStatus
[
]
statuses
;
try
{
statuses
=
this
.
listStatus
(
dst
)
;
}
catch
(
FileNotFoundException
e
)
{
private
void
createParent
(
Path
path
)
throws
IOException
{
Path
parent
=
path
.
getParent
(
)
;
if
(
parent
!=
null
)
{
String
parentKey
=
pathToKey
(
parent
)
;
else
{
PartETag
partETag
=
null
;
if
(
this
.
blockWritten
>
0
)
{
LOG
.
info
(
,
this
.
currentBlockId
,
this
.
blockWritten
)
;
partETag
=
store
.
uploadPart
(
new
ByteBufferInputStream
(
currentBlockBuffer
.
getByteBuffer
(
)
)
,
key
,
uploadId
,
currentBlockId
+
1
,
currentBlockBuffer
.
getByteBuffer
(
)
.
remaining
(
)
)
;
}
final
List
<
PartETag
>
futurePartETagList
=
this
.
waitForFinishPartUploads
(
)
;
if
(
null
==
futurePartETagList
)
{
throw
new
IOException
(
)
;
}
List
<
PartETag
>
tmpPartEtagList
=
new
LinkedList
<
>
(
futurePartETagList
)
;
if
(
null
!=
partETag
)
{
tmpPartEtagList
.
add
(
partETag
)
;
}
store
.
completeMultipartUpload
(
this
.
key
,
this
.
uploadId
,
tmpPartEtagList
)
;
}
try
{
BufferPool
.
getInstance
(
)
.
returnBuffer
(
this
.
currentBlockBuffer
)
;
}
catch
(
InterruptedException
e
)
{
private
void
storeFileWithRetry
(
String
key
,
InputStream
inputStream
,
byte
[
]
md5Hash
,
long
length
)
throws
IOException
{
try
{
ObjectMetadata
objectMetadata
=
new
ObjectMetadata
(
)
;
objectMetadata
.
setContentMD5
(
Base64
.
encodeAsString
(
md5Hash
)
)
;
objectMetadata
.
setContentLength
(
length
)
;
PutObjectRequest
putObjectRequest
=
new
PutObjectRequest
(
bucketName
,
key
,
inputStream
,
objectMetadata
)
;
PutObjectResult
putObjectResult
=
(
PutObjectResult
)
callCOSClientWithRetry
(
putObjectRequest
)
;
@
Override
public
void
storeFile
(
String
key
,
File
file
,
byte
[
]
md5Hash
)
throws
IOException
{
@
Override
public
void
storeFile
(
String
key
,
InputStream
inputStream
,
byte
[
]
md5Hash
,
long
contentLength
)
throws
IOException
{
public
void
abortMultipartUpload
(
String
key
,
String
uploadId
)
{
@
Override
public
InputStream
retrieve
(
String
key
)
throws
IOException
{
@
Override
public
InputStream
retrieve
(
String
key
,
long
byteRangeStart
)
throws
IOException
{
try
{
private
PartialListing
list
(
String
prefix
,
String
delimiter
,
int
maxListingLength
,
String
priorLastKey
)
throws
IOException
{
private
PartialListing
list
(
String
prefix
,
String
delimiter
,
int
maxListingLength
,
String
priorLastKey
)
throws
IOException
{
LOG
.
debug
(
+
,
prefix
,
delimiter
,
maxListingLength
,
priorLastKey
)
;
if
(
!
prefix
.
startsWith
(
CosNFileSystem
.
PATH_DELIMITER
)
)
{
prefix
+=
CosNFileSystem
.
PATH_DELIMITER
;
}
ListObjectsRequest
listObjectsRequest
=
new
ListObjectsRequest
(
)
;
listObjectsRequest
.
setBucketName
(
bucketName
)
;
listObjectsRequest
.
setPrefix
(
prefix
)
;
listObjectsRequest
.
setDelimiter
(
delimiter
)
;
listObjectsRequest
.
setMarker
(
priorLastKey
)
;
listObjectsRequest
.
setMaxKeys
(
maxListingLength
)
;
ObjectListing
objectListing
=
null
;
try
{
objectListing
=
(
ObjectListing
)
callCOSClientWithRetry
(
listObjectsRequest
)
;
}
catch
(
Exception
e
)
{
String
errMsg
=
String
.
format
(
+
+
,
prefix
,
(
delimiter
==
null
)
?
:
delimiter
,
maxListingLength
,
priorLastKey
,
e
.
toString
(
)
)
;
@
Override
public
void
delete
(
String
key
)
throws
IOException
{
public
void
rename
(
String
srcKey
,
String
dstKey
)
throws
IOException
{
@
Override
public
void
copy
(
String
srcKey
,
String
dstKey
)
throws
IOException
{
@
Override
public
long
getFileLength
(
String
key
)
throws
IOException
{
else
if
(
request
instanceof
ListObjectsRequest
)
{
sdkMethod
=
;
return
this
.
cosClient
.
listObjects
(
(
ListObjectsRequest
)
request
)
;
}
else
{
throw
new
IOException
(
)
;
}
}
catch
(
CosServiceException
cse
)
{
String
errMsg
=
String
.
format
(
+
+
,
retryIndex
,
this
.
maxRetryTimes
,
sdkMethod
,
cse
.
toString
(
)
)
;
int
statusCode
=
cse
.
getStatusCode
(
)
;
if
(
statusCode
/
100
==
5
)
{
if
(
retryIndex
<=
this
.
maxRetryTimes
)
{
LOG
.
info
(
errMsg
)
;
long
sleepLeast
=
retryIndex
*
300L
;
long
sleepBound
=
retryIndex
*
500L
;
try
{
if
(
request
instanceof
UploadPartRequest
)
{
}
}
catch
(
CosServiceException
cse
)
{
String
errMsg
=
String
.
format
(
+
+
,
retryIndex
,
this
.
maxRetryTimes
,
sdkMethod
,
cse
.
toString
(
)
)
;
int
statusCode
=
cse
.
getStatusCode
(
)
;
if
(
statusCode
/
100
==
5
)
{
if
(
retryIndex
<=
this
.
maxRetryTimes
)
{
LOG
.
info
(
errMsg
)
;
long
sleepLeast
=
retryIndex
*
300L
;
long
sleepBound
=
retryIndex
*
500L
;
try
{
if
(
request
instanceof
UploadPartRequest
)
{
if
(
(
(
UploadPartRequest
)
request
)
.
getInputStream
(
)
instanceof
ByteBufferInputStream
)
{
(
(
UploadPartRequest
)
request
)
.
getInputStream
(
)
.
reset
(
)
;
}
}
Thread
.
sleep
(
ThreadLocalRandom
.
current
(
)
.
nextLong
(
sleepLeast
,
sleepBound
)
)
;
++
retryIndex
;
}
catch
(
InterruptedException
e
)
{
@
Test
public
void
testSeek
(
)
throws
Exception
{
Path
seekTestFilePath
=
new
Path
(
this
.
testRootDir
+
+
)
;
long
fileSize
=
5
*
Unit
.
MB
;
ContractTestUtils
.
generateTestFile
(
this
.
fs
,
seekTestFilePath
,
fileSize
,
256
,
255
)
;
LOG
.
info
(
)
;
FSDataInputStream
inputStream
=
this
.
fs
.
open
(
seekTestFilePath
)
;
int
seekTimes
=
5
;
for
(
int
i
=
0
;
i
!=
seekTimes
;
i
++
)
{
long
pos
=
fileSize
/
(
seekTimes
-
i
)
-
1
;
inputStream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
inputStream
.
getPos
(
)
,
inputStream
.
getPos
(
)
==
pos
)
;
Path
seekTestFilePath
=
new
Path
(
this
.
testRootDir
+
+
)
;
long
fileSize
=
5
*
Unit
.
MB
;
ContractTestUtils
.
generateTestFile
(
this
.
fs
,
seekTestFilePath
,
fileSize
,
256
,
255
)
;
LOG
.
info
(
)
;
FSDataInputStream
inputStream
=
this
.
fs
.
open
(
seekTestFilePath
)
;
int
seekTimes
=
5
;
for
(
int
i
=
0
;
i
!=
seekTimes
;
i
++
)
{
long
pos
=
fileSize
/
(
seekTimes
-
i
)
-
1
;
inputStream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
inputStream
.
getPos
(
)
,
inputStream
.
getPos
(
)
==
pos
)
;
LOG
.
info
(
+
inputStream
.
getPos
(
)
)
;
}
LOG
.
info
(
)
;
Random
random
=
new
Random
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
Math
.
abs
(
random
.
nextLong
(
)
)
%
fileSize
;
LOG
.
info
(
)
;
FSDataInputStream
inputStream
=
this
.
fs
.
open
(
seekTestFilePath
)
;
int
seekTimes
=
5
;
for
(
int
i
=
0
;
i
!=
seekTimes
;
i
++
)
{
long
pos
=
fileSize
/
(
seekTimes
-
i
)
-
1
;
inputStream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
inputStream
.
getPos
(
)
,
inputStream
.
getPos
(
)
==
pos
)
;
LOG
.
info
(
+
inputStream
.
getPos
(
)
)
;
}
LOG
.
info
(
)
;
Random
random
=
new
Random
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
Math
.
abs
(
random
.
nextLong
(
)
)
%
fileSize
;
LOG
.
info
(
+
pos
)
;
inputStream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
inputStream
.
getPos
(
)
,
inputStream
.
getPos
(
)
==
pos
)
;
@
Test
public
void
testRead
(
)
throws
Exception
{
final
int
bufLen
=
256
;
Path
readTestFilePath
=
new
Path
(
this
.
testRootDir
+
+
)
;
long
fileSize
=
5
*
Unit
.
MB
;
ContractTestUtils
.
generateTestFile
(
this
.
fs
,
readTestFilePath
,
fileSize
,
256
,
255
)
;
LOG
.
info
(
+
readTestFilePath
+
)
;
FSDataInputStream
inputStream
=
this
.
fs
.
open
(
readTestFilePath
)
;
byte
[
]
buf
=
new
byte
[
bufLen
]
;
long
bytesRead
=
0
;
while
(
bytesRead
<
fileSize
)
{
int
bytes
=
0
;
if
(
fileSize
-
bytesRead
<
bufLen
)
{
int
remaining
=
(
int
)
(
fileSize
-
bytesRead
)
;
bytes
=
inputStream
.
read
(
buf
,
0
,
remaining
)
;
}
else
{
bytes
=
inputStream
.
read
(
buf
,
0
,
bufLen
)
;
}
bytesRead
+=
bytes
;
if
(
bytesRead
%
(
1
*
Unit
.
MB
)
==
0
)
{
int
available
=
inputStream
.
available
(
)
;
assertTrue
(
+
(
fileSize
-
bytesRead
)
+
+
available
,
(
fileSize
-
bytesRead
)
==
available
)
;
@
Override
public
void
doFilter
(
ServletRequest
request
,
ServletResponse
response
,
FilterChain
filterChain
)
throws
IOException
,
ServletException
{
boolean
unauthorizedResponse
=
true
;
int
errCode
=
HttpServletResponse
.
SC_UNAUTHORIZED
;
AuthenticationException
authenticationEx
=
null
;
HttpServletRequest
httpRequest
=
(
HttpServletRequest
)
request
;
HttpServletResponse
httpResponse
=
(
HttpServletResponse
)
response
;
boolean
isHttps
=
.
equals
(
httpRequest
.
getScheme
(
)
)
;
try
{
boolean
newToken
=
false
;
AuthenticationToken
token
;
try
{
token
=
getToken
(
httpRequest
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
boolean
isHttps
=
.
equals
(
httpRequest
.
getScheme
(
)
)
;
try
{
boolean
newToken
=
false
;
AuthenticationToken
token
;
try
{
token
=
getToken
(
httpRequest
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
token
,
getRequestURL
(
httpRequest
)
)
;
}
}
catch
(
AuthenticationException
ex
)
{
LOG
.
warn
(
+
ex
.
getMessage
(
)
)
;
authenticationEx
=
ex
;
token
=
null
;
}
if
(
authHandler
.
managementOperation
(
token
,
httpRequest
,
httpResponse
)
)
{
if
(
token
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
warn
(
+
ex
.
getMessage
(
)
)
;
authenticationEx
=
ex
;
token
=
null
;
}
if
(
authHandler
.
managementOperation
(
token
,
httpRequest
,
httpResponse
)
)
{
if
(
token
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
getRequestURL
(
httpRequest
)
,
authHandler
.
getClass
(
)
)
;
}
token
=
authHandler
.
authenticate
(
httpRequest
,
httpResponse
)
;
if
(
token
!=
null
&&
token
!=
AuthenticationToken
.
ANONYMOUS
)
{
if
(
token
.
getMaxInactives
(
)
>
0
)
{
token
.
setMaxInactives
(
System
.
currentTimeMillis
(
)
+
getMaxInactiveInterval
(
)
*
1000
)
;
}
if
(
token
.
getExpires
(
)
!=
0
)
{
token
.
setExpires
(
System
.
currentTimeMillis
(
)
+
getValidity
(
)
*
1000
)
;
}
}
newToken
=
true
;
}
if
(
token
!=
null
)
{
if
(
token
!=
null
)
{
unauthorizedResponse
=
false
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
getRequestURL
(
httpRequest
)
,
token
.
getUserName
(
)
)
;
}
final
AuthenticationToken
authToken
=
token
;
httpRequest
=
new
HttpServletRequestWrapper
(
httpRequest
)
{
@
Override
public
String
getAuthType
(
)
{
return
authToken
.
getType
(
)
;
}
@
Override
public
String
getRemoteUser
(
)
{
return
authToken
.
getUserName
(
)
;
}
@
Override
public
Principal
getUserPrincipal
(
)
{
return
(
authToken
!=
AuthenticationToken
.
ANONYMOUS
)
?
authToken
:
null
;
}
}
;
if
(
!
newToken
&&
!
isCookiePersistent
(
)
&&
getMaxInactiveInterval
(
)
>
0
)
{
token
.
setMaxInactives
(
System
.
currentTimeMillis
(
)
+
getMaxInactiveInterval
(
)
*
1000
)
;
@
Override
public
AuthenticationToken
alternateAuthenticate
(
HttpServletRequest
request
,
HttpServletResponse
response
)
throws
IOException
,
AuthenticationException
{
AuthenticationToken
token
=
null
;
String
serializedJWT
=
null
;
HttpServletRequest
req
=
(
HttpServletRequest
)
request
;
serializedJWT
=
getJWTFromCookie
(
req
)
;
if
(
serializedJWT
==
null
)
{
String
loginURL
=
constructLoginURL
(
request
)
;
HttpServletRequest
req
=
(
HttpServletRequest
)
request
;
serializedJWT
=
getJWTFromCookie
(
req
)
;
if
(
serializedJWT
==
null
)
{
String
loginURL
=
constructLoginURL
(
request
)
;
LOG
.
info
(
+
loginURL
)
;
(
(
HttpServletResponse
)
response
)
.
sendRedirect
(
loginURL
)
;
}
else
{
String
userName
=
null
;
SignedJWT
jwtToken
=
null
;
boolean
valid
=
false
;
try
{
jwtToken
=
SignedJWT
.
parse
(
serializedJWT
)
;
valid
=
validateToken
(
jwtToken
)
;
if
(
valid
)
{
userName
=
jwtToken
.
getJWTClaimsSet
(
)
.
getSubject
(
)
;
protected
String
getJWTFromCookie
(
HttpServletRequest
req
)
{
String
serializedJWT
=
null
;
Cookie
[
]
cookies
=
req
.
getCookies
(
)
;
if
(
cookies
!=
null
)
{
for
(
Cookie
cookie
:
cookies
)
{
if
(
cookieName
.
equals
(
cookie
.
getName
(
)
)
)
{
if
(
keytab
==
null
||
keytab
.
trim
(
)
.
length
(
)
==
0
)
{
throw
new
ServletException
(
)
;
}
File
keytabFile
=
new
File
(
keytab
)
;
if
(
!
keytabFile
.
exists
(
)
)
{
throw
new
ServletException
(
+
keytab
)
;
}
final
String
[
]
spnegoPrincipals
;
if
(
principal
.
equals
(
)
)
{
spnegoPrincipals
=
KerberosUtil
.
getPrincipalNames
(
keytab
,
Pattern
.
compile
(
)
)
;
if
(
spnegoPrincipals
.
length
==
0
)
{
throw
new
ServletException
(
)
;
}
}
else
{
spnegoPrincipals
=
new
String
[
]
{
principal
}
;
}
KeyTab
keytabInstance
=
KeyTab
.
getInstance
(
keytabFile
)
;
serverSubject
.
getPrivateCredentials
(
)
.
add
(
keytabInstance
)
;
for
(
String
spnegoPrincipal
:
spnegoPrincipals
)
{
env
.
put
(
Context
.
INITIAL_CONTEXT_FACTORY
,
)
;
env
.
put
(
Context
.
PROVIDER_URL
,
providerUrl
)
;
try
{
ctx
=
new
InitialLdapContext
(
env
,
null
)
;
StartTlsResponse
tls
=
(
StartTlsResponse
)
ctx
.
extendedOperation
(
new
StartTlsRequest
(
)
)
;
if
(
disableHostNameVerification
)
{
tls
.
setHostnameVerifier
(
new
HostnameVerifier
(
)
{
@
Override
public
boolean
verify
(
String
hostname
,
SSLSession
session
)
{
return
true
;
}
}
)
;
}
tls
.
negotiate
(
)
;
ctx
.
addToEnvironment
(
Context
.
SECURITY_AUTHENTICATION
,
SECURITY_AUTHENTICATION
)
;
ctx
.
addToEnvironment
(
Context
.
SECURITY_PRINCIPAL
,
userDN
)
;
ctx
.
addToEnvironment
(
Context
.
SECURITY_CREDENTIALS
,
password
)
;
ctx
.
lookup
(
userDN
)
;
private
void
authenticateWithoutTlsExtension
(
String
userDN
,
String
password
)
throws
AuthenticationException
{
Hashtable
<
String
,
Object
>
env
=
new
Hashtable
<
String
,
Object
>
(
)
;
env
.
put
(
Context
.
INITIAL_CONTEXT_FACTORY
,
)
;
env
.
put
(
Context
.
PROVIDER_URL
,
providerUrl
)
;
env
.
put
(
Context
.
SECURITY_AUTHENTICATION
,
SECURITY_AUTHENTICATION
)
;
env
.
put
(
Context
.
SECURITY_PRINCIPAL
,
userDN
)
;
env
.
put
(
Context
.
SECURITY_CREDENTIALS
,
password
)
;
try
{
Context
ctx
=
new
InitialDirContext
(
env
)
;
ctx
.
close
(
)
;
@
Override
public
void
init
(
Properties
config
)
throws
ServletException
{
for
(
Map
.
Entry
prop
:
config
.
entrySet
(
)
)
{
protected
AuthenticationHandler
initializeAuthHandler
(
String
authHandlerClassName
,
Properties
config
)
throws
ServletException
{
try
{
Preconditions
.
checkNotNull
(
authHandlerClassName
)
;
protected
AuthenticationHandler
initializeAuthHandler
(
String
authHandlerClassName
,
Properties
config
)
throws
ServletException
{
try
{
Preconditions
.
checkNotNull
(
authHandlerClassName
)
;
logger
.
debug
(
+
authHandlerClassName
)
;
Class
<
?
>
klass
=
Thread
.
currentThread
(
)
.
getContextClassLoader
(
)
.
loadClass
(
authHandlerClassName
)
;
AuthenticationHandler
authHandler
=
(
AuthenticationHandler
)
klass
.
newInstance
(
)
;
authHandler
.
init
(
config
)
;
@
VisibleForTesting
void
logDeprecation
(
String
message
)
{
void
logDeprecationOnce
(
String
name
,
String
source
)
{
DeprecatedKeyInfo
keyInfo
=
getDeprecatedKeyInfo
(
name
)
;
if
(
keyInfo
!=
null
&&
!
keyInfo
.
getAndSetAccessed
(
)
)
{
public
InputStream
getConfResourceAsInputStream
(
String
name
)
{
try
{
URL
url
=
getResource
(
name
)
;
if
(
url
==
null
)
{
public
Reader
getConfResourceAsReader
(
String
name
)
{
try
{
URL
url
=
getResource
(
name
)
;
if
(
url
==
null
)
{
private
XMLStreamReader
parse
(
URL
url
,
boolean
restricted
)
throws
IOException
,
XMLStreamException
{
if
(
!
quietmode
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
XMLStreamReader
parse
(
InputStream
is
,
String
systemIdStr
,
boolean
restricted
)
throws
IOException
,
XMLStreamException
{
if
(
!
quietmode
)
{
XMLStreamReader2
reader
=
getStreamReader
(
wrapper
,
quiet
)
;
if
(
reader
==
null
)
{
if
(
quiet
)
{
return
null
;
}
throw
new
RuntimeException
(
resource
+
)
;
}
Properties
toAddTo
=
properties
;
if
(
returnCachedProperties
)
{
toAddTo
=
new
Properties
(
)
;
}
List
<
ParsedItem
>
items
=
new
Parser
(
reader
,
wrapper
,
quiet
)
.
parse
(
)
;
for
(
ParsedItem
item
:
items
)
{
loadProperty
(
toAddTo
,
item
.
name
,
item
.
key
,
item
.
value
,
item
.
isFinal
,
item
.
sources
)
;
}
reader
.
close
(
)
;
if
(
returnCachedProperties
)
{
overlay
(
properties
,
toAddTo
)
;
return
new
Resource
(
toAddTo
,
name
,
wrapper
.
isParserRestricted
(
)
)
;
return
null
;
}
throw
new
RuntimeException
(
resource
+
)
;
}
Properties
toAddTo
=
properties
;
if
(
returnCachedProperties
)
{
toAddTo
=
new
Properties
(
)
;
}
List
<
ParsedItem
>
items
=
new
Parser
(
reader
,
wrapper
,
quiet
)
.
parse
(
)
;
for
(
ParsedItem
item
:
items
)
{
loadProperty
(
toAddTo
,
item
.
name
,
item
.
key
,
item
.
value
,
item
.
isFinal
,
item
.
sources
)
;
}
reader
.
close
(
)
;
if
(
returnCachedProperties
)
{
overlay
(
properties
,
toAddTo
)
;
return
new
Resource
(
toAddTo
,
name
,
wrapper
.
isParserRestricted
(
)
)
;
}
return
null
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
name
,
e
)
;
@
Override
public
String
get
(
String
name
)
{
String
value
=
super
.
get
(
name
)
;
@
Override
public
String
get
(
String
name
,
String
defaultValue
)
{
String
value
=
super
.
get
(
name
,
defaultValue
)
;
@
Override
public
boolean
getBoolean
(
String
name
,
boolean
defaultValue
)
{
boolean
value
=
super
.
getBoolean
(
name
,
defaultValue
)
;
@
Override
public
float
getFloat
(
String
name
,
float
defaultValue
)
{
float
value
=
super
.
getFloat
(
name
,
defaultValue
)
;
@
Override
public
int
getInt
(
String
name
,
int
defaultValue
)
{
int
value
=
super
.
getInt
(
name
,
defaultValue
)
;
@
Override
public
long
getLong
(
String
name
,
long
defaultValue
)
{
long
value
=
super
.
getLong
(
name
,
defaultValue
)
;
@
Override
public
void
set
(
String
name
,
String
value
,
String
source
)
{
@
Override
public
final
void
reconfigureProperty
(
String
property
,
String
newVal
)
throws
ReconfigurationException
{
if
(
isPropertyReconfigurable
(
property
)
)
{
private
Reconfigurable
getReconfigurable
(
HttpServletRequest
req
)
{
private
Reconfigurable
getReconfigurable
(
HttpServletRequest
req
)
{
LOG
.
info
(
+
req
.
getServletPath
(
)
)
;
String
rawParam
=
params
.
nextElement
(
)
;
String
param
=
StringEscapeUtils
.
unescapeHtml4
(
rawParam
)
;
String
value
=
StringEscapeUtils
.
unescapeHtml4
(
req
.
getParameter
(
rawParam
)
)
;
if
(
value
!=
null
)
{
if
(
value
.
equals
(
newConf
.
getRaw
(
param
)
)
||
value
.
equals
(
)
||
value
.
equals
(
)
||
value
.
isEmpty
(
)
)
{
if
(
(
value
.
equals
(
)
||
value
.
equals
(
)
||
value
.
isEmpty
(
)
)
&&
oldConf
.
getRaw
(
param
)
!=
null
)
{
out
.
println
(
+
StringEscapeUtils
.
escapeHtml4
(
param
)
+
+
StringEscapeUtils
.
escapeHtml4
(
oldConf
.
getRaw
(
param
)
)
+
)
;
reconf
.
reconfigureProperty
(
param
,
null
)
;
}
else
if
(
!
value
.
equals
(
)
&&
!
value
.
equals
(
)
&&
!
value
.
isEmpty
(
)
&&
(
oldConf
.
getRaw
(
param
)
==
null
||
!
oldConf
.
getRaw
(
param
)
.
equals
(
value
)
)
)
{
if
(
oldConf
.
getRaw
(
param
)
==
null
)
{
out
.
println
(
+
StringEscapeUtils
.
escapeHtml4
(
param
)
+
+
StringEscapeUtils
.
escapeHtml4
(
value
)
+
)
;
}
else
{
out
.
println
(
+
StringEscapeUtils
.
escapeHtml4
(
param
)
+
+
StringEscapeUtils
.
escapeHtml4
(
oldConf
.
getRaw
(
param
)
)
+
+
StringEscapeUtils
.
escapeHtml4
(
value
)
+
)
;
}
reconf
.
reconfigureProperty
(
param
,
value
)
;
}
else
{
@
Override
public
void
setConf
(
Configuration
conf
)
{
this
.
conf
=
conf
;
final
Class
<
?
extends
Random
>
klass
=
conf
.
getClass
(
HADOOP_SECURITY_SECURE_RANDOM_IMPL_KEY
,
OpensslSecureRandom
.
class
,
Random
.
class
)
;
try
{
random
=
ReflectionUtils
.
newInstance
(
klass
,
conf
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
FsPermission
loadAndReturnPerm
(
Path
pathToLoad
,
Path
pathToDelete
)
throws
NoSuchAlgorithmException
,
CertificateException
,
IOException
{
FsPermission
perm
=
null
;
try
{
perm
=
loadFromPath
(
pathToLoad
,
password
)
;
renameOrFail
(
pathToLoad
,
path
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
<
T
>
T
call
(
HttpURLConnection
conn
,
Object
jsonOutput
,
int
expectedResponse
,
Class
<
T
>
klass
,
int
authRetryCount
)
throws
IOException
{
T
ret
=
null
;
OutputStream
os
=
null
;
try
{
if
(
jsonOutput
!=
null
)
{
os
=
conn
.
getOutputStream
(
)
;
writeJson
(
jsonOutput
,
os
)
;
}
}
catch
(
IOException
ex
)
{
if
(
os
==
null
)
{
conn
.
disconnect
(
)
;
}
else
{
IOUtils
.
closeStream
(
conn
.
getInputStream
(
)
)
;
}
throw
ex
;
}
if
(
(
conn
.
getResponseCode
(
)
==
HttpURLConnection
.
HTTP_FORBIDDEN
&&
(
conn
.
getResponseMessage
(
)
.
equals
(
ANONYMOUS_REQUESTS_DISALLOWED
)
||
conn
.
getResponseMessage
(
)
.
contains
(
INVALID_SIGNATURE
)
)
)
||
conn
.
getResponseCode
(
)
==
HttpURLConnection
.
HTTP_UNAUTHORIZED
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
VisibleForTesting
DelegationTokenAuthenticatedURL
createAuthenticatedURL
(
)
{
return
new
DelegationTokenAuthenticatedURL
(
configurator
)
{
@
Override
public
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
extends
TokenIdentifier
>
selectDelegationToken
(
URL
url
,
Credentials
creds
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
static
Token
<
?
>
selectDelegationToken
(
Credentials
creds
,
Text
service
)
{
Token
<
?
>
token
=
creds
.
getToken
(
service
)
;
@
Override
public
Token
<
?
>
getDelegationToken
(
final
String
renewer
)
throws
IOException
{
final
URL
url
=
createURL
(
null
,
null
,
null
,
null
)
;
final
DelegationTokenAuthenticatedURL
authUrl
=
new
DelegationTokenAuthenticatedURL
(
configurator
)
;
Token
<
?
>
token
=
null
;
try
{
final
String
doAsUser
=
getDoAsUser
(
)
;
token
=
getActualUgi
(
)
.
doAs
(
new
PrivilegedExceptionAction
<
Token
<
?
>>
(
)
{
@
Override
public
Token
<
?
>
run
(
)
throws
Exception
{
@
Override
public
long
renewDelegationToken
(
final
Token
<
?
>
dToken
)
throws
IOException
{
try
{
final
String
doAsUser
=
getDoAsUser
(
)
;
final
DelegationTokenAuthenticatedURL
.
Token
token
=
generateDelegationToken
(
dToken
)
;
final
URL
url
=
createURL
(
null
,
null
,
null
,
null
)
;
@
Override
public
Void
cancelDelegationToken
(
final
Token
<
?
>
dToken
)
throws
IOException
{
try
{
final
String
doAsUser
=
getDoAsUser
(
)
;
final
DelegationTokenAuthenticatedURL
.
Token
token
=
generateDelegationToken
(
dToken
)
;
return
getActualUgi
(
)
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
final
URL
url
=
createURL
(
null
,
null
,
null
,
null
)
;
private
boolean
containsKmsDt
(
UserGroupInformation
ugi
)
throws
IOException
{
Credentials
creds
=
ugi
.
getCredentials
(
)
;
if
(
!
creds
.
getAllTokens
(
)
.
isEmpty
(
)
)
{
@
Override
public
Token
<
?
>
getDelegationToken
(
String
renewer
)
throws
IOException
{
return
doOp
(
new
ProviderCallable
<
Token
<
?
>>
(
)
{
@
Override
public
Token
<
?
>
call
(
KMSClientProvider
provider
)
throws
IOException
{
Token
<
?
>
token
=
provider
.
getDelegationToken
(
renewer
)
;
token
.
setService
(
dtService
)
;
private
int
readChecksumChunk
(
byte
b
[
]
,
final
int
off
,
final
int
len
)
throws
IOException
{
count
=
pos
=
0
;
int
read
=
0
;
boolean
retry
=
true
;
int
retriesLeft
=
numOfRetries
;
do
{
retriesLeft
--
;
try
{
read
=
readChunk
(
chunkPos
,
b
,
off
,
len
,
checksum
)
;
if
(
read
>
0
)
{
if
(
needChecksum
(
)
)
{
verifySums
(
b
,
off
,
read
)
;
}
chunkPos
+=
read
;
}
retry
=
false
;
}
catch
(
ChecksumException
ce
)
{
private
static
void
loadFileSystems
(
)
{
LOGGER
.
debug
(
)
;
synchronized
(
FileSystem
.
class
)
{
if
(
!
FILE_SYSTEMS_LOADED
)
{
ServiceLoader
<
FileSystem
>
serviceLoader
=
ServiceLoader
.
load
(
FileSystem
.
class
)
;
Iterator
<
FileSystem
>
it
=
serviceLoader
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
FileSystem
fs
;
try
{
fs
=
it
.
next
(
)
;
try
{
SERVICE_FILE_SYSTEMS
.
put
(
fs
.
getScheme
(
)
,
fs
.
getClass
(
)
)
;
if
(
LOGGER
.
isDebugEnabled
(
)
)
{
LOGGER
.
debug
(
)
;
synchronized
(
FileSystem
.
class
)
{
if
(
!
FILE_SYSTEMS_LOADED
)
{
ServiceLoader
<
FileSystem
>
serviceLoader
=
ServiceLoader
.
load
(
FileSystem
.
class
)
;
Iterator
<
FileSystem
>
it
=
serviceLoader
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
FileSystem
fs
;
try
{
fs
=
it
.
next
(
)
;
try
{
SERVICE_FILE_SYSTEMS
.
put
(
fs
.
getScheme
(
)
,
fs
.
getClass
(
)
)
;
if
(
LOGGER
.
isDebugEnabled
(
)
)
{
LOGGER
.
debug
(
,
fs
.
getScheme
(
)
,
fs
.
getClass
(
)
,
ClassUtil
.
findContainingJar
(
fs
.
getClass
(
)
)
)
;
}
}
catch
(
Exception
e
)
{
LOGGER
.
warn
(
,
fs
,
ClassUtil
.
findContainingJar
(
fs
.
getClass
(
)
)
)
;
public
static
Class
<
?
extends
FileSystem
>
getFileSystemClass
(
String
scheme
,
Configuration
conf
)
throws
IOException
{
if
(
!
FILE_SYSTEMS_LOADED
)
{
loadFileSystems
(
)
;
}
LOGGER
.
debug
(
,
scheme
)
;
Class
<
?
extends
FileSystem
>
clazz
=
null
;
if
(
conf
!=
null
)
{
String
property
=
+
scheme
+
;
LOGGER
.
debug
(
,
property
)
;
clazz
=
(
Class
<
?
extends
FileSystem
>
)
conf
.
getClass
(
property
,
null
)
;
}
else
{
LOGGER
.
debug
(
,
scheme
)
;
}
if
(
clazz
==
null
)
{
LOGGER
.
debug
(
)
;
clazz
=
SERVICE_FILE_SYSTEMS
.
get
(
scheme
)
;
}
else
{
LOGGER
.
debug
(
,
scheme
)
;
Class
<
?
extends
FileSystem
>
clazz
=
null
;
if
(
conf
!=
null
)
{
String
property
=
+
scheme
+
;
LOGGER
.
debug
(
,
property
)
;
clazz
=
(
Class
<
?
extends
FileSystem
>
)
conf
.
getClass
(
property
,
null
)
;
}
else
{
LOGGER
.
debug
(
,
scheme
)
;
}
if
(
clazz
==
null
)
{
LOGGER
.
debug
(
)
;
clazz
=
SERVICE_FILE_SYSTEMS
.
get
(
scheme
)
;
}
else
{
LOGGER
.
debug
(
,
scheme
)
;
}
if
(
clazz
==
null
)
{
throw
new
UnsupportedFileSystemException
(
+
+
scheme
+
)
;
private
static
void
runCommandOnStream
(
InputStream
inputStream
,
String
command
)
throws
IOException
,
InterruptedException
,
ExecutionException
{
ExecutorService
executor
=
null
;
ProcessBuilder
builder
=
new
ProcessBuilder
(
)
;
builder
.
command
(
Shell
.
WINDOWS
?
:
,
Shell
.
WINDOWS
?
:
,
command
)
;
Process
process
=
builder
.
start
(
)
;
int
exitCode
;
try
{
executor
=
Executors
.
newFixedThreadPool
(
2
)
;
Future
output
=
executor
.
submit
(
(
)
->
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
(
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
process
.
getInputStream
(
)
,
Charset
.
forName
(
)
)
)
)
{
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
builder
.
command
(
Shell
.
WINDOWS
?
:
,
Shell
.
WINDOWS
?
:
,
command
)
;
Process
process
=
builder
.
start
(
)
;
int
exitCode
;
try
{
executor
=
Executors
.
newFixedThreadPool
(
2
)
;
Future
output
=
executor
.
submit
(
(
)
->
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
(
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
process
.
getInputStream
(
)
,
Charset
.
forName
(
)
)
)
)
{
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
LOG
.
debug
(
line
)
;
}
}
}
else
{
org
.
apache
.
commons
.
io
.
IOUtils
.
copy
(
process
.
getInputStream
(
)
,
new
IOUtils
.
NullOutputStream
(
)
)
;
}
}
catch
(
IOException
e
)
{
try
(
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
process
.
getInputStream
(
)
,
Charset
.
forName
(
)
)
)
)
{
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
LOG
.
debug
(
line
)
;
}
}
}
else
{
org
.
apache
.
commons
.
io
.
IOUtils
.
copy
(
process
.
getInputStream
(
)
,
new
IOUtils
.
NullOutputStream
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
debug
(
e
.
getMessage
(
)
)
;
}
}
)
;
Future
error
=
executor
.
submit
(
(
)
->
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
(
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
process
.
getErrorStream
(
)
,
Charset
.
forName
(
)
)
)
)
{
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
}
}
}
else
{
org
.
apache
.
commons
.
io
.
IOUtils
.
copy
(
process
.
getInputStream
(
)
,
new
IOUtils
.
NullOutputStream
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
debug
(
e
.
getMessage
(
)
)
;
}
}
)
;
Future
error
=
executor
.
submit
(
(
)
->
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
(
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
process
.
getErrorStream
(
)
,
Charset
.
forName
(
)
)
)
)
{
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
LOG
.
debug
(
line
)
;
}
}
}
else
{
org
.
apache
.
commons
.
io
.
IOUtils
.
copy
(
process
.
getErrorStream
(
)
,
new
IOUtils
.
NullOutputStream
(
)
)
;
}
}
catch
(
IOException
e
)
{
ShellCommandExecutor
shExec
;
try
{
if
(
Shell
.
WINDOWS
&&
linkFile
.
getParentFile
(
)
!=
null
&&
!
new
Path
(
target
)
.
isAbsolute
(
)
)
{
shExec
=
new
ShellCommandExecutor
(
cmd
,
linkFile
.
getParentFile
(
)
)
;
}
else
{
shExec
=
new
ShellCommandExecutor
(
cmd
)
;
}
shExec
.
execute
(
)
;
}
catch
(
Shell
.
ExitCodeException
ec
)
{
int
returnVal
=
ec
.
getExitCode
(
)
;
if
(
Shell
.
WINDOWS
&&
returnVal
==
SYMLINK_NO_PRIVILEGE
)
{
LOG
.
warn
(
+
+
+
)
;
}
else
if
(
returnVal
!=
0
)
{
LOG
.
warn
(
+
StringUtils
.
join
(
,
cmd
)
+
+
returnVal
+
+
ec
.
getMessage
(
)
)
;
}
return
returnVal
;
}
catch
(
IOException
e
)
{
URI
srcUri
=
srcFs
.
getUri
(
)
;
URI
dstUri
=
destFs
.
getUri
(
)
;
if
(
srcUri
.
getScheme
(
)
==
null
)
{
return
false
;
}
if
(
!
srcUri
.
getScheme
(
)
.
equals
(
dstUri
.
getScheme
(
)
)
)
{
return
false
;
}
String
srcHost
=
srcUri
.
getHost
(
)
;
String
dstHost
=
dstUri
.
getHost
(
)
;
if
(
(
srcHost
!=
null
)
&&
(
dstHost
!=
null
)
)
{
if
(
srcHost
.
equals
(
dstHost
)
)
{
return
srcUri
.
getPort
(
)
==
dstUri
.
getPort
(
)
;
}
try
{
srcHost
=
InetAddress
.
getByName
(
srcHost
)
.
getCanonicalHostName
(
)
;
dstHost
=
InetAddress
.
getByName
(
dstHost
)
.
getCanonicalHostName
(
)
;
}
catch
(
UnknownHostException
ue
)
{
if
(
args
.
length
(
)
>
2048
)
{
args
=
args
.
substring
(
0
,
2048
)
;
}
scope
.
getSpan
(
)
.
addKVAnnotation
(
,
args
)
;
}
try
{
exitCode
=
instance
.
run
(
Arrays
.
copyOfRange
(
argv
,
1
,
argv
.
length
)
)
;
}
finally
{
scope
.
close
(
)
;
}
}
catch
(
IllegalArgumentException
e
)
{
if
(
e
.
getMessage
(
)
==
null
)
{
displayError
(
cmd
,
)
;
e
.
printStackTrace
(
System
.
err
)
;
}
else
{
displayError
(
cmd
,
e
.
getLocalizedMessage
(
)
)
;
}
printUsage
(
System
.
err
)
;
if
(
instance
!=
null
)
{
@
Override
public
java
.
net
.
URLStreamHandler
createURLStreamHandler
(
String
protocol
)
{
@
Override
public
java
.
net
.
URLStreamHandler
createURLStreamHandler
(
String
protocol
)
{
LOG
.
debug
(
,
protocol
)
;
if
(
!
protocols
.
containsKey
(
protocol
)
)
{
boolean
known
=
true
;
try
{
Class
<
?
extends
FileSystem
>
impl
=
FileSystem
.
getFileSystemClass
(
protocol
,
conf
)
;
private
FileStatus
[
]
doGlob
(
)
throws
IOException
{
String
scheme
=
schemeFromPath
(
pathPattern
)
;
String
authority
=
authorityFromPath
(
pathPattern
)
;
String
pathPatternString
=
pathPattern
.
toUri
(
)
.
getPath
(
)
;
List
<
String
>
flattenedPatterns
=
GlobExpander
.
expand
(
pathPatternString
)
;
private
FileStatus
[
]
doGlob
(
)
throws
IOException
{
String
scheme
=
schemeFromPath
(
pathPattern
)
;
String
authority
=
authorityFromPath
(
pathPattern
)
;
String
pathPatternString
=
pathPattern
.
toUri
(
)
.
getPath
(
)
;
List
<
String
>
flattenedPatterns
=
GlobExpander
.
expand
(
pathPatternString
)
;
LOG
.
debug
(
,
pathPatternString
)
;
ArrayList
<
FileStatus
>
results
=
new
ArrayList
<
>
(
flattenedPatterns
.
size
(
)
)
;
boolean
sawWildcard
=
false
;
for
(
String
flatPattern
:
flattenedPatterns
)
{
Path
absPattern
=
fixRelativePart
(
new
Path
(
flatPattern
.
isEmpty
(
)
?
Path
.
CUR_DIR
:
flatPattern
)
)
;
ArrayList
<
FileStatus
>
newCandidates
=
new
ArrayList
<
>
(
candidates
.
size
(
)
)
;
GlobFilter
globFilter
=
new
GlobFilter
(
components
.
get
(
componentIdx
)
)
;
String
component
=
unescapePathComponent
(
components
.
get
(
componentIdx
)
)
;
if
(
globFilter
.
hasPattern
(
)
)
{
sawWildcard
=
true
;
}
LOG
.
debug
(
,
component
,
sawWildcard
)
;
if
(
candidates
.
isEmpty
(
)
&&
sawWildcard
)
{
break
;
}
if
(
(
componentIdx
<
components
.
size
(
)
-
1
)
&&
(
!
globFilter
.
hasPattern
(
)
)
)
{
for
(
FileStatus
candidate
:
candidates
)
{
candidate
.
setPath
(
new
Path
(
candidate
.
getPath
(
)
,
component
)
)
;
}
continue
;
}
for
(
FileStatus
candidate
:
candidates
)
{
if
(
globFilter
.
hasPattern
(
)
)
{
FileStatus
[
]
children
=
listStatus
(
candidate
.
getPath
(
)
)
;
if
(
candidates
.
isEmpty
(
)
&&
sawWildcard
)
{
break
;
}
if
(
(
componentIdx
<
components
.
size
(
)
-
1
)
&&
(
!
globFilter
.
hasPattern
(
)
)
)
{
for
(
FileStatus
candidate
:
candidates
)
{
candidate
.
setPath
(
new
Path
(
candidate
.
getPath
(
)
,
component
)
)
;
}
continue
;
}
for
(
FileStatus
candidate
:
candidates
)
{
if
(
globFilter
.
hasPattern
(
)
)
{
FileStatus
[
]
children
=
listStatus
(
candidate
.
getPath
(
)
)
;
if
(
children
.
length
==
1
)
{
if
(
resolveSymlinks
)
{
LOG
.
debug
(
,
children
[
0
]
)
;
Path
path
=
candidate
.
getPath
(
)
;
FileStatus
status
=
getFileStatus
(
path
)
;
if
(
status
==
null
)
{
@
VisibleForTesting
public
final
boolean
handleEmptyDstDirectoryOnWindows
(
Path
src
,
File
srcFile
,
Path
dst
,
File
dstFile
)
throws
IOException
{
try
{
FileStatus
sdst
=
this
.
getFileStatus
(
dst
)
;
String
[
]
dstFileList
=
dstFile
.
list
(
)
;
if
(
dstFileList
!=
null
)
{
if
(
sdst
.
isDirectory
(
)
&&
dstFileList
.
length
==
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
}
catch
(
FileAlreadyExistsException
e
)
{
Path
existsFilePath
=
baseTrashPath
;
while
(
!
fs
.
exists
(
existsFilePath
)
)
{
existsFilePath
=
existsFilePath
.
getParent
(
)
;
}
baseTrashPath
=
new
Path
(
baseTrashPath
.
toString
(
)
.
replace
(
existsFilePath
.
toString
(
)
,
existsFilePath
.
toString
(
)
+
Time
.
now
(
)
)
)
;
trashPath
=
new
Path
(
baseTrashPath
,
trashPath
.
getName
(
)
)
;
--
i
;
continue
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
baseTrashPath
,
e
)
;
cause
=
e
;
break
;
}
try
{
String
orig
=
trashPath
.
toString
(
)
;
while
(
fs
.
exists
(
trashPath
)
)
{
@
SuppressWarnings
(
)
public
void
createCheckpoint
(
Date
date
)
throws
IOException
{
Collection
<
FileStatus
>
trashRoots
=
fs
.
getTrashRoots
(
false
)
;
for
(
FileStatus
trashRoot
:
trashRoots
)
{
private
void
deleteCheckpoint
(
boolean
deleteImmediately
)
throws
IOException
{
Collection
<
FileStatus
>
trashRoots
=
fs
.
getTrashRoots
(
false
)
;
for
(
FileStatus
trashRoot
:
trashRoots
)
{
private
void
deleteCheckpoint
(
Path
trashRoot
,
boolean
deleteImmediately
)
throws
IOException
{
return
;
}
long
now
=
Time
.
now
(
)
;
for
(
int
i
=
0
;
i
<
dirs
.
length
;
i
++
)
{
Path
path
=
dirs
[
i
]
.
getPath
(
)
;
String
dir
=
path
.
toUri
(
)
.
getPath
(
)
;
String
name
=
path
.
getName
(
)
;
if
(
name
.
equals
(
CURRENT
.
getName
(
)
)
)
{
continue
;
}
long
time
;
try
{
time
=
getTimeFromCheckpoint
(
name
)
;
}
catch
(
ParseException
e
)
{
LOG
.
warn
(
+
dir
+
)
;
continue
;
}
if
(
(
(
now
-
deletionInterval
)
>
time
)
||
deleteImmediately
)
{
continue
;
}
int
curVersion
=
higherVersion
;
try
{
curVersion
=
Integer
.
parseInt
(
nameParts
[
nameParts
.
length
-
2
]
)
;
}
catch
(
NumberFormatException
nfe
)
{
logInvalidFileNameFormat
(
cur
)
;
continue
;
}
if
(
curVersion
>
higherVersion
)
{
higherVersion
=
curVersion
;
lfs
=
curLfs
;
}
}
if
(
lfs
==
null
)
{
LOGGER
.
warn
(
+
+
,
mountTableConfigPath
)
;
return
;
}
Path
latestVersionMountTable
=
lfs
.
getPath
(
)
;
if
(
LOGGER
.
isDebugEnabled
(
)
)
{
}
for
(
final
MRNflyNode
dstNode
:
mrNodes
)
{
if
(
dstNode
.
status
==
null
||
srcNode
.
compareTo
(
dstNode
)
==
0
)
{
continue
;
}
try
{
final
FileStatus
srcStatus
=
srcNode
.
cloneStatus
(
)
;
srcStatus
.
setPath
(
f
)
;
final
Path
tmpPath
=
getNflyTmpPath
(
f
)
;
FileUtil
.
copy
(
srcNode
.
getFs
(
)
,
srcStatus
,
dstNode
.
getFs
(
)
,
tmpPath
,
false
,
true
,
getConf
(
)
)
;
dstNode
.
getFs
(
)
.
delete
(
f
,
false
)
;
if
(
dstNode
.
getFs
(
)
.
rename
(
tmpPath
,
f
)
)
{
try
{
dstNode
.
getFs
(
)
.
setTimes
(
f
,
srcNode
.
status
.
getModificationTime
(
)
,
srcNode
.
status
.
getAccessTime
(
)
)
;
}
finally
{
srcStatus
.
setPath
(
dstNode
.
getFs
(
)
.
makeQualified
(
f
)
)
;
dstNode
.
status
=
srcStatus
;
dstNode
.
getFs
(
)
.
setTimes
(
f
,
srcNode
.
status
.
getModificationTime
(
)
,
srcNode
.
status
.
getAccessTime
(
)
)
;
}
finally
{
srcStatus
.
setPath
(
dstNode
.
getFs
(
)
.
makeQualified
(
f
)
)
;
dstNode
.
status
=
srcStatus
;
}
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
f
+
+
srcNode
+
+
dstNode
+
,
ioe
)
;
}
}
}
if
(
maxMtime
>
0
)
{
final
List
<
MRNflyNode
>
mrList
=
new
ArrayList
<
MRNflyNode
>
(
)
;
for
(
final
MRNflyNode
openNode
:
mrNodes
)
{
if
(
openNode
.
status
!=
null
&&
openNode
.
status
.
getLen
(
)
>=
0L
)
{
if
(
openNode
.
status
.
getModificationTime
(
)
==
maxMtime
)
{
mrList
.
add
(
openNode
)
;
}
}
}
final
MRNflyNode
[
]
readNodes
=
mrList
.
toArray
(
new
MRNflyNode
[
0
]
)
;
topology
.
sortByDistance
(
myNode
,
readNodes
,
readNodes
.
length
)
;
for
(
final
MRNflyNode
rNode
:
readNodes
)
{
@
Override
public
void
initialize
(
URI
theUri
,
Configuration
conf
)
throws
IOException
{
this
.
myUri
=
theUri
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
synchronized
void
processResult
(
int
rc
,
String
path
,
Object
ctx
,
String
name
)
{
if
(
isStaleClient
(
ctx
)
)
return
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
rc
+
+
path
+
+
zkConnectionState
+
+
this
)
;
}
Code
code
=
Code
.
get
(
rc
)
;
if
(
isSuccess
(
code
)
)
{
if
(
becomeActive
(
)
)
{
monitorActiveStatus
(
)
;
}
else
{
reJoinElectionAfterFailureToBecomeActive
(
)
;
}
return
;
}
if
(
isNodeExists
(
code
)
)
{
if
(
createRetryCount
==
0
)
{
becomeStandby
(
)
;
}
monitorActiveStatus
(
)
;
return
;
}
String
errorMessage
=
+
code
.
toString
(
)
+
+
path
;
Code
code
=
Code
.
get
(
rc
)
;
if
(
isSuccess
(
code
)
)
{
if
(
becomeActive
(
)
)
{
monitorActiveStatus
(
)
;
}
else
{
reJoinElectionAfterFailureToBecomeActive
(
)
;
}
return
;
}
if
(
isNodeExists
(
code
)
)
{
if
(
createRetryCount
==
0
)
{
becomeStandby
(
)
;
}
monitorActiveStatus
(
)
;
return
;
}
String
errorMessage
=
+
code
.
toString
(
)
+
+
path
;
LOG
.
debug
(
errorMessage
)
;
if
(
shouldRetry
(
code
)
)
{
synchronized
void
processWatchEvent
(
ZooKeeper
zk
,
WatchedEvent
event
)
{
Event
.
EventType
eventType
=
event
.
getType
(
)
;
if
(
isStaleClient
(
zk
)
)
return
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
reJoinElection
(
0
)
;
break
;
case
SaslAuthenticated
:
LOG
.
info
(
)
;
break
;
default
:
fatalError
(
+
event
.
getState
(
)
)
;
break
;
}
return
;
}
String
path
=
event
.
getPath
(
)
;
if
(
path
!=
null
)
{
switch
(
eventType
)
{
case
NodeDeleted
:
if
(
state
==
State
.
ACTIVE
)
{
enterNeutralMode
(
)
;
}
joinElectionInternal
(
)
;
break
;
case
NodeDataChanged
:
monitorActiveStatus
(
)
;
private
void
fatalError
(
String
errorMessage
)
{
boolean
tryFence
=
true
;
if
(
tryGracefulFence
(
fromSvc
)
)
{
tryFence
=
forceFence
;
}
if
(
tryFence
)
{
if
(
!
fromSvc
.
getFencer
(
)
.
fence
(
fromSvc
,
toSvc
)
)
{
throw
new
FailoverFailedException
(
+
fromSvc
+
)
;
}
}
boolean
failed
=
false
;
Throwable
cause
=
null
;
try
{
HAServiceProtocolHelper
.
transitionToActive
(
toSvc
.
getProxy
(
conf
,
rpcTimeoutToNewActive
)
,
createReqInfo
(
)
)
;
}
catch
(
ServiceFailedException
sfe
)
{
LOG
.
error
(
,
toSvc
,
sfe
.
getMessage
(
)
)
;
failed
=
true
;
cause
=
sfe
;
}
catch
(
IOException
ioe
)
{
Throwable
cause
=
null
;
try
{
HAServiceProtocolHelper
.
transitionToActive
(
toSvc
.
getProxy
(
conf
,
rpcTimeoutToNewActive
)
,
createReqInfo
(
)
)
;
}
catch
(
ServiceFailedException
sfe
)
{
LOG
.
error
(
,
toSvc
,
sfe
.
getMessage
(
)
)
;
failed
=
true
;
cause
=
sfe
;
}
catch
(
IOException
ioe
)
{
LOG
.
error
(
,
toSvc
,
ioe
)
;
failed
=
true
;
cause
=
ioe
;
}
if
(
failed
)
{
String
msg
=
+
toSvc
;
if
(
!
tryFence
)
{
try
{
private
synchronized
void
enterState
(
State
newState
)
{
if
(
newState
!=
state
)
{
LOG
.
info
(
)
;
int
i
=
0
;
for
(
FenceMethodWithArg
method
:
methods
)
{
LOG
.
info
(
+
(
++
i
)
+
+
methods
.
size
(
)
+
+
method
)
;
try
{
boolean
toSvcFencingFailed
=
false
;
if
(
toSvc
!=
null
)
{
toSvcFencingFailed
=
!
method
.
method
.
tryFence
(
toSvc
,
method
.
arg
)
;
}
if
(
toSvcFencingFailed
)
{
LOG
.
error
(
+
)
;
}
else
{
if
(
method
.
method
.
tryFence
(
fromSvc
,
method
.
arg
)
)
{
LOG
.
info
(
+
method
+
)
;
return
true
;
}
}
}
catch
(
BadFencingConfigurationException
e
)
{
LOG
.
info
(
+
(
++
i
)
+
+
methods
.
size
(
)
+
+
method
)
;
try
{
boolean
toSvcFencingFailed
=
false
;
if
(
toSvc
!=
null
)
{
toSvcFencingFailed
=
!
method
.
method
.
tryFence
(
toSvc
,
method
.
arg
)
;
}
if
(
toSvcFencingFailed
)
{
LOG
.
error
(
+
)
;
}
else
{
if
(
method
.
method
.
tryFence
(
fromSvc
,
method
.
arg
)
)
{
LOG
.
info
(
+
method
+
)
;
return
true
;
}
}
}
catch
(
BadFencingConfigurationException
e
)
{
LOG
.
error
(
+
method
+
,
e
)
;
continue
;
}
catch
(
Throwable
t
)
{
@
Override
public
void
checkArgs
(
String
argStr
)
throws
BadFencingConfigurationException
{
private
String
buildPSScript
(
final
String
processName
,
final
String
host
)
{
private
String
buildPSScript
(
final
String
processName
,
final
String
host
)
{
LOG
.
info
(
+
processName
+
+
host
)
;
String
ps1script
=
null
;
BufferedWriter
writer
=
null
;
try
{
File
file
=
File
.
createTempFile
(
,
)
;
file
.
deleteOnExit
(
)
;
FileOutputStream
fos
=
new
FileOutputStream
(
file
,
false
)
;
OutputStreamWriter
osw
=
new
OutputStreamWriter
(
fos
,
StandardCharsets
.
UTF_8
)
;
writer
=
new
BufferedWriter
(
osw
)
;
String
filter
=
StringUtils
.
join
(
,
new
String
[
]
{
,
+
processName
+
}
)
;
String
cmd
=
;
cmd
+=
+
filter
+
;
cmd
+=
+
host
;
cmd
+=
;
File
file
=
File
.
createTempFile
(
,
)
;
file
.
deleteOnExit
(
)
;
FileOutputStream
fos
=
new
FileOutputStream
(
file
,
false
)
;
OutputStreamWriter
osw
=
new
OutputStreamWriter
(
fos
,
StandardCharsets
.
UTF_8
)
;
writer
=
new
BufferedWriter
(
osw
)
;
String
filter
=
StringUtils
.
join
(
,
new
String
[
]
{
,
+
processName
+
}
)
;
String
cmd
=
;
cmd
+=
+
filter
+
;
cmd
+=
+
host
;
cmd
+=
;
LOG
.
info
(
+
cmd
)
;
writer
.
write
(
cmd
)
;
writer
.
flush
(
)
;
ps1script
=
file
.
getAbsolutePath
(
)
;
}
catch
(
IOException
ioe
)
{
String
cmd
=
;
cmd
+=
+
filter
+
;
cmd
+=
+
host
;
cmd
+=
;
LOG
.
info
(
+
cmd
)
;
writer
.
write
(
cmd
)
;
writer
.
flush
(
)
;
ps1script
=
file
.
getAbsolutePath
(
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
error
(
,
ioe
)
;
}
finally
{
if
(
writer
!=
null
)
{
try
{
writer
.
close
(
)
;
}
catch
(
IOException
ioe
)
{
String
cmd
=
parseArgs
(
target
.
getTransitionTargetHAStatus
(
)
,
args
)
;
if
(
!
Shell
.
WINDOWS
)
{
builder
=
new
ProcessBuilder
(
,
,
,
cmd
)
;
}
else
{
builder
=
new
ProcessBuilder
(
,
,
cmd
)
;
}
setConfAsEnvVars
(
builder
.
environment
(
)
)
;
addTargetInfoAsEnvVars
(
target
,
builder
.
environment
(
)
)
;
Process
p
;
try
{
p
=
builder
.
start
(
)
;
p
.
getOutputStream
(
)
.
close
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
cmd
,
e
)
;
return
false
;
}
String
pid
=
tryGetPid
(
p
)
;
private
boolean
doFence
(
Session
session
,
InetSocketAddress
serviceAddr
)
throws
JSchException
{
int
port
=
serviceAddr
.
getPort
(
)
;
try
{
private
boolean
doFence
(
Session
session
,
InetSocketAddress
serviceAddr
)
throws
JSchException
{
int
port
=
serviceAddr
.
getPort
(
)
;
try
{
LOG
.
info
(
+
port
)
;
int
rc
=
execCommand
(
session
,
+
port
)
;
if
(
rc
==
0
)
{
private
int
execCommand
(
Session
session
,
String
cmd
)
throws
JSchException
,
InterruptedException
,
IOException
{
protected
void
pump
(
)
throws
IOException
{
InputStreamReader
inputStreamReader
=
new
InputStreamReader
(
stream
,
StandardCharsets
.
UTF_8
)
;
BufferedReader
br
=
new
BufferedReader
(
inputStreamReader
)
;
String
line
=
null
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
if
(
type
==
StreamType
.
STDOUT
)
{
public
int
run
(
final
String
[
]
args
)
throws
Exception
{
if
(
!
localTarget
.
isAutoFailoverEnabled
(
)
)
{
force
=
true
;
}
else
if
(
.
equals
(
args
[
i
]
)
)
{
interactive
=
false
;
}
else
{
badArg
(
args
[
i
]
)
;
}
}
return
formatZK
(
force
,
interactive
)
;
}
else
{
badArg
(
args
[
0
]
)
;
}
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
e
;
}
if
(
!
elector
.
parentZNodeExists
(
)
)
{
LOG
.
error
(
+
+
)
;
return
ERR_CODE_NO_PARENT_ZNODE
;
}
try
{
}
return
formatZK
(
force
,
interactive
)
;
}
else
{
badArg
(
args
[
0
]
)
;
}
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
e
;
}
if
(
!
elector
.
parentZNodeExists
(
)
)
{
LOG
.
error
(
+
+
)
;
return
ERR_CODE_NO_PARENT_ZNODE
;
}
try
{
localTarget
.
checkFencingConfigured
(
)
;
}
catch
(
BadFencingConfigurationException
e
)
{
LOG
.
error
(
+
localTarget
+
+
+
,
e
)
;
return
ERR_CODE_NO_FENCER
;
}
try
{
protected
void
initRPC
(
)
throws
IOException
{
InetSocketAddress
bindAddr
=
getRpcAddressToBindTo
(
)
;
private
synchronized
void
fatalError
(
String
err
)
{
private
synchronized
void
becomeActive
(
)
throws
ServiceFailedException
{
LOG
.
info
(
+
localTarget
+
)
;
try
{
HAServiceProtocolHelper
.
transitionToActive
(
localTarget
.
getProxy
(
conf
,
FailoverController
.
getRpcTimeoutToNewActive
(
conf
)
)
,
createReqInfo
(
)
)
;
String
msg
=
+
localTarget
+
;
private
synchronized
void
becomeStandby
(
)
{
LOG
.
info
(
+
localTarget
+
)
;
try
{
int
timeout
=
FailoverController
.
getGracefulFenceTimeout
(
conf
)
;
localTarget
.
getProxy
(
conf
,
timeout
)
.
transitionToStandby
(
createReqInfo
(
)
)
;
private
void
doFence
(
HAServiceTarget
target
)
{
private
void
doFence
(
HAServiceTarget
target
)
{
LOG
.
info
(
+
target
)
;
boolean
gracefulWorked
=
new
FailoverController
(
conf
,
RequestSource
.
REQUEST_BY_ZKFC
)
.
tryGracefulFence
(
target
)
;
if
(
gracefulWorked
)
{
private
ZKFCProtocol
cedeRemoteActive
(
HAServiceTarget
remote
,
int
timeout
)
throws
IOException
{
protected
synchronized
void
setLastHealthState
(
HealthMonitor
.
State
newState
)
{
;
try
{
isLog4JLogger
=
logger
instanceof
Log4JLogger
;
}
catch
(
NoClassDefFoundError
err
)
{
LOG
.
debug
(
,
err
)
;
isLog4JLogger
=
false
;
}
if
(
isLog4JLogger
)
{
Log4JLogger
httpLog4JLog
=
(
Log4JLogger
)
logger
;
org
.
apache
.
log4j
.
Logger
httpLogger
=
httpLog4JLog
.
getLogger
(
)
;
Appender
appender
=
null
;
try
{
appender
=
httpLogger
.
getAppender
(
appenderName
)
;
}
catch
(
LogConfigurationException
e
)
{
LOG
.
warn
(
,
loggerName
)
;
throw
e
;
public
void
addJerseyResourcePackage
(
final
String
packageName
,
final
String
pathSpec
,
Map
<
String
,
String
>
params
)
{
public
void
addInternalServlet
(
String
name
,
String
pathSpec
,
Class
<
?
extends
HttpServlet
>
clazz
,
Map
<
String
,
String
>
params
)
{
final
ServletHolder
sh
=
new
ServletHolder
(
clazz
)
;
sh
.
setName
(
name
)
;
sh
.
setInitParameters
(
params
)
;
final
ServletMapping
[
]
servletMappings
=
webAppContext
.
getServletHandler
(
)
.
getServletMappings
(
)
;
for
(
int
i
=
0
;
i
<
servletMappings
.
length
;
i
++
)
{
if
(
servletMappings
[
i
]
.
containsPathSpec
(
pathSpec
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
addFilter
(
String
name
,
String
classname
,
Map
<
String
,
String
>
parameters
)
{
FilterHolder
filterHolder
=
getFilterHolder
(
name
,
classname
,
parameters
)
;
final
String
[
]
userFacingUrls
=
{
,
}
;
FilterMapping
fmap
=
getFilterMapping
(
name
,
userFacingUrls
)
;
defineFilter
(
webAppContext
,
filterHolder
,
fmap
)
;
@
Override
public
void
addFilter
(
String
name
,
String
classname
,
Map
<
String
,
String
>
parameters
)
{
FilterHolder
filterHolder
=
getFilterHolder
(
name
,
classname
,
parameters
)
;
final
String
[
]
userFacingUrls
=
{
,
}
;
FilterMapping
fmap
=
getFilterMapping
(
name
,
userFacingUrls
)
;
defineFilter
(
webAppContext
,
filterHolder
,
fmap
)
;
LOG
.
info
(
+
name
+
+
classname
+
+
webAppContext
.
getDisplayName
(
)
)
;
final
String
[
]
ALL_URLS
=
{
}
;
fmap
=
getFilterMapping
(
name
,
ALL_URLS
)
;
for
(
Map
.
Entry
<
ServletContextHandler
,
Boolean
>
e
:
defaultContexts
.
entrySet
(
)
)
{
if
(
e
.
getValue
(
)
)
{
ServletContextHandler
ctx
=
e
.
getKey
(
)
;
defineFilter
(
ctx
,
filterHolder
,
fmap
)
;
private
static
void
bindListener
(
ServerConnector
listener
)
throws
Exception
{
listener
.
close
(
)
;
listener
.
open
(
)
;
try
{
c
.
close
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
webAppContext
.
getDisplayName
(
)
,
e
)
;
exception
=
addMultiException
(
exception
,
e
)
;
}
}
try
{
secretProvider
.
destroy
(
)
;
webAppContext
.
clearAttributes
(
)
;
webAppContext
.
stop
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
webAppContext
.
getDisplayName
(
)
,
e
)
;
exception
=
addMultiException
(
exception
,
e
)
;
}
try
{
webServer
.
stop
(
)
;
}
catch
(
Exception
e
)
{
public
static
Compressor
getCompressor
(
CompressionCodec
codec
,
Configuration
conf
)
{
Compressor
compressor
=
borrow
(
compressorPool
,
codec
.
getCompressorType
(
)
)
;
if
(
compressor
==
null
)
{
compressor
=
codec
.
createCompressor
(
)
;
public
static
Decompressor
getDecompressor
(
CompressionCodec
codec
)
{
Decompressor
decompressor
=
borrow
(
decompressorPool
,
codec
.
getDecompressorType
(
)
)
;
if
(
decompressor
==
null
)
{
decompressor
=
codec
.
createDecompressor
(
)
;
@
VisibleForTesting
void
updateCoders
(
Iterable
<
RawErasureCoderFactory
>
coderFactories
)
{
for
(
RawErasureCoderFactory
coderFactory
:
coderFactories
)
{
String
codecName
=
coderFactory
.
getCodecName
(
)
;
List
<
RawErasureCoderFactory
>
coders
=
coderMap
.
get
(
codecName
)
;
if
(
coders
==
null
)
{
coders
=
new
ArrayList
<
>
(
)
;
coders
.
add
(
coderFactory
)
;
coderMap
.
put
(
codecName
,
coders
)
;
@
Override
public
synchronized
boolean
isSupported
(
)
{
if
(
!
checked
||
reinitCodecInTests
)
{
checked
=
true
;
reinitCodecInTests
=
conf
.
getBoolean
(
,
false
)
;
clazz
=
getLzoCodecClass
(
)
;
try
{
public
void
returnCompressor
(
Compressor
compressor
)
{
if
(
compressor
!=
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
returnDecompressor
(
Decompressor
decompressor
)
{
if
(
decompressor
!=
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
Object
invokeMethod
(
Method
method
,
Object
[
]
args
)
throws
Throwable
{
Object
result
=
super
.
invokeMethod
(
method
,
args
)
;
int
retryCount
=
RetryCount
.
get
(
)
;
if
(
retryCount
<
this
.
numToDrop
)
{
RetryCount
.
set
(
++
retryCount
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
RetryInfo
handleException
(
final
Method
method
,
final
int
callId
,
final
RetryPolicy
policy
,
final
Counters
counters
,
final
long
expectFailoverCount
,
final
Exception
e
)
throws
Exception
{
final
RetryInfo
retryInfo
=
RetryInfo
.
newRetryInfo
(
policy
,
e
,
counters
,
proxyDescriptor
.
idempotentOrAtMostOnce
(
method
)
,
expectFailoverCount
)
;
if
(
retryInfo
.
isFail
(
)
)
{
if
(
retryInfo
.
action
.
reason
!=
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
log
(
final
Method
method
,
final
boolean
isFailover
,
final
int
failovers
,
final
long
delay
,
final
Exception
ex
)
{
boolean
info
=
true
;
if
(
!
failedAtLeastOnce
.
contains
(
proxyDescriptor
.
getProxyInfo
(
)
.
toString
(
)
)
)
{
failedAtLeastOnce
.
add
(
proxyDescriptor
.
getProxyInfo
(
)
.
toString
(
)
)
;
info
=
hasSuccessfulCall
||
asyncCallHandler
.
hasSuccessfulCall
(
)
;
if
(
!
info
&&
!
LOG
.
isDebugEnabled
(
)
)
{
return
;
}
}
final
StringBuilder
b
=
new
StringBuilder
(
)
.
append
(
ex
+
)
.
append
(
proxyDescriptor
.
getProxyInfo
(
)
.
getString
(
method
.
getName
(
)
)
)
;
if
(
failovers
>
0
)
{
b
.
append
(
)
.
append
(
failovers
)
.
append
(
)
;
}
b
.
append
(
isFailover
?
:
)
;
b
.
append
(
delay
>
0
?
+
delay
+
:
)
;
if
(
info
)
{
LOG
.
info
(
b
.
toString
(
)
)
;
}
else
{
public
static
RetryPolicy
getDefaultRetryPolicy
(
Configuration
conf
,
String
retryPolicyEnabledKey
,
boolean
defaultRetryPolicyEnabled
,
String
retryPolicySpecKey
,
String
defaultRetryPolicySpec
,
final
String
remoteExceptionToRetry
)
{
final
RetryPolicy
multipleLinearRandomRetry
=
getMultipleLinearRandomRetry
(
conf
,
retryPolicyEnabledKey
,
defaultRetryPolicyEnabled
,
retryPolicySpecKey
,
defaultRetryPolicySpec
)
;
private
void
decayCurrentCosts
(
)
{
LOG
.
debug
(
)
;
try
{
long
totalDecayedCost
=
0
;
long
totalRawCost
=
0
;
Iterator
<
Map
.
Entry
<
Object
,
List
<
AtomicLong
>>>
it
=
callCosts
.
entrySet
(
)
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
Map
.
Entry
<
Object
,
List
<
AtomicLong
>>
entry
=
it
.
next
(
)
;
AtomicLong
decayedCost
=
entry
.
getValue
(
)
.
get
(
0
)
;
AtomicLong
rawCost
=
entry
.
getValue
(
)
.
get
(
1
)
;
totalRawCost
+=
rawCost
.
get
(
)
;
long
currentValue
=
decayedCost
.
get
(
)
;
long
nextValue
=
(
long
)
(
currentValue
*
decayFactor
)
;
totalDecayedCost
+=
nextValue
;
decayedCost
.
set
(
nextValue
)
;
try
{
long
totalDecayedCost
=
0
;
long
totalRawCost
=
0
;
Iterator
<
Map
.
Entry
<
Object
,
List
<
AtomicLong
>>>
it
=
callCosts
.
entrySet
(
)
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
Map
.
Entry
<
Object
,
List
<
AtomicLong
>>
entry
=
it
.
next
(
)
;
AtomicLong
decayedCost
=
entry
.
getValue
(
)
.
get
(
0
)
;
AtomicLong
rawCost
=
entry
.
getValue
(
)
.
get
(
1
)
;
totalRawCost
+=
rawCost
.
get
(
)
;
long
currentValue
=
decayedCost
.
get
(
)
;
long
nextValue
=
(
long
)
(
currentValue
*
decayFactor
)
;
totalDecayedCost
+=
nextValue
;
decayedCost
.
set
(
nextValue
)
;
LOG
.
debug
(
,
entry
.
getKey
(
)
,
nextValue
,
rawCost
.
get
(
)
)
;
if
(
nextValue
==
0
)
{
while
(
it
.
hasNext
(
)
)
{
Map
.
Entry
<
Object
,
List
<
AtomicLong
>>
entry
=
it
.
next
(
)
;
AtomicLong
decayedCost
=
entry
.
getValue
(
)
.
get
(
0
)
;
AtomicLong
rawCost
=
entry
.
getValue
(
)
.
get
(
1
)
;
totalRawCost
+=
rawCost
.
get
(
)
;
long
currentValue
=
decayedCost
.
get
(
)
;
long
nextValue
=
(
long
)
(
currentValue
*
decayFactor
)
;
totalDecayedCost
+=
nextValue
;
decayedCost
.
set
(
nextValue
)
;
LOG
.
debug
(
,
entry
.
getKey
(
)
,
nextValue
,
rawCost
.
get
(
)
)
;
if
(
nextValue
==
0
)
{
LOG
.
debug
(
+
,
entry
.
getKey
(
)
)
;
it
.
remove
(
)
;
}
}
totalDecayedCallCost
.
set
(
totalDecayedCost
)
;
totalRawCallCost
.
set
(
totalRawCost
)
;
totalRawCost
+=
rawCost
.
get
(
)
;
long
currentValue
=
decayedCost
.
get
(
)
;
long
nextValue
=
(
long
)
(
currentValue
*
decayFactor
)
;
totalDecayedCost
+=
nextValue
;
decayedCost
.
set
(
nextValue
)
;
LOG
.
debug
(
,
entry
.
getKey
(
)
,
nextValue
,
rawCost
.
get
(
)
)
;
if
(
nextValue
==
0
)
{
LOG
.
debug
(
+
,
entry
.
getKey
(
)
)
;
it
.
remove
(
)
;
}
}
totalDecayedCallCost
.
set
(
totalDecayedCost
)
;
totalRawCallCost
.
set
(
totalRawCost
)
;
LOG
.
debug
(
+
,
totalDecayedCost
,
totalRawCost
)
;
recomputeScheduleCache
(
)
;
updateAverageResponseTime
(
true
)
;
}
catch
(
Exception
ex
)
{
private
int
cachedOrComputedPriorityLevel
(
Object
identity
)
{
Map
<
Object
,
Integer
>
scheduleCache
=
scheduleCacheRef
.
get
(
)
;
if
(
scheduleCache
!=
null
)
{
Integer
priority
=
scheduleCache
.
get
(
identity
)
;
if
(
priority
!=
null
)
{
@
Override
public
boolean
shouldBackOff
(
Schedulable
obj
)
{
Boolean
backOff
=
false
;
if
(
backOffByResponseTimeEnabled
)
{
int
priorityLevel
=
obj
.
getPriorityLevel
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
double
[
]
responseTimes
=
getAverageResponseTime
(
)
;
LOG
.
debug
(
,
obj
.
getUserGroupInformation
(
)
.
getUserName
(
)
,
obj
.
getPriorityLevel
(
)
)
;
for
(
int
i
=
0
;
i
<
numLevels
;
i
++
)
{
@
Override
public
void
addResponseTime
(
String
callName
,
Schedulable
schedulable
,
ProcessingDetails
details
)
{
String
user
=
identityProvider
.
makeIdentity
(
schedulable
)
;
long
processingCost
=
costProvider
.
getCost
(
details
)
;
addCost
(
user
,
processingCost
)
;
int
priorityLevel
=
schedulable
.
getPriorityLevel
(
)
;
long
queueTime
=
details
.
get
(
Timing
.
QUEUE
,
RpcMetrics
.
TIMEUNIT
)
;
long
processingTime
=
details
.
get
(
Timing
.
PROCESSING
,
RpcMetrics
.
TIMEUNIT
)
;
this
.
decayRpcSchedulerDetailedMetrics
.
addQueueTime
(
priorityLevel
,
queueTime
)
;
this
.
decayRpcSchedulerDetailedMetrics
.
addProcessingTime
(
priorityLevel
,
processingTime
)
;
responseTimeCountInCurrWindow
.
getAndIncrement
(
priorityLevel
)
;
responseTimeTotalInCurrWindow
.
getAndAdd
(
priorityLevel
,
queueTime
+
processingTime
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
totalResponseTime
=
responseTimeTotalInCurrWindow
.
get
(
i
)
;
long
responseTimeCount
=
responseTimeCountInCurrWindow
.
get
(
i
)
;
if
(
responseTimeCount
>
0
)
{
averageResponseTime
=
(
double
)
totalResponseTime
/
responseTimeCount
;
}
final
double
lastAvg
=
responseTimeAvgInLastWindow
.
get
(
i
)
;
if
(
lastAvg
>
PRECISION
||
averageResponseTime
>
PRECISION
)
{
if
(
enableDecay
)
{
final
double
decayed
=
decayFactor
*
lastAvg
+
averageResponseTime
;
responseTimeAvgInLastWindow
.
set
(
i
,
decayed
)
;
}
else
{
responseTimeAvgInLastWindow
.
set
(
i
,
averageResponseTime
)
;
}
}
else
{
responseTimeAvgInLastWindow
.
set
(
i
,
0
)
;
}
responseTimeCountInLastWindow
.
set
(
i
,
responseTimeCount
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
stopProxy
(
Object
proxy
)
{
if
(
proxy
==
null
)
{
throw
new
HadoopIllegalArgumentException
(
)
;
}
try
{
if
(
proxy
instanceof
Closeable
)
{
(
(
Closeable
)
proxy
)
.
close
(
)
;
return
;
}
else
{
InvocationHandler
handler
=
Proxy
.
getInvocationHandler
(
proxy
)
;
if
(
handler
instanceof
Closeable
)
{
(
(
Closeable
)
handler
)
.
close
(
)
;
return
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
}
catch
(
IllegalArgumentException
e
)
{
RpcSaslProto
saslResponse
=
null
;
try
{
try
{
saslResponse
=
processSaslMessage
(
saslMessage
)
;
}
catch
(
IOException
e
)
{
rpcMetrics
.
incrAuthenticationFailures
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
StringUtils
.
stringifyException
(
e
)
)
;
}
IOException
tce
=
(
IOException
)
getTrueCause
(
e
)
;
AUDITLOG
.
warn
(
AUTH_FAILED_FOR
+
this
.
toString
(
)
+
+
attemptingUser
+
+
e
.
getLocalizedMessage
(
)
+
+
tce
.
getLocalizedMessage
(
)
+
)
;
throw
tce
;
}
if
(
saslServer
!=
null
&&
saslServer
.
isComplete
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
saslServer
.
getNegotiatedProperty
(
Sasl
.
QOP
)
)
;
}
user
=
getAuthorizedUgi
(
saslServer
.
getAuthorizationID
(
)
)
;
try
{
saslResponse
=
processSaslMessage
(
saslMessage
)
;
}
catch
(
IOException
e
)
{
rpcMetrics
.
incrAuthenticationFailures
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
StringUtils
.
stringifyException
(
e
)
)
;
}
IOException
tce
=
(
IOException
)
getTrueCause
(
e
)
;
AUDITLOG
.
warn
(
AUTH_FAILED_FOR
+
this
.
toString
(
)
+
+
attemptingUser
+
+
e
.
getLocalizedMessage
(
)
+
+
tce
.
getLocalizedMessage
(
)
+
)
;
throw
tce
;
}
if
(
saslServer
!=
null
&&
saslServer
.
isComplete
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
saslServer
.
getNegotiatedProperty
(
Sasl
.
QOP
)
)
;
}
user
=
getAuthorizedUgi
(
saslServer
.
getAuthorizationID
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
user
)
;
private
RpcSaslProto
buildSaslResponse
(
SaslState
state
,
byte
[
]
replyToken
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
unwrapPacketAndProcessRpcs
(
byte
[
]
inBuf
)
throws
IOException
,
InterruptedException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
processOneRpc
(
ByteBuffer
bb
)
throws
IOException
,
InterruptedException
{
int
callId
=
-
1
;
int
retry
=
RpcConstants
.
INVALID_RETRY_COUNT
;
try
{
final
RpcWritable
.
Buffer
buffer
=
RpcWritable
.
Buffer
.
wrap
(
bb
)
;
final
RpcRequestHeaderProto
header
=
getMessage
(
RpcRequestHeaderProto
.
getDefaultInstance
(
)
,
buffer
)
;
callId
=
header
.
getCallId
(
)
;
retry
=
header
.
getRetryCount
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
final
RpcWritable
.
Buffer
buffer
=
RpcWritable
.
Buffer
.
wrap
(
bb
)
;
final
RpcRequestHeaderProto
header
=
getMessage
(
RpcRequestHeaderProto
.
getDefaultInstance
(
)
,
buffer
)
;
callId
=
header
.
getCallId
(
)
;
retry
=
header
.
getRetryCount
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
callId
)
;
}
checkRpcHeaders
(
header
)
;
if
(
callId
<
0
)
{
processRpcOutOfBandRequest
(
header
,
buffer
)
;
}
else
if
(
!
connectionContextRead
)
{
throw
new
FatalRpcServerException
(
RpcErrorCodeProto
.
FATAL_INVALID_RPC_HEADER
,
)
;
}
else
{
processRpcRequest
(
header
,
buffer
)
;
}
}
catch
(
RpcServerException
rse
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
moveToNextQueue
(
)
{
int
thisIdx
=
this
.
currentQueueIndex
.
get
(
)
;
int
nextIdx
=
(
thisIdx
+
1
)
%
this
.
numQueues
;
this
.
currentQueueIndex
.
set
(
nextIdx
)
;
this
.
requestsLeft
.
set
(
this
.
queueWeights
[
nextIdx
]
)
;
public
void
init
(
int
numLevels
)
{
return
;
}
listBeans
(
jg
,
new
ObjectName
(
splitStrings
[
0
]
)
,
splitStrings
[
1
]
,
response
)
;
return
;
}
String
qry
=
request
.
getParameter
(
)
;
if
(
qry
==
null
)
{
qry
=
;
}
listBeans
(
jg
,
new
ObjectName
(
qry
)
,
null
,
response
)
;
}
finally
{
if
(
jg
!=
null
)
{
jg
.
close
(
)
;
}
if
(
writer
!=
null
)
{
writer
.
close
(
)
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
response
.
setStatus
(
HttpServletResponse
.
SC_INTERNAL_SERVER_ERROR
)
;
private
void
listBeans
(
JsonGenerator
jg
,
ObjectName
qry
,
String
attribute
,
HttpServletResponse
response
)
throws
IOException
{
MBeanInfo
minfo
;
String
code
=
;
Object
attributeinfo
=
null
;
try
{
minfo
=
mBeanServer
.
getMBeanInfo
(
oname
)
;
code
=
minfo
.
getClassName
(
)
;
String
prs
=
;
try
{
if
(
.
equals
(
code
)
)
{
prs
=
;
code
=
(
String
)
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
if
(
attribute
!=
null
)
{
prs
=
attribute
;
attributeinfo
=
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
}
catch
(
AttributeNotFoundException
e
)
{
Object
attributeinfo
=
null
;
try
{
minfo
=
mBeanServer
.
getMBeanInfo
(
oname
)
;
code
=
minfo
.
getClassName
(
)
;
String
prs
=
;
try
{
if
(
.
equals
(
code
)
)
{
prs
=
;
code
=
(
String
)
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
if
(
attribute
!=
null
)
{
prs
=
attribute
;
attributeinfo
=
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
}
catch
(
AttributeNotFoundException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
MBeanException
e
)
{
minfo
=
mBeanServer
.
getMBeanInfo
(
oname
)
;
code
=
minfo
.
getClassName
(
)
;
String
prs
=
;
try
{
if
(
.
equals
(
code
)
)
{
prs
=
;
code
=
(
String
)
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
if
(
attribute
!=
null
)
{
prs
=
attribute
;
attributeinfo
=
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
}
catch
(
AttributeNotFoundException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
MBeanException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
RuntimeException
e
)
{
String
prs
=
;
try
{
if
(
.
equals
(
code
)
)
{
prs
=
;
code
=
(
String
)
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
if
(
attribute
!=
null
)
{
prs
=
attribute
;
attributeinfo
=
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
}
catch
(
AttributeNotFoundException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
MBeanException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
RuntimeException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
ReflectionException
e
)
{
code
=
(
String
)
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
if
(
attribute
!=
null
)
{
prs
=
attribute
;
attributeinfo
=
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
}
catch
(
AttributeNotFoundException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
MBeanException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
RuntimeException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
ReflectionException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
}
catch
(
InstanceNotFoundException
e
)
{
continue
;
}
catch
(
IntrospectionException
e
)
{
prs
=
attribute
;
attributeinfo
=
mBeanServer
.
getAttribute
(
oname
,
prs
)
;
}
}
catch
(
AttributeNotFoundException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
MBeanException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
RuntimeException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
catch
(
ReflectionException
e
)
{
LOG
.
error
(
+
prs
+
+
oname
+
,
e
)
;
}
}
catch
(
InstanceNotFoundException
e
)
{
continue
;
}
catch
(
IntrospectionException
e
)
{
LOG
.
error
(
+
qry
+
+
oname
,
e
)
;
continue
;
private
void
writeAttribute
(
JsonGenerator
jg
,
ObjectName
oname
,
MBeanAttributeInfo
attr
)
throws
IOException
{
if
(
!
attr
.
isReadable
(
)
)
{
return
;
}
String
attName
=
attr
.
getName
(
)
;
if
(
.
equals
(
attName
)
)
{
return
;
}
if
(
attName
.
indexOf
(
)
>=
0
||
attName
.
indexOf
(
)
>=
0
||
attName
.
indexOf
(
)
>=
0
)
{
return
;
}
Object
value
=
null
;
try
{
value
=
mBeanServer
.
getAttribute
(
oname
,
attName
)
;
}
catch
(
RuntimeMBeanException
e
)
{
if
(
e
.
getCause
(
)
instanceof
UnsupportedOperationException
)
{
LOG
.
debug
(
+
attName
+
+
oname
+
,
e
)
;
}
else
{
}
String
attName
=
attr
.
getName
(
)
;
if
(
.
equals
(
attName
)
)
{
return
;
}
if
(
attName
.
indexOf
(
)
>=
0
||
attName
.
indexOf
(
)
>=
0
||
attName
.
indexOf
(
)
>=
0
)
{
return
;
}
Object
value
=
null
;
try
{
value
=
mBeanServer
.
getAttribute
(
oname
,
attName
)
;
}
catch
(
RuntimeMBeanException
e
)
{
if
(
e
.
getCause
(
)
instanceof
UnsupportedOperationException
)
{
LOG
.
debug
(
+
attName
+
+
oname
+
,
e
)
;
}
else
{
LOG
.
error
(
+
attName
+
+
oname
+
,
e
)
;
}
return
;
}
catch
(
RuntimeErrorException
e
)
{
if
(
attName
.
indexOf
(
)
>=
0
||
attName
.
indexOf
(
)
>=
0
||
attName
.
indexOf
(
)
>=
0
)
{
return
;
}
Object
value
=
null
;
try
{
value
=
mBeanServer
.
getAttribute
(
oname
,
attName
)
;
}
catch
(
RuntimeMBeanException
e
)
{
if
(
e
.
getCause
(
)
instanceof
UnsupportedOperationException
)
{
LOG
.
debug
(
+
attName
+
+
oname
+
,
e
)
;
}
else
{
LOG
.
error
(
+
attName
+
+
oname
+
,
e
)
;
}
return
;
}
catch
(
RuntimeErrorException
e
)
{
LOG
.
error
(
,
attName
,
oname
,
e
)
;
return
;
}
catch
(
AttributeNotFoundException
e
)
{
Object
value
=
null
;
try
{
value
=
mBeanServer
.
getAttribute
(
oname
,
attName
)
;
}
catch
(
RuntimeMBeanException
e
)
{
if
(
e
.
getCause
(
)
instanceof
UnsupportedOperationException
)
{
LOG
.
debug
(
+
attName
+
+
oname
+
,
e
)
;
}
else
{
LOG
.
error
(
+
attName
+
+
oname
+
,
e
)
;
}
return
;
}
catch
(
RuntimeErrorException
e
)
{
LOG
.
error
(
,
attName
,
oname
,
e
)
;
return
;
}
catch
(
AttributeNotFoundException
e
)
{
return
;
}
catch
(
MBeanException
e
)
{
}
catch
(
RuntimeMBeanException
e
)
{
if
(
e
.
getCause
(
)
instanceof
UnsupportedOperationException
)
{
LOG
.
debug
(
+
attName
+
+
oname
+
,
e
)
;
}
else
{
LOG
.
error
(
+
attName
+
+
oname
+
,
e
)
;
}
return
;
}
catch
(
RuntimeErrorException
e
)
{
LOG
.
error
(
,
attName
,
oname
,
e
)
;
return
;
}
catch
(
AttributeNotFoundException
e
)
{
return
;
}
catch
(
MBeanException
e
)
{
LOG
.
error
(
+
attName
+
+
oname
+
,
e
)
;
return
;
}
catch
(
RuntimeException
e
)
{
static
MetricsConfig
loadFirst
(
String
prefix
,
String
...
fileNames
)
{
for
(
String
fname
:
fileNames
)
{
try
{
PropertiesConfiguration
pcf
=
new
PropertiesConfiguration
(
)
;
pcf
.
setListDelimiterHandler
(
new
DefaultListDelimiterHandler
(
','
)
)
;
FileHandler
fh
=
new
FileHandler
(
pcf
)
;
fh
.
setFileName
(
fname
)
;
fh
.
load
(
)
;
Configuration
cf
=
pcf
.
interpolatedConfiguration
(
)
;
static
MetricsConfig
loadFirst
(
String
prefix
,
String
...
fileNames
)
{
for
(
String
fname
:
fileNames
)
{
try
{
PropertiesConfiguration
pcf
=
new
PropertiesConfiguration
(
)
;
pcf
.
setListDelimiterHandler
(
new
DefaultListDelimiterHandler
(
','
)
)
;
FileHandler
fh
=
new
FileHandler
(
pcf
)
;
fh
.
setFileName
(
fname
)
;
fh
.
load
(
)
;
Configuration
cf
=
pcf
.
interpolatedConfiguration
(
)
;
LOG
.
info
(
,
fname
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
{
PropertiesConfiguration
pcf
=
new
PropertiesConfiguration
(
)
;
pcf
.
setListDelimiterHandler
(
new
DefaultListDelimiterHandler
(
','
)
)
;
FileHandler
fh
=
new
FileHandler
(
pcf
)
;
fh
.
setFileName
(
fname
)
;
fh
.
load
(
)
;
Configuration
cf
=
pcf
.
interpolatedConfiguration
(
)
;
LOG
.
info
(
,
fname
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
toString
(
cf
)
)
;
}
MetricsConfig
mc
=
new
MetricsConfig
(
cf
,
prefix
)
;
LOG
.
debug
(
,
mc
)
;
return
mc
;
}
catch
(
ConfigurationException
e
)
{
if
(
e
.
getMessage
(
)
.
startsWith
(
)
)
{
@
Override
public
Object
getPropertyInternal
(
String
key
)
{
Object
value
=
super
.
getPropertyInternal
(
key
)
;
if
(
value
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
getClassName
(
String
prefix
)
{
String
classKey
=
prefix
.
isEmpty
(
)
?
:
prefix
.
concat
(
)
;
String
clsName
=
getString
(
classKey
)
;
final
ClassLoader
defaultLoader
=
getClass
(
)
.
getClassLoader
(
)
;
Object
purls
=
super
.
getProperty
(
PLUGIN_URLS_KEY
)
;
if
(
purls
==
null
)
{
return
defaultLoader
;
}
Iterable
<
String
>
jars
=
SPLITTER
.
split
(
(
String
)
purls
)
;
int
len
=
Iterables
.
size
(
jars
)
;
if
(
len
>
0
)
{
final
URL
[
]
urls
=
new
URL
[
len
]
;
try
{
int
i
=
0
;
for
(
String
jar
:
jars
)
{
LOG
.
debug
(
,
jar
)
;
urls
[
i
++
]
=
new
URL
(
jar
)
;
}
}
catch
(
Exception
e
)
{
throw
new
MetricsConfigException
(
e
)
;
boolean
putMetrics
(
MetricsBuffer
buffer
,
long
logicalTimeMs
)
{
if
(
logicalTimeMs
%
periodMs
==
0
)
{
Random
rng
=
new
Random
(
System
.
nanoTime
(
)
)
;
while
(
!
stopping
)
{
try
{
queue
.
consumeAll
(
this
)
;
refreshQueueSizeGauge
(
)
;
retryDelay
=
firstRetryDelay
;
n
=
retryCount
;
inError
=
false
;
}
catch
(
InterruptedException
e
)
{
LOG
.
info
(
name
+
)
;
}
catch
(
Exception
e
)
{
if
(
n
>
0
)
{
int
retryWindow
=
Math
.
max
(
0
,
1000
/
2
*
retryDelay
-
minDelay
)
;
int
awhile
=
rng
.
nextInt
(
retryWindow
)
+
minDelay
;
if
(
!
inError
)
{
retryDelay
=
firstRetryDelay
;
n
=
retryCount
;
inError
=
false
;
}
catch
(
InterruptedException
e
)
{
LOG
.
info
(
name
+
)
;
}
catch
(
Exception
e
)
{
if
(
n
>
0
)
{
int
retryWindow
=
Math
.
max
(
0
,
1000
/
2
*
retryDelay
-
minDelay
)
;
int
awhile
=
rng
.
nextInt
(
retryWindow
)
+
minDelay
;
if
(
!
inError
)
{
LOG
.
error
(
+
awhile
+
,
e
)
;
}
retryDelay
*=
retryBackoff
;
try
{
Thread
.
sleep
(
awhile
)
;
}
catch
(
InterruptedException
e2
)
{
catch
(
InterruptedException
e
)
{
LOG
.
info
(
name
+
)
;
}
catch
(
Exception
e
)
{
if
(
n
>
0
)
{
int
retryWindow
=
Math
.
max
(
0
,
1000
/
2
*
retryDelay
-
minDelay
)
;
int
awhile
=
rng
.
nextInt
(
retryWindow
)
+
minDelay
;
if
(
!
inError
)
{
LOG
.
error
(
+
awhile
+
,
e
)
;
}
retryDelay
*=
retryBackoff
;
try
{
Thread
.
sleep
(
awhile
)
;
}
catch
(
InterruptedException
e2
)
{
LOG
.
info
(
name
+
,
e2
)
;
}
--
n
;
}
else
{
@
Override
public
void
consume
(
MetricsBuffer
buffer
)
{
long
ts
=
0
;
for
(
MetricsBuffer
.
Entry
entry
:
buffer
)
{
if
(
sourceFilter
==
null
||
sourceFilter
.
accepts
(
entry
.
name
(
)
)
)
{
for
(
MetricsRecordImpl
record
:
entry
.
records
(
)
)
{
if
(
(
context
==
null
||
context
.
equals
(
record
.
context
(
)
)
)
&&
(
recordFilter
==
null
||
recordFilter
.
accepts
(
record
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
AttributeList
getAttributes
(
String
[
]
attributes
)
{
updateJmxCache
(
)
;
synchronized
(
this
)
{
AttributeList
ret
=
new
AttributeList
(
)
;
for
(
String
key
:
attributes
)
{
Attribute
attr
=
attrCache
.
get
(
key
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
synchronized
void
stop
(
)
{
if
(
!
monitoring
&&
!
DefaultMetricsSystem
.
inMiniClusterMode
(
)
)
{
LOG
.
warn
(
prefix
+
,
new
MetricsException
(
)
)
;
return
;
}
if
(
!
monitoring
)
{
LOG
.
info
(
prefix
+
)
;
return
;
}
for
(
Callback
cb
:
callbacks
)
cb
.
preStop
(
)
;
for
(
Callback
cb
:
namedCallbacks
.
values
(
)
)
cb
.
preStop
(
)
;
LOG
.
info
(
+
prefix
+
)
;
stopTimer
(
)
;
stopSources
(
)
;
stopSinks
(
)
;
clearConfigs
(
)
;
monitoring
=
false
;
@
Override
public
synchronized
<
T
>
T
register
(
String
name
,
String
desc
,
T
source
)
{
MetricsSourceBuilder
sb
=
MetricsAnnotations
.
newSourceBuilder
(
source
)
;
final
MetricsSource
s
=
sb
.
build
(
)
;
MetricsInfo
si
=
sb
.
info
(
)
;
String
name2
=
name
==
null
?
si
.
name
(
)
:
name
;
final
String
finalDesc
=
desc
==
null
?
si
.
description
(
)
:
desc
;
final
String
finalName
=
DefaultMetricsSystem
.
sourceName
(
name2
,
!
monitoring
)
;
allSources
.
put
(
finalName
,
s
)
;
synchronized
void
registerSource
(
String
name
,
String
desc
,
MetricsSource
source
)
{
checkNotNull
(
config
,
)
;
MetricsConfig
conf
=
sourceConfigs
.
get
(
name
)
;
MetricsSourceAdapter
sa
=
new
MetricsSourceAdapter
(
prefix
,
name
,
desc
,
source
,
injectedTags
,
period
,
conf
!=
null
?
conf
:
config
.
subset
(
SOURCE_KEY
)
)
;
sources
.
put
(
name
,
sa
)
;
sa
.
start
(
)
;
@
Override
public
synchronized
<
T
extends
MetricsSink
>
T
register
(
final
String
name
,
final
String
description
,
final
T
sink
)
{
synchronized
void
registerSink
(
String
name
,
String
desc
,
MetricsSink
sink
)
{
checkNotNull
(
config
,
)
;
MetricsConfig
conf
=
sinkConfigs
.
get
(
name
)
;
MetricsSinkAdapter
sa
=
conf
!=
null
?
newSink
(
name
,
desc
,
sink
,
conf
)
:
newSink
(
name
,
desc
,
sink
,
config
.
subset
(
SINK_KEY
)
)
;
sinks
.
put
(
name
,
sa
)
;
sa
.
start
(
)
;
private
void
snapshotMetrics
(
MetricsSourceAdapter
sa
,
MetricsBufferBuilder
bufferBuilder
)
{
long
startTime
=
Time
.
monotonicNow
(
)
;
bufferBuilder
.
add
(
sa
.
name
(
)
,
sa
.
getMetrics
(
collector
,
true
)
)
;
collector
.
clear
(
)
;
snapshotStat
.
add
(
Time
.
monotonicNow
(
)
-
startTime
)
;
private
synchronized
void
stopSources
(
)
{
for
(
Entry
<
String
,
MetricsSourceAdapter
>
entry
:
sources
.
entrySet
(
)
)
{
MetricsSourceAdapter
sa
=
entry
.
getValue
(
)
;
private
synchronized
void
stopSinks
(
)
{
for
(
Entry
<
String
,
MetricsSinkAdapter
>
entry
:
sinks
.
entrySet
(
)
)
{
MetricsSinkAdapter
sa
=
entry
.
getValue
(
)
;
private
InitMode
initMode
(
)
{
LOG
.
debug
(
+
System
.
getProperty
(
MS_INIT_MODE_KEY
)
)
;
MutableMetric
newForField
(
Field
field
,
Metric
annotation
,
MetricsRegistry
registry
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
MutableMetric
newForMethod
(
Object
source
,
Method
method
,
Metric
annotation
,
MetricsRegistry
registry
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
init
(
Class
<
?
>
protocol
)
{
if
(
protocolCache
.
contains
(
protocol
)
)
return
;
protocolCache
.
add
(
protocol
)
;
for
(
Method
method
:
protocol
.
getDeclaredMethods
(
)
)
{
String
name
=
method
.
getName
(
)
;
else
{
try
{
hostName
=
DNS
.
getDefaultHost
(
conf
.
getString
(
,
)
,
conf
.
getString
(
,
)
)
;
}
catch
(
UnknownHostException
uhe
)
{
LOG
.
error
(
uhe
.
toString
(
)
)
;
hostName
=
;
}
}
metricsServers
=
Servers
.
parse
(
conf
.
getString
(
SERVERS_PROPERTY
)
,
DEFAULT_PORT
)
;
multicastEnabled
=
conf
.
getBoolean
(
MULTICAST_ENABLED_PROPERTY
,
DEFAULT_MULTICAST_ENABLED
)
;
multicastTtl
=
conf
.
getInt
(
MULTICAST_TTL_PROPERTY
,
DEFAULT_MULTICAST_TTL
)
;
gangliaConfMap
=
new
HashMap
<
String
,
GangliaConf
>
(
)
;
loadGangliaConf
(
GangliaConfType
.
units
)
;
loadGangliaConf
(
GangliaConfType
.
tmax
)
;
loadGangliaConf
(
GangliaConfType
.
dmax
)
;
loadGangliaConf
(
GangliaConfType
.
slope
)
;
try
{
private
void
loadGangliaConf
(
GangliaConfType
gtype
)
{
String
propertyarr
[
]
=
conf
.
getStringArray
(
gtype
.
name
(
)
)
;
if
(
propertyarr
!=
null
&&
propertyarr
.
length
>
0
)
{
for
(
String
metricNValue
:
propertyarr
)
{
String
metricNValueArr
[
]
=
metricNValue
.
split
(
EQUAL
)
;
if
(
metricNValueArr
.
length
!=
2
||
metricNValueArr
[
0
]
.
length
(
)
==
0
)
{
static
public
ObjectName
register
(
String
serviceName
,
String
nameName
,
Map
<
String
,
String
>
properties
,
Object
theMbean
)
{
final
MBeanServer
mbs
=
ManagementFactory
.
getPlatformMBeanServer
(
)
;
Preconditions
.
checkNotNull
(
properties
,
+
)
;
ObjectName
name
=
getMBeanName
(
serviceName
,
nameName
,
properties
)
;
if
(
name
!=
null
)
{
try
{
mbs
.
registerMBean
(
theMbean
,
name
)
;
static
public
void
unregister
(
ObjectName
mbeanName
)
{
if
(
!
isChildScope
(
excludedScope
,
scope
)
)
{
excludedScope
=
null
;
}
}
Node
node
=
getNode
(
scope
)
;
if
(
!
(
node
instanceof
InnerNode
)
)
{
return
excludedNodes
!=
null
&&
excludedNodes
.
contains
(
node
)
?
null
:
node
;
}
InnerNode
innerNode
=
(
InnerNode
)
node
;
int
numOfDatanodes
=
innerNode
.
getNumOfLeaves
(
)
;
if
(
excludedScope
==
null
)
{
node
=
null
;
}
else
{
node
=
getNode
(
excludedScope
)
;
if
(
!
(
node
instanceof
InnerNode
)
)
{
numOfDatanodes
-=
1
;
}
else
{
numOfDatanodes
-=
(
(
InnerNode
)
node
)
.
getNumOfLeaves
(
)
;
}
else
{
node
=
getNode
(
excludedScope
)
;
if
(
!
(
node
instanceof
InnerNode
)
)
{
numOfDatanodes
-=
1
;
}
else
{
numOfDatanodes
-=
(
(
InnerNode
)
node
)
.
getNumOfLeaves
(
)
;
}
}
if
(
numOfDatanodes
<=
0
)
{
LOG
.
debug
(
+
,
scope
,
excludedScope
,
numOfDatanodes
)
;
return
null
;
}
final
int
availableNodes
;
if
(
excludedScope
==
null
)
{
availableNodes
=
countNumOfAvailableNodes
(
scope
,
excludedNodes
)
;
}
else
{
netlock
.
readLock
(
)
.
lock
(
)
;
try
{
int
nthValidToReturn
=
r
.
nextInt
(
availableNodes
)
;
LOG
.
debug
(
,
nthValidToReturn
)
;
Node
ret
=
parentNode
.
getLeaf
(
r
.
nextInt
(
totalInScopeNodes
)
,
excludedScopeNode
)
;
if
(
!
excludedNodes
.
contains
(
ret
)
)
{
LOG
.
debug
(
,
ret
)
;
return
ret
;
}
else
{
ret
=
null
;
}
Node
lastValidNode
=
null
;
for
(
int
i
=
0
;
i
<
totalInScopeNodes
;
++
i
)
{
ret
=
parentNode
.
getLeaf
(
i
,
excludedScopeNode
)
;
if
(
!
excludedNodes
.
contains
(
ret
)
)
{
if
(
nthValidToReturn
==
0
)
{
break
;
}
--
nthValidToReturn
;
if
(
!
excludedNodes
.
contains
(
ret
)
)
{
LOG
.
debug
(
,
ret
)
;
return
ret
;
}
else
{
ret
=
null
;
}
Node
lastValidNode
=
null
;
for
(
int
i
=
0
;
i
<
totalInScopeNodes
;
++
i
)
{
ret
=
parentNode
.
getLeaf
(
i
,
excludedScopeNode
)
;
if
(
!
excludedNodes
.
contains
(
ret
)
)
{
if
(
nthValidToReturn
==
0
)
{
break
;
}
--
nthValidToReturn
;
lastValidNode
=
ret
;
}
else
{
LOG
.
debug
(
,
ret
)
;
if
(
node
==
null
)
return
;
if
(
node
instanceof
InnerNode
)
{
throw
new
IllegalArgumentException
(
+
NodeBase
.
getPath
(
node
)
)
;
}
netlock
.
writeLock
(
)
.
lock
(
)
;
try
{
Node
rack
=
null
;
if
(
NetworkTopology
.
DEFAULT_RACK
.
equals
(
node
.
getNetworkLocation
(
)
)
)
{
node
.
setNetworkLocation
(
node
.
getNetworkLocation
(
)
+
NetworkTopologyWithNodeGroup
.
DEFAULT_NODEGROUP
)
;
}
Node
nodeGroup
=
getNode
(
node
.
getNetworkLocation
(
)
)
;
if
(
nodeGroup
==
null
)
{
nodeGroup
=
new
InnerNodeWithNodeGroup
(
node
.
getNetworkLocation
(
)
)
;
}
rack
=
getNode
(
nodeGroup
.
getNetworkLocation
(
)
)
;
if
(
rack
!=
null
&&
(
!
(
rack
instanceof
InnerNode
)
||
rack
.
getParent
(
)
==
null
)
)
{
throw
new
IllegalArgumentException
(
+
node
.
toString
(
)
+
)
;
}
if
(
clusterMap
.
add
(
node
)
)
{
private
void
loadMappingProviders
(
)
{
String
[
]
providerNames
=
conf
.
getStrings
(
MAPPING_PROVIDERS_CONFIG_KEY
,
new
String
[
]
{
}
)
;
String
providerKey
;
for
(
String
name
:
providerNames
)
{
providerKey
=
MAPPING_PROVIDER_CONFIG_PREFIX
+
+
name
;
Class
<
?
>
providerClass
=
conf
.
getClass
(
providerKey
,
null
)
;
if
(
providerClass
==
null
)
{
@
Override
@
VisibleForTesting
public
Map
<
String
,
String
>
getServerProperties
(
InetAddress
clientAddress
,
int
ingressPort
)
{
static
private
void
logError
(
int
groupId
,
String
error
)
{
String
keytabName
=
popOptionWithArgument
(
ARG_KEYTAB
,
args
)
;
if
(
keytabName
!=
null
)
{
keytab
=
new
File
(
keytabName
)
;
}
principal
=
popOptionWithArgument
(
ARG_PRINCIPAL
,
args
)
;
String
outf
=
popOptionWithArgument
(
ARG_OUTPUT
,
args
)
;
String
mkl
=
popOptionWithArgument
(
ARG_KEYLEN
,
args
)
;
if
(
mkl
!=
null
)
{
minKeyLength
=
Integer
.
parseInt
(
mkl
)
;
}
securityRequired
=
popOption
(
ARG_SECURE
,
args
)
;
nofail
=
popOption
(
ARG_NOFAIL
,
args
)
;
jaas
=
popOption
(
ARG_JAAS
,
args
)
;
nologin
=
popOption
(
ARG_NOLOGIN
,
args
)
;
checkShortName
=
popOption
(
ARG_VERIFYSHORTNAME
,
args
)
;
String
resource
;
while
(
null
!=
(
resource
=
popOptionWithArgument
(
ARG_RESOURCE
,
args
)
)
)
{
String
identity
;
if
(
keytab
!=
null
)
{
File
kt
=
keytab
.
getCanonicalFile
(
)
;
println
(
,
kt
,
principal
)
;
identity
=
principal
;
failif
(
principal
==
null
,
CAT_KERBEROS
,
)
;
ugi
=
loginUserFromKeytabAndReturnUGI
(
principal
,
kt
.
getPath
(
)
)
;
dumpUGI
(
identity
,
ugi
)
;
validateUGI
(
principal
,
ugi
)
;
title
(
)
;
try
{
setShouldRenewImmediatelyForTests
(
true
)
;
ugi
.
reloginFromKeytab
(
)
;
}
catch
(
IllegalAccessError
e
)
{
warn
(
CAT_UGI
,
)
;
Set
<
String
>
doGetGroups
(
String
user
,
int
goUpHierarchy
)
throws
NamingException
{
DirContext
c
=
getDirContext
(
)
;
NamingEnumeration
<
SearchResult
>
results
=
c
.
search
(
userbaseDN
,
userSearchFilter
,
new
Object
[
]
{
user
}
,
SEARCH_CONTROLS
)
;
if
(
!
results
.
hasMoreElements
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
return
Collections
.
emptySet
(
)
;
}
SearchResult
result
=
results
.
nextElement
(
)
;
Set
<
String
>
groups
=
Collections
.
emptySet
(
)
;
if
(
useOneQuery
)
{
try
{
Attribute
groupDNAttr
=
result
.
getAttributes
(
)
.
get
(
memberOfAttr
)
;
if
(
groupDNAttr
==
null
)
{
throw
new
NamingException
(
+
memberOfAttr
+
+
+
result
.
toString
(
)
)
;
}
groups
=
new
LinkedHashSet
<
>
(
)
;
NamingEnumeration
groupEnumeration
=
groupDNAttr
.
getAll
(
)
;
while
(
groupEnumeration
.
hasMore
(
)
)
{
String
groupDN
=
groupEnumeration
.
next
(
)
.
toString
(
)
;
groups
.
add
(
getRelativeDistinguishedName
(
groupDN
)
)
;
}
}
catch
(
NamingException
e
)
{
Set
<
String
>
groups
=
Collections
.
emptySet
(
)
;
if
(
useOneQuery
)
{
try
{
Attribute
groupDNAttr
=
result
.
getAttributes
(
)
.
get
(
memberOfAttr
)
;
if
(
groupDNAttr
==
null
)
{
throw
new
NamingException
(
+
memberOfAttr
+
+
+
result
.
toString
(
)
)
;
}
groups
=
new
LinkedHashSet
<
>
(
)
;
NamingEnumeration
groupEnumeration
=
groupDNAttr
.
getAll
(
)
;
while
(
groupEnumeration
.
hasMore
(
)
)
{
String
groupDN
=
groupEnumeration
.
next
(
)
.
toString
(
)
;
groups
.
add
(
getRelativeDistinguishedName
(
groupDN
)
)
;
}
}
catch
(
NamingException
e
)
{
LOG
.
info
(
+
,
e
)
;
}
}
if
(
groups
.
isEmpty
(
)
||
goUpHierarchy
>
0
)
{
groups
=
lookupGroup
(
result
,
c
,
goUpHierarchy
)
;
protected
boolean
failover
(
int
attemptsMadeWithSameLdap
,
int
maxAttemptsBeforeFailover
)
{
if
(
attemptsMadeWithSameLdap
>=
maxAttemptsBeforeFailover
)
{
String
previousLdapUrl
=
currentLdapUrl
;
currentLdapUrl
=
ldapUrls
.
next
(
)
;
protected
void
switchBindUser
(
AuthenticationException
e
)
{
BindUserInfo
oldBindUser
=
this
.
currentBindUser
;
currentBindUser
=
this
.
bindUsers
.
next
(
)
;
if
(
!
oldBindUser
.
equals
(
currentBindUser
)
)
{
@
Override
public
synchronized
void
setConf
(
Configuration
conf
)
{
this
.
conf
=
conf
;
String
[
]
urls
=
conf
.
getStrings
(
LDAP_URL_KEY
,
LDAP_URL_DEFAULT
)
;
if
(
urls
==
null
||
urls
.
length
==
0
)
{
throw
new
RuntimeException
(
)
;
}
ldapUrls
=
Iterators
.
cycle
(
urls
)
;
currentLdapUrl
=
ldapUrls
.
next
(
)
;
useSsl
=
conf
.
getBoolean
(
LDAP_USE_SSL_KEY
,
LDAP_USE_SSL_DEFAULT
)
;
if
(
useSsl
)
{
loadSslConf
(
conf
)
;
}
initializeBindUsers
(
)
;
String
baseDN
=
conf
.
getTrimmed
(
BASE_DN_KEY
,
BASE_DN_DEFAULT
)
;
userbaseDN
=
conf
.
getTrimmed
(
USER_BASE_DN_KEY
,
baseDN
)
;
LOG
.
debug
(
,
userbaseDN
)
;
groupbaseDN
=
conf
.
getTrimmed
(
GROUP_BASE_DN_KEY
,
baseDN
)
;
StringBuffer
newProviderPath
=
new
StringBuffer
(
)
;
String
[
]
providers
=
providerPath
.
split
(
)
;
Path
path
=
null
;
for
(
String
provider
:
providers
)
{
try
{
path
=
unnestUri
(
new
URI
(
provider
)
)
;
Class
<
?
extends
FileSystem
>
clazz
=
null
;
try
{
String
scheme
=
path
.
toUri
(
)
.
getScheme
(
)
;
clazz
=
FileSystem
.
getFileSystemClass
(
scheme
,
config
)
;
}
catch
(
IOException
ioe
)
{
if
(
newProviderPath
.
length
(
)
>
0
)
{
newProviderPath
.
append
(
)
;
}
newProviderPath
.
append
(
provider
)
;
}
if
(
clazz
!=
null
)
{
}
AuthMethod
authMethod
=
AuthMethod
.
valueOf
(
authType
.
getMethod
(
)
)
;
if
(
authMethod
==
AuthMethod
.
SIMPLE
)
{
switchToSimple
=
true
;
}
else
{
saslClient
=
createSaslClient
(
authType
)
;
if
(
saslClient
==
null
)
{
continue
;
}
}
selectedAuthType
=
authType
;
break
;
}
if
(
saslClient
==
null
&&
!
switchToSimple
)
{
List
<
String
>
serverAuthMethods
=
new
ArrayList
<
String
>
(
)
;
for
(
SaslAuth
authType
:
authTypes
)
{
serverAuthMethods
.
add
(
authType
.
getMethod
(
)
)
;
}
throw
new
AccessControlException
(
+
serverAuthMethods
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
&&
selectedAuthType
!=
null
)
{
Token
<
?
>
token
=
getServerToken
(
authType
)
;
if
(
token
==
null
)
{
LOG
.
debug
(
+
)
;
return
null
;
}
saslCallback
=
new
SaslClientCallbackHandler
(
token
)
;
break
;
}
case
KERBEROS
:
{
if
(
ugi
.
getRealAuthenticationMethod
(
)
.
getAuthMethod
(
)
!=
AuthMethod
.
KERBEROS
)
{
LOG
.
debug
(
)
;
return
null
;
}
String
serverPrincipal
=
getServerPrincipal
(
authType
)
;
if
(
serverPrincipal
==
null
)
{
LOG
.
debug
(
)
;
return
null
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
saslCallback
=
new
SaslClientCallbackHandler
(
token
)
;
break
;
}
case
KERBEROS
:
{
if
(
ugi
.
getRealAuthenticationMethod
(
)
.
getAuthMethod
(
)
!=
AuthMethod
.
KERBEROS
)
{
LOG
.
debug
(
)
;
return
null
;
}
String
serverPrincipal
=
getServerPrincipal
(
authType
)
;
if
(
serverPrincipal
==
null
)
{
LOG
.
debug
(
)
;
return
null
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
protocol
.
getCanonicalName
(
)
+
+
serverPrincipal
)
;
}
break
;
}
default
:
throw
new
IOException
(
+
method
)
;
}
String
mechanism
=
method
.
getMechanismName
(
)
;
private
Token
<
?
>
getServerToken
(
SaslAuth
authType
)
throws
IOException
{
TokenInfo
tokenInfo
=
SecurityUtil
.
getTokenInfo
(
protocol
,
conf
)
;
@
VisibleForTesting
String
getServerPrincipal
(
SaslAuth
authType
)
throws
IOException
{
KerberosInfo
krbInfo
=
SecurityUtil
.
getKerberosInfo
(
protocol
,
conf
)
;
LOG
.
debug
(
+
protocol
+
+
krbInfo
)
;
if
(
krbInfo
==
null
)
{
return
null
;
}
String
serverKey
=
krbInfo
.
serverPrincipal
(
)
;
if
(
serverKey
==
null
)
{
throw
new
IllegalArgumentException
(
+
protocol
.
getCanonicalName
(
)
)
;
}
String
serverPrincipal
=
new
KerberosPrincipal
(
authType
.
getProtocol
(
)
+
+
authType
.
getServerId
(
)
,
KerberosPrincipal
.
KRB_NT_SRV_HST
)
.
getName
(
)
;
String
serverKeyPattern
=
conf
.
get
(
serverKey
+
)
;
if
(
serverKeyPattern
!=
null
&&
!
serverKeyPattern
.
isEmpty
(
)
)
{
Pattern
pattern
=
GlobPattern
.
compile
(
serverKeyPattern
)
;
if
(
!
pattern
.
matcher
(
serverPrincipal
)
.
matches
(
)
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
+
,
serverPrincipal
,
serverKeyPattern
)
)
;
}
}
else
{
String
confPrincipal
=
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
serverKey
)
,
serverAddr
.
getAddress
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
sendSaslMessage
(
OutputStream
out
,
RpcSaslProto
message
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
serverId
.
isEmpty
(
)
)
{
throw
new
AccessControlException
(
+
+
ugi
.
getUserName
(
)
)
;
}
callback
=
new
SaslGssCallbackHandler
(
)
;
break
;
}
default
:
throw
new
AccessControlException
(
+
authMethod
)
;
}
final
SaslServer
saslServer
;
if
(
ugi
!=
null
)
{
saslServer
=
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
SaslServer
>
(
)
{
@
Override
public
SaslServer
run
(
)
throws
SaslException
{
return
saslFactory
.
createSaslServer
(
mechanism
,
protocol
,
serverId
,
saslProperties
,
callback
)
;
}
}
)
;
}
else
{
saslServer
=
saslFactory
.
createSaslServer
(
mechanism
,
protocol
,
serverId
,
saslProperties
,
callback
)
;
}
if
(
saslServer
==
null
)
{
throw
new
AccessControlException
(
+
mechanism
)
;
}
else
if
(
callback
instanceof
NameCallback
)
{
nc
=
(
NameCallback
)
callback
;
}
else
if
(
callback
instanceof
PasswordCallback
)
{
pc
=
(
PasswordCallback
)
callback
;
}
else
if
(
callback
instanceof
RealmCallback
)
{
continue
;
}
else
{
throw
new
UnsupportedCallbackException
(
callback
,
)
;
}
}
if
(
pc
!=
null
)
{
TokenIdentifier
tokenIdentifier
=
getIdentifier
(
nc
.
getDefaultName
(
)
,
secretManager
)
;
char
[
]
password
=
getPassword
(
tokenIdentifier
)
;
UserGroupInformation
user
=
null
;
user
=
tokenIdentifier
.
getUser
(
)
;
connection
.
attemptingUser
=
user
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
InterfaceAudience
.
Private
@
VisibleForTesting
public
static
void
setTokenServiceUseIp
(
boolean
flag
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
setTokenService
(
Token
<
?
>
token
,
InetSocketAddress
addr
)
{
Text
service
=
buildTokenService
(
addr
)
;
if
(
token
!=
null
)
{
token
.
setService
(
service
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
mapName
+
+
nameId
[
0
]
+
+
nameId
[
1
]
)
;
final
Integer
key
=
staticMapping
.
get
(
parseId
(
nameId
[
1
]
)
)
;
final
String
value
=
nameId
[
0
]
;
if
(
map
.
containsKey
(
key
)
)
{
final
String
prevValue
=
map
.
get
(
key
)
;
if
(
value
.
equals
(
prevValue
)
)
{
continue
;
}
reportDuplicateEntry
(
,
key
,
value
,
key
,
prevValue
)
;
continue
;
}
if
(
map
.
containsValue
(
value
)
)
{
final
Integer
prevKey
=
map
.
inverse
(
)
.
get
(
value
)
;
reportDuplicateEntry
(
,
key
,
value
,
prevKey
,
value
)
;
continue
;
}
map
.
put
(
key
,
value
)
;
updated
=
true
;
final
Integer
key
=
staticMapping
.
get
(
parseId
(
nameId
[
1
]
)
)
;
final
String
value
=
nameId
[
0
]
;
if
(
map
.
containsKey
(
key
)
)
{
final
String
prevValue
=
map
.
get
(
key
)
;
if
(
value
.
equals
(
prevValue
)
)
{
continue
;
}
reportDuplicateEntry
(
,
key
,
value
,
key
,
prevValue
)
;
continue
;
}
if
(
map
.
containsValue
(
value
)
)
{
final
Integer
prevKey
=
map
.
inverse
(
)
.
get
(
value
)
;
reportDuplicateEntry
(
,
key
,
value
,
prevKey
,
value
)
;
continue
;
}
map
.
put
(
key
,
value
)
;
updated
=
true
;
}
LOG
.
debug
(
+
mapName
+
+
map
.
size
(
)
)
;
private
boolean
checkSupportedPlatform
(
)
{
if
(
!
OS
.
startsWith
(
)
&&
!
OS
.
startsWith
(
)
&&
!
OS
.
equals
(
)
&&
!
OS
.
contains
(
)
)
{
private
synchronized
void
updateStaticMapping
(
)
throws
IOException
{
final
boolean
init
=
(
staticMapping
==
null
)
;
if
(
staticMappingFile
.
exists
(
)
)
{
long
lmTime
=
staticMappingFile
.
lastModified
(
)
;
if
(
lmTime
!=
lastModificationTimeStaticMap
)
{
private
static
UserGroupInformation
createLoginUser
(
Subject
subject
)
throws
IOException
{
UserGroupInformation
realUser
=
doSubjectLogin
(
subject
,
null
)
;
UserGroupInformation
loginUser
=
null
;
try
{
String
proxyUser
=
System
.
getenv
(
HADOOP_PROXY_USER
)
;
if
(
proxyUser
==
null
)
{
proxyUser
=
System
.
getProperty
(
HADOOP_PROXY_USER
)
;
}
loginUser
=
proxyUser
==
null
?
realUser
:
createProxyUser
(
proxyUser
,
realUser
)
;
final
Collection
<
String
>
tokenFileLocations
=
new
LinkedHashSet
<
>
(
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
System
.
getProperty
(
HADOOP_TOKEN_FILES
)
)
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
conf
.
get
(
HADOOP_TOKEN_FILES
)
)
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
System
.
getenv
(
HADOOP_TOKEN_FILE_LOCATION
)
)
)
;
for
(
String
tokenFileLocation
:
tokenFileLocations
)
{
if
(
tokenFileLocation
!=
null
&&
tokenFileLocation
.
length
(
)
>
0
)
{
File
tokenFile
=
new
File
(
tokenFileLocation
)
;
try
{
String
proxyUser
=
System
.
getenv
(
HADOOP_PROXY_USER
)
;
if
(
proxyUser
==
null
)
{
proxyUser
=
System
.
getProperty
(
HADOOP_PROXY_USER
)
;
}
loginUser
=
proxyUser
==
null
?
realUser
:
createProxyUser
(
proxyUser
,
realUser
)
;
final
Collection
<
String
>
tokenFileLocations
=
new
LinkedHashSet
<
>
(
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
System
.
getProperty
(
HADOOP_TOKEN_FILES
)
)
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
conf
.
get
(
HADOOP_TOKEN_FILES
)
)
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
System
.
getenv
(
HADOOP_TOKEN_FILE_LOCATION
)
)
)
;
for
(
String
tokenFileLocation
:
tokenFileLocations
)
{
if
(
tokenFileLocation
!=
null
&&
tokenFileLocation
.
length
(
)
>
0
)
{
File
tokenFile
=
new
File
(
tokenFileLocation
)
;
LOG
.
debug
(
,
tokenFile
.
getCanonicalPath
(
)
)
;
if
(
tokenFile
.
exists
(
)
&&
tokenFile
.
isFile
(
)
)
{
Credentials
cred
=
Credentials
.
readTokenStorageFile
(
tokenFile
,
conf
)
;
proxyUser
=
System
.
getProperty
(
HADOOP_PROXY_USER
)
;
}
loginUser
=
proxyUser
==
null
?
realUser
:
createProxyUser
(
proxyUser
,
realUser
)
;
final
Collection
<
String
>
tokenFileLocations
=
new
LinkedHashSet
<
>
(
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
System
.
getProperty
(
HADOOP_TOKEN_FILES
)
)
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
conf
.
get
(
HADOOP_TOKEN_FILES
)
)
)
;
tokenFileLocations
.
addAll
(
getTrimmedStringCollection
(
System
.
getenv
(
HADOOP_TOKEN_FILE_LOCATION
)
)
)
;
for
(
String
tokenFileLocation
:
tokenFileLocations
)
{
if
(
tokenFileLocation
!=
null
&&
tokenFileLocation
.
length
(
)
>
0
)
{
File
tokenFile
=
new
File
(
tokenFileLocation
)
;
LOG
.
debug
(
,
tokenFile
.
getCanonicalPath
(
)
)
;
if
(
tokenFile
.
exists
(
)
&&
tokenFile
.
isFile
(
)
)
{
Credentials
cred
=
Credentials
.
readTokenStorageFile
(
tokenFile
,
conf
)
;
LOG
.
debug
(
,
cred
.
numberOfTokens
(
)
,
tokenFile
.
getCanonicalPath
(
)
)
;
loginUser
.
addCredentials
(
cred
)
;
}
else
{
else
{
LOG
.
info
(
,
tokenFile
.
getCanonicalPath
(
)
)
;
}
}
}
final
Collection
<
String
>
tokensBase64
=
new
LinkedHashSet
<
>
(
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
System
.
getProperty
(
HADOOP_TOKENS
)
)
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
conf
.
get
(
HADOOP_TOKENS
)
)
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
System
.
getenv
(
HADOOP_TOKEN
)
)
)
;
int
numTokenBase64
=
0
;
for
(
String
tokenBase64
:
tokensBase64
)
{
if
(
tokenBase64
!=
null
&&
tokenBase64
.
length
(
)
>
0
)
{
try
{
Token
<
TokenIdentifier
>
token
=
new
Token
<
>
(
)
;
token
.
decodeFromUrlString
(
tokenBase64
)
;
Credentials
cred
=
new
Credentials
(
)
;
cred
.
addToken
(
token
.
getService
(
)
,
token
)
;
loginUser
.
addCredentials
(
cred
)
;
}
}
}
final
Collection
<
String
>
tokensBase64
=
new
LinkedHashSet
<
>
(
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
System
.
getProperty
(
HADOOP_TOKENS
)
)
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
conf
.
get
(
HADOOP_TOKENS
)
)
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
System
.
getenv
(
HADOOP_TOKEN
)
)
)
;
int
numTokenBase64
=
0
;
for
(
String
tokenBase64
:
tokensBase64
)
{
if
(
tokenBase64
!=
null
&&
tokenBase64
.
length
(
)
>
0
)
{
try
{
Token
<
TokenIdentifier
>
token
=
new
Token
<
>
(
)
;
token
.
decodeFromUrlString
(
tokenBase64
)
;
Credentials
cred
=
new
Credentials
(
)
;
cred
.
addToken
(
token
.
getService
(
)
,
token
)
;
loginUser
.
addCredentials
(
cred
)
;
numTokenBase64
++
;
}
catch
(
IOException
ioe
)
{
}
final
Collection
<
String
>
tokensBase64
=
new
LinkedHashSet
<
>
(
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
System
.
getProperty
(
HADOOP_TOKENS
)
)
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
conf
.
get
(
HADOOP_TOKENS
)
)
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
System
.
getenv
(
HADOOP_TOKEN
)
)
)
;
int
numTokenBase64
=
0
;
for
(
String
tokenBase64
:
tokensBase64
)
{
if
(
tokenBase64
!=
null
&&
tokenBase64
.
length
(
)
>
0
)
{
try
{
Token
<
TokenIdentifier
>
token
=
new
Token
<
>
(
)
;
token
.
decodeFromUrlString
(
tokenBase64
)
;
Credentials
cred
=
new
Credentials
(
)
;
cred
.
addToken
(
token
.
getService
(
)
,
token
)
;
loginUser
.
addCredentials
(
cred
)
;
numTokenBase64
++
;
}
catch
(
IOException
ioe
)
{
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
conf
.
get
(
HADOOP_TOKENS
)
)
)
;
tokensBase64
.
addAll
(
getTrimmedStringCollection
(
System
.
getenv
(
HADOOP_TOKEN
)
)
)
;
int
numTokenBase64
=
0
;
for
(
String
tokenBase64
:
tokensBase64
)
{
if
(
tokenBase64
!=
null
&&
tokenBase64
.
length
(
)
>
0
)
{
try
{
Token
<
TokenIdentifier
>
token
=
new
Token
<
>
(
)
;
token
.
decodeFromUrlString
(
tokenBase64
)
;
Credentials
cred
=
new
Credentials
(
)
;
cred
.
addToken
(
token
.
getService
(
)
,
token
)
;
loginUser
.
addCredentials
(
cred
)
;
numTokenBase64
++
;
}
catch
(
IOException
ioe
)
{
LOG
.
error
(
,
tokenBase64
,
ioe
.
getMessage
(
)
)
;
}
}
}
if
(
numTokenBase64
>
0
)
{
@
InterfaceAudience
.
Public
@
InterfaceStability
.
Evolving
public
<
T
>
T
doAs
(
PrivilegedAction
<
T
>
action
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
InterfaceAudience
.
Public
@
InterfaceStability
.
Evolving
public
<
T
>
T
doAs
(
PrivilegedExceptionAction
<
T
>
action
)
throws
IOException
,
InterruptedException
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
InterfaceAudience
.
LimitedPrivate
(
{
,
}
)
@
InterfaceStability
.
Unstable
public
static
void
logUserInfo
(
Logger
log
,
String
caption
,
UserGroupInformation
ugi
)
throws
IOException
{
if
(
log
.
isDebugEnabled
(
)
)
{
@
InterfaceAudience
.
LimitedPrivate
(
{
,
}
)
@
InterfaceStability
.
Unstable
public
static
void
logUserInfo
(
Logger
log
,
String
caption
,
UserGroupInformation
ugi
)
throws
IOException
{
if
(
log
.
isDebugEnabled
(
)
)
{
log
.
debug
(
caption
+
+
ugi
)
;
for
(
Token
<
?
>
token
:
ugi
.
getTokens
(
)
)
{
protected
void
initFileSystem
(
URI
keystoreUri
)
throws
IOException
{
path
=
ProviderUtils
.
unnestUri
(
keystoreUri
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
initFileSystem
(
URI
uri
)
throws
IOException
{
super
.
initFileSystem
(
uri
)
;
try
{
file
=
new
File
(
new
URI
(
getPath
(
)
.
toString
(
)
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
initFileSystem
(
URI
uri
)
throws
IOException
{
super
.
initFileSystem
(
uri
)
;
try
{
file
=
new
File
(
new
URI
(
getPath
(
)
.
toString
(
)
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
file
+
)
;
if
(
file
.
exists
(
)
)
{
@
Override
protected
void
doFilter
(
FilterChain
filterChain
,
HttpServletRequest
request
,
HttpServletResponse
response
)
throws
IOException
,
ServletException
{
final
HttpServletRequest
lowerCaseRequest
=
toLowerCase
(
request
)
;
String
doAsUser
=
lowerCaseRequest
.
getParameter
(
DO_AS
)
;
if
(
doAsUser
!=
null
&&
!
doAsUser
.
equals
(
request
.
getRemoteUser
(
)
)
)
{
String
clientKey
=
krbInfo
.
clientPrincipal
(
)
;
if
(
clientKey
!=
null
&&
!
clientKey
.
isEmpty
(
)
)
{
try
{
clientPrincipal
=
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
clientKey
)
,
addr
)
;
}
catch
(
IOException
e
)
{
throw
(
AuthorizationException
)
new
AuthorizationException
(
+
addr
+
+
user
+
+
protocol
)
.
initCause
(
e
)
;
}
}
}
}
if
(
(
clientPrincipal
!=
null
&&
!
clientPrincipal
.
equals
(
user
.
getUserName
(
)
)
)
||
acls
.
length
!=
2
||
!
acls
[
0
]
.
isUserAllowed
(
user
)
||
acls
[
1
]
.
isUserAllowed
(
user
)
)
{
String
cause
=
clientPrincipal
!=
null
?
+
clientPrincipal
:
;
AUDITLOG
.
warn
(
AUTHZ_FAILED_FOR
+
user
+
+
protocol
+
cause
)
;
throw
new
AuthorizationException
(
+
user
+
+
protocol
+
cause
)
;
}
if
(
addr
!=
null
)
{
String
hostAddress
=
addr
.
getHostAddress
(
)
;
if
(
hosts
.
length
!=
2
||
!
hosts
[
0
]
.
includes
(
hostAddress
)
||
hosts
[
1
]
.
includes
(
hostAddress
)
)
{
AUDITLOG
.
warn
(
AUTHZ_FAILED_FOR
+
+
protocol
+
+
hostAddress
)
;
throw
new
AuthorizationException
(
+
hostAddress
+
+
protocol
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
}
return
;
}
if
(
!
areOriginsAllowed
(
originsList
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
originsList
+
)
;
}
return
;
}
String
accessControlRequestMethod
=
req
.
getHeader
(
ACCESS_CONTROL_REQUEST_METHOD
)
;
if
(
!
isMethodAllowed
(
accessControlRequestMethod
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
accessControlRequestMethod
+
)
;
}
return
;
}
String
accessControlRequestHeaders
=
req
.
getHeader
(
ACCESS_CONTROL_REQUEST_HEADERS
)
;
if
(
!
areHeadersAllowed
(
accessControlRequestHeaders
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
initializeSSLContext
(
SSLChannelMode
preferredChannelMode
)
throws
NoSuchAlgorithmException
,
KeyManagementException
,
IOException
{
private
String
[
]
alterCipherList
(
String
[
]
defaultCiphers
)
{
ArrayList
<
String
>
preferredSuites
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
defaultCiphers
.
length
;
i
++
)
{
if
(
defaultCiphers
[
i
]
.
contains
(
)
)
{
String
keystoreType
=
conf
.
get
(
resolvePropertyName
(
mode
,
SSL_KEYSTORE_TYPE_TPL_KEY
)
,
DEFAULT_KEYSTORE_TYPE
)
;
KeyStore
keystore
=
KeyStore
.
getInstance
(
keystoreType
)
;
String
keystoreKeyPassword
=
null
;
if
(
requireClientCert
||
mode
==
SSLFactory
.
Mode
.
SERVER
)
{
String
locationProperty
=
resolvePropertyName
(
mode
,
SSL_KEYSTORE_LOCATION_TPL_KEY
)
;
String
keystoreLocation
=
conf
.
get
(
locationProperty
,
)
;
if
(
keystoreLocation
.
isEmpty
(
)
)
{
throw
new
GeneralSecurityException
(
+
locationProperty
+
)
;
}
String
passwordProperty
=
resolvePropertyName
(
mode
,
SSL_KEYSTORE_PASSWORD_TPL_KEY
)
;
String
keystorePassword
=
getPassword
(
conf
,
passwordProperty
,
)
;
if
(
keystorePassword
.
isEmpty
(
)
)
{
throw
new
GeneralSecurityException
(
+
passwordProperty
+
)
;
}
String
keyPasswordProperty
=
resolvePropertyName
(
mode
,
SSL_KEYSTORE_KEYPASSWORD_TPL_KEY
)
;
keystoreKeyPassword
=
getPassword
(
conf
,
keyPasswordProperty
,
keystorePassword
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
throw
new
GeneralSecurityException
(
+
locationProperty
+
)
;
}
String
passwordProperty
=
resolvePropertyName
(
mode
,
SSL_KEYSTORE_PASSWORD_TPL_KEY
)
;
String
keystorePassword
=
getPassword
(
conf
,
passwordProperty
,
)
;
if
(
keystorePassword
.
isEmpty
(
)
)
{
throw
new
GeneralSecurityException
(
+
passwordProperty
+
)
;
}
String
keyPasswordProperty
=
resolvePropertyName
(
mode
,
SSL_KEYSTORE_KEYPASSWORD_TPL_KEY
)
;
keystoreKeyPassword
=
getPassword
(
conf
,
keyPasswordProperty
,
keystorePassword
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
mode
.
toString
(
)
+
+
keystoreLocation
)
;
}
InputStream
is
=
Files
.
newInputStream
(
Paths
.
get
(
keystoreLocation
)
)
;
try
{
keystore
.
load
(
is
,
keystorePassword
.
toCharArray
(
)
)
;
}
finally
{
is
.
close
(
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
finally
{
is
.
close
(
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
mode
.
toString
(
)
+
+
keystoreLocation
)
;
}
}
else
{
keystore
.
load
(
null
,
null
)
;
}
KeyManagerFactory
keyMgrFactory
=
KeyManagerFactory
.
getInstance
(
SSLFactory
.
SSLCERTIFICATE
)
;
keyMgrFactory
.
init
(
keystore
,
(
keystoreKeyPassword
!=
null
)
?
keystoreKeyPassword
.
toCharArray
(
)
:
null
)
;
keyManagers
=
keyMgrFactory
.
getKeyManagers
(
)
;
String
truststoreType
=
conf
.
get
(
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_TYPE_TPL_KEY
)
,
DEFAULT_KEYSTORE_TYPE
)
;
String
locationProperty
=
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_LOCATION_TPL_KEY
)
;
String
truststoreLocation
=
conf
.
get
(
locationProperty
,
)
;
if
(
!
truststoreLocation
.
isEmpty
(
)
)
{
String
passwordProperty
=
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_PASSWORD_TPL_KEY
)
;
String
truststorePassword
=
getPassword
(
conf
,
passwordProperty
,
)
;
LOG
.
debug
(
mode
.
toString
(
)
+
+
keystoreLocation
)
;
}
}
else
{
keystore
.
load
(
null
,
null
)
;
}
KeyManagerFactory
keyMgrFactory
=
KeyManagerFactory
.
getInstance
(
SSLFactory
.
SSLCERTIFICATE
)
;
keyMgrFactory
.
init
(
keystore
,
(
keystoreKeyPassword
!=
null
)
?
keystoreKeyPassword
.
toCharArray
(
)
:
null
)
;
keyManagers
=
keyMgrFactory
.
getKeyManagers
(
)
;
String
truststoreType
=
conf
.
get
(
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_TYPE_TPL_KEY
)
,
DEFAULT_KEYSTORE_TYPE
)
;
String
locationProperty
=
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_LOCATION_TPL_KEY
)
;
String
truststoreLocation
=
conf
.
get
(
locationProperty
,
)
;
if
(
!
truststoreLocation
.
isEmpty
(
)
)
{
String
passwordProperty
=
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_PASSWORD_TPL_KEY
)
;
String
truststorePassword
=
getPassword
(
conf
,
passwordProperty
,
)
;
if
(
truststorePassword
.
isEmpty
(
)
)
{
truststorePassword
=
null
;
}
long
truststoreReloadInterval
=
conf
.
getLong
(
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY
)
,
DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL
)
;
keystore
.
load
(
null
,
null
)
;
}
KeyManagerFactory
keyMgrFactory
=
KeyManagerFactory
.
getInstance
(
SSLFactory
.
SSLCERTIFICATE
)
;
keyMgrFactory
.
init
(
keystore
,
(
keystoreKeyPassword
!=
null
)
?
keystoreKeyPassword
.
toCharArray
(
)
:
null
)
;
keyManagers
=
keyMgrFactory
.
getKeyManagers
(
)
;
String
truststoreType
=
conf
.
get
(
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_TYPE_TPL_KEY
)
,
DEFAULT_KEYSTORE_TYPE
)
;
String
locationProperty
=
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_LOCATION_TPL_KEY
)
;
String
truststoreLocation
=
conf
.
get
(
locationProperty
,
)
;
if
(
!
truststoreLocation
.
isEmpty
(
)
)
{
String
passwordProperty
=
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_PASSWORD_TPL_KEY
)
;
String
truststorePassword
=
getPassword
(
conf
,
passwordProperty
,
)
;
if
(
truststorePassword
.
isEmpty
(
)
)
{
truststorePassword
=
null
;
}
long
truststoreReloadInterval
=
conf
.
getLong
(
resolvePropertyName
(
mode
,
SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY
)
,
DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
mode
.
toString
(
)
+
+
truststoreLocation
)
;
X509TrustManager
loadTrustManager
(
)
throws
IOException
,
GeneralSecurityException
{
X509TrustManager
trustManager
=
null
;
KeyStore
ks
=
KeyStore
.
getInstance
(
type
)
;
InputStream
in
=
Files
.
newInputStream
(
file
.
toPath
(
)
)
;
try
{
ks
.
load
(
in
,
(
password
==
null
)
?
null
:
password
.
toCharArray
(
)
)
;
lastLoaded
=
file
.
lastModified
(
)
;
private
void
disableExcludedCiphers
(
SSLEngine
sslEngine
)
{
String
[
]
cipherSuites
=
sslEngine
.
getEnabledCipherSuites
(
)
;
ArrayList
<
String
>
defaultEnabledCipherSuites
=
new
ArrayList
<
String
>
(
Arrays
.
asList
(
cipherSuites
)
)
;
Iterator
iterator
=
excludeCiphers
.
iterator
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
String
cipherName
=
(
String
)
iterator
.
next
(
)
;
if
(
defaultEnabledCipherSuites
.
contains
(
cipherName
)
)
{
defaultEnabledCipherSuites
.
remove
(
cipherName
)
;
public
static
void
getTokenFile
(
File
tokenFile
,
String
fileFormat
,
Text
alias
,
Text
service
,
String
url
,
String
renewer
,
Configuration
conf
)
throws
Exception
{
Token
<
?
>
token
=
null
;
Credentials
creds
=
tokenFile
.
exists
(
)
?
Credentials
.
readTokenStorageFile
(
tokenFile
,
conf
)
:
new
Credentials
(
)
;
ServiceLoader
<
DtFetcher
>
loader
=
ServiceLoader
.
load
(
DtFetcher
.
class
)
;
Iterator
<
DtFetcher
>
iterator
=
loader
.
iterator
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
DtFetcher
fetcher
;
try
{
fetcher
=
iterator
.
next
(
)
;
}
catch
(
ServiceConfigurationError
e
)
{
LOG
.
debug
(
,
e
)
;
continue
;
}
if
(
matchService
(
fetcher
,
service
,
url
)
)
{
if
(
!
fetcher
.
isTokenRequired
(
)
)
{
String
message
=
+
service
+
+
+
;
DtFetcher
fetcher
;
try
{
fetcher
=
iterator
.
next
(
)
;
}
catch
(
ServiceConfigurationError
e
)
{
LOG
.
debug
(
,
e
)
;
continue
;
}
if
(
matchService
(
fetcher
,
service
,
url
)
)
{
if
(
!
fetcher
.
isTokenRequired
(
)
)
{
String
message
=
+
service
+
+
+
;
LOG
.
error
(
message
)
;
throw
new
IllegalArgumentException
(
message
)
;
}
token
=
fetcher
.
addDelegationTokens
(
conf
,
creds
,
renewer
,
stripPrefix
(
url
)
)
;
}
}
if
(
alias
!=
null
)
{
if
(
token
==
null
)
{
String
message
=
+
service
+
+
+
alias
+
+
;
LOG
.
debug
(
,
e
)
;
continue
;
}
if
(
matchService
(
fetcher
,
service
,
url
)
)
{
if
(
!
fetcher
.
isTokenRequired
(
)
)
{
String
message
=
+
service
+
+
+
;
LOG
.
error
(
message
)
;
throw
new
IllegalArgumentException
(
message
)
;
}
token
=
fetcher
.
addDelegationTokens
(
conf
,
creds
,
renewer
,
stripPrefix
(
url
)
)
;
}
}
if
(
alias
!=
null
)
{
if
(
token
==
null
)
{
String
message
=
+
service
+
+
+
alias
+
+
;
LOG
.
error
(
message
)
;
throw
new
IOException
(
message
)
;
}
Token
<
?
>
aliasedToken
=
token
.
copyToken
(
)
;
aliasedToken
.
setService
(
alias
)
;
public
static
void
removeTokenFromFile
(
boolean
cancel
,
File
tokenFile
,
String
fileFormat
,
Text
alias
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
Credentials
newCreds
=
new
Credentials
(
)
;
Credentials
creds
=
Credentials
.
readTokenStorageFile
(
tokenFile
,
conf
)
;
for
(
Token
<
?
>
token
:
creds
.
getAllTokens
(
)
)
{
if
(
matchAlias
(
token
,
alias
)
)
{
if
(
token
.
isManaged
(
)
&&
cancel
)
{
token
.
cancel
(
conf
)
;
public
static
void
renewTokenFile
(
File
tokenFile
,
String
fileFormat
,
Text
alias
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
Credentials
creds
=
Credentials
.
readTokenStorageFile
(
tokenFile
,
conf
)
;
for
(
Token
<
?
>
token
:
creds
.
getAllTokens
(
)
)
{
if
(
token
.
isManaged
(
)
&&
matchAlias
(
token
,
alias
)
)
{
long
result
=
token
.
renew
(
conf
)
;
@
Override
protected
synchronized
byte
[
]
createPassword
(
TokenIdent
identifier
)
{
int
sequenceNum
;
long
now
=
Time
.
now
(
)
;
sequenceNum
=
incrementDelegationTokenSeqNum
(
)
;
identifier
.
setIssueDate
(
now
)
;
identifier
.
setMaxDate
(
now
+
tokenMaxLifetime
)
;
identifier
.
setMasterKeyId
(
currentKey
.
getKeyId
(
)
)
;
identifier
.
setSequenceNumber
(
sequenceNum
)
;
public
synchronized
long
renewToken
(
Token
<
TokenIdent
>
token
,
String
renewer
)
throws
InvalidToken
,
IOException
{
ByteArrayInputStream
buf
=
new
ByteArrayInputStream
(
token
.
getIdentifier
(
)
)
;
DataInputStream
in
=
new
DataInputStream
(
buf
)
;
TokenIdent
id
=
createIdentifier
(
)
;
id
.
readFields
(
in
)
;
public
synchronized
TokenIdent
cancelToken
(
Token
<
TokenIdent
>
token
,
String
canceller
)
throws
IOException
{
ByteArrayInputStream
buf
=
new
ByteArrayInputStream
(
token
.
getIdentifier
(
)
)
;
DataInputStream
in
=
new
DataInputStream
(
buf
)
;
TokenIdent
id
=
createIdentifier
(
)
;
id
.
readFields
(
in
)
;
protected
void
logExpireTokens
(
Collection
<
TokenIdent
>
expiredTokens
)
throws
IOException
{
for
(
TokenIdent
ident
:
expiredTokens
)
{
logExpireToken
(
ident
)
;
catch
(
Exception
e
)
{
throw
new
IOException
(
,
e
)
;
}
}
else
{
CuratorFramework
nullNsFw
=
zkClient
.
usingNamespace
(
null
)
;
EnsurePath
ensureNs
=
nullNsFw
.
newNamespaceAwareEnsurePath
(
+
zkClient
.
getNamespace
(
)
)
;
try
{
ensureNs
.
ensure
(
nullNsFw
.
getZookeeperClient
(
)
)
;
}
catch
(
Exception
e
)
{
throw
new
IOException
(
,
e
)
;
}
}
listenerThreadPool
=
Executors
.
newSingleThreadExecutor
(
)
;
try
{
delTokSeqCounter
=
new
SharedCount
(
zkClient
,
ZK_DTSM_SEQNUM_ROOT
,
0
)
;
if
(
delTokSeqCounter
!=
null
)
{
delTokSeqCounter
.
start
(
)
;
}
currentSeqNum
=
incrSharedCount
(
delTokSeqCounter
,
seqNumBatchSize
)
;
private
void
loadFromZKCache
(
final
boolean
isTokenCache
)
{
final
String
cacheName
=
isTokenCache
?
:
;
final
String
cacheName
=
isTokenCache
?
:
;
LOG
.
info
(
,
cacheName
)
;
final
List
<
ChildData
>
children
;
if
(
isTokenCache
)
{
children
=
tokenCache
.
getCurrentData
(
)
;
}
else
{
children
=
keyCache
.
getCurrentData
(
)
;
}
int
count
=
0
;
for
(
ChildData
child
:
children
)
{
try
{
if
(
isTokenCache
)
{
processTokenAddOrUpdate
(
child
.
getData
(
)
)
;
}
else
{
processKeyAddOrUpdate
(
child
.
getData
(
)
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
cacheName
)
;
final
List
<
ChildData
>
children
;
if
(
isTokenCache
)
{
children
=
tokenCache
.
getCurrentData
(
)
;
}
else
{
children
=
keyCache
.
getCurrentData
(
)
;
}
int
count
=
0
;
for
(
ChildData
child
:
children
)
{
try
{
if
(
isTokenCache
)
{
processTokenAddOrUpdate
(
child
.
getData
(
)
)
;
}
else
{
processKeyAddOrUpdate
(
child
.
getData
(
)
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
child
.
getPath
(
)
)
;
}
int
count
=
0
;
for
(
ChildData
child
:
children
)
{
try
{
if
(
isTokenCache
)
{
processTokenAddOrUpdate
(
child
.
getData
(
)
)
;
}
else
{
processKeyAddOrUpdate
(
child
.
getData
(
)
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
child
.
getPath
(
)
)
;
LOG
.
debug
(
,
e
)
;
++
count
;
}
}
if
(
isTokenCache
)
{
syncTokenOwnerStats
(
)
;
}
if
(
count
>
0
)
{
LOG
.
warn
(
,
count
,
cacheName
)
;
super
.
stopThreads
(
)
;
try
{
if
(
tokenCache
!=
null
)
{
tokenCache
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
if
(
delTokSeqCounter
!=
null
)
{
delTokSeqCounter
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
if
(
keyIdSeqCounter
!=
null
)
{
keyIdSeqCounter
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
if
(
delTokSeqCounter
!=
null
)
{
delTokSeqCounter
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
if
(
keyIdSeqCounter
!=
null
)
{
keyIdSeqCounter
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
if
(
keyCache
!=
null
)
{
keyCache
.
close
(
)
;
if
(
delTokSeqCounter
!=
null
)
{
delTokSeqCounter
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
if
(
keyIdSeqCounter
!=
null
)
{
keyIdSeqCounter
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
if
(
keyCache
!=
null
)
{
keyCache
.
close
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
try
{
@
Override
protected
int
incrementDelegationTokenSeqNum
(
)
{
if
(
currentSeqNum
>=
currentMaxSeqNum
)
{
try
{
currentSeqNum
=
incrSharedCount
(
delTokSeqCounter
,
seqNumBatchSize
)
;
currentMaxSeqNum
=
currentSeqNum
+
seqNumBatchSize
;
byte
[
]
data
=
zkClient
.
getData
(
)
.
forPath
(
nodePath
)
;
if
(
(
data
==
null
)
||
(
data
.
length
==
0
)
)
{
return
null
;
}
ByteArrayInputStream
bin
=
new
ByteArrayInputStream
(
data
)
;
DataInputStream
din
=
new
DataInputStream
(
bin
)
;
createIdentifier
(
)
.
readFields
(
din
)
;
long
renewDate
=
din
.
readLong
(
)
;
int
pwdLen
=
din
.
readInt
(
)
;
byte
[
]
password
=
new
byte
[
pwdLen
]
;
int
numRead
=
din
.
read
(
password
,
0
,
pwdLen
)
;
if
(
numRead
>
-
1
)
{
DelegationTokenInformation
tokenInfo
=
new
DelegationTokenInformation
(
renewDate
,
password
)
;
return
tokenInfo
;
}
}
catch
(
KeeperException
.
NoNodeException
e
)
{
if
(
!
quiet
)
{
private
void
addOrUpdateDelegationKey
(
DelegationKey
key
,
boolean
isUpdate
)
throws
IOException
{
String
nodeCreatePath
=
getNodePath
(
ZK_DTSM_MASTER_KEY_ROOT
,
DELEGATION_KEY_PREFIX
+
key
.
getKeyId
(
)
)
;
ByteArrayOutputStream
os
=
new
ByteArrayOutputStream
(
)
;
DataOutputStream
fsOut
=
new
DataOutputStream
(
os
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
addOrUpdateDelegationKey
(
DelegationKey
key
,
boolean
isUpdate
)
throws
IOException
{
String
nodeCreatePath
=
getNodePath
(
ZK_DTSM_MASTER_KEY_ROOT
,
DELEGATION_KEY_PREFIX
+
key
.
getKeyId
(
)
)
;
ByteArrayOutputStream
os
=
new
ByteArrayOutputStream
(
)
;
DataOutputStream
fsOut
=
new
DataOutputStream
(
os
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
key
.
getKeyId
(
)
)
;
}
key
.
write
(
fsOut
)
;
try
{
if
(
zkClient
.
checkExists
(
)
.
forPath
(
nodeCreatePath
)
!=
null
)
{
zkClient
.
setData
(
)
.
forPath
(
nodeCreatePath
,
os
.
toByteArray
(
)
)
.
setVersion
(
-
1
)
;
if
(
!
isUpdate
)
{
LOG
.
debug
(
+
nodeCreatePath
+
)
;
}
}
else
{
zkClient
.
create
(
)
.
withMode
(
CreateMode
.
PERSISTENT
)
.
forPath
(
nodeCreatePath
,
os
.
toByteArray
(
)
)
;
if
(
isUpdate
)
{
ByteArrayOutputStream
os
=
new
ByteArrayOutputStream
(
)
;
DataOutputStream
fsOut
=
new
DataOutputStream
(
os
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
key
.
getKeyId
(
)
)
;
}
key
.
write
(
fsOut
)
;
try
{
if
(
zkClient
.
checkExists
(
)
.
forPath
(
nodeCreatePath
)
!=
null
)
{
zkClient
.
setData
(
)
.
forPath
(
nodeCreatePath
,
os
.
toByteArray
(
)
)
.
setVersion
(
-
1
)
;
if
(
!
isUpdate
)
{
LOG
.
debug
(
+
nodeCreatePath
+
)
;
}
}
else
{
zkClient
.
create
(
)
.
withMode
(
CreateMode
.
PERSISTENT
)
.
forPath
(
nodeCreatePath
,
os
.
toByteArray
(
)
)
;
if
(
isUpdate
)
{
LOG
.
debug
(
+
nodeCreatePath
+
)
;
}
}
}
catch
(
KeeperException
.
NodeExistsException
ne
)
{
@
Override
protected
void
removeStoredMasterKey
(
DelegationKey
key
)
{
String
nodeRemovePath
=
getNodePath
(
ZK_DTSM_MASTER_KEY_ROOT
,
DELEGATION_KEY_PREFIX
+
key
.
getKeyId
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
void
removeStoredToken
(
TokenIdent
ident
,
boolean
checkAgainstZkBeforeDeletion
)
throws
IOException
{
String
nodeRemovePath
=
getNodePath
(
ZK_DTSM_TOKENS_ROOT
,
DELEGATION_TOKEN_PREFIX
+
ident
.
getSequenceNumber
(
)
)
;
try
{
DelegationTokenInformation
dtInfo
=
getTokenInfoFromZK
(
ident
,
true
)
;
if
(
dtInfo
!=
null
)
{
if
(
checkAgainstZkBeforeDeletion
&&
dtInfo
.
getRenewDate
(
)
>
now
(
)
)
{
String
nodeRemovePath
=
getNodePath
(
ZK_DTSM_TOKENS_ROOT
,
DELEGATION_TOKEN_PREFIX
+
ident
.
getSequenceNumber
(
)
)
;
try
{
DelegationTokenInformation
dtInfo
=
getTokenInfoFromZK
(
ident
,
true
)
;
if
(
dtInfo
!=
null
)
{
if
(
checkAgainstZkBeforeDeletion
&&
dtInfo
.
getRenewDate
(
)
>
now
(
)
)
{
LOG
.
info
(
+
nodeRemovePath
+
)
;
return
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
ident
.
getSequenceNumber
(
)
)
;
}
while
(
zkClient
.
checkExists
(
)
.
forPath
(
nodeRemovePath
)
!=
null
)
{
try
{
zkClient
.
delete
(
)
.
guaranteed
(
)
.
forPath
(
nodeRemovePath
)
;
}
catch
(
NoNodeException
nne
)
{
LOG
.
debug
(
+
nodeRemovePath
)
;
}
}
}
else
{
protected
void
addOrUpdateToken
(
TokenIdent
ident
,
DelegationTokenInformation
info
,
boolean
isUpdate
)
throws
Exception
{
String
nodeCreatePath
=
getNodePath
(
ZK_DTSM_TOKENS_ROOT
,
DELEGATION_TOKEN_PREFIX
+
ident
.
getSequenceNumber
(
)
)
;
try
(
ByteArrayOutputStream
tokenOs
=
new
ByteArrayOutputStream
(
)
;
DataOutputStream
tokenOut
=
new
DataOutputStream
(
tokenOs
)
)
{
ident
.
write
(
tokenOut
)
;
tokenOut
.
writeLong
(
info
.
getRenewDate
(
)
)
;
tokenOut
.
writeInt
(
info
.
getPassword
(
)
.
length
)
;
tokenOut
.
write
(
info
.
getPassword
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
SuppressWarnings
(
)
public
HttpURLConnection
openConnection
(
URL
url
,
Token
token
,
String
doAs
)
throws
IOException
,
AuthenticationException
{
Preconditions
.
checkNotNull
(
url
,
)
;
Preconditions
.
checkNotNull
(
token
,
)
;
Map
<
String
,
String
>
extraParams
=
new
HashMap
<
String
,
String
>
(
)
;
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
extends
TokenIdentifier
>
dToken
=
null
;
@
SuppressWarnings
(
)
public
HttpURLConnection
openConnection
(
URL
url
,
Token
token
,
String
doAs
)
throws
IOException
,
AuthenticationException
{
Preconditions
.
checkNotNull
(
url
,
)
;
Preconditions
.
checkNotNull
(
token
,
)
;
Map
<
String
,
String
>
extraParams
=
new
HashMap
<
String
,
String
>
(
)
;
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
extends
TokenIdentifier
>
dToken
=
null
;
LOG
.
debug
(
,
url
,
token
,
doAs
)
;
if
(
!
token
.
isSet
(
)
)
{
Credentials
creds
=
UserGroupInformation
.
getCurrentUser
(
)
.
getCredentials
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
InterfaceAudience
.
Private
public
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
extends
TokenIdentifier
>
selectDelegationToken
(
URL
url
,
Credentials
creds
)
{
final
InetSocketAddress
serviceAddr
=
new
InetSocketAddress
(
url
.
getHost
(
)
,
url
.
getPort
(
)
)
;
final
Text
service
=
SecurityUtil
.
buildTokenService
(
serviceAddr
)
;
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
extends
TokenIdentifier
>
dToken
=
creds
.
getToken
(
service
)
;
AuthenticationToken
authToken
=
(
AuthenticationToken
)
request
.
getUserPrincipal
(
)
;
if
(
authToken
!=
null
&&
authToken
!=
AuthenticationToken
.
ANONYMOUS
)
{
ugi
=
(
UserGroupInformation
)
request
.
getAttribute
(
DelegationTokenAuthenticationHandler
.
DELEGATION_TOKEN_UGI_ATTRIBUTE
)
;
if
(
ugi
==
null
)
{
String
realUser
=
request
.
getUserPrincipal
(
)
.
getName
(
)
;
ugi
=
UserGroupInformation
.
createRemoteUser
(
realUser
,
handlerAuthMethod
)
;
String
doAsUser
=
getDoAs
(
request
)
;
if
(
doAsUser
!=
null
)
{
ugi
=
UserGroupInformation
.
createProxyUser
(
doAsUser
,
ugi
)
;
try
{
ProxyUsers
.
authorize
(
ugi
,
request
.
getRemoteAddr
(
)
)
;
}
catch
(
AuthorizationException
ex
)
{
HttpExceptionUtils
.
createServletExceptionResponse
(
response
,
HttpServletResponse
.
SC_FORBIDDEN
,
ex
)
;
requestCompleted
=
true
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
SuppressWarnings
(
)
@
Override
public
AuthenticationToken
authenticate
(
HttpServletRequest
request
,
HttpServletResponse
response
)
throws
IOException
,
AuthenticationException
{
AuthenticationToken
token
;
String
delegationParam
=
getDelegationToken
(
request
)
;
if
(
delegationParam
!=
null
)
{
String
delegationParam
=
getDelegationToken
(
request
)
;
if
(
delegationParam
!=
null
)
{
LOG
.
debug
(
,
delegationParam
)
;
try
{
Token
<
AbstractDelegationTokenIdentifier
>
dt
=
new
Token
(
)
;
dt
.
decodeFromUrlString
(
delegationParam
)
;
UserGroupInformation
ugi
=
tokenManager
.
verifyToken
(
dt
)
;
final
String
shortName
=
ugi
.
getShortUserName
(
)
;
token
=
new
AuthenticationToken
(
shortName
,
ugi
.
getUserName
(
)
,
getType
(
)
)
;
token
.
setExpires
(
0
)
;
request
.
setAttribute
(
DELEGATION_TOKEN_UGI_ATTRIBUTE
,
ugi
)
;
}
catch
(
Throwable
ex
)
{
token
=
null
;
HttpExceptionUtils
.
createServletExceptionResponse
(
response
,
HttpServletResponse
.
SC_FORBIDDEN
,
new
AuthenticationException
(
ex
)
)
;
}
}
else
{
@
Override
public
void
authenticate
(
URL
url
,
AuthenticatedURL
.
Token
token
)
throws
IOException
,
AuthenticationException
{
if
(
!
hasDelegationToken
(
url
,
token
)
)
{
try
{
UserGroupInformation
.
getCurrentUser
(
)
.
checkTGTAndReloginFromKeytab
(
)
;
@
SuppressWarnings
(
)
public
Token
<
?
extends
AbstractDelegationTokenIdentifier
>
createToken
(
UserGroupInformation
ugi
,
String
renewer
,
String
service
)
{
@
SuppressWarnings
(
)
public
long
renewToken
(
Token
<
?
extends
AbstractDelegationTokenIdentifier
>
token
,
String
renewer
)
throws
IOException
{
@
SuppressWarnings
(
)
public
void
cancelToken
(
Token
<
?
extends
AbstractDelegationTokenIdentifier
>
token
,
String
canceler
)
throws
IOException
{
protected
final
void
noteFailure
(
Exception
exception
)
{
protected
void
addService
(
Service
service
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
boolean
removeService
(
Service
service
)
{
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
List
<
Service
>
services
=
getServices
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
void
serviceStart
(
)
throws
Exception
{
List
<
Service
>
services
=
getServices
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
void
serviceStop
(
)
throws
Exception
{
int
numOfServicesToStop
=
serviceList
.
size
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
stop
(
int
numOfServicesStarted
,
boolean
stopOnlyStartedServices
)
{
Exception
firstException
=
null
;
List
<
Service
>
services
=
getServices
(
)
;
for
(
int
i
=
numOfServicesStarted
-
1
;
i
>=
0
;
i
--
)
{
Service
service
=
services
.
get
(
i
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
stateChanged
(
Service
service
)
{
@
Override
public
Configuration
bindArgs
(
Configuration
config
,
List
<
String
>
args
)
throws
Exception
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
Configuration
bindArgs
(
Configuration
config
,
List
<
String
>
args
)
throws
Exception
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
getName
(
)
,
args
.
size
(
)
)
;
for
(
String
arg
:
args
)
{
@
Override
public
void
uncaughtException
(
Thread
thread
,
Throwable
exception
)
{
if
(
ShutdownHookManager
.
get
(
)
.
isShutdownInProgress
(
)
)
{
if
(
ShutdownHookManager
.
get
(
)
.
isShutdownInProgress
(
)
)
{
LOG
.
error
(
,
thread
.
toString
(
)
,
exception
,
exception
)
;
}
else
if
(
exception
instanceof
Error
)
{
try
{
LOG
.
error
(
,
thread
.
toString
(
)
,
exception
,
exception
)
;
}
catch
(
Throwable
err
)
{
}
if
(
exception
instanceof
OutOfMemoryError
)
{
try
{
System
.
err
.
println
(
)
;
}
catch
(
Throwable
err
)
{
}
ExitUtil
.
haltOnOutOfMemory
(
(
OutOfMemoryError
)
exception
)
;
}
else
{
ExitUtil
.
ExitException
ee
=
ServiceLauncher
.
convertToExitException
(
exception
)
;
ExitUtil
.
terminate
(
ee
.
status
,
ee
)
;
}
}
else
{
@
Override
public
void
handle
(
Signal
s
)
{
signalCount
.
incrementAndGet
(
)
;
InterruptData
data
=
new
InterruptData
(
s
.
getName
(
)
,
s
.
getNumber
(
)
)
;
void
noteException
(
ExitUtil
.
ExitException
exitException
)
{
int
exitCode
=
exitException
.
getExitCode
(
)
;
if
(
exitCode
!=
0
)
{
@
VisibleForTesting
public
int
loadConfigurationClasses
(
)
{
List
<
String
>
toCreate
=
getConfigurationsToCreate
(
)
;
int
loaded
=
0
;
for
(
String
classname
:
toCreate
)
{
try
{
Class
<
?
>
loadClass
=
getClassLoader
(
)
.
loadClass
(
classname
)
;
Object
instance
=
loadClass
.
getConstructor
(
)
.
newInstance
(
)
;
if
(
!
(
instance
instanceof
Configuration
)
)
{
throw
new
ExitUtil
.
ExitException
(
EXIT_SERVICE_CREATION_FAILURE
,
+
classname
+
)
;
}
loaded
++
;
}
catch
(
ClassNotFoundException
e
)
{
LOG
.
debug
(
,
classname
)
;
}
catch
(
ExitUtil
.
ExitException
e
)
{
throw
e
;
}
catch
(
Exception
e
)
{
public
ExitUtil
.
ExitException
launchService
(
Configuration
conf
,
S
instance
,
List
<
String
>
processedArgs
,
boolean
addShutdownHook
,
boolean
execute
)
{
ExitUtil
.
ExitException
exitException
;
try
{
int
exitCode
=
coreServiceLaunch
(
conf
,
instance
,
processedArgs
,
addShutdownHook
,
execute
)
;
if
(
service
!=
null
)
{
Throwable
failure
=
service
.
getFailureCause
(
)
;
if
(
failure
!=
null
)
{
Service
.
STATE
failureState
=
service
.
getFailureState
(
)
;
if
(
failureState
==
Service
.
STATE
.
STOPPED
)
{
Throwable
failure
=
service
.
getFailureCause
(
)
;
if
(
failure
!=
null
)
{
Service
.
STATE
failureState
=
service
.
getFailureState
(
)
;
if
(
failureState
==
Service
.
STATE
.
STOPPED
)
{
LOG
.
debug
(
,
failure
,
failure
)
;
}
else
{
throw
failure
;
}
}
}
String
name
=
getServiceName
(
)
;
if
(
exitCode
==
0
)
{
exitException
=
new
ServiceLaunchException
(
exitCode
,
,
name
)
;
}
else
{
exitException
=
new
ServiceLaunchException
(
exitCode
,
,
name
)
;
}
}
catch
(
ExitUtil
.
ExitException
ee
)
{
exitException
=
ee
;
}
catch
(
Throwable
thrown
)
{
if
(
service
instanceof
LaunchableService
)
{
LOG
.
debug
(
,
name
)
;
launchableService
=
(
LaunchableService
)
service
;
if
(
launchableService
.
isInState
(
Service
.
STATE
.
INITED
)
)
{
LOG
.
warn
(
+
,
name
)
;
}
Configuration
newconf
=
launchableService
.
bindArgs
(
configuration
,
processedArgs
)
;
if
(
newconf
!=
null
)
{
configuration
=
newconf
;
}
}
if
(
!
service
.
isInState
(
Service
.
STATE
.
INITED
)
)
{
service
.
init
(
configuration
)
;
}
int
exitCode
;
try
{
service
.
start
(
)
;
exitCode
=
EXIT_SUCCESS
;
if
(
execute
&&
service
.
isInState
(
Service
.
STATE
.
STARTED
)
)
{
@
Override
public
void
uncaughtException
(
Thread
thread
,
Throwable
exception
)
{
protected
void
error
(
String
message
,
Throwable
thrown
)
{
String
text
=
+
message
;
if
(
LOG
.
isErrorEnabled
(
)
)
{
argString
.
append
(
)
.
append
(
arg
)
.
append
(
)
;
}
LOG
.
debug
(
,
argString
)
;
try
{
String
[
]
argArray
=
args
.
toArray
(
new
String
[
args
.
size
(
)
]
)
;
GenericOptionsParser
parser
=
createGenericOptionsParser
(
conf
,
argArray
)
;
if
(
!
parser
.
isParseSuccessful
(
)
)
{
throw
new
ServiceLaunchException
(
EXIT_COMMAND_ARGUMENT_ERROR
,
E_PARSE_FAILED
+
,
argString
)
;
}
CommandLine
line
=
parser
.
getCommandLine
(
)
;
List
<
String
>
remainingArgs
=
Arrays
.
asList
(
parser
.
getRemainingArgs
(
)
)
;
LOG
.
debug
(
,
remainingArgs
)
;
if
(
line
.
hasOption
(
ARG_CONF
)
)
{
String
[
]
filenames
=
line
.
getOptionValues
(
ARG_CONF
)
;
verifyConfigurationFilesExist
(
filenames
)
;
for
(
String
filename
:
filenames
)
{
File
file
=
new
File
(
filename
)
;
}
if
(
StringUtils
.
popOption
(
,
args
)
||
StringUtils
.
popOption
(
,
args
)
)
{
usage
(
)
;
return
0
;
}
else
if
(
args
.
size
(
)
==
0
)
{
usage
(
)
;
return
0
;
}
String
hostPort
=
StringUtils
.
popOptionWithArgument
(
,
args
)
;
if
(
hostPort
==
null
)
{
System
.
err
.
println
(
)
;
return
1
;
}
if
(
args
.
isEmpty
(
)
)
{
System
.
err
.
println
(
)
;
return
1
;
}
String
servicePrincipal
=
StringUtils
.
popOptionWithArgument
(
,
args
)
;
if
(
servicePrincipal
!=
null
)
{
public
synchronized
void
removeSpanReceiver
(
long
spanReceiverId
)
throws
IOException
{
SpanReceiver
[
]
receivers
=
TracerPool
.
getGlobalTracerPool
(
)
.
getReceivers
(
)
;
for
(
SpanReceiver
receiver
:
receivers
)
{
if
(
receiver
.
getId
(
)
==
spanReceiverId
)
{
TracerPool
.
getGlobalTracerPool
(
)
.
removeAndCloseReceiver
(
receiver
)
;
@
Override
public
URL
getResource
(
String
name
)
{
URL
url
=
null
;
if
(
!
isSystemClass
(
name
,
systemClasses
)
)
{
url
=
findResource
(
name
)
;
if
(
url
==
null
&&
name
.
startsWith
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
synchronized
Class
<
?
>
loadClass
(
String
name
,
boolean
resolve
)
throws
ClassNotFoundException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
name
)
;
}
Class
<
?
>
c
=
findLoadedClass
(
name
)
;
ClassNotFoundException
ex
=
null
;
if
(
c
==
null
&&
!
isSystemClass
(
name
,
systemClasses
)
)
{
try
{
c
=
findClass
(
name
)
;
if
(
LOG
.
isDebugEnabled
(
)
&&
c
!=
null
)
{
LOG
.
debug
(
+
name
+
)
;
}
}
catch
(
ClassNotFoundException
e
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
e
.
toString
(
)
)
;
}
ex
=
e
;
}
}
if
(
c
==
null
)
{
c
=
parent
.
loadClass
(
name
)
;
if
(
LOG
.
isDebugEnabled
(
)
&&
c
!=
null
)
{
public
static
BlockingThreadPoolExecutorService
newInstance
(
int
activeTasks
,
int
waitingTasks
,
long
keepAliveTime
,
TimeUnit
unit
,
String
prefixName
)
{
final
BlockingQueue
<
Runnable
>
workQueue
=
new
LinkedBlockingQueue
<
>
(
waitingTasks
+
activeTasks
)
;
ThreadPoolExecutor
eventProcessingExecutor
=
new
ThreadPoolExecutor
(
activeTasks
,
activeTasks
,
keepAliveTime
,
unit
,
workQueue
,
newDaemonThreadFactory
(
prefixName
)
,
new
RejectedExecutionHandler
(
)
{
@
Override
public
void
rejectedExecution
(
Runnable
r
,
ThreadPoolExecutor
executor
)
{
public
static
synchronized
void
terminate
(
ExitException
ee
)
throws
ExitException
{
int
status
=
ee
.
getExitCode
(
)
;
String
msg
=
ee
.
getMessage
(
)
;
if
(
status
!=
0
)
{
public
static
synchronized
void
terminate
(
ExitException
ee
)
throws
ExitException
{
int
status
=
ee
.
getExitCode
(
)
;
String
msg
=
ee
.
getMessage
(
)
;
if
(
status
!=
0
)
{
LOG
.
debug
(
,
status
,
msg
,
ee
)
;
public
static
synchronized
void
halt
(
HaltException
ee
)
throws
HaltException
{
int
status
=
ee
.
getExitCode
(
)
;
String
msg
=
ee
.
getMessage
(
)
;
try
{
if
(
status
!=
0
)
{
public
static
synchronized
void
halt
(
HaltException
ee
)
throws
HaltException
{
int
status
=
ee
.
getExitCode
(
)
;
String
msg
=
ee
.
getMessage
(
)
;
try
{
if
(
status
!=
0
)
{
LOG
.
debug
(
,
status
,
msg
,
ee
)
;
try
{
if
(
fileName
!=
null
)
{
File
file
=
new
File
(
fileName
)
;
if
(
file
.
exists
(
)
)
{
try
(
Reader
fileReader
=
new
InputStreamReader
(
Files
.
newInputStream
(
file
.
toPath
(
)
)
,
StandardCharsets
.
UTF_8
)
;
BufferedReader
bufferedReader
=
new
BufferedReader
(
fileReader
)
)
{
List
<
String
>
lines
=
new
ArrayList
<
String
>
(
)
;
String
line
=
null
;
while
(
(
line
=
bufferedReader
.
readLine
(
)
)
!=
null
)
{
lines
.
add
(
line
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
lines
.
size
(
)
+
+
fileName
)
;
}
return
(
lines
.
toArray
(
new
String
[
lines
.
size
(
)
]
)
)
;
}
}
else
{
LOG
.
debug
(
+
fileName
)
;
}
}
}
catch
(
IOException
ioe
)
{
try
{
DocumentBuilder
db
=
builder
.
newDocumentBuilder
(
)
;
dom
=
db
.
parse
(
fileInputStream
)
;
Element
doc
=
dom
.
getDocumentElement
(
)
;
NodeList
nodes
=
doc
.
getElementsByTagName
(
)
;
for
(
int
i
=
0
;
i
<
nodes
.
getLength
(
)
;
i
++
)
{
Node
node
=
nodes
.
item
(
i
)
;
if
(
node
.
getNodeType
(
)
==
Node
.
ELEMENT_NODE
)
{
Element
e
=
(
Element
)
node
;
String
v
=
readFirstTagValue
(
e
,
)
;
String
[
]
hosts
=
StringUtils
.
getTrimmedStrings
(
v
)
;
String
str
=
readFirstTagValue
(
e
,
)
;
Integer
timeout
=
(
str
==
null
)
?
null
:
Integer
.
parseInt
(
str
)
;
for
(
String
host
:
hosts
)
{
map
.
put
(
host
,
timeout
)
;
dom
=
db
.
parse
(
fileInputStream
)
;
Element
doc
=
dom
.
getDocumentElement
(
)
;
NodeList
nodes
=
doc
.
getElementsByTagName
(
)
;
for
(
int
i
=
0
;
i
<
nodes
.
getLength
(
)
;
i
++
)
{
Node
node
=
nodes
.
item
(
i
)
;
if
(
node
.
getNodeType
(
)
==
Node
.
ELEMENT_NODE
)
{
Element
e
=
(
Element
)
node
;
String
v
=
readFirstTagValue
(
e
,
)
;
String
[
]
hosts
=
StringUtils
.
getTrimmedStrings
(
v
)
;
String
str
=
readFirstTagValue
(
e
,
)
;
Integer
timeout
=
(
str
==
null
)
?
null
:
Integer
.
parseInt
(
str
)
;
for
(
String
host
:
hosts
)
{
map
.
put
(
host
,
timeout
)
;
LOG
.
info
(
+
host
+
+
type
+
+
filename
)
;
}
}
}
}
catch
(
IOException
|
SAXException
|
ParserConfigurationException
e
)
{
private
void
refreshInternal
(
String
includesFile
,
String
excludesFile
,
boolean
lazy
)
throws
IOException
{
public
void
setIncludesFile
(
String
includesFile
)
{
public
void
setExcludesFile
(
String
excludesFile
)
{
public
void
updateFileNames
(
String
includesFile
,
String
excludesFile
)
{
public
void
updateFileNames
(
String
includesFile
,
String
excludesFile
)
{
LOG
.
info
(
+
includesFile
)
;
public
static
KeyProvider
createKeyProvider
(
final
Configuration
conf
,
final
String
configKeyName
)
throws
IOException
{
@
VisibleForTesting
static
int
computeCapacity
(
long
maxMemory
,
double
percentage
,
String
mapName
)
{
if
(
percentage
>
100.0
||
percentage
<
0.0
)
{
throw
new
HadoopIllegalArgumentException
(
+
percentage
+
+
)
;
}
if
(
maxMemory
<
0
)
{
throw
new
HadoopIllegalArgumentException
(
+
maxMemory
+
)
;
}
if
(
percentage
==
0.0
||
maxMemory
==
0
)
{
return
0
;
}
final
String
vmBit
=
System
.
getProperty
(
)
;
final
double
percentDivisor
=
100.0
/
percentage
;
final
double
percentMemory
=
maxMemory
/
percentDivisor
;
final
int
e1
=
(
int
)
(
Math
.
log
(
percentMemory
)
/
Math
.
log
(
2.0
)
+
0.5
)
;
final
int
e2
=
e1
-
(
.
equals
(
vmBit
)
?
2
:
3
)
;
final
int
exponent
=
e2
<
0
?
0
:
e2
>
30
?
30
:
e2
;
final
int
c
=
1
<<
exponent
;
LOG
.
info
(
+
mapName
)
;
if
(
percentage
>
100.0
||
percentage
<
0.0
)
{
throw
new
HadoopIllegalArgumentException
(
+
percentage
+
+
)
;
}
if
(
maxMemory
<
0
)
{
throw
new
HadoopIllegalArgumentException
(
+
maxMemory
+
)
;
}
if
(
percentage
==
0.0
||
maxMemory
==
0
)
{
return
0
;
}
final
String
vmBit
=
System
.
getProperty
(
)
;
final
double
percentDivisor
=
100.0
/
percentage
;
final
double
percentMemory
=
maxMemory
/
percentDivisor
;
final
int
e1
=
(
int
)
(
Math
.
log
(
percentMemory
)
/
Math
.
log
(
2.0
)
+
0.5
)
;
final
int
e2
=
e1
-
(
.
equals
(
vmBit
)
?
2
:
3
)
;
final
int
exponent
=
e2
<
0
?
0
:
e2
>
30
?
30
:
e2
;
final
int
c
=
1
<<
exponent
;
LOG
.
info
(
+
mapName
)
;
LOG
.
info
(
+
vmBit
+
)
;
throw
new
HadoopIllegalArgumentException
(
+
percentage
+
+
)
;
}
if
(
maxMemory
<
0
)
{
throw
new
HadoopIllegalArgumentException
(
+
maxMemory
+
)
;
}
if
(
percentage
==
0.0
||
maxMemory
==
0
)
{
return
0
;
}
final
String
vmBit
=
System
.
getProperty
(
)
;
final
double
percentDivisor
=
100.0
/
percentage
;
final
double
percentMemory
=
maxMemory
/
percentDivisor
;
final
int
e1
=
(
int
)
(
Math
.
log
(
percentMemory
)
/
Math
.
log
(
2.0
)
+
0.5
)
;
final
int
e2
=
e1
-
(
.
equals
(
vmBit
)
?
2
:
3
)
;
final
int
exponent
=
e2
<
0
?
0
:
e2
>
30
?
30
:
e2
;
final
int
c
=
1
<<
exponent
;
LOG
.
info
(
+
mapName
)
;
LOG
.
info
(
+
vmBit
+
)
;
LOG
.
info
(
percentage
+
+
StringUtils
.
TraditionalBinaryPrefix
.
long2String
(
maxMemory
,
,
1
)
+
+
StringUtils
.
TraditionalBinaryPrefix
.
long2String
(
(
long
)
percentMemory
,
,
1
)
)
;
public
void
info
(
String
msg
)
{
if
(
LOG
!=
null
)
{
public
void
debug
(
Throwable
t
)
{
if
(
LOG
!=
null
)
{
public
void
error
(
String
msg
)
{
if
(
LOG
!=
null
)
{
return
false
;
}
ShellCommandExecutor
shexec
=
null
;
boolean
setsidSupported
=
true
;
try
{
String
[
]
args
=
{
,
,
,
}
;
shexec
=
new
ShellCommandExecutor
(
args
)
;
shexec
.
execute
(
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
debug
(
)
;
setsidSupported
=
false
;
}
catch
(
SecurityException
se
)
{
LOG
.
debug
(
+
)
;
setsidSupported
=
false
;
}
catch
(
Error
err
)
{
if
(
err
.
getMessage
(
)
!=
null
&&
err
.
getMessage
(
)
.
contains
(
+
)
&&
(
Shell
.
FREEBSD
||
Shell
.
MAC
)
)
{
private
static
void
shutdownExecutor
(
final
Configuration
conf
)
{
try
{
EXECUTOR
.
shutdown
(
)
;
long
shutdownTimeout
=
getShutdownTimeout
(
conf
)
;
if
(
!
EXECUTOR
.
awaitTermination
(
shutdownTimeout
,
TIME_UNIT_DEFAULT
)
)
{
@
Override
public
void
handle
(
Signal
signal
)
{
if
(
registered
)
{
throw
new
IllegalStateException
(
)
;
}
registered
=
true
;
StringBuilder
bld
=
new
StringBuilder
(
)
;
bld
.
append
(
)
;
final
String
SIGNALS
[
]
=
{
,
,
}
;
String
separator
=
;
for
(
String
signalName
:
SIGNALS
)
{
try
{
new
Handler
(
signalName
,
LOG
)
;
bld
.
append
(
separator
)
.
append
(
signalName
)
;
separator
=
;
}
catch
(
Exception
e
)
{
LOG
.
debug
(
e
)
;
}
}
bld
.
append
(
)
;
static
void
startupShutdownMessage
(
Class
<
?
>
clazz
,
String
[
]
args
,
final
LogAdapter
LOG
)
{
final
String
hostname
=
NetUtils
.
getHostname
(
)
;
final
String
classname
=
clazz
.
getSimpleName
(
)
;
}
try
{
executorService
.
shutdown
(
)
;
logger
.
debug
(
,
timeout
,
unit
)
;
if
(
!
executorService
.
awaitTermination
(
timeout
,
unit
)
)
{
logger
.
debug
(
+
,
timeout
,
unit
)
;
executorService
.
shutdownNow
(
)
;
}
if
(
executorService
.
awaitTermination
(
timeout
,
unit
)
)
{
logger
.
debug
(
)
;
}
else
{
logger
.
error
(
,
(
2
*
timeout
)
,
unit
)
;
}
}
catch
(
InterruptedException
e
)
{
logger
.
error
(
,
e
)
;
executorService
.
shutdownNow
(
)
;
}
catch
(
Exception
e
)
{
logger
.
warn
(
,
e
.
getMessage
(
)
)
;
private
void
displayResults
(
)
{
LOG
.
info
(
)
;
LOG
.
info
(
)
;
for
(
int
i
=
0
;
i
<
testsFromConfigFile
.
size
(
)
;
i
++
)
{
CLITestData
td
=
testsFromConfigFile
.
get
(
i
)
;
boolean
testResult
=
td
.
getTestResult
(
)
;
if
(
!
testResult
)
{
LOG
.
info
(
)
;
LOG
.
info
(
+
(
i
+
1
)
+
)
;
private
void
displayResults
(
)
{
LOG
.
info
(
)
;
LOG
.
info
(
)
;
for
(
int
i
=
0
;
i
<
testsFromConfigFile
.
size
(
)
;
i
++
)
{
CLITestData
td
=
testsFromConfigFile
.
get
(
i
)
;
boolean
testResult
=
td
.
getTestResult
(
)
;
if
(
!
testResult
)
{
LOG
.
info
(
)
;
LOG
.
info
(
+
(
i
+
1
)
+
)
;
LOG
.
info
(
+
td
.
getTestDesc
(
)
+
)
;
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
testCommands
=
td
.
getTestCommands
(
)
;
for
(
CLICommand
cmd
:
testCommands
)
{
LOG
.
info
(
)
;
for
(
int
i
=
0
;
i
<
testsFromConfigFile
.
size
(
)
;
i
++
)
{
CLITestData
td
=
testsFromConfigFile
.
get
(
i
)
;
boolean
testResult
=
td
.
getTestResult
(
)
;
if
(
!
testResult
)
{
LOG
.
info
(
)
;
LOG
.
info
(
+
(
i
+
1
)
+
)
;
LOG
.
info
(
+
td
.
getTestDesc
(
)
+
)
;
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
testCommands
=
td
.
getTestCommands
(
)
;
for
(
CLICommand
cmd
:
testCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
cleanupCommands
=
td
.
getCleanupCommands
(
)
;
for
(
CLICommand
cmd
:
cleanupCommands
)
{
LOG
.
info
(
)
;
LOG
.
info
(
+
(
i
+
1
)
+
)
;
LOG
.
info
(
+
td
.
getTestDesc
(
)
+
)
;
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
testCommands
=
td
.
getTestCommands
(
)
;
for
(
CLICommand
cmd
:
testCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
cleanupCommands
=
td
.
getCleanupCommands
(
)
;
for
(
CLICommand
cmd
:
cleanupCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
ComparatorData
>
compdata
=
td
.
getComparatorData
(
)
;
for
(
ComparatorData
cd
:
compdata
)
{
boolean
resultBoolean
=
cd
.
getTestResult
(
)
;
LOG
.
info
(
+
(
i
+
1
)
+
)
;
LOG
.
info
(
+
td
.
getTestDesc
(
)
+
)
;
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
testCommands
=
td
.
getTestCommands
(
)
;
for
(
CLICommand
cmd
:
testCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
cleanupCommands
=
td
.
getCleanupCommands
(
)
;
for
(
CLICommand
cmd
:
cleanupCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
ComparatorData
>
compdata
=
td
.
getComparatorData
(
)
;
for
(
ComparatorData
cd
:
compdata
)
{
boolean
resultBoolean
=
cd
.
getTestResult
(
)
;
LOG
.
info
(
+
cd
.
getComparatorType
(
)
+
)
;
LOG
.
info
(
+
td
.
getTestDesc
(
)
+
)
;
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
testCommands
=
td
.
getTestCommands
(
)
;
for
(
CLICommand
cmd
:
testCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
cleanupCommands
=
td
.
getCleanupCommands
(
)
;
for
(
CLICommand
cmd
:
cleanupCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
ComparatorData
>
compdata
=
td
.
getComparatorData
(
)
;
for
(
ComparatorData
cd
:
compdata
)
{
boolean
resultBoolean
=
cd
.
getTestResult
(
)
;
LOG
.
info
(
+
cd
.
getComparatorType
(
)
+
)
;
LOG
.
info
(
+
(
resultBoolean
?
:
)
+
)
;
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
testCommands
=
td
.
getTestCommands
(
)
;
for
(
CLICommand
cmd
:
testCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
CLICommand
>
cleanupCommands
=
td
.
getCleanupCommands
(
)
;
for
(
CLICommand
cmd
:
cleanupCommands
)
{
LOG
.
info
(
+
expandCommand
(
cmd
.
getCmd
(
)
)
+
)
;
}
LOG
.
info
(
)
;
ArrayList
<
ComparatorData
>
compdata
=
td
.
getComparatorData
(
)
;
for
(
ComparatorData
cd
:
compdata
)
{
boolean
resultBoolean
=
cd
.
getTestResult
(
)
;
LOG
.
info
(
+
cd
.
getComparatorType
(
)
+
)
;
LOG
.
info
(
+
(
resultBoolean
?
:
)
+
)
;
LOG
.
info
(
+
expandCommand
(
cd
.
getExpectedOutput
(
)
)
+
)
;
boolean
overallResults
=
true
;
int
totalPass
=
0
;
int
totalFail
=
0
;
int
totalComparators
=
0
;
for
(
int
i
=
0
;
i
<
testsFromConfigFile
.
size
(
)
;
i
++
)
{
CLITestData
td
=
testsFromConfigFile
.
get
(
i
)
;
totalComparators
+=
testsFromConfigFile
.
get
(
i
)
.
getComparatorData
(
)
.
size
(
)
;
boolean
resultBoolean
=
td
.
getTestResult
(
)
;
if
(
resultBoolean
)
{
totalPass
++
;
}
else
{
totalFail
++
;
}
overallResults
&=
resultBoolean
;
}
LOG
.
info
(
+
testMode
)
;
LOG
.
info
(
)
;
int
totalPass
=
0
;
int
totalFail
=
0
;
int
totalComparators
=
0
;
for
(
int
i
=
0
;
i
<
testsFromConfigFile
.
size
(
)
;
i
++
)
{
CLITestData
td
=
testsFromConfigFile
.
get
(
i
)
;
totalComparators
+=
testsFromConfigFile
.
get
(
i
)
.
getComparatorData
(
)
.
size
(
)
;
boolean
resultBoolean
=
td
.
getTestResult
(
)
;
if
(
resultBoolean
)
{
totalPass
++
;
}
else
{
totalFail
++
;
}
overallResults
&=
resultBoolean
;
}
LOG
.
info
(
+
testMode
)
;
LOG
.
info
(
)
;
LOG
.
info
(
+
(
overallResults
?
:
)
)
;
int
totalComparators
=
0
;
for
(
int
i
=
0
;
i
<
testsFromConfigFile
.
size
(
)
;
i
++
)
{
CLITestData
td
=
testsFromConfigFile
.
get
(
i
)
;
totalComparators
+=
testsFromConfigFile
.
get
(
i
)
.
getComparatorData
(
)
.
size
(
)
;
boolean
resultBoolean
=
td
.
getTestResult
(
)
;
if
(
resultBoolean
)
{
totalPass
++
;
}
else
{
totalFail
++
;
}
overallResults
&=
resultBoolean
;
}
LOG
.
info
(
+
testMode
)
;
LOG
.
info
(
)
;
LOG
.
info
(
+
(
overallResults
?
:
)
)
;
if
(
(
totalPass
+
totalFail
)
==
0
)
{
LOG
.
info
(
+
0
)
;
}
else
{
LOG
.
info
(
+
totalPass
+
+
(
100
*
totalPass
/
(
totalPass
+
totalFail
)
)
+
)
;
LOG
.
info
(
+
totalFail
+
+
(
100
*
totalFail
/
(
totalPass
+
totalFail
)
)
+
)
;
}
LOG
.
info
(
+
totalComparators
+
)
;
LOG
.
info
(
)
;
LOG
.
info
(
)
;
LOG
.
info
(
)
;
int
i
=
0
;
boolean
foundTests
=
false
;
for
(
i
=
0
;
i
<
testsFromConfigFile
.
size
(
)
;
i
++
)
{
boolean
resultBoolean
=
testsFromConfigFile
.
get
(
i
)
.
getTestResult
(
)
;
if
(
!
resultBoolean
)
{
LOG
.
info
(
(
i
+
1
)
+
+
testsFromConfigFile
.
get
(
i
)
.
getTestDesc
(
)
)
;
foundTests
=
true
;
}
}
if
(
!
foundTests
)
{
private
HashMap
<
String
,
String
>
extractMemberVariablesFromConfigurationFields
(
Field
[
]
fields
)
{
if
(
fields
==
null
)
return
null
;
HashMap
<
String
,
String
>
retVal
=
new
HashMap
<
>
(
)
;
String
propRegex
=
;
Pattern
p
=
Pattern
.
compile
(
propRegex
)
;
String
value
;
for
(
Field
f
:
fields
)
{
}
LOG_CONFIG
.
debug
(
,
value
)
;
if
(
value
.
endsWith
(
)
||
value
.
endsWith
(
)
||
value
.
endsWith
(
)
)
continue
;
if
(
configurationPropsToSkipCompare
.
contains
(
value
)
)
{
continue
;
}
boolean
skipPrefix
=
false
;
for
(
String
cfgPrefix
:
configurationPrefixToSkipCompare
)
{
if
(
value
.
startsWith
(
cfgPrefix
)
)
{
skipPrefix
=
true
;
break
;
}
}
if
(
skipPrefix
)
{
continue
;
}
Matcher
m
=
p
.
matcher
(
value
)
;
if
(
!
m
.
find
(
)
)
{
LOG_CONFIG
.
debug
(
)
;
continue
;
private
HashMap
<
String
,
String
>
extractPropertiesFromXml
(
String
filename
)
{
if
(
filename
==
null
)
{
return
null
;
}
Configuration
conf
=
new
Configuration
(
false
)
;
conf
.
setAllowNullValueProperties
(
true
)
;
conf
.
addResource
(
filename
)
;
HashMap
<
String
,
String
>
retVal
=
new
HashMap
<
>
(
)
;
Iterator
<
Map
.
Entry
<
String
,
String
>>
kvItr
=
conf
.
iterator
(
)
;
while
(
kvItr
.
hasNext
(
)
)
{
Map
.
Entry
<
String
,
String
>
entry
=
kvItr
.
next
(
)
;
String
key
=
entry
.
getKey
(
)
;
if
(
xmlPropsToSkipCompare
.
contains
(
key
)
)
{
LOG_XML
.
debug
(
,
key
)
;
continue
;
}
if
(
xmlPrefixToSkipCompare
.
stream
(
)
.
anyMatch
(
key
::
startsWith
)
)
{
Configuration
conf
=
new
Configuration
(
false
)
;
conf
.
setAllowNullValueProperties
(
true
)
;
conf
.
addResource
(
filename
)
;
HashMap
<
String
,
String
>
retVal
=
new
HashMap
<
>
(
)
;
Iterator
<
Map
.
Entry
<
String
,
String
>>
kvItr
=
conf
.
iterator
(
)
;
while
(
kvItr
.
hasNext
(
)
)
{
Map
.
Entry
<
String
,
String
>
entry
=
kvItr
.
next
(
)
;
String
key
=
entry
.
getKey
(
)
;
if
(
xmlPropsToSkipCompare
.
contains
(
key
)
)
{
LOG_XML
.
debug
(
,
key
)
;
continue
;
}
if
(
xmlPrefixToSkipCompare
.
stream
(
)
.
anyMatch
(
key
::
startsWith
)
)
{
LOG_XML
.
debug
(
+
key
)
;
continue
;
}
if
(
conf
.
onlyKeyExists
(
key
)
)
{
Iterator
<
Map
.
Entry
<
String
,
String
>>
kvItr
=
conf
.
iterator
(
)
;
while
(
kvItr
.
hasNext
(
)
)
{
Map
.
Entry
<
String
,
String
>
entry
=
kvItr
.
next
(
)
;
String
key
=
entry
.
getKey
(
)
;
if
(
xmlPropsToSkipCompare
.
contains
(
key
)
)
{
LOG_XML
.
debug
(
,
key
)
;
continue
;
}
if
(
xmlPrefixToSkipCompare
.
stream
(
)
.
anyMatch
(
key
::
startsWith
)
)
{
LOG_XML
.
debug
(
+
key
)
;
continue
;
}
if
(
conf
.
onlyKeyExists
(
key
)
)
{
retVal
.
put
(
key
,
null
)
;
LOG_XML
.
debug
(
+
key
)
;
}
else
{
if
(
conf
.
get
(
key
)
!=
null
)
{
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
short
shValue
=
(
short
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Integer
.
toString
(
shValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
int
iValue
=
(
int
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Integer
.
toString
(
iValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
long
lValue
=
(
long
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Long
.
toString
(
lValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
float
fValue
=
(
float
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Float
.
toString
(
fValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
double
dValue
=
(
double
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Double
.
toString
(
dValue
)
)
;
retVal
.
put
(
f
.
getName
(
)
,
Integer
.
toString
(
shValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
int
iValue
=
(
int
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Integer
.
toString
(
iValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
long
lValue
=
(
long
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Long
.
toString
(
lValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
float
fValue
=
(
float
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Float
.
toString
(
fValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
double
dValue
=
(
double
)
f
.
get
(
null
)
;
retVal
.
put
(
f
.
getName
(
)
,
Double
.
toString
(
dValue
)
)
;
}
else
if
(
f
.
getType
(
)
.
getName
(
)
.
equals
(
)
)
{
boolean
bValue
=
(
boolean
)
f
.
get
(
null
)
;
@
Test
public
void
testCompareConfigurationClassAgainstXml
(
)
{
assertNotNull
(
xmlFilename
)
;
assertNotNull
(
configurationClasses
)
;
final
int
missingXmlSize
=
configurationFieldsMissingInXmlFile
.
size
(
)
;
for
(
Class
c
:
configurationClasses
)
{
@
Test
public
void
testCompareConfigurationClassAgainstXml
(
)
{
assertNotNull
(
xmlFilename
)
;
assertNotNull
(
configurationClasses
)
;
final
int
missingXmlSize
=
configurationFieldsMissingInXmlFile
.
size
(
)
;
for
(
Class
c
:
configurationClasses
)
{
LOG
.
info
(
c
.
toString
(
)
)
;
}
LOG
.
info
(
,
configurationMemberVariables
.
size
(
)
)
;
StringBuilder
xmlErrorMsg
=
new
StringBuilder
(
)
;
for
(
Class
c
:
configurationClasses
)
{
xmlErrorMsg
.
append
(
c
)
;
xmlErrorMsg
.
append
(
)
;
}
xmlErrorMsg
.
append
(
)
;
xmlErrorMsg
.
append
(
missingXmlSize
)
;
xmlErrorMsg
.
append
(
)
;
xmlErrorMsg
.
append
(
xmlFilename
)
;
private
void
appendMissingEntries
(
StringBuilder
sb
,
Set
<
String
>
missing
)
{
sb
.
append
(
)
;
new
TreeSet
<
>
(
missing
)
.
forEach
(
(
s
)
->
{
@
Test
public
void
testCompareXmlAgainstConfigurationClass
(
)
{
assertNotNull
(
xmlFilename
)
;
assertNotNull
(
configurationClasses
)
;
final
int
missingConfigSize
=
xmlFieldsMissingInConfiguration
.
size
(
)
;
@
Test
public
void
testCompareXmlAgainstConfigurationClass
(
)
{
assertNotNull
(
xmlFilename
)
;
assertNotNull
(
configurationClasses
)
;
final
int
missingConfigSize
=
xmlFieldsMissingInConfiguration
.
size
(
)
;
LOG
.
info
(
,
xmlFilename
,
xmlKeyValueMap
.
size
(
)
)
;
StringBuilder
configErrorMsg
=
new
StringBuilder
(
)
;
configErrorMsg
.
append
(
xmlFilename
)
;
configErrorMsg
.
append
(
)
;
configErrorMsg
.
append
(
missingConfigSize
)
;
configErrorMsg
.
append
(
)
;
Arrays
.
stream
(
configurationClasses
)
.
forEach
(
c
->
configErrorMsg
.
append
(
)
.
append
(
c
)
)
;
}
else
if
(
defaultValueCheck3
!=
null
)
{
defaultConfigName
=
defaultNameCheck3
;
defaultConfigValue
=
defaultValueCheck3
;
}
if
(
defaultConfigValue
!=
null
)
{
if
(
xmlDefaultValue
==
null
)
{
xmlPropertiesWithEmptyValue
.
add
(
xmlProperty
)
;
}
else
if
(
!
xmlDefaultValue
.
equals
(
defaultConfigValue
)
)
{
HashMap
<
String
,
String
>
xmlEntry
=
new
HashMap
<
>
(
)
;
xmlEntry
.
put
(
xmlProperty
,
xmlDefaultValue
)
;
HashMap
<
String
,
String
>
configEntry
=
new
HashMap
<
>
(
)
;
configEntry
.
put
(
defaultConfigName
,
defaultConfigValue
)
;
mismatchingXmlConfig
.
put
(
xmlEntry
,
configEntry
)
;
}
else
{
xmlPropertiesMatchingConfigDefault
.
put
(
xmlProperty
,
defaultConfigName
)
;
}
}
else
{
HashMap
<
String
,
String
>
xmlEntry
=
new
HashMap
<
>
(
)
;
xmlEntry
.
put
(
xmlProperty
,
xmlDefaultValue
)
;
HashMap
<
String
,
String
>
configEntry
=
new
HashMap
<
>
(
)
;
configEntry
.
put
(
defaultConfigName
,
defaultConfigValue
)
;
mismatchingXmlConfig
.
put
(
xmlEntry
,
configEntry
)
;
}
else
{
xmlPropertiesMatchingConfigDefault
.
put
(
xmlProperty
,
defaultConfigName
)
;
}
}
else
{
configPropertiesWithNoDefaultConfig
.
add
(
configProperty
)
;
}
}
}
LOG
.
info
(
,
xmlFilename
,
mismatchingXmlConfig
.
size
(
)
)
;
if
(
mismatchingXmlConfig
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
}
else
{
for
(
Map
.
Entry
<
HashMap
<
String
,
String
>
,
HashMap
<
String
,
String
>>
xcEntry
:
mismatchingXmlConfig
.
entrySet
(
)
)
{
xcEntry
.
getKey
(
)
.
forEach
(
(
key
,
value
)
->
{
mismatchingXmlConfig
.
put
(
xmlEntry
,
configEntry
)
;
}
else
{
xmlPropertiesMatchingConfigDefault
.
put
(
xmlProperty
,
defaultConfigName
)
;
}
}
else
{
configPropertiesWithNoDefaultConfig
.
add
(
configProperty
)
;
}
}
}
LOG
.
info
(
,
xmlFilename
,
mismatchingXmlConfig
.
size
(
)
)
;
if
(
mismatchingXmlConfig
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
}
else
{
for
(
Map
.
Entry
<
HashMap
<
String
,
String
>
,
HashMap
<
String
,
String
>>
xcEntry
:
mismatchingXmlConfig
.
entrySet
(
)
)
{
xcEntry
.
getKey
(
)
.
forEach
(
(
key
,
value
)
->
{
LOG
.
info
(
,
key
)
;
LOG
.
info
(
,
value
)
;
}
)
;
xcEntry
.
getValue
(
)
.
forEach
(
(
key
,
value
)
->
{
else
{
configPropertiesWithNoDefaultConfig
.
add
(
configProperty
)
;
}
}
}
LOG
.
info
(
,
xmlFilename
,
mismatchingXmlConfig
.
size
(
)
)
;
if
(
mismatchingXmlConfig
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
}
else
{
for
(
Map
.
Entry
<
HashMap
<
String
,
String
>
,
HashMap
<
String
,
String
>>
xcEntry
:
mismatchingXmlConfig
.
entrySet
(
)
)
{
xcEntry
.
getKey
(
)
.
forEach
(
(
key
,
value
)
->
{
LOG
.
info
(
,
key
)
;
LOG
.
info
(
,
value
)
;
}
)
;
xcEntry
.
getValue
(
)
.
forEach
(
(
key
,
value
)
->
{
LOG
.
info
(
,
key
)
;
LOG
.
info
(
,
value
)
;
}
)
;
@
Test
public
void
testDefaultValueCollision
(
)
{
for
(
String
filter
:
filtersForDefaultValueCollisionCheck
)
{
RandomDatum
key
=
generator
.
getKey
(
)
;
RandomDatum
value
=
generator
.
getValue
(
)
;
key
.
write
(
data
)
;
value
.
write
(
data
)
;
}
LOG
.
info
(
+
count
+
)
;
DataOutputBuffer
encryptedDataBuffer
=
new
DataOutputBuffer
(
)
;
CryptoOutputStream
out
=
new
CryptoOutputStream
(
encryptedDataBuffer
,
encCodec
,
bufferSize
,
key
,
iv
)
;
out
.
write
(
data
.
getData
(
)
,
0
,
data
.
getLength
(
)
)
;
out
.
flush
(
)
;
out
.
close
(
)
;
LOG
.
info
(
)
;
CryptoCodec
decCodec
=
null
;
try
{
decCodec
=
(
CryptoCodec
)
ReflectionUtils
.
newInstance
(
conf
.
getClassByName
(
decCodecClass
)
,
conf
)
;
}
catch
(
ClassNotFoundException
cnfe
)
{
private
void
waitForRefill
(
ValueQueue
<
?
>
valueQueue
,
String
queueName
,
int
queueSize
)
throws
TimeoutException
,
InterruptedException
{
GenericTestUtils
.
waitFor
(
(
)
->
{
int
size
=
valueQueue
.
getSize
(
queueName
)
;
if
(
size
!=
queueSize
)
{
@
After
public
void
tearDown
(
)
throws
Exception
{
if
(
fc
!=
null
)
{
final
Path
testRoot
=
fileContextTestHelper
.
getAbsoluteTestRootPath
(
fc
)
;
protected
void
describe
(
String
text
)
{
private
void
cleanupDir
(
Path
p
)
{
try
{
protected
final
Path
path
(
String
pathString
)
{
Path
p
=
new
Path
(
pathString
)
.
makeQualified
(
fs
.
getUri
(
)
,
getTestBaseDir
(
)
)
;
@
Test
public
void
testGetLongStatistics
(
)
{
Iterator
<
LongStatistic
>
iter
=
storageStatistics
.
getLongStatistics
(
)
;
while
(
iter
.
hasNext
(
)
)
{
final
LongStatistic
longStat
=
iter
.
next
(
)
;
assertNotNull
(
longStat
)
;
final
long
expectedStat
=
getStatisticsValue
(
longStat
.
getName
(
)
)
;
@
Test
public
void
testGetLong
(
)
{
for
(
String
key
:
STATISTICS_KEYS
)
{
final
long
expectedStat
=
getStatisticsValue
(
key
)
;
final
long
storageStat
=
storageStatistics
.
getLong
(
key
)
;
int
errors
=
0
;
for
(
Method
m
:
FileSystem
.
class
.
getDeclaredMethods
(
)
)
{
if
(
Modifier
.
isStatic
(
m
.
getModifiers
(
)
)
||
Modifier
.
isPrivate
(
m
.
getModifiers
(
)
)
||
Modifier
.
isFinal
(
m
.
getModifiers
(
)
)
)
{
continue
;
}
try
{
MustNotImplement
.
class
.
getMethod
(
m
.
getName
(
)
,
m
.
getParameterTypes
(
)
)
;
try
{
FilterFileSystem
.
class
.
getDeclaredMethod
(
m
.
getName
(
)
,
m
.
getParameterTypes
(
)
)
;
LOG
.
error
(
+
m
)
;
errors
++
;
}
catch
(
NoSuchMethodException
ex
)
{
}
}
catch
(
NoSuchMethodException
exc
)
{
try
{
FilterFileSystem
.
class
.
getDeclaredMethod
(
m
.
getName
(
)
,
m
.
getParameterTypes
(
)
)
;
}
catch
(
NoSuchMethodException
exc2
)
{
@
Test
public
void
testFilterFileSystem
(
)
throws
Exception
{
for
(
Method
m
:
AbstractFileSystem
.
class
.
getDeclaredMethods
(
)
)
{
if
(
Modifier
.
isStatic
(
m
.
getModifiers
(
)
)
)
continue
;
if
(
Modifier
.
isPrivate
(
m
.
getModifiers
(
)
)
)
continue
;
if
(
Modifier
.
isFinal
(
m
.
getModifiers
(
)
)
)
continue
;
try
{
DontCheck
.
class
.
getMethod
(
m
.
getName
(
)
,
m
.
getParameterTypes
(
)
)
;
private
int
shellRun
(
String
...
args
)
throws
Exception
{
int
exitCode
=
shell
.
run
(
args
)
;
shell
.
setConf
(
conf
)
;
String
[
]
args
=
new
String
[
2
]
;
args
[
0
]
=
;
args
[
1
]
=
;
int
res
=
shell
.
run
(
args
)
;
System
.
out
.
println
(
+
res
)
;
shell
.
setConf
(
conf
)
;
final
ByteArrayOutputStream
bytes
=
new
ByteArrayOutputStream
(
)
;
final
PrintStream
out
=
new
PrintStream
(
bytes
)
;
final
PrintStream
oldErr
=
System
.
err
;
System
.
setErr
(
out
)
;
final
String
results
;
try
{
int
run
=
shell
.
run
(
args
)
;
results
=
bytes
.
toString
(
)
;
private
int
shellRun
(
String
...
args
)
throws
Exception
{
int
exitCode
=
shell
.
run
(
args
)
;
int
errors
=
0
;
for
(
Method
m
:
FileSystem
.
class
.
getDeclaredMethods
(
)
)
{
if
(
Modifier
.
isStatic
(
m
.
getModifiers
(
)
)
||
Modifier
.
isPrivate
(
m
.
getModifiers
(
)
)
||
Modifier
.
isFinal
(
m
.
getModifiers
(
)
)
)
{
continue
;
}
try
{
MustNotImplement
.
class
.
getMethod
(
m
.
getName
(
)
,
m
.
getParameterTypes
(
)
)
;
try
{
HarFileSystem
.
class
.
getDeclaredMethod
(
m
.
getName
(
)
,
m
.
getParameterTypes
(
)
)
;
LOG
.
error
(
+
m
)
;
errors
++
;
}
catch
(
NoSuchMethodException
ex
)
{
}
}
catch
(
NoSuchMethodException
exc
)
{
try
{
HarFileSystem
.
class
.
getDeclaredMethod
(
m
.
getName
(
)
,
m
.
getParameterTypes
(
)
)
;
}
catch
(
NoSuchMethodException
exc2
)
{
@
Test
public
void
testLocalFSsetOwner
(
)
throws
IOException
{
assumeNotWindows
(
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
CommonConfigurationKeys
.
FS_PERMISSIONS_UMASK_KEY
,
)
;
LocalFileSystem
localfs
=
FileSystem
.
getLocal
(
conf
)
;
String
filename
=
;
Path
f
=
writeFile
(
localfs
,
filename
)
;
List
<
String
>
groups
;
try
{
groups
=
getGroups
(
)
;
@
Test
public
void
testSetUmaskInRealTime
(
)
throws
Exception
{
assumeNotWindows
(
)
;
LocalFileSystem
localfs
=
FileSystem
.
getLocal
(
new
Configuration
(
)
)
;
Configuration
conf
=
localfs
.
getConf
(
)
;
conf
.
set
(
CommonConfigurationKeys
.
FS_PERMISSIONS_UMASK_KEY
,
)
;
String
testFilename
=
;
Path
path
=
new
Path
(
testDir
,
testFilename
)
;
RawLocalFileSystem
rfs
=
new
RawLocalFileSystem
(
)
;
Configuration
conf
=
new
Configuration
(
)
;
rfs
.
initialize
(
rfs
.
getUri
(
)
,
conf
)
;
rfs
.
createNewFile
(
path
)
;
File
file
=
rfs
.
pathToFile
(
path
)
;
long
defaultBlockSize
=
rfs
.
getDefaultBlockSize
(
path
)
;
RawLocalFileSystem
.
DeprecatedRawLocalFileStatus
fsNIO
=
new
RawLocalFileSystem
.
DeprecatedRawLocalFileStatus
(
file
,
defaultBlockSize
,
rfs
)
;
fsNIO
.
loadPermissionInfoByNativeIO
(
)
;
RawLocalFileSystem
.
DeprecatedRawLocalFileStatus
fsnonNIO
=
new
RawLocalFileSystem
.
DeprecatedRawLocalFileStatus
(
file
,
defaultBlockSize
,
rfs
)
;
fsnonNIO
.
loadPermissionInfoByNonNativeIO
(
)
;
assertEquals
(
fsNIO
.
getOwner
(
)
,
fsnonNIO
.
getOwner
(
)
)
;
assertEquals
(
fsNIO
.
getGroup
(
)
,
fsnonNIO
.
getGroup
(
)
)
;
assertEquals
(
fsNIO
.
getPermission
(
)
,
fsnonNIO
.
getPermission
(
)
)
;
protected
void
verifyContents
(
final
Path
file
,
final
MessageDigest
origDigest
,
final
int
expectedLength
)
throws
IOException
{
ContractTestUtils
.
NanoTimer
timer2
=
new
ContractTestUtils
.
NanoTimer
(
)
;
Assertions
.
assertThat
(
digest
(
file
)
)
.
describedAs
(
,
file
)
.
isEqualTo
(
origDigest
.
digest
(
)
)
;
timer2
.
end
(
,
file
)
;
MessageDigest
digest1
=
DigestUtils
.
getMd5Digest
(
)
;
digest1
.
update
(
payload1
)
;
UploadHandle
upload1
=
startUpload
(
file
)
;
Map
<
Integer
,
PartHandle
>
partHandles1
=
new
HashMap
<
>
(
)
;
int
size2
=
size1
*
2
;
int
partId2
=
2
;
byte
[
]
payload2
=
generatePayload
(
partId1
,
size2
)
;
MessageDigest
digest2
=
DigestUtils
.
getMd5Digest
(
)
;
digest2
.
update
(
payload2
)
;
UploadHandle
upload2
;
try
{
upload2
=
startUpload
(
file
)
;
Assume
.
assumeTrue
(
,
concurrent
)
;
}
catch
(
IOException
e
)
{
if
(
!
concurrent
)
{
@
Test
public
void
testRmEmptyRootDirRecursive
(
)
throws
Throwable
{
skipIfUnsupported
(
TEST_ROOT_TESTS_ENABLED
)
;
Path
root
=
new
Path
(
)
;
assertIsDirectory
(
root
)
;
boolean
deleted
=
getFileSystem
(
)
.
delete
(
root
,
true
)
;
skipIfUnsupported
(
TEST_ROOT_TESTS_ENABLED
)
;
final
Path
root
=
new
Path
(
)
;
assertIsDirectory
(
root
)
;
final
FileSystem
fs
=
getFileSystem
(
)
;
final
AtomicInteger
iterations
=
new
AtomicInteger
(
0
)
;
final
FileStatus
[
]
originalChildren
=
listChildren
(
fs
,
root
)
;
LambdaTestUtils
.
eventually
(
OBJECTSTORE_RETRY_TIMEOUT
,
new
Callable
<
Void
>
(
)
{
@
Override
public
Void
call
(
)
throws
Exception
{
FileStatus
[
]
deleted
=
deleteChildren
(
fs
,
root
,
true
)
;
FileStatus
[
]
children
=
listChildren
(
fs
,
root
)
;
if
(
children
.
length
>
0
)
{
fail
(
String
.
format
(
+
,
iterations
.
incrementAndGet
(
)
,
dumpStats
(
,
children
)
,
dumpStats
(
,
deleted
)
,
dumpStats
(
,
originalChildren
)
)
)
;
}
return
null
;
}
}
,
new
LambdaTestUtils
.
ProportionalRetryInterval
(
50
,
1000
)
)
;
boolean
deleted
=
fs
.
delete
(
root
,
false
)
;
@
Test
public
void
testRmRootRecursive
(
)
throws
Throwable
{
skipIfUnsupported
(
TEST_ROOT_TESTS_ENABLED
)
;
Path
root
=
new
Path
(
)
;
assertIsDirectory
(
root
)
;
Path
file
=
new
Path
(
)
;
try
{
ContractTestUtils
.
touch
(
getFileSystem
(
)
,
file
)
;
boolean
deleted
=
getFileSystem
(
)
.
delete
(
root
,
true
)
;
assertIsDirectory
(
root
)
;
@
Before
public
void
setup
(
)
throws
Exception
{
Thread
.
currentThread
(
)
.
setName
(
)
;
LOG
.
debug
(
)
;
contract
=
createContract
(
createConfiguration
(
)
)
;
contract
.
init
(
)
;
assumeEnabled
(
)
;
fileSystem
=
contract
.
getTestFileSystem
(
)
;
assertNotNull
(
,
fileSystem
)
;
URI
fsURI
=
fileSystem
.
getUri
(
)
;
protected
void
describe
(
String
text
)
{
public
static
FileStatus
[
]
deleteChildren
(
FileSystem
fileSystem
,
Path
path
,
boolean
recursive
)
throws
IOException
{
public
static
FileStatus
[
]
deleteChildren
(
FileSystem
fileSystem
,
Path
path
,
boolean
recursive
)
throws
IOException
{
LOG
.
debug
(
,
path
,
recursive
)
;
FileStatus
[
]
children
=
listChildren
(
fileSystem
,
path
)
;
for
(
FileStatus
entry
:
children
)
{
public
static
void
noteAction
(
String
action
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
skip
(
String
message
)
{
public
static
void
bandwidth
(
NanoTimer
timer
,
long
bytes
)
{
public
static
TreeScanResults
treeWalk
(
FileSystem
fs
,
Path
path
)
throws
IOException
{
TreeScanResults
dirsAndFiles
=
new
TreeScanResults
(
)
;
FileStatus
[
]
statuses
=
fs
.
listStatus
(
path
)
;
for
(
FileStatus
status
:
statuses
)
{
barrier
(
)
;
DFSClientThread
[
]
threads
=
new
DFSClientThread
[
numOfThreads
]
;
for
(
int
i
=
0
;
i
<
numOfThreads
;
i
++
)
{
threads
[
i
]
=
new
DFSClientThread
(
i
)
;
threads
[
i
]
.
start
(
)
;
}
if
(
durations
[
0
]
>
0
)
{
if
(
durations
.
length
==
1
)
{
while
(
shouldRun
)
{
Thread
.
sleep
(
2000
)
;
totalTime
+=
2
;
if
(
totalTime
>=
durations
[
0
]
||
stopFileCreated
(
)
)
{
shouldRun
=
false
;
}
}
}
else
{
while
(
shouldRun
)
{
Thread
.
sleep
(
durations
[
currentIndex
]
*
1000
)
;
@
Test
public
void
testGetMountPoints
(
)
{
ViewFileSystem
viewfs
=
(
ViewFileSystem
)
fsView
;
MountPoint
[
]
mountPoints
=
viewfs
.
getMountPoints
(
)
;
for
(
MountPoint
mountPoint
:
mountPoints
)
{
final
Path
actualMountLinkTarget
=
fsView
.
getLinkTarget
(
mountTargetSymLinkPath
)
;
assertEquals
(
,
expectedMountLinkTarget
,
actualMountLinkTarget
)
;
final
String
relativeFileName
=
+
targetFileName
;
final
String
link2FileName
=
;
final
Path
relTargetFile
=
new
Path
(
targetTestRoot
,
relativeFileName
)
;
final
Path
relativeSymLink
=
new
Path
(
targetTestRoot
,
link2FileName
)
;
fsTarget
.
createSymlink
(
relTargetFile
,
relativeSymLink
,
true
)
;
final
Path
mountTargetRelativeSymLinkPath
=
new
Path
(
mountTargetRootPath
,
link2FileName
)
;
final
Path
expectedMountRelLinkTarget
=
fsTarget
.
makeQualified
(
new
Path
(
targetTestRoot
,
relativeFileName
)
)
;
final
Path
actualMountRelLinkTarget
=
fsView
.
getLinkTarget
(
mountTargetRelativeSymLinkPath
)
;
assertEquals
(
,
expectedMountRelLinkTarget
,
actualMountRelLinkTarget
)
;
try
{
fsView
.
getLinkTarget
(
new
Path
(
)
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
fsTarget
.
createSymlink
(
relTargetFile
,
relativeSymLink
,
true
)
;
final
Path
mountTargetRelativeSymLinkPath
=
new
Path
(
mountTargetRootPath
,
link2FileName
)
;
final
Path
expectedMountRelLinkTarget
=
fsTarget
.
makeQualified
(
new
Path
(
targetTestRoot
,
relativeFileName
)
)
;
final
Path
actualMountRelLinkTarget
=
fsView
.
getLinkTarget
(
mountTargetRelativeSymLinkPath
)
;
assertEquals
(
,
expectedMountRelLinkTarget
,
actualMountRelLinkTarget
)
;
try
{
fsView
.
getLinkTarget
(
new
Path
(
)
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
e
)
;
assertThat
(
e
.
getMessage
(
)
,
containsString
(
)
)
;
}
try
{
fsView
.
getLinkTarget
(
fsView
.
makeQualified
(
new
Path
(
mountTargetRootPath
,
targetFileName
)
)
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
fsView
.
getLinkTarget
(
new
Path
(
)
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
e
)
;
assertThat
(
e
.
getMessage
(
)
,
containsString
(
)
)
;
}
try
{
fsView
.
getLinkTarget
(
fsView
.
makeQualified
(
new
Path
(
mountTargetRootPath
,
targetFileName
)
)
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
e
)
;
assertThat
(
e
.
getMessage
(
)
,
containsString
(
)
)
;
}
try
{
fsView
.
getLinkTarget
(
new
Path
(
)
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
public
static
String
send4LetterWord
(
String
host
,
int
port
,
String
cmd
)
throws
IOException
{
public
void
expireActiveLockHolder
(
int
idx
)
throws
NoNodeException
{
Stat
stat
=
new
Stat
(
)
;
byte
[
]
data
=
zks
.
getZKDatabase
(
)
.
getData
(
DummyZKFC
.
LOCK_ZNODE
,
stat
,
null
)
;
assertArrayEquals
(
Ints
.
toByteArray
(
svcs
.
get
(
idx
)
.
index
)
,
data
)
;
long
session
=
stat
.
getEphemeralOwner
(
)
;
private
Object
runTool
(
String
...
args
)
throws
Exception
{
errOutBytes
.
reset
(
)
;
outBytes
.
reset
(
)
;
LOG
.
info
(
+
Joiner
.
on
(
)
.
join
(
args
)
)
;
int
ret
=
tool
.
run
(
args
)
;
errOutput
=
new
String
(
errOutBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
output
=
new
String
(
outBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
@
Test
(
timeout
=
(
STRESS_RUNTIME_SECS
+
EXTRA_TIMEOUT_SECS
)
*
1000
)
public
void
testExpireBackAndForth
(
)
throws
Exception
{
cluster
.
start
(
)
;
long
st
=
Time
.
now
(
)
;
long
runFor
=
STRESS_RUNTIME_SECS
*
1000
;
int
i
=
0
;
while
(
Time
.
now
(
)
-
st
<
runFor
)
{
int
from
=
i
%
2
;
int
to
=
(
i
+
1
)
%
2
;
@
Test
(
timeout
=
(
STRESS_RUNTIME_SECS
+
EXTRA_TIMEOUT_SECS
)
*
1000
)
public
void
testRandomExpirations
(
)
throws
Exception
{
cluster
.
start
(
)
;
long
st
=
Time
.
now
(
)
;
long
runFor
=
STRESS_RUNTIME_SECS
*
1000
;
Random
r
=
new
Random
(
)
;
while
(
Time
.
now
(
)
-
st
<
runFor
)
{
cluster
.
getTestContext
(
)
.
checkException
(
)
;
int
targetIdx
=
r
.
nextInt
(
2
)
;
ActiveStandbyElector
target
=
cluster
.
getElector
(
targetIdx
)
;
long
sessId
=
target
.
getZKSessionIdForTests
(
)
;
if
(
sessId
!=
-
1
)
{
@
BeforeClass
public
static
void
setup
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
HttpServer2
.
HTTP_MAX_THREADS_KEY
,
MAX_THREADS
)
;
server
=
createTestServer
(
conf
)
;
server
.
addServlet
(
,
,
EchoServlet
.
class
)
;
server
.
addServlet
(
,
,
EchoMapServlet
.
class
)
;
server
.
addServlet
(
,
,
HtmlContentServlet
.
class
)
;
server
.
addServlet
(
,
,
LongHeaderServlet
.
class
)
;
server
.
addJerseyResourcePackage
(
JerseyResource
.
class
.
getPackage
(
)
.
getName
(
)
,
)
;
server
.
start
(
)
;
baseUrl
=
getServerURL
(
server
)
;
@
Test
public
void
testJersey
(
)
throws
Exception
{
LOG
.
info
(
)
;
final
String
js
=
readOutput
(
new
URL
(
baseUrl
,
)
)
;
final
Map
<
String
,
Object
>
m
=
parse
(
js
)
;
private
void
startServer
(
Configuration
conf
)
throws
Exception
{
server
=
createTestServer
(
conf
)
;
server
.
addJerseyResourcePackage
(
JerseyResource
.
class
.
getPackage
(
)
.
getName
(
)
,
)
;
server
.
start
(
)
;
baseUrl
=
getServerURL
(
server
)
;
turnOnSSLDebugLogging
(
)
;
storeHttpsCipherSuites
(
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
HttpServer2
.
HTTP_MAX_THREADS_KEY
,
10
)
;
File
base
=
new
File
(
BASEDIR
)
;
FileUtil
.
fullyDelete
(
base
)
;
base
.
mkdirs
(
)
;
keystoreDir
=
new
File
(
BASEDIR
)
.
getAbsolutePath
(
)
;
sslConfDir
=
KeyStoreTestUtil
.
getClasspathDir
(
TestSSLHttpServer
.
class
)
;
KeyStoreTestUtil
.
setupSSLConfig
(
keystoreDir
,
sslConfDir
,
conf
,
false
,
true
,
EXCLUDED_CIPHERS
)
;
Configuration
sslConf
=
KeyStoreTestUtil
.
getSslConfig
(
)
;
clientSslFactory
=
new
SSLFactory
(
SSLFactory
.
Mode
.
CLIENT
,
sslConf
)
;
clientSslFactory
.
init
(
)
;
setupServer
(
conf
,
sslConf
)
;
baseUrl
=
new
URL
(
+
NetUtils
.
getHostPortString
(
server
.
getConnectorAddress
(
0
)
)
)
;
static
void
storeHttpsCipherSuites
(
)
{
String
cipherSuites
=
System
.
getProperty
(
HTTPS_CIPHER_SUITES_KEY
)
;
if
(
cipherSuites
!=
null
)
{
static
void
restoreHttpsCipherSuites
(
)
{
if
(
cipherSuitesPropertyValue
!=
null
)
{
private
HttpsURLConnection
getConnectionWithSSLSocketFactory
(
URL
url
,
String
ciphers
)
throws
IOException
,
GeneralSecurityException
{
HttpsURLConnection
conn
=
(
HttpsURLConnection
)
url
.
openConnection
(
)
;
SSLSocketFactory
sslSocketFactory
=
clientSslFactory
.
createSSLSocketFactory
(
)
;
private
HttpsURLConnection
getConnectionWithPreferredProtocolSSLSocketFactory
(
URL
url
,
String
protocols
)
throws
IOException
,
GeneralSecurityException
{
HttpsURLConnection
conn
=
(
HttpsURLConnection
)
url
.
openConnection
(
)
;
SSLSocketFactory
sslSocketFactory
=
clientSslFactory
.
createSSLSocketFactory
(
)
;
@
GET
@
Path
(
+
PATH
+
)
@
Produces
(
{
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
}
)
public
Response
get
(
@
PathParam
(
PATH
)
@
DefaultValue
(
+
PATH
)
final
String
path
,
@
QueryParam
(
OP
)
@
DefaultValue
(
+
OP
)
final
String
op
)
throws
IOException
{
private
static
RandomDatum
[
]
generate
(
int
count
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
writeTest
(
FileSystem
fs
,
RandomDatum
[
]
data
,
String
file
)
throws
IOException
{
Configuration
conf
=
new
Configuration
(
)
;
MapFile
.
delete
(
fs
,
file
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
readTest
(
FileSystem
fs
,
RandomDatum
[
]
data
,
String
file
,
Configuration
conf
)
throws
IOException
{
RandomDatum
v
=
new
RandomDatum
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
readTest
(
FileSystem
fs
,
RandomDatum
[
]
data
,
String
file
,
Configuration
conf
)
throws
IOException
{
RandomDatum
v
=
new
RandomDatum
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
data
.
length
+
)
;
}
ArrayFile
.
Reader
reader
=
new
ArrayFile
.
Reader
(
fs
,
file
,
conf
)
;
try
{
for
(
int
i
=
0
;
i
<
data
.
length
;
i
++
)
{
reader
.
get
(
i
,
v
)
;
if
(
!
v
.
equals
(
data
[
i
]
)
)
{
throw
new
RuntimeException
(
+
i
)
;
}
}
for
(
int
i
=
data
.
length
-
1
;
i
>=
0
;
i
--
)
{
reader
.
get
(
i
,
v
)
;
if
(
!
v
.
equals
(
data
[
i
]
)
)
{
throw
new
RuntimeException
(
+
i
)
;
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
Path
fpath
=
null
;
FileSystem
fs
=
null
;
try
{
for
(
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
==
null
)
{
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
FileSystem
fs
=
null
;
try
{
for
(
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
==
null
)
{
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
}
}
fs
=
fpath
.
getFileSystem
(
conf
)
;
try
{
for
(
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
==
null
)
{
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
}
}
fs
=
fpath
.
getFileSystem
(
conf
)
;
LOG
.
info
(
+
count
)
;
for
(
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
==
null
)
{
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
}
}
fs
=
fpath
.
getFileSystem
(
conf
)
;
LOG
.
info
(
+
count
)
;
LOG
.
info
(
+
create
)
;
@
Test
public
void
testWithJavaSerialization
(
)
throws
Exception
{
conf
.
set
(
,
)
;
LOG
.
info
(
)
;
Integer
testInt
=
Integer
.
valueOf
(
42
)
;
DefaultStringifier
<
Integer
>
stringifier
=
new
DefaultStringifier
<
Integer
>
(
conf
,
Integer
.
class
)
;
String
str
=
stringifier
.
toString
(
testInt
)
;
Integer
claimedInt
=
stringifier
.
fromString
(
str
)
;
public
void
compressedSeqFileTest
(
CompressionCodec
codec
)
throws
Exception
{
int
count
=
1024
*
10
;
int
megabytes
=
1
;
int
factor
=
5
;
Path
file
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
Path
recordCompressedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
Path
blockCompressedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
@
SuppressWarnings
(
)
private
void
writeTest
(
FileSystem
fs
,
int
count
,
int
seed
,
Path
file
,
CompressionType
compressionType
,
CompressionCodec
codec
)
throws
IOException
{
fs
.
delete
(
file
,
true
)
;
@
SuppressWarnings
(
)
private
void
readTest
(
FileSystem
fs
,
int
count
,
int
seed
,
Path
file
)
throws
IOException
{
RandomDatum
key
=
generator
.
getKey
(
)
;
RandomDatum
value
=
generator
.
getValue
(
)
;
try
{
if
(
(
i
%
5
)
==
0
)
{
rawKey
.
reset
(
)
;
reader
.
nextRaw
(
rawKey
,
rawValue
)
;
}
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
RandomDatum
value
=
generator
.
getValue
(
)
;
try
{
if
(
(
i
%
5
)
==
0
)
{
rawKey
.
reset
(
)
;
reader
.
nextRaw
(
rawKey
,
rawValue
)
;
}
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
try
{
if
(
(
i
%
5
)
==
0
)
{
rawKey
.
reset
(
)
;
reader
.
nextRaw
(
rawKey
,
rawValue
)
;
}
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
if
(
(
i
%
5
)
==
0
)
{
rawKey
.
reset
(
)
;
reader
.
nextRaw
(
rawKey
,
rawValue
)
;
}
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
rawKey
.
reset
(
)
;
reader
.
nextRaw
(
rawKey
,
rawValue
)
;
}
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
LOG
.
info
(
+
k
)
;
reader
.
nextRaw
(
rawKey
,
rawValue
)
;
}
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
LOG
.
info
(
+
k
)
;
LOG
.
info
(
+
k
.
getLength
(
)
)
;
}
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
LOG
.
info
(
+
k
)
;
LOG
.
info
(
+
k
.
getLength
(
)
)
;
LOG
.
info
(
+
value
)
;
else
{
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
LOG
.
info
(
+
k
)
;
LOG
.
info
(
+
k
.
getLength
(
)
)
;
LOG
.
info
(
+
value
)
;
if
(
(
i
%
2
)
==
0
)
{
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
LOG
.
info
(
+
k
)
;
LOG
.
info
(
+
k
.
getLength
(
)
)
;
LOG
.
info
(
+
value
)
;
LOG
.
info
(
+
value
.
getLength
(
)
)
;
reader
.
next
(
k
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
LOG
.
info
(
+
k
)
;
LOG
.
info
(
+
k
.
getLength
(
)
)
;
LOG
.
info
(
+
value
)
;
LOG
.
info
(
+
value
.
getLength
(
)
)
;
LOG
.
info
(
+
v
)
;
reader
.
getCurrentValue
(
v
)
;
}
else
{
reader
.
next
(
k
,
v
)
;
}
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
+
i
)
;
LOG
.
info
(
+
key
)
;
LOG
.
info
(
+
key
.
getLength
(
)
)
;
LOG
.
info
(
+
k
)
;
LOG
.
info
(
+
k
.
getLength
(
)
)
;
LOG
.
info
(
+
value
)
;
LOG
.
info
(
+
value
.
getLength
(
)
)
;
LOG
.
info
(
+
v
)
;
LOG
.
info
(
+
v
.
getLength
(
)
)
;
private
void
sortTest
(
FileSystem
fs
,
int
count
,
int
megabytes
,
int
factor
,
boolean
fast
,
Path
file
)
throws
IOException
{
fs
.
delete
(
new
Path
(
file
+
)
,
true
)
;
SequenceFile
.
Sorter
sorter
=
newSorter
(
fs
,
fast
,
megabytes
,
factor
)
;
private
void
sortTest
(
FileSystem
fs
,
int
count
,
int
megabytes
,
int
factor
,
boolean
fast
,
Path
file
)
throws
IOException
{
fs
.
delete
(
new
Path
(
file
+
)
,
true
)
;
SequenceFile
.
Sorter
sorter
=
newSorter
(
fs
,
fast
,
megabytes
,
factor
)
;
LOG
.
debug
(
+
count
+
)
;
sorter
.
sort
(
file
,
file
.
suffix
(
)
)
;
@
SuppressWarnings
(
)
private
void
checkSort
(
FileSystem
fs
,
int
count
,
int
seed
,
Path
file
)
throws
IOException
{
RandomDatum
value
=
generator
.
getValue
(
)
;
map
.
put
(
key
,
value
)
;
}
LOG
.
debug
(
+
count
+
)
;
RandomDatum
k
=
new
RandomDatum
(
)
;
RandomDatum
v
=
new
RandomDatum
(
)
;
Iterator
<
Map
.
Entry
<
RandomDatum
,
RandomDatum
>>
iterator
=
map
.
entrySet
(
)
.
iterator
(
)
;
SequenceFile
.
Reader
reader
=
new
SequenceFile
.
Reader
(
fs
,
file
.
suffix
(
)
,
conf
)
;
for
(
int
i
=
0
;
i
<
count
;
i
++
)
{
Map
.
Entry
<
RandomDatum
,
RandomDatum
>
entry
=
iterator
.
next
(
)
;
RandomDatum
key
=
entry
.
getKey
(
)
;
RandomDatum
value
=
entry
.
getValue
(
)
;
reader
.
next
(
k
,
v
)
;
if
(
!
k
.
equals
(
key
)
)
throw
new
RuntimeException
(
+
i
)
;
if
(
!
v
.
equals
(
value
)
)
throw
new
RuntimeException
(
+
i
)
;
}
reader
.
close
(
)
;
Path
file
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
Path
sortedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
Path
recordCompressedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
Path
blockCompressedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
FileSystem
fs
=
FileSystem
.
getLocal
(
conf
)
;
SequenceFile
.
Metadata
theMetadata
=
new
SequenceFile
.
Metadata
(
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
try
{
writeMetadataTest
(
fs
,
count
,
seed
,
file
,
CompressionType
.
NONE
,
null
,
theMetadata
)
;
SequenceFile
.
Metadata
aMetadata
=
readMetadata
(
fs
,
file
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
Path
sortedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
Path
recordCompressedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
Path
blockCompressedFile
=
new
Path
(
GenericTestUtils
.
getTempPath
(
)
)
;
FileSystem
fs
=
FileSystem
.
getLocal
(
conf
)
;
SequenceFile
.
Metadata
theMetadata
=
new
SequenceFile
.
Metadata
(
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
try
{
writeMetadataTest
(
fs
,
count
,
seed
,
file
,
CompressionType
.
NONE
,
null
,
theMetadata
)
;
SequenceFile
.
Metadata
aMetadata
=
readMetadata
(
fs
,
file
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
try
{
writeMetadataTest
(
fs
,
count
,
seed
,
file
,
CompressionType
.
NONE
,
null
,
theMetadata
)
;
SequenceFile
.
Metadata
aMetadata
=
readMetadata
(
fs
,
file
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
1
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
recordCompressedFile
,
CompressionType
.
RECORD
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
theMetadata
.
set
(
new
Text
(
)
,
new
Text
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
try
{
writeMetadataTest
(
fs
,
count
,
seed
,
file
,
CompressionType
.
NONE
,
null
,
theMetadata
)
;
SequenceFile
.
Metadata
aMetadata
=
readMetadata
(
fs
,
file
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
1
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
recordCompressedFile
,
CompressionType
.
RECORD
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
writeMetadataTest
(
fs
,
count
,
seed
,
file
,
CompressionType
.
NONE
,
null
,
theMetadata
)
;
SequenceFile
.
Metadata
aMetadata
=
readMetadata
(
fs
,
file
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
1
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
recordCompressedFile
,
CompressionType
.
RECORD
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
2
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
blockCompressedFile
,
CompressionType
.
BLOCK
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
blockCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
SequenceFile
.
Metadata
aMetadata
=
readMetadata
(
fs
,
file
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
1
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
recordCompressedFile
,
CompressionType
.
RECORD
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
2
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
blockCompressedFile
,
CompressionType
.
BLOCK
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
blockCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
recordCompressedFile
,
CompressionType
.
RECORD
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
2
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
blockCompressedFile
,
CompressionType
.
BLOCK
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
blockCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
3
)
;
}
sortMetadataTest
(
fs
,
file
,
sortedFile
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
writeMetadataTest
(
fs
,
count
,
seed
,
recordCompressedFile
,
CompressionType
.
RECORD
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
2
)
;
}
writeMetadataTest
(
fs
,
count
,
seed
,
blockCompressedFile
,
CompressionType
.
BLOCK
,
codec
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
blockCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
LOG
.
info
(
+
theMetadata
.
toString
(
)
)
;
LOG
.
info
(
+
aMetadata
.
toString
(
)
)
;
throw
new
RuntimeException
(
+
3
)
;
}
sortMetadataTest
(
fs
,
file
,
sortedFile
,
theMetadata
)
;
aMetadata
=
readMetadata
(
fs
,
recordCompressedFile
)
;
if
(
!
theMetadata
.
equals
(
aMetadata
)
)
{
@
SuppressWarnings
(
)
private
SequenceFile
.
Metadata
readMetadata
(
FileSystem
fs
,
Path
file
)
throws
IOException
{
@
SuppressWarnings
(
)
private
void
writeMetadataTest
(
FileSystem
fs
,
int
count
,
int
seed
,
Path
file
,
CompressionType
compressionType
,
CompressionCodec
codec
,
SequenceFile
.
Metadata
metadata
)
throws
IOException
{
fs
.
delete
(
file
,
true
)
;
private
void
sortMetadataTest
(
FileSystem
fs
,
Path
unsortedFile
,
Path
sortedFile
,
SequenceFile
.
Metadata
metadata
)
throws
IOException
{
fs
.
delete
(
sortedFile
,
true
)
;
else
if
(
args
[
i
]
.
equals
(
)
)
{
rwonly
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fast
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
merge
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressType
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressionCodec
=
args
[
++
i
]
;
}
else
{
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fast
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
merge
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressType
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressionCodec
=
args
[
++
i
]
;
}
else
{
file
=
new
Path
(
args
[
i
]
)
;
}
}
TestSequenceFile
test
=
new
TestSequenceFile
(
)
;
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fast
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
merge
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressType
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressionCodec
=
args
[
++
i
]
;
}
else
{
file
=
new
Path
(
args
[
i
]
)
;
}
}
TestSequenceFile
test
=
new
TestSequenceFile
(
)
;
fs
=
file
.
getFileSystem
(
test
.
conf
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fast
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
merge
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressType
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressionCodec
=
args
[
++
i
]
;
}
else
{
file
=
new
Path
(
args
[
i
]
)
;
}
}
TestSequenceFile
test
=
new
TestSequenceFile
(
)
;
fs
=
file
.
getFileSystem
(
test
.
conf
)
;
LOG
.
info
(
+
count
)
;
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fast
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
merge
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressType
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressionCodec
=
args
[
++
i
]
;
}
else
{
file
=
new
Path
(
args
[
i
]
)
;
}
}
TestSequenceFile
test
=
new
TestSequenceFile
(
)
;
fs
=
file
.
getFileSystem
(
test
.
conf
)
;
LOG
.
info
(
+
count
)
;
check
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fast
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
merge
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressType
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressionCodec
=
args
[
++
i
]
;
}
else
{
file
=
new
Path
(
args
[
i
]
)
;
}
}
TestSequenceFile
test
=
new
TestSequenceFile
(
)
;
fs
=
file
.
getFileSystem
(
test
.
conf
)
;
LOG
.
info
(
+
count
)
;
LOG
.
info
(
+
megabytes
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fast
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
merge
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressType
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compressionCodec
=
args
[
++
i
]
;
}
else
{
file
=
new
Path
(
args
[
i
]
)
;
}
}
TestSequenceFile
test
=
new
TestSequenceFile
(
)
;
fs
=
file
.
getFileSystem
(
test
.
conf
)
;
LOG
.
info
(
+
count
)
;
LOG
.
info
(
+
megabytes
)
;
LOG
.
info
(
+
factor
)
;
private
static
RandomDatum
[
]
generate
(
int
count
)
{
private
static
void
writeTest
(
FileSystem
fs
,
RandomDatum
[
]
data
,
String
file
,
CompressionType
compress
)
throws
IOException
{
MapFile
.
delete
(
fs
,
file
)
;
private
static
void
readTest
(
FileSystem
fs
,
RandomDatum
[
]
data
,
String
file
)
throws
IOException
{
RandomDatum
v
=
new
RandomDatum
(
)
;
int
sample
=
(
int
)
Math
.
sqrt
(
data
.
length
)
;
Random
random
=
new
Random
(
)
;
try
{
for
(
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
==
null
)
{
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compress
=
args
[
++
i
]
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
for
(
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
==
null
)
{
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compress
=
args
[
++
i
]
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
}
}
fs
=
fpath
.
getFileSystem
(
conf
)
;
if
(
args
[
i
]
==
null
)
{
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compress
=
args
[
++
i
]
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
}
}
fs
=
fpath
.
getFileSystem
(
conf
)
;
LOG
.
info
(
+
count
)
;
continue
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compress
=
args
[
++
i
]
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
}
}
fs
=
fpath
.
getFileSystem
(
conf
)
;
LOG
.
info
(
+
count
)
;
LOG
.
info
(
+
create
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
count
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
create
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
check
=
false
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
compress
=
args
[
++
i
]
;
}
else
{
file
=
args
[
i
]
;
fpath
=
new
Path
(
file
)
;
}
}
fs
=
fpath
.
getFileSystem
(
conf
)
;
LOG
.
info
(
+
count
)
;
LOG
.
info
(
+
create
)
;
LOG
.
info
(
+
check
)
;
private
void
testValue
(
int
val
,
int
vintlen
)
throws
IOException
{
DataOutputBuffer
buf
=
new
DataOutputBuffer
(
)
;
DataInputBuffer
inbuf
=
new
DataInputBuffer
(
)
;
WritableUtils
.
writeVInt
(
buf
,
val
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
testValue
(
int
val
,
int
vintlen
)
throws
IOException
{
DataOutputBuffer
buf
=
new
DataOutputBuffer
(
)
;
DataInputBuffer
inbuf
=
new
DataInputBuffer
(
)
;
WritableUtils
.
writeVInt
(
buf
,
val
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
val
)
;
BytesWritable
printer
=
new
BytesWritable
(
)
;
printer
.
set
(
buf
.
getData
(
)
,
0
,
buf
.
getLength
(
)
)
;
private
void
testSplitableCodec
(
Class
<
?
extends
SplittableCompressionCodec
>
codecClass
)
throws
IOException
{
final
long
DEFLBYTES
=
2
*
1024
*
1024
;
final
Configuration
conf
=
new
Configuration
(
)
;
final
Random
rand
=
new
Random
(
)
;
final
long
seed
=
rand
.
nextLong
(
)
;
private
void
codecTestMapFile
(
Class
<
?
extends
CompressionCodec
>
clazz
,
CompressionType
type
,
int
records
)
throws
Exception
{
FileSystem
fs
=
FileSystem
.
get
(
conf
)
;
private
void
codecTestMapFile
(
Class
<
?
extends
CompressionCodec
>
clazz
,
CompressionType
type
,
int
records
)
throws
Exception
{
FileSystem
fs
=
FileSystem
.
get
(
conf
)
;
LOG
.
info
(
+
records
+
+
clazz
.
getSimpleName
(
)
)
;
Path
path
=
new
Path
(
GenericTestUtils
.
getTempPath
(
clazz
.
getSimpleName
(
)
+
+
type
+
+
records
)
)
;
@
Test
public
void
testGzipCompatibility
(
)
throws
IOException
{
Random
r
=
new
Random
(
)
;
long
seed
=
r
.
nextLong
(
)
;
r
.
setSeed
(
seed
)
;
void
GzipConcatTest
(
Configuration
conf
,
Class
<
?
extends
Decompressor
>
decomClass
)
throws
IOException
{
Random
r
=
new
Random
(
)
;
long
seed
=
r
.
nextLong
(
)
;
r
.
setSeed
(
seed
)
;
deflateFilter
.
write
(
data
.
getData
(
)
,
0
,
data
.
getLength
(
)
)
;
deflateFilter
.
finish
(
)
;
deflateFilter
.
flush
(
)
;
LOG
.
info
(
)
;
DataInputBuffer
deCompressedDataBuffer
=
new
DataInputBuffer
(
)
;
deCompressedDataBuffer
.
reset
(
compressedDataBuffer
.
getData
(
)
,
0
,
compressedDataBuffer
.
getLength
(
)
)
;
CompressionInputStream
inflateFilter
=
codec
.
createInputStream
(
deCompressedDataBuffer
)
;
DataInputStream
inflateIn
=
new
DataInputStream
(
new
BufferedInputStream
(
inflateFilter
)
)
;
for
(
int
i
=
0
;
i
<
count
;
++
i
)
{
RandomDatum
k1
=
new
RandomDatum
(
)
;
RandomDatum
v1
=
new
RandomDatum
(
)
;
k1
.
readFields
(
originalIn
)
;
v1
.
readFields
(
originalIn
)
;
RandomDatum
k2
=
new
RandomDatum
(
)
;
RandomDatum
v2
=
new
RandomDatum
(
)
;
@
Test
public
void
testWriteOverride
(
)
throws
IOException
{
Random
r
=
new
Random
(
)
;
long
seed
=
r
.
nextLong
(
)
;
@
Test
public
void
testSnappyCompressDecompress
(
)
throws
Exception
{
int
BYTE_SIZE
=
1024
*
54
;
byte
[
]
bytes
=
BytesGenerator
.
get
(
BYTE_SIZE
)
;
SnappyCompressor
compressor
=
new
SnappyCompressor
(
)
;
compressor
.
setInput
(
bytes
,
0
,
bytes
.
length
)
;
assertTrue
(
,
compressor
.
getBytesRead
(
)
>
0
)
;
assertEquals
(
,
0
,
compressor
.
getBytesWritten
(
)
)
;
int
maxSize
=
32
+
BYTE_SIZE
+
BYTE_SIZE
/
6
;
byte
[
]
compressed
=
new
byte
[
maxSize
]
;
int
cSize
=
compressor
.
compress
(
compressed
,
0
,
compressed
.
length
)
;
@
Test
public
void
testSnappyCompressDecompress
(
)
throws
Exception
{
int
BYTE_SIZE
=
1024
*
54
;
byte
[
]
bytes
=
BytesGenerator
.
get
(
BYTE_SIZE
)
;
SnappyCompressor
compressor
=
new
SnappyCompressor
(
)
;
compressor
.
setInput
(
bytes
,
0
,
bytes
.
length
)
;
assertTrue
(
,
compressor
.
getBytesRead
(
)
>
0
)
;
assertEquals
(
,
0
,
compressor
.
getBytesWritten
(
)
)
;
int
maxSize
=
32
+
BYTE_SIZE
+
BYTE_SIZE
/
6
;
byte
[
]
compressed
=
new
byte
[
maxSize
]
;
int
cSize
=
compressor
.
compress
(
compressed
,
0
,
compressed
.
length
)
;
LOG
.
info
(
,
BYTE_SIZE
)
;
@
Test
(
timeout
=
30000
)
public
void
testFstat
(
)
throws
Exception
{
FileOutputStream
fos
=
new
FileOutputStream
(
new
File
(
TEST_DIR
,
)
)
;
NativeIO
.
POSIX
.
Stat
stat
=
NativeIO
.
POSIX
.
getFstat
(
fos
.
getFD
(
)
)
;
fos
.
close
(
)
;
private
boolean
doStatTest
(
String
testFilePath
)
throws
Exception
{
NativeIO
.
POSIX
.
Stat
stat
=
NativeIO
.
POSIX
.
getStat
(
testFilePath
)
;
String
owner
=
stat
.
getOwner
(
)
;
String
group
=
stat
.
getGroup
(
)
;
int
mode
=
stat
.
getMode
(
)
;
String
expectedOwner
=
System
.
getProperty
(
)
;
assertEquals
(
expectedOwner
,
owner
)
;
assertNotNull
(
group
)
;
assertTrue
(
!
group
.
isEmpty
(
)
)
;
StatUtils
.
Permission
expected
=
StatUtils
.
getPermissionFromProcess
(
testFilePath
)
;
StatUtils
.
Permission
permission
=
new
StatUtils
.
Permission
(
owner
,
group
,
new
FsPermission
(
mode
)
)
;
assertEquals
(
expected
.
getOwner
(
)
,
permission
.
getOwner
(
)
)
;
assertEquals
(
expected
.
getGroup
(
)
,
permission
.
getGroup
(
)
)
;
assertEquals
(
expected
.
getFsPermission
(
)
,
permission
.
getFsPermission
(
)
)
;
private
boolean
doStatTest
(
String
testFilePath
)
throws
Exception
{
NativeIO
.
POSIX
.
Stat
stat
=
NativeIO
.
POSIX
.
getStat
(
testFilePath
)
;
String
owner
=
stat
.
getOwner
(
)
;
String
group
=
stat
.
getGroup
(
)
;
int
mode
=
stat
.
getMode
(
)
;
String
expectedOwner
=
System
.
getProperty
(
)
;
assertEquals
(
expectedOwner
,
owner
)
;
assertNotNull
(
group
)
;
assertTrue
(
!
group
.
isEmpty
(
)
)
;
StatUtils
.
Permission
expected
=
StatUtils
.
getPermissionFromProcess
(
testFilePath
)
;
StatUtils
.
Permission
permission
=
new
StatUtils
.
Permission
(
owner
,
group
,
new
FsPermission
(
mode
)
)
;
assertEquals
(
expected
.
getOwner
(
)
,
permission
.
getOwner
(
)
)
;
assertEquals
(
expected
.
getGroup
(
)
,
permission
.
getGroup
(
)
)
;
assertEquals
(
expected
.
getFsPermission
(
)
,
permission
.
getFsPermission
(
)
)
;
LOG
.
info
(
,
testFilePath
,
stat
)
;
@
Test
(
timeout
=
30000
)
public
void
testOpenWithCreate
(
)
throws
Exception
{
assumeNotWindows
(
)
;
LOG
.
info
(
)
;
FileDescriptor
fd
=
NativeIO
.
POSIX
.
open
(
new
File
(
TEST_DIR
,
)
.
getAbsolutePath
(
)
,
O_WRONLY
|
O_CREAT
,
0700
)
;
assertNotNull
(
true
)
;
assertTrue
(
fd
.
valid
(
)
)
;
FileOutputStream
fos
=
new
FileOutputStream
(
fd
)
;
fos
.
write
(
.
getBytes
(
)
)
;
fos
.
close
(
)
;
assertFalse
(
fd
.
valid
(
)
)
;
LOG
.
info
(
)
;
try
{
fd
=
NativeIO
.
POSIX
.
open
(
new
File
(
TEST_DIR
,
)
.
getAbsolutePath
(
)
,
O_WRONLY
|
O_CREAT
|
O_EXCL
,
0700
)
;
fail
(
)
;
}
catch
(
NativeIOException
nioe
)
{
assumeTrue
(
NativeIO
.
POSIX
.
isPmdkAvailable
(
)
)
;
String
filePath
=
;
long
length
=
0
;
long
volumnSize
=
16
*
1024
*
1024
*
1024L
;
try
{
NativeIO
.
POSIX
.
Pmem
.
mapBlock
(
filePath
,
length
,
false
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
}
filePath
=
;
length
=
-
1L
;
try
{
NativeIO
.
POSIX
.
Pmem
.
mapBlock
(
filePath
,
length
,
false
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
@
Test
(
timeout
=
10000
)
public
void
testPmemMapMultipleFiles
(
)
{
assumeNotWindows
(
)
;
assumeTrue
(
NativeIO
.
POSIX
.
isPmdkAvailable
(
)
)
;
String
filePath
=
;
long
length
=
0
;
long
volumnSize
=
16
*
1024
*
1024
*
1024L
;
length
=
128
*
1024
*
1024L
;
long
fileNumber
=
volumnSize
/
length
;
@
Test
(
timeout
=
10000
)
public
void
testPmemMapMultipleFiles
(
)
{
assumeNotWindows
(
)
;
assumeTrue
(
NativeIO
.
POSIX
.
isPmdkAvailable
(
)
)
;
String
filePath
=
;
long
length
=
0
;
long
volumnSize
=
16
*
1024
*
1024
*
1024L
;
length
=
128
*
1024
*
1024L
;
long
fileNumber
=
volumnSize
/
length
;
LOG
.
info
(
+
fileNumber
)
;
for
(
int
i
=
0
;
i
<
fileNumber
;
i
++
)
{
String
path
=
filePath
+
i
;
assumeTrue
(
NativeIO
.
POSIX
.
isPmdkAvailable
(
)
)
;
String
filePath
=
;
long
length
=
0
;
long
volumnSize
=
16
*
1024
*
1024
*
1024L
;
length
=
128
*
1024
*
1024L
;
long
fileNumber
=
volumnSize
/
length
;
LOG
.
info
(
+
fileNumber
)
;
for
(
int
i
=
0
;
i
<
fileNumber
;
i
++
)
{
String
path
=
filePath
+
i
;
LOG
.
info
(
+
path
)
;
NativeIO
.
POSIX
.
Pmem
.
mapBlock
(
path
,
length
,
false
)
;
}
try
{
NativeIO
.
POSIX
.
Pmem
.
mapBlock
(
filePath
,
length
,
false
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
@
Test
(
timeout
=
10000
)
public
void
testPmemMapBigFile
(
)
{
assumeNotWindows
(
)
;
assumeTrue
(
NativeIO
.
POSIX
.
isPmdkAvailable
(
)
)
;
String
filePath
=
;
long
length
=
0
;
long
volumeSize
=
16
*
1024
*
1024
*
1024L
;
length
=
volumeSize
+
1024L
;
try
{
private
static
void
assertExceptionContains
(
Throwable
t
,
String
substring
)
{
String
msg
=
StringUtils
.
stringifyException
(
t
)
;
assertTrue
(
+
substring
+
+
msg
,
msg
.
contains
(
substring
)
)
;
@
Test
(
timeout
=
60000
)
public
void
testRTEDuringConnectionSetup
(
)
throws
IOException
{
SocketFactory
spyFactory
=
spy
(
NetUtils
.
getDefaultSocketFactory
(
conf
)
)
;
Mockito
.
doAnswer
(
new
Answer
<
Socket
>
(
)
{
@
Override
public
Socket
answer
(
InvocationOnMock
invocation
)
{
return
new
MockSocket
(
)
;
}
}
)
.
when
(
spyFactory
)
.
createSocket
(
)
;
Server
server
=
new
TestServer
(
1
,
true
)
;
Client
client
=
new
Client
(
LongWritable
.
class
,
conf
,
spyFactory
)
;
server
.
start
(
)
;
try
{
InetSocketAddress
address
=
NetUtils
.
getConnectAddress
(
server
)
;
try
{
call
(
client
,
RANDOM
.
nextLong
(
)
,
address
,
conf
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
final
CountDownLatch
callFinishedLatch
=
new
CountDownLatch
(
clients
)
;
final
TestServerQueue
server
=
new
TestServerQueue
(
clients
,
readers
,
callQ
,
handlers
,
conf
)
;
CallQueueManager
<
Call
>
spy
=
spy
(
(
CallQueueManager
<
Call
>
)
Whitebox
.
getInternalState
(
server
,
)
)
;
Whitebox
.
setInternalState
(
server
,
,
spy
)
;
final
InetSocketAddress
addr
=
NetUtils
.
getConnectAddress
(
server
)
;
server
.
start
(
)
;
Client
.
setConnectTimeout
(
conf
,
10000
)
;
Thread
[
]
threads
=
new
Thread
[
clients
]
;
for
(
int
i
=
0
;
i
<
clients
;
i
++
)
{
threads
[
i
]
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
Client
client
=
new
Client
(
LongWritable
.
class
,
conf
)
;
try
{
call
(
client
,
new
LongWritable
(
Thread
.
currentThread
(
)
.
getId
(
)
)
,
addr
,
60000
,
conf
)
;
}
catch
(
Throwable
e
)
{
finally
{
callFinishedLatch
.
countDown
(
)
;
client
.
stop
(
)
;
}
}
}
)
;
}
for
(
int
i
=
0
;
i
<
initialClients
;
i
++
)
{
threads
[
i
]
.
start
(
)
;
if
(
i
==
0
)
{
server
.
firstCallLatch
.
await
(
)
;
}
verify
(
spy
,
timeout
(
5000
)
.
times
(
i
+
1
)
)
.
put
(
any
(
)
)
;
}
try
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
return
server
.
getNumOpenConnections
(
)
>=
initialClients
;
}
}
,
100
,
3000
)
;
}
catch
(
TimeoutException
e
)
{
}
try
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
return
server
.
getNumOpenConnections
(
)
>=
initialClients
;
}
}
,
100
,
3000
)
;
}
catch
(
TimeoutException
e
)
{
fail
(
)
;
}
LOG
.
info
(
+
initialClients
+
+
server
.
getNumOpenConnections
(
)
)
;
LOG
.
info
(
)
;
assertEquals
(
callQ
,
server
.
getCallQueueLen
(
)
)
;
assertEquals
(
initialClients
,
server
.
getNumOpenConnections
(
)
)
;
for
(
int
i
=
initialClients
;
i
<
clients
;
i
++
)
{
threads
[
i
]
.
start
(
)
;
}
Thread
.
sleep
(
10
)
;
try
{
final
CountDownLatch
allCallLatch
=
new
CountDownLatch
(
clients
)
;
final
AtomicBoolean
error
=
new
AtomicBoolean
(
)
;
final
TestServer
server
=
new
TestServer
(
clients
,
false
)
;
Thread
[
]
threads
=
new
Thread
[
clients
]
;
try
{
server
.
callListener
=
new
Runnable
(
)
{
AtomicBoolean
first
=
new
AtomicBoolean
(
true
)
;
@
Override
public
void
run
(
)
{
try
{
allCallLatch
.
countDown
(
)
;
if
(
first
.
compareAndSet
(
true
,
false
)
)
{
firstCallBarrier
.
await
(
)
;
}
else
{
callBarrier
.
await
(
)
;
}
}
catch
(
Throwable
t
)
{
@
Test
(
timeout
=
20000
)
public
void
test
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
TestProtoBufRpcServerHandoffServer
serverImpl
=
new
TestProtoBufRpcServerHandoffServer
(
)
;
BlockingService
blockingService
=
TestProtobufRpcHandoffProto
.
newReflectiveBlockingService
(
serverImpl
)
;
RPC
.
setProtocolEngine
(
conf
,
TestProtoBufRpcServerHandoffProtocol
.
class
,
ProtobufRpcEngine2
.
class
)
;
RPC
.
Server
server
=
new
RPC
.
Builder
(
conf
)
.
setProtocol
(
TestProtoBufRpcServerHandoffProtocol
.
class
)
.
setInstance
(
blockingService
)
.
setVerbose
(
true
)
.
setNumHandlers
(
1
)
.
build
(
)
;
server
.
start
(
)
;
InetSocketAddress
address
=
server
.
getListenerAddress
(
)
;
long
serverStartTime
=
System
.
currentTimeMillis
(
)
;
RPC
.
Server
server
=
new
RPC
.
Builder
(
conf
)
.
setProtocol
(
TestProtoBufRpcServerHandoffProtocol
.
class
)
.
setInstance
(
blockingService
)
.
setVerbose
(
true
)
.
setNumHandlers
(
1
)
.
build
(
)
;
server
.
start
(
)
;
InetSocketAddress
address
=
server
.
getListenerAddress
(
)
;
long
serverStartTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
address
+
+
serverStartTime
)
;
final
TestProtoBufRpcServerHandoffProtocol
client
=
RPC
.
getProxy
(
TestProtoBufRpcServerHandoffProtocol
.
class
,
1
,
address
,
conf
)
;
ExecutorService
executorService
=
Executors
.
newFixedThreadPool
(
2
)
;
CompletionService
<
ClientInvocationCallable
>
completionService
=
new
ExecutorCompletionService
<
ClientInvocationCallable
>
(
executorService
)
;
completionService
.
submit
(
new
ClientInvocationCallable
(
client
,
5000l
)
)
;
completionService
.
submit
(
new
ClientInvocationCallable
(
client
,
5000l
)
)
;
long
submitTime
=
System
.
currentTimeMillis
(
)
;
Future
<
ClientInvocationCallable
>
future1
=
completionService
.
take
(
)
;
Future
<
ClientInvocationCallable
>
future2
=
completionService
.
take
(
)
;
ClientInvocationCallable
callable1
=
future1
.
get
(
)
;
ClientInvocationCallable
callable2
=
future2
.
get
(
)
;
server
.
start
(
)
;
InetSocketAddress
address
=
server
.
getListenerAddress
(
)
;
long
serverStartTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
address
+
+
serverStartTime
)
;
final
TestProtoBufRpcServerHandoffProtocol
client
=
RPC
.
getProxy
(
TestProtoBufRpcServerHandoffProtocol
.
class
,
1
,
address
,
conf
)
;
ExecutorService
executorService
=
Executors
.
newFixedThreadPool
(
2
)
;
CompletionService
<
ClientInvocationCallable
>
completionService
=
new
ExecutorCompletionService
<
ClientInvocationCallable
>
(
executorService
)
;
completionService
.
submit
(
new
ClientInvocationCallable
(
client
,
5000l
)
)
;
completionService
.
submit
(
new
ClientInvocationCallable
(
client
,
5000l
)
)
;
long
submitTime
=
System
.
currentTimeMillis
(
)
;
Future
<
ClientInvocationCallable
>
future1
=
completionService
.
take
(
)
;
Future
<
ClientInvocationCallable
>
future2
=
completionService
.
take
(
)
;
ClientInvocationCallable
callable1
=
future1
.
get
(
)
;
ClientInvocationCallable
callable2
=
future2
.
get
(
)
;
LOG
.
info
(
callable1
.
toString
(
)
)
;
echoRequest2
=
TestProtos
.
EchoRequestProto2
.
newBuilder
(
)
.
addAllMessage
(
Collections
.
<
String
>
emptyList
(
)
)
.
build
(
)
;
echoResponse2
=
proxy
.
echo2
(
null
,
echoRequest2
)
;
assertTrue
(
Arrays
.
equals
(
echoResponse2
.
getMessageList
(
)
.
toArray
(
)
,
new
String
[
]
{
}
)
)
;
TestProtos
.
AddRequestProto
addRequest
=
TestProtos
.
AddRequestProto
.
newBuilder
(
)
.
setParam1
(
1
)
.
setParam2
(
2
)
.
build
(
)
;
TestProtos
.
AddResponseProto
addResponse
=
proxy
.
add
(
null
,
addRequest
)
;
assertThat
(
addResponse
.
getResult
(
)
)
.
isEqualTo
(
3
)
;
Integer
[
]
integers
=
new
Integer
[
]
{
1
,
2
}
;
TestProtos
.
AddRequestProto2
addRequest2
=
TestProtos
.
AddRequestProto2
.
newBuilder
(
)
.
addAllParams
(
Arrays
.
asList
(
integers
)
)
.
build
(
)
;
addResponse
=
proxy
.
add2
(
null
,
addRequest2
)
;
assertThat
(
addResponse
.
getResult
(
)
)
.
isEqualTo
(
3
)
;
boolean
caught
=
false
;
try
{
proxy
.
error
(
null
,
newEmptyRequest
(
)
)
;
}
catch
(
ServiceException
e
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Test
public
void
testErrorMsgForInsecureClient
(
)
throws
IOException
{
Server
server
;
TestRpcService
proxy
=
null
;
Configuration
serverConf
=
new
Configuration
(
conf
)
;
SecurityUtil
.
setAuthenticationMethod
(
AuthenticationMethod
.
KERBEROS
,
serverConf
)
;
UserGroupInformation
.
setConfiguration
(
serverConf
)
;
server
=
setupTestServer
(
serverConf
,
5
)
;
boolean
succeeded
=
false
;
try
{
UserGroupInformation
.
setConfiguration
(
conf
)
;
proxy
=
getClient
(
addr
,
conf
)
;
proxy
.
echo
(
null
,
newEchoRequest
(
)
)
;
}
catch
(
ServiceException
e
)
{
assertTrue
(
e
.
getCause
(
)
instanceof
RemoteException
)
;
RemoteException
re
=
(
RemoteException
)
e
.
getCause
(
)
;
assertTrue
(
re
.
unwrapRemoteException
(
)
instanceof
AccessControlException
)
;
succeeded
=
true
;
}
finally
{
stop
(
server
,
proxy
)
;
}
assertTrue
(
succeeded
)
;
conf
.
setInt
(
CommonConfigurationKeys
.
IPC_SERVER_RPC_READ_THREADS_KEY
,
2
)
;
UserGroupInformation
.
setConfiguration
(
serverConf
)
;
server
=
setupTestServer
(
serverConf
,
5
)
;
succeeded
=
false
;
proxy
=
null
;
try
{
UserGroupInformation
.
setConfiguration
(
conf
)
;
proxy
=
getClient
(
addr
,
conf
)
;
proxy
.
echo
(
null
,
newEchoRequest
(
)
)
;
}
catch
(
ServiceException
e
)
{
}
}
)
)
;
verify
(
spy
,
timeout
(
500
)
.
times
(
i
+
1
)
)
.
addInternal
(
any
(
)
,
eq
(
false
)
)
;
}
try
{
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
catch
(
ServiceException
e
)
{
RemoteException
re
=
(
RemoteException
)
e
.
getCause
(
)
;
IOException
unwrapExeption
=
re
.
unwrapRemoteException
(
)
;
if
(
unwrapExeption
instanceof
RetriableException
)
{
succeeded
=
true
;
}
else
{
lastException
=
unwrapExeption
;
}
}
}
finally
{
executorService
.
shutdown
(
)
;
stop
(
server
,
proxy
)
;
}
if
(
lastException
!=
null
)
{
}
)
)
;
verify
(
spy
,
timeout
(
500
)
.
times
(
i
+
1
)
)
.
addInternal
(
any
(
)
,
eq
(
false
)
)
;
}
try
{
Thread
.
sleep
(
5500
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
catch
(
ServiceException
e
)
{
RemoteException
re
=
(
RemoteException
)
e
.
getCause
(
)
;
IOException
unwrapExeption
=
re
.
unwrapRemoteException
(
)
;
if
(
unwrapExeption
instanceof
RetriableException
)
{
succeeded
=
true
;
}
else
{
lastException
=
unwrapExeption
;
}
}
}
finally
{
executorService
.
shutdown
(
)
;
stop
(
server
,
proxy
)
;
final
long
beginRawCallVolume
=
MetricsAsserts
.
getLongCounter
(
,
rb1
)
;
final
int
beginUniqueCaller
=
MetricsAsserts
.
getIntCounter
(
,
rb1
)
;
TestRpcService
proxy
=
getClient
(
addr
,
conf
)
;
try
{
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
GenericTestUtils
.
waitFor
(
(
)
->
{
MetricsRecordBuilder
rb2
=
getMetrics
(
+
ns
)
;
long
decayedCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
long
rawCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
int
uniqueCaller1
=
MetricsAsserts
.
getIntCounter
(
,
rb2
)
;
long
callVolumePriority0
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
long
callVolumePriority1
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
double
avgRespTimePriority0
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
double
avgRespTimePriority1
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
final
int
beginUniqueCaller
=
MetricsAsserts
.
getIntCounter
(
,
rb1
)
;
TestRpcService
proxy
=
getClient
(
addr
,
conf
)
;
try
{
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
GenericTestUtils
.
waitFor
(
(
)
->
{
MetricsRecordBuilder
rb2
=
getMetrics
(
+
ns
)
;
long
decayedCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
long
rawCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
int
uniqueCaller1
=
MetricsAsserts
.
getIntCounter
(
,
rb2
)
;
long
callVolumePriority0
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
long
callVolumePriority1
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
double
avgRespTimePriority0
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
double
avgRespTimePriority1
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
LOG
.
info
(
,
decayedCallVolume1
)
;
TestRpcService
proxy
=
getClient
(
addr
,
conf
)
;
try
{
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
GenericTestUtils
.
waitFor
(
(
)
->
{
MetricsRecordBuilder
rb2
=
getMetrics
(
+
ns
)
;
long
decayedCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
long
rawCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
int
uniqueCaller1
=
MetricsAsserts
.
getIntCounter
(
,
rb2
)
;
long
callVolumePriority0
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
long
callVolumePriority1
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
double
avgRespTimePriority0
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
double
avgRespTimePriority1
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
LOG
.
info
(
,
decayedCallVolume1
)
;
LOG
.
info
(
,
rawCallVolume1
)
;
try
{
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
GenericTestUtils
.
waitFor
(
(
)
->
{
MetricsRecordBuilder
rb2
=
getMetrics
(
+
ns
)
;
long
decayedCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
long
rawCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
int
uniqueCaller1
=
MetricsAsserts
.
getIntCounter
(
,
rb2
)
;
long
callVolumePriority0
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
long
callVolumePriority1
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
double
avgRespTimePriority0
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
double
avgRespTimePriority1
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
LOG
.
info
(
,
decayedCallVolume1
)
;
LOG
.
info
(
,
rawCallVolume1
)
;
LOG
.
info
(
,
uniqueCaller1
)
;
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
GenericTestUtils
.
waitFor
(
(
)
->
{
MetricsRecordBuilder
rb2
=
getMetrics
(
+
ns
)
;
long
decayedCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
long
rawCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
int
uniqueCaller1
=
MetricsAsserts
.
getIntCounter
(
,
rb2
)
;
long
callVolumePriority0
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
long
callVolumePriority1
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
double
avgRespTimePriority0
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
double
avgRespTimePriority1
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
LOG
.
info
(
,
decayedCallVolume1
)
;
LOG
.
info
(
,
rawCallVolume1
)
;
LOG
.
info
(
,
uniqueCaller1
)
;
LOG
.
info
(
,
callVolumePriority0
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
100
)
)
;
}
GenericTestUtils
.
waitFor
(
(
)
->
{
MetricsRecordBuilder
rb2
=
getMetrics
(
+
ns
)
;
long
decayedCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
long
rawCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
int
uniqueCaller1
=
MetricsAsserts
.
getIntCounter
(
,
rb2
)
;
long
callVolumePriority0
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
long
callVolumePriority1
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
double
avgRespTimePriority0
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
double
avgRespTimePriority1
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
LOG
.
info
(
,
decayedCallVolume1
)
;
LOG
.
info
(
,
rawCallVolume1
)
;
LOG
.
info
(
,
uniqueCaller1
)
;
LOG
.
info
(
,
callVolumePriority0
)
;
LOG
.
info
(
,
callVolumePriority1
)
;
}
GenericTestUtils
.
waitFor
(
(
)
->
{
MetricsRecordBuilder
rb2
=
getMetrics
(
+
ns
)
;
long
decayedCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
long
rawCallVolume1
=
MetricsAsserts
.
getLongCounter
(
,
rb2
)
;
int
uniqueCaller1
=
MetricsAsserts
.
getIntCounter
(
,
rb2
)
;
long
callVolumePriority0
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
long
callVolumePriority1
=
MetricsAsserts
.
getLongGauge
(
,
rb2
)
;
double
avgRespTimePriority0
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
double
avgRespTimePriority1
=
MetricsAsserts
.
getDoubleGauge
(
,
rb2
)
;
LOG
.
info
(
,
decayedCallVolume1
)
;
LOG
.
info
(
,
rawCallVolume1
)
;
LOG
.
info
(
,
uniqueCaller1
)
;
LOG
.
info
(
,
callVolumePriority0
)
;
LOG
.
info
(
,
callVolumePriority1
)
;
LOG
.
info
(
,
avgRespTimePriority0
)
;
proxy
=
getClient
(
addr
,
c
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
3000
)
)
;
fail
(
)
;
}
catch
(
ServiceException
e
)
{
assertTrue
(
e
.
getCause
(
)
instanceof
SocketTimeoutException
)
;
LOG
.
info
(
,
e
)
;
}
try
{
Configuration
c
=
new
Configuration
(
conf
)
;
c
.
setBoolean
(
CommonConfigurationKeys
.
IPC_CLIENT_PING_KEY
,
false
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_CLIENT_RPC_TIMEOUT_KEY
,
1000
)
;
proxy
=
getClient
(
addr
,
c
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
3000
)
)
;
fail
(
)
;
}
catch
(
ServiceException
e
)
{
assertTrue
(
e
.
getCause
(
)
instanceof
SocketTimeoutException
)
;
}
try
{
Configuration
c
=
new
Configuration
(
conf
)
;
c
.
setBoolean
(
CommonConfigurationKeys
.
IPC_CLIENT_PING_KEY
,
false
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_CLIENT_RPC_TIMEOUT_KEY
,
1000
)
;
proxy
=
getClient
(
addr
,
c
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
3000
)
)
;
fail
(
)
;
}
catch
(
ServiceException
e
)
{
assertTrue
(
e
.
getCause
(
)
instanceof
SocketTimeoutException
)
;
LOG
.
info
(
,
e
)
;
}
try
{
Configuration
c
=
new
Configuration
(
conf
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_CLIENT_RPC_TIMEOUT_KEY
,
-
1
)
;
proxy
=
getClient
(
addr
,
c
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
2000
)
)
;
LOG
.
info
(
,
e
)
;
}
try
{
Configuration
c
=
new
Configuration
(
conf
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_CLIENT_RPC_TIMEOUT_KEY
,
-
1
)
;
proxy
=
getClient
(
addr
,
c
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
2000
)
)
;
}
catch
(
ServiceException
e
)
{
LOG
.
info
(
,
e
)
;
fail
(
)
;
}
try
{
Configuration
c
=
new
Configuration
(
conf
)
;
c
.
setBoolean
(
CommonConfigurationKeys
.
IPC_CLIENT_PING_KEY
,
true
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_PING_INTERVAL_KEY
,
800
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_CLIENT_RPC_TIMEOUT_KEY
,
1000
)
;
proxy
=
getClient
(
addr
,
c
)
;
proxy
.
sleep
(
null
,
newSleepRequest
(
2000
)
)
;
}
catch
(
ServiceException
e
)
{
LOG
.
info
(
,
e
)
;
fail
(
)
;
}
try
{
Configuration
c
=
new
Configuration
(
conf
)
;
c
.
setBoolean
(
CommonConfigurationKeys
.
IPC_CLIENT_PING_KEY
,
true
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_PING_INTERVAL_KEY
,
800
)
;
c
.
setInt
(
CommonConfigurationKeys
.
IPC_CLIENT_RPC_TIMEOUT_KEY
,
1000
)
;
proxy
=
getClient
(
addr
,
c
)
;
try
{
proxy
.
sleep
(
null
,
newSleepRequest
(
1300
)
)
;
}
catch
(
ServiceException
e
)
{
LOG
.
info
(
,
e
)
;
fail
(
)
;
@
SuppressWarnings
(
)
IOException
rseFatal
=
new
RpcServerException
(
,
expectedIOE
)
{
@
Override
public
RpcStatusProto
getRpcStatusProto
(
)
{
return
RpcStatusProto
.
FATAL
;
}
}
;
try
{
RPC
.
Builder
builder
=
newServerBuilder
(
conf
)
.
setQueueSizePerHandler
(
1
)
.
setNumHandlers
(
1
)
.
setVerbose
(
true
)
;
server
=
setupTestServer
(
builder
)
;
Whitebox
.
setInternalState
(
server
,
,
FakeRequestClass
.
class
)
;
MutableCounterLong
authMetric
=
(
MutableCounterLong
)
Whitebox
.
getInternalState
(
server
.
getRpcMetrics
(
)
,
)
;
proxy
=
getClient
(
addr
,
conf
)
;
boolean
isDisconnected
=
true
;
Connection
lastConn
=
null
;
long
expectedAuths
=
0
;
for
(
int
i
=
0
;
i
<
128
;
i
++
)
{
String
reqName
=
+
i
+
;
return
null
;
}
}
)
)
;
}
while
(
server
.
getCallQueueLen
(
)
!=
1
||
countThreads
(
CallQueueManager
.
class
.
getName
(
)
)
!=
1
||
countThreads
(
PBServerImpl
.
class
.
getName
(
)
)
!=
1
)
{
Thread
.
sleep
(
100
)
;
}
}
finally
{
try
{
stop
(
server
,
proxy
)
;
assertEquals
(
,
numClients
,
res
.
size
(
)
)
;
for
(
Future
<
Void
>
f
:
res
)
{
try
{
f
.
get
(
)
;
fail
(
)
;
}
catch
(
ExecutionException
e
)
{
ServiceException
se
=
(
ServiceException
)
e
.
getCause
(
)
;
assertTrue
(
+
se
,
se
.
getCause
(
)
instanceof
IOException
)
;
for
(
int
i
=
0
;
i
<
futures
.
length
;
i
++
)
{
futures
[
i
]
=
executor
.
submit
(
new
Callable
<
Void
>
(
)
{
@
Override
public
Void
call
(
)
throws
Exception
{
String
expect
=
+
count
.
getAndIncrement
(
)
;
String
answer
=
convert
(
proxy
.
echoPostponed
(
null
,
newEchoRequest
(
expect
)
)
)
;
assertEquals
(
expect
,
answer
)
;
return
null
;
}
}
)
;
try
{
futures
[
i
]
.
get
(
100
,
TimeUnit
.
MILLISECONDS
)
;
}
catch
(
TimeoutException
te
)
{
continue
;
}
Assert
.
fail
(
+
i
+
)
;
}
proxy
.
sendPostponed
(
null
,
newEmptyRequest
(
)
)
;
for
(
int
i
=
0
;
i
<
futures
.
length
;
i
++
)
{
UserGroupInformation
.
setConfiguration
(
clientConf
)
;
final
UserGroupInformation
clientUgi
=
UserGroupInformation
.
createRemoteUser
(
)
;
clientUgi
.
setAuthenticationMethod
(
clientAuth
)
;
final
InetSocketAddress
addr
=
NetUtils
.
getConnectAddress
(
server
)
;
if
(
tokenType
!=
UseToken
.
NONE
)
{
TestTokenIdentifier
tokenId
=
new
TestTokenIdentifier
(
new
Text
(
clientUgi
.
getUserName
(
)
)
)
;
Token
<
TestTokenIdentifier
>
token
=
null
;
switch
(
tokenType
)
{
case
VALID
:
token
=
new
Token
<
>
(
tokenId
,
sm
)
;
SecurityUtil
.
setTokenService
(
token
,
addr
)
;
break
;
case
INVALID
:
token
=
new
Token
<
>
(
tokenId
.
getBytes
(
)
,
.
getBytes
(
)
,
tokenId
.
getKind
(
)
,
null
)
;
SecurityUtil
.
setTokenService
(
token
,
addr
)
;
break
;
case
OTHER
:
token
=
new
Token
<
>
(
)
;
private
String
logOut
(
String
message
,
Throwable
throwable
)
{
StringWriter
writer
=
new
StringWriter
(
)
;
Logger
logger
=
createLogger
(
writer
)
;
@
Test
public
void
testCommon
(
)
throws
Exception
{
String
filename
=
getTestFilename
(
)
;
new
ConfigBuilder
(
)
.
add
(
,
)
.
add
(
,
)
.
add
(
,
)
.
add
(
,
)
.
add
(
,
)
.
add
(
,
)
.
add
(
,
)
.
save
(
filename
)
;
MetricsConfig
mc
=
MetricsConfig
.
create
(
,
filename
)
;
private
void
checkMetricsRecords
(
List
<
MetricsRecord
>
recs
)
{
@
Test
public
void
testMutableRatesWithAggregationManyThreads
(
)
throws
InterruptedException
{
final
MutableRatesWithAggregation
rates
=
new
MutableRatesWithAggregation
(
)
;
final
int
n
=
10
;
long
[
]
opCount
=
new
long
[
n
]
;
double
[
]
opTotalTime
=
new
double
[
n
]
;
for
(
int
i
=
0
;
i
<
n
;
i
++
)
{
opCount
[
i
]
=
0
;
opTotalTime
[
i
]
=
0
;
rates
.
add
(
+
i
,
0
)
;
}
Thread
[
]
threads
=
new
Thread
[
n
]
;
final
CountDownLatch
firstAddsFinished
=
new
CountDownLatch
(
threads
.
length
)
;
final
CountDownLatch
firstSnapshotsFinished
=
new
CountDownLatch
(
1
)
;
final
CountDownLatch
secondAddsFinished
=
new
CountDownLatch
(
threads
.
length
)
;
final
CountDownLatch
secondSnapshotsFinished
=
new
CountDownLatch
(
1
)
;
long
seed
=
new
Random
(
)
.
nextLong
(
)
;
@
SuppressWarnings
(
)
@
Test
public
void
testGet
(
)
{
MetricsCache
cache
=
new
MetricsCache
(
)
;
assertNull
(
,
cache
.
get
(
,
Arrays
.
asList
(
makeTag
(
,
)
)
)
)
;
MetricsRecord
mr
=
makeRecord
(
,
Arrays
.
asList
(
makeTag
(
,
)
)
,
Arrays
.
asList
(
makeMetric
(
,
1
)
)
)
;
cache
.
update
(
mr
)
;
MetricsCache
.
Record
cr
=
cache
.
get
(
,
mr
.
tags
(
)
)
;
@
Test
public
void
testRDNS
(
)
throws
Exception
{
InetAddress
localhost
=
getLocalIPAddr
(
)
;
try
{
String
s
=
DNS
.
reverseDns
(
localhost
,
null
)
;
@
Test
public
void
testLocalhostResolves
(
)
throws
Exception
{
InetAddress
localhost
=
InetAddress
.
getByName
(
)
;
assertNotNull
(
,
localhost
)
;
private
IOException
verifyExceptionClass
(
IOException
e
,
Class
expectedClass
)
throws
Throwable
{
assertNotNull
(
,
e
)
;
IOException
wrapped
=
NetUtils
.
wrapException
(
,
DEST_PORT
,
,
LOCAL_PORT
,
e
)
;
out
.
write
(
byteWithHighBit
)
;
doIO
(
null
,
out
,
TIMEOUT
)
;
in
.
read
(
readBytes
)
;
assertTrue
(
Arrays
.
equals
(
writeBytes
,
readBytes
)
)
;
assertEquals
(
byteWithHighBit
&
0xff
,
in
.
read
(
)
)
;
doIO
(
in
,
null
,
TIMEOUT
)
;
(
(
SocketInputStream
)
in
)
.
setTimeout
(
TIMEOUT
*
2
)
;
doIO
(
in
,
null
,
TIMEOUT
*
2
)
;
(
(
SocketInputStream
)
in
)
.
setTimeout
(
0
)
;
TestingThread
thread
=
new
TestingThread
(
ctx
)
{
@
Override
public
void
doWork
(
)
throws
Exception
{
try
{
in
.
read
(
)
;
fail
(
)
;
}
catch
(
InterruptedIOException
ste
)
{
@
Test
public
void
testAddResolveNodes
(
)
throws
Throwable
{
StaticMapping
mapping
=
newInstance
(
)
;
StaticMapping
.
addNodeToRack
(
,
)
;
List
<
String
>
queryList
=
createQueryList
(
)
;
List
<
String
>
resolved
=
mapping
.
resolve
(
queryList
)
;
assertEquals
(
2
,
resolved
.
size
(
)
)
;
assertEquals
(
,
resolved
.
get
(
0
)
)
;
assertEquals
(
NetworkTopology
.
DEFAULT_RACK
,
resolved
.
get
(
1
)
)
;
Map
<
String
,
String
>
switchMap
=
mapping
.
getSwitchMap
(
)
;
String
topology
=
mapping
.
dumpTopology
(
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
StaticMapping
.
KEY_HADOOP_CONFIGURED_NODE_MAPPING
,
)
;
mapping
.
setConf
(
conf
)
;
assertSingleSwitch
(
mapping
)
;
List
<
String
>
l1
=
new
ArrayList
<
String
>
(
3
)
;
l1
.
add
(
)
;
l1
.
add
(
)
;
l1
.
add
(
)
;
List
<
String
>
resolved
=
mapping
.
resolve
(
l1
)
;
assertEquals
(
3
,
resolved
.
size
(
)
)
;
assertEquals
(
,
resolved
.
get
(
0
)
)
;
assertEquals
(
NetworkTopology
.
DEFAULT_RACK
,
resolved
.
get
(
1
)
)
;
assertEquals
(
,
resolved
.
get
(
2
)
)
;
Map
<
String
,
String
>
switchMap
=
mapping
.
getSwitchMap
(
)
;
String
topology
=
mapping
.
dumpTopology
(
)
;
@
Test
public
void
testCachingRelaysSingleSwitchQueries
(
)
throws
Throwable
{
StaticMapping
staticMapping
=
newInstance
(
null
)
;
assertSingleSwitch
(
staticMapping
)
;
CachedDNSToSwitchMapping
cachedMap
=
new
CachedDNSToSwitchMapping
(
staticMapping
)
;
@
Test
public
void
testCachingRelaysMultiSwitchQueries
(
)
throws
Throwable
{
StaticMapping
staticMapping
=
newInstance
(
)
;
assertMultiSwitch
(
staticMapping
)
;
CachedDNSToSwitchMapping
cachedMap
=
new
CachedDNSToSwitchMapping
(
staticMapping
)
;
@
Override
public
void
run
(
)
{
try
{
for
(
int
i
=
0
;
i
<
SOCKET_NUM
;
i
++
)
{
DomainSocket
pair
[
]
=
DomainSocket
.
socketpair
(
)
;
watcher
.
add
(
pair
[
1
]
,
new
DomainSocketWatcher
.
Handler
(
)
{
@
Override
public
boolean
handle
(
DomainSocket
sock
)
{
handled
.
incrementAndGet
(
)
;
return
true
;
}
}
)
;
lock
.
lock
(
)
;
try
{
pairs
.
add
(
pair
)
;
}
finally
{
lock
.
unlock
(
)
;
}
}
}
catch
(
Throwable
e
)
{
catch
(
Throwable
e
)
{
LOG
.
error
(
e
.
toString
(
)
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
final
Thread
removerThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
final
Random
random
=
new
Random
(
)
;
try
{
while
(
handled
.
get
(
)
!=
SOCKET_NUM
)
{
lock
.
lock
(
)
;
try
{
if
(
!
pairs
.
isEmpty
(
)
)
{
int
idx
=
random
.
nextInt
(
pairs
.
size
(
)
)
;
DomainSocket
pair
[
]
=
pairs
.
remove
(
idx
)
;
if
(
random
.
nextBoolean
(
)
)
{
try
{
for
(
int
i
=
0
;
i
<
SOCKET_NUM
;
i
++
)
{
DomainSocket
pair
[
]
=
DomainSocket
.
socketpair
(
)
;
watcher
.
add
(
pair
[
1
]
,
new
DomainSocketWatcher
.
Handler
(
)
{
@
Override
public
boolean
handle
(
DomainSocket
sock
)
{
handled
.
incrementAndGet
(
)
;
return
true
;
}
}
)
;
lock
.
lock
(
)
;
try
{
pairs
.
add
(
pair
)
;
}
finally
{
lock
.
unlock
(
)
;
}
TimeUnit
.
MILLISECONDS
.
sleep
(
1
)
;
}
}
catch
(
Throwable
e
)
{
LOG
.
error
(
e
.
toString
(
)
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
final
Thread
removerThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
final
Random
random
=
new
Random
(
)
;
try
{
while
(
handled
.
get
(
)
!=
SOCKET_NUM
)
{
lock
.
lock
(
)
;
try
{
if
(
!
pairs
.
isEmpty
(
)
)
{
int
idx
=
random
.
nextInt
(
pairs
.
size
(
)
)
;
DomainSocket
pair
[
]
=
pairs
.
remove
(
idx
)
;
if
(
random
.
nextBoolean
(
)
)
{
pair
[
0
]
.
close
(
)
;
private
void
configureSuperUserIPAddresses
(
Configuration
conf
,
String
superUserShortName
)
throws
IOException
{
ArrayList
<
String
>
ipList
=
new
ArrayList
<
>
(
)
;
Enumeration
<
NetworkInterface
>
netInterfaceList
=
NetworkInterface
.
getNetworkInterfaces
(
)
;
while
(
netInterfaceList
.
hasMoreElements
(
)
)
{
NetworkInterface
inf
=
netInterfaceList
.
nextElement
(
)
;
Enumeration
<
InetAddress
>
addrList
=
inf
.
getInetAddresses
(
)
;
while
(
addrList
.
hasMoreElements
(
)
)
{
InetAddress
addr
=
addrList
.
nextElement
(
)
;
ipList
.
add
(
addr
.
getHostAddress
(
)
)
;
}
}
StringBuilder
builder
=
new
StringBuilder
(
)
;
for
(
String
ip
:
ipList
)
{
builder
.
append
(
ip
)
;
builder
.
append
(
','
)
;
}
builder
.
append
(
)
;
builder
.
append
(
InetAddress
.
getLocalHost
(
)
.
getCanonicalHostName
(
)
)
;
@
Test
public
void
testGroupShell
(
)
throws
Exception
{
GenericTestUtils
.
setRootLogLevel
(
Level
.
DEBUG
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
CommonConfigurationKeys
.
HADOOP_SECURITY_GROUP_MAPPING
,
)
;
Groups
groups
=
new
Groups
(
conf
)
;
String
username
=
System
.
getProperty
(
)
;
List
<
String
>
groupList
=
groups
.
getGroups
(
username
)
;
@
Test
public
void
testNetgroupShell
(
)
throws
Exception
{
GenericTestUtils
.
setRootLogLevel
(
Level
.
DEBUG
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
CommonConfigurationKeys
.
HADOOP_SECURITY_GROUP_MAPPING
,
)
;
Groups
groups
=
new
Groups
(
conf
)
;
String
username
=
System
.
getProperty
(
)
;
List
<
String
>
groupList
=
groups
.
getGroups
(
username
)
;
@
Test
public
void
testGroupWithFallback
(
)
throws
Exception
{
LOG
.
info
(
+
+
)
;
GenericTestUtils
.
setRootLogLevel
(
Level
.
DEBUG
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
CommonConfigurationKeys
.
HADOOP_SECURITY_GROUP_MAPPING
,
)
;
Groups
groups
=
new
Groups
(
conf
)
;
String
username
=
System
.
getProperty
(
)
;
List
<
String
>
groupList
=
groups
.
getGroups
(
username
)
;
@
Test
public
void
testNetgroupWithFallback
(
)
throws
Exception
{
LOG
.
info
(
+
+
)
;
GenericTestUtils
.
setRootLogLevel
(
Level
.
DEBUG
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
CommonConfigurationKeys
.
HADOOP_SECURITY_GROUP_MAPPING
,
)
;
Groups
groups
=
new
Groups
(
conf
)
;
String
username
=
System
.
getProperty
(
)
;
List
<
String
>
groupList
=
groups
.
getGroups
(
username
)
;
@
Test
public
void
testGroupsCaching
(
)
throws
Exception
{
conf
.
setLong
(
CommonConfigurationKeys
.
HADOOP_SECURITY_GROUPS_NEGATIVE_CACHE_SECS
,
0
)
;
Groups
groups
=
new
Groups
(
conf
)
;
groups
.
cacheGroupsAdd
(
Arrays
.
asList
(
myGroups
)
)
;
groups
.
refresh
(
)
;
FakeGroupMapping
.
clearBlackList
(
)
;
FakeGroupMapping
.
addToBlackList
(
)
;
assertTrue
(
groups
.
getGroups
(
)
.
size
(
)
==
2
)
;
FakeGroupMapping
.
addToBlackList
(
)
;
assertTrue
(
groups
.
getGroups
(
)
.
size
(
)
==
2
)
;
try
{
TESTLOG
.
error
(
+
groups
.
getGroups
(
)
.
toString
(
)
)
;
fail
(
)
;
}
catch
(
IOException
ioe
)
{
if
(
!
ioe
.
getMessage
(
)
.
startsWith
(
)
)
{
void
kdiagFailure
(
String
category
,
String
...
args
)
throws
Exception
{
try
{
int
ex
=
exec
(
conf
,
args
)
;
@
Test
public
void
testFileOutput
(
)
throws
Throwable
{
File
f
=
new
File
(
)
;
kdiag
(
ARG_KEYLEN
,
KEYLEN
,
ARG_KEYTAB
,
keytab
.
getAbsolutePath
(
)
,
ARG_PRINCIPAL
,
,
ARG_OUTPUT
,
f
.
getAbsolutePath
(
)
)
;
private
void
dump
(
File
file
)
throws
IOException
{
try
(
FileInputStream
in
=
new
FileInputStream
(
file
)
)
{
for
(
String
line
:
IOUtils
.
readLines
(
in
)
)
{
void
kdiagFailure
(
String
category
,
String
...
args
)
throws
Exception
{
try
{
int
ex
=
exec
(
conf
,
args
)
;
try
(
Socket
ignored
=
serverSock
.
accept
(
)
)
{
finLatch
.
await
(
)
;
}
}
catch
(
Exception
e
)
{
e
.
printStackTrace
(
)
;
}
}
}
)
;
ldapServer
.
start
(
)
;
final
LdapGroupsMapping
mapping
=
new
LdapGroupsMapping
(
)
;
String
ldapUrl
=
+
serverSock
.
getLocalPort
(
)
;
final
Configuration
conf
=
getBaseConf
(
ldapUrl
,
null
)
;
conf
.
setInt
(
CONNECTION_TIMEOUT
,
connectionTimeoutMs
)
;
mapping
.
setConf
(
conf
)
;
try
{
mapping
.
doGetGroups
(
,
1
)
;
fail
(
)
;
}
catch
(
NamingException
ne
)
{
clientSock
.
getOutputStream
(
)
.
write
(
AUTHENTICATE_SUCCESS_MSG
)
;
finLatch
.
await
(
)
;
}
}
catch
(
Exception
e
)
{
e
.
printStackTrace
(
)
;
}
}
}
)
;
ldapServer
.
start
(
)
;
final
LdapGroupsMapping
mapping
=
new
LdapGroupsMapping
(
)
;
String
ldapUrl
=
+
serverSock
.
getLocalPort
(
)
;
final
Configuration
conf
=
getBaseConf
(
ldapUrl
,
null
)
;
conf
.
setInt
(
READ_TIMEOUT
,
readTimeoutMs
)
;
mapping
.
setConf
(
conf
)
;
try
{
mapping
.
doGetGroups
(
,
1
)
;
fail
(
)
;
}
catch
(
NamingException
ne
)
{
RetryPolicy
rp
=
RetryPolicies
.
exponentialBackoffRetry
(
Long
.
SIZE
-
2
,
1000
,
TimeUnit
.
MILLISECONDS
)
;
long
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
assertWithinBounds
(
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
value
(
)
,
lastRetry
,
reloginIntervalMs
,
currentTime
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
assertWithinBounds
(
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
value
(
)
,
lastRetry
,
reloginIntervalMs
,
currentTime
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
assertWithinBounds
(
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
value
(
)
,
lastRetry
,
reloginIntervalMs
,
currentTime
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
assertWithinBounds
(
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
value
(
)
,
lastRetry
,
reloginIntervalMs
,
currentTime
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
String
str
=
+
currentTime
+
+
lastRetry
;
assertWithinBounds
(
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
value
(
)
,
lastRetry
,
reloginIntervalMs
,
currentTime
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
assertWithinBounds
(
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
value
(
)
,
lastRetry
,
reloginIntervalMs
,
currentTime
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
assertWithinBounds
(
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
value
(
)
,
lastRetry
,
reloginIntervalMs
,
currentTime
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
String
str
=
+
currentTime
+
+
lastRetry
;
LOG
.
info
(
str
)
;
assertEquals
(
str
,
endTime
-
reloginIntervalMs
,
lastRetry
)
;
UserGroupInformation
.
metrics
.
getRenewalFailures
(
)
.
incr
(
)
;
lastRetry
=
UserGroupInformation
.
getNextTgtRenewalTime
(
endTime
,
currentTime
,
rp
)
;
str
=
+
currentTime
+
+
lastRetry
;
private
void
assertWithinBounds
(
final
int
numFailures
,
final
long
lastRetry
,
final
long
reloginIntervalMs
,
long
now
)
{
int
shift
=
numFailures
+
1
;
final
long
lower
=
now
+
reloginIntervalMs
*
(
long
)
(
(
1
<<
shift
)
*
0.5
)
;
final
long
upper
=
now
+
reloginIntervalMs
*
(
long
)
(
(
1
<<
shift
)
*
1.5
)
;
final
String
str
=
new
String
(
+
(
numFailures
+
1
)
+
+
now
+
+
lower
+
+
upper
+
+
lastRetry
)
;
SSLEngine
serverSSLEngine
=
serverSSLFactory
.
createSSLEngine
(
)
;
SSLEngine
clientSSLEngine
=
clientSSLFactory
.
createSSLEngine
(
)
;
clientSSLEngine
.
setEnabledCipherSuites
(
StringUtils
.
getTrimmedStrings
(
excludeCiphers
)
)
;
SSLSession
session
=
clientSSLEngine
.
getSession
(
)
;
int
appBufferMax
=
session
.
getApplicationBufferSize
(
)
;
int
netBufferMax
=
session
.
getPacketBufferSize
(
)
;
ByteBuffer
clientOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
clientIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
serverOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
serverIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
cTOs
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
ByteBuffer
sTOc
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
boolean
dataDone
=
false
;
try
{
while
(
!
isEngineClosed
(
clientSSLEngine
)
||
!
isEngineClosed
(
serverSSLEngine
)
)
{
SSLEngine
clientSSLEngine
=
clientSSLFactory
.
createSSLEngine
(
)
;
clientSSLEngine
.
setEnabledCipherSuites
(
StringUtils
.
getTrimmedStrings
(
excludeCiphers
)
)
;
SSLSession
session
=
clientSSLEngine
.
getSession
(
)
;
int
appBufferMax
=
session
.
getApplicationBufferSize
(
)
;
int
netBufferMax
=
session
.
getPacketBufferSize
(
)
;
ByteBuffer
clientOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
clientIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
serverOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
serverIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
cTOs
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
ByteBuffer
sTOc
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
boolean
dataDone
=
false
;
try
{
while
(
!
isEngineClosed
(
clientSSLEngine
)
||
!
isEngineClosed
(
serverSSLEngine
)
)
{
LOG
.
info
(
+
wrap
(
clientSSLEngine
,
clientOut
,
cTOs
)
)
;
int
appBufferMax
=
session
.
getApplicationBufferSize
(
)
;
int
netBufferMax
=
session
.
getPacketBufferSize
(
)
;
ByteBuffer
clientOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
clientIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
serverOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
serverIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
cTOs
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
ByteBuffer
sTOc
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
boolean
dataDone
=
false
;
try
{
while
(
!
isEngineClosed
(
clientSSLEngine
)
||
!
isEngineClosed
(
serverSSLEngine
)
)
{
LOG
.
info
(
+
wrap
(
clientSSLEngine
,
clientOut
,
cTOs
)
)
;
LOG
.
info
(
+
wrap
(
serverSSLEngine
,
serverOut
,
sTOc
)
)
;
cTOs
.
flip
(
)
;
sTOc
.
flip
(
)
;
int
netBufferMax
=
session
.
getPacketBufferSize
(
)
;
ByteBuffer
clientOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
clientIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
serverOut
=
ByteBuffer
.
wrap
(
.
getBytes
(
)
)
;
ByteBuffer
serverIn
=
ByteBuffer
.
allocate
(
appBufferMax
)
;
ByteBuffer
cTOs
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
ByteBuffer
sTOc
=
ByteBuffer
.
allocateDirect
(
netBufferMax
)
;
boolean
dataDone
=
false
;
try
{
while
(
!
isEngineClosed
(
clientSSLEngine
)
||
!
isEngineClosed
(
serverSSLEngine
)
)
{
LOG
.
info
(
+
wrap
(
clientSSLEngine
,
clientOut
,
cTOs
)
)
;
LOG
.
info
(
+
wrap
(
serverSSLEngine
,
serverOut
,
sTOc
)
)
;
cTOs
.
flip
(
)
;
sTOc
.
flip
(
)
;
LOG
.
info
(
+
unwrap
(
clientSSLEngine
,
sTOc
,
clientIn
)
)
;
protected
void
assertLaunchOutcome
(
int
expected
,
String
text
,
String
...
args
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
void
assertLaunchOutcome
(
int
expected
,
String
text
,
String
...
args
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
expected
)
;
for
(
String
arg
:
args
)
{
@
Override
public
Configuration
bindArgs
(
Configuration
config
,
List
<
String
>
args
)
throws
Exception
{
Assert
.
assertEquals
(
STATE
.
NOTINITED
,
getServiceState
(
)
)
;
for
(
String
arg
:
args
)
{
Preconditions
.
checkArgument
(
timeoutMillis
>=
0
,
)
;
Preconditions
.
checkNotNull
(
timeoutHandler
)
;
long
endTime
=
Time
.
now
(
)
+
timeoutMillis
;
Throwable
ex
=
null
;
boolean
running
=
true
;
int
iterations
=
0
;
while
(
running
)
{
iterations
++
;
try
{
if
(
check
.
call
(
)
)
{
return
iterations
;
}
ex
=
null
;
}
catch
(
InterruptedException
|
FailFastException
|
VirtualMachineError
e
)
{
throw
e
;
}
catch
(
Throwable
e
)
{
}
catch
(
InterruptedException
|
FailFastException
|
VirtualMachineError
e
)
{
throw
e
;
}
catch
(
Throwable
e
)
{
LOG
.
debug
(
,
iterations
,
e
)
;
ex
=
e
;
}
running
=
Time
.
now
(
)
<
endTime
;
if
(
running
)
{
int
sleeptime
=
retry
.
call
(
)
;
if
(
sleeptime
>=
0
)
{
Thread
.
sleep
(
sleeptime
)
;
}
else
{
running
=
false
;
}
}
}
Throwable
evaluate
;
try
{
evaluate
=
timeoutHandler
.
evaluate
(
timeoutMillis
,
ex
)
;
@
Test
(
timeout
=
10000
)
public
void
testLogCapturer
(
)
{
final
Logger
log
=
LoggerFactory
.
getLogger
(
TestGenericTestUtils
.
class
)
;
LogCapturer
logCapturer
=
LogCapturer
.
captureLogs
(
log
)
;
final
String
infoMessage
=
;
@
Test
(
timeout
=
10000
)
public
void
testLogCapturer
(
)
{
final
Logger
log
=
LoggerFactory
.
getLogger
(
TestGenericTestUtils
.
class
)
;
LogCapturer
logCapturer
=
LogCapturer
.
captureLogs
(
log
)
;
final
String
infoMessage
=
;
log
.
info
(
infoMessage
)
;
assertTrue
(
logCapturer
.
getOutput
(
)
.
endsWith
(
String
.
format
(
infoMessage
+
)
)
)
;
logCapturer
.
clearOutput
(
)
;
assertTrue
(
logCapturer
.
getOutput
(
)
.
isEmpty
(
)
)
;
logCapturer
.
stopCapturing
(
)
;
@
Test
(
timeout
=
10000
)
public
void
testLogCapturerSlf4jLogger
(
)
{
final
Logger
logger
=
LoggerFactory
.
getLogger
(
TestGenericTestUtils
.
class
)
;
LogCapturer
logCapturer
=
LogCapturer
.
captureLogs
(
logger
)
;
final
String
infoMessage
=
;
@
Test
(
timeout
=
10000
)
public
void
testLogCapturerSlf4jLogger
(
)
{
final
Logger
logger
=
LoggerFactory
.
getLogger
(
TestGenericTestUtils
.
class
)
;
LogCapturer
logCapturer
=
LogCapturer
.
captureLogs
(
logger
)
;
final
String
infoMessage
=
;
logger
.
info
(
infoMessage
)
;
assertTrue
(
logCapturer
.
getOutput
(
)
.
endsWith
(
String
.
format
(
infoMessage
+
)
)
)
;
logCapturer
.
clearOutput
(
)
;
assertTrue
(
logCapturer
.
getOutput
(
)
.
isEmpty
(
)
)
;
logCapturer
.
stopCapturing
(
)
;
@
SuppressWarnings
(
)
@
Test
public
void
testPrintLog4J
(
)
throws
Throwable
{
ByteArrayOutputStream
baos
=
new
ByteArrayOutputStream
(
)
;
PrintStream
out
=
new
PrintStream
(
baos
)
;
FindClass
.
setOutputStreams
(
out
,
System
.
err
)
;
run
(
FindClass
.
SUCCESS
,
FindClass
.
A_PRINTRESOURCE
,
LOG4J_PROPERTIES
)
;
out
.
flush
(
)
;
String
body
=
baos
.
toString
(
)
;
@
Test
(
timeout
=
60000
)
public
void
testAdditionsAndRemovals
(
)
{
IdentityHashStore
<
Key
,
Integer
>
store
=
new
IdentityHashStore
<
Key
,
Integer
>
(
0
)
;
final
int
NUM_KEYS
=
1000
;
@
POST
@
Path
(
KMSRESTConstants
.
KEYS_RESOURCE
)
@
Consumes
(
MediaType
.
APPLICATION_JSON
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
@
SuppressWarnings
(
)
public
Response
createKey
(
Map
jsonKey
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
final
String
name
=
(
String
)
jsonKey
.
get
(
KMSRESTConstants
.
NAME_FIELD
)
;
checkNotEmpty
(
name
,
KMSRESTConstants
.
NAME_FIELD
)
;
assertAccess
(
KMSACLs
.
Type
.
CREATE
,
user
,
KMSOp
.
CREATE_KEY
,
name
)
;
String
cipher
=
(
String
)
jsonKey
.
get
(
KMSRESTConstants
.
CIPHER_FIELD
)
;
final
String
material
;
material
=
(
String
)
jsonKey
.
get
(
KMSRESTConstants
.
MATERIAL_FIELD
)
;
int
length
=
(
jsonKey
.
containsKey
(
KMSRESTConstants
.
LENGTH_FIELD
)
)
?
(
Integer
)
jsonKey
.
get
(
KMSRESTConstants
.
LENGTH_FIELD
)
:
0
;
String
description
=
(
String
)
jsonKey
.
get
(
KMSRESTConstants
.
DESCRIPTION_FIELD
)
;
@
DELETE
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
)
public
Response
deleteKey
(
@
PathParam
(
)
final
String
name
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
assertAccess
(
KMSACLs
.
Type
.
DELETE
,
user
,
KMSOp
.
DELETE_KEY
,
name
)
;
checkNotEmpty
(
name
,
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
assertAccess
(
KMSACLs
.
Type
.
DELETE
,
user
,
KMSOp
.
DELETE_KEY
,
name
)
;
checkNotEmpty
(
name
,
)
;
LOG
.
debug
(
,
name
)
;
user
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
provider
.
deleteKey
(
name
)
;
provider
.
flush
(
)
;
return
null
;
}
}
)
;
kmsAudit
.
ok
(
user
,
KMSOp
.
DELETE_KEY
,
name
,
)
;
LOG
.
trace
(
)
;
return
Response
.
ok
(
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
@
POST
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
)
@
Consumes
(
MediaType
.
APPLICATION_JSON
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
rolloverKey
(
@
PathParam
(
)
final
String
name
,
Map
jsonMaterial
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
assertAccess
(
KMSACLs
.
Type
.
ROLLOVER
,
user
,
KMSOp
.
ROLL_NEW_VERSION
,
name
)
;
checkNotEmpty
(
name
,
)
;
if
(
material
!=
null
)
{
assertAccess
(
KMSACLs
.
Type
.
SET_KEY_MATERIAL
,
user
,
KMSOp
.
ROLL_NEW_VERSION
,
name
)
;
}
KeyProvider
.
KeyVersion
keyVersion
=
user
.
doAs
(
new
PrivilegedExceptionAction
<
KeyVersion
>
(
)
{
@
Override
public
KeyVersion
run
(
)
throws
Exception
{
KeyVersion
keyVersion
=
(
material
!=
null
)
?
provider
.
rollNewVersion
(
name
,
Base64
.
decodeBase64
(
material
)
)
:
provider
.
rollNewVersion
(
name
)
;
provider
.
flush
(
)
;
return
keyVersion
;
}
}
)
;
kmsAudit
.
ok
(
user
,
KMSOp
.
ROLL_NEW_VERSION
,
name
,
+
(
material
!=
null
)
+
+
keyVersion
.
getVersionName
(
)
)
;
if
(
!
KMSWebApp
.
getACLs
(
)
.
hasAccess
(
KMSACLs
.
Type
.
GET
,
user
)
)
{
keyVersion
=
removeKeyMaterial
(
keyVersion
)
;
}
Map
json
=
KMSUtil
.
toJSON
(
keyVersion
)
;
LOG
.
trace
(
)
;
return
Response
.
ok
(
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
json
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
@
POST
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
+
KMSRESTConstants
.
INVALIDATECACHE_RESOURCE
)
public
Response
invalidateCache
(
@
PathParam
(
)
final
String
name
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
checkNotEmpty
(
name
,
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
assertAccess
(
KMSACLs
.
Type
.
ROLLOVER
,
user
,
KMSOp
.
INVALIDATE_CACHE
,
name
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
checkNotEmpty
(
name
,
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
assertAccess
(
KMSACLs
.
Type
.
ROLLOVER
,
user
,
KMSOp
.
INVALIDATE_CACHE
,
name
)
;
LOG
.
debug
(
,
name
)
;
user
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
provider
.
invalidateCache
(
name
)
;
provider
.
flush
(
)
;
return
null
;
}
}
)
;
kmsAudit
.
ok
(
user
,
KMSOp
.
INVALIDATE_CACHE
,
name
,
)
;
LOG
.
trace
(
,
name
)
;
return
Response
.
ok
(
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
try
{
LOG
.
trace
(
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
final
String
[
]
keyNames
=
keyNamesList
.
toArray
(
new
String
[
keyNamesList
.
size
(
)
]
)
;
assertAccess
(
KMSACLs
.
Type
.
GET_METADATA
,
user
,
KMSOp
.
GET_KEYS_METADATA
)
;
KeyProvider
.
Metadata
[
]
keysMeta
=
user
.
doAs
(
new
PrivilegedExceptionAction
<
KeyProvider
.
Metadata
[
]
>
(
)
{
@
Override
public
KeyProvider
.
Metadata
[
]
run
(
)
throws
Exception
{
return
provider
.
getKeysMetadata
(
keyNames
)
;
}
}
)
;
Object
json
=
KMSServerJSONUtils
.
toJSON
(
keyNames
,
keysMeta
)
;
kmsAudit
.
ok
(
user
,
KMSOp
.
GET_KEYS_METADATA
,
)
;
LOG
.
trace
(
)
;
return
Response
.
ok
(
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
json
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
@
GET
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
)
public
Response
getKey
(
@
PathParam
(
)
String
name
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
@
GET
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
+
KMSRESTConstants
.
METADATA_SUB_RESOURCE
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
getMetadata
(
@
PathParam
(
)
final
String
name
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET_METADATA
,
user
,
KMSOp
.
GET_METADATA
,
name
)
;
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
KMSWebApp
.
getAdminCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET_METADATA
,
user
,
KMSOp
.
GET_METADATA
,
name
)
;
LOG
.
debug
(
,
name
)
;
KeyProvider
.
Metadata
metadata
=
user
.
doAs
(
new
PrivilegedExceptionAction
<
KeyProvider
.
Metadata
>
(
)
{
@
Override
public
KeyProvider
.
Metadata
run
(
)
throws
Exception
{
return
provider
.
getMetadata
(
name
)
;
}
}
)
;
Object
json
=
KMSServerJSONUtils
.
toJSON
(
name
,
metadata
)
;
kmsAudit
.
ok
(
user
,
KMSOp
.
GET_METADATA
,
name
,
)
;
LOG
.
trace
(
)
;
return
Response
.
ok
(
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
json
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
@
GET
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
+
KMSRESTConstants
.
CURRENT_VERSION_SUB_RESOURCE
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
getCurrentVersion
(
@
PathParam
(
)
final
String
name
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
KMSWebApp
.
getKeyCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET
,
user
,
KMSOp
.
GET_CURRENT_KEY
,
name
)
;
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
KMSWebApp
.
getKeyCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET
,
user
,
KMSOp
.
GET_CURRENT_KEY
,
name
)
;
LOG
.
debug
(
,
name
)
;
KeyVersion
keyVersion
=
user
.
doAs
(
new
PrivilegedExceptionAction
<
KeyVersion
>
(
)
{
@
Override
public
KeyVersion
run
(
)
throws
Exception
{
return
provider
.
getCurrentKey
(
name
)
;
}
}
)
;
Object
json
=
KMSUtil
.
toJSON
(
keyVersion
)
;
kmsAudit
.
ok
(
user
,
KMSOp
.
GET_CURRENT_KEY
,
name
,
)
;
LOG
.
trace
(
)
;
return
Response
.
ok
(
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
json
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
@
GET
@
Path
(
KMSRESTConstants
.
KEY_VERSION_RESOURCE
+
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
getKeyVersion
(
@
PathParam
(
)
final
String
versionName
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
versionName
,
)
;
KMSWebApp
.
getKeyCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET
,
user
,
KMSOp
.
GET_KEY_VERSION
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
versionName
,
)
;
KMSWebApp
.
getKeyCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET
,
user
,
KMSOp
.
GET_KEY_VERSION
)
;
LOG
.
debug
(
,
versionName
)
;
KeyVersion
keyVersion
=
user
.
doAs
(
new
PrivilegedExceptionAction
<
KeyVersion
>
(
)
{
@
Override
public
KeyVersion
run
(
)
throws
Exception
{
return
provider
.
getKeyVersion
(
versionName
)
;
}
}
)
;
if
(
keyVersion
!=
null
)
{
kmsAudit
.
ok
(
user
,
KMSOp
.
GET_KEY_VERSION
,
keyVersion
.
getName
(
)
,
)
;
}
Object
json
=
KMSUtil
.
toJSON
(
keyVersion
)
;
LOG
.
trace
(
)
;
return
Response
.
ok
(
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
json
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
@
SuppressWarnings
(
{
,
}
)
@
GET
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
+
KMSRESTConstants
.
EEK_SUB_RESOURCE
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
generateEncryptedKeys
(
@
PathParam
(
)
final
String
name
,
@
QueryParam
(
KMSRESTConstants
.
EEK_OP
)
String
edekOp
,
@
DefaultValue
(
)
@
QueryParam
(
KMSRESTConstants
.
EEK_NUM_KEYS
)
final
int
numKeys
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
checkNotNull
(
edekOp
,
)
;
@
SuppressWarnings
(
{
,
}
)
@
GET
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
+
KMSRESTConstants
.
EEK_SUB_RESOURCE
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
generateEncryptedKeys
(
@
PathParam
(
)
final
String
name
,
@
QueryParam
(
KMSRESTConstants
.
EEK_OP
)
String
edekOp
,
@
DefaultValue
(
)
@
QueryParam
(
KMSRESTConstants
.
EEK_NUM_KEYS
)
final
int
numKeys
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
checkNotNull
(
edekOp
,
)
;
LOG
.
debug
(
+
,
name
,
edekOp
)
;
Object
retJSON
;
if
(
edekOp
.
equals
(
KMSRESTConstants
.
EEK_GENERATE
)
)
{
LOG
.
debug
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GENERATE_EEK
,
user
,
KMSOp
.
GENERATE_EEK
,
name
)
;
final
List
<
EncryptedKeyVersion
>
retEdeks
=
new
LinkedList
<
EncryptedKeyVersion
>
(
)
;
try
{
user
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
LOG
.
debug
(
+
,
name
,
edekOp
)
;
Object
retJSON
;
if
(
edekOp
.
equals
(
KMSRESTConstants
.
EEK_GENERATE
)
)
{
LOG
.
debug
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GENERATE_EEK
,
user
,
KMSOp
.
GENERATE_EEK
,
name
)
;
final
List
<
EncryptedKeyVersion
>
retEdeks
=
new
LinkedList
<
EncryptedKeyVersion
>
(
)
;
try
{
user
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
LOG
.
debug
(
+
,
numKeys
)
;
for
(
int
i
=
0
;
i
<
numKeys
;
i
++
)
{
retEdeks
.
add
(
provider
.
generateEncryptedKey
(
name
)
)
;
}
return
null
;
}
}
)
;
}
catch
(
Exception
e
)
{
}
}
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
IOException
(
e
)
;
}
kmsAudit
.
ok
(
user
,
KMSOp
.
GENERATE_EEK
,
name
,
)
;
retJSON
=
new
ArrayList
(
)
;
for
(
EncryptedKeyVersion
edek
:
retEdeks
)
{
(
(
ArrayList
)
retJSON
)
.
add
(
KMSUtil
.
toJSON
(
edek
)
)
;
}
}
else
{
StringBuilder
error
;
error
=
new
StringBuilder
(
)
;
error
.
append
(
KMSRESTConstants
.
EEK_OP
)
;
error
.
append
(
)
;
error
.
append
(
KMSRESTConstants
.
EEK_GENERATE
)
;
error
.
append
(
)
;
throw
new
IOException
(
e
)
;
}
kmsAudit
.
ok
(
user
,
KMSOp
.
GENERATE_EEK
,
name
,
)
;
retJSON
=
new
ArrayList
(
)
;
for
(
EncryptedKeyVersion
edek
:
retEdeks
)
{
(
(
ArrayList
)
retJSON
)
.
add
(
KMSUtil
.
toJSON
(
edek
)
)
;
}
}
else
{
StringBuilder
error
;
error
=
new
StringBuilder
(
)
;
error
.
append
(
KMSRESTConstants
.
EEK_OP
)
;
error
.
append
(
)
;
error
.
append
(
KMSRESTConstants
.
EEK_GENERATE
)
;
error
.
append
(
)
;
error
.
append
(
KMSRESTConstants
.
EEK_DECRYPT
)
;
LOG
.
error
(
error
.
toString
(
)
)
;
throw
new
IllegalArgumentException
(
error
.
toString
(
)
)
;
}
assertAccess
(
KMSACLs
.
Type
.
GENERATE_EEK
,
user
,
KMSOp
.
REENCRYPT_EEK_BATCH
,
name
)
;
LOG
.
debug
(
,
jsonPayload
.
size
(
)
,
name
)
;
final
List
<
EncryptedKeyVersion
>
ekvs
=
KMSUtil
.
parseJSONEncKeyVersions
(
name
,
jsonPayload
)
;
Preconditions
.
checkArgument
(
ekvs
.
size
(
)
==
jsonPayload
.
size
(
)
,
)
;
for
(
EncryptedKeyVersion
ekv
:
ekvs
)
{
Preconditions
.
checkArgument
(
name
.
equals
(
ekv
.
getEncryptionKeyName
(
)
)
,
+
name
)
;
}
user
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
provider
.
reencryptEncryptedKeys
(
ekvs
)
;
return
null
;
}
}
)
;
List
retJSON
=
new
ArrayList
<
>
(
ekvs
.
size
(
)
)
;
for
(
EncryptedKeyVersion
ekv
:
ekvs
)
{
retJSON
.
add
(
KMSUtil
.
toJSON
(
ekv
)
)
;
}
kmsAudit
.
ok
(
user
,
KMSOp
.
REENCRYPT_EEK_BATCH
,
name
,
+
ekvs
.
size
(
)
+
)
;
@
GET
@
Path
(
KMSRESTConstants
.
KEY_RESOURCE
+
+
KMSRESTConstants
.
VERSIONS_SUB_RESOURCE
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
getKeyVersions
(
@
PathParam
(
)
final
String
name
)
throws
Exception
{
try
{
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
KMSWebApp
.
getKeyCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET
,
user
,
KMSOp
.
GET_KEY_VERSIONS
,
name
)
;
LOG
.
trace
(
)
;
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
checkNotEmpty
(
name
,
)
;
KMSWebApp
.
getKeyCallsMeter
(
)
.
mark
(
)
;
assertAccess
(
KMSACLs
.
Type
.
GET
,
user
,
KMSOp
.
GET_KEY_VERSIONS
,
name
)
;
LOG
.
debug
(
,
name
)
;
List
<
KeyVersion
>
ret
=
user
.
doAs
(
new
PrivilegedExceptionAction
<
List
<
KeyVersion
>>
(
)
{
@
Override
public
List
<
KeyVersion
>
run
(
)
throws
Exception
{
return
provider
.
getKeyVersions
(
name
)
;
}
}
)
;
Object
json
=
KMSServerJSONUtils
.
toJSON
(
ret
)
;
kmsAudit
.
ok
(
user
,
KMSOp
.
GET_KEY_VERSIONS
,
name
,
)
;
LOG
.
trace
(
)
;
return
Response
.
ok
(
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
json
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
private
void
setKMSACLs
(
Configuration
conf
)
{
Map
<
Type
,
AccessControlList
>
tempAcls
=
new
HashMap
<
Type
,
AccessControlList
>
(
)
;
Map
<
Type
,
AccessControlList
>
tempBlacklist
=
new
HashMap
<
Type
,
AccessControlList
>
(
)
;
for
(
Type
aclType
:
Type
.
values
(
)
)
{
String
aclStr
=
conf
.
get
(
aclType
.
getAclConfigKey
(
)
,
ACL_DEFAULT
)
;
tempAcls
.
put
(
aclType
,
new
AccessControlList
(
aclStr
)
)
;
String
blacklistStr
=
conf
.
get
(
aclType
.
getBlacklistConfigKey
(
)
)
;
if
(
blacklistStr
!=
null
)
{
tempBlacklist
.
put
(
aclType
,
new
AccessControlList
(
blacklistStr
)
)
;
}
else
{
String
aclStr
=
keyAcl
.
getValue
(
)
;
String
keyName
=
k
.
substring
(
keyNameStarts
,
keyNameEnds
)
;
String
keyOp
=
k
.
substring
(
keyNameEnds
+
1
)
;
KeyOpType
aclType
=
null
;
try
{
aclType
=
KeyOpType
.
valueOf
(
keyOp
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
warn
(
,
keyOp
)
;
}
if
(
aclType
!=
null
)
{
HashMap
<
KeyOpType
,
AccessControlList
>
aclMap
=
tempKeyAcls
.
get
(
keyName
)
;
if
(
aclMap
==
null
)
{
aclMap
=
new
HashMap
<
KeyOpType
,
AccessControlList
>
(
)
;
tempKeyAcls
.
put
(
keyName
,
aclMap
)
;
}
aclMap
.
put
(
aclType
,
new
AccessControlList
(
aclStr
)
)
;
public
boolean
hasAccess
(
Type
type
,
UserGroupInformation
ugi
)
{
boolean
access
=
acls
.
get
(
type
)
.
isUserAllowed
(
ugi
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
boolean
hasAccess
(
Type
type
,
UserGroupInformation
ugi
)
{
boolean
access
=
acls
.
get
(
type
)
.
isUserAllowed
(
ugi
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
ugi
.
getShortUserName
(
)
,
type
.
toString
(
)
,
acls
.
get
(
type
)
.
getAclString
(
)
)
;
}
if
(
access
)
{
AccessControlList
blacklist
=
blacklistedAcls
.
get
(
type
)
;
access
=
(
blacklist
==
null
)
||
!
blacklist
.
isUserInList
(
ugi
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
blacklist
==
null
)
{
LOG
.
debug
(
,
type
.
toString
(
)
)
;
}
else
if
(
access
)
{
LOG
.
debug
(
,
blacklist
.
getAclString
(
)
)
;
}
else
{
LOG
.
debug
(
,
blacklist
.
getAclString
(
)
)
;
}
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
boolean
checkKeyAccess
(
String
keyName
,
UserGroupInformation
ugi
,
KeyOpType
opType
)
{
Map
<
KeyOpType
,
AccessControlList
>
keyAcl
=
keyAcls
.
get
(
keyName
)
;
if
(
keyAcl
==
null
)
{
private
boolean
checkKeyAccess
(
Map
<
KeyOpType
,
AccessControlList
>
keyAcl
,
UserGroupInformation
ugi
,
KeyOpType
opType
)
{
AccessControlList
acl
=
keyAcl
.
get
(
opType
)
;
if
(
acl
==
null
)
{
@
Override
public
void
contextInitialized
(
ServletContextEvent
sce
)
{
try
{
kmsConf
=
KMSConfiguration
.
getKMSConf
(
)
;
UserGroupInformation
.
setConfiguration
(
kmsConf
)
;
LOG
.
info
(
)
;
LOG
.
info
(
,
System
.
getProperty
(
)
)
;
if
(
providerString
==
null
)
{
throw
new
IllegalStateException
(
)
;
}
KeyProvider
keyProvider
=
KeyProviderFactory
.
get
(
new
URI
(
providerString
)
,
kmsConf
)
;
Preconditions
.
checkNotNull
(
keyProvider
,
String
.
format
(
+
+
+
,
KMSConfiguration
.
KEY_PROVIDER_URI
,
providerString
)
)
;
if
(
kmsConf
.
getBoolean
(
KMSConfiguration
.
KEY_CACHE_ENABLE
,
KMSConfiguration
.
KEY_CACHE_ENABLE_DEFAULT
)
)
{
long
keyTimeOutMillis
=
kmsConf
.
getLong
(
KMSConfiguration
.
KEY_CACHE_TIMEOUT_KEY
,
KMSConfiguration
.
KEY_CACHE_TIMEOUT_DEFAULT
)
;
long
currKeyTimeOutMillis
=
kmsConf
.
getLong
(
KMSConfiguration
.
CURR_KEY_CACHE_TIMEOUT_KEY
,
KMSConfiguration
.
CURR_KEY_CACHE_TIMEOUT_DEFAULT
)
;
keyProvider
=
new
CachingKeyProvider
(
keyProvider
,
keyTimeOutMillis
,
currKeyTimeOutMillis
)
;
}
LOG
.
info
(
+
keyProvider
)
;
keyProviderCryptoExtension
=
KeyProviderCryptoExtension
.
createKeyProviderCryptoExtension
(
keyProvider
)
;
keyProviderCryptoExtension
=
new
EagerKeyGeneratorKeyProviderCryptoExtension
(
kmsConf
,
keyProviderCryptoExtension
)
;
if
(
kmsConf
.
getBoolean
(
KMSConfiguration
.
KEY_AUTHORIZATION_ENABLE
,
KMSConfiguration
.
KEY_AUTHORIZATION_ENABLE_DEFAULT
)
)
{
keyProviderCryptoExtension
=
new
KeyAuthorizationKeyProvider
(
keyProviderCryptoExtension
,
kmsAcls
)
;
}
LOG
.
info
(
+
keyProviderCryptoExtension
)
;
final
int
defaultBitlength
=
kmsConf
.
getInt
(
KeyProvider
.
DEFAULT_BITLENGTH_NAME
,
KeyProvider
.
DEFAULT_BITLENGTH
)
;
printUsage
(
)
;
}
String
type
=
args
.
get
(
1
)
;
boolean
runAll
=
OperationStatsBase
.
OP_ALL_NAME
.
equals
(
type
)
;
List
<
OperationStatsBase
>
ops
=
new
ArrayList
<
OperationStatsBase
>
(
)
;
OperationStatsBase
opStat
=
null
;
try
{
if
(
runAll
||
EncryptKeyStats
.
OP_ENCRYPT_KEY
.
equals
(
type
)
)
{
opStat
=
new
EncryptKeyStats
(
args
)
;
ops
.
add
(
opStat
)
;
}
if
(
runAll
||
DecryptKeyStats
.
OP_DECRYPT_KEY
.
equals
(
type
)
)
{
opStat
=
new
DecryptKeyStats
(
args
)
;
ops
.
add
(
opStat
)
;
}
if
(
ops
.
isEmpty
(
)
)
{
printUsage
(
)
;
}
for
(
OperationStatsBase
op
:
ops
)
{
File
confDir
=
getTestDir
(
)
;
conf
=
createBaseKMSConf
(
confDir
,
conf
)
;
conf
.
set
(
KeyAuthorizationKeyProvider
.
KEY_ACL
+
specialKey
+
,
)
;
writeConf
(
confDir
,
conf
)
;
runServer
(
null
,
null
,
confDir
,
new
KMSCallable
<
Void
>
(
)
{
@
Override
public
Void
call
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
URI
uri
=
createKMSUri
(
getKMSUrl
(
)
)
;
KeyProvider
kp
=
createProvider
(
uri
,
conf
)
;
Assert
.
assertTrue
(
kp
.
getKeys
(
)
.
isEmpty
(
)
)
;
Assert
.
assertEquals
(
0
,
kp
.
getKeysMetadata
(
)
.
length
)
;
KeyProvider
.
Options
options
=
new
KeyProvider
.
Options
(
conf
)
;
options
.
setCipher
(
)
;
options
.
setBitLength
(
128
)
;
options
.
setDescription
(
)
;
EncryptedKeyVersion
ek1
=
kpCE
.
generateEncryptedKey
(
currKv
.
getName
(
)
)
;
return
ek1
;
}
catch
(
Exception
ex
)
{
Assert
.
fail
(
ex
.
toString
(
)
)
;
}
return
null
;
}
}
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
KeyProvider
kp
=
createProvider
(
uri
,
conf
)
;
KeyProviderCryptoExtension
kpCE
=
KeyProviderCryptoExtension
.
createKeyProviderCryptoExtension
(
kp
)
;
kpCE
.
reencryptEncryptedKey
(
encKv
)
;
List
<
EncryptedKeyVersion
>
ekvs
=
new
ArrayList
<
>
(
2
)
;
ekvs
.
add
(
encKv
)
;
ekvs
.
add
(
encKv
)
;
kpCE
.
reencryptEncryptedKeys
(
ekvs
)
;
KeyProviderCryptoExtension
kpCE
=
KeyProviderCryptoExtension
.
createKeyProviderCryptoExtension
(
kp
)
;
kpCE
.
reencryptEncryptedKey
(
encKv
)
;
List
<
EncryptedKeyVersion
>
ekvs
=
new
ArrayList
<
>
(
2
)
;
ekvs
.
add
(
encKv
)
;
ekvs
.
add
(
encKv
)
;
kpCE
.
reencryptEncryptedKeys
(
ekvs
)
;
return
null
;
}
}
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
KeyProvider
kp
=
createProvider
(
uri
,
conf
)
;
try
{
KeyProviderCryptoExtension
kpCE
=
KeyProviderCryptoExtension
.
createKeyProviderCryptoExtension
(
kp
)
;
kpCE
.
decryptEncryptedKey
(
encKv
)
;
}
catch
(
Exception
ex
)
{
KeyProviderCryptoExtension
kpCE
=
KeyProviderCryptoExtension
.
createKeyProviderCryptoExtension
(
kp
)
;
kpCE
.
decryptEncryptedKey
(
encKv
)
;
}
catch
(
Exception
ex
)
{
Assert
.
fail
(
ex
.
getMessage
(
)
)
;
}
return
null
;
}
}
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
KeyProvider
kp
=
createProvider
(
uri
,
conf
)
;
try
{
kp
.
getKeys
(
)
;
}
catch
(
Exception
ex
)
{
Assert
.
fail
(
ex
.
getMessage
(
)
)
;
}
return
null
;
}
}
)
;
@
Override
public
Void
call
(
)
throws
Exception
{
final
Configuration
clientConf
=
new
Configuration
(
)
;
final
URI
uri
=
createKMSUri
(
getKMSUrl
(
)
)
;
clientConf
.
set
(
KeyProviderFactory
.
KEY_PROVIDER_PATH
,
createKMSUri
(
getKMSUrl
(
)
)
.
toString
(
)
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
KeyProvider
kp
=
createProvider
(
uri
,
clientConf
)
;
clientConf
.
unset
(
HADOOP_SECURITY_KEY_PROVIDER_PATH
)
;
KeyProviderDelegationTokenExtension
kpdte
=
KeyProviderDelegationTokenExtension
.
createKeyProviderDelegationTokenExtension
(
kp
)
;
final
Credentials
credentials
=
new
Credentials
(
)
;
final
Token
<
?
>
[
]
tokens
=
kpdte
.
addDelegationTokens
(
,
credentials
)
;
Text
tokenService
=
getTokenService
(
kp
)
;
Assert
.
assertEquals
(
1
,
credentials
.
getAllTokens
(
)
.
size
(
)
)
;
Assert
.
assertEquals
(
KMSDelegationToken
.
TOKEN_KIND
,
credentials
.
getToken
(
tokenService
)
.
getKind
(
)
)
;
for
(
Token
<
?
>
token
:
tokens
)
{
LOG
.
info
(
+
uri
+
+
token
)
;
try
{
token
.
renew
(
clientConf
)
;
Assert
.
fail
(
+
)
;
}
catch
(
Exception
e
)
{
final
DelegationTokenIdentifier
identifier
=
(
DelegationTokenIdentifier
)
token
.
decodeIdentifier
(
)
;
GenericTestUtils
.
assertExceptionContains
(
+
identifier
+
,
e
)
;
}
}
final
UserGroupInformation
otherUgi
;
if
(
kerb
)
{
UserGroupInformation
.
loginUserFromKeytab
(
,
keytab
.
getAbsolutePath
(
)
)
;
otherUgi
=
UserGroupInformation
.
getLoginUser
(
)
;
}
else
{
otherUgi
=
UserGroupInformation
.
createUserForTesting
(
,
new
String
[
]
{
}
)
;
UserGroupInformation
.
setLoginUser
(
otherUgi
)
;
}
try
{
Assert
.
fail
(
+
)
;
}
catch
(
Exception
e
)
{
final
DelegationTokenIdentifier
identifier
=
(
DelegationTokenIdentifier
)
token
.
decodeIdentifier
(
)
;
GenericTestUtils
.
assertExceptionContains
(
+
identifier
+
,
e
)
;
}
}
final
UserGroupInformation
otherUgi
;
if
(
kerb
)
{
UserGroupInformation
.
loginUserFromKeytab
(
,
keytab
.
getAbsolutePath
(
)
)
;
otherUgi
=
UserGroupInformation
.
getLoginUser
(
)
;
}
else
{
otherUgi
=
UserGroupInformation
.
createUserForTesting
(
,
new
String
[
]
{
}
)
;
UserGroupInformation
.
setLoginUser
(
otherUgi
)
;
}
try
{
otherUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
boolean
renewed
=
false
;
}
catch
(
Exception
e
)
{
final
DelegationTokenIdentifier
identifier
=
(
DelegationTokenIdentifier
)
token
.
decodeIdentifier
(
)
;
GenericTestUtils
.
assertExceptionContains
(
+
identifier
+
,
e
)
;
}
}
final
UserGroupInformation
otherUgi
;
if
(
kerb
)
{
UserGroupInformation
.
loginUserFromKeytab
(
,
keytab
.
getAbsolutePath
(
)
)
;
otherUgi
=
UserGroupInformation
.
getLoginUser
(
)
;
}
else
{
otherUgi
=
UserGroupInformation
.
createUserForTesting
(
,
new
String
[
]
{
}
)
;
UserGroupInformation
.
setLoginUser
(
otherUgi
)
;
}
try
{
otherUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
boolean
renewed
=
false
;
for
(
Token
<
?
>
token
:
tokens
)
{
GenericTestUtils
.
assertExceptionContains
(
+
identifier
+
,
e
)
;
}
}
final
UserGroupInformation
otherUgi
;
if
(
kerb
)
{
UserGroupInformation
.
loginUserFromKeytab
(
,
keytab
.
getAbsolutePath
(
)
)
;
otherUgi
=
UserGroupInformation
.
getLoginUser
(
)
;
}
else
{
otherUgi
=
UserGroupInformation
.
createUserForTesting
(
,
new
String
[
]
{
}
)
;
UserGroupInformation
.
setLoginUser
(
otherUgi
)
;
}
try
{
otherUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
boolean
renewed
=
false
;
for
(
Token
<
?
>
token
:
tokens
)
{
if
(
!
(
token
.
getKind
(
)
.
equals
(
KMSDelegationToken
.
TOKEN_KIND
)
)
)
{
LOG
.
info
(
,
token
)
;
otherUgi
=
UserGroupInformation
.
getLoginUser
(
)
;
}
else
{
otherUgi
=
UserGroupInformation
.
createUserForTesting
(
,
new
String
[
]
{
}
)
;
UserGroupInformation
.
setLoginUser
(
otherUgi
)
;
}
try
{
otherUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
boolean
renewed
=
false
;
for
(
Token
<
?
>
token
:
tokens
)
{
if
(
!
(
token
.
getKind
(
)
.
equals
(
KMSDelegationToken
.
TOKEN_KIND
)
)
)
{
LOG
.
info
(
,
token
)
;
continue
;
}
LOG
.
info
(
+
uri
+
+
token
)
;
long
tokenLife
=
token
.
renew
(
clientConf
)
;
LOG
.
info
(
,
token
.
getKind
(
)
,
tokenLife
)
;
otherUgi
=
UserGroupInformation
.
createUserForTesting
(
,
new
String
[
]
{
}
)
;
UserGroupInformation
.
setLoginUser
(
otherUgi
)
;
}
try
{
otherUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
boolean
renewed
=
false
;
for
(
Token
<
?
>
token
:
tokens
)
{
if
(
!
(
token
.
getKind
(
)
.
equals
(
KMSDelegationToken
.
TOKEN_KIND
)
)
)
{
LOG
.
info
(
,
token
)
;
continue
;
}
LOG
.
info
(
+
uri
+
+
token
)
;
long
tokenLife
=
token
.
renew
(
clientConf
)
;
LOG
.
info
(
,
token
.
getKind
(
)
,
tokenLife
)
;
Thread
.
sleep
(
100
)
;
long
newTokenLife
=
token
.
renew
(
clientConf
)
;
UserGroupInformation
.
setLoginUser
(
otherUgi
)
;
}
try
{
otherUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
boolean
renewed
=
false
;
for
(
Token
<
?
>
token
:
tokens
)
{
if
(
!
(
token
.
getKind
(
)
.
equals
(
KMSDelegationToken
.
TOKEN_KIND
)
)
)
{
LOG
.
info
(
,
token
)
;
continue
;
}
LOG
.
info
(
+
uri
+
+
token
)
;
long
tokenLife
=
token
.
renew
(
clientConf
)
;
LOG
.
info
(
,
token
.
getKind
(
)
,
tokenLife
)
;
Thread
.
sleep
(
100
)
;
long
newTokenLife
=
token
.
renew
(
clientConf
)
;
LOG
.
info
(
,
token
.
getKind
(
)
,
newTokenLife
)
;
Token
<
?
>
token
=
UserGroupInformation
.
getCurrentUser
(
)
.
getCredentials
(
)
.
getToken
(
tokenService
)
;
Assert
.
assertNotNull
(
token
)
;
job1Token
.
add
(
token
)
;
ByteArrayInputStream
buf
=
new
ByteArrayInputStream
(
token
.
getIdentifier
(
)
)
;
DataInputStream
dis
=
new
DataInputStream
(
buf
)
;
DelegationTokenIdentifier
id
=
new
DelegationTokenIdentifier
(
token
.
getKind
(
)
)
;
id
.
readFields
(
dis
)
;
dis
.
close
(
)
;
final
long
maxTime
=
id
.
getMaxDate
(
)
;
Thread
.
sleep
(
5100
)
;
Assert
.
assertTrue
(
+
maxTime
+
,
maxTime
>
0
&&
maxTime
<
Time
.
now
(
)
)
;
try
{
kp
.
getKeys
(
)
;
Assert
.
fail
(
)
;
}
catch
(
Exception
e
)
{
final
String
lbUri
=
generateLoadBalancingKeyProviderUriString
(
)
;
final
LoadBalancingKMSClientProvider
lbkp
=
createHAProvider
(
URI
.
create
(
lbUri
)
,
uris
,
conf
)
;
conf
.
unset
(
HADOOP_SECURITY_KEY_PROVIDER_PATH
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
KeyProviderDelegationTokenExtension
kpdte
=
KeyProviderDelegationTokenExtension
.
createKeyProviderDelegationTokenExtension
(
lbkp
)
;
kpdte
.
addDelegationTokens
(
,
credentials
)
;
return
null
;
}
}
)
;
nonKerberosUgi
.
addCredentials
(
credentials
)
;
nonKerberosUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
int
i
=
0
;
for
(
KMSClientProvider
provider
:
lbkp
.
getProviders
(
)
)
{
final
String
key
=
+
i
++
;
nonKerberosUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
int
i
=
0
;
for
(
KMSClientProvider
provider
:
lbkp
.
getProviders
(
)
)
{
final
String
key
=
+
i
++
;
LOG
.
info
(
,
provider
,
key
)
;
provider
.
createKey
(
key
,
new
KeyProvider
.
Options
(
conf
)
)
;
}
return
null
;
}
}
)
;
final
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
=
credentials
.
getAllTokens
(
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
assertEquals
(
1
,
tokens
.
size
(
)
)
;
Token
token
=
tokens
.
iterator
(
)
.
next
(
)
;
assertEquals
(
KMSDelegationToken
.
TOKEN_KIND
,
token
.
getKind
(
)
)
;
for
(
KMSClientProvider
provider
:
lbkp
.
getProviders
(
)
)
{
final
String
key
=
+
i
++
;
LOG
.
info
(
,
provider
,
key
)
;
provider
.
createKey
(
key
,
new
KeyProvider
.
Options
(
conf
)
)
;
}
return
null
;
}
}
)
;
final
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
=
credentials
.
getAllTokens
(
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
assertEquals
(
1
,
tokens
.
size
(
)
)
;
Token
token
=
tokens
.
iterator
(
)
.
next
(
)
;
assertEquals
(
KMSDelegationToken
.
TOKEN_KIND
,
token
.
getKind
(
)
)
;
LOG
.
info
(
,
token
)
;
final
long
tokenLife
=
token
.
renew
(
conf
)
;
LOG
.
info
(
,
token
,
tokenLife
)
;
LOG
.
info
(
,
provider
,
key
)
;
provider
.
createKey
(
key
,
new
KeyProvider
.
Options
(
conf
)
)
;
}
return
null
;
}
}
)
;
final
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
=
credentials
.
getAllTokens
(
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
assertEquals
(
1
,
tokens
.
size
(
)
)
;
Token
token
=
tokens
.
iterator
(
)
.
next
(
)
;
assertEquals
(
KMSDelegationToken
.
TOKEN_KIND
,
token
.
getKind
(
)
)
;
LOG
.
info
(
,
token
)
;
final
long
tokenLife
=
token
.
renew
(
conf
)
;
LOG
.
info
(
,
token
,
tokenLife
)
;
Thread
.
sleep
(
10
)
;
final
long
newTokenLife
=
token
.
renew
(
conf
)
;
}
return
null
;
}
}
)
;
final
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
=
credentials
.
getAllTokens
(
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
assertEquals
(
1
,
tokens
.
size
(
)
)
;
Token
token
=
tokens
.
iterator
(
)
.
next
(
)
;
assertEquals
(
KMSDelegationToken
.
TOKEN_KIND
,
token
.
getKind
(
)
)
;
LOG
.
info
(
,
token
)
;
final
long
tokenLife
=
token
.
renew
(
conf
)
;
LOG
.
info
(
,
token
,
tokenLife
)
;
Thread
.
sleep
(
10
)
;
final
long
newTokenLife
=
token
.
renew
(
conf
)
;
LOG
.
info
(
,
token
,
newTokenLife
)
;
assertTrue
(
newTokenLife
>
tokenLife
)
;
}
}
)
;
final
Credentials
newCredentials
=
new
Credentials
(
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
KeyProviderDelegationTokenExtension
kpdte
=
KeyProviderDelegationTokenExtension
.
createKeyProviderDelegationTokenExtension
(
lbkp
)
;
kpdte
.
addDelegationTokens
(
,
newCredentials
)
;
return
null
;
}
}
)
;
doAs
(
,
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
KMSClientProvider
kp1
=
lbkp
.
getProviders
(
)
[
0
]
;
URL
[
]
urls
=
getKMSHAUrl
(
)
;
final
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
=
newCredentials
.
getAllTokens
(
)
;
assertEquals
(
1
,
tokens
.
size
(
)
)
;
Token
token
=
tokens
.
iterator
(
)
.
next
(
)
;
@
Test
public
void
testKMSJMX
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
final
File
confDir
=
getTestDir
(
)
;
conf
=
createBaseKMSConf
(
confDir
,
conf
)
;
final
String
processName
=
;
conf
.
set
(
KMSConfiguration
.
METRICS_PROCESS_NAME_KEY
,
processName
)
;
writeConf
(
confDir
,
conf
)
;
runServer
(
null
,
null
,
confDir
,
new
KMSCallable
<
Void
>
(
)
{
@
Override
public
Void
call
(
)
throws
Exception
{
final
URL
jmxUrl
=
new
URL
(
getKMSUrl
(
)
+
+
processName
+
)
;
public
synchronized
void
createPrincipal
(
File
keytabFile
,
String
...
principals
)
throws
Exception
{
simpleKdc
.
createPrincipals
(
principals
)
;
if
(
keytabFile
.
exists
(
)
&&
!
keytabFile
.
delete
(
)
)
{
private
static
Match
getMatch
(
String
line
)
{
String
[
]
parts
=
line
.
split
(
)
;
final
String
host
;
AccessPrivilege
privilege
=
AccessPrivilege
.
READ_ONLY
;
switch
(
parts
.
length
)
{
case
1
:
host
=
StringUtils
.
toLowerCase
(
parts
[
0
]
)
.
trim
(
)
;
break
;
case
2
:
host
=
StringUtils
.
toLowerCase
(
parts
[
0
]
)
.
trim
(
)
;
String
option
=
parts
[
1
]
.
trim
(
)
;
if
(
.
equalsIgnoreCase
(
option
)
)
{
privilege
=
AccessPrivilege
.
READ_WRITE
;
}
break
;
default
:
throw
new
IllegalArgumentException
(
+
line
+
)
;
}
if
(
host
.
equals
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
switch
(
parts
.
length
)
{
case
1
:
host
=
StringUtils
.
toLowerCase
(
parts
[
0
]
)
.
trim
(
)
;
break
;
case
2
:
host
=
StringUtils
.
toLowerCase
(
parts
[
0
]
)
.
trim
(
)
;
String
option
=
parts
[
1
]
.
trim
(
)
;
if
(
.
equalsIgnoreCase
(
option
)
)
{
privilege
=
AccessPrivilege
.
READ_WRITE
;
}
break
;
default
:
throw
new
IllegalArgumentException
(
+
line
+
)
;
}
if
(
host
.
equals
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
AnonymousMatch
(
privilege
)
;
}
else
if
(
CIDR_FORMAT_SHORT
.
matcher
(
host
)
.
matches
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
option
=
parts
[
1
]
.
trim
(
)
;
if
(
.
equalsIgnoreCase
(
option
)
)
{
privilege
=
AccessPrivilege
.
READ_WRITE
;
}
break
;
default
:
throw
new
IllegalArgumentException
(
+
line
+
)
;
}
if
(
host
.
equals
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
AnonymousMatch
(
privilege
)
;
}
else
if
(
CIDR_FORMAT_SHORT
.
matcher
(
host
)
.
matches
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
CIDRMatch
(
privilege
,
new
SubnetUtils
(
host
)
.
getInfo
(
)
)
;
}
else
if
(
CIDR_FORMAT_LONG
.
matcher
(
host
)
.
matches
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
default
:
throw
new
IllegalArgumentException
(
+
line
+
)
;
}
if
(
host
.
equals
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
AnonymousMatch
(
privilege
)
;
}
else
if
(
CIDR_FORMAT_SHORT
.
matcher
(
host
)
.
matches
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
CIDRMatch
(
privilege
,
new
SubnetUtils
(
host
)
.
getInfo
(
)
)
;
}
else
if
(
CIDR_FORMAT_LONG
.
matcher
(
host
)
.
matches
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
String
[
]
pair
=
host
.
split
(
)
;
return
new
CIDRMatch
(
privilege
,
new
SubnetUtils
(
pair
[
0
]
,
pair
[
1
]
)
.
getInfo
(
)
)
;
}
else
if
(
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
AnonymousMatch
(
privilege
)
;
}
else
if
(
CIDR_FORMAT_SHORT
.
matcher
(
host
)
.
matches
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
CIDRMatch
(
privilege
,
new
SubnetUtils
(
host
)
.
getInfo
(
)
)
;
}
else
if
(
CIDR_FORMAT_LONG
.
matcher
(
host
)
.
matches
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
String
[
]
pair
=
host
.
split
(
)
;
return
new
CIDRMatch
(
privilege
,
new
SubnetUtils
(
pair
[
0
]
,
pair
[
1
]
)
.
getInfo
(
)
)
;
}
else
if
(
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
||
host
.
contains
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
privilege
)
;
}
return
new
RegexMatch
(
privilege
,
host
)
;
public
void
register
(
int
transport
,
int
boundPort
)
{
if
(
boundPort
!=
port
)
{
public
void
unregister
(
int
transport
,
int
boundPort
)
{
if
(
boundPort
!=
port
)
{
factory
=
new
NioServerSocketChannelFactory
(
Executors
.
newCachedThreadPool
(
)
,
Executors
.
newCachedThreadPool
(
)
)
;
}
else
{
factory
=
new
NioServerSocketChannelFactory
(
Executors
.
newCachedThreadPool
(
)
,
Executors
.
newCachedThreadPool
(
)
,
workerCount
)
;
}
server
=
new
ServerBootstrap
(
factory
)
;
server
.
setPipelineFactory
(
new
ChannelPipelineFactory
(
)
{
@
Override
public
ChannelPipeline
getPipeline
(
)
throws
Exception
{
return
Channels
.
pipeline
(
RpcUtil
.
constructRpcFrameDecoder
(
)
,
RpcUtil
.
STAGE_RPC_MESSAGE_PARSER
,
rpcProgram
,
RpcUtil
.
STAGE_RPC_TCP_RESPONSE
)
;
}
}
)
;
server
.
setOption
(
,
true
)
;
server
.
setOption
(
,
true
)
;
server
.
setOption
(
,
true
)
;
server
.
setOption
(
,
true
)
;
ch
=
server
.
bind
(
new
InetSocketAddress
(
port
)
)
;
InetSocketAddress
socketAddr
=
(
InetSocketAddress
)
ch
.
getLocalAddress
(
)
;
boundPort
=
socketAddr
.
getPort
(
)
;
public
void
run
(
)
{
DatagramChannelFactory
f
=
new
NioDatagramChannelFactory
(
Executors
.
newCachedThreadPool
(
)
,
workerCount
)
;
server
=
new
ConnectionlessBootstrap
(
f
)
;
server
.
setPipeline
(
Channels
.
pipeline
(
RpcUtil
.
STAGE_RPC_MESSAGE_PARSER
,
rpcProgram
,
RpcUtil
.
STAGE_RPC_UDP_RESPONSE
)
)
;
server
.
setOption
(
,
)
;
server
.
setOption
(
,
SEND_BUFFER_SIZE
)
;
server
.
setOption
(
,
RECEIVE_BUFFER_SIZE
)
;
server
.
setOption
(
,
true
)
;
ch
=
server
.
bind
(
new
InetSocketAddress
(
port
)
)
;
InetSocketAddress
socketAddr
=
(
InetSocketAddress
)
ch
.
getLocalAddress
(
)
;
boundPort
=
socketAddr
.
getPort
(
)
;
tcpServer
.
setPipelineFactory
(
new
ChannelPipelineFactory
(
)
{
private
final
HashedWheelTimer
timer
=
new
HashedWheelTimer
(
)
;
private
final
IdleStateHandler
idleStateHandler
=
new
IdleStateHandler
(
timer
,
0
,
0
,
idleTimeMilliSeconds
,
TimeUnit
.
MILLISECONDS
)
;
@
Override
public
ChannelPipeline
getPipeline
(
)
throws
Exception
{
return
Channels
.
pipeline
(
RpcUtil
.
constructRpcFrameDecoder
(
)
,
RpcUtil
.
STAGE_RPC_MESSAGE_PARSER
,
idleStateHandler
,
handler
,
RpcUtil
.
STAGE_RPC_TCP_RESPONSE
)
;
}
}
)
;
tcpServer
.
setOption
(
,
true
)
;
tcpServer
.
setOption
(
,
true
)
;
udpServer
=
new
ConnectionlessBootstrap
(
new
NioDatagramChannelFactory
(
Executors
.
newCachedThreadPool
(
)
)
)
;
udpServer
.
setPipeline
(
Channels
.
pipeline
(
RpcUtil
.
STAGE_RPC_MESSAGE_PARSER
,
handler
,
RpcUtil
.
STAGE_RPC_UDP_RESPONSE
)
)
;
udpServer
.
setOption
(
,
true
)
;
tcpChannel
=
tcpServer
.
bind
(
tcpAddress
)
;
udpChannel
=
udpServer
.
bind
(
udpAddress
)
;
allChannels
.
add
(
tcpChannel
)
;
allChannels
.
add
(
udpChannel
)
;
private
XDR
set
(
int
xid
,
XDR
in
,
XDR
out
)
{
PortmapMapping
mapping
=
PortmapRequest
.
mapping
(
in
)
;
String
key
=
PortmapMapping
.
key
(
mapping
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
XDR
getport
(
int
xid
,
XDR
in
,
XDR
out
)
{
PortmapMapping
mapping
=
PortmapRequest
.
mapping
(
in
)
;
String
key
=
PortmapMapping
.
key
(
mapping
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
XDR
in
=
new
XDR
(
info
.
data
(
)
.
toByteBuffer
(
)
.
asReadOnlyBuffer
(
)
,
XDR
.
State
.
READING
)
;
XDR
out
=
new
XDR
(
)
;
if
(
portmapProc
==
PMAPPROC_NULL
)
{
out
=
nullOp
(
xid
,
in
,
out
)
;
}
else
if
(
portmapProc
==
PMAPPROC_SET
)
{
out
=
set
(
xid
,
in
,
out
)
;
}
else
if
(
portmapProc
==
PMAPPROC_UNSET
)
{
out
=
unset
(
xid
,
in
,
out
)
;
}
else
if
(
portmapProc
==
PMAPPROC_DUMP
)
{
out
=
dump
(
xid
,
in
,
out
)
;
}
else
if
(
portmapProc
==
PMAPPROC_GETPORT
)
{
out
=
getport
(
xid
,
in
,
out
)
;
}
else
if
(
portmapProc
==
PMAPPROC_GETVERSADDR
)
{
out
=
getport
(
xid
,
in
,
out
)
;
}
else
{
String
analyzeException
(
String
operation
,
Exception
e
,
List
<
String
>
argsList
)
{
String
pathArg
=
!
argsList
.
isEmpty
(
)
?
argsList
.
get
(
1
)
:
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
Map
<
String
,
ServiceRecord
>
results
=
new
HashMap
<
String
,
ServiceRecord
>
(
stats
.
size
(
)
)
;
for
(
RegistryPathStatus
stat
:
stats
)
{
if
(
stat
.
size
>
ServiceRecord
.
RECORD_TYPE
.
length
(
)
)
{
String
path
=
join
(
parentpath
,
stat
.
path
)
;
try
{
ServiceRecord
serviceRecord
=
operations
.
resolve
(
path
)
;
results
.
put
(
path
,
serviceRecord
)
;
}
catch
(
EOFException
ignored
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
path
)
;
}
}
catch
(
InvalidRecordException
record
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
path
)
;
}
}
catch
(
NoRecordException
record
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
registryRoot
=
conf
.
getTrimmed
(
KEY_REGISTRY_ZK_ROOT
,
DEFAULT_ZK_REGISTRY_ROOT
)
;
addService
(
registrySecurity
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
Configuration
conf
=
getConfig
(
)
;
createEnsembleProvider
(
)
;
int
sessionTimeout
=
conf
.
getInt
(
KEY_REGISTRY_ZK_SESSION_TIMEOUT
,
DEFAULT_ZK_SESSION_TIMEOUT
)
;
int
connectionTimeout
=
conf
.
getInt
(
KEY_REGISTRY_ZK_CONNECTION_TIMEOUT
,
DEFAULT_ZK_CONNECTION_TIMEOUT
)
;
int
retryTimes
=
conf
.
getInt
(
KEY_REGISTRY_ZK_RETRY_TIMES
,
DEFAULT_ZK_RETRY_TIMES
)
;
int
retryInterval
=
conf
.
getInt
(
KEY_REGISTRY_ZK_RETRY_INTERVAL
,
DEFAULT_ZK_RETRY_INTERVAL
)
;
int
retryCeiling
=
conf
.
getInt
(
KEY_REGISTRY_ZK_RETRY_CEILING
,
DEFAULT_ZK_RETRY_CEILING
)
;
LOG
.
info
(
,
connectionDescription
)
;
CuratorFramework
framework
;
synchronized
(
CuratorService
.
class
)
{
CuratorFrameworkFactory
.
Builder
builder
=
CuratorFrameworkFactory
.
builder
(
)
;
builder
.
ensembleProvider
(
ensembleProvider
)
.
connectionTimeoutMs
(
connectionTimeout
)
.
sessionTimeoutMs
(
sessionTimeout
)
.
retryPolicy
(
new
BoundedExponentialBackoffRetry
(
retryInterval
,
retryCeiling
,
retryTimes
)
)
;
registrySecurity
.
applySecurityEnvironment
(
builder
)
;
securityConnectionDiagnostics
=
buildSecurityDiagnostics
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
Stat
zkStat
(
String
path
)
throws
IOException
{
checkServiceLive
(
)
;
String
fullpath
=
createFullPath
(
path
)
;
Stat
stat
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
List
<
ACL
>
zkGetACLS
(
String
path
)
throws
IOException
{
checkServiceLive
(
)
;
String
fullpath
=
createFullPath
(
path
)
;
List
<
ACL
>
acls
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
checkServiceLive
(
)
;
path
=
createFullPath
(
path
)
;
if
(
acls
==
null
||
acls
.
isEmpty
(
)
)
{
throw
new
NoPathPermissionsException
(
path
,
)
;
}
try
{
RegistrySecurity
.
AclListInfo
aclInfo
=
new
RegistrySecurity
.
AclListInfo
(
acls
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
path
,
mode
,
aclInfo
)
;
}
CreateBuilder
createBuilder
=
curator
.
create
(
)
;
createBuilder
.
withMode
(
mode
)
.
withACL
(
acls
)
;
if
(
createParents
)
{
createBuilder
.
creatingParentsIfNeeded
(
)
;
}
createBuilder
.
forPath
(
path
)
;
}
catch
(
KeeperException
.
NodeExistsException
e
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
zkCreate
(
String
path
,
CreateMode
mode
,
byte
[
]
data
,
List
<
ACL
>
acls
)
throws
IOException
{
Preconditions
.
checkArgument
(
data
!=
null
,
)
;
checkServiceLive
(
)
;
String
fullpath
=
createFullPath
(
path
)
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
zkUpdate
(
String
path
,
byte
[
]
data
)
throws
IOException
{
Preconditions
.
checkArgument
(
data
!=
null
,
)
;
checkServiceLive
(
)
;
path
=
createFullPath
(
path
)
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
zkDelete
(
String
path
,
boolean
recursive
,
BackgroundCallback
backgroundCallback
)
throws
IOException
{
checkServiceLive
(
)
;
String
fullpath
=
createFullPath
(
path
)
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
List
<
String
>
zkList
(
String
path
)
throws
IOException
{
checkServiceLive
(
)
;
String
fullpath
=
createFullPath
(
path
)
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
byte
[
]
zkRead
(
String
path
)
throws
IOException
{
checkServiceLive
(
)
;
String
fullpath
=
createFullPath
(
path
)
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
bind
(
String
path
,
ServiceRecord
record
,
int
flags
)
throws
IOException
{
Preconditions
.
checkArgument
(
record
!=
null
,
)
;
validatePath
(
path
)
;
RegistryTypeUtils
.
validateServiceRecord
(
path
,
record
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
RegistryPathStatus
stat
(
String
path
)
throws
IOException
{
validatePath
(
path
)
;
Stat
stat
=
zkStat
(
path
)
;
String
name
=
RegistryPathUtils
.
lastPathEntry
(
path
)
;
RegistryPathStatus
status
=
new
RegistryPathStatus
(
name
,
stat
.
getCtime
(
)
,
stat
.
getDataLength
(
)
,
stat
.
getNumChildren
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
initSecurity
(
)
throws
IOException
{
secureRegistry
=
getConfig
(
)
.
getBoolean
(
KEY_REGISTRY_SECURE
,
DEFAULT_REGISTRY_SECURE
)
;
systemACLs
.
clear
(
)
;
if
(
secureRegistry
)
{
addSystemACL
(
ALL_READ_ACCESS
)
;
kerberosRealm
=
getConfig
(
)
.
get
(
KEY_REGISTRY_KERBEROS_REALM
,
getDefaultRealmInJVM
(
)
)
;
String
system
=
getOrFail
(
KEY_REGISTRY_SYSTEM_ACCOUNTS
,
DEFAULT_REGISTRY_SYSTEM_ACCOUNTS
)
;
usesRealm
=
system
.
contains
(
)
;
systemACLs
.
addAll
(
buildACLs
(
system
,
kerberosRealm
,
ZooDefs
.
Perms
.
ALL
)
)
;
String
user
=
getConfig
(
)
.
get
(
KEY_REGISTRY_USER_ACCOUNTS
,
DEFAULT_REGISTRY_USER_ACCOUNTS
)
;
List
<
ACL
>
userACLs
=
buildACLs
(
user
,
kerberosRealm
,
ZooDefs
.
Perms
.
ALL
)
;
ACL
self
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
self
=
createSaslACLFromCurrentUser
(
ZooDefs
.
Perms
.
ALL
)
;
if
(
self
!=
null
)
{
userACLs
.
add
(
self
)
;
}
}
LOG
.
info
(
+
System
.
lineSeparator
(
)
+
userACLs
)
;
switch
(
access
)
{
case
sasl
:
if
(
!
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
throw
new
IOException
(
)
;
}
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
jaasClientEntry
=
getOrFail
(
KEY_REGISTRY_CLIENT_JAAS_CONTEXT
,
DEFAULT_REGISTRY_CLIENT_JAAS_CONTEXT
)
;
jaasClientIdentity
=
currentUser
.
getShortUserName
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
jaasClientEntry
=
getOrFail
(
KEY_REGISTRY_CLIENT_JAAS_CONTEXT
,
DEFAULT_REGISTRY_CLIENT_JAAS_CONTEXT
)
;
jaasClientIdentity
=
currentUser
.
getShortUserName
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
jaasClientIdentity
,
jaasClientEntry
)
;
}
break
;
case
digest
:
String
id
=
getOrFail
(
KEY_REGISTRY_CLIENT_AUTHENTICATION_ID
,
)
;
String
pass
=
getOrFail
(
KEY_REGISTRY_CLIENT_AUTHENTICATION_PASSWORD
,
)
;
if
(
userACLs
.
isEmpty
(
)
)
{
throw
new
ServiceStateException
(
E_NO_USER_DETERMINED_FOR_ACLS
)
;
}
digest
(
id
,
pass
)
;
ACL
acl
=
new
ACL
(
ZooDefs
.
Perms
.
ALL
,
toDigestId
(
id
,
pass
)
)
;
userACLs
.
add
(
acl
)
;
digestAuthUser
=
id
;
digestAuthPassword
=
pass
;
public
boolean
addDigestACL
(
ACL
acl
)
{
if
(
secureRegistry
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
bindJVMtoJAASFile
(
File
jaasFile
)
{
String
path
=
jaasFile
.
getAbsolutePath
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
case
anon
:
clearZKSaslClientProperties
(
)
;
break
;
case
digest
:
clearZKSaslClientProperties
(
)
;
builder
.
authorization
(
SCHEME_DIGEST
,
digestAuthData
)
;
break
;
case
sasl
:
String
existingJaasConf
=
System
.
getProperty
(
)
;
if
(
existingJaasConf
==
null
||
existingJaasConf
.
isEmpty
(
)
)
{
if
(
principal
==
null
||
keytab
==
null
)
{
throw
new
IOException
(
+
+
)
;
}
LOG
.
info
(
+
jaasClientEntry
+
+
principal
+
+
keytab
)
;
JaasConfiguration
jconf
=
new
JaasConfiguration
(
jaasClientEntry
,
principal
,
keytab
)
;
javax
.
security
.
auth
.
login
.
Configuration
.
setConfiguration
(
jconf
)
;
setSystemPropertyIfUnset
(
ZKClientConfig
.
ENABLE_CLIENT_SASL_KEY
,
)
;
setSystemPropertyIfUnset
(
ZKClientConfig
.
LOGIN_CONTEXT_NAME_KEY
,
jaasClientEntry
)
;
}
else
{
public
void
logCurrentHadoopUser
(
)
{
try
{
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
public
void
logCurrentHadoopUser
(
)
{
try
{
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
LOG
.
info
(
,
currentUser
)
;
UserGroupInformation
realUser
=
currentUser
.
getRealUser
(
)
;
public
ACL
createACLfromUsername
(
String
username
,
int
perms
)
{
if
(
usesRealm
&&
!
username
.
contains
(
)
)
{
username
=
username
+
+
kerberosRealm
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
initTypeToInfoMapping
(
ServiceRecord
serviceRecord
)
throws
Exception
{
if
(
serviceRecord
.
external
.
isEmpty
(
)
)
{
try
{
if
(
port
!=
53
)
{
throw
new
SocketException
(
)
;
}
Enumeration
<
NetworkInterface
>
net
=
NetworkInterface
.
getNetworkInterfaces
(
)
;
while
(
net
.
hasMoreElements
(
)
)
{
NetworkInterface
n
=
(
NetworkInterface
)
net
.
nextElement
(
)
;
Enumeration
<
InetAddress
>
ee
=
n
.
getInetAddresses
(
)
;
while
(
ee
.
hasMoreElements
(
)
)
{
InetAddress
i
=
(
InetAddress
)
ee
.
nextElement
(
)
;
list
.
add
(
i
)
;
}
}
}
catch
(
SocketException
e
)
{
}
ResolverConfig
.
refresh
(
)
;
ExtendedResolver
resolver
;
try
{
resolver
=
new
ExtendedResolver
(
)
;
}
}
}
catch
(
SocketException
e
)
{
}
ResolverConfig
.
refresh
(
)
;
ExtendedResolver
resolver
;
try
{
resolver
=
new
ExtendedResolver
(
)
;
}
catch
(
UnknownHostException
e
)
{
LOG
.
error
(
,
e
)
;
return
;
}
for
(
Resolver
check
:
resolver
.
getResolvers
(
)
)
{
if
(
check
instanceof
SimpleResolver
)
{
InetAddress
address
=
(
(
SimpleResolver
)
check
)
.
getAddress
(
)
.
getAddress
(
)
;
if
(
list
.
contains
(
address
)
)
{
resolver
.
deleteResolver
(
check
)
;
continue
;
}
else
{
private
void
signSiteRecord
(
Zone
zone
,
Record
record
)
throws
DNSSEC
.
DNSSECException
{
RRset
rrset
=
zone
.
findExactMatch
(
record
.
getName
(
)
,
record
.
getType
(
)
)
;
Calendar
cal
=
Calendar
.
getInstance
(
)
;
Date
inception
=
cal
.
getTime
(
)
;
cal
.
add
(
Calendar
.
YEAR
,
1
)
;
Date
expiration
=
cal
.
getTime
(
)
;
RRSIGRecord
rrsigRecord
=
DNSSEC
.
sign
(
rrset
,
dnsKeyRecs
.
get
(
zone
.
getOrigin
(
)
)
,
privateKey
,
inception
,
expiration
)
;
public
void
nioTCPClient
(
SocketChannel
ch
)
throws
IOException
{
try
{
ByteBuffer
buf
=
ByteBuffer
.
allocate
(
1024
)
;
ch
.
read
(
buf
)
;
buf
.
flip
(
)
;
int
messageLength
=
getMessgeLength
(
buf
)
;
byte
[
]
in
=
new
byte
[
messageLength
]
;
buf
.
get
(
in
,
0
,
messageLength
)
;
Message
query
;
byte
[
]
response
;
try
{
query
=
new
Message
(
in
)
;
while
(
true
)
{
input
.
clear
(
)
;
try
{
remoteAddress
=
channel
.
receive
(
input
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
e
)
;
continue
;
}
Message
query
;
byte
[
]
response
=
null
;
try
{
int
position
=
input
.
position
(
)
;
in
=
new
byte
[
position
]
;
input
.
flip
(
)
;
input
.
get
(
in
)
;
query
=
new
Message
(
in
)
;
Message
query
;
byte
[
]
response
=
null
;
try
{
int
position
=
input
.
position
(
)
;
in
=
new
byte
[
position
]
;
input
.
flip
(
)
;
input
.
get
(
in
)
;
query
=
new
Message
(
in
)
;
LOG
.
info
(
,
remoteAddress
,
query
.
getQuestion
(
)
)
;
response
=
generateReply
(
query
,
null
)
;
if
(
response
==
null
)
{
continue
;
}
}
catch
(
IOException
e
)
{
response
=
formErrorMessage
(
in
)
;
}
output
.
clear
(
)
;
private
Message
createPrimaryQuery
(
Message
query
)
throws
NameTooLongException
,
TextParseException
{
Name
name
=
query
.
getQuestion
(
)
.
getName
(
)
;
if
(
name
.
labels
(
)
>
0
&&
name
.
labels
(
)
<=
2
)
{
int
id
=
query
.
getHeader
(
)
.
getID
(
)
;
String
queryName
=
name
.
getLabelString
(
0
)
;
Name
qualifiedName
=
Name
.
concatenate
(
Name
.
fromString
(
queryName
)
,
Name
.
fromString
(
domainName
)
)
;
if
(
sr
!=
null
)
{
if
(
sr
.
isCNAME
(
)
)
{
CNAMERecord
cname
=
sr
.
getCNAME
(
)
;
RRset
rrset
=
zone
.
findExactMatch
(
cname
.
getName
(
)
,
Type
.
CNAME
)
;
addRRset
(
name
,
response
,
rrset
,
Section
.
ANSWER
,
flags
)
;
if
(
iterations
==
0
)
{
response
.
getHeader
(
)
.
setFlag
(
Flags
.
AA
)
;
}
rcode
=
addAnswer
(
response
,
cname
.
getTarget
(
)
,
type
,
dclass
,
iterations
+
1
,
flags
)
;
}
if
(
sr
.
isNXDOMAIN
(
)
)
{
response
.
getHeader
(
)
.
setRcode
(
Rcode
.
NXDOMAIN
)
;
if
(
isDNSSECEnabled
(
)
)
{
try
{
addNXT
(
response
,
flags
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
e
)
;
try
{
parsedRange
=
Integer
.
parseInt
(
range
)
;
}
catch
(
NumberFormatException
e
)
{
LOG
.
error
(
,
range
)
;
throw
e
;
}
if
(
parsedRange
<
0
)
{
String
msg
=
String
.
format
(
,
parsedRange
)
;
LOG
.
error
(
msg
)
;
throw
new
IllegalArgumentException
(
msg
)
;
}
long
ipCount
;
try
{
SubnetUtils
subnetUtils
=
new
SubnetUtils
(
subnet
,
mask
)
;
subnetUtils
.
setInclusiveHostCount
(
true
)
;
ipCount
=
subnetUtils
.
getInfo
(
)
.
getAddressCountLong
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
@
Override
public
void
processResult
(
CuratorFramework
client
,
CuratorEvent
event
)
throws
Exception
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
serviceStart
(
)
throws
Exception
{
setupSecurity
(
)
;
FileTxnSnapLog
ftxn
=
new
FileTxnSnapLog
(
dataDir
,
dataDir
)
;
ZooKeeperServer
zkServer
=
new
ZooKeeperServer
(
ftxn
,
tickTime
)
;
LOG
.
info
(
)
;
factory
=
ServerCnxnFactory
.
createFactory
(
)
;
factory
.
configure
(
getAddress
(
port
)
,
-
1
)
;
factory
.
startup
(
zkServer
)
;
String
connectString
=
getConnectionString
(
)
;
@
Override
protected
void
serviceStart
(
)
throws
Exception
{
setupSecurity
(
)
;
FileTxnSnapLog
ftxn
=
new
FileTxnSnapLog
(
dataDir
,
dataDir
)
;
ZooKeeperServer
zkServer
=
new
ZooKeeperServer
(
ftxn
,
tickTime
)
;
LOG
.
info
(
)
;
factory
=
ServerCnxnFactory
.
createFactory
(
)
;
factory
.
configure
(
getAddress
(
port
)
,
-
1
)
;
factory
.
startup
(
zkServer
)
;
String
connectString
=
getConnectionString
(
)
;
LOG
.
info
(
,
connectString
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
StringWriter
sw
=
new
StringWriter
(
)
;
PrintWriter
pw
=
new
PrintWriter
(
sw
)
;
zkServer
.
dumpConf
(
pw
)
;
pw
.
flush
(
)
;
public
<
V
>
Future
<
V
>
submit
(
Callable
<
V
>
callable
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
super
.
serviceInit
(
conf
)
;
RegistrySecurity
registrySecurity
=
getRegistrySecurity
(
)
;
if
(
registrySecurity
.
isSecureRegistry
(
)
)
{
ACL
sasl
=
registrySecurity
.
createSaslACLFromCurrentUser
(
ZooDefs
.
Perms
.
ALL
)
;
registrySecurity
.
addSystemACL
(
sasl
)
;
@
VisibleForTesting
public
void
createRootRegistryPaths
(
)
throws
IOException
{
List
<
ACL
>
systemACLs
=
getRegistrySecurity
(
)
.
getSystemACLs
(
)
;
childEntries
=
RegistryUtils
.
statChildren
(
this
,
path
)
;
entries
=
childEntries
.
values
(
)
;
}
catch
(
PathNotFoundException
e
)
{
return
0
;
}
try
{
RegistryPathStatus
registryPathStatus
=
stat
(
path
)
;
ServiceRecord
serviceRecord
=
resolve
(
path
)
;
toDelete
=
selector
.
shouldSelect
(
path
,
registryPathStatus
,
serviceRecord
)
;
}
catch
(
EOFException
ignored
)
{
}
catch
(
InvalidRecordException
ignored
)
{
}
catch
(
NoRecordException
ignored
)
{
}
catch
(
PathNotFoundException
e
)
{
return
0
;
}
if
(
toDelete
&&
!
entries
.
isEmpty
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
logLoginDetails
(
String
name
,
LoginContext
loginContext
)
{
assertNotNull
(
,
loginContext
)
;
Subject
subject
=
loginContext
.
getSubject
(
)
;
public
static
void
logRecord
(
String
name
,
ServiceRecord
record
)
throws
IOException
{
public
static
void
describe
(
Logger
log
,
String
text
,
Object
...
args
)
{
log
.
info
(
)
;
public
static
LoginContext
logout
(
LoginContext
login
)
{
try
{
if
(
login
!=
null
)
{
public
static
UserGroupInformation
loginUGI
(
String
user
,
File
keytab
)
throws
IOException
{
@
Test
public
void
testRoundTrip
(
)
throws
Throwable
{
String
persistence
=
PersistencePolicies
.
PERMANENT
;
ServiceRecord
record
=
createRecord
(
persistence
)
;
record
.
set
(
,
)
;
record
.
set
(
,
)
;
RegistryTypeUtils
.
validateServiceRecord
(
,
record
)
;
@
Override
public
void
processResult
(
CuratorFramework
client
,
CuratorEvent
event
)
throws
Exception
{
@
Test
public
void
testBackgroundDelete
(
)
throws
Throwable
{
mkPath
(
,
CreateMode
.
PERSISTENT
)
;
mkPath
(
,
CreateMode
.
PERSISTENT
)
;
CuratorEventCatcher
events
=
new
CuratorEventCatcher
(
)
;
curatorService
.
zkDelete
(
,
true
,
events
)
;
CuratorEvent
taken
=
events
.
take
(
)
;
}
kdcConf
=
MiniKdc
.
createConf
(
)
;
kdcConf
.
setProperty
(
MiniKdc
.
DEBUG
,
)
;
kdc
=
new
MiniKdc
(
kdcConf
,
kdcWorkDir
)
;
kdc
.
start
(
)
;
keytab_zk
=
createKeytab
(
ZOOKEEPER
,
)
;
keytab_alice
=
createKeytab
(
ALICE
,
)
;
keytab_bob
=
createKeytab
(
BOB
,
)
;
zkServerPrincipal
=
Shell
.
WINDOWS
?
ZOOKEEPER_1270001
:
ZOOKEEPER_LOCALHOST
;
StringBuilder
jaas
=
new
StringBuilder
(
1024
)
;
jaas
.
append
(
registrySecurity
.
createJAASEntry
(
ZOOKEEPER_CLIENT_CONTEXT
,
ZOOKEEPER
,
keytab_zk
)
)
;
jaas
.
append
(
registrySecurity
.
createJAASEntry
(
ZOOKEEPER_SERVER_CONTEXT
,
zkServerPrincipal
,
keytab_zk
)
)
;
jaas
.
append
(
registrySecurity
.
createJAASEntry
(
ALICE_CLIENT_CONTEXT
,
ALICE_LOCALHOST
,
keytab_alice
)
)
;
jaas
.
append
(
registrySecurity
.
createJAASEntry
(
BOB_CLIENT_CONTEXT
,
BOB_LOCALHOST
,
keytab_bob
)
)
;
jaasFile
=
new
File
(
kdcWorkDir
,
)
;
FileUtils
.
write
(
jaasFile
,
jaas
.
toString
(
)
,
Charset
.
defaultCharset
(
)
)
;
protected
LoginContext
login
(
String
principal
,
String
context
,
File
keytab
)
throws
LoginException
,
FileNotFoundException
{
@
Test
public
void
testDefaultRealm
(
)
throws
Throwable
{
String
realm
=
RegistrySecurity
.
getDefaultRealmInJVM
(
)
;
@
Test
public
void
testUGIProperties
(
)
throws
Throwable
{
UserGroupInformation
user
=
UserGroupInformation
.
getCurrentUser
(
)
;
ACL
acl
=
registrySecurity
.
createACLForUser
(
user
,
ZooDefs
.
Perms
.
ALL
)
;
assertFalse
(
RegistrySecurity
.
ALL_READWRITE_ACCESS
.
equals
(
acl
)
)
;
@
Test
public
void
testClientLogin
(
)
throws
Throwable
{
LoginContext
client
=
login
(
ALICE_LOCALHOST
,
ALICE_CLIENT_CONTEXT
,
keytab_alice
)
;
try
{
logLoginDetails
(
ALICE_LOCALHOST
,
client
)
;
String
confFilename
=
System
.
getProperty
(
Environment
.
JAAS_CONF_KEY
)
;
assertNotNull
(
+
Environment
.
JAAS_CONF_KEY
,
confFilename
)
;
String
config
=
FileUtils
.
readFileToString
(
new
File
(
confFilename
)
,
Charset
.
defaultCharset
(
)
)
;
@
Test
public
void
testKerberosAuth
(
)
throws
Throwable
{
File
krb5conf
=
getKdc
(
)
.
getKrb5conf
(
)
;
String
krbConfig
=
FileUtils
.
readFileToString
(
krb5conf
,
Charset
.
defaultCharset
(
)
)
;
@
Test
public
void
testDefaultRealmValid
(
)
throws
Throwable
{
String
defaultRealm
=
KerberosUtil
.
getDefaultRealm
(
)
;
assertNotEmpty
(
,
defaultRealm
)
;
@
Test
public
void
testKerberosRulesValid
(
)
throws
Throwable
{
assertTrue
(
,
KerberosName
.
hasRulesBeenSet
(
)
)
;
String
rules
=
KerberosName
.
getRules
(
)
;
assertEquals
(
kerberosRule
,
rules
)
;
@
Test
public
void
testUGILogin
(
)
throws
Throwable
{
UserGroupInformation
ugi
=
loginUGI
(
ZOOKEEPER
,
keytab_zk
)
;
RegistrySecurity
.
UgiInfo
ugiInfo
=
new
RegistrySecurity
.
UgiInfo
(
ugi
)
;
@
Test
public
void
testZookeeperCanWrite
(
)
throws
Throwable
{
System
.
setProperty
(
,
)
;
startSecureZK
(
)
;
CuratorService
curator
=
null
;
LoginContext
login
=
login
(
ZOOKEEPER_LOCALHOST
,
ZOOKEEPER_CLIENT_CONTEXT
,
keytab_zk
)
;
try
{
logLoginDetails
(
ZOOKEEPER
,
login
)
;
RegistrySecurity
.
setZKSaslClientProperties
(
ZOOKEEPER
,
ZOOKEEPER_CLIENT_CONTEXT
)
;
curator
=
startCuratorServiceInstance
(
,
true
)
;
protected
CuratorService
startCuratorServiceInstance
(
String
name
,
boolean
secure
)
{
Configuration
clientConf
=
new
Configuration
(
)
;
clientConf
.
set
(
KEY_REGISTRY_ZK_ROOT
,
)
;
clientConf
.
setBoolean
(
KEY_REGISTRY_SECURE
,
secure
)
;
describe
(
LOG
,
)
;
CuratorService
curatorService
=
new
CuratorService
(
name
,
secureZK
)
;
curatorService
.
init
(
clientConf
)
;
curatorService
.
start
(
)
;
public
void
userZookeeperToCreateRoot
(
)
throws
Throwable
{
System
.
setProperty
(
,
)
;
CuratorService
curator
=
null
;
LoginContext
login
=
login
(
ZOOKEEPER_LOCALHOST
,
ZOOKEEPER_CLIENT_CONTEXT
,
keytab_zk
)
;
try
{
logLoginDetails
(
ZOOKEEPER
,
login
)
;
RegistrySecurity
.
setZKSaslClientProperties
(
ZOOKEEPER
,
ZOOKEEPER_CLIENT_CONTEXT
)
;
curator
=
startCuratorServiceInstance
(
,
true
)
;
public
void
userZookeeperToCreateRoot
(
)
throws
Throwable
{
System
.
setProperty
(
,
)
;
CuratorService
curator
=
null
;
LoginContext
login
=
login
(
ZOOKEEPER_LOCALHOST
,
ZOOKEEPER_CLIENT_CONTEXT
,
keytab_zk
)
;
try
{
logLoginDetails
(
ZOOKEEPER
,
login
)
;
RegistrySecurity
.
setZKSaslClientProperties
(
ZOOKEEPER
,
ZOOKEEPER_CLIENT_CONTEXT
)
;
curator
=
startCuratorServiceInstance
(
,
true
)
;
LOG
.
info
(
curator
.
toString
(
)
)
;
addToTeardown
(
curator
)
;
curator
.
zkMkPath
(
,
CreateMode
.
PERSISTENT
,
false
,
RegistrySecurity
.
WorldReadWriteACL
)
;
ZKPathDumper
pathDumper
=
curator
.
dumpPath
(
true
)
;
for
(
;
;
)
{
final
long
inodeId
;
final
DFSOutputStream
out
;
synchronized
(
filesBeingWritten
)
{
if
(
filesBeingWritten
.
isEmpty
(
)
)
{
return
;
}
inodeId
=
filesBeingWritten
.
keySet
(
)
.
iterator
(
)
.
next
(
)
;
out
=
filesBeingWritten
.
remove
(
inodeId
)
;
}
if
(
out
!=
null
)
{
try
{
if
(
abort
)
{
out
.
abort
(
)
;
}
else
{
out
.
close
(
)
;
}
}
catch
(
IOException
ie
)
{
@
Deprecated
public
long
renewDelegationToken
(
Token
<
DelegationTokenIdentifier
>
token
)
throws
IOException
{
@
Deprecated
public
void
cancelDelegationToken
(
Token
<
DelegationTokenIdentifier
>
token
)
throws
IOException
{
public
HdfsDataOutputStream
createWrappedOutputStream
(
DFSOutputStream
dfsos
,
FileSystem
.
Statistics
statistics
,
long
startPos
)
throws
IOException
{
final
FileEncryptionInfo
feInfo
=
dfsos
.
getFileEncryptionInfo
(
)
;
if
(
feInfo
!=
null
)
{
HdfsKMSUtil
.
getCryptoProtocolVersion
(
feInfo
)
;
final
CryptoCodec
codec
=
HdfsKMSUtil
.
getCryptoCodec
(
conf
,
feInfo
)
;
KeyVersion
decrypted
;
try
(
TraceScope
ignored
=
tracer
.
newScope
(
)
)
{
public
HdfsDataOutputStream
createWrappedOutputStream
(
DFSOutputStream
dfsos
,
FileSystem
.
Statistics
statistics
,
long
startPos
)
throws
IOException
{
final
FileEncryptionInfo
feInfo
=
dfsos
.
getFileEncryptionInfo
(
)
;
if
(
feInfo
!=
null
)
{
HdfsKMSUtil
.
getCryptoProtocolVersion
(
feInfo
)
;
final
CryptoCodec
codec
=
HdfsKMSUtil
.
getCryptoCodec
(
conf
,
feInfo
)
;
KeyVersion
decrypted
;
try
(
TraceScope
ignored
=
tracer
.
newScope
(
)
)
{
LOG
.
debug
(
,
dfsos
.
getSrc
(
)
,
Integer
.
toHexString
(
dfsos
.
hashCode
(
)
)
)
;
decrypted
=
HdfsKMSUtil
.
decryptEncryptedDataEncryptionKey
(
feInfo
,
getKeyProvider
(
)
)
;
public
DFSOutputStream
create
(
String
src
,
FsPermission
permission
,
EnumSet
<
CreateFlag
>
flag
,
boolean
createParent
,
short
replication
,
long
blockSize
,
Progressable
progress
,
int
buffersize
,
ChecksumOpt
checksumOpt
,
InetSocketAddress
[
]
favoredNodes
,
String
ecPolicyName
,
String
storagePolicy
)
throws
IOException
{
checkOpen
(
)
;
final
FsPermission
masked
=
applyUMask
(
permission
)
;
private
DFSOutputStream
callAppend
(
String
src
,
EnumSet
<
CreateFlag
>
flag
,
Progressable
progress
,
String
[
]
favoredNodes
)
throws
IOException
{
CreateFlag
.
validateForAppend
(
flag
)
;
try
{
final
LastBlockWithStatus
blkWithStatus
=
callAppend
(
src
,
new
EnumSetWritable
<
>
(
flag
,
CreateFlag
.
class
)
)
;
HdfsFileStatus
status
=
blkWithStatus
.
getFileStatus
(
)
;
if
(
status
==
null
)
{
private
static
synchronized
void
initThreadsNumForHedgedReads
(
int
num
)
{
if
(
num
<=
0
||
HEDGED_READ_THREAD_POOL
!=
null
)
return
;
HEDGED_READ_THREAD_POOL
=
new
ThreadPoolExecutor
(
1
,
num
,
60
,
TimeUnit
.
SECONDS
,
new
SynchronousQueue
<
Runnable
>
(
)
,
new
Daemon
.
DaemonFactory
(
)
{
private
final
AtomicInteger
threadIndex
=
new
AtomicInteger
(
0
)
;
@
Override
public
Thread
newThread
(
Runnable
r
)
{
Thread
t
=
super
.
newThread
(
r
)
;
t
.
setName
(
+
threadIndex
.
getAndIncrement
(
)
)
;
return
t
;
}
}
,
new
ThreadPoolExecutor
.
CallerRunsPolicy
(
)
{
@
Override
public
void
rejectedExecution
(
Runnable
runnable
,
ThreadPoolExecutor
e
)
{
LOG
.
info
(
)
;
HEDGED_READ_METRIC
.
incHedgedReadOpsInCurThread
(
)
;
super
.
rejectedExecution
(
runnable
,
e
)
;
}
}
)
;
HEDGED_READ_THREAD_POOL
.
allowCoreThreadTimeOut
(
true
)
;
public
void
addNodeToDeadNodeDetector
(
DFSInputStream
dfsInputStream
,
DatanodeInfo
datanodeInfo
)
{
if
(
!
isDeadNodeDetectionEnabled
(
)
)
{
public
void
removeNodeFromDeadNodeDetector
(
DFSInputStream
dfsInputStream
,
DatanodeInfo
datanodeInfo
)
{
if
(
!
isDeadNodeDetectionEnabled
(
)
)
{
public
void
removeNodeFromDeadNodeDetector
(
DFSInputStream
dfsInputStream
,
LocatedBlocks
locatedBlocks
)
{
if
(
!
isDeadNodeDetectionEnabled
(
)
||
locatedBlocks
==
null
)
{
try
(
TraceScope
ignored
=
tracer
.
newScope
(
)
)
{
if
(
lastReadTxid
==
-
1
)
{
LOG
.
debug
(
)
;
lastReadTxid
=
namenode
.
getCurrentEditLogTxid
(
)
;
return
null
;
}
if
(
!
it
.
hasNext
(
)
)
{
EventBatchList
el
=
namenode
.
getEditsFromTxid
(
lastReadTxid
+
1
)
;
if
(
el
.
getLastTxid
(
)
!=
-
1
)
{
syncTxid
=
el
.
getSyncTxid
(
)
;
it
=
el
.
getBatches
(
)
.
iterator
(
)
;
long
formerLastReadTxid
=
lastReadTxid
;
lastReadTxid
=
el
.
getLastTxid
(
)
;
if
(
el
.
getFirstTxid
(
)
!=
formerLastReadTxid
+
1
)
{
throw
new
MissingEventsException
(
formerLastReadTxid
+
1
,
el
.
getFirstTxid
(
)
)
;
}
}
else
{
public
EventBatch
take
(
)
throws
IOException
,
InterruptedException
,
MissingEventsException
{
EventBatch
next
;
try
(
TraceScope
ignored
=
tracer
.
newScope
(
)
)
{
int
nextWaitMin
=
INITIAL_WAIT_MS
;
while
(
(
next
=
poll
(
)
)
==
null
)
{
int
sleepTime
=
nextWaitMin
+
rng
.
nextInt
(
nextWaitMin
)
;
long
sleeptime
=
conf
.
getBlockWriteLocateFollowingInitialDelayMs
(
)
;
long
maxSleepTime
=
conf
.
getBlockWriteLocateFollowingMaxDelayMs
(
)
;
long
localstart
=
Time
.
monotonicNow
(
)
;
while
(
true
)
{
try
{
return
dfsClient
.
namenode
.
addBlock
(
src
,
dfsClient
.
clientName
,
prevBlock
,
excludedNodes
,
fileId
,
favoredNodes
,
allocFlags
)
;
}
catch
(
RemoteException
e
)
{
IOException
ue
=
e
.
unwrapRemoteException
(
FileNotFoundException
.
class
,
AccessControlException
.
class
,
NSQuotaExceededException
.
class
,
DSQuotaExceededException
.
class
,
QuotaByStorageTypeExceededException
.
class
,
UnresolvedPathException
.
class
)
;
if
(
ue
!=
e
)
{
throw
ue
;
}
if
(
NotReplicatedYetException
.
class
.
getName
(
)
.
equals
(
e
.
getClassName
(
)
)
)
{
if
(
retries
==
0
)
{
throw
e
;
}
else
{
--
retries
;
final
int
expectedNum
=
healthyStreamers
.
size
(
)
;
final
long
socketTimeout
=
dfsClient
.
getConf
(
)
.
getSocketTimeout
(
)
;
long
remaingTime
=
socketTimeout
>
0
?
socketTimeout
/
2
:
Long
.
MAX_VALUE
;
final
long
waitInterval
=
1000
;
synchronized
(
coordinator
)
{
while
(
checkStreamerUpdates
(
failed
,
healthyStreamers
)
<
expectedNum
&&
remaingTime
>
0
)
{
try
{
long
start
=
Time
.
monotonicNow
(
)
;
coordinator
.
wait
(
waitInterval
)
;
remaingTime
-=
Time
.
monotonicNow
(
)
-
start
;
}
catch
(
InterruptedException
e
)
{
throw
DFSUtilClient
.
toInterruptedIOException
(
+
,
e
)
;
}
}
}
synchronized
(
coordinator
)
{
for
(
StripedDataStreamer
streamer
:
healthyStreamers
)
{
if
(
!
coordinator
.
updateStreamerMap
.
containsKey
(
streamer
)
)
{
@
Override
public
void
hsync
(
EnumSet
<
SyncFlag
>
syncFlags
)
{
public
static
IOStreamPair
connectToDN
(
DatanodeInfo
dn
,
int
timeout
,
Configuration
conf
,
SaslDataTransferClient
saslClient
,
SocketFactory
socketFactory
,
boolean
connectToDnViaHostname
,
DataEncryptionKeyFactory
dekFactory
,
Token
<
BlockTokenIdentifier
>
blockToken
)
throws
IOException
{
boolean
success
=
false
;
Socket
sock
=
null
;
try
{
sock
=
socketFactory
.
createSocket
(
)
;
String
dnAddr
=
dn
.
getXferAddr
(
connectToDnViaHostname
)
;
static
Socket
createSocketForPipeline
(
final
DatanodeInfo
first
,
final
int
length
,
final
DFSClient
client
)
throws
IOException
{
final
DfsClientConf
conf
=
client
.
getConf
(
)
;
final
String
dnAddr
=
first
.
getXferAddr
(
conf
.
isConnectToDnViaHostname
(
)
)
;
void
waitForAckedSeqno
(
long
seqno
)
throws
IOException
{
try
(
TraceScope
ignored
=
dfsClient
.
getTracer
(
)
.
newScope
(
)
)
{
try
(
TraceScope
ignored
=
dfsClient
.
getTracer
(
)
.
newScope
(
)
)
{
LOG
.
debug
(
,
this
,
seqno
)
;
long
begin
=
Time
.
monotonicNow
(
)
;
try
{
synchronized
(
dataQueue
)
{
while
(
!
streamerClosed
)
{
checkClosed
(
)
;
if
(
lastAckedSeqno
>=
seqno
)
{
break
;
}
try
{
dataQueue
.
wait
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
throw
new
InterruptedIOException
(
)
;
}
}
}
checkClosed
(
)
;
}
catch
(
ClosedChannelException
cce
)
{
if
(
span
!=
null
)
{
span
.
addTimelineAnnotation
(
)
;
}
firstWait
=
false
;
}
try
{
dataQueue
.
wait
(
)
;
}
catch
(
InterruptedException
e
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
break
;
}
}
}
finally
{
Span
span
=
Tracer
.
getCurrentSpan
(
)
;
if
(
(
span
!=
null
)
&&
(
!
firstWait
)
)
{
span
.
addTimelineAnnotation
(
)
;
}
}
checkClosed
(
)
;
queuePacket
(
packet
)
;
}
catch
(
ClosedChannelException
cce
)
{
void
queuePacket
(
DFSPacket
packet
)
{
synchronized
(
dataQueue
)
{
if
(
packet
==
null
)
return
;
packet
.
addTraceParent
(
Tracer
.
getCurrentSpanId
(
)
)
;
dataQueue
.
addLast
(
packet
)
;
lastQueuedSeqno
=
packet
.
getSeqno
(
)
;
private
static
LoadingCache
<
DatanodeInfo
,
DatanodeInfo
>
initExcludedNodes
(
long
excludedNodesCacheExpiry
)
{
return
CacheBuilder
.
newBuilder
(
)
.
expireAfterWrite
(
excludedNodesCacheExpiry
,
TimeUnit
.
MILLISECONDS
)
.
removalListener
(
new
RemovalListener
<
DatanodeInfo
,
DatanodeInfo
>
(
)
{
@
Override
public
void
onRemoval
(
@
Nonnull
RemovalNotification
<
DatanodeInfo
,
DatanodeInfo
>
notification
)
{
private
void
scheduleProbe
(
ProbeType
type
)
{
private
void
scheduleProbe
(
ProbeType
type
)
{
LOG
.
debug
(
,
type
)
;
DatanodeInfo
datanodeInfo
=
null
;
if
(
type
==
ProbeType
.
CHECK_DEAD
)
{
while
(
(
datanodeInfo
=
deadNodesProbeQueue
.
poll
(
)
)
!=
null
)
{
if
(
probeInProg
.
containsKey
(
datanodeInfo
.
getDatanodeUuid
(
)
)
)
{
private
void
probeCallBack
(
Probe
probe
,
boolean
success
)
{
private
void
probeCallBack
(
Probe
probe
,
boolean
success
)
{
LOG
.
debug
(
,
probe
.
getDatanodeInfo
(
)
,
success
,
probe
.
getType
(
)
)
;
probeInProg
.
remove
(
probe
.
getDatanodeInfo
(
)
.
getDatanodeUuid
(
)
)
;
if
(
success
)
{
if
(
probe
.
getType
(
)
==
ProbeType
.
CHECK_DEAD
)
{
private
void
checkDeadNodes
(
)
{
Set
<
DatanodeInfo
>
datanodeInfos
=
clearAndGetDetectedDeadNodes
(
)
;
for
(
DatanodeInfo
datanodeInfo
:
datanodeInfos
)
{
private
void
checkDeadNodes
(
)
{
Set
<
DatanodeInfo
>
datanodeInfos
=
clearAndGetDetectedDeadNodes
(
)
;
for
(
DatanodeInfo
datanodeInfo
:
datanodeInfos
)
{
LOG
.
debug
(
,
datanodeInfo
)
;
if
(
!
deadNodesProbeQueue
.
offer
(
datanodeInfo
)
)
{
public
static
void
cloneDelegationTokenForLogicalUri
(
UserGroupInformation
ugi
,
URI
haUri
,
Collection
<
InetSocketAddress
>
nnAddrs
)
{
Text
haService
=
HAUtilClient
.
buildTokenServiceForLogicalUri
(
haUri
,
HdfsConstants
.
HDFS_URI_SCHEME
)
;
Token
<
DelegationTokenIdentifier
>
haToken
=
tokenSelector
.
selectToken
(
haService
,
ugi
.
getTokens
(
)
)
;
if
(
haToken
!=
null
)
{
for
(
InetSocketAddress
singleNNAddr
:
nnAddrs
)
{
Token
<
DelegationTokenIdentifier
>
specificToken
=
haToken
.
privateClone
(
buildTokenService
(
singleNNAddr
)
)
;
Text
alias
=
new
Text
(
HAUtilClient
.
buildTokenServicePrefixForLogicalUri
(
HdfsConstants
.
HDFS_URI_SCHEME
)
+
+
specificToken
.
getService
(
)
)
;
ugi
.
addToken
(
alias
,
specificToken
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
URI
createKeyProviderURI
(
Configuration
conf
)
{
final
String
providerUriStr
=
conf
.
getTrimmed
(
CommonConfigurationKeysPublic
.
HADOOP_SECURITY_KEY_PROVIDER_PATH
)
;
if
(
providerUriStr
==
null
||
providerUriStr
.
isEmpty
(
)
)
{
protected
static
<
T
>
AbstractNNFailoverProxyProvider
<
T
>
createFailoverProxyProvider
(
Configuration
conf
,
URI
nameNodeUri
,
Class
<
T
>
xface
,
boolean
checkPort
,
AtomicBoolean
fallbackToSimpleAuth
,
HAProxyFactory
<
T
>
proxyFactory
)
throws
IOException
{
Class
<
FailoverProxyProvider
<
T
>>
failoverProxyProviderClass
=
null
;
AbstractNNFailoverProxyProvider
<
T
>
providerNN
;
try
{
failoverProxyProviderClass
=
getFailoverProxyProviderClass
(
conf
,
nameNodeUri
)
;
if
(
failoverProxyProviderClass
==
null
)
{
return
null
;
}
Constructor
<
FailoverProxyProvider
<
T
>>
ctor
=
failoverProxyProviderClass
.
getConstructor
(
Configuration
.
class
,
URI
.
class
,
Class
.
class
,
HAProxyFactory
.
class
)
;
FailoverProxyProvider
<
T
>
provider
=
ctor
.
newInstance
(
conf
,
nameNodeUri
,
xface
,
proxyFactory
)
;
if
(
!
(
provider
instanceof
AbstractNNFailoverProxyProvider
)
)
{
providerNN
=
new
WrappedFailoverProxyProvider
<
>
(
provider
)
;
}
else
{
providerNN
=
(
AbstractNNFailoverProxyProvider
<
T
>
)
provider
;
}
}
catch
(
Exception
e
)
{
final
String
message
=
+
failoverProxyProviderClass
;
}
final
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setBoolean
(
String
.
format
(
,
scheme
)
,
true
)
;
conf
.
setBoolean
(
HdfsClientConfigKeys
.
Retry
.
POLICY_ENABLED_KEY
,
false
)
;
conf
.
setInt
(
CommonConfigurationKeysPublic
.
IPC_CLIENT_CONNECT_MAX_RETRIES_KEY
,
0
)
;
DistributedFileSystem
fs
=
null
;
try
{
fs
=
(
DistributedFileSystem
)
FileSystem
.
get
(
uri
,
conf
)
;
final
boolean
safemode
=
fs
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_GET
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
safemode
+
+
uri
)
;
}
fs
.
close
(
)
;
fs
=
null
;
return
!
safemode
;
}
catch
(
IOException
e
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
ShortCircuitCache
cache
=
clientContext
.
getShortCircuitCache
(
block
.
getBlockId
(
)
)
;
try
{
MutableBoolean
usedPeer
=
new
MutableBoolean
(
false
)
;
slot
=
cache
.
allocShmSlot
(
datanode
,
peer
,
usedPeer
,
new
ExtendedBlockId
(
block
.
getBlockId
(
)
,
block
.
getBlockPoolId
(
)
)
,
clientName
)
;
if
(
usedPeer
.
booleanValue
(
)
)
{
LOG
.
trace
(
+
,
this
,
peer
.
getDomainSocket
(
)
)
;
curPeer
=
nextDomainPeer
(
)
;
if
(
curPeer
==
null
)
break
;
peer
=
(
DomainPeer
)
curPeer
.
peer
;
}
ShortCircuitReplicaInfo
info
=
requestFileDescriptors
(
peer
,
slot
)
;
clientContext
.
getPeerCache
(
)
.
put
(
datanode
,
peer
)
;
return
info
;
}
catch
(
IOException
e
)
{
if
(
slot
!=
null
)
{
cache
.
freeSlot
(
slot
)
;
sock
.
getOutputStream
(
)
.
write
(
0
)
;
}
replica
=
new
ShortCircuitReplica
(
key
,
fis
[
0
]
,
fis
[
1
]
,
cache
,
Time
.
monotonicNow
(
)
,
slot
)
;
return
new
ShortCircuitReplicaInfo
(
replica
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
this
+
,
e
)
;
return
null
;
}
finally
{
if
(
replica
==
null
)
{
IOUtilsClient
.
cleanupWithLogger
(
DFSClient
.
LOG
,
fis
[
0
]
,
fis
[
1
]
)
;
}
}
case
ERROR_UNSUPPORTED
:
if
(
!
resp
.
hasShortCircuitAccessVersion
(
)
)
{
LOG
.
warn
(
+
+
datanode
+
+
resp
.
getMessage
(
)
)
;
clientContext
.
getDomainSocketFactory
(
)
.
disableShortCircuitForPath
(
pathInfo
.
getPath
(
)
)
;
}
else
{
LOG
.
warn
(
+
fileName
+
+
datanode
+
+
resp
.
getMessage
(
)
)
;
}
return
null
;
}
LOG
.
trace
(
+
,
this
,
pathInfo
.
getPath
(
)
)
;
while
(
true
)
{
BlockReaderPeer
curPeer
=
nextDomainPeer
(
)
;
if
(
curPeer
==
null
)
break
;
if
(
curPeer
.
fromCache
)
remainingCacheTries
--
;
DomainPeer
peer
=
(
DomainPeer
)
curPeer
.
peer
;
BlockReader
blockReader
=
null
;
try
{
blockReader
=
getRemoteBlockReader
(
peer
)
;
return
blockReader
;
}
catch
(
IOException
ioe
)
{
IOUtilsClient
.
cleanupWithLogger
(
LOG
,
peer
)
;
if
(
isSecurityException
(
ioe
)
)
{
LOG
.
trace
(
+
,
this
,
pathInfo
.
getPath
(
)
,
ioe
)
;
throw
ioe
;
BlockReader
blockReader
=
null
;
while
(
true
)
{
BlockReaderPeer
curPeer
=
null
;
Peer
peer
=
null
;
try
{
curPeer
=
nextTcpPeer
(
)
;
if
(
curPeer
.
fromCache
)
remainingCacheTries
--
;
peer
=
curPeer
.
peer
;
blockReader
=
getRemoteBlockReader
(
peer
)
;
return
blockReader
;
}
catch
(
IOException
ioe
)
{
if
(
isSecurityException
(
ioe
)
)
{
LOG
.
trace
(
+
,
this
,
peer
,
ioe
)
;
throw
ioe
;
}
if
(
(
curPeer
!=
null
)
&&
curPeer
.
fromCache
)
{
static
BlockReaderLocalLegacy
newBlockReader
(
DfsClientConf
conf
,
UserGroupInformation
userGroupInformation
,
Configuration
configuration
,
String
file
,
ExtendedBlock
blk
,
Token
<
BlockTokenIdentifier
>
token
,
DatanodeInfo
node
,
long
startOffset
,
long
length
,
StorageType
storageType
)
throws
IOException
{
final
ShortCircuitConf
scConf
=
conf
.
getShortCircuitConf
(
)
;
LocalDatanodeInfo
localDatanodeInfo
=
getLocalDatanodeInfo
(
node
.
getIpcPort
(
)
)
;
BlockLocalPathInfo
pathinfo
=
localDatanodeInfo
.
getBlockLocalPathInfo
(
blk
)
;
if
(
pathinfo
==
null
)
{
if
(
userGroupInformation
==
null
)
{
userGroupInformation
=
UserGroupInformation
.
getCurrentUser
(
)
;
}
pathinfo
=
getBlockPathInfo
(
userGroupInformation
,
blk
,
node
,
configuration
,
conf
.
getSocketTimeout
(
)
,
token
,
conf
.
isConnectToDnViaHostname
(
)
,
storageType
)
;
}
FileInputStream
dataIn
=
null
;
FileInputStream
checksumIn
=
null
;
BlockReaderLocalLegacy
localBlockReader
=
null
;
final
boolean
skipChecksumCheck
=
scConf
.
isSkipShortCircuitChecksums
(
)
||
storageType
.
isTransient
(
)
;
try
{
File
blkfile
=
new
File
(
pathinfo
.
getBlockPath
(
)
)
;
dataIn
=
new
FileInputStream
(
blkfile
)
;
private
static
BlockLocalPathInfo
getBlockPathInfo
(
UserGroupInformation
ugi
,
ExtendedBlock
blk
,
DatanodeInfo
node
,
Configuration
conf
,
int
timeout
,
Token
<
BlockTokenIdentifier
>
token
,
boolean
connectToDnViaHostname
,
StorageType
storageType
)
throws
IOException
{
LocalDatanodeInfo
localDatanodeInfo
=
getLocalDatanodeInfo
(
node
.
getIpcPort
(
)
)
;
BlockLocalPathInfo
pathinfo
;
ClientDatanodeProtocol
proxy
=
localDatanodeInfo
.
getDatanodeProxy
(
ugi
,
node
,
conf
,
timeout
,
connectToDnViaHostname
)
;
try
{
pathinfo
=
proxy
.
getBlockLocalPathInfo
(
blk
,
token
)
;
if
(
pathinfo
!=
null
&&
!
storageType
.
isTransient
(
)
)
{
public
synchronized
void
put
(
final
DFSClient
dfsc
)
{
if
(
dfsc
.
isClientRunning
(
)
)
{
if
(
!
isRunning
(
)
||
isRenewerExpired
(
)
)
{
final
int
id
=
++
currentId
;
daemon
=
new
Daemon
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
dfsc
.
isClientRunning
(
)
)
{
if
(
!
isRunning
(
)
||
isRenewerExpired
(
)
)
{
final
int
id
=
++
currentId
;
daemon
=
new
Daemon
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
clientsString
(
)
+
+
id
+
)
;
}
LeaseRenewer
.
this
.
run
(
id
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
debug
(
,
e
)
;
}
finally
{
synchronized
(
LeaseRenewer
.
this
)
{
Factory
.
INSTANCE
.
remove
(
LeaseRenewer
.
this
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
renew
(
)
throws
IOException
{
final
List
<
DFSClient
>
copies
;
synchronized
(
this
)
{
copies
=
new
ArrayList
<
>
(
dfsclients
)
;
}
Collections
.
sort
(
copies
,
new
Comparator
<
DFSClient
>
(
)
{
@
Override
public
int
compare
(
final
DFSClient
left
,
final
DFSClient
right
)
{
return
left
.
getClientName
(
)
.
compareTo
(
right
.
getClientName
(
)
)
;
}
}
)
;
String
previousName
=
;
for
(
final
DFSClient
c
:
copies
)
{
if
(
!
c
.
getClientName
(
)
.
equals
(
previousName
)
)
{
if
(
!
c
.
renewLease
(
)
)
{
LOG
.
debug
(
,
c
)
;
continue
;
}
previousName
=
c
.
getClientName
(
)
;
private
void
run
(
final
int
id
)
throws
InterruptedException
{
for
(
long
lastRenewed
=
Time
.
monotonicNow
(
)
;
!
Thread
.
interrupted
(
)
;
Thread
.
sleep
(
getSleepPeriod
(
)
)
)
{
final
long
elapsed
=
Time
.
monotonicNow
(
)
-
lastRenewed
;
if
(
elapsed
>=
getRenewalTime
(
)
)
{
try
{
renew
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
warn
(
+
clientsString
(
)
+
+
(
elapsed
/
1000
)
+
,
ie
)
;
List
<
DFSClient
>
dfsclientsCopy
;
synchronized
(
this
)
{
DFSClientFaultInjector
.
get
(
)
.
delayWhenRenewLeaseTimeout
(
)
;
dfsclientsCopy
=
new
ArrayList
<
>
(
dfsclients
)
;
dfsclients
.
clear
(
)
;
emptyTime
=
0
;
Factory
.
INSTANCE
.
remove
(
LeaseRenewer
.
this
)
;
}
for
(
DFSClient
dfsClient
:
dfsclientsCopy
)
{
dfsClient
.
closeAllFilesBeingWritten
(
true
)
;
}
break
;
}
catch
(
IOException
ie
)
{
LOG
.
warn
(
+
clientsString
(
)
+
+
(
elapsed
/
1000
)
+
,
ie
)
;
}
}
synchronized
(
this
)
{
if
(
id
!=
currentId
||
isRenewerExpired
(
)
)
{
synchronized
(
this
)
{
DFSClientFaultInjector
.
get
(
)
.
delayWhenRenewLeaseTimeout
(
)
;
dfsclientsCopy
=
new
ArrayList
<
>
(
dfsclients
)
;
dfsclients
.
clear
(
)
;
emptyTime
=
0
;
Factory
.
INSTANCE
.
remove
(
LeaseRenewer
.
this
)
;
}
for
(
DFSClient
dfsClient
:
dfsclientsCopy
)
{
dfsClient
.
closeAllFilesBeingWritten
(
true
)
;
}
break
;
}
catch
(
IOException
ie
)
{
LOG
.
warn
(
+
clientsString
(
)
+
+
(
elapsed
/
1000
)
+
,
ie
)
;
}
}
synchronized
(
this
)
{
if
(
id
!=
currentId
||
isRenewerExpired
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
id
!=
currentId
)
{
public
void
markZoneForRetry
(
final
Long
zoneId
)
{
final
ZoneReencryptionStatus
zs
=
zoneStatuses
.
get
(
zoneId
)
;
Preconditions
.
checkNotNull
(
zs
,
+
zoneId
)
;
public
void
markZoneStarted
(
final
Long
zoneId
)
{
final
ZoneReencryptionStatus
zs
=
zoneStatuses
.
get
(
zoneId
)
;
Preconditions
.
checkNotNull
(
zs
,
+
zoneId
)
;
public
void
markZoneCompleted
(
final
Long
zoneId
)
{
final
ZoneReencryptionStatus
zs
=
zoneStatuses
.
get
(
zoneId
)
;
Preconditions
.
checkNotNull
(
zs
,
+
zoneId
)
;
private
boolean
addZoneIfNecessary
(
final
Long
zoneId
,
final
String
name
,
final
ReencryptionInfoProto
reProto
)
{
if
(
!
zoneStatuses
.
containsKey
(
zoneId
)
)
{
public
boolean
removeZone
(
final
Long
zoneId
)
{
public
static
SaslPropertiesResolver
getSaslPropertiesResolver
(
Configuration
conf
)
{
String
qops
=
conf
.
get
(
DFS_DATA_TRANSFER_PROTECTION_KEY
)
;
if
(
qops
==
null
||
qops
.
isEmpty
(
)
)
{
private
IOStreamPair
checkTrustAndSend
(
InetAddress
addr
,
OutputStream
underlyingOut
,
InputStream
underlyingIn
,
DataEncryptionKeyFactory
encryptionKeyFactory
,
Token
<
BlockTokenIdentifier
>
accessToken
,
DatanodeID
datanodeId
,
SecretKey
secretKey
)
throws
IOException
{
boolean
localTrusted
=
trustedChannelResolver
.
isTrusted
(
)
;
boolean
remoteTrusted
=
trustedChannelResolver
.
isTrusted
(
addr
)
;
private
IOStreamPair
send
(
InetAddress
addr
,
OutputStream
underlyingOut
,
InputStream
underlyingIn
,
DataEncryptionKey
encryptionKey
,
Token
<
BlockTokenIdentifier
>
accessToken
,
DatanodeID
datanodeId
,
SecretKey
secretKey
)
throws
IOException
{
if
(
encryptionKey
!=
null
)
{
LOG
.
debug
(
+
,
addr
,
datanodeId
)
;
return
getEncryptedStreams
(
addr
,
underlyingOut
,
underlyingIn
,
encryptionKey
,
accessToken
,
secretKey
)
;
}
else
if
(
!
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
LOG
.
debug
(
+
,
addr
,
datanodeId
)
;
return
null
;
}
else
if
(
SecurityUtil
.
isPrivilegedPort
(
datanodeId
.
getXferPort
(
)
)
)
{
LOG
.
debug
(
+
,
addr
,
datanodeId
)
;
return
null
;
}
else
if
(
fallbackToSimpleAuth
!=
null
&&
fallbackToSimpleAuth
.
get
(
)
)
{
LOG
.
debug
(
+
,
addr
,
datanodeId
)
;
return
null
;
}
else
if
(
saslPropsResolver
!=
null
)
{
LOG
.
debug
(
,
addr
,
datanodeId
)
;
return
getSaslStreams
(
addr
,
underlyingOut
,
underlyingIn
,
accessToken
,
secretKey
)
;
}
else
{
static
ClientDatanodeProtocolPB
createClientDatanodeProtocolProxy
(
DatanodeID
datanodeid
,
Configuration
conf
,
int
socketTimeout
,
boolean
connectToDnViaHostname
,
LocatedBlock
locatedBlock
)
throws
IOException
{
final
String
dnAddr
=
datanodeid
.
getIpcAddr
(
connectToDnViaHostname
)
;
InetSocketAddress
addr
=
NetUtils
.
createSocketAddr
(
dnAddr
)
;
private
static
URI
getFailoverVirtualIP
(
Configuration
conf
,
String
nameServiceID
)
{
String
configKey
=
IPFAILOVER_CONFIG_PREFIX
+
+
nameServiceID
;
String
virtualIP
=
conf
.
get
(
configKey
)
;
private
void
logProxyException
(
Exception
ex
,
String
proxyInfo
)
{
if
(
isStandbyException
(
ex
)
)
{
final
String
feature
;
if
(
conf
.
isShortCircuitLocalReads
(
)
&&
(
!
conf
.
isUseLegacyBlockReaderLocal
(
)
)
)
{
feature
=
;
}
else
if
(
conf
.
isDomainSocketDataTraffic
(
)
)
{
feature
=
;
}
else
{
feature
=
null
;
}
if
(
feature
==
null
)
{
PerformanceAdvisory
.
LOG
.
debug
(
)
;
}
else
{
if
(
conf
.
getDomainSocketPath
(
)
.
isEmpty
(
)
)
{
throw
new
HadoopIllegalArgumentException
(
feature
+
+
HdfsClientConfigKeys
.
DFS_DOMAIN_SOCKET_PATH_KEY
+
)
;
}
else
if
(
DomainSocket
.
getLoadingFailureReason
(
)
!=
null
)
{
LOG
.
warn
(
feature
+
+
DomainSocket
.
getLoadingFailureReason
(
)
)
;
}
else
{
public
ShortCircuitReplicaInfo
fetchOrCreate
(
ExtendedBlockId
key
,
ShortCircuitReplicaCreator
creator
)
{
Waitable
<
ShortCircuitReplicaInfo
>
newWaitable
;
lock
.
lock
(
)
;
try
{
ShortCircuitReplicaInfo
info
=
null
;
for
(
int
i
=
0
;
i
<
FETCH_OR_CREATE_RETRY_TIMES
;
i
++
)
{
if
(
closed
)
{
LOG
.
trace
(
,
this
,
key
)
;
return
null
;
}
Waitable
<
ShortCircuitReplicaInfo
>
waitable
=
replicaInfoMap
.
get
(
key
)
;
if
(
waitable
!=
null
)
{
try
{
info
=
fetch
(
key
,
waitable
)
;
break
;
}
catch
(
RetriableException
e
)
{
info
=
waitable
.
await
(
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
info
(
this
+
+
key
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
throw
new
RetriableException
(
)
;
}
if
(
info
.
getInvalidTokenException
(
)
!=
null
)
{
LOG
.
info
(
this
+
+
key
+
+
,
info
.
getInvalidTokenException
(
)
)
;
return
info
;
}
ShortCircuitReplica
replica
=
info
.
getReplica
(
)
;
if
(
replica
==
null
)
{
LOG
.
warn
(
this
+
+
key
)
;
return
info
;
}
if
(
replica
.
purged
)
{
throw
new
RetriableException
(
+
replica
+
)
;
}
if
(
replica
.
isStale
(
)
)
{
try
{
LOG
.
trace
(
,
this
,
key
)
;
info
=
creator
.
createShortCircuitReplicaInfo
(
)
;
}
catch
(
RuntimeException
e
)
{
LOG
.
warn
(
this
+
+
key
,
e
)
;
}
if
(
info
==
null
)
info
=
new
ShortCircuitReplicaInfo
(
)
;
lock
.
lock
(
)
;
try
{
if
(
info
.
getReplica
(
)
!=
null
)
{
LOG
.
trace
(
,
this
,
info
.
getReplica
(
)
)
;
startCacheCleanerThreadIfNeeded
(
)
;
}
else
{
Waitable
<
ShortCircuitReplicaInfo
>
waitableInMap
=
replicaInfoMap
.
get
(
key
)
;
if
(
waitableInMap
==
newWaitable
)
replicaInfoMap
.
remove
(
key
)
;
if
(
info
.
getInvalidTokenException
(
)
!=
null
)
{
private
void
startCacheCleanerThreadIfNeeded
(
)
{
if
(
cacheCleaner
==
null
)
{
cacheCleaner
=
new
CacheCleaner
(
)
;
long
rateMs
=
cacheCleaner
.
getRateInMs
(
)
;
ScheduledFuture
<
?
>
future
=
cleanerExecutor
.
scheduleAtFixedRate
(
cacheCleaner
,
rateMs
,
rateMs
,
TimeUnit
.
MILLISECONDS
)
;
cacheCleaner
.
setFuture
(
future
)
;
purge
(
(
ShortCircuitReplica
)
evictable
.
get
(
eldestKey
)
)
;
}
while
(
!
evictableMmapped
.
isEmpty
(
)
)
{
Object
eldestKey
=
evictableMmapped
.
firstKey
(
)
;
purge
(
(
ShortCircuitReplica
)
evictableMmapped
.
get
(
eldestKey
)
)
;
}
}
finally
{
lock
.
unlock
(
)
;
}
releaserExecutor
.
shutdown
(
)
;
cleanerExecutor
.
shutdown
(
)
;
try
{
if
(
!
releaserExecutor
.
awaitTermination
(
30
,
TimeUnit
.
SECONDS
)
)
{
LOG
.
error
(
)
;
releaserExecutor
.
shutdownNow
(
)
;
}
}
catch
(
InterruptedException
e
)
{
releaserExecutor
.
shutdownNow
(
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
lock
.
unlock
(
)
;
}
releaserExecutor
.
shutdown
(
)
;
cleanerExecutor
.
shutdown
(
)
;
try
{
if
(
!
releaserExecutor
.
awaitTermination
(
30
,
TimeUnit
.
SECONDS
)
)
{
LOG
.
error
(
)
;
releaserExecutor
.
shutdownNow
(
)
;
}
}
catch
(
InterruptedException
e
)
{
releaserExecutor
.
shutdownNow
(
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
LOG
.
error
(
+
,
e
)
;
}
try
{
if
(
!
cleanerExecutor
.
awaitTermination
(
30
,
TimeUnit
.
SECONDS
)
)
{
LOG
.
error
(
)
;
cleanerExecutor
.
shutdownNow
(
)
;
private
static
void
logDebugMessage
(
)
{
final
StringBuilder
b
=
DEBUG_MESSAGE
.
get
(
)
;
private
List
<
ErasureCodingPolicy
>
loadECPolicies
(
File
policyFile
)
throws
ParserConfigurationException
,
IOException
,
SAXException
{
Future
<
BlockReadStats
>
future
=
null
;
try
{
if
(
timeoutMillis
>
0
)
{
future
=
readService
.
poll
(
timeoutMillis
,
TimeUnit
.
MILLISECONDS
)
;
}
else
{
future
=
readService
.
take
(
)
;
}
if
(
future
!=
null
)
{
final
BlockReadStats
stats
=
future
.
get
(
)
;
return
new
StripingChunkReadResult
(
futures
.
remove
(
future
)
,
StripingChunkReadResult
.
SUCCESSFUL
,
stats
)
;
}
else
{
return
new
StripingChunkReadResult
(
StripingChunkReadResult
.
TIMEOUT
)
;
}
}
catch
(
ExecutionException
e
)
{
LOG
.
debug
(
,
e
)
;
return
new
StripingChunkReadResult
(
futures
.
remove
(
future
)
,
StripingChunkReadResult
.
FAILED
)
;
}
catch
(
CancellationException
e
)
{
synchronized
void
ensureTokenInitialized
(
)
throws
IOException
{
if
(
!
hasInitedToken
||
(
action
!=
null
&&
!
action
.
isValid
(
)
)
)
{
Token
<
?
>
token
=
fs
.
getDelegationToken
(
null
)
;
if
(
token
!=
null
)
{
fs
.
setDelegationToken
(
token
)
;
addRenewAction
(
fs
)
;
synchronized
void
initDelegationToken
(
UserGroupInformation
ugi
)
{
Token
<
?
>
token
=
selectDelegationToken
(
ugi
)
;
if
(
token
!=
null
)
{
public
URLConnection
openConnection
(
URL
url
,
boolean
isSpnego
)
throws
IOException
,
AuthenticationException
{
if
(
isSpnego
)
{
protected
synchronized
Token
<
?
>
getDelegationToken
(
)
throws
IOException
{
if
(
delegationToken
==
null
)
{
Token
<
?
>
token
=
tokenSelector
.
selectToken
(
new
Text
(
getCanonicalServiceName
(
)
)
,
ugi
.
getTokens
(
)
)
;
if
(
token
!=
null
)
{
@
VisibleForTesting
synchronized
boolean
replaceExpiredDelegationToken
(
)
throws
IOException
{
boolean
replaced
=
false
;
if
(
canRefreshDelegationToken
)
{
Token
<
?
>
token
=
getDelegationToken
(
null
)
;
final
int
numThreads
=
10
;
final
ExecutorService
threadPool
=
newFixedThreadPool
(
numThreads
)
;
try
{
final
CountDownLatch
allReady
=
new
CountDownLatch
(
numThreads
)
;
final
CountDownLatch
startBlocker
=
new
CountDownLatch
(
1
)
;
final
CountDownLatch
allDone
=
new
CountDownLatch
(
numThreads
)
;
final
AtomicReference
<
Throwable
>
childError
=
new
AtomicReference
<
>
(
)
;
for
(
int
i
=
0
;
i
<
numThreads
;
i
++
)
{
threadPool
.
submit
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
allReady
.
countDown
(
)
;
try
{
startBlocker
.
await
(
)
;
incrementOpsCountByRandomNumbers
(
)
;
}
catch
(
Throwable
t
)
{
for
(
int
i
=
0
;
i
<
runners
.
length
;
i
++
)
{
runners
[
i
]
=
new
Runner
(
i
,
countThreshold
,
countLimit
,
pool
,
i
,
bam
)
;
threads
[
i
]
=
runners
[
i
]
.
start
(
num
)
;
}
final
List
<
Exception
>
exceptions
=
new
ArrayList
<
Exception
>
(
)
;
final
Thread
randomRecycler
=
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
LOG
.
info
(
)
;
for
(
int
i
=
0
;
shouldRun
(
)
;
i
++
)
{
final
int
j
=
ThreadLocalRandom
.
current
(
)
.
nextInt
(
runners
.
length
)
;
try
{
runners
[
j
]
.
recycle
(
)
;
}
catch
(
Exception
e
)
{
e
.
printStackTrace
(
)
;
exceptions
.
add
(
new
Exception
(
this
+
,
e
)
)
;
}
if
(
(
i
&
0xFF
)
==
0
)
{
LOG
.
info
(
+
i
)
;
sleepMs
(
100
)
;
}
}
LOG
.
info
(
)
;
}
boolean
shouldRun
(
)
{
for
(
int
i
=
0
;
i
<
runners
.
length
;
i
++
)
{
if
(
threads
[
i
]
.
isAlive
(
)
)
{
return
true
;
}
if
(
!
runners
[
i
]
.
isEmpty
(
)
)
{
return
true
;
}
}
return
false
;
}
}
;
randomRecycler
.
start
(
)
;
randomRecycler
.
join
(
)
;
Assert
.
assertTrue
(
exceptions
.
isEmpty
(
)
)
;
Assert
.
assertNull
(
counters
.
get
(
0
,
false
)
)
;
if
(
noRedirect
)
{
URI
redirectURL
=
createOpenRedirectionURL
(
uriInfo
)
;
final
String
js
=
JsonUtil
.
toJsonString
(
,
redirectURL
)
;
response
=
Response
.
ok
(
js
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
else
{
final
FSOperations
.
FSOpen
command
=
new
FSOperations
.
FSOpen
(
path
)
;
final
FileSystem
fs
=
createFileSystem
(
user
)
;
InputStream
is
=
null
;
UserGroupInformation
ugi
=
UserGroupInformation
.
createProxyUser
(
user
.
getShortUserName
(
)
,
UserGroupInformation
.
getLoginUser
(
)
)
;
try
{
is
=
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
InputStream
>
(
)
{
@
Override
public
InputStream
run
(
)
throws
Exception
{
return
command
.
execute
(
fs
)
;
}
}
)
;
}
catch
(
InterruptedException
ie
)
{
is
=
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
InputStream
>
(
)
{
@
Override
public
InputStream
run
(
)
throws
Exception
{
return
command
.
execute
(
fs
)
;
}
}
)
;
}
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
,
ie
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
}
Long
offset
=
params
.
get
(
OffsetParam
.
NAME
,
OffsetParam
.
class
)
;
Long
len
=
params
.
get
(
LenParam
.
NAME
,
LenParam
.
class
)
;
AUDIT_LOG
.
info
(
,
new
Object
[
]
{
path
,
offset
,
len
}
)
;
InputStreamEntity
entity
=
new
InputStreamEntity
(
is
,
offset
,
len
)
;
response
=
Response
.
ok
(
entity
)
.
type
(
MediaType
.
APPLICATION_OCTET_STREAM
)
.
build
(
)
;
}
break
;
}
case
GETFILESTATUS
:
{
FSOperations
.
FSFileStatus
command
=
new
FSOperations
.
FSFileStatus
(
path
)
;
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
,
ie
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
}
Long
offset
=
params
.
get
(
OffsetParam
.
NAME
,
OffsetParam
.
class
)
;
Long
len
=
params
.
get
(
LenParam
.
NAME
,
LenParam
.
class
)
;
AUDIT_LOG
.
info
(
,
new
Object
[
]
{
path
,
offset
,
len
}
)
;
InputStreamEntity
entity
=
new
InputStreamEntity
(
is
,
offset
,
len
)
;
response
=
Response
.
ok
(
entity
)
.
type
(
MediaType
.
APPLICATION_OCTET_STREAM
)
.
build
(
)
;
}
break
;
}
case
GETFILESTATUS
:
{
FSOperations
.
FSFileStatus
command
=
new
FSOperations
.
FSFileStatus
(
path
)
;
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
AUDIT_LOG
.
info
(
,
path
,
(
filter
!=
null
)
?
filter
:
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETHOMEDIRECTORY
:
{
enforceRootPath
(
op
.
value
(
)
,
path
)
;
FSOperations
.
FSHomeDir
command
=
new
FSOperations
.
FSHomeDir
(
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
INSTRUMENTATION
:
{
enforceRootPath
(
op
.
value
(
)
,
path
)
;
Groups
groups
=
HttpFSServerWebApp
.
get
(
)
.
get
(
Groups
.
class
)
;
Set
<
String
>
userGroups
=
groups
.
getGroupsSet
(
user
.
getShortUserName
(
)
)
;
if
(
!
userGroups
.
contains
(
HttpFSServerWebApp
.
get
(
)
.
getAdminGroup
(
)
)
)
{
FSOperations
.
FSHomeDir
command
=
new
FSOperations
.
FSHomeDir
(
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
INSTRUMENTATION
:
{
enforceRootPath
(
op
.
value
(
)
,
path
)
;
Groups
groups
=
HttpFSServerWebApp
.
get
(
)
.
get
(
Groups
.
class
)
;
Set
<
String
>
userGroups
=
groups
.
getGroupsSet
(
user
.
getShortUserName
(
)
)
;
if
(
!
userGroups
.
contains
(
HttpFSServerWebApp
.
get
(
)
.
getAdminGroup
(
)
)
)
{
throw
new
AccessControlException
(
)
;
}
Instrumentation
instrumentation
=
HttpFSServerWebApp
.
get
(
)
.
get
(
Instrumentation
.
class
)
;
Map
snapshot
=
instrumentation
.
getSnapshot
(
)
;
response
=
Response
.
ok
(
snapshot
)
.
build
(
)
;
break
;
case
INSTRUMENTATION
:
{
enforceRootPath
(
op
.
value
(
)
,
path
)
;
Groups
groups
=
HttpFSServerWebApp
.
get
(
)
.
get
(
Groups
.
class
)
;
Set
<
String
>
userGroups
=
groups
.
getGroupsSet
(
user
.
getShortUserName
(
)
)
;
if
(
!
userGroups
.
contains
(
HttpFSServerWebApp
.
get
(
)
.
getAdminGroup
(
)
)
)
{
throw
new
AccessControlException
(
)
;
}
Instrumentation
instrumentation
=
HttpFSServerWebApp
.
get
(
)
.
get
(
Instrumentation
.
class
)
;
Map
snapshot
=
instrumentation
.
getSnapshot
(
)
;
response
=
Response
.
ok
(
snapshot
)
.
build
(
)
;
break
;
}
case
GETCONTENTSUMMARY
:
{
FSOperations
.
FSContentSummary
command
=
new
FSOperations
.
FSContentSummary
(
path
)
;
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETQUOTAUSAGE
:
{
FSOperations
.
FSQuotaUsage
command
=
new
FSOperations
.
FSQuotaUsage
(
path
)
;
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETFILECHECKSUM
:
{
FSOperations
.
FSFileChecksum
command
=
new
FSOperations
.
FSFileChecksum
(
path
)
;
Boolean
noRedirect
=
params
.
get
(
NoRedirectParam
.
NAME
,
NoRedirectParam
.
class
)
;
AUDIT_LOG
.
info
(
,
path
)
;
if
(
noRedirect
)
{
URI
redirectURL
=
createOpenRedirectionURL
(
uriInfo
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETFILECHECKSUM
:
{
FSOperations
.
FSFileChecksum
command
=
new
FSOperations
.
FSFileChecksum
(
path
)
;
Boolean
noRedirect
=
params
.
get
(
NoRedirectParam
.
NAME
,
NoRedirectParam
.
class
)
;
AUDIT_LOG
.
info
(
,
path
)
;
if
(
noRedirect
)
{
URI
redirectURL
=
createOpenRedirectionURL
(
uriInfo
)
;
final
String
js
=
JsonUtil
.
toJsonString
(
,
redirectURL
)
;
response
=
Response
.
ok
(
js
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
else
{
Map
json
=
fsExecute
(
user
,
command
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
break
;
}
case
GETFILEBLOCKLOCATIONS
:
{
AUDIT_LOG
.
info
(
,
path
)
;
if
(
noRedirect
)
{
URI
redirectURL
=
createOpenRedirectionURL
(
uriInfo
)
;
final
String
js
=
JsonUtil
.
toJsonString
(
,
redirectURL
)
;
response
=
Response
.
ok
(
js
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
else
{
Map
json
=
fsExecute
(
user
,
command
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
break
;
}
case
GETFILEBLOCKLOCATIONS
:
{
response
=
Response
.
status
(
Response
.
Status
.
BAD_REQUEST
)
.
build
(
)
;
break
;
}
case
GETACLSTATUS
:
{
FSOperations
.
FSAclStatus
command
=
new
FSOperations
.
FSAclStatus
(
path
)
;
Map
json
=
fsExecute
(
user
,
command
)
;
break
;
}
case
GETFILEBLOCKLOCATIONS
:
{
response
=
Response
.
status
(
Response
.
Status
.
BAD_REQUEST
)
.
build
(
)
;
break
;
}
case
GETACLSTATUS
:
{
FSOperations
.
FSAclStatus
command
=
new
FSOperations
.
FSAclStatus
(
path
)
;
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETXATTRS
:
{
List
<
String
>
xattrNames
=
params
.
getValues
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
XAttrCodec
encoding
=
params
.
get
(
XAttrEncodingParam
.
NAME
,
XAttrEncodingParam
.
class
)
;
FSOperations
.
FSGetXAttrs
command
=
new
FSOperations
.
FSGetXAttrs
(
path
,
xattrNames
,
encoding
)
;
@
SuppressWarnings
(
)
Map
json
=
fsExecute
(
user
,
command
)
;
case
GETACLSTATUS
:
{
FSOperations
.
FSAclStatus
command
=
new
FSOperations
.
FSAclStatus
(
path
)
;
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETXATTRS
:
{
List
<
String
>
xattrNames
=
params
.
getValues
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
XAttrCodec
encoding
=
params
.
get
(
XAttrEncodingParam
.
NAME
,
XAttrEncodingParam
.
class
)
;
FSOperations
.
FSGetXAttrs
command
=
new
FSOperations
.
FSGetXAttrs
(
path
,
xattrNames
,
encoding
)
;
@
SuppressWarnings
(
)
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
LISTXATTRS
:
{
}
case
GETXATTRS
:
{
List
<
String
>
xattrNames
=
params
.
getValues
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
XAttrCodec
encoding
=
params
.
get
(
XAttrEncodingParam
.
NAME
,
XAttrEncodingParam
.
class
)
;
FSOperations
.
FSGetXAttrs
command
=
new
FSOperations
.
FSGetXAttrs
(
path
,
xattrNames
,
encoding
)
;
@
SuppressWarnings
(
)
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
LISTXATTRS
:
{
FSOperations
.
FSListXAttrs
command
=
new
FSOperations
.
FSListXAttrs
(
path
)
;
@
SuppressWarnings
(
)
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
LISTSTATUS_BATCH
:
{
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
LISTXATTRS
:
{
FSOperations
.
FSListXAttrs
command
=
new
FSOperations
.
FSListXAttrs
(
path
)
;
@
SuppressWarnings
(
)
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
LISTSTATUS_BATCH
:
{
String
startAfter
=
params
.
get
(
HttpFSParametersProvider
.
StartAfterParam
.
NAME
,
HttpFSParametersProvider
.
StartAfterParam
.
class
)
;
byte
[
]
token
=
HttpFSUtils
.
EMPTY_BYTES
;
if
(
startAfter
!=
null
)
{
token
=
startAfter
.
getBytes
(
Charsets
.
UTF_8
)
;
}
FSOperations
.
FSListStatusBatch
command
=
new
FSOperations
.
FSListStatusBatch
(
path
,
token
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
LISTSTATUS_BATCH
:
{
String
startAfter
=
params
.
get
(
HttpFSParametersProvider
.
StartAfterParam
.
NAME
,
HttpFSParametersProvider
.
StartAfterParam
.
class
)
;
byte
[
]
token
=
HttpFSUtils
.
EMPTY_BYTES
;
if
(
startAfter
!=
null
)
{
token
=
startAfter
.
getBytes
(
Charsets
.
UTF_8
)
;
}
FSOperations
.
FSListStatusBatch
command
=
new
FSOperations
.
FSListStatusBatch
(
path
,
token
)
;
@
SuppressWarnings
(
)
Map
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
token
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETTRASHROOT
:
{
FSOperations
.
FSTrashRoot
command
=
new
FSOperations
.
FSTrashRoot
(
path
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETSTORAGEPOLICY
:
{
FSOperations
.
FSGetStoragePolicy
command
=
new
FSOperations
.
FSGetStoragePolicy
(
path
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
GETSNAPSHOTDIFF
:
{
String
oldSnapshotName
=
params
.
get
(
OldSnapshotNameParam
.
NAME
,
OldSnapshotNameParam
.
class
)
;
String
snapshotName
=
params
.
get
(
SnapshotNameParam
.
NAME
,
SnapshotNameParam
.
class
)
;
FSOperations
.
FSGetSnapshotDiff
command
=
new
FSOperations
.
FSGetSnapshotDiff
(
path
,
oldSnapshotName
,
snapshotName
)
;
String
js
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
js
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
@
DELETE
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Response
delete
(
@
PathParam
(
)
String
path
,
@
QueryParam
(
OperationParam
.
NAME
)
OperationParam
op
,
@
Context
Parameters
params
,
@
Context
HttpServletRequest
request
)
throws
IOException
,
FileSystemAccessException
{
UserGroupInformation
user
=
HttpUserGroupInformation
.
get
(
)
;
Response
response
;
path
=
makeAbsolute
(
path
)
;
MDC
.
put
(
HttpFSFileSystem
.
OP_PARAM
,
op
.
value
(
)
.
name
(
)
)
;
MDC
.
put
(
,
request
.
getRemoteAddr
(
)
)
;
switch
(
op
.
value
(
)
)
{
case
DELETE
:
{
Boolean
recursive
=
params
.
get
(
RecursiveParam
.
NAME
,
RecursiveParam
.
class
)
;
path
=
makeAbsolute
(
path
)
;
MDC
.
put
(
HttpFSFileSystem
.
OP_PARAM
,
op
.
value
(
)
.
name
(
)
)
;
MDC
.
put
(
,
request
.
getRemoteAddr
(
)
)
;
switch
(
op
.
value
(
)
)
{
case
DELETE
:
{
Boolean
recursive
=
params
.
get
(
RecursiveParam
.
NAME
,
RecursiveParam
.
class
)
;
AUDIT_LOG
.
info
(
,
path
,
recursive
)
;
FSOperations
.
FSDelete
command
=
new
FSOperations
.
FSDelete
(
path
,
recursive
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
DELETESNAPSHOT
:
{
String
snapshotName
=
params
.
get
(
SnapshotNameParam
.
NAME
,
SnapshotNameParam
.
class
)
;
FSOperations
.
FSDeleteSnapshot
command
=
new
FSOperations
.
FSDeleteSnapshot
(
path
,
snapshotName
)
;
fsExecute
(
user
,
command
)
;
Response
response
;
path
=
makeAbsolute
(
path
)
;
MDC
.
put
(
HttpFSFileSystem
.
OP_PARAM
,
op
.
value
(
)
.
name
(
)
)
;
MDC
.
put
(
,
request
.
getRemoteAddr
(
)
)
;
switch
(
op
.
value
(
)
)
{
case
APPEND
:
{
Boolean
hasData
=
params
.
get
(
DataParam
.
NAME
,
DataParam
.
class
)
;
URI
redirectURL
=
createUploadRedirectionURL
(
uriInfo
,
HttpFSFileSystem
.
Operation
.
APPEND
)
;
Boolean
noRedirect
=
params
.
get
(
NoRedirectParam
.
NAME
,
NoRedirectParam
.
class
)
;
if
(
noRedirect
)
{
final
String
js
=
JsonUtil
.
toJsonString
(
,
redirectURL
)
;
response
=
Response
.
ok
(
js
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
else
if
(
hasData
)
{
FSOperations
.
FSAppend
command
=
new
FSOperations
.
FSAppend
(
is
,
path
)
;
fsExecute
(
user
,
command
)
;
final
String
js
=
JsonUtil
.
toJsonString
(
,
redirectURL
)
;
response
=
Response
.
ok
(
js
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
else
if
(
hasData
)
{
FSOperations
.
FSAppend
command
=
new
FSOperations
.
FSAppend
(
is
,
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
}
else
{
response
=
Response
.
temporaryRedirect
(
redirectURL
)
.
build
(
)
;
}
break
;
}
case
CONCAT
:
{
System
.
out
.
println
(
)
;
String
sources
=
params
.
get
(
SourcesParam
.
NAME
,
SourcesParam
.
class
)
;
FSOperations
.
FSConcat
command
=
new
FSOperations
.
FSConcat
(
path
,
sources
.
split
(
)
)
;
fsExecute
(
user
,
command
)
;
}
else
{
response
=
Response
.
temporaryRedirect
(
redirectURL
)
.
build
(
)
;
}
break
;
}
case
CONCAT
:
{
System
.
out
.
println
(
)
;
String
sources
=
params
.
get
(
SourcesParam
.
NAME
,
SourcesParam
.
class
)
;
FSOperations
.
FSConcat
command
=
new
FSOperations
.
FSConcat
(
path
,
sources
.
split
(
)
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
System
.
out
.
println
(
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
TRUNCATE
:
{
Long
newLength
=
params
.
get
(
NewLengthParam
.
NAME
,
NewLengthParam
.
class
)
;
FSOperations
.
FSTruncate
command
=
new
FSOperations
.
FSTruncate
(
path
,
newLength
)
;
case
CONCAT
:
{
System
.
out
.
println
(
)
;
String
sources
=
params
.
get
(
SourcesParam
.
NAME
,
SourcesParam
.
class
)
;
FSOperations
.
FSConcat
command
=
new
FSOperations
.
FSConcat
(
path
,
sources
.
split
(
)
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
System
.
out
.
println
(
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
TRUNCATE
:
{
Long
newLength
=
params
.
get
(
NewLengthParam
.
NAME
,
NewLengthParam
.
class
)
;
FSOperations
.
FSTruncate
command
=
new
FSOperations
.
FSTruncate
(
path
,
newLength
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
newLength
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
System
.
out
.
println
(
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
TRUNCATE
:
{
Long
newLength
=
params
.
get
(
NewLengthParam
.
NAME
,
NewLengthParam
.
class
)
;
FSOperations
.
FSTruncate
command
=
new
FSOperations
.
FSTruncate
(
path
,
newLength
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
newLength
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
UNSETSTORAGEPOLICY
:
{
FSOperations
.
FSUnsetStoragePolicy
command
=
new
FSOperations
.
FSUnsetStoragePolicy
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
Short
permission
=
params
.
get
(
PermissionParam
.
NAME
,
PermissionParam
.
class
)
;
Short
unmaskedPermission
=
params
.
get
(
UnmaskedPermissionParam
.
NAME
,
UnmaskedPermissionParam
.
class
)
;
Boolean
override
=
params
.
get
(
OverwriteParam
.
NAME
,
OverwriteParam
.
class
)
;
Short
replication
=
params
.
get
(
ReplicationParam
.
NAME
,
ReplicationParam
.
class
)
;
Long
blockSize
=
params
.
get
(
BlockSizeParam
.
NAME
,
BlockSizeParam
.
class
)
;
FSOperations
.
FSCreate
command
=
new
FSOperations
.
FSCreate
(
is
,
path
,
permission
,
override
,
replication
,
blockSize
,
unmaskedPermission
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
+
,
new
Object
[
]
{
path
,
permission
,
override
,
replication
,
blockSize
,
unmaskedPermission
}
)
;
final
String
js
=
JsonUtil
.
toJsonString
(
,
uriInfo
.
getAbsolutePath
(
)
)
;
response
=
Response
.
created
(
uriInfo
.
getAbsolutePath
(
)
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
js
)
.
build
(
)
;
}
else
{
response
=
Response
.
temporaryRedirect
(
redirectURL
)
.
build
(
)
;
}
break
;
}
case
ALLOWSNAPSHOT
:
{
FSOperations
.
FSAllowSnapshot
command
=
new
FSOperations
.
FSAllowSnapshot
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
+
,
new
Object
[
]
{
path
,
permission
,
override
,
replication
,
blockSize
,
unmaskedPermission
}
)
;
final
String
js
=
JsonUtil
.
toJsonString
(
,
uriInfo
.
getAbsolutePath
(
)
)
;
response
=
Response
.
created
(
uriInfo
.
getAbsolutePath
(
)
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
js
)
.
build
(
)
;
}
else
{
response
=
Response
.
temporaryRedirect
(
redirectURL
)
.
build
(
)
;
}
break
;
}
case
ALLOWSNAPSHOT
:
{
FSOperations
.
FSAllowSnapshot
command
=
new
FSOperations
.
FSAllowSnapshot
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
DISALLOWSNAPSHOT
:
{
FSOperations
.
FSDisallowSnapshot
command
=
new
FSOperations
.
FSDisallowSnapshot
(
path
)
;
}
break
;
}
case
ALLOWSNAPSHOT
:
{
FSOperations
.
FSAllowSnapshot
command
=
new
FSOperations
.
FSAllowSnapshot
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
DISALLOWSNAPSHOT
:
{
FSOperations
.
FSDisallowSnapshot
command
=
new
FSOperations
.
FSDisallowSnapshot
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
CREATESNAPSHOT
:
{
String
snapshotName
=
params
.
get
(
SnapshotNameParam
.
NAME
,
SnapshotNameParam
.
class
)
;
}
case
DISALLOWSNAPSHOT
:
{
FSOperations
.
FSDisallowSnapshot
command
=
new
FSOperations
.
FSDisallowSnapshot
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
CREATESNAPSHOT
:
{
String
snapshotName
=
params
.
get
(
SnapshotNameParam
.
NAME
,
SnapshotNameParam
.
class
)
;
FSOperations
.
FSCreateSnapshot
command
=
new
FSOperations
.
FSCreateSnapshot
(
path
,
snapshotName
)
;
String
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
snapshotName
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
SETXATTR
:
{
String
xattrName
=
params
.
get
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
case
CREATESNAPSHOT
:
{
String
snapshotName
=
params
.
get
(
SnapshotNameParam
.
NAME
,
SnapshotNameParam
.
class
)
;
FSOperations
.
FSCreateSnapshot
command
=
new
FSOperations
.
FSCreateSnapshot
(
path
,
snapshotName
)
;
String
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
snapshotName
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
SETXATTR
:
{
String
xattrName
=
params
.
get
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
String
xattrValue
=
params
.
get
(
XAttrValueParam
.
NAME
,
XAttrValueParam
.
class
)
;
EnumSet
<
XAttrSetFlag
>
flag
=
params
.
get
(
XAttrSetFlagParam
.
NAME
,
XAttrSetFlagParam
.
class
)
;
FSOperations
.
FSSetXAttr
command
=
new
FSOperations
.
FSSetXAttr
(
path
,
xattrName
,
xattrValue
,
flag
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
xattrName
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
}
case
SETXATTR
:
{
String
xattrName
=
params
.
get
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
String
xattrValue
=
params
.
get
(
XAttrValueParam
.
NAME
,
XAttrValueParam
.
class
)
;
EnumSet
<
XAttrSetFlag
>
flag
=
params
.
get
(
XAttrSetFlagParam
.
NAME
,
XAttrSetFlagParam
.
class
)
;
FSOperations
.
FSSetXAttr
command
=
new
FSOperations
.
FSSetXAttr
(
path
,
xattrName
,
xattrValue
,
flag
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
xattrName
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
RENAMESNAPSHOT
:
{
String
oldSnapshotName
=
params
.
get
(
OldSnapshotNameParam
.
NAME
,
OldSnapshotNameParam
.
class
)
;
String
snapshotName
=
params
.
get
(
SnapshotNameParam
.
NAME
,
SnapshotNameParam
.
class
)
;
FSOperations
.
FSRenameSnapshot
command
=
new
FSOperations
.
FSRenameSnapshot
(
path
,
oldSnapshotName
,
snapshotName
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
oldSnapshotName
,
snapshotName
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
RENAMESNAPSHOT
:
{
String
oldSnapshotName
=
params
.
get
(
OldSnapshotNameParam
.
NAME
,
OldSnapshotNameParam
.
class
)
;
String
snapshotName
=
params
.
get
(
SnapshotNameParam
.
NAME
,
SnapshotNameParam
.
class
)
;
FSOperations
.
FSRenameSnapshot
command
=
new
FSOperations
.
FSRenameSnapshot
(
path
,
oldSnapshotName
,
snapshotName
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
oldSnapshotName
,
snapshotName
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEXATTR
:
{
String
xattrName
=
params
.
get
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
FSOperations
.
FSRemoveXAttr
command
=
new
FSOperations
.
FSRemoveXAttr
(
path
,
xattrName
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
xattrName
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
oldSnapshotName
,
snapshotName
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEXATTR
:
{
String
xattrName
=
params
.
get
(
XAttrNameParam
.
NAME
,
XAttrNameParam
.
class
)
;
FSOperations
.
FSRemoveXAttr
command
=
new
FSOperations
.
FSRemoveXAttr
(
path
,
xattrName
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
xattrName
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
MKDIRS
:
{
Short
permission
=
params
.
get
(
PermissionParam
.
NAME
,
PermissionParam
.
class
)
;
Short
unmaskedPermission
=
params
.
get
(
UnmaskedPermissionParam
.
NAME
,
UnmaskedPermissionParam
.
class
)
;
FSOperations
.
FSMkdirs
command
=
new
FSOperations
.
FSMkdirs
(
path
,
permission
,
unmaskedPermission
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
xattrName
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
MKDIRS
:
{
Short
permission
=
params
.
get
(
PermissionParam
.
NAME
,
PermissionParam
.
class
)
;
Short
unmaskedPermission
=
params
.
get
(
UnmaskedPermissionParam
.
NAME
,
UnmaskedPermissionParam
.
class
)
;
FSOperations
.
FSMkdirs
command
=
new
FSOperations
.
FSMkdirs
(
path
,
permission
,
unmaskedPermission
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
permission
,
unmaskedPermission
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
RENAME
:
{
String
toPath
=
params
.
get
(
DestinationParam
.
NAME
,
DestinationParam
.
class
)
;
FSOperations
.
FSRename
command
=
new
FSOperations
.
FSRename
(
path
,
toPath
)
;
Short
unmaskedPermission
=
params
.
get
(
UnmaskedPermissionParam
.
NAME
,
UnmaskedPermissionParam
.
class
)
;
FSOperations
.
FSMkdirs
command
=
new
FSOperations
.
FSMkdirs
(
path
,
permission
,
unmaskedPermission
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
permission
,
unmaskedPermission
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
RENAME
:
{
String
toPath
=
params
.
get
(
DestinationParam
.
NAME
,
DestinationParam
.
class
)
;
FSOperations
.
FSRename
command
=
new
FSOperations
.
FSRename
(
path
,
toPath
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
toPath
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
SETOWNER
:
{
String
owner
=
params
.
get
(
OwnerParam
.
NAME
,
OwnerParam
.
class
)
;
case
RENAME
:
{
String
toPath
=
params
.
get
(
DestinationParam
.
NAME
,
DestinationParam
.
class
)
;
FSOperations
.
FSRename
command
=
new
FSOperations
.
FSRename
(
path
,
toPath
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
toPath
)
;
response
=
Response
.
ok
(
json
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
build
(
)
;
break
;
}
case
SETOWNER
:
{
String
owner
=
params
.
get
(
OwnerParam
.
NAME
,
OwnerParam
.
class
)
;
String
group
=
params
.
get
(
GroupParam
.
NAME
,
GroupParam
.
class
)
;
FSOperations
.
FSSetOwner
command
=
new
FSOperations
.
FSSetOwner
(
path
,
owner
,
group
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
owner
+
+
group
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
case
SETOWNER
:
{
String
owner
=
params
.
get
(
OwnerParam
.
NAME
,
OwnerParam
.
class
)
;
String
group
=
params
.
get
(
GroupParam
.
NAME
,
GroupParam
.
class
)
;
FSOperations
.
FSSetOwner
command
=
new
FSOperations
.
FSSetOwner
(
path
,
owner
,
group
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
owner
+
+
group
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
SETPERMISSION
:
{
Short
permission
=
params
.
get
(
PermissionParam
.
NAME
,
PermissionParam
.
class
)
;
FSOperations
.
FSSetPermission
command
=
new
FSOperations
.
FSSetPermission
(
path
,
permission
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
permission
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
break
;
}
case
SETPERMISSION
:
{
Short
permission
=
params
.
get
(
PermissionParam
.
NAME
,
PermissionParam
.
class
)
;
FSOperations
.
FSSetPermission
command
=
new
FSOperations
.
FSSetPermission
(
path
,
permission
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
permission
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
SETREPLICATION
:
{
Short
replication
=
params
.
get
(
ReplicationParam
.
NAME
,
ReplicationParam
.
class
)
;
FSOperations
.
FSSetReplication
command
=
new
FSOperations
.
FSSetReplication
(
path
,
replication
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
replication
)
;
response
=
Response
.
ok
(
json
)
.
build
(
)
;
break
;
AUDIT_LOG
.
info
(
,
path
,
permission
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
SETREPLICATION
:
{
Short
replication
=
params
.
get
(
ReplicationParam
.
NAME
,
ReplicationParam
.
class
)
;
FSOperations
.
FSSetReplication
command
=
new
FSOperations
.
FSSetReplication
(
path
,
replication
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
replication
)
;
response
=
Response
.
ok
(
json
)
.
build
(
)
;
break
;
}
case
SETTIMES
:
{
Long
modifiedTime
=
params
.
get
(
ModifiedTimeParam
.
NAME
,
ModifiedTimeParam
.
class
)
;
Long
accessTime
=
params
.
get
(
AccessTimeParam
.
NAME
,
AccessTimeParam
.
class
)
;
FSOperations
.
FSSetTimes
command
=
new
FSOperations
.
FSSetTimes
(
path
,
modifiedTime
,
accessTime
)
;
fsExecute
(
user
,
command
)
;
JSONObject
json
=
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
replication
)
;
response
=
Response
.
ok
(
json
)
.
build
(
)
;
break
;
}
case
SETTIMES
:
{
Long
modifiedTime
=
params
.
get
(
ModifiedTimeParam
.
NAME
,
ModifiedTimeParam
.
class
)
;
Long
accessTime
=
params
.
get
(
AccessTimeParam
.
NAME
,
AccessTimeParam
.
class
)
;
FSOperations
.
FSSetTimes
command
=
new
FSOperations
.
FSSetTimes
(
path
,
modifiedTime
,
accessTime
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
modifiedTime
+
+
accessTime
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
SETACL
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
FSOperations
.
FSSetAcl
command
=
new
FSOperations
.
FSSetAcl
(
path
,
aclSpec
)
;
Long
accessTime
=
params
.
get
(
AccessTimeParam
.
NAME
,
AccessTimeParam
.
class
)
;
FSOperations
.
FSSetTimes
command
=
new
FSOperations
.
FSSetTimes
(
path
,
modifiedTime
,
accessTime
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
modifiedTime
+
+
accessTime
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
SETACL
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
FSOperations
.
FSSetAcl
command
=
new
FSOperations
.
FSSetAcl
(
path
,
aclSpec
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
aclSpec
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEACL
:
{
FSOperations
.
FSRemoveAcl
command
=
new
FSOperations
.
FSRemoveAcl
(
path
)
;
}
case
SETACL
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
FSOperations
.
FSSetAcl
command
=
new
FSOperations
.
FSSetAcl
(
path
,
aclSpec
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
aclSpec
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEACL
:
{
FSOperations
.
FSRemoveAcl
command
=
new
FSOperations
.
FSRemoveAcl
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
MODIFYACLENTRIES
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
break
;
}
case
REMOVEACL
:
{
FSOperations
.
FSRemoveAcl
command
=
new
FSOperations
.
FSRemoveAcl
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
MODIFYACLENTRIES
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
FSOperations
.
FSModifyAclEntries
command
=
new
FSOperations
.
FSModifyAclEntries
(
path
,
aclSpec
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
aclSpec
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEACLENTRIES
:
{
break
;
}
case
MODIFYACLENTRIES
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
FSOperations
.
FSModifyAclEntries
command
=
new
FSOperations
.
FSModifyAclEntries
(
path
,
aclSpec
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
aclSpec
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEACLENTRIES
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
FSOperations
.
FSRemoveAclEntries
command
=
new
FSOperations
.
FSRemoveAclEntries
(
path
,
aclSpec
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
aclSpec
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
AUDIT_LOG
.
info
(
,
path
,
aclSpec
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEACLENTRIES
:
{
String
aclSpec
=
params
.
get
(
AclPermissionParam
.
NAME
,
AclPermissionParam
.
class
)
;
FSOperations
.
FSRemoveAclEntries
command
=
new
FSOperations
.
FSRemoveAclEntries
(
path
,
aclSpec
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
,
aclSpec
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
break
;
}
case
REMOVEDEFAULTACL
:
{
FSOperations
.
FSRemoveDefaultAcl
command
=
new
FSOperations
.
FSRemoveDefaultAcl
(
path
)
;
fsExecute
(
user
,
command
)
;
AUDIT_LOG
.
info
(
,
path
)
;
response
=
Response
.
ok
(
)
.
build
(
)
;
public
void
init
(
)
throws
ServerException
{
if
(
status
!=
Status
.
UNDEF
)
{
throw
new
IllegalStateException
(
)
;
}
status
=
Status
.
BOOTING
;
verifyDir
(
homeDir
)
;
verifyDir
(
tempDir
)
;
Properties
serverInfo
=
new
Properties
(
)
;
try
{
InputStream
is
=
getResource
(
name
+
)
;
serverInfo
.
load
(
is
)
;
is
.
close
(
)
;
}
catch
(
IOException
ex
)
{
throw
new
RuntimeException
(
+
name
+
)
;
}
initLog
(
)
;
log
.
info
(
)
;
throw
new
IllegalStateException
(
)
;
}
status
=
Status
.
BOOTING
;
verifyDir
(
homeDir
)
;
verifyDir
(
tempDir
)
;
Properties
serverInfo
=
new
Properties
(
)
;
try
{
InputStream
is
=
getResource
(
name
+
)
;
serverInfo
.
load
(
is
)
;
is
.
close
(
)
;
}
catch
(
IOException
ex
)
{
throw
new
RuntimeException
(
+
name
+
)
;
}
initLog
(
)
;
log
.
info
(
)
;
log
.
info
(
,
name
)
;
log
.
info
(
)
;
}
status
=
Status
.
BOOTING
;
verifyDir
(
homeDir
)
;
verifyDir
(
tempDir
)
;
Properties
serverInfo
=
new
Properties
(
)
;
try
{
InputStream
is
=
getResource
(
name
+
)
;
serverInfo
.
load
(
is
)
;
is
.
close
(
)
;
}
catch
(
IOException
ex
)
{
throw
new
RuntimeException
(
+
name
+
)
;
}
initLog
(
)
;
log
.
info
(
)
;
log
.
info
(
,
name
)
;
log
.
info
(
)
;
log
.
info
(
,
serverInfo
.
getProperty
(
name
+
,
)
)
;
status
=
Status
.
BOOTING
;
verifyDir
(
homeDir
)
;
verifyDir
(
tempDir
)
;
Properties
serverInfo
=
new
Properties
(
)
;
try
{
InputStream
is
=
getResource
(
name
+
)
;
serverInfo
.
load
(
is
)
;
is
.
close
(
)
;
}
catch
(
IOException
ex
)
{
throw
new
RuntimeException
(
+
name
+
)
;
}
initLog
(
)
;
log
.
info
(
)
;
log
.
info
(
,
name
)
;
log
.
info
(
)
;
log
.
info
(
,
serverInfo
.
getProperty
(
name
+
,
)
)
;
verifyDir
(
homeDir
)
;
verifyDir
(
tempDir
)
;
Properties
serverInfo
=
new
Properties
(
)
;
try
{
InputStream
is
=
getResource
(
name
+
)
;
serverInfo
.
load
(
is
)
;
is
.
close
(
)
;
}
catch
(
IOException
ex
)
{
throw
new
RuntimeException
(
+
name
+
)
;
}
initLog
(
)
;
log
.
info
(
)
;
log
.
info
(
,
name
)
;
log
.
info
(
)
;
log
.
info
(
,
serverInfo
.
getProperty
(
name
+
,
)
)
;
log
.
info
(
,
serverInfo
.
getProperty
(
name
+
,
)
)
;
verifyDir
(
tempDir
)
;
Properties
serverInfo
=
new
Properties
(
)
;
try
{
InputStream
is
=
getResource
(
name
+
)
;
serverInfo
.
load
(
is
)
;
is
.
close
(
)
;
}
catch
(
IOException
ex
)
{
throw
new
RuntimeException
(
+
name
+
)
;
}
initLog
(
)
;
log
.
info
(
)
;
log
.
info
(
,
name
)
;
log
.
info
(
)
;
log
.
info
(
,
serverInfo
.
getProperty
(
name
+
,
)
)
;
log
.
info
(
,
serverInfo
.
getProperty
(
name
+
,
)
)
;
log
.
info
(
,
serverInfo
.
getProperty
(
name
+
,
)
)
;
else
{
try
{
defaultConf
=
new
Configuration
(
false
)
;
ConfigurationUtils
.
load
(
defaultConf
,
inputStream
)
;
}
catch
(
Exception
ex
)
{
throw
new
ServerException
(
ServerException
.
ERROR
.
S03
,
defaultConfig
,
ex
.
getMessage
(
)
,
ex
)
;
}
}
if
(
config
==
null
)
{
Configuration
siteConf
;
File
siteFile
=
new
File
(
file
,
name
+
)
;
if
(
!
siteFile
.
exists
(
)
)
{
log
.
warn
(
,
siteFile
)
;
siteConf
=
new
Configuration
(
false
)
;
}
else
{
if
(
!
siteFile
.
isFile
(
)
)
{
throw
new
ServerException
(
ServerException
.
ERROR
.
S05
,
siteFile
.
getAbsolutePath
(
)
)
;
}
else
{
if
(
!
siteFile
.
isFile
(
)
)
{
throw
new
ServerException
(
ServerException
.
ERROR
.
S05
,
siteFile
.
getAbsolutePath
(
)
)
;
}
try
{
log
.
debug
(
,
siteFile
)
;
inputStream
=
Files
.
newInputStream
(
siteFile
.
toPath
(
)
)
;
siteConf
=
new
Configuration
(
false
)
;
ConfigurationUtils
.
load
(
siteConf
,
inputStream
)
;
}
catch
(
IOException
ex
)
{
throw
new
ServerException
(
ServerException
.
ERROR
.
S06
,
siteFile
,
ex
.
getMessage
(
)
,
ex
)
;
}
}
config
=
new
Configuration
(
false
)
;
ConfigurationUtils
.
copy
(
siteConf
,
config
)
;
}
ConfigurationUtils
.
injectDefaults
(
defaultConf
,
config
)
;
ConfigRedactor
redactor
=
new
ConfigRedactor
(
config
)
;
for
(
String
name
:
System
.
getProperties
(
)
.
stringPropertyNames
(
)
)
{
inputStream
=
Files
.
newInputStream
(
siteFile
.
toPath
(
)
)
;
siteConf
=
new
Configuration
(
false
)
;
ConfigurationUtils
.
load
(
siteConf
,
inputStream
)
;
}
catch
(
IOException
ex
)
{
throw
new
ServerException
(
ServerException
.
ERROR
.
S06
,
siteFile
,
ex
.
getMessage
(
)
,
ex
)
;
}
}
config
=
new
Configuration
(
false
)
;
ConfigurationUtils
.
copy
(
siteConf
,
config
)
;
}
ConfigurationUtils
.
injectDefaults
(
defaultConf
,
config
)
;
ConfigRedactor
redactor
=
new
ConfigRedactor
(
config
)
;
for
(
String
name
:
System
.
getProperties
(
)
.
stringPropertyNames
(
)
)
{
String
value
=
System
.
getProperty
(
name
)
;
if
(
name
.
startsWith
(
getPrefix
(
)
+
)
)
{
config
.
set
(
name
,
value
)
;
String
redacted
=
redactor
.
redact
(
name
,
value
)
;
log
.
info
(
,
name
,
redacted
)
;
private
void
loadServices
(
Class
[
]
classes
,
List
<
Service
>
list
)
throws
ServerException
{
for
(
Class
klass
:
classes
)
{
try
{
Service
service
=
(
Service
)
klass
.
newInstance
(
)
;
protected
List
<
Service
>
loadServices
(
)
throws
ServerException
{
try
{
Map
<
Class
,
Service
>
map
=
new
LinkedHashMap
<
Class
,
Service
>
(
)
;
Class
[
]
classes
=
getConfig
(
)
.
getClasses
(
getPrefixedName
(
CONF_SERVICES
)
)
;
Class
[
]
classesExt
=
getConfig
(
)
.
getClasses
(
getPrefixedName
(
CONF_SERVICES_EXT
)
)
;
List
<
Service
>
list
=
new
ArrayList
<
Service
>
(
)
;
loadServices
(
classes
,
list
)
;
loadServices
(
classesExt
,
list
)
;
for
(
Service
service
:
list
)
{
if
(
map
.
containsKey
(
service
.
getInterface
(
)
)
)
{
protected
void
initServices
(
List
<
Service
>
services
)
throws
ServerException
{
for
(
Service
service
:
services
)
{
protected
void
destroyServices
(
)
{
List
<
Service
>
list
=
new
ArrayList
<
Service
>
(
services
.
values
(
)
)
;
Collections
.
reverse
(
list
)
;
for
(
Service
service
:
list
)
{
try
{
ensureOperational
(
)
;
Check
.
notNull
(
klass
,
)
;
if
(
getStatus
(
)
==
Status
.
SHUTTING_DOWN
)
{
throw
new
IllegalStateException
(
)
;
}
try
{
Service
newService
=
klass
.
newInstance
(
)
;
Service
oldService
=
services
.
get
(
newService
.
getInterface
(
)
)
;
if
(
oldService
!=
null
)
{
try
{
oldService
.
destroy
(
)
;
}
catch
(
Throwable
ex
)
{
log
.
error
(
,
new
Object
[
]
{
oldService
.
getInterface
(
)
,
ex
.
getMessage
(
)
,
ex
}
)
;
}
}
newService
.
init
(
this
)
;
services
.
put
(
newService
.
getInterface
(
)
,
newService
)
;
}
catch
(
Exception
ex
)
{
@
Override
public
void
schedule
(
final
Callable
<
?
>
callable
,
long
delay
,
long
interval
,
TimeUnit
unit
)
{
Check
.
notNull
(
callable
,
)
;
if
(
!
scheduler
.
isShutdown
(
)
)
{
LOG
.
debug
(
,
new
Object
[
]
{
callable
,
delay
,
interval
,
unit
}
)
;
Runnable
r
=
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
String
instrName
=
callable
.
getClass
(
)
.
getSimpleName
(
)
;
Instrumentation
instr
=
getServer
(
)
.
get
(
Instrumentation
.
class
)
;
if
(
getServer
(
)
.
getStatus
(
)
==
Server
.
Status
.
HALTED
)
{
protected
void
log
(
Response
.
Status
status
,
Throwable
throwable
)
{
private
static
void
execWaitRet
(
String
cmd
)
throws
IOException
{
private
static
void
execIgnoreRet
(
String
cmd
)
throws
IOException
{
private
static
void
execAssertSucceeds
(
String
cmd
)
throws
IOException
{
private
static
void
execAssertFails
(
String
cmd
)
throws
IOException
{
private
static
Process
establishMount
(
URI
uri
)
throws
IOException
{
Runtime
r
=
Runtime
.
getRuntime
(
)
;
String
cp
=
System
.
getProperty
(
)
;
String
buildTestDir
=
System
.
getProperty
(
)
;
String
fuseCmd
=
buildTestDir
+
;
String
libHdfs
=
buildTestDir
+
;
String
arch
=
System
.
getProperty
(
)
;
String
jvm
=
System
.
getProperty
(
)
+
+
arch
+
;
String
lp
=
System
.
getProperty
(
)
+
+
libHdfs
+
+
jvm
;
private
void
addExports
(
)
throws
IOException
{
FileSystem
fs
=
FileSystem
.
get
(
config
)
;
String
[
]
exportsPath
=
config
.
getStrings
(
NfsConfigKeys
.
DFS_NFS_EXPORT_POINT_KEY
,
NfsConfigKeys
.
DFS_NFS_EXPORT_POINT_DEFAULT
)
;
for
(
String
exportPath
:
exportsPath
)
{
URI
exportURI
=
Nfs3Utils
.
getResolvedURI
(
fs
,
exportPath
)
;
@
Override
public
XDR
nullOp
(
XDR
out
,
int
xid
,
InetAddress
client
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
return
MountResponse
.
writeMNTResponse
(
Nfs3Status
.
NFS3ERR_ACCES
,
out
,
xid
,
null
)
;
}
String
path
=
xdr
.
readString
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
path
+
+
client
)
;
}
String
host
=
client
.
getHostName
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
host
+
+
path
)
;
}
URI
exportURI
=
exports
.
get
(
path
)
;
if
(
exportURI
==
null
)
{
LOG
.
info
(
+
path
+
)
;
MountResponse
.
writeMNTResponse
(
Nfs3Status
.
NFS3ERR_NOENT
,
out
,
xid
,
null
)
;
return
out
;
}
DFSClient
dfsClient
=
null
;
try
{
dfsClient
=
new
DFSClient
(
exportURI
,
config
)
;
LOG
.
debug
(
+
host
+
+
path
)
;
}
URI
exportURI
=
exports
.
get
(
path
)
;
if
(
exportURI
==
null
)
{
LOG
.
info
(
+
path
+
)
;
MountResponse
.
writeMNTResponse
(
Nfs3Status
.
NFS3ERR_NOENT
,
out
,
xid
,
null
)
;
return
out
;
}
DFSClient
dfsClient
=
null
;
try
{
dfsClient
=
new
DFSClient
(
exportURI
,
config
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
path
,
e
)
;
MountResponse
.
writeMNTResponse
(
Nfs3Status
.
NFS3ERR_NOENT
,
out
,
xid
,
null
)
;
return
out
;
}
FileHandle
handle
=
null
;
try
{
LOG
.
info
(
+
path
+
)
;
MountResponse
.
writeMNTResponse
(
Nfs3Status
.
NFS3ERR_NOENT
,
out
,
xid
,
null
)
;
return
out
;
}
DFSClient
dfsClient
=
null
;
try
{
dfsClient
=
new
DFSClient
(
exportURI
,
config
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
path
,
e
)
;
MountResponse
.
writeMNTResponse
(
Nfs3Status
.
NFS3ERR_NOENT
,
out
,
xid
,
null
)
;
return
out
;
}
FileHandle
handle
=
null
;
try
{
HdfsFileStatus
exFileStatus
=
dfsClient
.
getFileInfo
(
exportURI
.
getPath
(
)
)
;
handle
=
new
FileHandle
(
exFileStatus
.
getFileId
(
)
,
Nfs3Utils
.
getNamenodeId
(
config
,
exportURI
)
)
;
}
catch
(
IOException
e
)
{
@
Override
public
XDR
dump
(
XDR
out
,
int
xid
,
InetAddress
client
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
XDR
umnt
(
XDR
xdr
,
XDR
out
,
int
xid
,
InetAddress
client
)
{
String
path
=
xdr
.
readString
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
XDR
umntall
(
XDR
out
,
int
xid
,
InetAddress
client
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
prepareAddressMap
(
)
throws
IOException
{
FileSystem
fs
=
FileSystem
.
get
(
config
)
;
String
[
]
exportsPath
=
config
.
getStrings
(
NfsConfigKeys
.
DFS_NFS_EXPORT_POINT_KEY
,
NfsConfigKeys
.
DFS_NFS_EXPORT_POINT_DEFAULT
)
;
for
(
String
exportPath
:
exportsPath
)
{
URI
exportURI
=
Nfs3Utils
.
getResolvedURI
(
fs
,
exportPath
)
;
int
namenodeId
=
Nfs3Utils
.
getNamenodeId
(
config
,
exportURI
)
;
URI
value
=
namenodeUriMap
.
get
(
namenodeId
)
;
if
(
value
==
null
)
{
UserGroupInformation
getUserGroupInformation
(
String
effectiveUser
,
UserGroupInformation
realUser
)
throws
IOException
{
Preconditions
.
checkNotNull
(
effectiveUser
)
;
Preconditions
.
checkNotNull
(
realUser
)
;
realUser
.
checkTGTAndReloginFromKeytab
(
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
createProxyUser
(
effectiveUser
,
realUser
)
;
private
long
updateNonSequentialWriteInMemory
(
long
count
)
{
long
newValue
=
nonSequentialWriteInMemory
.
addAndGet
(
count
)
;
@
VisibleForTesting
public
static
void
alterWriteRequest
(
WRITE3Request
request
,
long
cachedOffset
)
{
long
offset
=
request
.
getOffset
(
)
;
int
count
=
request
.
getCount
(
)
;
long
smallerCount
=
offset
+
count
-
cachedOffset
;
private
synchronized
WriteCtx
addWritesToCache
(
WRITE3Request
request
,
Channel
channel
,
int
xid
)
{
long
offset
=
request
.
getOffset
(
)
;
int
count
=
request
.
getCount
(
)
;
long
cachedOffset
=
nextOffset
.
get
(
)
;
int
originalCount
=
WriteCtx
.
INVALID_ORIGINAL_COUNT
;
LOG
.
warn
(
String
.
format
(
+
+
,
offset
,
(
offset
+
count
)
,
cachedOffset
)
)
;
return
null
;
}
if
(
(
offset
<
cachedOffset
)
&&
(
offset
+
count
>
cachedOffset
)
)
{
LOG
.
warn
(
String
.
format
(
+
+
+
,
offset
,
(
offset
+
count
)
,
cachedOffset
,
offset
,
cachedOffset
,
cachedOffset
,
(
offset
+
count
)
)
)
;
LOG
.
warn
(
)
;
alterWriteRequest
(
request
,
cachedOffset
)
;
originalCount
=
count
;
offset
=
request
.
getOffset
(
)
;
count
=
request
.
getCount
(
)
;
}
if
(
offset
<
cachedOffset
)
{
LOG
.
warn
(
,
offset
,
count
,
nextOffset
)
;
return
null
;
}
else
{
DataState
dataState
=
offset
==
cachedOffset
?
WriteCtx
.
DataState
.
NO_DUMP
:
WriteCtx
.
DataState
.
ALLOW_DUMP
;
WriteCtx
writeCtx
=
new
WriteCtx
(
request
.
getHandle
(
)
,
request
.
getOffset
(
)
,
request
.
getCount
(
)
,
originalCount
,
request
.
getStableHow
(
)
,
request
.
getData
(
)
,
channel
,
xid
,
false
,
dataState
)
;
alterWriteRequest
(
request
,
cachedOffset
)
;
originalCount
=
count
;
offset
=
request
.
getOffset
(
)
;
count
=
request
.
getCount
(
)
;
}
if
(
offset
<
cachedOffset
)
{
LOG
.
warn
(
,
offset
,
count
,
nextOffset
)
;
return
null
;
}
else
{
DataState
dataState
=
offset
==
cachedOffset
?
WriteCtx
.
DataState
.
NO_DUMP
:
WriteCtx
.
DataState
.
ALLOW_DUMP
;
WriteCtx
writeCtx
=
new
WriteCtx
(
request
.
getHandle
(
)
,
request
.
getOffset
(
)
,
request
.
getCount
(
)
,
originalCount
,
request
.
getStableHow
(
)
,
request
.
getData
(
)
,
channel
,
xid
,
false
,
dataState
)
;
LOG
.
debug
(
+
,
cachedOffset
,
offset
)
;
if
(
writeCtx
.
getDataState
(
)
==
WriteCtx
.
DataState
.
ALLOW_DUMP
)
{
updateNonSequentialWriteInMemory
(
count
)
;
}
WriteCtx
oldWriteCtx
=
checkRepeatedWriteRequest
(
request
,
channel
,
xid
)
;
if
(
oldWriteCtx
==
null
)
{
private
synchronized
boolean
checkAndStartWrite
(
AsyncDataService
asyncDataService
,
WriteCtx
writeCtx
)
{
if
(
writeCtx
.
getOffset
(
)
==
nextOffset
.
get
(
)
)
{
if
(
!
asyncStatus
)
{
WRITE3Response
response
;
byte
[
]
readbuffer
=
new
byte
[
count
]
;
int
readCount
=
0
;
FSDataInputStream
fis
=
null
;
try
{
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
}
catch
(
ClosedChannelException
closedException
)
{
LOG
.
info
(
+
)
;
}
catch
(
IOException
e
)
{
LOG
.
info
(
+
,
path
,
e
.
toString
(
)
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
wccData
,
0
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
}
try
{
fis
=
dfsClient
.
createWrappedInputStream
(
dfsClient
.
open
(
path
)
)
;
readCount
=
fis
.
read
(
offset
,
readbuffer
,
0
,
count
)
;
if
(
readCount
<
count
)
{
FSDataInputStream
fis
=
null
;
try
{
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
}
catch
(
ClosedChannelException
closedException
)
{
LOG
.
info
(
+
)
;
}
catch
(
IOException
e
)
{
LOG
.
info
(
+
,
path
,
e
.
toString
(
)
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
wccData
,
0
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
}
try
{
fis
=
dfsClient
.
createWrappedInputStream
(
dfsClient
.
open
(
path
)
)
;
readCount
=
fis
.
read
(
offset
,
readbuffer
,
0
,
count
)
;
if
(
readCount
<
count
)
{
LOG
.
error
(
,
count
,
readCount
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
wccData
,
0
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
count
,
readCount
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
wccData
,
0
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
info
(
+
,
path
,
e
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
wccData
,
0
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
}
finally
{
IOUtils
.
cleanupWithLogger
(
LOG
,
fis
)
;
}
Comparator
comparator
=
new
Comparator
(
)
;
if
(
comparator
.
compare
(
readbuffer
,
0
,
readCount
,
data
,
0
,
count
)
!=
0
)
{
LOG
.
info
(
)
;
response
=
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_INVAL
,
wccData
,
0
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
}
else
{
LOG
.
info
(
+
)
;
Nfs3FileAttributes
postOpAttr
=
null
;
try
{
Preconditions
.
checkState
(
channel
!=
null
&&
preOpAttr
!=
null
)
;
updateLastAccessTime
(
)
;
}
Preconditions
.
checkState
(
commitOffset
>=
0
)
;
COMMIT_STATUS
ret
=
checkCommitInternal
(
commitOffset
,
channel
,
xid
,
preOpAttr
,
fromRead
)
;
LOG
.
debug
(
,
ret
.
name
(
)
)
;
if
(
ret
==
COMMIT_STATUS
.
COMMIT_DO_SYNC
||
ret
==
COMMIT_STATUS
.
COMMIT_FINISHED
)
{
try
{
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
ret
=
COMMIT_STATUS
.
COMMIT_FINISHED
;
}
catch
(
ClosedChannelException
cce
)
{
if
(
pendingWrites
.
isEmpty
(
)
)
{
ret
=
COMMIT_STATUS
.
COMMIT_FINISHED
;
}
else
{
ret
=
COMMIT_STATUS
.
COMMIT_ERROR
;
}
}
catch
(
IOException
e
)
{
if
(
pendingWrites
.
isEmpty
(
)
)
{
LOG
.
debug
(
,
latestAttr
.
getFileId
(
)
)
;
processCommits
(
nextOffset
.
get
(
)
)
;
this
.
asyncStatus
=
false
;
return
null
;
}
Entry
<
OffsetRange
,
WriteCtx
>
lastEntry
=
pendingWrites
.
lastEntry
(
)
;
OffsetRange
range
=
lastEntry
.
getKey
(
)
;
WriteCtx
toWrite
=
lastEntry
.
getValue
(
)
;
LOG
.
trace
(
,
range
.
getMin
(
)
,
nextOffset
)
;
long
offset
=
nextOffset
.
get
(
)
;
if
(
range
.
getMin
(
)
>
offset
)
{
LOG
.
debug
(
)
;
processCommits
(
nextOffset
.
get
(
)
)
;
this
.
asyncStatus
=
false
;
}
else
if
(
range
.
getMax
(
)
<=
offset
)
{
OffsetRange
range
=
lastEntry
.
getKey
(
)
;
WriteCtx
toWrite
=
lastEntry
.
getValue
(
)
;
LOG
.
trace
(
,
range
.
getMin
(
)
,
nextOffset
)
;
long
offset
=
nextOffset
.
get
(
)
;
if
(
range
.
getMin
(
)
>
offset
)
{
LOG
.
debug
(
)
;
processCommits
(
nextOffset
.
get
(
)
)
;
this
.
asyncStatus
=
false
;
}
else
if
(
range
.
getMax
(
)
<=
offset
)
{
LOG
.
debug
(
,
range
)
;
pendingWrites
.
remove
(
range
)
;
}
else
if
(
range
.
getMin
(
)
<
offset
&&
range
.
getMax
(
)
>
offset
)
{
LOG
.
warn
(
+
,
range
,
offset
)
;
pendingWrites
.
remove
(
range
)
;
trimWriteRequest
(
toWrite
,
offset
)
;
long
offset
=
nextOffset
.
get
(
)
;
if
(
range
.
getMin
(
)
>
offset
)
{
LOG
.
debug
(
)
;
processCommits
(
nextOffset
.
get
(
)
)
;
this
.
asyncStatus
=
false
;
}
else
if
(
range
.
getMax
(
)
<=
offset
)
{
LOG
.
debug
(
,
range
)
;
pendingWrites
.
remove
(
range
)
;
}
else
if
(
range
.
getMin
(
)
<
offset
&&
range
.
getMax
(
)
>
offset
)
{
LOG
.
warn
(
+
,
range
,
offset
)
;
pendingWrites
.
remove
(
range
)
;
trimWriteRequest
(
toWrite
,
offset
)
;
nextOffset
.
addAndGet
(
toWrite
.
getCount
(
)
)
;
LOG
.
debug
(
,
nextOffset
.
get
(
)
)
;
return
toWrite
;
processCommits
(
nextOffset
.
get
(
)
)
;
this
.
asyncStatus
=
false
;
}
else
if
(
range
.
getMax
(
)
<=
offset
)
{
LOG
.
debug
(
,
range
)
;
pendingWrites
.
remove
(
range
)
;
}
else
if
(
range
.
getMin
(
)
<
offset
&&
range
.
getMax
(
)
>
offset
)
{
LOG
.
warn
(
+
,
range
,
offset
)
;
pendingWrites
.
remove
(
range
)
;
trimWriteRequest
(
toWrite
,
offset
)
;
nextOffset
.
addAndGet
(
toWrite
.
getCount
(
)
)
;
LOG
.
debug
(
,
nextOffset
.
get
(
)
)
;
return
toWrite
;
}
else
{
LOG
.
debug
(
,
range
)
;
pendingWrites
.
remove
(
range
)
;
try
{
while
(
activeState
)
{
WriteCtx
toWrite
=
offerNextToWrite
(
)
;
if
(
toWrite
!=
null
)
{
doSingleWrite
(
toWrite
)
;
updateLastAccessTime
(
)
;
}
else
{
break
;
}
}
if
(
!
activeState
)
{
LOG
.
debug
(
,
latestAttr
.
getFileId
(
)
)
;
}
}
finally
{
synchronized
(
this
)
{
if
(
startOffset
==
asyncWriteBackStartOffset
)
{
asyncStatus
=
false
;
}
else
{
private
void
processCommits
(
long
offset
)
{
Preconditions
.
checkState
(
offset
>
0
)
;
long
flushedOffset
=
getFlushedOffset
(
)
;
Entry
<
Long
,
CommitCtx
>
entry
=
pendingCommits
.
firstEntry
(
)
;
if
(
entry
==
null
||
entry
.
getValue
(
)
.
offset
>
flushedOffset
)
{
return
;
}
int
status
=
Nfs3Status
.
NFS3ERR_IO
;
try
{
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
status
=
Nfs3Status
.
NFS3_OK
;
}
catch
(
ClosedChannelException
cce
)
{
if
(
!
pendingWrites
.
isEmpty
(
)
)
{
LOG
.
error
(
+
,
latestAttr
.
getFileId
(
)
,
cce
)
;
}
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
catch
(
IOException
e
)
{
return
;
}
int
status
=
Nfs3Status
.
NFS3ERR_IO
;
try
{
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
status
=
Nfs3Status
.
NFS3_OK
;
}
catch
(
ClosedChannelException
cce
)
{
if
(
!
pendingWrites
.
isEmpty
(
)
)
{
LOG
.
error
(
+
,
latestAttr
.
getFileId
(
)
,
cce
)
;
}
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
try
{
latestAttr
=
Nfs3Utils
.
getFileAttr
(
client
,
Nfs3Utils
.
getFileIdPath
(
latestAttr
.
getFileId
(
)
)
,
iug
)
;
}
catch
(
IOException
e
)
{
try
{
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
status
=
Nfs3Status
.
NFS3_OK
;
}
catch
(
ClosedChannelException
cce
)
{
if
(
!
pendingWrites
.
isEmpty
(
)
)
{
LOG
.
error
(
+
,
latestAttr
.
getFileId
(
)
,
cce
)
;
}
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
try
{
latestAttr
=
Nfs3Utils
.
getFileAttr
(
client
,
Nfs3Utils
.
getFileIdPath
(
latestAttr
.
getFileId
(
)
)
,
iug
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
latestAttr
.
getFileId
(
)
,
e
)
;
status
=
Nfs3Status
.
NFS3ERR_IO
;
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
try
{
latestAttr
=
Nfs3Utils
.
getFileAttr
(
client
,
Nfs3Utils
.
getFileIdPath
(
latestAttr
.
getFileId
(
)
)
,
iug
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
latestAttr
.
getFileId
(
)
,
e
)
;
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
if
(
latestAttr
.
getSize
(
)
!=
offset
)
{
LOG
.
error
(
+
,
offset
,
latestAttr
.
getSize
(
)
)
;
status
=
Nfs3Status
.
NFS3ERR_IO
;
}
WccData
wccData
=
new
WccData
(
Nfs3Utils
.
getWccAttr
(
latestAttr
)
,
latestAttr
)
;
while
(
entry
!=
null
&&
entry
.
getValue
(
)
.
offset
<=
flushedOffset
)
{
pendingCommits
.
remove
(
entry
.
getKey
(
)
)
;
CommitCtx
commit
=
entry
.
getValue
(
)
;
private
void
doSingleWrite
(
final
WriteCtx
writeCtx
)
{
Channel
channel
=
writeCtx
.
getChannel
(
)
;
int
xid
=
writeCtx
.
getXid
(
)
;
long
offset
=
writeCtx
.
getOffset
(
)
;
int
count
=
writeCtx
.
getCount
(
)
;
WriteStableHow
stableHow
=
writeCtx
.
getStableHow
(
)
;
FileHandle
handle
=
writeCtx
.
getHandle
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
FileHandle
handle
=
writeCtx
.
getHandle
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
offset
,
count
,
stableHow
.
name
(
)
)
;
}
try
{
writeCtx
.
writeData
(
fos
)
;
RpcProgramNfs3
.
metrics
.
incrBytesWritten
(
writeCtx
.
getCount
(
)
)
;
long
flushedOffset
=
getFlushedOffset
(
)
;
if
(
flushedOffset
!=
(
offset
+
count
)
)
{
throw
new
IOException
(
+
flushedOffset
+
+
(
offset
+
count
)
)
;
}
if
(
writeCtx
.
getDataState
(
)
==
WriteCtx
.
DataState
.
ALLOW_DUMP
)
{
synchronized
(
writeCtx
)
{
if
(
writeCtx
.
getDataState
(
)
==
WriteCtx
.
DataState
.
ALLOW_DUMP
)
{
writeCtx
.
setDataState
(
WriteCtx
.
DataState
.
NO_DUMP
)
;
updateNonSequentialWriteInMemory
(
-
count
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
try
{
writeCtx
.
writeData
(
fos
)
;
RpcProgramNfs3
.
metrics
.
incrBytesWritten
(
writeCtx
.
getCount
(
)
)
;
long
flushedOffset
=
getFlushedOffset
(
)
;
if
(
flushedOffset
!=
(
offset
+
count
)
)
{
throw
new
IOException
(
+
flushedOffset
+
+
(
offset
+
count
)
)
;
}
if
(
writeCtx
.
getDataState
(
)
==
WriteCtx
.
DataState
.
ALLOW_DUMP
)
{
synchronized
(
writeCtx
)
{
if
(
writeCtx
.
getDataState
(
)
==
WriteCtx
.
DataState
.
ALLOW_DUMP
)
{
writeCtx
.
setDataState
(
WriteCtx
.
DataState
.
NO_DUMP
)
;
updateNonSequentialWriteInMemory
(
-
count
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
handle
.
dumpFileHandle
(
)
,
offset
,
nonSequentialWriteInMemory
.
get
(
)
)
;
}
}
}
}
if
(
!
writeCtx
.
getReplied
(
)
)
{
if
(
stableHow
!=
WriteStableHow
.
UNSTABLE
)
{
if
(
writeCtx
.
getDataState
(
)
==
WriteCtx
.
DataState
.
ALLOW_DUMP
)
{
synchronized
(
writeCtx
)
{
if
(
writeCtx
.
getDataState
(
)
==
WriteCtx
.
DataState
.
ALLOW_DUMP
)
{
writeCtx
.
setDataState
(
WriteCtx
.
DataState
.
NO_DUMP
)
;
updateNonSequentialWriteInMemory
(
-
count
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
handle
.
dumpFileHandle
(
)
,
offset
,
nonSequentialWriteInMemory
.
get
(
)
)
;
}
}
}
}
if
(
!
writeCtx
.
getReplied
(
)
)
{
if
(
stableHow
!=
WriteStableHow
.
UNSTABLE
)
{
LOG
.
info
(
,
writeCtx
)
;
try
{
if
(
stableHow
==
WriteStableHow
.
DATA_SYNC
)
{
fos
.
hsync
(
)
;
}
else
{
Preconditions
.
checkState
(
stableHow
==
WriteStableHow
.
FILE_SYNC
,
+
stableHow
)
;
if
(
stableHow
!=
WriteStableHow
.
UNSTABLE
)
{
LOG
.
info
(
,
writeCtx
)
;
try
{
if
(
stableHow
==
WriteStableHow
.
DATA_SYNC
)
{
fos
.
hsync
(
)
;
}
else
{
Preconditions
.
checkState
(
stableHow
==
WriteStableHow
.
FILE_SYNC
,
+
stableHow
)
;
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
writeCtx
,
e
)
;
throw
e
;
}
}
WccAttr
preOpAttr
=
latestAttr
.
getWccAttr
(
)
;
WccData
fileWcc
=
new
WccData
(
preOpAttr
,
latestAttr
)
;
if
(
writeCtx
.
getOriginalCount
(
)
!=
WriteCtx
.
INVALID_ORIGINAL_COUNT
)
{
LOG
.
warn
(
,
writeCtx
.
getOriginalCount
(
)
,
count
)
;
}
else
{
Preconditions
.
checkState
(
stableHow
==
WriteStableHow
.
FILE_SYNC
,
+
stableHow
)
;
fos
.
hsync
(
EnumSet
.
of
(
SyncFlag
.
UPDATE_LENGTH
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
writeCtx
,
e
)
;
throw
e
;
}
}
WccAttr
preOpAttr
=
latestAttr
.
getWccAttr
(
)
;
WccData
fileWcc
=
new
WccData
(
preOpAttr
,
latestAttr
)
;
if
(
writeCtx
.
getOriginalCount
(
)
!=
WriteCtx
.
INVALID_ORIGINAL_COUNT
)
{
LOG
.
warn
(
,
writeCtx
.
getOriginalCount
(
)
,
count
)
;
count
=
writeCtx
.
getOriginalCount
(
)
;
}
WRITE3Response
response
=
new
WRITE3Response
(
Nfs3Status
.
NFS3_OK
,
fileWcc
,
count
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
RpcProgramNfs3
.
metrics
.
addWrite
(
Nfs3Utils
.
getElapsedTime
(
writeCtx
.
startTime
)
)
;
Nfs3Utils
.
writeChannel
(
channel
,
response
.
serialize
(
new
XDR
(
)
,
xid
,
new
VerifierNone
(
)
)
,
xid
)
;
}
processCommits
(
writeCtx
.
getOffset
(
)
+
writeCtx
.
getCount
(
)
)
;
}
activeState
=
false
;
if
(
dumpThread
!=
null
&&
dumpThread
.
isAlive
(
)
)
{
dumpThread
.
interrupt
(
)
;
try
{
dumpThread
.
join
(
3000
)
;
}
catch
(
InterruptedException
ignored
)
{
}
}
try
{
if
(
fos
!=
null
)
{
fos
.
close
(
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
info
(
,
latestAttr
.
getFileId
(
)
,
e
.
toString
(
)
)
;
}
LOG
.
info
(
,
pendingWrites
.
size
(
)
)
;
WccAttr
preOpAttr
=
latestAttr
.
getWccAttr
(
)
;
while
(
!
pendingWrites
.
isEmpty
(
)
)
{
OffsetRange
key
=
pendingWrites
.
firstKey
(
)
;
try
{
if
(
fos
!=
null
)
{
fos
.
close
(
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
info
(
,
latestAttr
.
getFileId
(
)
,
e
.
toString
(
)
)
;
}
LOG
.
info
(
,
pendingWrites
.
size
(
)
)
;
WccAttr
preOpAttr
=
latestAttr
.
getWccAttr
(
)
;
while
(
!
pendingWrites
.
isEmpty
(
)
)
{
OffsetRange
key
=
pendingWrites
.
firstKey
(
)
;
LOG
.
info
(
,
key
,
nextOffset
.
get
(
)
)
;
WriteCtx
writeCtx
=
pendingWrites
.
remove
(
key
)
;
if
(
!
writeCtx
.
getReplied
(
)
)
{
WccData
fileWcc
=
new
WccData
(
preOpAttr
,
latestAttr
)
;
WRITE3Response
response
=
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
fileWcc
,
0
,
writeCtx
.
getStableHow
(
)
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
Nfs3Utils
.
writeChannel
(
writeCtx
.
getChannel
(
)
,
response
.
serialize
(
new
XDR
(
)
,
writeCtx
.
getXid
(
)
,
new
VerifierNone
(
)
)
,
writeCtx
.
getXid
(
)
)
;
LOG
.
info
(
,
pendingWrites
.
size
(
)
)
;
WccAttr
preOpAttr
=
latestAttr
.
getWccAttr
(
)
;
while
(
!
pendingWrites
.
isEmpty
(
)
)
{
OffsetRange
key
=
pendingWrites
.
firstKey
(
)
;
LOG
.
info
(
,
key
,
nextOffset
.
get
(
)
)
;
WriteCtx
writeCtx
=
pendingWrites
.
remove
(
key
)
;
if
(
!
writeCtx
.
getReplied
(
)
)
{
WccData
fileWcc
=
new
WccData
(
preOpAttr
,
latestAttr
)
;
WRITE3Response
response
=
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
fileWcc
,
0
,
writeCtx
.
getStableHow
(
)
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
Nfs3Utils
.
writeChannel
(
writeCtx
.
getChannel
(
)
,
response
.
serialize
(
new
XDR
(
)
,
writeCtx
.
getXid
(
)
,
new
VerifierNone
(
)
)
,
writeCtx
.
getXid
(
)
)
;
}
}
if
(
dumpOut
!=
null
)
{
try
{
dumpOut
.
close
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
dumpFilePath
,
e
)
;
LOG
.
debug
(
+
ctx
)
;
}
return
pairs
;
}
if
(
ctx
.
hasPendingWork
(
)
)
{
continue
;
}
if
(
idlest
==
null
)
{
idlest
=
pairs
;
}
else
{
if
(
ctx
.
getLastAccessTime
(
)
<
idlest
.
getValue
(
)
.
getLastAccessTime
(
)
)
{
idlest
=
pairs
;
}
}
}
if
(
idlest
==
null
)
{
LOG
.
warn
(
)
;
return
null
;
}
else
{
long
idleTime
=
Time
.
monotonicNow
(
)
-
idlest
.
getValue
(
)
.
getLastAccessTime
(
)
;
if
(
idleTime
<
NfsConfigKeys
.
DFS_NFS_STREAM_TIMEOUT_MIN_DEFAULT
)
{
Iterator
<
Entry
<
FileHandle
,
OpenFileCtx
>>
it
=
openFileMap
.
entrySet
(
)
.
iterator
(
)
;
if
(
LOG
.
isTraceEnabled
(
)
)
{
LOG
.
trace
(
+
size
(
)
)
;
}
while
(
it
.
hasNext
(
)
)
{
Entry
<
FileHandle
,
OpenFileCtx
>
pairs
=
it
.
next
(
)
;
FileHandle
handle
=
pairs
.
getKey
(
)
;
OpenFileCtx
ctx
=
pairs
.
getValue
(
)
;
if
(
!
ctx
.
streamCleanup
(
handle
,
streamTimeout
)
)
{
continue
;
}
synchronized
(
this
)
{
OpenFileCtx
ctx2
=
openFileMap
.
get
(
handle
)
;
if
(
ctx2
!=
null
)
{
if
(
ctx2
.
streamCleanup
(
handle
,
streamTimeout
)
)
{
openFileMap
.
remove
(
handle
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
clearDirectory
(
String
writeDumpDir
)
throws
IOException
{
File
dumpDir
=
new
File
(
writeDumpDir
)
;
if
(
dumpDir
.
exists
(
)
)
{
@
VisibleForTesting
GETATTR3Response
getattr
(
XDR
xdr
,
SecurityHandler
securityHandler
,
SocketAddress
remoteAddress
)
{
GETATTR3Response
response
=
new
GETATTR3Response
(
Nfs3Status
.
NFS3_OK
)
;
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_ONLY
)
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
return
response
;
}
GETATTR3Request
request
;
try
{
request
=
GETATTR3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
return
response
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
Nfs3FileAttributes
attrs
=
null
;
try
{
attrs
=
writeManager
.
getFileAttr
(
dfsClient
,
handle
,
iug
)
;
}
catch
(
RemoteException
r
)
{
LOG
.
warn
(
,
r
)
;
IOException
io
=
r
.
unwrapRemoteException
(
)
;
if
(
io
instanceof
AuthorizationException
)
{
return
new
GETATTR3Response
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
Nfs3FileAttributes
attrs
=
null
;
try
{
attrs
=
writeManager
.
getFileAttr
(
dfsClient
,
handle
,
iug
)
;
}
catch
(
RemoteException
r
)
{
LOG
.
warn
(
,
r
)
;
IOException
io
=
r
.
unwrapRemoteException
(
)
;
if
(
io
instanceof
AuthorizationException
)
{
return
new
GETATTR3Response
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
}
else
{
return
new
GETATTR3Response
(
Nfs3Status
.
NFS3ERR_IO
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
info
(
,
handle
.
getFileId
(
)
,
e
)
;
private
void
setattrInternal
(
DFSClient
dfsClient
,
String
fileIdPath
,
SetAttr3
newAttr
,
boolean
setMode
)
throws
IOException
{
EnumSet
<
SetAttrField
>
updateFields
=
newAttr
.
getUpdateFields
(
)
;
if
(
setMode
&&
updateFields
.
contains
(
SetAttrField
.
MODE
)
)
{
try
{
request
=
SETATTR3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
return
response
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
request
.
getAttr
(
)
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
SIZE
)
)
{
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
request
.
getAttr
(
)
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
SIZE
)
)
{
LOG
.
error
(
,
handle
.
getFileId
(
)
)
;
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
return
response
;
}
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
Nfs3FileAttributes
preOpAttr
=
null
;
try
{
LOG
.
info
(
,
handle
.
getFileId
(
)
)
;
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_STALE
)
;
return
response
;
}
WccAttr
preOpWcc
=
Nfs3Utils
.
getWccAttr
(
preOpAttr
)
;
if
(
request
.
isCheck
(
)
)
{
if
(
!
preOpAttr
.
getCtime
(
)
.
equals
(
request
.
getCtime
(
)
)
)
{
WccData
wccData
=
new
WccData
(
preOpWcc
,
preOpAttr
)
;
return
new
SETATTR3Response
(
Nfs3Status
.
NFS3ERR_NOT_SYNC
,
wccData
)
;
}
}
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_WRITE
)
)
{
return
new
SETATTR3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
new
WccData
(
preOpWcc
,
preOpAttr
)
)
;
}
setattrInternal
(
dfsClient
,
fileIdPath
,
request
.
getAttr
(
)
,
true
)
;
Nfs3FileAttributes
postOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
WccData
wccData
=
new
WccData
(
preOpWcc
,
postOpAttr
)
;
return
new
SETATTR3Response
(
Nfs3Status
.
NFS3_OK
,
wccData
)
;
}
catch
(
IOException
e
)
{
@
VisibleForTesting
LOOKUP3Response
lookup
(
XDR
xdr
,
SecurityHandler
securityHandler
,
SocketAddress
remoteAddress
)
{
LOOKUP3Response
response
=
new
LOOKUP3Response
(
Nfs3Status
.
NFS3_OK
)
;
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_ONLY
)
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
return
response
;
}
LOOKUP3Request
request
;
try
{
request
=
LOOKUP3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
LOOKUP3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
fileName
=
request
.
getName
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
LOOKUP3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
fileName
=
request
.
getName
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
dirHandle
.
dumpFileHandle
(
)
,
fileName
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
try
{
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
;
Nfs3FileAttributes
postOpObjAttr
=
writeManager
.
getFileAttr
(
dfsClient
,
dirHandle
,
fileName
,
namenodeId
)
;
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
fileName
=
request
.
getName
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
dirHandle
.
dumpFileHandle
(
)
,
fileName
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
try
{
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
;
Nfs3FileAttributes
postOpObjAttr
=
writeManager
.
getFileAttr
(
dfsClient
,
dirHandle
,
fileName
,
namenodeId
)
;
if
(
postOpObjAttr
==
null
)
{
LOG
.
debug
(
,
dirHandle
.
getFileId
(
)
,
fileName
)
;
Nfs3FileAttributes
postOpDirAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
dirFileIdPath
,
iug
)
;
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
return
response
;
}
ACCESS3Request
request
;
try
{
request
=
ACCESS3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
ACCESS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
request
=
ACCESS3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
ACCESS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
Nfs3FileAttributes
attrs
;
try
{
attrs
=
writeManager
.
getFileAttr
(
dfsClient
,
handle
,
iug
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
READLINK3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
try
{
String
target
=
dfsClient
.
getLinkTarget
(
fileIdPath
)
;
Nfs3FileAttributes
postOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
return
new
READLINK3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
try
{
String
target
=
dfsClient
.
getLinkTarget
(
fileIdPath
)
;
Nfs3FileAttributes
postOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
if
(
postOpAttr
==
null
)
{
LOG
.
info
(
,
handle
.
getFileId
(
)
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
try
{
String
target
=
dfsClient
.
getLinkTarget
(
fileIdPath
)
;
Nfs3FileAttributes
postOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
if
(
postOpAttr
==
null
)
{
LOG
.
info
(
,
handle
.
getFileId
(
)
)
;
return
new
READLINK3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
if
(
postOpAttr
.
getType
(
)
!=
NfsFileType
.
NFSLNK
.
toValue
(
)
)
{
final
String
userName
=
securityHandler
.
getUser
(
)
;
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_ONLY
)
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
return
response
;
}
READ3Request
request
;
try
{
request
=
READ3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
READ3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
offset
=
request
.
getOffset
(
)
;
int
count
=
request
.
getCount
(
)
;
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
return
new
READ3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
offset
=
request
.
getOffset
(
)
;
int
count
=
request
.
getCount
(
)
;
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
offset
,
count
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
userName
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
Nfs3FileAttributes
attrs
;
boolean
eof
;
if
(
count
==
0
)
{
try
{
int
count
=
request
.
getCount
(
)
;
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
offset
,
count
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
userName
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
Nfs3FileAttributes
attrs
;
boolean
eof
;
if
(
count
==
0
)
{
try
{
attrs
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
Nfs3Utils
.
getFileIdPath
(
handle
)
,
iug
)
;
}
catch
(
IOException
e
)
{
return
new
READ3Response
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
}
}
int
ret
=
writeManager
.
commitBeforeRead
(
dfsClient
,
handle
,
offset
+
count
)
;
if
(
ret
!=
Nfs3Status
.
NFS3_OK
)
{
LOG
.
warn
(
+
,
ret
)
;
}
try
{
int
rtmax
=
config
.
getInt
(
NfsConfigKeys
.
DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY
,
NfsConfigKeys
.
DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT
)
;
int
buffSize
=
Math
.
min
(
rtmax
,
count
)
;
byte
[
]
readbuffer
=
new
byte
[
buffSize
]
;
int
readCount
=
0
;
for
(
int
i
=
0
;
i
<
1
;
++
i
)
{
FSDataInputStream
fis
=
clientCache
.
getDfsInputStream
(
userName
,
Nfs3Utils
.
getFileIdPath
(
handle
)
,
namenodeId
)
;
if
(
fis
==
null
)
{
return
new
READ3Response
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
}
try
{
readCount
=
fis
.
read
(
offset
,
readbuffer
,
0
,
count
)
;
try
{
request
=
WRITE3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
offset
=
request
.
getOffset
(
)
;
int
count
=
request
.
getCount
(
)
;
WriteStableHow
stableHow
=
request
.
getStableHow
(
)
;
byte
[
]
data
=
request
.
getData
(
)
.
array
(
)
;
if
(
data
.
length
<
count
)
{
LOG
.
error
(
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
WriteStableHow
stableHow
=
request
.
getStableHow
(
)
;
byte
[
]
data
=
request
.
getData
(
)
.
array
(
)
;
if
(
data
.
length
<
count
)
{
LOG
.
error
(
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
handle
.
dumpFileHandle
(
)
,
offset
,
count
,
stableHow
.
getValue
(
)
,
xid
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
Nfs3FileAttributes
preOpAttr
=
null
;
try
{
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
handle
.
dumpFileHandle
(
)
,
offset
,
count
,
stableHow
.
getValue
(
)
,
xid
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
Nfs3FileAttributes
preOpAttr
=
null
;
try
{
preOpAttr
=
writeManager
.
getFileAttr
(
dfsClient
,
handle
,
iug
)
;
if
(
preOpAttr
==
null
)
{
LOG
.
error
(
,
handle
.
getFileId
(
)
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_WRITE
)
)
{
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
handle
.
dumpFileHandle
(
)
,
offset
,
count
,
stableHow
.
getValue
(
)
,
xid
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
Nfs3FileAttributes
preOpAttr
=
null
;
try
{
preOpAttr
=
writeManager
.
getFileAttr
(
dfsClient
,
handle
,
iug
)
;
if
(
preOpAttr
==
null
)
{
LOG
.
error
(
,
handle
.
getFileId
(
)
)
;
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_WRITE
)
)
{
return
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
new
WccData
(
Nfs3Utils
.
getWccAttr
(
preOpAttr
)
,
preOpAttr
)
,
0
,
stableHow
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
request
=
CREATE3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
CREATE3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
fileName
=
request
.
getName
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
dirHandle
.
dumpFileHandle
(
)
,
fileName
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
int
createMode
=
request
.
getMode
(
)
;
if
(
(
createMode
!=
Nfs3Constant
.
CREATE_EXCLUSIVE
)
&&
request
.
getObjAttr
(
)
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
SIZE
)
&&
request
.
getObjAttr
(
)
.
getSize
(
)
!=
0
)
{
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
int
createMode
=
request
.
getMode
(
)
;
if
(
(
createMode
!=
Nfs3Constant
.
CREATE_EXCLUSIVE
)
&&
request
.
getObjAttr
(
)
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
SIZE
)
&&
request
.
getObjAttr
(
)
.
getSize
(
)
!=
0
)
{
LOG
.
error
(
+
,
fileName
,
dirHandle
.
getFileId
(
)
)
;
return
new
CREATE3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
HdfsDataOutputStream
fos
=
null
;
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
;
Nfs3FileAttributes
preOpDirAttr
=
null
;
Nfs3FileAttributes
postOpObjAttr
=
null
;
FileHandle
fileHandle
=
null
;
WccData
dirWcc
=
null
;
try
{
return
new
CREATE3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
null
,
preOpDirAttr
,
new
WccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
preOpDirAttr
)
)
;
}
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
+
+
fileName
;
SetAttr3
setAttr3
=
request
.
getObjAttr
(
)
;
assert
(
setAttr3
!=
null
)
;
FsPermission
permission
=
setAttr3
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
MODE
)
?
new
FsPermission
(
(
short
)
setAttr3
.
getMode
(
)
)
:
FsPermission
.
getDefault
(
)
.
applyUMask
(
umask
)
;
EnumSet
<
CreateFlag
>
flag
=
(
createMode
!=
Nfs3Constant
.
CREATE_EXCLUSIVE
)
?
EnumSet
.
of
(
CreateFlag
.
CREATE
,
CreateFlag
.
OVERWRITE
)
:
EnumSet
.
of
(
CreateFlag
.
CREATE
)
;
fos
=
dfsClient
.
createWrappedOutputStream
(
dfsClient
.
create
(
fileIdPath
,
permission
,
flag
,
false
,
replication
,
blockSize
,
null
,
bufferSize
,
null
)
,
null
)
;
if
(
(
createMode
==
Nfs3Constant
.
CREATE_UNCHECKED
)
||
(
createMode
==
Nfs3Constant
.
CREATE_GUARDED
)
)
{
if
(
!
setAttr3
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
GID
)
)
{
setAttr3
.
getUpdateFields
(
)
.
add
(
SetAttrField
.
GID
)
;
setAttr3
.
setGid
(
securityHandler
.
getGid
(
)
)
;
}
setattrInternal
(
dfsClient
,
fileIdPath
,
setAttr3
,
false
)
;
}
postOpObjAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
dirWcc
=
Nfs3Utils
.
createWccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
dfsClient
,
dirFileIdPath
,
iug
)
;
OpenFileCtx
openFileCtx
=
new
OpenFileCtx
(
fos
,
postOpObjAttr
,
writeDumpDir
+
+
postOpObjAttr
.
getFileId
(
)
,
dfsClient
,
iug
,
aixCompatMode
,
config
)
;
MKDIR3Response
response
=
new
MKDIR3Response
(
Nfs3Status
.
NFS3_OK
)
;
MKDIR3Request
request
;
try
{
request
=
MKDIR3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
MKDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
fileName
=
request
.
getName
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
{
request
=
MKDIR3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
MKDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
fileName
=
request
.
getName
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
dirHandle
.
dumpFileHandle
(
)
,
fileName
,
remoteAddress
)
;
}
if
(
request
.
getObjAttr
(
)
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
SIZE
)
)
{
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
dirHandle
.
dumpFileHandle
(
)
,
fileName
,
remoteAddress
)
;
}
if
(
request
.
getObjAttr
(
)
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
SIZE
)
)
{
LOG
.
error
(
+
,
fileName
,
dirHandle
)
;
return
new
MKDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
;
Nfs3FileAttributes
preOpDirAttr
=
null
;
Nfs3FileAttributes
postOpDirAttr
=
null
;
Nfs3FileAttributes
postOpObjAttr
=
null
;
FileHandle
objFileHandle
=
null
;
try
{
return
new
MKDIR3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
null
,
preOpDirAttr
,
new
WccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
preOpDirAttr
)
)
;
}
final
String
fileIdPath
=
dirFileIdPath
+
+
fileName
;
SetAttr3
setAttr3
=
request
.
getObjAttr
(
)
;
FsPermission
permission
=
setAttr3
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
MODE
)
?
new
FsPermission
(
(
short
)
setAttr3
.
getMode
(
)
)
:
FsPermission
.
getDefault
(
)
.
applyUMask
(
umask
)
;
if
(
!
dfsClient
.
mkdirs
(
fileIdPath
,
permission
,
false
)
)
{
WccData
dirWcc
=
Nfs3Utils
.
createWccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
dfsClient
,
dirFileIdPath
,
iug
)
;
return
new
MKDIR3Response
(
Nfs3Status
.
NFS3ERR_IO
,
null
,
null
,
dirWcc
)
;
}
if
(
!
setAttr3
.
getUpdateFields
(
)
.
contains
(
SetAttrField
.
GID
)
)
{
setAttr3
.
getUpdateFields
(
)
.
add
(
SetAttrField
.
GID
)
;
setAttr3
.
setGid
(
securityHandler
.
getGid
(
)
)
;
}
setattrInternal
(
dfsClient
,
fileIdPath
,
setAttr3
,
false
)
;
postOpObjAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
objFileHandle
=
new
FileHandle
(
postOpObjAttr
.
getFileId
(
)
,
namenodeId
)
;
WccData
dirWcc
=
Nfs3Utils
.
createWccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
dfsClient
,
dirFileIdPath
,
iug
)
;
return
new
MKDIR3Response
(
Nfs3Status
.
NFS3_OK
,
new
FileHandle
(
postOpObjAttr
.
getFileId
(
)
,
namenodeId
)
,
postOpObjAttr
,
dirWcc
)
;
return
new
REMOVE3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
String
fileName
=
request
.
getName
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
dirHandle
.
dumpFileHandle
(
)
,
fileName
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
;
Nfs3FileAttributes
preOpDirAttr
=
null
;
Nfs3FileAttributes
postOpDirAttr
=
null
;
try
{
preOpDirAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
dirFileIdPath
,
iug
)
;
WccData
errWcc
=
new
WccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
preOpDirAttr
)
;
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_WRITE
)
)
{
return
new
REMOVE3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
errWcc
)
;
}
String
fileIdPath
=
dirFileIdPath
+
+
fileName
;
HdfsFileStatus
fstat
=
Nfs3Utils
.
getFileStatus
(
dfsClient
,
fileIdPath
)
;
if
(
fstat
==
null
)
{
return
new
REMOVE3Response
(
Nfs3Status
.
NFS3ERR_NOENT
,
errWcc
)
;
}
if
(
fstat
.
isDirectory
(
)
)
{
return
new
REMOVE3Response
(
Nfs3Status
.
NFS3ERR_ISDIR
,
errWcc
)
;
}
boolean
result
=
dfsClient
.
delete
(
fileIdPath
,
false
)
;
WccData
dirWcc
=
Nfs3Utils
.
createWccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
dfsClient
,
dirFileIdPath
,
iug
)
;
if
(
!
result
)
{
return
new
REMOVE3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
dirWcc
)
;
}
return
new
REMOVE3Response
(
Nfs3Status
.
NFS3_OK
,
dirWcc
)
;
}
catch
(
IOException
e
)
{
return
new
RMDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
fileName
=
request
.
getName
(
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
dirHandle
.
dumpFileHandle
(
)
,
fileName
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
;
Nfs3FileAttributes
preOpDirAttr
=
null
;
Nfs3FileAttributes
postOpDirAttr
=
null
;
try
{
preOpDirAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
dirFileIdPath
,
iug
)
;
return
new
RMDIR3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
errWcc
)
;
}
String
fileIdPath
=
dirFileIdPath
+
+
fileName
;
HdfsFileStatus
fstat
=
Nfs3Utils
.
getFileStatus
(
dfsClient
,
fileIdPath
)
;
if
(
fstat
==
null
)
{
return
new
RMDIR3Response
(
Nfs3Status
.
NFS3ERR_NOENT
,
errWcc
)
;
}
if
(
!
fstat
.
isDirectory
(
)
)
{
return
new
RMDIR3Response
(
Nfs3Status
.
NFS3ERR_NOTDIR
,
errWcc
)
;
}
if
(
fstat
.
getChildrenNum
(
)
>
0
)
{
return
new
RMDIR3Response
(
Nfs3Status
.
NFS3ERR_NOTEMPTY
,
errWcc
)
;
}
boolean
result
=
dfsClient
.
delete
(
fileIdPath
,
false
)
;
WccData
dirWcc
=
Nfs3Utils
.
createWccData
(
Nfs3Utils
.
getWccAttr
(
preOpDirAttr
)
,
dfsClient
,
dirFileIdPath
,
iug
)
;
if
(
!
result
)
{
return
new
RMDIR3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
dirWcc
)
;
}
return
new
RMDIR3Response
(
Nfs3Status
.
NFS3_OK
,
dirWcc
)
;
}
catch
(
IOException
e
)
{
@
VisibleForTesting
RENAME3Response
rename
(
XDR
xdr
,
SecurityHandler
securityHandler
,
SocketAddress
remoteAddress
)
{
RENAME3Response
response
=
new
RENAME3Response
(
Nfs3Status
.
NFS3_OK
)
;
RENAME3Request
request
=
null
;
try
{
request
=
RENAME3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
RENAME3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
fromHandle
=
request
.
getFromDirHandle
(
)
;
int
fromNamenodeId
=
fromHandle
.
getNamenodeId
(
)
;
String
fromName
=
request
.
getFromName
(
)
;
FileHandle
toHandle
=
request
.
getToDirHandle
(
)
;
int
toNamenodeId
=
toHandle
.
getNamenodeId
(
)
;
String
toName
=
request
.
getToName
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
fromHandle
.
dumpFileHandle
(
)
,
fromName
,
toHandle
.
dumpFileHandle
(
)
,
toName
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
fromNamenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
fromNamenodeId
!=
toNamenodeId
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
return
response
;
}
String
fromDirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
fromHandle
)
;
String
toDirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
toHandle
)
;
Nfs3FileAttributes
fromPreOpAttr
=
null
;
Nfs3FileAttributes
toPreOpAttr
=
null
;
WccData
fromDirWcc
=
null
;
WccData
toDirWcc
=
null
;
try
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
if
(
fromNamenodeId
!=
toNamenodeId
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
return
response
;
}
String
fromDirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
fromHandle
)
;
String
toDirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
toHandle
)
;
Nfs3FileAttributes
fromPreOpAttr
=
null
;
Nfs3FileAttributes
toPreOpAttr
=
null
;
WccData
fromDirWcc
=
null
;
WccData
toDirWcc
=
null
;
try
{
fromPreOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fromDirFileIdPath
,
iug
)
;
if
(
fromPreOpAttr
==
null
)
{
LOG
.
info
(
,
fromHandle
.
getFileId
(
)
)
;
}
toPreOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
toDirFileIdPath
,
iug
)
;
if
(
toPreOpAttr
==
null
)
{
LOG
.
info
(
,
toHandle
.
getFileId
(
)
)
;
return
new
RENAME3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_WRITE
)
)
{
WccData
fromWcc
=
new
WccData
(
Nfs3Utils
.
getWccAttr
(
fromPreOpAttr
)
,
fromPreOpAttr
)
;
WccData
toWcc
=
new
WccData
(
Nfs3Utils
.
getWccAttr
(
toPreOpAttr
)
,
toPreOpAttr
)
;
return
new
RENAME3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
fromWcc
,
toWcc
)
;
}
String
src
=
fromDirFileIdPath
+
+
fromName
;
String
dst
=
toDirFileIdPath
+
+
toName
;
dfsClient
.
rename
(
src
,
dst
,
Options
.
Rename
.
NONE
)
;
fromDirWcc
=
Nfs3Utils
.
createWccData
(
Nfs3Utils
.
getWccAttr
(
fromPreOpAttr
)
,
dfsClient
,
fromDirFileIdPath
,
iug
)
;
toDirWcc
=
Nfs3Utils
.
createWccData
(
Nfs3Utils
.
getWccAttr
(
toPreOpAttr
)
,
dfsClient
,
toDirFileIdPath
,
iug
)
;
return
new
RENAME3Response
(
Nfs3Status
.
NFS3_OK
,
fromDirWcc
,
toDirWcc
)
;
}
catch
(
IOException
e
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
return
response
;
}
SYMLINK3Request
request
;
try
{
request
=
SYMLINK3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
return
response
;
}
FileHandle
dirHandle
=
request
.
getHandle
(
)
;
String
name
=
request
.
getName
(
)
;
String
symData
=
request
.
getSymData
(
)
;
String
linkDirIdPath
=
Nfs3Utils
.
getFileIdPath
(
dirHandle
)
;
int
namenodeId
=
dirHandle
.
getNamenodeId
(
)
;
String
linkIdPath
=
linkDirIdPath
+
+
name
;
public
READDIR3Response
readdir
(
XDR
xdr
,
SecurityHandler
securityHandler
,
SocketAddress
remoteAddress
)
{
READDIR3Response
response
=
new
READDIR3Response
(
Nfs3Status
.
NFS3_OK
)
;
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_ONLY
)
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
return
response
;
}
READDIR3Request
request
;
try
{
request
=
READDIR3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
long
cookie
=
request
.
getCookie
(
)
;
if
(
cookie
<
0
)
{
return
response
;
}
READDIR3Request
request
;
try
{
request
=
READDIR3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
long
cookie
=
request
.
getCookie
(
)
;
if
(
cookie
<
0
)
{
LOG
.
error
(
,
cookie
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
count
=
request
.
getCount
(
)
;
if
(
count
<=
0
)
{
try
{
request
=
READDIR3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
long
cookie
=
request
.
getCookie
(
)
;
if
(
cookie
<
0
)
{
LOG
.
error
(
,
cookie
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
count
=
request
.
getCount
(
)
;
if
(
count
<=
0
)
{
LOG
.
info
(
,
count
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3_OK
)
;
}
long
count
=
request
.
getCount
(
)
;
if
(
count
<=
0
)
{
LOG
.
info
(
,
count
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3_OK
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
cookie
,
count
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
HdfsFileStatus
dirStatus
;
DirectoryListing
dlisting
;
Nfs3FileAttributes
postOpAttr
;
long
dotdotFileId
=
0
;
try
{
LOG
.
info
(
,
count
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3_OK
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
cookie
,
count
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
HdfsFileStatus
dirStatus
;
DirectoryListing
dlisting
;
Nfs3FileAttributes
postOpAttr
;
long
dotdotFileId
=
0
;
try
{
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
dirStatus
=
dfsClient
.
getFileInfo
(
dirFileIdPath
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
HdfsFileStatus
dirStatus
;
DirectoryListing
dlisting
;
Nfs3FileAttributes
postOpAttr
;
long
dotdotFileId
=
0
;
try
{
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
dirStatus
=
dfsClient
.
getFileInfo
(
dirFileIdPath
)
;
if
(
dirStatus
==
null
)
{
LOG
.
info
(
,
handle
.
getFileId
(
)
)
;
return
new
READDIR3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
if
(
!
dirStatus
.
isDirectory
(
)
)
{
LOG
.
error
(
,
handle
.
getFileId
(
)
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
}
READDIRPLUS3Request
request
=
null
;
try
{
request
=
READDIRPLUS3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
long
cookie
=
request
.
getCookie
(
)
;
if
(
cookie
<
0
)
{
LOG
.
error
(
,
cookie
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
dirCount
=
request
.
getDirCount
(
)
;
if
(
dirCount
<=
0
)
{
request
=
READDIRPLUS3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
long
cookie
=
request
.
getCookie
(
)
;
if
(
cookie
<
0
)
{
LOG
.
error
(
,
cookie
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
dirCount
=
request
.
getDirCount
(
)
;
if
(
dirCount
<=
0
)
{
LOG
.
info
(
,
dirCount
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
int
maxCount
=
request
.
getMaxCount
(
)
;
LOG
.
error
(
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
long
cookie
=
request
.
getCookie
(
)
;
if
(
cookie
<
0
)
{
LOG
.
error
(
,
cookie
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
long
dirCount
=
request
.
getDirCount
(
)
;
if
(
dirCount
<=
0
)
{
LOG
.
info
(
,
dirCount
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
int
maxCount
=
request
.
getMaxCount
(
)
;
if
(
maxCount
<=
0
)
{
LOG
.
info
(
,
maxCount
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
int
maxCount
=
request
.
getMaxCount
(
)
;
if
(
maxCount
<=
0
)
{
LOG
.
info
(
,
maxCount
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
handle
.
dumpFileHandle
(
)
,
cookie
,
dirCount
,
maxCount
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
}
HdfsFileStatus
dirStatus
;
DirectoryListing
dlisting
;
Nfs3FileAttributes
postOpDirAttr
;
long
dotdotFileId
=
0
;
HdfsFileStatus
dotdotStatus
=
null
;
if
(
maxCount
<=
0
)
{
LOG
.
info
(
,
maxCount
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
handle
.
dumpFileHandle
(
)
,
cookie
,
dirCount
,
maxCount
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
}
HdfsFileStatus
dirStatus
;
DirectoryListing
dlisting
;
Nfs3FileAttributes
postOpDirAttr
;
long
dotdotFileId
=
0
;
HdfsFileStatus
dotdotStatus
=
null
;
try
{
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
}
HdfsFileStatus
dirStatus
;
DirectoryListing
dlisting
;
Nfs3FileAttributes
postOpDirAttr
;
long
dotdotFileId
=
0
;
HdfsFileStatus
dotdotStatus
=
null
;
try
{
String
dirFileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
dirStatus
=
dfsClient
.
getFileInfo
(
dirFileIdPath
)
;
if
(
dirStatus
==
null
)
{
LOG
.
info
(
,
handle
.
getFileId
(
)
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
if
(
!
dirStatus
.
isDirectory
(
)
)
{
LOG
.
error
(
,
handle
.
getFileId
(
)
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_NOTDIR
)
;
}
long
cookieVerf
=
request
.
getCookieVerf
(
)
;
if
(
(
cookieVerf
!=
0
)
&&
(
cookieVerf
!=
dirStatus
.
getModificationTime
(
)
)
)
{
if
(
aixCompatMode
)
{
LOG
.
warn
(
+
)
;
}
else
{
LOG
.
error
(
+
,
cookieVerf
,
dirStatus
.
getModificationTime
(
)
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_BAD_COOKIE
,
Nfs3Utils
.
getFileAttr
(
dfsClient
,
dirFileIdPath
,
iug
)
,
0
,
null
)
;
}
}
if
(
cookie
==
0
)
{
String
dotdotFileIdPath
=
dirFileIdPath
+
;
dotdotStatus
=
dfsClient
.
getFileInfo
(
dotdotFileIdPath
)
;
if
(
dotdotStatus
==
null
)
{
throw
new
IOException
(
+
dotdotFileIdPath
)
;
}
dotdotFileId
=
dotdotStatus
.
getFileId
(
)
;
byte
[
]
startAfter
;
if
(
cookie
==
0
)
{
startAfter
=
HdfsFileStatus
.
EMPTY_NAME
;
}
else
{
String
inodeIdPath
=
Nfs3Utils
.
getFileIdPath
(
cookie
)
;
startAfter
=
inodeIdPath
.
getBytes
(
Charset
.
forName
(
)
)
;
}
dlisting
=
listPaths
(
dfsClient
,
dirFileIdPath
,
startAfter
)
;
postOpDirAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
dirFileIdPath
,
iug
)
;
if
(
postOpDirAttr
==
null
)
{
LOG
.
info
(
,
handle
.
getFileId
(
)
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
e
)
;
int
status
=
mapErrorStatus
(
e
)
;
return
new
READDIRPLUS3Response
(
status
)
;
return
new
READDIRPLUS3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
e
)
;
int
status
=
mapErrorStatus
(
e
)
;
return
new
READDIRPLUS3Response
(
status
)
;
}
HdfsFileStatus
[
]
fstatus
=
dlisting
.
getPartialListing
(
)
;
int
n
=
(
int
)
Math
.
min
(
fstatus
.
length
,
dirCount
-
2
)
;
boolean
eof
=
(
n
>=
fstatus
.
length
)
&&
!
dlisting
.
hasMore
(
)
;
READDIRPLUS3Response
.
EntryPlus3
[
]
entries
;
if
(
cookie
==
0
)
{
entries
=
new
READDIRPLUS3Response
.
EntryPlus3
[
n
+
2
]
;
entries
[
0
]
=
new
READDIRPLUS3Response
.
EntryPlus3
(
postOpDirAttr
.
getFileId
(
)
,
,
0
,
postOpDirAttr
,
new
FileHandle
(
postOpDirAttr
.
getFileId
(
)
,
namenodeId
)
)
;
entries
[
1
]
=
new
READDIRPLUS3Response
.
EntryPlus3
(
dotdotFileId
,
,
dotdotFileId
,
Nfs3Utils
.
getNfs3FileAttrFromFileStatus
(
dotdotStatus
,
iug
)
,
new
FileHandle
(
dotdotFileId
,
namenodeId
)
)
;
for
(
int
i
=
2
;
i
<
n
+
2
;
i
++
)
{
long
fileId
=
fstatus
[
i
-
2
]
.
getFileId
(
)
;
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
FSSTAT3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
try
{
FsStatus
fsStatus
=
dfsClient
.
getDiskStatus
(
)
;
long
totalBytes
=
fsStatus
.
getCapacity
(
)
;
long
freeBytes
=
fsStatus
.
getRemaining
(
)
;
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
FSINFO3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
remoteAddress
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
try
{
int
rtmax
=
config
.
getInt
(
NfsConfigKeys
.
DFS_NFS_MAX_READ_TRANSFER_SIZE_KEY
,
NfsConfigKeys
.
DFS_NFS_MAX_READ_TRANSFER_SIZE_DEFAULT
)
;
int
wtmax
=
config
.
getInt
(
NfsConfigKeys
.
DFS_NFS_MAX_WRITE_TRANSFER_SIZE_KEY
,
NfsConfigKeys
.
DFS_NFS_MAX_WRITE_TRANSFER_SIZE_DEFAULT
)
;
int
dtperf
=
config
.
getInt
(
NfsConfigKeys
.
DFS_NFS_MAX_READDIR_TRANSFER_SIZE_KEY
,
NfsConfigKeys
.
DFS_NFS_MAX_READDIR_TRANSFER_SIZE_DEFAULT
)
;
@
VisibleForTesting
PATHCONF3Response
pathconf
(
XDR
xdr
,
SecurityHandler
securityHandler
,
SocketAddress
remoteAddress
)
{
PATHCONF3Response
response
=
new
PATHCONF3Response
(
Nfs3Status
.
NFS3_OK
)
;
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_ONLY
)
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_ACCES
)
;
return
response
;
}
PATHCONF3Request
request
;
try
{
request
=
PATHCONF3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
PATHCONF3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
Nfs3FileAttributes
attrs
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
request
=
PATHCONF3Request
.
deserialize
(
xdr
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
return
new
PATHCONF3Response
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
Nfs3FileAttributes
attrs
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
try
{
attrs
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
Nfs3Utils
.
getFileIdPath
(
handle
)
,
iug
)
;
LOG
.
error
(
)
;
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_INVAL
)
;
return
response
;
}
FileHandle
handle
=
request
.
getHandle
(
)
;
int
namenodeId
=
handle
.
getNamenodeId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
handle
.
dumpFileHandle
(
)
,
request
.
getOffset
(
)
,
request
.
getCount
(
)
,
remoteAddress
)
;
}
DFSClient
dfsClient
=
clientCache
.
getDfsClient
(
securityHandler
.
getUser
(
)
,
namenodeId
)
;
if
(
dfsClient
==
null
)
{
response
.
setStatus
(
Nfs3Status
.
NFS3ERR_SERVERFAULT
)
;
return
response
;
}
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
Nfs3FileAttributes
preOpAttr
=
null
;
try
{
preOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
}
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
handle
)
;
Nfs3FileAttributes
preOpAttr
=
null
;
try
{
preOpAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
if
(
preOpAttr
==
null
)
{
LOG
.
info
(
,
handle
.
getFileId
(
)
)
;
return
new
COMMIT3Response
(
Nfs3Status
.
NFS3ERR_STALE
)
;
}
if
(
!
checkAccessPrivilege
(
remoteAddress
,
AccessPrivilege
.
READ_WRITE
)
)
{
return
new
COMMIT3Response
(
Nfs3Status
.
NFS3ERR_ACCES
,
new
WccData
(
Nfs3Utils
.
getWccAttr
(
preOpAttr
)
,
preOpAttr
)
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
}
long
commitOffset
=
(
request
.
getCount
(
)
==
0
)
?
0
:
(
request
.
getOffset
(
)
+
request
.
getCount
(
)
)
;
writeManager
.
handleCommit
(
dfsClient
,
handle
,
commitOffset
,
channel
,
xid
,
preOpAttr
,
namenodeId
)
;
return
null
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
e
)
;
Nfs3FileAttributes
postOpAttr
=
null
;
@
Override
public
void
handleInternal
(
ChannelHandlerContext
ctx
,
RpcInfo
info
)
{
RpcCall
rpcCall
=
(
RpcCall
)
info
.
header
(
)
;
final
NFSPROC3
nfsproc3
=
NFSPROC3
.
fromValue
(
rpcCall
.
getProcedure
(
)
)
;
int
xid
=
rpcCall
.
getXid
(
)
;
byte
[
]
data
=
new
byte
[
info
.
data
(
)
.
readableBytes
(
)
]
;
info
.
data
(
)
.
readBytes
(
data
)
;
XDR
xdr
=
new
XDR
(
data
)
;
XDR
out
=
new
XDR
(
)
;
InetAddress
client
=
(
(
InetSocketAddress
)
info
.
remoteAddress
(
)
)
.
getAddress
(
)
;
Credentials
credentials
=
rpcCall
.
getCredential
(
)
;
if
(
nfsproc3
!=
NFSPROC3
.
NULL
)
{
if
(
credentials
.
getFlavor
(
)
!=
AuthFlavor
.
AUTH_SYS
&&
credentials
.
getFlavor
(
)
!=
AuthFlavor
.
RPCSEC_GSS
)
{
Credentials
credentials
=
rpcCall
.
getCredential
(
)
;
if
(
nfsproc3
!=
NFSPROC3
.
NULL
)
{
if
(
credentials
.
getFlavor
(
)
!=
AuthFlavor
.
AUTH_SYS
&&
credentials
.
getFlavor
(
)
!=
AuthFlavor
.
RPCSEC_GSS
)
{
LOG
.
info
(
,
credentials
.
getFlavor
(
)
)
;
XDR
reply
=
new
XDR
(
)
;
RpcDeniedReply
rdr
=
new
RpcDeniedReply
(
xid
,
RpcReply
.
ReplyState
.
MSG_ACCEPTED
,
RpcDeniedReply
.
RejectState
.
AUTH_ERROR
,
new
VerifierNone
(
)
)
;
rdr
.
write
(
reply
)
;
ChannelBuffer
buf
=
ChannelBuffers
.
wrappedBuffer
(
reply
.
asReadOnlyWrap
(
)
.
buffer
(
)
)
;
RpcResponse
rsp
=
new
RpcResponse
(
buf
,
info
.
remoteAddress
(
)
)
;
RpcUtil
.
sendRpcResponse
(
ctx
,
rsp
)
;
return
;
}
}
if
(
!
isIdempotent
(
rpcCall
)
)
{
RpcCallCache
.
CacheEntry
entry
=
rpcCallCache
.
checkOrAddToCache
(
client
,
xid
)
;
if
(
entry
!=
null
)
{
if
(
entry
.
isCompleted
(
)
)
{
XDR
reply
=
new
XDR
(
)
;
RpcDeniedReply
rdr
=
new
RpcDeniedReply
(
xid
,
RpcReply
.
ReplyState
.
MSG_ACCEPTED
,
RpcDeniedReply
.
RejectState
.
AUTH_ERROR
,
new
VerifierNone
(
)
)
;
rdr
.
write
(
reply
)
;
ChannelBuffer
buf
=
ChannelBuffers
.
wrappedBuffer
(
reply
.
asReadOnlyWrap
(
)
.
buffer
(
)
)
;
RpcResponse
rsp
=
new
RpcResponse
(
buf
,
info
.
remoteAddress
(
)
)
;
RpcUtil
.
sendRpcResponse
(
ctx
,
rsp
)
;
return
;
}
}
if
(
!
isIdempotent
(
rpcCall
)
)
{
RpcCallCache
.
CacheEntry
entry
=
rpcCallCache
.
checkOrAddToCache
(
client
,
xid
)
;
if
(
entry
!=
null
)
{
if
(
entry
.
isCompleted
(
)
)
{
LOG
.
info
(
,
xid
)
;
RpcUtil
.
sendRpcResponse
(
ctx
,
entry
.
getResponse
(
)
)
;
return
;
}
else
{
}
final
long
startTime
=
System
.
nanoTime
(
)
;
NFS3Response
response
=
null
;
if
(
nfsproc3
==
NFSPROC3
.
NULL
)
{
response
=
nullProcedure
(
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
GETATTR
)
{
response
=
getattr
(
xdr
,
info
)
;
metrics
.
addGetattr
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
SETATTR
)
{
response
=
setattr
(
xdr
,
info
)
;
metrics
.
addSetattr
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
LOOKUP
)
{
response
=
lookup
(
xdr
,
info
)
;
metrics
.
addLookup
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
ACCESS
)
{
response
=
access
(
xdr
,
info
)
;
NFS3Response
response
=
null
;
if
(
nfsproc3
==
NFSPROC3
.
NULL
)
{
response
=
nullProcedure
(
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
GETATTR
)
{
response
=
getattr
(
xdr
,
info
)
;
metrics
.
addGetattr
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
SETATTR
)
{
response
=
setattr
(
xdr
,
info
)
;
metrics
.
addSetattr
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
LOOKUP
)
{
response
=
lookup
(
xdr
,
info
)
;
metrics
.
addLookup
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
ACCESS
)
{
response
=
access
(
xdr
,
info
)
;
metrics
.
addAccess
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
GETATTR
)
{
response
=
getattr
(
xdr
,
info
)
;
metrics
.
addGetattr
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
SETATTR
)
{
response
=
setattr
(
xdr
,
info
)
;
metrics
.
addSetattr
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
LOOKUP
)
{
response
=
lookup
(
xdr
,
info
)
;
metrics
.
addLookup
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
ACCESS
)
{
response
=
access
(
xdr
,
info
)
;
metrics
.
addAccess
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
}
else
if
(
nfsproc3
==
NFSPROC3
.
READLINK
)
{
response
=
readlink
(
xdr
,
info
)
;
metrics
.
addReadlink
(
Nfs3Utils
.
getElapsedTime
(
startTime
)
)
;
public
void
trimWrite
(
int
delta
)
{
Preconditions
.
checkState
(
delta
<
count
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
OpenFileCtx
openFileCtx
=
fileContextCache
.
get
(
fileHandle
)
;
if
(
openFileCtx
==
null
)
{
LOG
.
info
(
+
fileHandle
.
dumpFileHandle
(
)
)
;
String
fileIdPath
=
Nfs3Utils
.
getFileIdPath
(
fileHandle
.
getFileId
(
)
)
;
HdfsDataOutputStream
fos
=
null
;
Nfs3FileAttributes
latestAttr
=
null
;
try
{
int
bufferSize
=
config
.
getInt
(
CommonConfigurationKeysPublic
.
IO_FILE_BUFFER_SIZE_KEY
,
CommonConfigurationKeysPublic
.
IO_FILE_BUFFER_SIZE_DEFAULT
)
;
fos
=
dfsClient
.
append
(
fileIdPath
,
bufferSize
,
EnumSet
.
of
(
CreateFlag
.
APPEND
)
,
null
,
null
)
;
latestAttr
=
Nfs3Utils
.
getFileAttr
(
dfsClient
,
fileIdPath
,
iug
)
;
}
catch
(
RemoteException
e
)
{
IOException
io
=
e
.
unwrapRemoteException
(
)
;
if
(
io
instanceof
AlreadyBeingCreatedException
)
{
LOG
.
warn
(
+
fileIdPath
+
+
request
+
)
;
return
;
LOG
.
warn
(
+
fileIdPath
+
+
request
+
)
;
return
;
}
throw
e
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
fileIdPath
,
e
)
;
if
(
fos
!=
null
)
{
fos
.
close
(
)
;
}
WccData
fileWcc
=
new
WccData
(
Nfs3Utils
.
getWccAttr
(
preOpAttr
)
,
preOpAttr
)
;
WRITE3Response
response
=
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
fileWcc
,
count
,
request
.
getStableHow
(
)
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
Nfs3Utils
.
writeChannel
(
channel
,
response
.
serialize
(
new
XDR
(
)
,
xid
,
new
VerifierNone
(
)
)
,
xid
)
;
return
;
}
String
writeDumpDir
=
config
.
get
(
NfsConfigKeys
.
DFS_NFS_FILE_DUMP_DIR_KEY
,
NfsConfigKeys
.
DFS_NFS_FILE_DUMP_DIR_DEFAULT
)
;
openFileCtx
=
new
OpenFileCtx
(
fos
,
latestAttr
,
writeDumpDir
+
+
fileHandle
.
getFileId
(
)
,
dfsClient
,
iug
,
aixCompatMode
,
config
)
;
if
(
!
addOpenFileStream
(
fileHandle
,
openFileCtx
)
)
{
LOG
.
info
(
)
;
LOG
.
error
(
+
fileIdPath
,
e
)
;
if
(
fos
!=
null
)
{
fos
.
close
(
)
;
}
WccData
fileWcc
=
new
WccData
(
Nfs3Utils
.
getWccAttr
(
preOpAttr
)
,
preOpAttr
)
;
WRITE3Response
response
=
new
WRITE3Response
(
Nfs3Status
.
NFS3ERR_IO
,
fileWcc
,
count
,
request
.
getStableHow
(
)
,
Nfs3Constant
.
WRITE_COMMIT_VERF
)
;
Nfs3Utils
.
writeChannel
(
channel
,
response
.
serialize
(
new
XDR
(
)
,
xid
,
new
VerifierNone
(
)
)
,
xid
)
;
return
;
}
String
writeDumpDir
=
config
.
get
(
NfsConfigKeys
.
DFS_NFS_FILE_DUMP_DIR_KEY
,
NfsConfigKeys
.
DFS_NFS_FILE_DUMP_DIR_DEFAULT
)
;
openFileCtx
=
new
OpenFileCtx
(
fos
,
latestAttr
,
writeDumpDir
+
+
fileHandle
.
getFileId
(
)
,
dfsClient
,
iug
,
aixCompatMode
,
config
)
;
if
(
!
addOpenFileStream
(
fileHandle
,
openFileCtx
)
)
{
LOG
.
info
(
)
;
try
{
fos
.
close
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
handle
.
dumpFileHandle
(
)
,
e
)
;
int
commitBeforeRead
(
DFSClient
dfsClient
,
FileHandle
fileHandle
,
long
commitOffset
)
{
int
status
;
OpenFileCtx
openFileCtx
=
fileContextCache
.
get
(
fileHandle
)
;
if
(
openFileCtx
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
void
handleCommit
(
DFSClient
dfsClient
,
FileHandle
fileHandle
,
long
commitOffset
,
Channel
channel
,
int
xid
,
Nfs3FileAttributes
preOpAttr
,
int
namenodeId
)
{
long
startTime
=
System
.
nanoTime
(
)
;
int
status
;
OpenFileCtx
openFileCtx
=
fileContextCache
.
get
(
fileHandle
)
;
if
(
openFileCtx
==
null
)
{
switch
(
ret
)
{
case
COMMIT_FINISHED
:
case
COMMIT_INACTIVE_CTX
:
status
=
Nfs3Status
.
NFS3_OK
;
break
;
case
COMMIT_INACTIVE_WITH_PENDING_WRITE
:
case
COMMIT_ERROR
:
status
=
Nfs3Status
.
NFS3ERR_IO
;
break
;
case
COMMIT_WAIT
:
return
;
case
COMMIT_SPECIAL_WAIT
:
status
=
Nfs3Status
.
NFS3ERR_JUKEBOX
;
break
;
case
COMMIT_SPECIAL_SUCCESS
:
status
=
Nfs3Status
.
NFS3_OK
;
break
;
default
:
LOG
.
error
(
+
ret
.
name
(
)
)
;
throw
new
RuntimeException
(
+
ret
.
name
(
)
)
;
}
}
Nfs3FileAttributes
postOpAttr
=
null
;
try
{
postOpAttr
=
getFileAttr
(
dfsClient
,
new
FileHandle
(
preOpAttr
.
getFileId
(
)
,
namenodeId
)
,
iug
)
;
@
Override
public
void
init
(
Configuration
configuration
,
RouterRpcServer
rpcServer
,
StateStoreService
stateStore
)
{
this
.
conf
=
configuration
;
this
.
server
=
rpcServer
;
this
.
store
=
stateStore
;
this
.
metrics
=
FederationRPCMetrics
.
create
(
conf
,
server
)
;
ThreadFactory
threadFactory
=
new
ThreadFactoryBuilder
(
)
.
setNameFormat
(
)
.
build
(
)
;
this
.
executor
=
Executors
.
newFixedThreadPool
(
1
,
threadFactory
)
;
try
{
StandardMBean
bean
=
new
StandardMBean
(
this
.
metrics
,
FederationRPCMBean
.
class
)
;
registeredBean
=
MBeans
.
register
(
,
,
bean
)
;
innerinfo
.
put
(
,
node
.
getAdminState
(
)
.
toString
(
)
)
;
innerinfo
.
put
(
,
node
.
getNonDfsUsed
(
)
)
;
innerinfo
.
put
(
,
node
.
getCapacity
(
)
)
;
innerinfo
.
put
(
,
-
1
)
;
innerinfo
.
put
(
,
(
node
.
getSoftwareVersion
(
)
==
null
?
:
node
.
getSoftwareVersion
(
)
)
)
;
innerinfo
.
put
(
,
node
.
getDfsUsed
(
)
)
;
innerinfo
.
put
(
,
node
.
getRemaining
(
)
)
;
innerinfo
.
put
(
,
-
1
)
;
innerinfo
.
put
(
,
node
.
getBlockPoolUsed
(
)
)
;
innerinfo
.
put
(
,
node
.
getBlockPoolUsedPercent
(
)
)
;
innerinfo
.
put
(
,
-
1
)
;
info
.
put
(
node
.
getHostName
(
)
+
+
node
.
getXferPort
(
)
,
Collections
.
unmodifiableMap
(
innerinfo
)
)
;
}
}
catch
(
StandbyException
e
)
{
LOG
.
error
(
,
type
)
;
}
catch
(
SubClusterTimeoutException
e
)
{
innerinfo
.
put
(
,
node
.
getCapacity
(
)
)
;
innerinfo
.
put
(
,
-
1
)
;
innerinfo
.
put
(
,
(
node
.
getSoftwareVersion
(
)
==
null
?
:
node
.
getSoftwareVersion
(
)
)
)
;
innerinfo
.
put
(
,
node
.
getDfsUsed
(
)
)
;
innerinfo
.
put
(
,
node
.
getRemaining
(
)
)
;
innerinfo
.
put
(
,
-
1
)
;
innerinfo
.
put
(
,
node
.
getBlockPoolUsed
(
)
)
;
innerinfo
.
put
(
,
node
.
getBlockPoolUsedPercent
(
)
)
;
innerinfo
.
put
(
,
-
1
)
;
info
.
put
(
node
.
getHostName
(
)
+
+
node
.
getXferPort
(
)
,
Collections
.
unmodifiableMap
(
innerinfo
)
)
;
}
}
catch
(
StandbyException
e
)
{
LOG
.
error
(
,
type
)
;
}
catch
(
SubClusterTimeoutException
e
)
{
LOG
.
error
(
,
type
)
;
}
catch
(
IOException
e
)
{
final
List
<
MembershipState
>
namenodes
=
getActiveNamenodeRegistrations
(
)
;
List
<
MembershipState
>
namenodesOrder
=
new
ArrayList
<
>
(
namenodes
)
;
Collections
.
sort
(
namenodesOrder
,
MembershipState
.
NAME_COMPARATOR
)
;
for
(
MembershipState
namenode
:
namenodesOrder
)
{
Map
<
String
,
Object
>
innerInfo
=
new
HashMap
<
>
(
)
;
Map
<
String
,
Object
>
map
=
getJson
(
namenode
)
;
innerInfo
.
putAll
(
map
)
;
long
dateModified
=
namenode
.
getDateModified
(
)
;
long
lastHeartbeat
=
getSecondsSince
(
dateModified
)
;
innerInfo
.
put
(
,
lastHeartbeat
)
;
MembershipStats
stats
=
namenode
.
getStats
(
)
;
long
used
=
stats
.
getTotalSpace
(
)
-
stats
.
getAvailableSpace
(
)
;
innerInfo
.
put
(
,
used
)
;
info
.
put
(
namenode
.
getNamenodeKey
(
)
,
Collections
.
unmodifiableMap
(
innerInfo
)
)
;
}
}
catch
(
IOException
e
)
{
float
totalDfsUsed
=
0
;
float
[
]
usages
=
new
float
[
live
.
length
]
;
int
i
=
0
;
for
(
DatanodeInfo
dn
:
live
)
{
usages
[
i
++
]
=
dn
.
getDfsUsedPercent
(
)
;
totalDfsUsed
+=
dn
.
getDfsUsedPercent
(
)
;
}
totalDfsUsed
/=
live
.
length
;
Arrays
.
sort
(
usages
)
;
median
=
usages
[
usages
.
length
/
2
]
;
max
=
usages
[
usages
.
length
-
1
]
;
min
=
usages
[
0
]
;
for
(
i
=
0
;
i
<
usages
.
length
;
i
++
)
{
dev
+=
(
usages
[
i
]
-
totalDfsUsed
)
*
(
usages
[
i
]
-
totalDfsUsed
)
;
}
dev
=
(
float
)
Math
.
sqrt
(
dev
/
usages
.
length
)
;
}
}
catch
(
IOException
e
)
{
MembershipState
partial
=
MembershipState
.
newInstance
(
)
;
String
rpcAddress
=
address
.
getHostName
(
)
+
+
address
.
getPort
(
)
;
partial
.
setRpcAddress
(
rpcAddress
)
;
partial
.
setNameserviceId
(
nsId
)
;
GetNamenodeRegistrationsRequest
request
=
GetNamenodeRegistrationsRequest
.
newInstance
(
partial
)
;
MembershipStore
membership
=
getMembershipStore
(
)
;
GetNamenodeRegistrationsResponse
response
=
membership
.
getNamenodeRegistrations
(
request
)
;
List
<
MembershipState
>
records
=
response
.
getNamenodeMemberships
(
)
;
if
(
records
!=
null
&&
records
.
size
(
)
==
1
)
{
MembershipState
record
=
records
.
get
(
0
)
;
UpdateNamenodeRegistrationRequest
updateRequest
=
UpdateNamenodeRegistrationRequest
.
newInstance
(
record
.
getNameserviceId
(
)
,
record
.
getNamenodeId
(
)
,
ACTIVE
)
;
membership
.
updateNamenodeRegistration
(
updateRequest
)
;
cacheNS
.
remove
(
nsId
)
;
cacheBP
.
clear
(
)
;
}
}
catch
(
StateStoreUnavailableException
e
)
{
@
Override
public
List
<
?
extends
FederationNamenodeContext
>
getNamenodesForBlockPoolId
(
final
String
bpId
)
throws
IOException
{
List
<
?
extends
FederationNamenodeContext
>
ret
=
cacheBP
.
get
(
bpId
)
;
if
(
ret
==
null
)
{
try
{
MembershipState
partial
=
MembershipState
.
newInstance
(
)
;
partial
.
setBlockPoolId
(
bpId
)
;
GetNamenodeRegistrationsRequest
request
=
GetNamenodeRegistrationsRequest
.
newInstance
(
partial
)
;
final
List
<
MembershipState
>
result
=
getRecentRegistrationForQuery
(
request
,
true
,
false
)
;
if
(
result
==
null
||
result
.
isEmpty
(
)
)
{
private
List
<
MembershipState
>
getRecentRegistrationForQuery
(
GetNamenodeRegistrationsRequest
request
,
boolean
addUnavailable
,
boolean
addExpired
)
throws
IOException
{
MembershipStore
membershipStore
=
getMembershipStore
(
)
;
GetNamenodeRegistrationsResponse
response
=
membershipStore
.
getNamenodeRegistrations
(
request
)
;
List
<
MembershipState
>
memberships
=
response
.
getNamenodeMemberships
(
)
;
if
(
!
addExpired
||
!
addUnavailable
)
{
Iterator
<
MembershipState
>
iterator
=
memberships
.
iterator
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
MembershipState
membership
=
iterator
.
next
(
)
;
if
(
membership
.
getState
(
)
==
EXPIRED
&&
!
addExpired
)
{
iterator
.
remove
(
)
;
}
else
if
(
membership
.
getState
(
)
==
UNAVAILABLE
&&
!
addUnavailable
)
{
iterator
.
remove
(
)
;
}
}
}
List
<
MembershipState
>
priorityList
=
new
ArrayList
<
>
(
)
;
priorityList
.
addAll
(
memberships
)
;
Collections
.
sort
(
priorityList
,
new
NamenodePriorityComparator
(
)
)
;
private
void
invalidateLocationCache
(
final
String
path
)
{
}
ConcurrentMap
<
String
,
PathLocation
>
map
=
locationCache
.
asMap
(
)
;
Set
<
Entry
<
String
,
PathLocation
>>
entries
=
map
.
entrySet
(
)
;
Iterator
<
Entry
<
String
,
PathLocation
>>
it
=
entries
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
Entry
<
String
,
PathLocation
>
entry
=
it
.
next
(
)
;
String
key
=
entry
.
getKey
(
)
;
PathLocation
loc
=
entry
.
getValue
(
)
;
String
src
=
loc
.
getSourcePath
(
)
;
if
(
src
!=
null
)
{
if
(
isParentEntry
(
key
,
path
)
)
{
LOG
.
debug
(
,
src
)
;
it
.
remove
(
)
;
}
}
else
{
String
dest
=
loc
.
getDefaultLocation
(
)
.
getDest
(
)
;
if
(
dest
.
startsWith
(
path
)
)
{
@
VisibleForTesting
public
void
refreshEntries
(
final
Collection
<
MountTable
>
entries
)
{
writeLock
.
lock
(
)
;
try
{
Map
<
String
,
MountTable
>
newEntries
=
new
ConcurrentHashMap
<
>
(
)
;
for
(
MountTable
entry
:
entries
)
{
String
srcPath
=
entry
.
getSourcePath
(
)
;
newEntries
.
put
(
srcPath
,
entry
)
;
}
Set
<
String
>
oldEntries
=
new
TreeSet
<
>
(
Collections
.
reverseOrder
(
)
)
;
for
(
MountTable
entry
:
getTreeValues
(
)
)
{
String
srcPath
=
entry
.
getSourcePath
(
)
;
oldEntries
.
add
(
srcPath
)
;
}
for
(
String
srcPath
:
oldEntries
)
{
if
(
!
newEntries
.
containsKey
(
srcPath
)
)
{
this
.
tree
.
remove
(
srcPath
)
;
invalidateLocationCache
(
srcPath
)
;
newEntries
.
put
(
srcPath
,
entry
)
;
}
Set
<
String
>
oldEntries
=
new
TreeSet
<
>
(
Collections
.
reverseOrder
(
)
)
;
for
(
MountTable
entry
:
getTreeValues
(
)
)
{
String
srcPath
=
entry
.
getSourcePath
(
)
;
oldEntries
.
add
(
srcPath
)
;
}
for
(
String
srcPath
:
oldEntries
)
{
if
(
!
newEntries
.
containsKey
(
srcPath
)
)
{
this
.
tree
.
remove
(
srcPath
)
;
invalidateLocationCache
(
srcPath
)
;
LOG
.
info
(
,
srcPath
)
;
}
}
for
(
MountTable
entry
:
entries
)
{
String
srcPath
=
entry
.
getSourcePath
(
)
;
if
(
!
oldEntries
.
contains
(
srcPath
)
)
{
this
.
tree
.
put
(
srcPath
,
entry
)
;
invalidateLocationCache
(
srcPath
)
;
String
srcPath
=
entry
.
getSourcePath
(
)
;
oldEntries
.
add
(
srcPath
)
;
}
for
(
String
srcPath
:
oldEntries
)
{
if
(
!
newEntries
.
containsKey
(
srcPath
)
)
{
this
.
tree
.
remove
(
srcPath
)
;
invalidateLocationCache
(
srcPath
)
;
LOG
.
info
(
,
srcPath
)
;
}
}
for
(
MountTable
entry
:
entries
)
{
String
srcPath
=
entry
.
getSourcePath
(
)
;
if
(
!
oldEntries
.
contains
(
srcPath
)
)
{
this
.
tree
.
put
(
srcPath
,
entry
)
;
invalidateLocationCache
(
srcPath
)
;
LOG
.
info
(
,
srcPath
)
;
}
else
{
MountTable
existingEntry
=
this
.
tree
.
get
(
srcPath
)
;
for
(
String
srcPath
:
oldEntries
)
{
if
(
!
newEntries
.
containsKey
(
srcPath
)
)
{
this
.
tree
.
remove
(
srcPath
)
;
invalidateLocationCache
(
srcPath
)
;
LOG
.
info
(
,
srcPath
)
;
}
}
for
(
MountTable
entry
:
entries
)
{
String
srcPath
=
entry
.
getSourcePath
(
)
;
if
(
!
oldEntries
.
contains
(
srcPath
)
)
{
this
.
tree
.
put
(
srcPath
,
entry
)
;
invalidateLocationCache
(
srcPath
)
;
LOG
.
info
(
,
srcPath
)
;
}
else
{
MountTable
existingEntry
=
this
.
tree
.
get
(
srcPath
)
;
if
(
existingEntry
!=
null
&&
!
existingEntry
.
equals
(
entry
)
)
{
LOG
.
info
(
,
existingEntry
,
entry
)
;
private
PathLocation
buildLocation
(
final
String
path
,
final
MountTable
entry
)
throws
IOException
{
String
srcPath
=
entry
.
getSourcePath
(
)
;
if
(
!
path
.
startsWith
(
srcPath
)
)
{
@
Override
public
PathLocation
getDestinationForPath
(
String
path
)
throws
IOException
{
PathLocation
mountTableResult
=
super
.
getDestinationForPath
(
path
)
;
if
(
mountTableResult
==
null
)
{
@
Override
public
PathLocation
getDestinationForPath
(
String
path
)
throws
IOException
{
PathLocation
mountTableResult
=
super
.
getDestinationForPath
(
path
)
;
if
(
mountTableResult
==
null
)
{
LOG
.
error
(
,
super
.
getClass
(
)
.
getSimpleName
(
)
,
path
)
;
}
else
if
(
mountTableResult
.
hasMultipleDestinations
(
)
)
{
DestinationOrder
order
=
mountTableResult
.
getDestinationOrder
(
)
;
OrderedResolver
orderedResolver
=
orderedResolvers
.
get
(
order
)
;
if
(
orderedResolver
==
null
)
{
LOG
.
error
(
,
order
)
;
}
else
{
String
firstNamespace
=
orderedResolver
.
getFirstNamespace
(
path
,
mountTableResult
)
;
if
(
firstNamespace
!=
null
)
{
mountTableResult
=
new
PathLocation
(
mountTableResult
,
firstNamespace
)
;
LOG
.
debug
(
,
order
,
mountTableResult
)
;
}
else
{
@
Override
public
String
getFirstNamespace
(
final
String
path
,
final
PathLocation
loc
)
{
String
srcPath
=
loc
.
getSourcePath
(
)
;
String
trimmedPath
=
trimPathToChild
(
path
,
srcPath
)
;
@
Override
public
String
getFirstNamespace
(
final
String
path
,
final
PathLocation
loc
)
{
String
finalPath
=
extractTempFileName
(
path
)
;
Set
<
String
>
namespaces
=
loc
.
getNamespaces
(
)
;
ConsistentHashRing
locator
=
getHashResolver
(
namespaces
)
;
String
hashedSubcluster
=
locator
.
getLocation
(
finalPath
)
;
if
(
hashedSubcluster
==
null
)
{
String
srcPath
=
loc
.
getSourcePath
(
)
;
@
Override
protected
String
chooseFirstNamespace
(
String
path
,
PathLocation
loc
)
{
String
localSubcluster
=
null
;
String
clientAddr
=
getClientAddr
(
)
;
Map
<
String
,
String
>
subclusterInfo
=
getSubclusterMapping
(
)
;
if
(
subclusterInfo
!=
null
)
{
localSubcluster
=
subclusterInfo
.
get
(
clientAddr
)
;
if
(
localSubcluster
!=
null
)
{
Map
<
String
,
DatanodeStorageReport
[
]
>
dnMap
=
loginUser
.
doAs
(
new
PrivilegedAction
<
Map
<
String
,
DatanodeStorageReport
[
]
>>
(
)
{
@
Override
public
Map
<
String
,
DatanodeStorageReport
[
]
>
run
(
)
{
try
{
return
rpcServer
.
getDatanodeStorageReportMap
(
DatanodeReportType
.
ALL
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
return
null
;
}
}
}
)
;
for
(
Entry
<
String
,
DatanodeStorageReport
[
]
>
entry
:
dnMap
.
entrySet
(
)
)
{
String
nsId
=
entry
.
getKey
(
)
;
DatanodeStorageReport
[
]
dns
=
entry
.
getValue
(
)
;
for
(
DatanodeStorageReport
dn
:
dns
)
{
DatanodeInfo
dnInfo
=
dn
.
getDatanodeInfo
(
)
;
String
ipAddr
=
dnInfo
.
getIpAddr
(
)
;
ret
.
put
(
ipAddr
,
nsId
)
;
Map
<
String
,
String
>
ret
=
new
HashMap
<
>
(
)
;
try
{
GetNamenodeRegistrationsRequest
request
=
GetNamenodeRegistrationsRequest
.
newInstance
(
)
;
GetNamenodeRegistrationsResponse
response
=
membershipStore
.
getNamenodeRegistrations
(
request
)
;
final
List
<
MembershipState
>
nns
=
response
.
getNamenodeMemberships
(
)
;
for
(
MembershipState
nn
:
nns
)
{
try
{
String
nsId
=
nn
.
getNameserviceId
(
)
;
String
rpcAddress
=
nn
.
getRpcAddress
(
)
;
String
hostname
=
HostAndPort
.
fromString
(
rpcAddress
)
.
getHost
(
)
;
ret
.
put
(
hostname
,
nsId
)
;
if
(
hostname
.
equals
(
localHostname
)
)
{
ret
.
put
(
localIp
,
nsId
)
;
}
InetAddress
addr
=
InetAddress
.
getByName
(
hostname
)
;
String
ipAddr
=
addr
.
getHostAddress
(
)
;
public
String
getFirstNamespace
(
final
String
path
,
final
PathLocation
loc
)
{
final
Set
<
String
>
namespaces
=
(
loc
==
null
)
?
null
:
loc
.
getNamespaces
(
)
;
if
(
CollectionUtils
.
isEmpty
(
namespaces
)
)
{
public
ConnectionContext
getConnection
(
UserGroupInformation
ugi
,
String
nnAddress
,
Class
<
?
>
protocol
)
throws
IOException
{
if
(
!
this
.
running
)
{
}
finally
{
readLock
.
unlock
(
)
;
}
if
(
pool
==
null
)
{
writeLock
.
lock
(
)
;
try
{
pool
=
this
.
pools
.
get
(
connectionId
)
;
if
(
pool
==
null
)
{
pool
=
new
ConnectionPool
(
this
.
conf
,
nnAddress
,
ugi
,
this
.
minSize
,
this
.
maxSize
,
this
.
minActiveRatio
,
protocol
)
;
this
.
pools
.
put
(
connectionId
,
pool
)
;
}
}
finally
{
writeLock
.
unlock
(
)
;
}
}
ConnectionContext
conn
=
pool
.
getConnection
(
)
;
if
(
conn
==
null
||
!
conn
.
isUsable
(
)
)
{
if
(
!
this
.
creatorQueue
.
offer
(
pool
)
)
{
LOG
.
error
(
,
this
.
creatorQueueMaxSize
)
;
protected
synchronized
void
close
(
)
{
long
timeSinceLastActive
=
TimeUnit
.
MILLISECONDS
.
toSeconds
(
Time
.
now
(
)
-
getLastActiveTime
(
)
)
;
protected
static
<
T
>
ConnectionContext
newConnection
(
Configuration
conf
,
String
nnAddress
,
UserGroupInformation
ugi
,
Class
<
T
>
proto
)
throws
IOException
{
if
(
!
PROTO_MAP
.
containsKey
(
proto
)
)
{
String
msg
=
+
(
(
proto
!=
null
)
?
proto
.
getName
(
)
:
)
;
URL
jmxURL
=
new
URL
(
scheme
,
host
,
port
,
+
beanQuery
)
;
LOG
.
debug
(
,
jmxURL
)
;
URLConnection
conn
=
connectionFactory
.
openConnection
(
jmxURL
,
UserGroupInformation
.
isSecurityEnabled
(
)
)
;
conn
.
setConnectTimeout
(
5
*
1000
)
;
conn
.
setReadTimeout
(
5
*
1000
)
;
InputStream
in
=
conn
.
getInputStream
(
)
;
InputStreamReader
isr
=
new
InputStreamReader
(
in
,
)
;
reader
=
new
BufferedReader
(
isr
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
String
line
=
null
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
sb
.
append
(
line
)
;
}
String
jmxOutput
=
sb
.
toString
(
)
;
JSONObject
json
=
new
JSONObject
(
jmxOutput
)
;
ret
=
json
.
getJSONArray
(
)
;
URLConnection
conn
=
connectionFactory
.
openConnection
(
jmxURL
,
UserGroupInformation
.
isSecurityEnabled
(
)
)
;
conn
.
setConnectTimeout
(
5
*
1000
)
;
conn
.
setReadTimeout
(
5
*
1000
)
;
InputStream
in
=
conn
.
getInputStream
(
)
;
InputStreamReader
isr
=
new
InputStreamReader
(
in
,
)
;
reader
=
new
BufferedReader
(
isr
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
String
line
=
null
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
sb
.
append
(
line
)
;
}
String
jmxOutput
=
sb
.
toString
(
)
;
JSONObject
json
=
new
JSONObject
(
jmxOutput
)
;
ret
=
json
.
getJSONArray
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
beanQuery
,
webAddress
,
e
)
;
conn
.
setReadTimeout
(
5
*
1000
)
;
InputStream
in
=
conn
.
getInputStream
(
)
;
InputStreamReader
isr
=
new
InputStreamReader
(
in
,
)
;
reader
=
new
BufferedReader
(
isr
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
String
line
=
null
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
sb
.
append
(
line
)
;
}
String
jmxOutput
=
sb
.
toString
(
)
;
JSONObject
json
=
new
JSONObject
(
jmxOutput
)
;
ret
=
json
.
getJSONArray
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
beanQuery
,
webAddress
,
e
)
;
}
catch
(
JSONException
e
)
{
LOG
.
error
(
,
beanQuery
,
webAddress
,
e
.
getMessage
(
)
)
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
sb
.
append
(
line
)
;
}
String
jmxOutput
=
sb
.
toString
(
)
;
JSONObject
json
=
new
JSONObject
(
jmxOutput
)
;
ret
=
json
.
getJSONArray
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
beanQuery
,
webAddress
,
e
)
;
}
catch
(
JSONException
e
)
{
LOG
.
error
(
,
beanQuery
,
webAddress
,
e
.
getMessage
(
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
beanQuery
,
webAddress
,
e
)
;
}
finally
{
if
(
reader
!=
null
)
{
try
{
reader
.
close
(
)
;
nnDesc
+=
+
namenodeId
;
}
else
{
this
.
localTarget
=
null
;
}
this
.
rpcAddress
=
getRpcAddress
(
conf
,
nameserviceId
,
namenodeId
)
;
LOG
.
info
(
,
nnDesc
,
rpcAddress
)
;
this
.
serviceAddress
=
DFSUtil
.
getNamenodeServiceAddr
(
conf
,
nameserviceId
,
namenodeId
)
;
if
(
this
.
serviceAddress
==
null
)
{
LOG
.
error
(
+
,
nnDesc
,
this
.
rpcAddress
)
;
this
.
serviceAddress
=
this
.
rpcAddress
;
}
LOG
.
info
(
,
nnDesc
,
serviceAddress
)
;
this
.
lifelineAddress
=
DFSUtil
.
getNamenodeLifelineAddr
(
conf
,
nameserviceId
,
namenodeId
)
;
if
(
this
.
lifelineAddress
==
null
)
{
this
.
lifelineAddress
=
this
.
serviceAddress
;
}
LOG
.
info
(
,
nnDesc
,
lifelineAddress
)
;
this
.
webAddress
=
DFSUtil
.
getNamenodeWebAddr
(
conf
,
nameserviceId
,
namenodeId
)
;
NamenodeStatusReport
report
=
getNamenodeStatusReport
(
)
;
if
(
!
report
.
registrationValid
(
)
)
{
LOG
.
error
(
,
getNamenodeDesc
(
)
)
;
}
else
if
(
report
.
haStateValid
(
)
)
{
LOG
.
debug
(
,
report
.
getState
(
)
,
getNamenodeDesc
(
)
)
;
}
else
if
(
localTarget
==
null
)
{
LOG
.
debug
(
+
getNamenodeDesc
(
)
)
;
}
else
{
return
;
}
try
{
if
(
!
resolver
.
registerNamenode
(
report
)
)
{
LOG
.
warn
(
,
report
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
info
(
)
;
}
catch
(
Exception
ex
)
{
protected
NamenodeStatusReport
getNamenodeStatusReport
(
)
{
NamenodeStatusReport
report
=
new
NamenodeStatusReport
(
nameserviceId
,
namenodeId
,
rpcAddress
,
serviceAddress
,
lifelineAddress
,
scheme
,
webAddress
)
;
try
{
LOG
.
debug
(
,
serviceAddress
)
;
URI
serviceURI
=
new
URI
(
+
serviceAddress
)
;
NamenodeProtocol
nn
=
NameNodeProxies
.
createProxy
(
this
.
conf
,
serviceURI
,
NamenodeProtocol
.
class
)
.
getProxy
(
)
;
if
(
nn
!=
null
)
{
NamespaceInfo
info
=
nn
.
versionRequest
(
)
;
if
(
info
!=
null
)
{
report
.
setNamespaceInfo
(
info
)
;
}
}
if
(
!
report
.
registrationValid
(
)
)
{
return
report
;
}
try
{
ClientProtocol
client
=
NameNodeProxies
.
createProxy
(
this
.
conf
,
serviceURI
,
ClientProtocol
.
class
)
.
getProxy
(
)
;
if
(
client
!=
null
)
{
boolean
isSafeMode
=
client
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_GET
,
false
)
;
report
.
setSafeMode
(
isSafeMode
)
;
}
}
catch
(
Exception
e
)
{
ClientProtocol
client
=
NameNodeProxies
.
createProxy
(
this
.
conf
,
serviceURI
,
ClientProtocol
.
class
)
.
getProxy
(
)
;
if
(
client
!=
null
)
{
boolean
isSafeMode
=
client
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_GET
,
false
)
;
report
.
setSafeMode
(
isSafeMode
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
getNamenodeDesc
(
)
,
e
)
;
}
updateJMXParameters
(
webAddress
,
report
)
;
if
(
localTarget
!=
null
)
{
try
{
if
(
localTargetHAProtocol
==
null
)
{
localTargetHAProtocol
=
localTarget
.
getProxy
(
conf
,
30
*
1000
)
;
}
HAServiceStatus
status
=
localTargetHAProtocol
.
getServiceStatus
(
)
;
report
.
setHAServiceState
(
status
.
getState
(
)
)
;
}
catch
(
Throwable
e
)
{
if
(
e
.
getMessage
(
)
.
startsWith
(
)
)
{
report
.
setSafeMode
(
isSafeMode
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
getNamenodeDesc
(
)
,
e
)
;
}
updateJMXParameters
(
webAddress
,
report
)
;
if
(
localTarget
!=
null
)
{
try
{
if
(
localTargetHAProtocol
==
null
)
{
localTargetHAProtocol
=
localTarget
.
getProxy
(
conf
,
30
*
1000
)
;
}
HAServiceStatus
status
=
localTargetHAProtocol
.
getServiceStatus
(
)
;
report
.
setHAServiceState
(
status
.
getState
(
)
)
;
}
catch
(
Throwable
e
)
{
if
(
e
.
getMessage
(
)
.
startsWith
(
)
)
{
LOG
.
error
(
,
getNamenodeDesc
(
)
)
;
localTarget
=
null
;
}
else
{
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
getNamenodeDesc
(
)
,
e
)
;
}
updateJMXParameters
(
webAddress
,
report
)
;
if
(
localTarget
!=
null
)
{
try
{
if
(
localTargetHAProtocol
==
null
)
{
localTargetHAProtocol
=
localTarget
.
getProxy
(
conf
,
30
*
1000
)
;
}
HAServiceStatus
status
=
localTargetHAProtocol
.
getServiceStatus
(
)
;
report
.
setHAServiceState
(
status
.
getState
(
)
)
;
}
catch
(
Throwable
e
)
{
if
(
e
.
getMessage
(
)
.
startsWith
(
)
)
{
LOG
.
error
(
,
getNamenodeDesc
(
)
)
;
localTarget
=
null
;
}
else
{
LOG
.
error
(
,
getNamenodeDesc
(
)
,
e
.
getMessage
(
)
,
e
)
;
RemoteLocation
loc
=
entry
.
getKey
(
)
;
QuotaUsage
usage
=
entry
.
getValue
(
)
;
if
(
isMountEntry
)
{
nsCount
+=
usage
.
getFileAndDirectoryCount
(
)
;
ssCount
+=
usage
.
getSpaceConsumed
(
)
;
eachByStorageType
(
t
->
typeCount
[
t
.
ordinal
(
)
]
+=
usage
.
getTypeConsumed
(
t
)
)
;
}
else
if
(
usage
!=
null
)
{
if
(
!
RouterQuotaManager
.
isQuotaSet
(
usage
)
)
{
hasQuotaUnset
=
true
;
}
nsQuota
=
usage
.
getQuota
(
)
;
ssQuota
=
usage
.
getSpaceQuota
(
)
;
eachByStorageType
(
t
->
typeQuota
[
t
.
ordinal
(
)
]
=
usage
.
getTypeQuota
(
t
)
)
;
nsCount
+=
usage
.
getFileAndDirectoryCount
(
)
;
ssCount
+=
usage
.
getSpaceConsumed
(
)
;
eachByStorageType
(
t
->
typeCount
[
t
.
ordinal
(
)
]
+=
usage
.
getTypeConsumed
(
t
)
)
;
NamenodeHeartbeatService
localHeartbeatService
=
createLocalNamenodeHeartbeatService
(
)
;
if
(
localHeartbeatService
!=
null
)
{
String
nnDesc
=
localHeartbeatService
.
getNamenodeDesc
(
)
;
ret
.
put
(
nnDesc
,
localHeartbeatService
)
;
}
}
Collection
<
String
>
namenodes
=
this
.
conf
.
getTrimmedStringCollection
(
RBFConfigKeys
.
DFS_ROUTER_MONITOR_NAMENODE
)
;
for
(
String
namenode
:
namenodes
)
{
String
[
]
namenodeSplit
=
namenode
.
split
(
)
;
String
nsId
=
null
;
String
nnId
=
null
;
if
(
namenodeSplit
.
length
==
2
)
{
nsId
=
namenodeSplit
[
0
]
;
nnId
=
namenodeSplit
[
1
]
;
}
else
if
(
namenodeSplit
.
length
==
1
)
{
nsId
=
namenode
;
}
else
{
protected
NamenodeHeartbeatService
createLocalNamenodeHeartbeatService
(
)
{
String
nsId
=
DFSUtil
.
getNamenodeNameServiceId
(
conf
)
;
String
nnId
=
null
;
if
(
HAUtil
.
isHAEnabled
(
conf
,
nsId
)
)
{
nnId
=
HAUtil
.
getNameNodeId
(
conf
,
nsId
)
;
if
(
nnId
==
null
)
{
protected
NamenodeHeartbeatService
createNamenodeHeartbeatService
(
String
nsId
,
String
nnId
)
{
@
Override
public
DisableNameserviceResponse
disableNameservice
(
DisableNameserviceRequest
request
)
throws
IOException
{
checkSuperuserPrivilege
(
)
;
String
nsId
=
request
.
getNameServiceId
(
)
;
boolean
success
=
false
;
if
(
namespaceExists
(
nsId
)
)
{
success
=
getDisabledNameserviceStore
(
)
.
disableNameservice
(
nsId
)
;
if
(
success
)
{
@
Override
public
EnableNameserviceResponse
enableNameservice
(
EnableNameserviceRequest
request
)
throws
IOException
{
checkSuperuserPrivilege
(
)
;
String
nsId
=
request
.
getNameServiceId
(
)
;
DisabledNameserviceStore
store
=
getDisabledNameserviceStore
(
)
;
Set
<
String
>
disabled
=
store
.
getDisabledNameservices
(
)
;
boolean
success
=
false
;
if
(
disabled
.
contains
(
nsId
)
)
{
success
=
store
.
enableNameservice
(
nsId
)
;
if
(
success
)
{
@
Override
public
HdfsFileStatus
create
(
String
src
,
FsPermission
masked
,
String
clientName
,
EnumSetWritable
<
CreateFlag
>
flag
,
boolean
createParent
,
short
replication
,
long
blockSize
,
CryptoProtocolVersion
[
]
supportedVersions
,
String
ecPolicyName
,
String
storagePolicy
)
throws
IOException
{
rpcServer
.
checkOperation
(
NameNode
.
OperationCategory
.
WRITE
)
;
if
(
createParent
&&
rpcServer
.
isPathAll
(
src
)
)
{
int
index
=
src
.
lastIndexOf
(
Path
.
SEPARATOR
)
;
String
parent
=
src
.
substring
(
0
,
index
)
;
@
Override
public
HdfsFileStatus
create
(
String
src
,
FsPermission
masked
,
String
clientName
,
EnumSetWritable
<
CreateFlag
>
flag
,
boolean
createParent
,
short
replication
,
long
blockSize
,
CryptoProtocolVersion
[
]
supportedVersions
,
String
ecPolicyName
,
String
storagePolicy
)
throws
IOException
{
rpcServer
.
checkOperation
(
NameNode
.
OperationCategory
.
WRITE
)
;
if
(
createParent
&&
rpcServer
.
isPathAll
(
src
)
)
{
int
index
=
src
.
lastIndexOf
(
Path
.
SEPARATOR
)
;
String
parent
=
src
.
substring
(
0
,
index
)
;
LOG
.
debug
(
,
src
,
parent
)
;
FsPermission
parentPermissions
=
getParentPermission
(
masked
)
;
boolean
success
=
mkdirs
(
parent
,
parentPermissions
,
createParent
)
;
if
(
!
success
)
{
@
Override
public
DirectoryListing
getListing
(
String
src
,
byte
[
]
startAfter
,
boolean
needLocation
)
throws
IOException
{
rpcServer
.
checkOperation
(
NameNode
.
OperationCategory
.
READ
)
;
List
<
RemoteResult
<
RemoteLocation
,
DirectoryListing
>>
listings
=
getListingInt
(
src
,
startAfter
,
needLocation
)
;
TreeMap
<
String
,
HdfsFileStatus
>
nnListing
=
new
TreeMap
<
>
(
)
;
int
totalRemainingEntries
=
0
;
int
remainingEntries
=
0
;
boolean
namenodeListingExists
=
false
;
String
lastName
=
null
;
if
(
listings
!=
null
)
{
for
(
RemoteResult
<
RemoteLocation
,
DirectoryListing
>
result
:
listings
)
{
if
(
result
.
hasException
(
)
)
{
IOException
ioe
=
result
.
getException
(
)
;
if
(
ioe
instanceof
FileNotFoundException
)
{
RemoteLocation
location
=
result
.
getLocation
(
)
;
if
(
ioe
instanceof
FileNotFoundException
)
{
notFoundException
=
(
FileNotFoundException
)
ioe
;
}
else
if
(
!
allowPartialList
)
{
throw
ioe
;
}
}
else
if
(
result
.
getResult
(
)
!=
null
)
{
summaries
.
add
(
result
.
getResult
(
)
)
;
}
}
final
List
<
String
>
children
=
subclusterResolver
.
getMountPoints
(
path
)
;
if
(
children
!=
null
)
{
for
(
String
child
:
children
)
{
Path
childPath
=
new
Path
(
path
,
child
)
;
try
{
ContentSummary
mountSummary
=
getContentSummary
(
childPath
.
toString
(
)
)
;
if
(
mountSummary
!=
null
)
{
summaries
.
add
(
mountSummary
)
;
}
}
catch
(
Exception
e
)
{
MountTableResolver
mountTable
=
(
MountTableResolver
)
subclusterResolver
;
MountTable
entry
=
mountTable
.
getMountPoint
(
mName
)
;
if
(
entry
!=
null
)
{
permission
=
entry
.
getMode
(
)
;
owner
=
entry
.
getOwnerName
(
)
;
group
=
entry
.
getGroupName
(
)
;
RemoteMethod
method
=
new
RemoteMethod
(
,
new
Class
<
?
>
[
]
{
String
.
class
}
,
new
RemoteParam
(
)
)
;
HdfsFileStatus
fInfo
=
getFileInfoAll
(
entry
.
getDestinations
(
)
,
method
,
mountStatusTimeOut
)
;
if
(
fInfo
!=
null
)
{
permission
=
fInfo
.
getPermission
(
)
;
owner
=
fInfo
.
getOwner
(
)
;
group
=
fInfo
.
getGroup
(
)
;
childrenNum
=
fInfo
.
getChildrenNum
(
)
;
flags
=
DFSUtil
.
getFlags
(
fInfo
.
isEncrypted
(
)
,
fInfo
.
isErasureCoded
(
)
,
fInfo
.
isSnapshotEnabled
(
)
,
fInfo
.
hasAcl
(
)
)
;
}
}
}
catch
(
IOException
e
)
{
permission
=
fInfo
.
getPermission
(
)
;
owner
=
fInfo
.
getOwner
(
)
;
group
=
fInfo
.
getGroup
(
)
;
childrenNum
=
fInfo
.
getChildrenNum
(
)
;
flags
=
DFSUtil
.
getFlags
(
fInfo
.
isEncrypted
(
)
,
fInfo
.
isErasureCoded
(
)
,
fInfo
.
isSnapshotEnabled
(
)
,
fInfo
.
hasAcl
(
)
)
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
.
getMessage
(
)
)
;
}
}
else
{
try
{
UserGroupInformation
ugi
=
RouterRpcServer
.
getRemoteUser
(
)
;
owner
=
ugi
.
getUserName
(
)
;
group
=
ugi
.
getPrimaryGroupName
(
)
;
}
catch
(
IOException
e
)
{
String
msg
=
+
e
.
getMessage
(
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
group
=
fInfo
.
getGroup
(
)
;
childrenNum
=
fInfo
.
getChildrenNum
(
)
;
flags
=
DFSUtil
.
getFlags
(
fInfo
.
isEncrypted
(
)
,
fInfo
.
isErasureCoded
(
)
,
fInfo
.
isSnapshotEnabled
(
)
,
fInfo
.
hasAcl
(
)
)
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
.
getMessage
(
)
)
;
}
}
else
{
try
{
UserGroupInformation
ugi
=
RouterRpcServer
.
getRemoteUser
(
)
;
owner
=
ugi
.
getUserName
(
)
;
group
=
ugi
.
getPrimaryGroupName
(
)
;
}
catch
(
IOException
e
)
{
String
msg
=
+
e
.
getMessage
(
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
LOG
.
error
(
msg
)
;
}
else
{
if
(
path
.
equals
(
Path
.
SEPARATOR
)
)
{
srcPath
=
Path
.
SEPARATOR
+
child
;
}
else
{
srcPath
=
path
+
Path
.
SEPARATOR
+
child
;
}
Long
modTime
=
0L
;
try
{
MountTable
entry
=
mountTable
.
getMountPoint
(
srcPath
)
;
if
(
entry
==
null
)
{
List
<
MountTable
>
entries
=
mountTable
.
getMounts
(
srcPath
)
;
for
(
MountTable
eachEntry
:
entries
)
{
if
(
ret
.
get
(
child
)
==
null
||
ret
.
get
(
child
)
<
eachEntry
.
getDateModified
(
)
)
{
modTime
=
eachEntry
.
getDateModified
(
)
;
}
}
}
else
{
modTime
=
entry
.
getDateModified
(
)
;
}
}
catch
(
IOException
e
)
{
public
void
fsck
(
)
{
final
long
startTime
=
Time
.
monotonicNow
(
)
;
try
{
String
warnMsg
=
+
;
LOG
.
warn
(
warnMsg
)
;
out
.
println
(
warnMsg
)
;
String
msg
=
+
UserGroupInformation
.
getCurrentUser
(
)
+
+
remoteAddress
+
+
new
Date
(
)
;
LOG
.
error
(
)
;
return
;
}
if
(
isStoreAvailable
(
)
)
{
RouterStore
routerStore
=
router
.
getRouterStateManager
(
)
;
try
{
RouterState
record
=
RouterState
.
newInstance
(
routerId
,
router
.
getStartTime
(
)
,
router
.
getRouterState
(
)
)
;
StateStoreVersion
stateStoreVersion
=
StateStoreVersion
.
newInstance
(
getStateStoreVersion
(
MembershipStore
.
class
)
,
getStateStoreVersion
(
MountTableStore
.
class
)
)
;
record
.
setStateStoreVersion
(
stateStoreVersion
)
;
String
hostPort
=
StateStoreUtils
.
getHostPortString
(
router
.
getAdminServerAddress
(
)
)
;
record
.
setAdminAddress
(
hostPort
)
;
RouterHeartbeatRequest
request
=
RouterHeartbeatRequest
.
newInstance
(
record
)
;
RouterHeartbeatResponse
response
=
routerStore
.
routerHeartbeat
(
request
)
;
if
(
!
response
.
getStatus
(
)
)
{
LOG
.
warn
(
,
routerId
)
;
}
else
{
}
if
(
isStoreAvailable
(
)
)
{
RouterStore
routerStore
=
router
.
getRouterStateManager
(
)
;
try
{
RouterState
record
=
RouterState
.
newInstance
(
routerId
,
router
.
getStartTime
(
)
,
router
.
getRouterState
(
)
)
;
StateStoreVersion
stateStoreVersion
=
StateStoreVersion
.
newInstance
(
getStateStoreVersion
(
MembershipStore
.
class
)
,
getStateStoreVersion
(
MountTableStore
.
class
)
)
;
record
.
setStateStoreVersion
(
stateStoreVersion
)
;
String
hostPort
=
StateStoreUtils
.
getHostPortString
(
router
.
getAdminServerAddress
(
)
)
;
record
.
setAdminAddress
(
hostPort
)
;
RouterHeartbeatRequest
request
=
RouterHeartbeatRequest
.
newInstance
(
record
)
;
RouterHeartbeatResponse
response
=
routerStore
.
routerHeartbeat
(
request
)
;
if
(
!
response
.
getStatus
(
)
)
{
LOG
.
warn
(
,
routerId
)
;
}
else
{
LOG
.
debug
(
,
routerId
)
;
}
}
catch
(
IOException
e
)
{
long
ssQuota
=
oldQuota
.
getSpaceQuota
(
)
;
long
[
]
typeQuota
=
new
long
[
StorageType
.
values
(
)
.
length
]
;
Quota
.
eachByStorageType
(
t
->
typeQuota
[
t
.
ordinal
(
)
]
=
oldQuota
.
getTypeQuota
(
t
)
)
;
QuotaUsage
currentQuotaUsage
=
null
;
HdfsFileStatus
ret
=
this
.
rpcServer
.
getFileInfo
(
src
)
;
if
(
ret
==
null
||
ret
.
getModificationTime
(
)
==
0
)
{
long
[
]
zeroConsume
=
new
long
[
StorageType
.
values
(
)
.
length
]
;
currentQuotaUsage
=
new
RouterQuotaUsage
.
Builder
(
)
.
fileAndDirectoryCount
(
0
)
.
quota
(
nsQuota
)
.
spaceConsumed
(
0
)
.
spaceQuota
(
ssQuota
)
.
typeConsumed
(
zeroConsume
)
.
typeQuota
(
typeQuota
)
.
build
(
)
;
}
else
{
try
{
Quota
quotaModule
=
this
.
rpcServer
.
getQuotaModule
(
)
;
Map
<
RemoteLocation
,
QuotaUsage
>
usageMap
=
quotaModule
.
getEachQuotaUsage
(
src
)
;
currentQuotaUsage
=
quotaModule
.
aggregateQuota
(
src
,
usageMap
)
;
remoteQuotaUsage
.
putAll
(
usageMap
)
;
}
catch
(
IOException
ioe
)
{
private
void
fixGlobalQuota
(
RemoteLocation
location
,
QuotaUsage
remoteQuota
)
throws
IOException
{
QuotaUsage
gQuota
=
this
.
rpcServer
.
getQuotaModule
(
)
.
getGlobalQuota
(
location
.
getSrc
(
)
)
;
if
(
remoteQuota
.
getQuota
(
)
!=
gQuota
.
getQuota
(
)
||
remoteQuota
.
getSpaceQuota
(
)
!=
gQuota
.
getSpaceQuota
(
)
)
{
this
.
rpcServer
.
getQuotaModule
(
)
.
setQuotaInternal
(
location
.
getSrc
(
)
,
Arrays
.
asList
(
location
)
,
gQuota
.
getQuota
(
)
,
gQuota
.
getSpaceQuota
(
)
,
null
)
;
ioes
.
put
(
namenode
,
ioe
)
;
if
(
ioe
instanceof
StandbyException
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpFailureStandby
(
)
;
}
failover
=
true
;
}
else
if
(
isUnavailableException
(
ioe
)
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpFailureCommunicate
(
)
;
}
failover
=
true
;
}
else
if
(
ioe
instanceof
RemoteException
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpComplete
(
true
)
;
}
RemoteException
re
=
(
RemoteException
)
ioe
;
ioe
=
re
.
unwrapRemoteException
(
)
;
ioe
=
getCleanException
(
ioe
)
;
else
if
(
isUnavailableException
(
ioe
)
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpFailureCommunicate
(
)
;
}
failover
=
true
;
}
else
if
(
ioe
instanceof
RemoteException
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpComplete
(
true
)
;
}
RemoteException
re
=
(
RemoteException
)
ioe
;
ioe
=
re
.
unwrapRemoteException
(
)
;
ioe
=
getCleanException
(
ioe
)
;
throw
ioe
;
}
else
if
(
ioe
instanceof
ConnectionNullException
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpFailureCommunicate
(
)
;
}
LOG
.
error
(
,
nsId
,
rpcAddress
,
ioe
.
getMessage
(
)
)
;
StandbyException
se
=
new
StandbyException
(
ioe
.
getMessage
(
)
)
;
se
.
initCause
(
ioe
)
;
throw
se
;
}
else
if
(
ioe
instanceof
NoNamenodesAvailableException
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpNoNamenodes
(
)
;
}
LOG
.
error
(
,
nsId
,
rpcAddress
,
ioe
.
getMessage
(
)
)
;
throw
new
RetriableException
(
ioe
)
;
}
else
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpFailureCommunicate
(
)
;
this
.
rpcMonitor
.
proxyOpComplete
(
false
)
;
}
throw
ioe
;
}
}
finally
{
if
(
connection
!=
null
)
{
}
else
if
(
ioe
instanceof
NoNamenodesAvailableException
)
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpNoNamenodes
(
)
;
}
LOG
.
error
(
,
nsId
,
rpcAddress
,
ioe
.
getMessage
(
)
)
;
throw
new
RetriableException
(
ioe
)
;
}
else
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpFailureCommunicate
(
)
;
this
.
rpcMonitor
.
proxyOpComplete
(
false
)
;
}
throw
ioe
;
}
}
finally
{
if
(
connection
!=
null
)
{
connection
.
release
(
)
;
}
}
}
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpComplete
(
false
)
;
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpNoNamenodes
(
)
;
}
LOG
.
error
(
,
nsId
,
rpcAddress
,
ioe
.
getMessage
(
)
)
;
throw
new
RetriableException
(
ioe
)
;
}
else
{
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpFailureCommunicate
(
)
;
this
.
rpcMonitor
.
proxyOpComplete
(
false
)
;
}
throw
ioe
;
}
}
finally
{
if
(
connection
!=
null
)
{
connection
.
release
(
)
;
}
}
}
if
(
this
.
rpcMonitor
!=
null
)
{
this
.
rpcMonitor
.
proxyOpComplete
(
false
)
;
}
String
msg
=
+
method
.
getName
(
)
+
+
Arrays
.
deepToString
(
params
)
+
+
namenodes
+
+
router
.
getRouterId
(
)
;
String
methodName
=
matcher
.
group
(
2
)
;
String
fileName
=
matcher
.
group
(
3
)
;
int
lineNumber
=
Integer
.
parseInt
(
matcher
.
group
(
4
)
)
;
StackTraceElement
element
=
new
StackTraceElement
(
declaringClass
,
methodName
,
fileName
,
lineNumber
)
;
elements
.
add
(
element
)
;
}
}
stackTrace
=
elements
.
toArray
(
new
StackTraceElement
[
elements
.
size
(
)
]
)
;
}
if
(
ioe
instanceof
RemoteException
)
{
RemoteException
re
=
(
RemoteException
)
ioe
;
ret
=
new
RemoteException
(
re
.
getClassName
(
)
,
msg
)
;
}
else
{
Class
<
?
extends
IOException
>
ioeClass
=
ioe
.
getClass
(
)
;
try
{
Constructor
<
?
extends
IOException
>
constructor
=
ioeClass
.
getDeclaredConstructor
(
String
.
class
)
;
ret
=
constructor
.
newInstance
(
msg
)
;
}
catch
(
ReflectiveOperationException
e
)
{
List
<
?
extends
FederationNamenodeContext
>
namenodes
=
getNamenodesForNameservice
(
ns
)
;
try
{
Class
<
?
>
proto
=
remoteMethod
.
getProtocol
(
)
;
Object
[
]
params
=
remoteMethod
.
getParams
(
loc
)
;
Object
result
=
invokeMethod
(
ugi
,
namenodes
,
proto
,
m
,
params
)
;
if
(
isExpectedClass
(
expectedResultClass
,
result
)
&&
isExpectedValue
(
expectedResultValue
,
result
)
)
{
@
SuppressWarnings
(
)
R
location
=
(
R
)
loc
;
@
SuppressWarnings
(
)
T
ret
=
(
T
)
result
;
return
new
RemoteResult
<
>
(
location
,
ret
)
;
}
if
(
firstResult
==
null
)
{
firstResult
=
result
;
}
}
catch
(
IOException
ioe
)
{
ioe
=
processException
(
ioe
,
loc
)
;
thrownExceptions
.
add
(
ioe
)
;
}
catch
(
Exception
e
)
{
}
if
(
rpcMonitor
!=
null
)
{
rpcMonitor
.
proxyOp
(
)
;
}
try
{
List
<
Future
<
Object
>>
futures
=
null
;
if
(
timeOutMs
>
0
)
{
futures
=
executorService
.
invokeAll
(
callables
,
timeOutMs
,
TimeUnit
.
MILLISECONDS
)
;
}
else
{
futures
=
executorService
.
invokeAll
(
callables
)
;
}
List
<
RemoteResult
<
T
,
R
>>
results
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
futures
.
size
(
)
;
i
++
)
{
T
location
=
orderedLocations
.
get
(
i
)
;
try
{
Future
<
Object
>
future
=
futures
.
get
(
i
)
;
R
result
=
(
R
)
future
.
get
(
)
;
results
.
add
(
new
RemoteResult
<
>
(
location
,
result
)
)
;
T
location
=
orderedLocations
.
get
(
i
)
;
try
{
Future
<
Object
>
future
=
futures
.
get
(
i
)
;
R
result
=
(
R
)
future
.
get
(
)
;
results
.
add
(
new
RemoteResult
<
>
(
location
,
result
)
)
;
}
catch
(
CancellationException
ce
)
{
T
loc
=
orderedLocations
.
get
(
i
)
;
String
msg
=
+
loc
+
+
method
.
getMethodName
(
)
+
;
LOG
.
error
(
msg
)
;
IOException
ioe
=
new
SubClusterTimeoutException
(
msg
)
;
results
.
add
(
new
RemoteResult
<
>
(
location
,
ioe
)
)
;
}
catch
(
ExecutionException
ex
)
{
Throwable
cause
=
ex
.
getCause
(
)
;
LOG
.
debug
(
,
m
.
getName
(
)
,
location
,
cause
.
getMessage
(
)
)
;
IOException
ioe
=
null
;
R
result
=
(
R
)
future
.
get
(
)
;
results
.
add
(
new
RemoteResult
<
>
(
location
,
result
)
)
;
}
catch
(
CancellationException
ce
)
{
T
loc
=
orderedLocations
.
get
(
i
)
;
String
msg
=
+
loc
+
+
method
.
getMethodName
(
)
+
;
LOG
.
error
(
msg
)
;
IOException
ioe
=
new
SubClusterTimeoutException
(
msg
)
;
results
.
add
(
new
RemoteResult
<
>
(
location
,
ioe
)
)
;
}
catch
(
ExecutionException
ex
)
{
Throwable
cause
=
ex
.
getCause
(
)
;
LOG
.
debug
(
,
m
.
getName
(
)
,
location
,
cause
.
getMessage
(
)
)
;
IOException
ioe
=
null
;
if
(
cause
instanceof
IOException
)
{
ioe
=
(
IOException
)
cause
;
}
else
{
private
DatanodeInfo
[
]
getCachedDatanodeReportImpl
(
final
DatanodeReportType
type
)
throws
IOException
{
UserGroupInformation
loginUser
=
UserGroupInformation
.
getLoginUser
(
)
;
RouterRpcServer
.
setCurrentUser
(
loginUser
)
;
try
{
DatanodeInfo
[
]
dns
=
clientProto
.
getDatanodeReport
(
type
)
;
checkOperation
(
OperationCategory
.
UNCHECKED
)
;
Map
<
String
,
DatanodeInfo
>
datanodesMap
=
new
LinkedHashMap
<
>
(
)
;
RemoteMethod
method
=
new
RemoteMethod
(
,
new
Class
<
?
>
[
]
{
DatanodeReportType
.
class
}
,
type
)
;
Set
<
FederationNamespaceInfo
>
nss
=
namenodeResolver
.
getNamespaces
(
)
;
Map
<
FederationNamespaceInfo
,
DatanodeInfo
[
]
>
results
=
rpcClient
.
invokeConcurrent
(
nss
,
method
,
requireResponse
,
false
,
timeOutMs
,
DatanodeInfo
[
]
.
class
)
;
for
(
Entry
<
FederationNamespaceInfo
,
DatanodeInfo
[
]
>
entry
:
results
.
entrySet
(
)
)
{
FederationNamespaceInfo
ns
=
entry
.
getKey
(
)
;
DatanodeInfo
[
]
result
=
entry
.
getValue
(
)
;
for
(
DatanodeInfo
node
:
result
)
{
String
nodeId
=
node
.
getXferAddr
(
)
;
DatanodeInfo
dn
=
datanodesMap
.
get
(
nodeId
)
;
if
(
dn
==
null
||
node
.
getLastUpdate
(
)
>
dn
.
getLastUpdate
(
)
)
{
node
.
setNetworkLocation
(
NodeBase
.
PATH_SEPARATOR_STR
+
ns
.
getNameserviceId
(
)
+
node
.
getNetworkLocation
(
)
)
;
datanodesMap
.
put
(
nodeId
,
node
)
;
}
else
{
private
void
leave
(
)
{
long
timeInSafemode
=
now
(
)
-
enterSafeModeTime
;
@
Override
public
void
periodicInvoke
(
)
{
long
now
=
Time
.
now
(
)
;
long
delta
=
now
-
startupTime
;
if
(
delta
<
startupInterval
)
{
@
Override
public
String
[
]
getGroupsForUser
(
String
user
)
throws
IOException
{
public
Token
<
DelegationTokenIdentifier
>
getDelegationToken
(
Text
renewer
)
throws
IOException
{
public
void
cancelDelegationToken
(
Token
<
DelegationTokenIdentifier
>
token
)
throws
IOException
{
LOG
.
debug
(
)
;
final
String
operationName
=
;
boolean
success
=
false
;
String
tokenId
=
;
try
{
String
canceller
=
getRemoteUser
(
)
.
getUserName
(
)
;
void
logAuditEvent
(
boolean
succeeded
,
String
cmd
,
String
tokenId
)
throws
IOException
{
LOG
.
info
(
)
;
try
{
checkAgainstZkBeforeDeletion
.
set
(
true
)
;
if
(
zkClient
.
checkExists
(
)
.
forPath
(
ZK_DTSM_TOKENS_ROOT
)
==
null
)
{
zkClient
.
create
(
)
.
creatingParentsIfNeeded
(
)
.
withMode
(
CreateMode
.
PERSISTENT
)
.
forPath
(
ZK_DTSM_TOKENS_ROOT
)
;
}
try
{
zookeeper
=
zkClient
.
getZookeeperClient
(
)
.
getZooKeeper
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
e
)
;
}
finally
{
if
(
zookeeper
==
null
)
{
throw
new
IOException
(
)
;
}
}
LOG
.
info
(
)
;
long
start
=
Time
.
now
(
)
;
rebuildTokenCache
(
true
)
;
catch
(
Exception
e
)
{
LOG
.
info
(
,
e
)
;
}
finally
{
if
(
zookeeper
==
null
)
{
throw
new
IOException
(
)
;
}
}
LOG
.
info
(
)
;
long
start
=
Time
.
now
(
)
;
rebuildTokenCache
(
true
)
;
LOG
.
info
(
,
Time
.
now
(
)
-
start
)
;
int
syncInterval
=
conf
.
getInt
(
ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL
,
ZK_DTSM_ROUTER_TOKEN_SYNC_INTERVAL_DEFAULT
)
;
scheduler
.
scheduleAtFixedRate
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
rebuildTokenCache
(
false
)
;
}
catch
(
Exception
e
)
{
List
<
R
>
newRecords
=
query
.
getRecords
(
)
;
long
currentDriverTime
=
query
.
getTimestamp
(
)
;
if
(
newRecords
==
null
||
currentDriverTime
<=
0
)
{
LOG
.
error
(
)
;
return
;
}
for
(
R
record
:
newRecords
)
{
if
(
record
.
shouldBeDeleted
(
currentDriverTime
)
)
{
String
recordName
=
StateStoreUtils
.
getRecordName
(
record
.
getClass
(
)
)
;
if
(
getDriver
(
)
.
remove
(
record
)
)
{
deleteRecords
.
add
(
record
)
;
LOG
.
info
(
,
recordName
,
record
)
;
}
else
{
LOG
.
warn
(
,
recordName
,
record
)
;
}
}
else
if
(
record
.
checkExpired
(
currentDriverTime
)
)
{
String
recordName
=
StateStoreUtils
.
getRecordName
(
record
.
getClass
(
)
)
;
this
.
addService
(
monitorService
)
;
MembershipState
.
setExpirationMs
(
conf
.
getTimeDuration
(
RBFConfigKeys
.
FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS
,
RBFConfigKeys
.
FEDERATION_STORE_MEMBERSHIP_EXPIRATION_MS_DEFAULT
,
TimeUnit
.
MILLISECONDS
)
)
;
MembershipState
.
setDeletionMs
(
conf
.
getTimeDuration
(
RBFConfigKeys
.
FEDERATION_STORE_MEMBERSHIP_EXPIRATION_DELETION_MS
,
RBFConfigKeys
.
FEDERATION_STORE_MEMBERSHIP_EXPIRATION_DELETION_MS_DEFAULT
,
TimeUnit
.
MILLISECONDS
)
)
;
RouterState
.
setExpirationMs
(
conf
.
getTimeDuration
(
RBFConfigKeys
.
FEDERATION_STORE_ROUTER_EXPIRATION_MS
,
RBFConfigKeys
.
FEDERATION_STORE_ROUTER_EXPIRATION_MS_DEFAULT
,
TimeUnit
.
MILLISECONDS
)
)
;
RouterState
.
setDeletionMs
(
conf
.
getTimeDuration
(
RBFConfigKeys
.
FEDERATION_STORE_ROUTER_EXPIRATION_DELETION_MS
,
RBFConfigKeys
.
FEDERATION_STORE_ROUTER_EXPIRATION_DELETION_MS_DEFAULT
,
TimeUnit
.
MILLISECONDS
)
)
;
this
.
cacheUpdater
=
new
StateStoreCacheUpdateService
(
this
)
;
addService
(
this
.
cacheUpdater
)
;
if
(
conf
.
getBoolean
(
RBFConfigKeys
.
DFS_ROUTER_METRICS_ENABLE
,
RBFConfigKeys
.
DFS_ROUTER_METRICS_ENABLE_DEFAULT
)
)
{
this
.
metrics
=
StateStoreMetrics
.
create
(
conf
)
;
try
{
StandardMBean
bean
=
new
StandardMBean
(
metrics
,
StateStoreMBean
.
class
)
;
ObjectName
registeredObject
=
MBeans
.
register
(
,
,
bean
)
;
LOG
.
info
(
,
registeredObject
)
;
}
catch
(
NotCompliantMBeanException
e
)
{
throw
new
RuntimeException
(
,
e
)
;
public
void
loadDriver
(
)
{
synchronized
(
this
.
driver
)
{
if
(
!
isDriverReady
(
)
)
{
String
driverName
=
this
.
driver
.
getClass
(
)
.
getSimpleName
(
)
;
if
(
this
.
driver
.
init
(
conf
,
getIdentifier
(
)
,
getSupportedRecords
(
)
,
metrics
)
)
{
boolean
success
=
true
;
if
(
isDriverReady
(
)
)
{
List
<
StateStoreCache
>
cachesToUpdate
=
new
LinkedList
<
>
(
)
;
cachesToUpdate
.
addAll
(
cachesToUpdateInternal
)
;
cachesToUpdate
.
addAll
(
cachesToUpdateExternal
)
;
for
(
StateStoreCache
cachedStore
:
cachesToUpdate
)
{
String
cacheName
=
cachedStore
.
getClass
(
)
.
getSimpleName
(
)
;
boolean
result
=
false
;
try
{
result
=
cachedStore
.
loadCache
(
force
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
cacheName
,
e
)
;
result
=
false
;
}
if
(
!
result
)
{
success
=
false
;
@
Override
public
<
T
extends
BaseRecord
>
boolean
initRecordStorage
(
String
className
,
Class
<
T
>
recordClass
)
{
String
dataDirPath
=
getRootDir
(
)
+
+
className
;
try
{
if
(
!
exists
(
dataDirPath
)
)
{
@
Override
public
<
T
extends
BaseRecord
>
boolean
initRecordStorage
(
String
className
,
Class
<
T
>
recordClass
)
{
String
dataDirPath
=
getRootDir
(
)
+
+
className
;
try
{
if
(
!
exists
(
dataDirPath
)
)
{
LOG
.
info
(
,
dataDirPath
)
;
if
(
!
mkdir
(
dataDirPath
)
)
{
@
Override
public
<
T
extends
BaseRecord
>
QueryResult
<
T
>
get
(
Class
<
T
>
clazz
)
throws
IOException
{
verifyDriverReady
(
)
;
long
start
=
monotonicNow
(
)
;
StateStoreMetrics
metrics
=
getMetrics
(
)
;
List
<
T
>
ret
=
new
ArrayList
<
>
(
)
;
try
{
String
path
=
getPathForClass
(
clazz
)
;
List
<
String
>
children
=
getChildren
(
path
)
;
for
(
String
child
:
children
)
{
String
pathRecord
=
path
+
+
child
;
if
(
child
.
endsWith
(
TMP_MARK
)
)
{
List
<
String
>
children
=
getChildren
(
path
)
;
for
(
String
child
:
children
)
{
String
pathRecord
=
path
+
+
child
;
if
(
child
.
endsWith
(
TMP_MARK
)
)
{
LOG
.
debug
(
,
child
,
path
)
;
if
(
isOldTempRecord
(
child
)
)
{
LOG
.
warn
(
,
child
)
;
remove
(
pathRecord
)
;
}
}
else
{
T
record
=
getRecord
(
pathRecord
,
clazz
)
;
ret
.
add
(
record
)
;
}
}
}
catch
(
Exception
e
)
{
if
(
metrics
!=
null
)
{
metrics
.
addFailure
(
monotonicNow
(
)
-
start
)
;
}
String
msg
=
+
clazz
.
getSimpleName
(
)
;
if
(
records
.
isEmpty
(
)
)
{
return
true
;
}
long
start
=
monotonicNow
(
)
;
StateStoreMetrics
metrics
=
getMetrics
(
)
;
Map
<
String
,
T
>
toWrite
=
new
HashMap
<
>
(
)
;
for
(
T
record
:
records
)
{
Class
<
?
extends
BaseRecord
>
recordClass
=
record
.
getClass
(
)
;
String
path
=
getPathForClass
(
recordClass
)
;
String
primaryKey
=
getPrimaryKey
(
record
)
;
String
recordPath
=
path
+
+
primaryKey
;
if
(
exists
(
recordPath
)
)
{
if
(
allowUpdate
)
{
record
.
setDateModified
(
this
.
getTime
(
)
)
;
toWrite
.
put
(
recordPath
,
record
)
;
}
else
if
(
errorIfExists
)
{
Map
<
String
,
T
>
toWrite
=
new
HashMap
<
>
(
)
;
for
(
T
record
:
records
)
{
Class
<
?
extends
BaseRecord
>
recordClass
=
record
.
getClass
(
)
;
String
path
=
getPathForClass
(
recordClass
)
;
String
primaryKey
=
getPrimaryKey
(
record
)
;
String
recordPath
=
path
+
+
primaryKey
;
if
(
exists
(
recordPath
)
)
{
if
(
allowUpdate
)
{
record
.
setDateModified
(
this
.
getTime
(
)
)
;
toWrite
.
put
(
recordPath
,
record
)
;
}
else
if
(
errorIfExists
)
{
LOG
.
error
(
,
recordPath
)
;
if
(
metrics
!=
null
)
{
metrics
.
addFailure
(
monotonicNow
(
)
-
start
)
;
}
return
false
;
if
(
metrics
!=
null
)
{
metrics
.
addFailure
(
monotonicNow
(
)
-
start
)
;
}
return
false
;
}
else
{
LOG
.
debug
(
,
record
)
;
}
}
else
{
toWrite
.
put
(
recordPath
,
record
)
;
}
}
boolean
success
=
true
;
for
(
Entry
<
String
,
T
>
entry
:
toWrite
.
entrySet
(
)
)
{
String
recordPath
=
entry
.
getKey
(
)
;
String
recordPathTemp
=
recordPath
+
+
now
(
)
+
TMP_MARK
;
BufferedWriter
writer
=
getWriter
(
recordPathTemp
)
;
try
{
T
record
=
entry
.
getValue
(
)
;
String
line
=
serializeString
(
record
)
;
}
}
else
{
toWrite
.
put
(
recordPath
,
record
)
;
}
}
boolean
success
=
true
;
for
(
Entry
<
String
,
T
>
entry
:
toWrite
.
entrySet
(
)
)
{
String
recordPath
=
entry
.
getKey
(
)
;
String
recordPathTemp
=
recordPath
+
+
now
(
)
+
TMP_MARK
;
BufferedWriter
writer
=
getWriter
(
recordPathTemp
)
;
try
{
T
record
=
entry
.
getValue
(
)
;
String
line
=
serializeString
(
record
)
;
writer
.
write
(
line
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
recordPathTemp
,
e
)
;
success
=
false
;
}
finally
{
else
{
toWrite
.
put
(
recordPath
,
record
)
;
}
}
boolean
success
=
true
;
for
(
Entry
<
String
,
T
>
entry
:
toWrite
.
entrySet
(
)
)
{
String
recordPath
=
entry
.
getKey
(
)
;
String
recordPathTemp
=
recordPath
+
+
now
(
)
+
TMP_MARK
;
BufferedWriter
writer
=
getWriter
(
recordPathTemp
)
;
try
{
T
record
=
entry
.
getValue
(
)
;
String
line
=
serializeString
(
record
)
;
writer
.
write
(
line
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
recordPathTemp
,
e
)
;
success
=
false
;
}
finally
{
}
long
start
=
Time
.
monotonicNow
(
)
;
StateStoreMetrics
metrics
=
getMetrics
(
)
;
int
removed
=
0
;
try
{
final
QueryResult
<
T
>
result
=
get
(
clazz
)
;
final
List
<
T
>
existingRecords
=
result
.
getRecords
(
)
;
final
List
<
T
>
recordsToRemove
=
filterMultiple
(
query
,
existingRecords
)
;
boolean
success
=
true
;
for
(
T
recordToRemove
:
recordsToRemove
)
{
String
path
=
getPathForClass
(
clazz
)
;
String
primaryKey
=
getPrimaryKey
(
recordToRemove
)
;
String
recordToRemovePath
=
path
+
+
primaryKey
;
if
(
remove
(
recordToRemovePath
)
)
{
removed
++
;
}
else
{
int
removed
=
0
;
try
{
final
QueryResult
<
T
>
result
=
get
(
clazz
)
;
final
List
<
T
>
existingRecords
=
result
.
getRecords
(
)
;
final
List
<
T
>
recordsToRemove
=
filterMultiple
(
query
,
existingRecords
)
;
boolean
success
=
true
;
for
(
T
recordToRemove
:
recordsToRemove
)
{
String
path
=
getPathForClass
(
clazz
)
;
String
primaryKey
=
getPrimaryKey
(
recordToRemove
)
;
String
recordToRemovePath
=
path
+
+
primaryKey
;
if
(
remove
(
recordToRemovePath
)
)
{
removed
++
;
}
else
{
LOG
.
error
(
,
recordToRemovePath
)
;
success
=
false
;
final
List
<
T
>
recordsToRemove
=
filterMultiple
(
query
,
existingRecords
)
;
boolean
success
=
true
;
for
(
T
recordToRemove
:
recordsToRemove
)
{
String
path
=
getPathForClass
(
clazz
)
;
String
primaryKey
=
getPrimaryKey
(
recordToRemove
)
;
String
recordToRemovePath
=
path
+
+
primaryKey
;
if
(
remove
(
recordToRemovePath
)
)
{
removed
++
;
}
else
{
LOG
.
error
(
,
recordToRemovePath
)
;
success
=
false
;
}
}
if
(
!
success
)
{
LOG
.
error
(
,
clazz
,
query
)
;
if
(
metrics
!=
null
)
{
metrics
.
addFailure
(
monotonicNow
(
)
-
start
)
;
@
Override
protected
<
T
extends
BaseRecord
>
BufferedReader
getReader
(
String
filename
)
{
BufferedReader
reader
=
null
;
try
{
@
Override
protected
<
T
extends
BaseRecord
>
BufferedWriter
getWriter
(
String
filename
)
{
BufferedWriter
writer
=
null
;
try
{
try
{
List
<
String
>
children
=
zkManager
.
getChildren
(
znode
)
;
for
(
String
child
:
children
)
{
try
{
String
path
=
getNodePath
(
znode
,
child
)
;
Stat
stat
=
new
Stat
(
)
;
String
data
=
zkManager
.
getStringData
(
path
,
stat
)
;
boolean
corrupted
=
false
;
if
(
data
==
null
||
data
.
equals
(
)
)
{
corrupted
=
true
;
}
else
{
try
{
T
record
=
createRecord
(
data
,
stat
,
clazz
)
;
ret
.
add
(
record
)
;
}
catch
(
IOException
e
)
{
try
{
String
path
=
getNodePath
(
znode
,
child
)
;
Stat
stat
=
new
Stat
(
)
;
String
data
=
zkManager
.
getStringData
(
path
,
stat
)
;
boolean
corrupted
=
false
;
if
(
data
==
null
||
data
.
equals
(
)
)
{
corrupted
=
true
;
}
else
{
try
{
T
record
=
createRecord
(
data
,
stat
,
clazz
)
;
ret
.
add
(
record
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
clazz
.
getSimpleName
(
)
,
data
,
e
.
getMessage
(
)
)
;
corrupted
=
true
;
}
}
if
(
corrupted
)
{
String
data
=
zkManager
.
getStringData
(
path
,
stat
)
;
boolean
corrupted
=
false
;
if
(
data
==
null
||
data
.
equals
(
)
)
{
corrupted
=
true
;
}
else
{
try
{
T
record
=
createRecord
(
data
,
stat
,
clazz
)
;
ret
.
add
(
record
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
clazz
.
getSimpleName
(
)
,
data
,
e
.
getMessage
(
)
)
;
corrupted
=
true
;
}
}
if
(
corrupted
)
{
LOG
.
error
(
,
child
,
path
)
;
zkManager
.
delete
(
path
)
;
}
}
catch
(
Exception
e
)
{
}
else
{
try
{
T
record
=
createRecord
(
data
,
stat
,
clazz
)
;
ret
.
add
(
record
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
clazz
.
getSimpleName
(
)
,
data
,
e
.
getMessage
(
)
)
;
corrupted
=
true
;
}
}
if
(
corrupted
)
{
LOG
.
error
(
,
child
,
path
)
;
zkManager
.
delete
(
path
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
child
,
e
.
getMessage
(
)
)
;
}
}
}
catch
(
Exception
e
)
{
getMetrics
(
)
.
addFailure
(
monotonicNow
(
)
-
start
)
;
String
msg
=
+
znode
+
+
e
.
getMessage
(
)
;
if
(
query
==
null
)
{
return
0
;
}
long
start
=
monotonicNow
(
)
;
List
<
T
>
records
=
null
;
try
{
QueryResult
<
T
>
result
=
get
(
clazz
)
;
records
=
result
.
getRecords
(
)
;
}
catch
(
IOException
ex
)
{
LOG
.
error
(
,
ex
)
;
getMetrics
(
)
.
addFailure
(
monotonicNow
(
)
-
start
)
;
return
0
;
}
String
znode
=
getZNodeForClass
(
clazz
)
;
List
<
T
>
recordsToRemove
=
filterMultiple
(
query
,
records
)
;
int
removed
=
0
;
for
(
T
existingRecord
:
recordsToRemove
)
{
records
=
result
.
getRecords
(
)
;
}
catch
(
IOException
ex
)
{
LOG
.
error
(
,
ex
)
;
getMetrics
(
)
.
addFailure
(
monotonicNow
(
)
-
start
)
;
return
0
;
}
String
znode
=
getZNodeForClass
(
clazz
)
;
List
<
T
>
recordsToRemove
=
filterMultiple
(
query
,
records
)
;
int
removed
=
0
;
for
(
T
existingRecord
:
recordsToRemove
)
{
LOG
.
info
(
,
existingRecord
)
;
try
{
String
primaryKey
=
getPrimaryKey
(
existingRecord
)
;
String
path
=
getNodePath
(
znode
,
primaryKey
)
;
if
(
zkManager
.
delete
(
path
)
)
{
removed
++
;
catch
(
IOException
ex
)
{
LOG
.
error
(
,
ex
)
;
getMetrics
(
)
.
addFailure
(
monotonicNow
(
)
-
start
)
;
return
0
;
}
String
znode
=
getZNodeForClass
(
clazz
)
;
List
<
T
>
recordsToRemove
=
filterMultiple
(
query
,
records
)
;
int
removed
=
0
;
for
(
T
existingRecord
:
recordsToRemove
)
{
LOG
.
info
(
,
existingRecord
)
;
try
{
String
primaryKey
=
getPrimaryKey
(
existingRecord
)
;
String
path
=
getNodePath
(
znode
,
primaryKey
)
;
if
(
zkManager
.
delete
(
path
)
)
{
removed
++
;
}
else
{
@
Override
public
<
T
extends
BaseRecord
>
boolean
removeAll
(
Class
<
T
>
clazz
)
throws
IOException
{
long
start
=
monotonicNow
(
)
;
boolean
status
=
true
;
String
znode
=
getZNodeForClass
(
clazz
)
;
@
Override
public
<
T
extends
BaseRecord
>
boolean
removeAll
(
Class
<
T
>
clazz
)
throws
IOException
{
long
start
=
monotonicNow
(
)
;
boolean
status
=
true
;
String
znode
=
getZNodeForClass
(
clazz
)
;
LOG
.
info
(
,
znode
)
;
try
{
List
<
String
>
children
=
zkManager
.
getChildren
(
znode
)
;
for
(
String
child
:
children
)
{
String
path
=
getNodePath
(
znode
,
child
)
;
private
boolean
writeNode
(
String
znode
,
byte
[
]
bytes
,
boolean
update
,
boolean
error
)
{
try
{
boolean
created
=
zkManager
.
create
(
znode
)
;
if
(
!
update
&&
!
created
&&
error
)
{
@
Override
public
NamenodeHeartbeatResponse
namenodeHeartbeat
(
NamenodeHeartbeatRequest
request
)
throws
IOException
{
MembershipState
record
=
request
.
getNamenodeMembership
(
)
;
String
nnId
=
record
.
getNamenodeKey
(
)
;
MembershipState
existingEntry
=
null
;
cacheReadLock
.
lock
(
)
;
try
{
existingEntry
=
this
.
activeRegistrations
.
get
(
nnId
)
;
}
finally
{
cacheReadLock
.
unlock
(
)
;
}
if
(
existingEntry
!=
null
)
{
if
(
existingEntry
.
getState
(
)
!=
record
.
getState
(
)
)
{
LOG
.
info
(
,
existingEntry
,
record
)
;
}
else
{
LOG
.
debug
(
,
existingEntry
,
record
)
;
}
}
else
{
for
(
MembershipState
record
:
records
)
{
FederationNamenodeServiceState
state
=
record
.
getState
(
)
;
TreeSet
<
MembershipState
>
matchingSet
=
occurenceMap
.
get
(
state
)
;
if
(
matchingSet
==
null
)
{
matchingSet
=
new
TreeSet
<
>
(
)
;
occurenceMap
.
put
(
state
,
matchingSet
)
;
}
matchingSet
.
add
(
record
)
;
}
TreeSet
<
MembershipState
>
largestSet
=
new
TreeSet
<
>
(
)
;
for
(
TreeSet
<
MembershipState
>
matchingSet
:
occurenceMap
.
values
(
)
)
{
if
(
largestSet
.
size
(
)
<
matchingSet
.
size
(
)
)
{
largestSet
=
matchingSet
;
}
}
if
(
largestSet
.
size
(
)
>
records
.
size
(
)
/
2
)
{
return
largestSet
.
first
(
)
;
}
else
if
(
records
.
size
(
)
>
0
)
{
TreeSet
<
MembershipState
>
sortedList
=
new
TreeSet
<
>
(
records
)
;
public
static
void
simulateSlowNamenode
(
final
NameNode
nn
,
final
int
seconds
)
throws
Exception
{
FSNamesystem
namesystem
=
nn
.
getNamesystem
(
)
;
HAContext
haContext
=
namesystem
.
getHAContext
(
)
;
HAContext
spyHAContext
=
spy
(
haContext
)
;
doAnswer
(
new
Answer
<
Object
>
(
)
{
@
Override
public
Object
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
public
static
void
simulateThrowExceptionRouterRpcServer
(
final
RouterRpcServer
server
)
throws
IOException
{
RouterRpcClient
rpcClient
=
server
.
getRPCClient
(
)
;
ConnectionManager
connectionManager
=
new
ConnectionManager
(
server
.
getConfig
(
)
)
;
ConnectionManager
spyConnectionManager
=
spy
(
connectionManager
)
;
doAnswer
(
new
Answer
(
)
{
@
Override
public
Object
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
public
void
waitRouterRegistrationQuorum
(
RouterContext
router
,
FederationNamenodeServiceState
state
,
String
nsId
,
String
nnId
)
throws
Exception
{
public
void
stopRouter
(
RouterContext
router
)
{
try
{
router
.
router
.
shutDown
(
)
;
int
loopCount
=
0
;
while
(
router
.
router
.
getServiceState
(
)
!=
STATE
.
STOPPED
)
{
loopCount
++
;
Thread
.
sleep
(
1000
)
;
if
(
loopCount
>
20
)
{
public
void
addFileSystemMock
(
)
throws
IOException
{
final
SortedMap
<
String
,
String
>
fs
=
new
ConcurrentSkipListMap
<
String
,
String
>
(
)
;
DirectoryListing
l
=
mockNn
.
getListing
(
anyString
(
)
,
any
(
)
,
anyBoolean
(
)
)
;
when
(
l
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
if
(
fs
.
get
(
src
)
==
null
)
{
throw
new
FileNotFoundException
(
+
src
)
;
}
if
(
!
src
.
endsWith
(
)
)
{
src
+=
;
}
Map
<
String
,
String
>
files
=
fs
.
subMap
(
src
,
src
+
Character
.
MAX_VALUE
)
;
List
<
HdfsFileStatus
>
list
=
new
ArrayList
<
>
(
)
;
for
(
String
file
:
files
.
keySet
(
)
)
{
if
(
file
.
substring
(
src
.
length
(
)
)
.
indexOf
(
'/'
)
<
0
)
{
HdfsFileStatus
fileStatus
=
getMockHdfsFileStatus
(
file
,
fs
.
get
(
file
)
)
;
list
.
add
(
fileStatus
)
;
}
}
HdfsFileStatus
[
]
array
=
list
.
toArray
(
new
HdfsFileStatus
[
list
.
size
(
)
]
)
;
return
new
DirectoryListing
(
array
,
0
)
;
}
)
;
when
(
mockNn
.
getFileInfo
(
anyString
(
)
)
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
Map
<
String
,
String
>
files
=
fs
.
subMap
(
src
,
src
+
Character
.
MAX_VALUE
)
;
List
<
HdfsFileStatus
>
list
=
new
ArrayList
<
>
(
)
;
for
(
String
file
:
files
.
keySet
(
)
)
{
if
(
file
.
substring
(
src
.
length
(
)
)
.
indexOf
(
'/'
)
<
0
)
{
HdfsFileStatus
fileStatus
=
getMockHdfsFileStatus
(
file
,
fs
.
get
(
file
)
)
;
list
.
add
(
fileStatus
)
;
}
}
HdfsFileStatus
[
]
array
=
list
.
toArray
(
new
HdfsFileStatus
[
list
.
size
(
)
]
)
;
return
new
DirectoryListing
(
array
,
0
)
;
}
)
;
when
(
mockNn
.
getFileInfo
(
anyString
(
)
)
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
return
getMockHdfsFileStatus
(
src
,
fs
.
get
(
src
)
)
;
}
)
;
HdfsFileStatus
c
=
mockNn
.
create
(
anyString
(
)
,
any
(
)
,
anyString
(
)
,
any
(
)
,
anyBoolean
(
)
,
anyShort
(
)
,
anyLong
(
)
,
any
(
)
,
any
(
)
,
any
(
)
)
;
list
.
add
(
fileStatus
)
;
}
}
HdfsFileStatus
[
]
array
=
list
.
toArray
(
new
HdfsFileStatus
[
list
.
size
(
)
]
)
;
return
new
DirectoryListing
(
array
,
0
)
;
}
)
;
when
(
mockNn
.
getFileInfo
(
anyString
(
)
)
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
return
getMockHdfsFileStatus
(
src
,
fs
.
get
(
src
)
)
;
}
)
;
HdfsFileStatus
c
=
mockNn
.
create
(
anyString
(
)
,
any
(
)
,
anyString
(
)
,
any
(
)
,
anyBoolean
(
)
,
anyShort
(
)
,
anyLong
(
)
,
any
(
)
,
any
(
)
,
any
(
)
)
;
when
(
c
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
boolean
createParent
=
(
boolean
)
invocation
.
getArgument
(
4
)
;
if
(
createParent
)
{
LOG
.
info
(
,
nsId
,
src
)
;
return
getMockHdfsFileStatus
(
src
,
fs
.
get
(
src
)
)
;
}
)
;
HdfsFileStatus
c
=
mockNn
.
create
(
anyString
(
)
,
any
(
)
,
anyString
(
)
,
any
(
)
,
anyBoolean
(
)
,
anyShort
(
)
,
anyLong
(
)
,
any
(
)
,
any
(
)
,
any
(
)
)
;
when
(
c
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
boolean
createParent
=
(
boolean
)
invocation
.
getArgument
(
4
)
;
if
(
createParent
)
{
Path
path
=
new
Path
(
src
)
.
getParent
(
)
;
while
(
!
path
.
isRoot
(
)
)
{
LOG
.
info
(
,
nsId
,
path
)
;
fs
.
put
(
path
.
toString
(
)
,
)
;
path
=
path
.
getParent
(
)
;
}
}
fs
.
put
(
src
,
)
;
}
)
;
HdfsFileStatus
c
=
mockNn
.
create
(
anyString
(
)
,
any
(
)
,
anyString
(
)
,
any
(
)
,
anyBoolean
(
)
,
anyShort
(
)
,
anyLong
(
)
,
any
(
)
,
any
(
)
,
any
(
)
)
;
when
(
c
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
boolean
createParent
=
(
boolean
)
invocation
.
getArgument
(
4
)
;
if
(
createParent
)
{
Path
path
=
new
Path
(
src
)
.
getParent
(
)
;
while
(
!
path
.
isRoot
(
)
)
{
LOG
.
info
(
,
nsId
,
path
)
;
fs
.
put
(
path
.
toString
(
)
,
)
;
path
=
path
.
getParent
(
)
;
}
}
fs
.
put
(
src
,
)
;
return
getMockHdfsFileStatus
(
src
,
)
;
}
)
;
Path
path
=
new
Path
(
src
)
.
getParent
(
)
;
while
(
!
path
.
isRoot
(
)
)
{
LOG
.
info
(
,
nsId
,
path
)
;
fs
.
put
(
path
.
toString
(
)
,
)
;
path
=
path
.
getParent
(
)
;
}
}
fs
.
put
(
src
,
)
;
return
getMockHdfsFileStatus
(
src
,
)
;
}
)
;
LocatedBlocks
b
=
mockNn
.
getBlockLocations
(
anyString
(
)
,
anyLong
(
)
,
anyLong
(
)
)
;
when
(
b
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
return
getMockHdfsFileStatus
(
src
,
)
;
}
)
;
LocatedBlocks
b
=
mockNn
.
getBlockLocations
(
anyString
(
)
,
anyLong
(
)
,
anyLong
(
)
)
;
when
(
b
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
}
return
mock
(
LocatedBlocks
.
class
)
;
}
)
;
boolean
f
=
mockNn
.
complete
(
anyString
(
)
,
anyString
(
)
,
any
(
)
,
anyLong
(
)
)
;
when
(
f
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
}
return
mock
(
LocatedBlocks
.
class
)
;
}
)
;
boolean
f
=
mockNn
.
complete
(
anyString
(
)
,
anyString
(
)
,
any
(
)
,
anyLong
(
)
)
;
when
(
f
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
}
return
true
;
}
)
;
LocatedBlock
a
=
mockNn
.
addBlock
(
anyString
(
)
,
anyString
(
)
,
any
(
)
,
any
(
)
,
anyLong
(
)
,
any
(
)
,
any
(
)
)
;
when
(
a
)
.
thenAnswer
(
invocation
->
{
}
)
;
boolean
f
=
mockNn
.
complete
(
anyString
(
)
,
anyString
(
)
,
any
(
)
,
anyLong
(
)
)
;
when
(
f
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
}
return
true
;
}
)
;
LocatedBlock
a
=
mockNn
.
addBlock
(
anyString
(
)
,
anyString
(
)
,
any
(
)
,
any
(
)
,
anyLong
(
)
,
any
(
)
,
any
(
)
)
;
when
(
a
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
}
return
true
;
}
)
;
LocatedBlock
a
=
mockNn
.
addBlock
(
anyString
(
)
,
anyString
(
)
,
any
(
)
,
any
(
)
,
anyLong
(
)
,
any
(
)
,
any
(
)
)
;
when
(
a
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
}
return
getMockLocatedBlock
(
nsId
)
;
}
)
;
boolean
m
=
mockNn
.
mkdirs
(
anyString
(
)
,
any
(
)
,
anyBoolean
(
)
)
;
when
(
m
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
if
(
!
fs
.
containsKey
(
src
)
)
{
LOG
.
error
(
,
nsId
,
src
)
;
throw
new
FileNotFoundException
(
+
src
)
;
}
return
getMockLocatedBlock
(
nsId
)
;
}
)
;
boolean
m
=
mockNn
.
mkdirs
(
anyString
(
)
,
any
(
)
,
anyBoolean
(
)
)
;
when
(
m
)
.
thenAnswer
(
invocation
->
{
String
src
=
getSrc
(
invocation
)
;
LOG
.
info
(
,
nsId
,
src
)
;
boolean
createParent
=
(
boolean
)
invocation
.
getArgument
(
2
)
;
if
(
createParent
)
{
Path
path
=
new
Path
(
src
)
.
getParent
(
)
;
while
(
!
path
.
isRoot
(
)
)
{
LOG
.
info
(
,
nsId
,
path
)
;
fs
.
put
(
path
.
toString
(
)
,
)
;
public
void
addDatanodeMock
(
)
throws
IOException
{
when
(
mockNn
.
getDatanodeReport
(
any
(
DatanodeReportType
.
class
)
)
)
.
thenAnswer
(
invocation
->
{
public
static
void
registerSubclusters
(
List
<
Router
>
routers
,
Collection
<
MockNamenode
>
namenodes
,
Set
<
String
>
unavailableSubclusters
)
throws
IOException
{
for
(
final
Router
router
:
routers
)
{
MembershipNamenodeResolver
resolver
=
(
MembershipNamenodeResolver
)
router
.
getNamenodeResolver
(
)
;
for
(
final
MockNamenode
nn
:
namenodes
)
{
String
nsId
=
nn
.
getNameserviceId
(
)
;
String
rpcAddress
=
+
nn
.
getRPCPort
(
)
;
String
httpAddress
=
+
nn
.
getHTTPPort
(
)
;
String
scheme
=
;
NamenodeStatusReport
report
=
new
NamenodeStatusReport
(
nsId
,
null
,
rpcAddress
,
rpcAddress
,
rpcAddress
,
scheme
,
httpAddress
)
;
if
(
unavailableSubclusters
.
contains
(
nsId
)
)
{
@
Test
public
void
testMountTableScalability
(
)
throws
IOException
{
List
<
MountTable
>
emptyList
=
new
ArrayList
<
>
(
)
;
mountTable
.
refreshEntries
(
emptyList
)
;
for
(
int
i
=
0
;
i
<
100000
;
i
++
)
{
Map
<
String
,
String
>
map
=
getMountTableEntry
(
,
+
i
)
;
MountTable
record
=
MountTable
.
newInstance
(
+
i
,
map
)
;
mountTable
.
addEntry
(
record
)
;
if
(
i
%
10000
==
0
)
{
finally
{
if
(
routerClient
!=
null
)
{
try
{
routerClient
.
close
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
)
;
}
}
}
}
)
;
GenericTestUtils
.
waitFor
(
(
)
->
{
try
{
Map
<
String
,
Integer
>
newResult
=
objectMapper
.
readValue
(
metrics
.
getAsyncCallerPool
(
)
,
Map
.
class
)
;
if
(
newResult
.
get
(
)
!=
1
)
{
return
false
;
}
if
(
newResult
.
get
(
)
!=
4
)
{
return
false
;
}
int
total
=
newResult
.
get
(
)
;
private
void
testWriteWithFailedSubcluster
(
final
DestinationOrder
order
)
throws
Exception
{
final
FileSystem
router0Fs
=
getFileSystem
(
routers
.
get
(
0
)
)
;
final
FileSystem
router1Fs
=
getFileSystem
(
routers
.
get
(
1
)
)
;
final
FileSystem
ns0Fs
=
getFileSystem
(
namenodes
.
get
(
)
.
getRPCPort
(
)
)
;
final
String
mountPoint
=
+
order
+
;
final
Path
mountPath
=
new
Path
(
mountPoint
)
;
private
void
testWriteWithFailedSubcluster
(
final
DestinationOrder
order
)
throws
Exception
{
final
FileSystem
router0Fs
=
getFileSystem
(
routers
.
get
(
0
)
)
;
final
FileSystem
router1Fs
=
getFileSystem
(
routers
.
get
(
1
)
)
;
final
FileSystem
ns0Fs
=
getFileSystem
(
namenodes
.
get
(
)
.
getRPCPort
(
)
)
;
final
String
mountPoint
=
+
order
+
;
final
Path
mountPath
=
new
Path
(
mountPoint
)
;
LOG
.
info
(
,
mountPoint
,
order
)
;
createMountTableEntry
(
getRandomRouter
(
)
,
mountPoint
,
order
,
namenodes
.
keySet
(
)
)
;
refreshRoutersCaches
(
routers
)
;
private
void
testWriteWithFailedSubcluster
(
final
DestinationOrder
order
)
throws
Exception
{
final
FileSystem
router0Fs
=
getFileSystem
(
routers
.
get
(
0
)
)
;
final
FileSystem
router1Fs
=
getFileSystem
(
routers
.
get
(
1
)
)
;
final
FileSystem
ns0Fs
=
getFileSystem
(
namenodes
.
get
(
)
.
getRPCPort
(
)
)
;
final
String
mountPoint
=
+
order
+
;
final
Path
mountPath
=
new
Path
(
mountPoint
)
;
LOG
.
info
(
,
mountPoint
,
order
)
;
createMountTableEntry
(
getRandomRouter
(
)
,
mountPoint
,
order
,
namenodes
.
keySet
(
)
)
;
refreshRoutersCaches
(
routers
)
;
LOG
.
info
(
,
mountPath
)
;
checkDirectoriesFaultTolerant
(
mountPath
,
order
,
router0Fs
,
router1Fs
,
ns0Fs
,
false
)
;
checkFilesFaultTolerant
(
mountPath
,
order
,
router0Fs
,
router1Fs
,
ns0Fs
,
false
)
;
private
void
checkDirectoriesFaultTolerant
(
Path
mountPoint
,
DestinationOrder
order
,
FileSystem
router0Fs
,
FileSystem
router1Fs
,
FileSystem
ns0Fs
,
boolean
faultTolerant
)
throws
Exception
{
final
FileStatus
[
]
dirs0
=
listStatus
(
router1Fs
,
mountPoint
)
;
private
void
checkFilesFaultTolerant
(
Path
mountPoint
,
DestinationOrder
order
,
FileSystem
router0Fs
,
FileSystem
router1Fs
,
FileSystem
ns0Fs
,
boolean
faultTolerant
)
throws
Exception
{
final
FileStatus
[
]
dirs0
=
listStatus
(
router1Fs
,
mountPoint
)
;
final
Path
dir0
=
Path
.
getPathWithoutSchemeAndAuthority
(
dirs0
[
0
]
.
getPath
(
)
)
;
private
TaskResults
collectResults
(
final
String
tag
,
final
Collection
<
Callable
<
Boolean
>>
tasks
)
throws
Exception
{
final
TaskResults
results
=
new
TaskResults
(
)
;
service
.
invokeAll
(
tasks
)
.
forEach
(
task
->
{
try
{
boolean
succeeded
=
task
.
get
(
)
;
if
(
succeeded
)
{
@
Test
public
void
testReadWithFailedSubcluster
(
)
throws
Exception
{
DestinationOrder
order
=
DestinationOrder
.
HASH_ALL
;
final
String
mountPoint
=
+
order
+
;
final
Path
mountPath
=
new
Path
(
mountPoint
)
;
FSDataInputStream
fsdis
=
fs
.
open
(
fileexisting
)
;
assertNotNull
(
,
fsdis
)
;
LambdaTestUtils
.
intercept
(
FileNotFoundException
.
class
,
(
)
->
fs
.
open
(
filenotexisting
)
)
;
String
nsIdWithFile
=
null
;
for
(
Entry
<
String
,
MockNamenode
>
entry
:
namenodes
.
entrySet
(
)
)
{
String
nsId
=
entry
.
getKey
(
)
;
MockNamenode
nn
=
entry
.
getValue
(
)
;
int
rpc
=
nn
.
getRPCPort
(
)
;
FileSystem
nnfs
=
getFileSystem
(
rpc
)
;
try
{
FileStatus
fileStatus
=
nnfs
.
getFileStatus
(
fileexisting
)
;
assertNotNull
(
fileStatus
)
;
assertNull
(
,
nsIdWithFile
)
;
nsIdWithFile
=
nsId
;
}
catch
(
FileNotFoundException
fnfe
)
{
LambdaTestUtils
.
intercept
(
FileNotFoundException
.
class
,
(
)
->
fs
.
open
(
filenotexisting
)
)
;
String
nsIdWithFile
=
null
;
for
(
Entry
<
String
,
MockNamenode
>
entry
:
namenodes
.
entrySet
(
)
)
{
String
nsId
=
entry
.
getKey
(
)
;
MockNamenode
nn
=
entry
.
getValue
(
)
;
int
rpc
=
nn
.
getRPCPort
(
)
;
FileSystem
nnfs
=
getFileSystem
(
rpc
)
;
try
{
FileStatus
fileStatus
=
nnfs
.
getFileStatus
(
fileexisting
)
;
assertNotNull
(
fileStatus
)
;
assertNull
(
,
nsIdWithFile
)
;
nsIdWithFile
=
nsId
;
}
catch
(
FileNotFoundException
fnfe
)
{
LOG
.
debug
(
,
nsId
)
;
}
}
assertNotNull
(
,
nsIdWithFile
)
;
@
Test
public
void
testFsck
(
)
throws
Exception
{
MountTable
addEntry
=
MountTable
.
newInstance
(
,
Collections
.
singletonMap
(
,
)
)
;
assertTrue
(
addMountTable
(
addEntry
)
)
;
addEntry
=
MountTable
.
newInstance
(
,
Collections
.
singletonMap
(
,
)
)
;
assertTrue
(
addMountTable
(
addEntry
)
)
;
routerFs
.
createNewFile
(
new
Path
(
)
)
;
routerFs
.
createNewFile
(
new
Path
(
)
)
;
routerFs
.
createNewFile
(
new
Path
(
)
)
;
routerFs
.
createNewFile
(
new
Path
(
)
)
;
try
(
CloseableHttpClient
httpClient
=
HttpClients
.
createDefault
(
)
)
{
HttpGet
httpGet
=
new
HttpGet
(
+
webAddress
.
getHostName
(
)
+
+
webAddress
.
getPort
(
)
+
)
;
try
(
CloseableHttpResponse
httpResponse
=
httpClient
.
execute
(
httpGet
)
)
{
assertEquals
(
HttpStatus
.
SC_OK
,
httpResponse
.
getStatusLine
(
)
.
getStatusCode
(
)
)
;
String
out
=
EntityUtils
.
toString
(
httpResponse
.
getEntity
(
)
,
StandardCharsets
.
UTF_8
)
;
LOG
.
info
(
out
)
;
assertTrue
(
out
.
contains
(
)
)
;
assertTrue
(
out
.
contains
(
)
)
;
assertTrue
(
out
.
contains
(
)
)
;
assertTrue
(
out
.
contains
(
)
)
;
int
nnCount
=
0
;
for
(
MembershipState
nn
:
memberships
)
{
if
(
nn
.
getState
(
)
==
FederationNamenodeServiceState
.
ACTIVE
)
{
assertTrue
(
out
.
contains
(
+
nn
+
+
nn
.
getWebAddress
(
)
+
)
)
;
nnCount
++
;
}
}
assertEquals
(
2
,
nnCount
)
;
}
httpGet
=
new
HttpGet
(
+
webAddress
.
getHostName
(
)
+
+
webAddress
.
getPort
(
)
+
)
;
try
(
CloseableHttpResponse
httpResponse
=
httpClient
.
execute
(
httpGet
)
)
{
assertEquals
(
HttpStatus
.
SC_OK
,
httpResponse
.
getStatusLine
(
)
.
getStatusCode
(
)
)
;
String
out
=
EntityUtils
.
toString
(
httpResponse
.
getEntity
(
)
,
StandardCharsets
.
UTF_8
)
;
@
Test
public
void
testProxyGetStats
(
)
throws
Exception
{
Supplier
<
Boolean
>
check
=
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
try
{
long
[
]
combinedData
=
routerProtocol
.
getStats
(
)
;
long
[
]
individualData
=
getAggregateStats
(
)
;
int
len
=
Math
.
min
(
combinedData
.
length
,
individualData
.
length
)
;
for
(
int
i
=
0
;
i
<
len
;
i
++
)
{
if
(
combinedData
[
i
]
!=
individualData
[
i
]
)
{
@
Test
public
void
testManageSnapshot
(
)
throws
Exception
{
final
String
mountPoint
=
;
final
String
snapshotFolder
=
mountPoint
+
;
@
Test
public
void
testErasureCoding
(
)
throws
Exception
{
LOG
.
info
(
)
;
ErasureCodingPolicyInfo
[
]
policies
=
checkErasureCodingPolicies
(
)
;
for
(
ErasureCodingPolicyInfo
policy
:
policies
)
{
Map
<
String
,
String
>
codecsNamenode
=
nnProtocol
.
getErasureCodingCodecs
(
)
;
assertTrue
(
Maps
.
difference
(
codecsRouter
,
codecsNamenode
)
.
areEqual
(
)
)
;
for
(
Entry
<
String
,
String
>
entry
:
codecsRouter
.
entrySet
(
)
)
{
LOG
.
info
(
,
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
LOG
.
info
(
)
;
String
dirPath
=
;
String
filePath1
=
dirPath
+
;
FsPermission
permission
=
new
FsPermission
(
)
;
routerProtocol
.
mkdirs
(
dirPath
,
permission
,
false
)
;
createFile
(
routerFS
,
filePath1
,
32
)
;
assertTrue
(
verifyFileExists
(
routerFS
,
filePath1
)
)
;
DFSClient
file1Protocol
=
getFileDFSClient
(
filePath1
)
;
LOG
.
info
(
)
;
assertNull
(
routerProtocol
.
getErasureCodingPolicy
(
filePath1
)
)
;
assertNull
(
file1Protocol
.
getErasureCodingPolicy
(
filePath1
)
)
;
String
filePath1
=
dirPath
+
;
FsPermission
permission
=
new
FsPermission
(
)
;
routerProtocol
.
mkdirs
(
dirPath
,
permission
,
false
)
;
createFile
(
routerFS
,
filePath1
,
32
)
;
assertTrue
(
verifyFileExists
(
routerFS
,
filePath1
)
)
;
DFSClient
file1Protocol
=
getFileDFSClient
(
filePath1
)
;
LOG
.
info
(
)
;
assertNull
(
routerProtocol
.
getErasureCodingPolicy
(
filePath1
)
)
;
assertNull
(
file1Protocol
.
getErasureCodingPolicy
(
filePath1
)
)
;
String
policyName
=
;
LOG
.
info
(
,
policyName
,
dirPath
)
;
routerProtocol
.
setErasureCodingPolicy
(
dirPath
,
policyName
)
;
String
filePath2
=
dirPath
+
;
LOG
.
info
(
,
filePath2
)
;
createFile
(
routerFS
,
filePath2
,
32
)
;
private
DFSClient
getFileDFSClient
(
final
String
path
)
{
for
(
String
nsId
:
cluster
.
getNameservices
(
)
)
{
when
(
ugi1
.
getRealUser
(
)
)
.
thenReturn
(
suUgi
)
;
when
(
ugi2
.
getRealUser
(
)
)
.
thenReturn
(
suUgi
)
;
when
(
suUgi
.
getShortUserName
(
)
)
.
thenReturn
(
superUser
)
;
when
(
suUgi
.
getUserName
(
)
)
.
thenReturn
(
superUser
+
)
;
when
(
ugi1
.
getShortUserName
(
)
)
.
thenReturn
(
)
;
when
(
ugi2
.
getShortUserName
(
)
)
.
thenReturn
(
)
;
when
(
ugi1
.
getUserName
(
)
)
.
thenReturn
(
)
;
when
(
ugi2
.
getUserName
(
)
)
.
thenReturn
(
)
;
when
(
ugi1
.
getGroups
(
)
)
.
thenReturn
(
groupNames1
)
;
when
(
ugi2
.
getGroups
(
)
)
.
thenReturn
(
groupNames2
)
;
when
(
ugi1
.
getGroupsSet
(
)
)
.
thenReturn
(
groupNamesSet1
)
;
when
(
ugi2
.
getGroupsSet
(
)
)
.
thenReturn
(
groupNamesSet2
)
;
LambdaTestUtils
.
intercept
(
AuthorizationException
.
class
,
(
)
->
ProxyUsers
.
authorize
(
ugi1
,
LOOPBACK_ADDRESS
)
)
;
try
{
ProxyUsers
.
authorize
(
ugi2
,
LOOPBACK_ADDRESS
)
;
LambdaTestUtils
.
intercept
(
AuthorizationException
.
class
,
(
)
->
ProxyUsers
.
authorize
(
ugi1
,
LOOPBACK_ADDRESS
)
)
;
try
{
ProxyUsers
.
authorize
(
ugi2
,
LOOPBACK_ADDRESS
)
;
LOG
.
info
(
,
ugi2
.
getUserName
(
)
)
;
}
catch
(
AuthorizationException
e
)
{
fail
(
+
ugi2
.
getShortUserName
(
)
+
+
e
.
getLocalizedMessage
(
)
)
;
}
String
rsrc
=
;
tempResource
=
addNewConfigResource
(
rsrc
,
userKeyGroups
,
,
userKeyHosts
,
LOOPBACK_ADDRESS
)
;
conf
.
set
(
DFSConfigKeys
.
FS_DEFAULT_NAME_KEY
,
defaultFs
)
;
DFSAdmin
admin
=
new
DFSAdmin
(
conf
)
;
String
[
]
args
=
new
String
[
]
{
}
;
admin
.
run
(
args
)
;
LambdaTestUtils
.
intercept
(
AuthorizationException
.
class
,
(
)
->
ProxyUsers
.
authorize
(
ugi2
,
LOOPBACK_ADDRESS
)
)
;
try
{
ProxyUsers
.
authorize
(
ugi1
,
LOOPBACK_ADDRESS
)
;
private
void
testGroupMappingRefreshInternal
(
String
defaultFs
)
throws
Exception
{
Groups
groups
=
Groups
.
getUserToGroupsMappingService
(
conf
)
;
String
user
=
;
LOG
.
info
(
)
;
List
<
String
>
g1
=
groups
.
getGroups
(
user
)
;
private
void
testGroupMappingRefreshInternal
(
String
defaultFs
)
throws
Exception
{
Groups
groups
=
Groups
.
getUserToGroupsMappingService
(
conf
)
;
String
user
=
;
LOG
.
info
(
)
;
List
<
String
>
g1
=
groups
.
getGroups
(
user
)
;
LOG
.
info
(
,
g1
)
;
LOG
.
info
(
)
;
List
<
String
>
g2
=
groups
.
getGroups
(
user
)
;
String
user
=
;
LOG
.
info
(
)
;
List
<
String
>
g1
=
groups
.
getGroups
(
user
)
;
LOG
.
info
(
,
g1
)
;
LOG
.
info
(
)
;
List
<
String
>
g2
=
groups
.
getGroups
(
user
)
;
LOG
.
info
(
,
g2
)
;
for
(
int
i
=
0
;
i
<
g2
.
size
(
)
;
i
++
)
{
assertEquals
(
,
g1
.
get
(
i
)
,
g2
.
get
(
i
)
)
;
}
conf
.
set
(
DFSConfigKeys
.
FS_DEFAULT_NAME_KEY
,
defaultFs
)
;
DFSAdmin
admin
=
new
DFSAdmin
(
conf
)
;
String
[
]
args
=
new
String
[
]
{
}
;
admin
.
run
(
args
)
;
LOG
.
info
(
)
;
List
<
String
>
g3
=
groups
.
getGroups
(
user
)
;
}
conf
.
set
(
DFSConfigKeys
.
FS_DEFAULT_NAME_KEY
,
defaultFs
)
;
DFSAdmin
admin
=
new
DFSAdmin
(
conf
)
;
String
[
]
args
=
new
String
[
]
{
}
;
admin
.
run
(
args
)
;
LOG
.
info
(
)
;
List
<
String
>
g3
=
groups
.
getGroups
(
user
)
;
LOG
.
info
(
,
g3
)
;
for
(
int
i
=
0
;
i
<
g3
.
size
(
)
;
i
++
)
{
assertNotEquals
(
+
g1
.
get
(
i
)
+
+
g3
.
get
(
i
)
,
g1
.
get
(
i
)
,
g3
.
get
(
i
)
)
;
}
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
(
)
->
{
List
<
String
>
g4
;
try
{
g4
=
groups
.
getGroups
(
user
)
;
}
catch
(
IOException
e
)
{
String
[
]
args
=
new
String
[
]
{
}
;
admin
.
run
(
args
)
;
LOG
.
info
(
)
;
List
<
String
>
g3
=
groups
.
getGroups
(
user
)
;
LOG
.
info
(
,
g3
)
;
for
(
int
i
=
0
;
i
<
g3
.
size
(
)
;
i
++
)
{
assertNotEquals
(
+
g1
.
get
(
i
)
+
+
g3
.
get
(
i
)
,
g1
.
get
(
i
)
,
g3
.
get
(
i
)
)
;
}
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
(
)
->
{
List
<
String
>
g4
;
try
{
g4
=
groups
.
getGroups
(
user
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
user
)
;
return
false
;
ZKDelegationTokenSecretManagerImpl
dtsm1
=
new
ZKDelegationTokenSecretManagerImpl
(
conf
)
;
ZKDelegationTokenSecretManagerImpl
dtsm2
=
new
ZKDelegationTokenSecretManagerImpl
(
conf
)
;
ZKDelegationTokenSecretManagerImpl
dtsm3
=
new
ZKDelegationTokenSecretManagerImpl
(
conf
)
;
DelegationTokenManager
tm1
,
tm2
,
tm3
;
tm1
=
new
DelegationTokenManager
(
conf
,
new
Text
(
)
)
;
tm1
.
setExternalDelegationTokenSecretManager
(
dtsm1
)
;
tm2
=
new
DelegationTokenManager
(
conf
,
new
Text
(
)
)
;
tm2
.
setExternalDelegationTokenSecretManager
(
dtsm2
)
;
tm3
=
new
DelegationTokenManager
(
conf
,
new
Text
(
)
)
;
tm3
.
setExternalDelegationTokenSecretManager
(
dtsm3
)
;
Token
<
DelegationTokenIdentifier
>
token
=
(
Token
<
DelegationTokenIdentifier
>
)
tm1
.
createToken
(
UserGroupInformation
.
getCurrentUser
(
)
,
)
;
Assert
.
assertNotNull
(
token
)
;
tm2
.
verifyToken
(
token
)
;
Thread
.
sleep
(
9
*
1000
)
;
long
renewalTime
=
tm2
.
renewToken
(
token
,
)
;
String
filterInitializerConfKey
=
;
String
initializers
=
conf
.
get
(
filterInitializerConfKey
,
)
;
String
[
]
parts
=
initializers
.
split
(
)
;
Set
<
String
>
target
=
new
LinkedHashSet
<
String
>
(
)
;
for
(
String
filterInitializer
:
parts
)
{
filterInitializer
=
filterInitializer
.
trim
(
)
;
if
(
filterInitializer
.
equals
(
AuthenticationFilterInitializer
.
class
.
getName
(
)
)
||
filterInitializer
.
equals
(
ProxyUserAuthenticationFilterInitializer
.
class
.
getName
(
)
)
||
filterInitializer
.
isEmpty
(
)
)
{
continue
;
}
target
.
add
(
filterInitializer
)
;
}
target
.
add
(
AuthFilterInitializer
.
class
.
getName
(
)
)
;
initializers
=
StringUtils
.
join
(
target
,
)
;
conf
.
set
(
filterInitializerConfKey
,
initializers
)
;
LOG
.
info
(
+
initializers
)
;
HttpServer2
.
Builder
builder
=
new
HttpServer2
.
Builder
(
)
.
setName
(
name
)
.
setConf
(
conf
)
.
setACL
(
new
AccessControlList
(
conf
.
get
(
DFS_ADMIN
,
)
)
)
.
setSecurityEnabled
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
.
setUsernameConfKey
(
spnegoUserNameKey
)
.
setKeytabConfKey
(
getSpnegoKeytabKey
(
conf
,
spnegoKeytabFileKey
)
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
if
(
filterInitializer
.
equals
(
AuthenticationFilterInitializer
.
class
.
getName
(
)
)
||
filterInitializer
.
equals
(
ProxyUserAuthenticationFilterInitializer
.
class
.
getName
(
)
)
||
filterInitializer
.
isEmpty
(
)
)
{
continue
;
}
target
.
add
(
filterInitializer
)
;
}
target
.
add
(
AuthFilterInitializer
.
class
.
getName
(
)
)
;
initializers
=
StringUtils
.
join
(
target
,
)
;
conf
.
set
(
filterInitializerConfKey
,
initializers
)
;
LOG
.
info
(
+
initializers
)
;
HttpServer2
.
Builder
builder
=
new
HttpServer2
.
Builder
(
)
.
setName
(
name
)
.
setConf
(
conf
)
.
setACL
(
new
AccessControlList
(
conf
.
get
(
DFS_ADMIN
,
)
)
)
.
setSecurityEnabled
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
.
setUsernameConfKey
(
spnegoUserNameKey
)
.
setKeytabConfKey
(
getSpnegoKeytabKey
(
conf
,
spnegoKeytabFileKey
)
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
LOG
.
info
(
+
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
spnegoUserNameKey
)
,
httpAddr
.
getHostName
(
)
)
)
;
}
if
(
policy
.
isHttpEnabled
(
)
)
{
if
(
httpAddr
.
getPort
(
)
==
0
)
{
builder
.
setFindPort
(
true
)
;
}
URI
uri
=
URI
.
create
(
+
NetUtils
.
getHostPortString
(
httpAddr
)
)
;
builder
.
addEndpoint
(
uri
)
;
LOG
.
info
(
+
initializers
)
;
HttpServer2
.
Builder
builder
=
new
HttpServer2
.
Builder
(
)
.
setName
(
name
)
.
setConf
(
conf
)
.
setACL
(
new
AccessControlList
(
conf
.
get
(
DFS_ADMIN
,
)
)
)
.
setSecurityEnabled
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
.
setUsernameConfKey
(
spnegoUserNameKey
)
.
setKeytabConfKey
(
getSpnegoKeytabKey
(
conf
,
spnegoKeytabFileKey
)
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
LOG
.
info
(
+
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
spnegoUserNameKey
)
,
httpAddr
.
getHostName
(
)
)
)
;
}
if
(
policy
.
isHttpEnabled
(
)
)
{
if
(
httpAddr
.
getPort
(
)
==
0
)
{
builder
.
setFindPort
(
true
)
;
}
URI
uri
=
URI
.
create
(
+
NetUtils
.
getHostPortString
(
httpAddr
)
)
;
builder
.
addEndpoint
(
uri
)
;
LOG
.
info
(
+
name
+
+
uri
)
;
}
if
(
policy
.
isHttpsEnabled
(
)
&&
httpsAddr
!=
null
)
{
Configuration
sslConf
=
loadSslConfiguration
(
conf
)
;
loadSslConfToHttpServerBuilder
(
builder
,
sslConf
)
;
if
(
httpsAddr
.
getPort
(
)
==
0
)
{
builder
.
setFindPort
(
true
)
;
InetSocketAddress
inAddr
=
null
;
if
(
!
(
fs
instanceof
DistributedFileSystem
)
)
{
throw
new
IllegalArgumentException
(
+
fs
+
)
;
}
fs
.
exists
(
new
Path
(
)
)
;
DistributedFileSystem
dfs
=
(
DistributedFileSystem
)
fs
;
Configuration
dfsConf
=
dfs
.
getConf
(
)
;
URI
dfsUri
=
dfs
.
getUri
(
)
;
String
nsId
=
dfsUri
.
getHost
(
)
;
if
(
isHAEnabled
(
dfsConf
,
nsId
)
)
{
List
<
ClientProtocol
>
namenodes
=
getProxiesForAllNameNodesInNameservice
(
dfsConf
,
nsId
)
;
for
(
ClientProtocol
proxy
:
namenodes
)
{
try
{
if
(
proxy
.
getHAServiceState
(
)
.
equals
(
HAServiceState
.
ACTIVE
)
)
{
inAddr
=
RPC
.
getServerAddress
(
proxy
)
;
}
}
catch
(
Exception
e
)
{
searchScope
=
NodeBase
.
ROOT
;
excludedScope
=
scope
.
substring
(
1
)
;
}
else
{
searchScope
=
scope
;
excludedScope
=
null
;
}
Node
n
=
chooseRandom
(
searchScope
,
excludedScope
,
excludedNodes
)
;
if
(
n
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
}
return
null
;
}
Preconditions
.
checkArgument
(
n
instanceof
DatanodeDescriptor
)
;
DatanodeDescriptor
dnDescriptor
=
(
DatanodeDescriptor
)
n
;
if
(
dnDescriptor
.
hasStorageType
(
type
)
)
{
return
dnDescriptor
;
}
else
{
int
availableCount
=
root
.
getSubtreeStorageCount
(
type
)
;
if
(
excludeRoot
!=
null
&&
root
.
isAncestor
(
excludeRoot
)
)
{
if
(
excludeRoot
instanceof
DFSTopologyNodeImpl
)
{
availableCount
-=
(
(
DFSTopologyNodeImpl
)
excludeRoot
)
.
getSubtreeStorageCount
(
type
)
;
}
else
{
availableCount
-=
(
(
DatanodeDescriptor
)
excludeRoot
)
.
hasStorageType
(
type
)
?
1
:
0
;
}
}
if
(
excludedNodes
!=
null
)
{
for
(
Node
excludedNode
:
excludedNodes
)
{
if
(
excludeRoot
!=
null
&&
isNodeInScope
(
excludedNode
,
excludedScope
)
)
{
continue
;
}
if
(
excludedNode
instanceof
DatanodeDescriptor
)
{
availableCount
-=
(
(
DatanodeDescriptor
)
excludedNode
)
.
hasStorageType
(
type
)
?
1
:
0
;
}
else
if
(
excludedNode
instanceof
DFSTopologyNodeImpl
)
{
availableCount
-=
(
(
DFSTopologyNodeImpl
)
excludedNode
)
.
getSubtreeStorageCount
(
type
)
;
}
else
if
(
excludedNode
instanceof
DatanodeInfo
)
{
@
Override
public
boolean
add
(
Node
n
)
{
@
Override
public
boolean
remove
(
Node
n
)
{
public
synchronized
void
childAddStorage
(
String
childName
,
StorageType
type
)
{
public
synchronized
void
childRemoveStorage
(
String
childName
,
StorageType
type
)
{
public
IOStreamPair
receive
(
Peer
peer
,
OutputStream
underlyingOut
,
InputStream
underlyingIn
,
int
xferPort
,
DatanodeID
datanodeId
)
throws
IOException
{
if
(
dnConf
.
getEncryptDataTransfer
(
)
)
{
Collection
<
InMemoryAliasMapProtocol
>
aliasMaps
=
new
ArrayList
<
>
(
)
;
for
(
String
nsId
:
getNameServiceIds
(
conf
)
)
{
try
{
URI
namenodeURI
=
null
;
Configuration
newConf
=
new
Configuration
(
conf
)
;
if
(
HAUtil
.
isHAEnabled
(
conf
,
nsId
)
)
{
newConf
.
setClass
(
addKeySuffixes
(
PROXY_PROVIDER_KEY_PREFIX
,
nsId
)
,
InMemoryAliasMapFailoverProxyProvider
.
class
,
AbstractNNFailoverProxyProvider
.
class
)
;
namenodeURI
=
new
URI
(
HdfsConstants
.
HDFS_URI_SCHEME
+
+
nsId
)
;
}
else
{
String
key
=
addKeySuffixes
(
DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS
,
nsId
)
;
String
addr
=
conf
.
get
(
key
)
;
if
(
addr
!=
null
)
{
namenodeURI
=
createUri
(
HdfsConstants
.
HDFS_URI_SCHEME
,
NetUtils
.
createSocketAddr
(
addr
)
)
;
}
}
if
(
namenodeURI
!=
null
)
{
aliasMaps
.
add
(
NameNodeProxies
.
createProxy
(
newConf
,
namenodeURI
,
InMemoryAliasMapProtocol
.
class
)
.
getProxy
(
)
)
;
namenodeURI
=
new
URI
(
HdfsConstants
.
HDFS_URI_SCHEME
+
+
nsId
)
;
}
else
{
String
key
=
addKeySuffixes
(
DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS
,
nsId
)
;
String
addr
=
conf
.
get
(
key
)
;
if
(
addr
!=
null
)
{
namenodeURI
=
createUri
(
HdfsConstants
.
HDFS_URI_SCHEME
,
NetUtils
.
createSocketAddr
(
addr
)
)
;
}
}
if
(
namenodeURI
!=
null
)
{
aliasMaps
.
add
(
NameNodeProxies
.
createProxy
(
newConf
,
namenodeURI
,
InMemoryAliasMapProtocol
.
class
)
.
getProxy
(
)
)
;
LOG
.
info
(
,
namenodeURI
)
;
}
}
catch
(
IOException
|
URISyntaxException
e
)
{
LOG
.
warn
(
+
,
nsId
,
e
)
;
}
}
if
(
conf
.
get
(
DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS
)
!=
null
)
{
URI
uri
=
createUri
(
,
NetUtils
.
createSocketAddr
(
conf
.
get
(
DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS
)
)
)
;
try
{
aliasMaps
.
add
(
NameNodeProxies
.
createProxy
(
conf
,
uri
,
InMemoryAliasMapProtocol
.
class
)
.
getProxy
(
)
)
;
@
Override
public
void
purgeLogsOlderThan
(
long
minTxIdToKeep
)
throws
IOException
{
@
Override
public
void
recoverUnfinalizedSegments
(
)
throws
IOException
{
Preconditions
.
checkState
(
!
isActiveWriter
,
)
;
LOG
.
info
(
)
;
Map
<
AsyncLogger
,
NewEpochResponseProto
>
resps
=
createNewUniqueEpoch
(
)
;
LOG
.
info
(
+
loggers
.
getEpoch
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
selectInputStreams
(
Collection
<
EditLogInputStream
>
streams
,
long
fromTxnId
,
boolean
inProgressOk
,
boolean
onlyDurableTxns
)
throws
IOException
{
if
(
inProgressOk
&&
inProgressTailingEnabled
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
File
getOrCreatePaxosDir
(
)
{
File
paxosDir
=
new
File
(
sd
.
getCurrentDir
(
)
,
)
;
if
(
!
paxosDir
.
exists
(
)
)
{
File
getOrCreatePaxosDir
(
)
{
File
paxosDir
=
new
File
(
sd
.
getCurrentDir
(
)
,
)
;
if
(
!
paxosDir
.
exists
(
)
)
{
LOG
.
info
(
,
paxosDir
.
toPath
(
)
)
;
if
(
!
paxosDir
.
mkdir
(
)
)
{
private
static
void
purgeMatching
(
File
dir
,
List
<
Pattern
>
patterns
,
long
minTxIdToKeep
)
throws
IOException
{
for
(
File
f
:
FileUtil
.
listFiles
(
dir
)
)
{
if
(
!
f
.
isFile
(
)
)
continue
;
for
(
Pattern
p
:
patterns
)
{
Matcher
matcher
=
p
.
matcher
(
f
.
getName
(
)
)
;
if
(
matcher
.
matches
(
)
)
{
long
txid
=
Long
.
parseLong
(
matcher
.
group
(
1
)
)
;
if
(
txid
<
minTxIdToKeep
)
{
void
format
(
NamespaceInfo
nsInfo
,
boolean
force
)
throws
IOException
{
Preconditions
.
checkState
(
nsInfo
.
getNamespaceID
(
)
!=
0
,
,
nsInfo
)
;
private
void
updateLastPromisedEpoch
(
long
newEpoch
)
throws
IOException
{
checkRequest
(
reqInfo
)
;
boolean
needsValidation
=
true
;
if
(
startTxId
==
curSegmentTxId
)
{
if
(
curSegment
!=
null
)
{
curSegment
.
close
(
)
;
curSegment
=
null
;
curSegmentTxId
=
HdfsServerConstants
.
INVALID_TXID
;
curSegmentLayoutVersion
=
0
;
}
checkSync
(
nextTxId
==
endTxId
+
1
,
+
,
startTxId
,
endTxId
,
nextTxId
-
1
,
journalId
)
;
needsValidation
=
false
;
}
FileJournalManager
.
EditLogFile
elf
=
fjm
.
getLogFile
(
startTxId
)
;
if
(
elf
==
null
)
{
throw
new
JournalOutOfSyncException
(
+
+
startTxId
+
+
journalId
)
;
}
if
(
elf
.
isInProgress
(
)
)
{
if
(
needsValidation
)
{
checkRequest
(
reqInfo
)
;
abortCurSegment
(
)
;
long
segmentTxId
=
segment
.
getStartTxId
(
)
;
Preconditions
.
checkArgument
(
segment
.
getEndTxId
(
)
>
0
&&
segment
.
getEndTxId
(
)
>=
segmentTxId
,
,
segmentTxId
,
TextFormat
.
shortDebugString
(
segment
)
,
journalId
)
;
PersistedRecoveryPaxosData
oldData
=
getPersistedPaxosData
(
segmentTxId
)
;
PersistedRecoveryPaxosData
newData
=
PersistedRecoveryPaxosData
.
newBuilder
(
)
.
setAcceptedInEpoch
(
reqInfo
.
getEpoch
(
)
)
.
setSegmentState
(
segment
)
.
build
(
)
;
if
(
oldData
!=
null
)
{
alwaysAssert
(
oldData
.
getAcceptedInEpoch
(
)
<=
reqInfo
.
getEpoch
(
)
,
+
,
oldData
,
newData
,
journalId
)
;
}
File
syncedFile
=
null
;
SegmentStateProto
currentSegment
=
getSegmentInfo
(
segmentTxId
)
;
if
(
currentSegment
==
null
||
currentSegment
.
getEndTxId
(
)
!=
segment
.
getEndTxId
(
)
)
{
if
(
currentSegment
==
null
)
{
LOG
.
info
(
+
TextFormat
.
shortDebugString
(
segment
)
+
+
journalId
)
;
updateHighestWrittenTxId
(
Math
.
max
(
segment
.
getEndTxId
(
)
,
highestWrittenTxId
)
)
;
}
else
{
}
File
syncedFile
=
null
;
SegmentStateProto
currentSegment
=
getSegmentInfo
(
segmentTxId
)
;
if
(
currentSegment
==
null
||
currentSegment
.
getEndTxId
(
)
!=
segment
.
getEndTxId
(
)
)
{
if
(
currentSegment
==
null
)
{
LOG
.
info
(
+
TextFormat
.
shortDebugString
(
segment
)
+
+
journalId
)
;
updateHighestWrittenTxId
(
Math
.
max
(
segment
.
getEndTxId
(
)
,
highestWrittenTxId
)
)
;
}
else
{
LOG
.
info
(
+
TextFormat
.
shortDebugString
(
segment
)
+
+
TextFormat
.
shortDebugString
(
currentSegment
)
+
+
journalId
)
;
if
(
txnRange
(
currentSegment
)
.
contains
(
committedTxnId
.
get
(
)
)
&&
!
txnRange
(
segment
)
.
contains
(
committedTxnId
.
get
(
)
)
)
{
throw
new
AssertionError
(
+
TextFormat
.
shortDebugString
(
currentSegment
)
+
+
TextFormat
.
shortDebugString
(
segment
)
+
+
committedTxnId
.
get
(
)
+
+
journalId
)
;
}
alwaysAssert
(
currentSegment
.
getIsInProgress
(
)
,
+
+
journalId
)
;
if
(
txnRange
(
currentSegment
)
.
contains
(
highestWrittenTxId
)
)
{
updateHighestWrittenTxId
(
segment
.
getEndTxId
(
)
)
;
}
}
syncedFile
=
syncLog
(
reqInfo
,
segment
,
fromUrl
)
;
}
else
{
}
else
{
LOG
.
info
(
+
TextFormat
.
shortDebugString
(
segment
)
+
+
TextFormat
.
shortDebugString
(
currentSegment
)
+
+
journalId
)
;
if
(
txnRange
(
currentSegment
)
.
contains
(
committedTxnId
.
get
(
)
)
&&
!
txnRange
(
segment
)
.
contains
(
committedTxnId
.
get
(
)
)
)
{
throw
new
AssertionError
(
+
TextFormat
.
shortDebugString
(
currentSegment
)
+
+
TextFormat
.
shortDebugString
(
segment
)
+
+
committedTxnId
.
get
(
)
+
+
journalId
)
;
}
alwaysAssert
(
currentSegment
.
getIsInProgress
(
)
,
+
+
journalId
)
;
if
(
txnRange
(
currentSegment
)
.
contains
(
highestWrittenTxId
)
)
{
updateHighestWrittenTxId
(
segment
.
getEndTxId
(
)
)
;
}
}
syncedFile
=
syncLog
(
reqInfo
,
segment
,
fromUrl
)
;
}
else
{
LOG
.
info
(
+
TextFormat
.
shortDebugString
(
segment
)
+
+
journalId
)
;
}
JournalFaultInjector
.
get
(
)
.
beforePersistPaxosData
(
)
;
persistPaxosData
(
segmentTxId
,
newData
)
;
JournalFaultInjector
.
get
(
)
.
afterPersistPaxosData
(
)
;
if
(
syncedFile
!=
null
)
{
FileUtil
.
replaceFile
(
syncedFile
,
storage
.
getInProgressEditLog
(
segmentTxId
)
)
;
public
synchronized
void
doUpgrade
(
StorageInfo
sInfo
)
throws
IOException
{
long
oldCTime
=
storage
.
getCTime
(
)
;
storage
.
cTime
=
sInfo
.
cTime
;
int
oldLV
=
storage
.
getLayoutVersion
(
)
;
storage
.
layoutVersion
=
sInfo
.
layoutVersion
;
synchronized
boolean
moveTmpSegmentToCurrent
(
File
tmpFile
,
File
finalFile
,
long
endTxId
)
throws
IOException
{
final
boolean
success
;
if
(
endTxId
<=
committedTxnId
.
get
(
)
)
{
if
(
!
finalFile
.
getParentFile
(
)
.
exists
(
)
)
{
synchronized
Journal
getOrCreateJournal
(
String
jid
,
String
nameServiceId
,
StartupOption
startOpt
)
throws
IOException
{
QuorumJournalManager
.
checkJournalId
(
jid
)
;
Journal
journal
=
journalsById
.
get
(
jid
)
;
if
(
journal
==
null
)
{
File
logDir
=
getLogDir
(
jid
,
nameServiceId
)
;
Preconditions
.
checkState
(
!
isStarted
(
)
,
)
;
try
{
for
(
File
journalDir
:
localDir
)
{
validateAndCreateJournalDir
(
journalDir
)
;
}
DefaultMetricsSystem
.
initialize
(
)
;
JvmMetrics
.
create
(
,
conf
.
get
(
DFSConfigKeys
.
DFS_METRICS_SESSION_ID_KEY
)
,
DefaultMetricsSystem
.
instance
(
)
)
;
InetSocketAddress
socAddr
=
JournalNodeRpcServer
.
getAddress
(
conf
)
;
SecurityUtil
.
login
(
conf
,
DFSConfigKeys
.
DFS_JOURNALNODE_KEYTAB_FILE_KEY
,
DFSConfigKeys
.
DFS_JOURNALNODE_KERBEROS_PRINCIPAL_KEY
,
socAddr
.
getHostName
(
)
)
;
registerJNMXBean
(
)
;
httpServer
=
new
JournalNodeHttpServer
(
conf
,
this
,
getHttpServerBindAddress
(
conf
)
)
;
httpServer
.
start
(
)
;
httpServerURI
=
httpServer
.
getServerURI
(
)
.
toString
(
)
;
rpcServer
=
new
JournalNodeRpcServer
(
conf
,
this
)
;
rpcServer
.
start
(
)
;
}
catch
(
IOException
ioe
)
{
private
boolean
createEditsSyncDir
(
)
{
File
editsSyncDir
=
journal
.
getStorage
(
)
.
getEditsSyncDir
(
)
;
if
(
editsSyncDir
.
exists
(
)
)
{
if
(
!
journal
.
isFormatted
(
)
)
{
LOG
.
warn
(
)
;
}
else
{
syncJournals
(
)
;
}
}
catch
(
Throwable
t
)
{
if
(
!
shouldSync
)
{
if
(
t
instanceof
InterruptedException
)
{
LOG
.
info
(
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
return
;
}
else
{
LOG
.
warn
(
+
,
t
)
;
}
break
;
}
else
{
if
(
t
instanceof
InterruptedException
)
{
LOG
.
info
(
+
jn
.
getBoundIpcAddress
(
)
.
getAddress
(
)
+
+
jn
.
getBoundIpcAddress
(
)
.
getPort
(
)
+
+
otherJNProxies
.
get
(
index
)
+
+
jid
)
;
final
InterQJournalProtocol
jnProxy
=
otherJNProxies
.
get
(
index
)
.
jnProxy
;
if
(
jnProxy
==
null
)
{
LOG
.
error
(
)
;
return
;
}
List
<
RemoteEditLog
>
thisJournalEditLogs
;
try
{
thisJournalEditLogs
=
journal
.
getEditLogManifest
(
0
,
false
)
.
getLogs
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
return
;
}
GetEditLogManifestResponseProto
editLogManifest
;
try
{
editLogManifest
=
jnProxy
.
getEditLogManifestFromJournal
(
jid
,
nameServiceId
,
0
,
false
)
;
}
catch
(
IOException
e
)
{
try
{
uriStr
=
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
)
;
if
(
uriStr
==
null
||
uriStr
.
isEmpty
(
)
)
{
if
(
nameServiceId
!=
null
)
{
uriStr
=
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
+
+
nameServiceId
)
;
}
}
if
(
uriStr
==
null
||
uriStr
.
isEmpty
(
)
)
{
HashSet
<
String
>
sharedEditsUri
=
Sets
.
newHashSet
(
)
;
if
(
nameServiceId
!=
null
)
{
Collection
<
String
>
nnIds
=
DFSUtilClient
.
getNameNodeIds
(
conf
,
nameServiceId
)
;
for
(
String
nnId
:
nnIds
)
{
String
suffix
=
nameServiceId
+
+
nnId
;
uriStr
=
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
+
+
suffix
)
;
sharedEditsUri
.
add
(
uriStr
)
;
}
if
(
sharedEditsUri
.
size
(
)
>
1
)
{
uriStr
=
null
;
if
(
uriStr
==
null
||
uriStr
.
isEmpty
(
)
)
{
HashSet
<
String
>
sharedEditsUri
=
Sets
.
newHashSet
(
)
;
if
(
nameServiceId
!=
null
)
{
Collection
<
String
>
nnIds
=
DFSUtilClient
.
getNameNodeIds
(
conf
,
nameServiceId
)
;
for
(
String
nnId
:
nnIds
)
{
String
suffix
=
nameServiceId
+
+
nnId
;
uriStr
=
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
+
+
suffix
)
;
sharedEditsUri
.
add
(
uriStr
)
;
}
if
(
sharedEditsUri
.
size
(
)
>
1
)
{
uriStr
=
null
;
LOG
.
error
(
+
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
+
+
+
sharedEditsUri
.
toString
(
)
+
+
+
nameServiceId
)
;
}
}
}
if
(
uriStr
==
null
||
uriStr
.
isEmpty
(
)
)
{
LOG
.
error
(
)
;
return
null
;
}
else
{
if
(
nameServiceId
!=
null
)
{
Collection
<
String
>
nnIds
=
DFSUtilClient
.
getNameNodeIds
(
conf
,
nameServiceId
)
;
for
(
String
nnId
:
nnIds
)
{
String
suffix
=
nameServiceId
+
+
nnId
;
uriStr
=
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
+
+
suffix
)
;
sharedEditsUri
.
add
(
uriStr
)
;
}
if
(
sharedEditsUri
.
size
(
)
>
1
)
{
uriStr
=
null
;
LOG
.
error
(
+
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
+
+
+
sharedEditsUri
.
toString
(
)
+
+
+
nameServiceId
)
;
}
}
}
if
(
uriStr
==
null
||
uriStr
.
isEmpty
(
)
)
{
LOG
.
error
(
)
;
return
null
;
}
else
{
return
getJournalAddrList
(
uriStr
)
;
}
}
catch
(
URISyntaxException
e
)
{
for
(
RemoteEditLog
missingLog
:
missingLogs
)
{
URL
url
=
null
;
boolean
success
=
false
;
try
{
if
(
remoteJNproxy
.
httpServerUrl
==
null
)
{
if
(
response
.
hasFromURL
(
)
)
{
remoteJNproxy
.
httpServerUrl
=
getHttpServerURI
(
response
.
getFromURL
(
)
,
remoteJNproxy
.
jnAddr
.
getHostName
(
)
)
;
}
else
{
LOG
.
error
(
+
)
;
break
;
}
}
String
urlPath
=
GetJournalEditServlet
.
buildPath
(
jid
,
missingLog
.
getStartTxId
(
)
,
nsInfo
,
false
)
;
url
=
new
URL
(
remoteJNproxy
.
httpServerUrl
,
urlPath
)
;
success
=
downloadMissingLogSegment
(
url
,
missingLog
)
;
}
catch
(
URISyntaxException
e
)
{
LOG
.
error
(
,
e
)
;
boolean
success
=
false
;
try
{
if
(
remoteJNproxy
.
httpServerUrl
==
null
)
{
if
(
response
.
hasFromURL
(
)
)
{
remoteJNproxy
.
httpServerUrl
=
getHttpServerURI
(
response
.
getFromURL
(
)
,
remoteJNproxy
.
jnAddr
.
getHostName
(
)
)
;
}
else
{
LOG
.
error
(
+
)
;
break
;
}
}
String
urlPath
=
GetJournalEditServlet
.
buildPath
(
jid
,
missingLog
.
getStartTxId
(
)
,
nsInfo
,
false
)
;
url
=
new
URL
(
remoteJNproxy
.
httpServerUrl
,
urlPath
)
;
success
=
downloadMissingLogSegment
(
url
,
missingLog
)
;
}
catch
(
URISyntaxException
e
)
{
LOG
.
error
(
,
e
)
;
}
catch
(
MalformedURLException
e
)
{
LOG
.
error
(
,
e
)
;
public
void
checkAccess
(
BlockTokenIdentifier
id
,
String
userId
,
ExtendedBlock
block
,
BlockTokenIdentifier
.
AccessMode
mode
)
throws
InvalidToken
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
addFileToTarGzRecursively
(
TarArchiveOutputStream
tOut
,
File
file
,
String
prefix
,
Configuration
conf
)
throws
IOException
{
String
entryName
=
prefix
+
file
.
getName
(
)
;
TarArchiveEntry
tarEntry
=
new
TarArchiveEntry
(
file
,
entryName
)
;
tOut
.
putArchiveEntry
(
tarEntry
)
;
public
void
start
(
)
throws
IOException
{
RPC
.
setProtocolEngine
(
getConf
(
)
,
AliasMapProtocolPB
.
class
,
ProtobufRpcEngine2
.
class
)
;
AliasMapProtocolServerSideTranslatorPB
aliasMapProtocolXlator
=
new
AliasMapProtocolServerSideTranslatorPB
(
this
)
;
BlockingService
aliasMapProtocolService
=
AliasMapProtocolService
.
newReflectiveBlockingService
(
aliasMapProtocolXlator
)
;
InetSocketAddress
rpcAddress
=
getBindAddress
(
conf
,
DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS
,
DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_ADDRESS_DEFAULT
,
DFS_PROVIDED_ALIASMAP_INMEMORY_RPC_BIND_HOST
)
;
boolean
setVerbose
=
conf
.
getBoolean
(
DFS_PROVIDED_ALIASMAP_INMEMORY_SERVER_LOG
,
DFS_PROVIDED_ALIASMAP_INMEMORY_SERVER_LOG_DEFAULT
)
;
aliasMapServer
=
new
RPC
.
Builder
(
conf
)
.
setProtocol
(
AliasMapProtocolPB
.
class
)
.
setInstance
(
aliasMapProtocolService
)
.
setBindAddress
(
rpcAddress
.
getHostName
(
)
)
.
setPort
(
rpcAddress
.
getPort
(
)
)
.
setNumHandlers
(
1
)
.
setVerbose
(
setVerbose
)
.
build
(
)
;
static
long
getLong
(
Configuration
conf
,
String
key
,
long
defaultValue
)
{
final
long
v
=
conf
.
getLong
(
key
,
defaultValue
)
;
static
long
getLongBytes
(
Configuration
conf
,
String
key
,
long
defaultValue
)
{
final
long
v
=
conf
.
getLongBytes
(
key
,
defaultValue
)
;
static
int
getInt
(
Configuration
conf
,
String
key
,
int
defaultValue
)
{
final
int
v
=
conf
.
getInt
(
key
,
defaultValue
)
;
private
static
<
T
extends
StorageGroup
>
void
logUtilizationCollection
(
String
name
,
Collection
<
T
>
items
)
{
private
void
chooseStorageGroups
(
final
Matcher
matcher
)
{
private
void
chooseStorageGroups
(
final
Matcher
matcher
)
{
LOG
.
info
(
+
matcher
+
)
;
chooseStorageGroups
(
overUtilized
,
underUtilized
,
matcher
)
;
private
void
chooseStorageGroups
(
final
Matcher
matcher
)
{
LOG
.
info
(
+
matcher
+
)
;
chooseStorageGroups
(
overUtilized
,
underUtilized
,
matcher
)
;
LOG
.
info
(
+
matcher
+
)
;
chooseStorageGroups
(
overUtilized
,
belowAvgUtilized
,
matcher
)
;
private
void
matchSourceWithTargetToMove
(
Source
source
,
StorageGroup
target
)
{
long
size
=
Math
.
min
(
source
.
availableSizeToMove
(
)
,
target
.
availableSizeToMove
(
)
)
;
final
Task
task
=
new
Task
(
target
,
size
)
;
source
.
addTask
(
task
)
;
target
.
incScheduledSize
(
task
.
getSize
(
)
)
;
dispatcher
.
add
(
source
,
target
)
;
final
List
<
DatanodeStorageReport
>
reports
=
dispatcher
.
init
(
)
;
final
long
bytesLeftToMove
=
init
(
reports
)
;
if
(
bytesLeftToMove
==
0
)
{
return
newResult
(
ExitStatus
.
SUCCESS
,
bytesLeftToMove
,
0
)
;
}
else
{
LOG
.
info
(
+
StringUtils
.
byteDesc
(
bytesLeftToMove
)
+
)
;
}
if
(
!
runDuringUpgrade
&&
nnc
.
isUpgrading
(
)
)
{
System
.
err
.
println
(
+
)
;
LOG
.
error
(
+
)
;
return
newResult
(
ExitStatus
.
UNFINALIZED_UPGRADE
,
bytesLeftToMove
,
-
1
)
;
}
final
long
bytesBeingMoved
=
chooseStorageGroups
(
)
;
if
(
bytesBeingMoved
==
0
)
{
System
.
out
.
println
(
)
;
return
newResult
(
ExitStatus
.
NO_MOVE_BLOCK
,
bytesLeftToMove
,
bytesBeingMoved
)
;
}
else
{
static
private
int
doBalance
(
Collection
<
URI
>
namenodes
,
Collection
<
String
>
nsIds
,
final
BalancerParameters
p
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
*
2
+
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
static
private
int
doBalance
(
Collection
<
URI
>
namenodes
,
Collection
<
String
>
nsIds
,
final
BalancerParameters
p
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
*
2
+
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
LOG
.
info
(
+
namenodes
)
;
static
private
int
doBalance
(
Collection
<
URI
>
namenodes
,
Collection
<
String
>
nsIds
,
final
BalancerParameters
p
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
*
2
+
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
LOG
.
info
(
+
namenodes
)
;
LOG
.
info
(
+
p
)
;
static
private
int
doBalance
(
Collection
<
URI
>
namenodes
,
Collection
<
String
>
nsIds
,
final
BalancerParameters
p
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
*
2
+
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
LOG
.
info
(
+
namenodes
)
;
LOG
.
info
(
+
p
)
;
LOG
.
info
(
+
p
.
getIncludedNodes
(
)
)
;
static
private
int
doBalance
(
Collection
<
URI
>
namenodes
,
Collection
<
String
>
nsIds
,
final
BalancerParameters
p
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
*
2
+
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
LOG
.
info
(
+
namenodes
)
;
LOG
.
info
(
+
p
)
;
LOG
.
info
(
+
p
.
getIncludedNodes
(
)
)
;
LOG
.
info
(
+
p
.
getExcludedNodes
(
)
)
;
boolean
done
=
false
;
for
(
int
iteration
=
0
;
!
done
;
iteration
++
)
{
done
=
true
;
Collections
.
shuffle
(
connectors
)
;
for
(
NameNodeConnector
nnc
:
connectors
)
{
if
(
p
.
getBlockPools
(
)
.
size
(
)
==
0
||
p
.
getBlockPools
(
)
.
contains
(
nnc
.
getBlockpoolID
(
)
)
)
{
final
Balancer
b
=
new
Balancer
(
nnc
,
p
,
conf
)
;
final
Result
r
=
b
.
runOneIteration
(
)
;
r
.
print
(
iteration
,
System
.
out
)
;
b
.
resetData
(
conf
)
;
if
(
r
.
exitStatus
==
ExitStatus
.
IN_PROGRESS
)
{
done
=
false
;
}
else
if
(
r
.
exitStatus
!=
ExitStatus
.
SUCCESS
)
{
return
r
.
exitStatus
.
getExitCode
(
)
;
}
}
else
{
}
long
scheduleInterval
=
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_BALANCER_SERVICE_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_BALANCER_SERVICE_INTERVAL_DEFAULT
,
TimeUnit
.
MILLISECONDS
)
;
int
retryOnException
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_BALANCER_SERVICE_RETRIES_ON_EXCEPTION
,
DFSConfigKeys
.
DFS_BALANCER_SERVICE_RETRIES_ON_EXCEPTION_DEFAULT
)
;
while
(
serviceRunning
)
{
try
{
int
retCode
=
doBalance
(
namenodes
,
nsIds
,
p
,
conf
)
;
if
(
retCode
<
0
)
{
LOG
.
info
(
+
retCode
)
;
failedTimesSinceLastSuccessfulBalance
++
;
}
else
{
LOG
.
info
(
)
;
failedTimesSinceLastSuccessfulBalance
=
0
;
}
exceptionsSinceLastBalance
=
0
;
}
catch
(
Exception
e
)
{
if
(
++
exceptionsSinceLastBalance
>
retryOnException
)
{
throw
e
;
if
(
getBlocksRateLimiter
!=
null
)
{
getBlocksRateLimiter
.
acquire
(
)
;
}
boolean
isRequestStandby
=
false
;
NamenodeProtocol
nnproxy
=
null
;
try
{
if
(
requestToStandby
&&
nsId
!=
null
&&
HAUtil
.
isHAEnabled
(
config
,
nsId
)
)
{
List
<
ClientProtocol
>
namenodes
=
HAUtil
.
getProxiesForAllNameNodesInNameservice
(
config
,
nsId
)
;
for
(
ClientProtocol
proxy
:
namenodes
)
{
try
{
if
(
proxy
.
getHAServiceState
(
)
.
equals
(
HAServiceProtocol
.
HAServiceState
.
STANDBY
)
)
{
NamenodeProtocol
sbn
=
NameNodeProxies
.
createNonHAProxy
(
config
,
RPC
.
getServerAddress
(
proxy
)
,
NamenodeProtocol
.
class
,
UserGroupInformation
.
getCurrentUser
(
)
,
false
)
.
getProxy
(
)
;
nnproxy
=
sbn
;
isRequestStandby
=
true
;
break
;
}
}
catch
(
Exception
e
)
{
@
Override
public
void
initialize
(
Configuration
conf
,
FSClusterStats
stats
,
NetworkTopology
clusterMap
,
Host2NodesMap
host2datanodeMap
)
{
super
.
initialize
(
conf
,
stats
,
clusterMap
,
host2datanodeMap
)
;
float
balancedPreferencePercent
=
conf
.
getFloat
(
DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY
,
DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT
)
;
@
Override
public
void
initialize
(
Configuration
conf
,
FSClusterStats
stats
,
NetworkTopology
clusterMap
,
Host2NodesMap
host2datanodeMap
)
{
super
.
initialize
(
conf
,
stats
,
clusterMap
,
host2datanodeMap
)
;
float
balancedPreferencePercent
=
conf
.
getFloat
(
DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY
,
DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_RACK_FAULT_TOLERANT_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT
)
;
private
static
BlockTokenSecretManager
createBlockTokenSecretManager
(
final
Configuration
conf
)
throws
IOException
{
final
boolean
isEnabled
=
conf
.
getBoolean
(
DFSConfigKeys
.
DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY
,
DFSConfigKeys
.
DFS_BLOCK_ACCESS_TOKEN_ENABLE_DEFAULT
)
;
private
void
markBlockAsCorrupt
(
BlockToMarkCorrupt
b
,
DatanodeStorageInfo
storageInfo
,
DatanodeDescriptor
node
)
throws
IOException
{
if
(
b
.
getStored
(
)
.
isDeleted
(
)
)
{
private
boolean
invalidateBlock
(
BlockToMarkCorrupt
b
,
DatanodeInfo
dn
,
NumberReplicas
nr
)
throws
IOException
{
}
final
BlockPlacementPolicy
placementPolicy
=
placementPolicies
.
getPolicy
(
rw
.
getBlock
(
)
.
getBlockType
(
)
)
;
rw
.
chooseTargets
(
placementPolicy
,
storagePolicySuite
,
excludedNodes
)
;
}
namesystem
.
writeLock
(
)
;
try
{
for
(
BlockReconstructionWork
rw
:
reconWork
)
{
final
DatanodeStorageInfo
[
]
targets
=
rw
.
getTargets
(
)
;
if
(
targets
==
null
||
targets
.
length
==
0
)
{
rw
.
resetTargets
(
)
;
continue
;
}
synchronized
(
neededReconstruction
)
{
if
(
validateReconstructionWork
(
rw
)
)
{
scheduledWork
++
;
}
}
}
}
finally
{
namesystem
.
writeUnlock
(
)
;
}
if
(
blockLog
.
isDebugEnabled
(
)
)
{
final
BlockPlacementPolicy
placementPolicy
=
placementPolicies
.
getPolicy
(
rw
.
getBlock
(
)
.
getBlockType
(
)
)
;
rw
.
chooseTargets
(
placementPolicy
,
storagePolicySuite
,
excludedNodes
)
;
}
namesystem
.
writeLock
(
)
;
try
{
for
(
BlockReconstructionWork
rw
:
reconWork
)
{
final
DatanodeStorageInfo
[
]
targets
=
rw
.
getTargets
(
)
;
if
(
targets
==
null
||
targets
.
length
==
0
)
{
rw
.
resetTargets
(
)
;
continue
;
}
synchronized
(
neededReconstruction
)
{
if
(
validateReconstructionWork
(
rw
)
)
{
scheduledWork
++
;
}
}
}
}
finally
{
namesystem
.
writeUnlock
(
)
;
}
if
(
blockLog
.
isDebugEnabled
(
)
)
{
final
int
pendingNum
=
pendingReconstruction
.
getNumReplicas
(
block
)
;
if
(
hasEnoughEffectiveReplicas
(
block
,
numReplicas
,
pendingNum
)
)
{
neededReconstruction
.
remove
(
block
,
priority
)
;
rw
.
resetTargets
(
)
;
blockLog
.
debug
(
+
,
block
)
;
return
false
;
}
DatanodeStorageInfo
[
]
targets
=
rw
.
getTargets
(
)
;
BlockPlacementStatus
placementStatus
=
getBlockPlacementStatus
(
block
)
;
if
(
(
numReplicas
.
liveReplicas
(
)
>=
requiredRedundancy
)
&&
(
!
placementStatus
.
isPlacementPolicySatisfied
(
)
)
)
{
BlockPlacementStatus
newPlacementStatus
=
getBlockPlacementStatus
(
block
,
targets
)
;
if
(
!
newPlacementStatus
.
isPlacementPolicySatisfied
(
)
&&
(
newPlacementStatus
.
getAdditionalReplicasRequired
(
)
>=
placementStatus
.
getAdditionalReplicasRequired
(
)
)
)
{
return
false
;
}
rw
.
setNotEnoughRack
(
)
;
}
rw
.
addTaskToDatanode
(
numReplicas
)
;
DatanodeStorageInfo
.
incrementBlocksScheduled
(
targets
)
;
public
boolean
processReport
(
final
DatanodeID
nodeID
,
final
DatanodeStorage
storage
,
final
BlockListAsLongs
newReport
,
BlockReportContext
context
)
throws
IOException
{
namesystem
.
writeLock
(
)
;
final
long
startTime
=
Time
.
monotonicNow
(
)
;
final
long
endTime
;
DatanodeDescriptor
node
;
Collection
<
Block
>
invalidatedBlocks
=
Collections
.
emptyList
(
)
;
String
strBlockReportId
=
context
!=
null
?
Long
.
toHexString
(
context
.
getReportId
(
)
)
:
;
try
{
node
=
datanodeManager
.
getDatanode
(
nodeID
)
;
if
(
node
==
null
||
!
node
.
isRegistered
(
)
)
{
throw
new
IOException
(
+
nodeID
)
;
}
DatanodeStorageInfo
storageInfo
=
providedStorageMap
.
getStorage
(
node
,
storage
)
;
if
(
storageInfo
==
null
)
{
storageInfo
=
node
.
updateStorage
(
storage
)
;
}
if
(
namesystem
.
isInStartupSafeMode
(
)
&&
!
StorageType
.
PROVIDED
.
equals
(
storageInfo
.
getStorageType
(
)
)
&&
storageInfo
.
getBlockReportCount
(
)
>
0
)
{
DatanodeDescriptor
node
;
Collection
<
Block
>
invalidatedBlocks
=
Collections
.
emptyList
(
)
;
String
strBlockReportId
=
context
!=
null
?
Long
.
toHexString
(
context
.
getReportId
(
)
)
:
;
try
{
node
=
datanodeManager
.
getDatanode
(
nodeID
)
;
if
(
node
==
null
||
!
node
.
isRegistered
(
)
)
{
throw
new
IOException
(
+
nodeID
)
;
}
DatanodeStorageInfo
storageInfo
=
providedStorageMap
.
getStorage
(
node
,
storage
)
;
if
(
storageInfo
==
null
)
{
storageInfo
=
node
.
updateStorage
(
storage
)
;
}
if
(
namesystem
.
isInStartupSafeMode
(
)
&&
!
StorageType
.
PROVIDED
.
equals
(
storageInfo
.
getStorageType
(
)
)
&&
storageInfo
.
getBlockReportCount
(
)
>
0
)
{
blockLog
.
info
(
+
+
,
strBlockReportId
,
nodeID
)
;
blockReportLeaseManager
.
removeLease
(
node
)
;
return
!
node
.
hasStaleStorages
(
)
;
}
if
(
storageInfo
.
getBlockReportCount
(
)
==
0
)
{
storageInfo
=
node
.
updateStorage
(
storage
)
;
}
if
(
namesystem
.
isInStartupSafeMode
(
)
&&
!
StorageType
.
PROVIDED
.
equals
(
storageInfo
.
getStorageType
(
)
)
&&
storageInfo
.
getBlockReportCount
(
)
>
0
)
{
blockLog
.
info
(
+
+
,
strBlockReportId
,
nodeID
)
;
blockReportLeaseManager
.
removeLease
(
node
)
;
return
!
node
.
hasStaleStorages
(
)
;
}
if
(
storageInfo
.
getBlockReportCount
(
)
==
0
)
{
blockLog
.
info
(
+
,
strBlockReportId
,
storageInfo
.
getStorageID
(
)
,
nodeID
.
getDatanodeUuid
(
)
)
;
processFirstBlockReport
(
storageInfo
,
newReport
)
;
}
else
{
if
(
!
StorageType
.
PROVIDED
.
equals
(
storageInfo
.
getStorageType
(
)
)
)
{
invalidatedBlocks
=
processReport
(
storageInfo
,
newReport
,
context
)
;
}
}
storageInfo
.
receivedBlockReport
(
)
;
}
finally
{
endTime
=
Time
.
monotonicNow
(
)
;
namesystem
.
writeUnlock
(
)
;
return
!
node
.
hasStaleStorages
(
)
;
}
if
(
storageInfo
.
getBlockReportCount
(
)
==
0
)
{
blockLog
.
info
(
+
,
strBlockReportId
,
storageInfo
.
getStorageID
(
)
,
nodeID
.
getDatanodeUuid
(
)
)
;
processFirstBlockReport
(
storageInfo
,
newReport
)
;
}
else
{
if
(
!
StorageType
.
PROVIDED
.
equals
(
storageInfo
.
getStorageType
(
)
)
)
{
invalidatedBlocks
=
processReport
(
storageInfo
,
newReport
,
context
)
;
}
}
storageInfo
.
receivedBlockReport
(
)
;
}
finally
{
endTime
=
Time
.
monotonicNow
(
)
;
namesystem
.
writeUnlock
(
)
;
}
for
(
Block
b
:
invalidatedBlocks
)
{
blockLog
.
debug
(
+
,
strBlockReportId
,
b
,
node
,
b
.
getNumBytes
(
)
)
;
}
final
NameNodeMetrics
metrics
=
NameNode
.
getNameNodeMetrics
(
)
;
if
(
metrics
!=
null
)
{
if
(
getPostponedMisreplicatedBlocksCount
(
)
==
0
)
{
return
;
}
namesystem
.
writeLock
(
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
long
startSize
=
postponedMisreplicatedBlocks
.
size
(
)
;
try
{
Iterator
<
Block
>
it
=
postponedMisreplicatedBlocks
.
iterator
(
)
;
for
(
int
i
=
0
;
i
<
blocksPerPostpondedRescan
&&
it
.
hasNext
(
)
;
i
++
)
{
Block
b
=
it
.
next
(
)
;
it
.
remove
(
)
;
BlockInfo
bi
=
getStoredBlock
(
b
)
;
if
(
bi
==
null
)
{
LOG
.
debug
(
+
+
,
b
)
;
continue
;
}
MisReplicationResult
res
=
processMisReplicatedBlock
(
bi
)
;
for
(
int
i
=
0
;
i
<
blocksPerPostpondedRescan
&&
it
.
hasNext
(
)
;
i
++
)
{
Block
b
=
it
.
next
(
)
;
it
.
remove
(
)
;
BlockInfo
bi
=
getStoredBlock
(
b
)
;
if
(
bi
==
null
)
{
LOG
.
debug
(
+
+
,
b
)
;
continue
;
}
MisReplicationResult
res
=
processMisReplicatedBlock
(
bi
)
;
LOG
.
debug
(
+
,
b
,
res
)
;
if
(
res
==
MisReplicationResult
.
POSTPONE
)
{
rescannedMisreplicatedBlocks
.
add
(
b
)
;
}
}
}
finally
{
postponedMisreplicatedBlocks
.
addAll
(
rescannedMisreplicatedBlocks
)
;
rescannedMisreplicatedBlocks
.
clear
(
)
;
long
endSize
=
postponedMisreplicatedBlocks
.
size
(
)
;
for
(
BlockReportReplica
iblk
:
report
)
{
set
.
add
(
new
BlockReportReplica
(
iblk
)
)
;
}
sortedReport
=
set
;
}
else
{
sortedReport
=
report
;
}
reportDiffSorted
(
storageInfo
,
sortedReport
,
toAdd
,
toRemove
,
toInvalidate
,
toCorrupt
,
toUC
)
;
DatanodeDescriptor
node
=
storageInfo
.
getDatanodeDescriptor
(
)
;
for
(
StatefulBlockInfo
b
:
toUC
)
{
addStoredBlockUnderConstruction
(
b
,
storageInfo
)
;
}
for
(
BlockInfo
b
:
toRemove
)
{
removeStoredBlock
(
b
,
node
)
;
}
int
numBlocksLogged
=
0
;
for
(
BlockInfoToAdd
b
:
toAdd
)
{
addStoredBlock
(
b
.
stored
,
b
.
reported
,
storageInfo
,
null
,
numBlocksLogged
<
maxNumBlocksToLog
)
;
numBlocksLogged
++
;
void
processFirstBlockReport
(
final
DatanodeStorageInfo
storageInfo
,
final
BlockListAsLongs
report
)
throws
IOException
{
if
(
report
==
null
)
return
;
assert
(
namesystem
.
hasWriteLock
(
)
)
;
assert
(
storageInfo
.
getBlockReportCount
(
)
==
0
)
;
for
(
BlockReportReplica
iblk
:
report
)
{
ReplicaState
reportedState
=
iblk
.
getState
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
queueReportedBlock
(
DatanodeStorageInfo
storageInfo
,
Block
block
,
ReplicaState
reportedState
,
String
reason
)
{
assert
shouldPostponeBlocksFromFuture
;
private
void
processQueuedMessages
(
Iterable
<
ReportedBlockInfo
>
rbis
)
throws
IOException
{
boolean
isPreviousMessageProcessed
=
true
;
for
(
ReportedBlockInfo
rbi
:
rbis
)
{
nrPostponed
++
;
postponeBlock
(
block
)
;
break
;
case
UNDER_CONSTRUCTION
:
LOG
.
trace
(
,
block
,
res
)
;
nrUnderConstruction
++
;
break
;
case
OK
:
break
;
default
:
throw
new
AssertionError
(
+
res
)
;
}
processed
++
;
}
totalProcessed
+=
processed
;
reconstructionQueuesInitProgress
=
Math
.
min
(
(
double
)
totalProcessed
/
totalBlocks
,
1.0
)
;
if
(
!
blocksItr
.
hasNext
(
)
)
{
LOG
.
info
(
,
blocksMap
.
size
(
)
)
;
LOG
.
info
(
,
nrInvalid
)
;
LOG
.
info
(
,
nrUnderReplicated
)
;
postponeBlock
(
block
)
;
break
;
case
UNDER_CONSTRUCTION
:
LOG
.
trace
(
,
block
,
res
)
;
nrUnderConstruction
++
;
break
;
case
OK
:
break
;
default
:
throw
new
AssertionError
(
+
res
)
;
}
processed
++
;
}
totalProcessed
+=
processed
;
reconstructionQueuesInitProgress
=
Math
.
min
(
(
double
)
totalProcessed
/
totalBlocks
,
1.0
)
;
if
(
!
blocksItr
.
hasNext
(
)
)
{
LOG
.
info
(
,
blocksMap
.
size
(
)
)
;
LOG
.
info
(
,
nrInvalid
)
;
LOG
.
info
(
,
nrUnderReplicated
)
;
LOG
.
info
(
,
nrOverReplicated
,
(
(
nrPostponed
>
0
)
?
(
+
nrPostponed
+
)
:
)
)
;
public
int
processMisReplicatedBlocks
(
List
<
BlockInfo
>
blocks
)
{
int
processed
=
0
;
Iterator
<
BlockInfo
>
iter
=
blocks
.
iterator
(
)
;
try
{
while
(
isPopulatingReplQueues
(
)
&&
namesystem
.
isRunning
(
)
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
&&
iter
.
hasNext
(
)
)
{
int
limit
=
processed
+
numBlocksPerIteration
;
namesystem
.
writeLockInterruptibly
(
)
;
try
{
while
(
iter
.
hasNext
(
)
&&
processed
<
limit
)
{
BlockInfo
blk
=
iter
.
next
(
)
;
MisReplicationResult
r
=
processMisReplicatedBlock
(
blk
)
;
processed
++
;
private
void
processChosenExcessRedundancy
(
final
Collection
<
DatanodeStorageInfo
>
nonExcess
,
final
DatanodeStorageInfo
chosen
,
BlockInfo
storedBlock
)
{
nonExcess
.
remove
(
chosen
)
;
excessRedundancyMap
.
add
(
chosen
.
getDatanodeDescriptor
(
)
,
storedBlock
)
;
final
Block
blockToInvalidate
=
getBlockOnStorage
(
storedBlock
,
chosen
)
;
addToInvalidates
(
blockToInvalidate
,
chosen
.
getDatanodeDescriptor
(
)
)
;
public
void
removeStoredBlock
(
BlockInfo
storedBlock
,
DatanodeDescriptor
node
)
{
private
boolean
processAndHandleReportedBlock
(
DatanodeStorageInfo
storageInfo
,
Block
block
,
ReplicaState
reportedState
,
DatanodeDescriptor
delHintNode
)
throws
IOException
{
final
DatanodeDescriptor
node
=
storageInfo
.
getDatanodeDescriptor
(
)
;
while
(
it
.
hasNext
(
)
)
{
final
BlockInfo
block
=
it
.
next
(
)
;
if
(
block
.
isDeleted
(
)
)
{
continue
;
}
int
expectedReplication
=
this
.
getExpectedRedundancyNum
(
block
)
;
NumberReplicas
num
=
countNodes
(
block
)
;
if
(
shouldProcessExtraRedundancy
(
num
,
expectedReplication
)
)
{
processExtraRedundancyBlock
(
block
,
(
short
)
expectedReplication
,
null
,
null
)
;
numExtraRedundancy
++
;
}
}
if
(
namesystem
.
hasWriteLock
(
)
)
{
namesystem
.
writeUnlock
(
)
;
try
{
Thread
.
sleep
(
1
)
;
}
catch
(
InterruptedException
e
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
boolean
isNodeHealthyForDecommissionOrMaintenance
(
DatanodeDescriptor
node
)
{
if
(
!
node
.
checkBlockReportReceived
(
)
)
{
final
EnumMap
<
StorageType
,
Integer
>
storageTypes
=
getRequiredStorageTypes
(
requiredStorageTypes
)
;
List
<
DatanodeStorageInfo
>
results
=
new
ArrayList
<
>
(
)
;
boolean
avoidStaleNodes
=
stats
!=
null
&&
stats
.
isAvoidingStaleDataNodesForWrite
(
)
;
int
maxNodesAndReplicas
[
]
=
getMaxNodesPerRack
(
0
,
numOfReplicas
)
;
numOfReplicas
=
maxNodesAndReplicas
[
0
]
;
int
maxNodesPerRack
=
maxNodesAndReplicas
[
1
]
;
chooseFavouredNodes
(
src
,
numOfReplicas
,
favoredNodes
,
favoriteAndExcludedNodes
,
blocksize
,
maxNodesPerRack
,
results
,
avoidStaleNodes
,
storageTypes
)
;
if
(
results
.
size
(
)
<
numOfReplicas
)
{
numOfReplicas
-=
results
.
size
(
)
;
for
(
DatanodeStorageInfo
storage
:
results
)
{
addToExcludedNodes
(
storage
.
getDatanodeDescriptor
(
)
,
favoriteAndExcludedNodes
)
;
}
DatanodeStorageInfo
[
]
remainingTargets
=
chooseTarget
(
src
,
numOfReplicas
,
writer
,
new
ArrayList
<
DatanodeStorageInfo
>
(
numOfReplicas
)
,
false
,
favoriteAndExcludedNodes
,
blocksize
,
storagePolicy
,
flags
,
storageTypes
)
;
for
(
int
i
=
0
;
i
<
remainingTargets
.
length
;
i
++
)
{
results
.
add
(
remainingTargets
[
i
]
)
;
}
}
return
getPipeline
(
writer
,
results
.
toArray
(
new
DatanodeStorageInfo
[
results
.
size
(
)
]
)
)
;
final
Map
<
String
,
List
<
DatanodeStorageInfo
>>
rackMap
=
new
HashMap
<
>
(
)
;
final
List
<
DatanodeStorageInfo
>
moreThanOne
=
new
ArrayList
<
>
(
)
;
final
List
<
DatanodeStorageInfo
>
exactlyOne
=
new
ArrayList
<
>
(
)
;
splitNodesWithRack
(
availableReplicas
,
delCandidates
,
rackMap
,
moreThanOne
,
exactlyOne
)
;
boolean
firstOne
=
true
;
final
DatanodeStorageInfo
delNodeHintStorage
=
DatanodeStorageInfo
.
getDatanodeStorageInfo
(
delCandidates
,
delNodeHint
)
;
final
DatanodeStorageInfo
addedNodeStorage
=
DatanodeStorageInfo
.
getDatanodeStorageInfo
(
delCandidates
,
addedNode
)
;
while
(
delCandidates
.
size
(
)
-
expectedNumOfReplicas
>
excessReplicas
.
size
(
)
)
{
final
DatanodeStorageInfo
cur
;
if
(
firstOne
&&
useDelHint
(
delNodeHintStorage
,
addedNodeStorage
,
moreThanOne
,
exactlyOne
,
excessTypes
)
)
{
cur
=
delNodeHintStorage
;
}
else
{
cur
=
chooseReplicaToDelete
(
moreThanOne
,
exactlyOne
,
excessTypes
,
rackMap
)
;
}
firstOne
=
false
;
if
(
cur
==
null
)
{
final
Set
<
Node
>
newExcludeNodes
=
new
HashSet
<
>
(
)
;
for
(
DatanodeStorageInfo
resultStorage
:
results
)
{
addToExcludedNodes
(
resultStorage
.
getDatanodeDescriptor
(
)
,
newExcludeNodes
)
;
}
LOG
.
trace
(
,
results
)
;
LOG
.
trace
(
,
excludedNodes
)
;
LOG
.
trace
(
,
newExcludeNodes
)
;
final
int
numOfReplicas
=
totalReplicaExpected
-
results
.
size
(
)
;
numResultsOflastChoose
=
results
.
size
(
)
;
try
{
chooseOnce
(
numOfReplicas
,
writer
,
newExcludeNodes
,
blocksize
,
++
bestEffortMaxNodesPerRack
,
results
,
avoidStaleNodes
,
storageTypes
)
;
}
catch
(
NotEnoughReplicasException
nere
)
{
lastException
=
nere
;
}
finally
{
excludedNodes
.
addAll
(
newExcludeNodes
)
;
}
}
if
(
numResultsOflastChoose
!=
totalReplicaExpected
)
{
private
synchronized
NodeData
registerNode
(
DatanodeDescriptor
dn
)
{
if
(
nodes
.
containsKey
(
dn
.
getDatanodeUuid
(
)
)
)
{
public
synchronized
void
unregister
(
DatanodeDescriptor
dn
)
{
NodeData
node
=
nodes
.
remove
(
dn
.
getDatanodeUuid
(
)
)
;
if
(
node
==
null
)
{
remove
(
node
)
;
long
monotonicNowMs
=
Time
.
monotonicNow
(
)
;
pruneExpiredPending
(
monotonicNowMs
)
;
if
(
numPending
>=
maxPending
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
StringBuilder
allLeases
=
new
StringBuilder
(
)
;
String
prefix
=
;
for
(
NodeData
cur
=
pendingHead
.
next
;
cur
!=
pendingHead
;
cur
=
cur
.
next
)
{
allLeases
.
append
(
prefix
)
.
append
(
cur
.
datanodeUuid
)
;
prefix
=
;
}
LOG
.
debug
(
+
,
dn
.
getDatanodeUuid
(
)
,
numPending
,
allLeases
.
toString
(
)
)
;
}
return
0
;
}
numPending
++
;
node
.
leaseId
=
getNextId
(
)
;
node
.
leaseTimeMs
=
monotonicNowMs
;
public
synchronized
boolean
checkLease
(
DatanodeDescriptor
dn
,
long
monotonicNowMs
,
long
id
)
{
if
(
id
==
0
)
{
public
synchronized
long
removeLease
(
DatanodeDescriptor
dn
)
{
NodeData
node
=
nodes
.
get
(
dn
.
getDatanodeUuid
(
)
)
;
if
(
node
==
null
)
{
LOG
.
info
(
+
intervalMs
+
)
;
try
{
long
curTimeMs
=
Time
.
monotonicNow
(
)
;
while
(
true
)
{
lock
.
lock
(
)
;
try
{
while
(
true
)
{
if
(
shutdown
)
{
LOG
.
info
(
)
;
return
;
}
if
(
completedScanCount
<
neededScanCount
)
{
LOG
.
debug
(
)
;
break
;
}
long
delta
=
(
startTimeMs
+
intervalMs
)
-
curTimeMs
;
if
(
delta
<=
0
)
{
break
;
}
doRescan
.
await
(
delta
,
TimeUnit
.
MILLISECONDS
)
;
curTimeMs
=
Time
.
monotonicNow
(
)
;
}
}
finally
{
lock
.
unlock
(
)
;
}
startTimeMs
=
curTimeMs
;
mark
=
!
mark
;
rescan
(
)
;
curTimeMs
=
Time
.
monotonicNow
(
)
;
lock
.
lock
(
)
;
try
{
completedScanCount
=
curScanCount
;
curScanCount
=
-
1
;
scanFinished
.
signalAll
(
)
;
}
finally
{
private
void
rescanCacheDirectives
(
)
{
FSDirectory
fsDir
=
namesystem
.
getFSDirectory
(
)
;
final
long
now
=
new
Date
(
)
.
getTime
(
)
;
for
(
CacheDirective
directive
:
cacheManager
.
getCacheDirectives
(
)
)
{
scannedDirectives
++
;
if
(
directive
.
getExpiryTime
(
)
>
0
&&
directive
.
getExpiryTime
(
)
<=
now
)
{
FSDirectory
fsDir
=
namesystem
.
getFSDirectory
(
)
;
final
long
now
=
new
Date
(
)
.
getTime
(
)
;
for
(
CacheDirective
directive
:
cacheManager
.
getCacheDirectives
(
)
)
{
scannedDirectives
++
;
if
(
directive
.
getExpiryTime
(
)
>
0
&&
directive
.
getExpiryTime
(
)
<=
now
)
{
LOG
.
debug
(
,
directive
.
getId
(
)
,
directive
.
getExpiryTime
(
)
,
now
)
;
continue
;
}
String
path
=
directive
.
getPath
(
)
;
INode
node
;
try
{
node
=
fsDir
.
getINode
(
path
,
DirOp
.
READ
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
directive
.
getId
(
)
,
path
,
e
.
getMessage
(
)
)
;
continue
;
}
if
(
node
==
null
)
{
try
{
node
=
fsDir
.
getINode
(
path
,
DirOp
.
READ
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
directive
.
getId
(
)
,
path
,
e
.
getMessage
(
)
)
;
continue
;
}
if
(
node
==
null
)
{
LOG
.
debug
(
,
directive
.
getId
(
)
,
path
)
;
}
else
if
(
node
.
isDirectory
(
)
)
{
INodeDirectory
dir
=
node
.
asDirectory
(
)
;
ReadOnlyList
<
INode
>
children
=
dir
.
getChildrenList
(
Snapshot
.
CURRENT_STATE_ID
)
;
for
(
INode
child
:
children
)
{
if
(
child
.
isFile
(
)
)
{
rescanFile
(
directive
,
child
.
asFile
(
)
)
;
}
}
}
else
if
(
node
.
isFile
(
)
)
{
rescanFile
(
directive
,
node
.
asFile
(
)
)
;
private
void
rescanFile
(
CacheDirective
directive
,
INodeFile
file
)
{
BlockInfo
[
]
blockInfos
=
file
.
getBlocks
(
)
;
directive
.
addFilesNeeded
(
1
)
;
long
neededTotal
=
file
.
computeFileSizeNotIncludingLastUcBlock
(
)
*
directive
.
getReplication
(
)
;
directive
.
addBytesNeeded
(
neededTotal
)
;
CachePool
pool
=
directive
.
getPool
(
)
;
if
(
pool
.
getBytesNeeded
(
)
>
pool
.
getLimit
(
)
)
{
Block
block
=
new
Block
(
blockInfo
.
getBlockId
(
)
)
;
CachedBlock
ncblock
=
new
CachedBlock
(
block
.
getBlockId
(
)
,
directive
.
getReplication
(
)
,
mark
)
;
CachedBlock
ocblock
=
cachedBlocks
.
get
(
ncblock
)
;
if
(
ocblock
==
null
)
{
cachedBlocks
.
put
(
ncblock
)
;
ocblock
=
ncblock
;
}
else
{
List
<
DatanodeDescriptor
>
cachedOn
=
ocblock
.
getDatanodes
(
Type
.
CACHED
)
;
long
cachedByBlock
=
Math
.
min
(
cachedOn
.
size
(
)
,
directive
.
getReplication
(
)
)
*
blockInfo
.
getNumBytes
(
)
;
cachedTotal
+=
cachedByBlock
;
if
(
(
mark
!=
ocblock
.
getMark
(
)
)
||
(
ocblock
.
getReplication
(
)
<
directive
.
getReplication
(
)
)
)
{
ocblock
.
setReplicationAndMark
(
directive
.
getReplication
(
)
,
mark
)
;
}
}
LOG
.
trace
(
,
directive
.
getId
(
)
,
blockInfo
,
ocblock
.
getReplication
(
)
)
;
}
directive
.
addBytesCached
(
cachedTotal
)
;
if
(
cachedTotal
==
neededTotal
)
{
private
void
rescanCachedBlockMap
(
)
{
Set
<
DatanodeDescriptor
>
datanodes
=
blockManager
.
getDatanodeManager
(
)
.
getDatanodes
(
)
;
for
(
DatanodeDescriptor
dn
:
datanodes
)
{
long
remaining
=
dn
.
getCacheRemaining
(
)
;
for
(
Iterator
<
CachedBlock
>
it
=
dn
.
getPendingCached
(
)
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
CachedBlock
cblock
=
it
.
next
(
)
;
BlockInfo
blockInfo
=
blockManager
.
getStoredBlock
(
new
Block
(
cblock
.
getBlockId
(
)
)
)
;
if
(
blockInfo
==
null
)
{
private
void
addNewPendingCached
(
final
int
neededCached
,
CachedBlock
cachedBlock
,
List
<
DatanodeDescriptor
>
cached
,
List
<
DatanodeDescriptor
>
pendingCached
)
{
BlockInfo
blockInfo
=
blockManager
.
getStoredBlock
(
new
Block
(
cachedBlock
.
getBlockId
(
)
)
)
;
if
(
blockInfo
==
null
)
{
if
(
info
!=
null
)
{
pendingBytes
-=
info
.
getNumBytes
(
)
;
}
}
it
=
datanode
.
getPendingUncached
(
)
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
CachedBlock
cBlock
=
it
.
next
(
)
;
BlockInfo
info
=
blockManager
.
getStoredBlock
(
new
Block
(
cBlock
.
getBlockId
(
)
)
)
;
if
(
info
!=
null
)
{
pendingBytes
+=
info
.
getNumBytes
(
)
;
}
}
long
pendingCapacity
=
pendingBytes
+
datanode
.
getCacheRemaining
(
)
;
if
(
pendingCapacity
<
blockInfo
.
getNumBytes
(
)
)
{
LOG
.
trace
(
+
+
,
blockInfo
.
getBlockId
(
)
,
datanode
.
getDatanodeUuid
(
)
,
blockInfo
.
getNumBytes
(
)
,
pendingCapacity
,
pendingBytes
,
datanode
.
getCacheRemaining
(
)
)
;
outOfCapacity
++
;
continue
;
}
possibilities
.
add
(
datanode
)
;
}
List
<
DatanodeDescriptor
>
chosen
=
chooseDatanodesForCaching
(
possibilities
,
neededCached
,
blockManager
.
getDatanodeManager
(
)
.
getStaleInterval
(
)
)
;
@
Override
protected
void
processConf
(
)
{
this
.
pendingRepLimit
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_BACKOFF_MONITOR_PENDING_LIMIT
,
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_BACKOFF_MONITOR_PENDING_LIMIT_DEFAULT
)
;
if
(
this
.
pendingRepLimit
<
1
)
{
if
(
!
namesystem
.
isRunning
(
)
)
{
LOG
.
info
(
+
)
;
return
;
}
numBlocksChecked
=
0
;
try
{
namesystem
.
writeLock
(
)
;
try
{
processCancelledNodes
(
)
;
processPendingNodes
(
)
;
}
finally
{
namesystem
.
writeUnlock
(
)
;
}
check
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
e
)
;
}
if
(
numBlocksChecked
+
outOfServiceNodeBlocks
.
size
(
)
>
0
)
{
if
(
toRemove
.
size
(
)
==
0
)
{
return
;
}
namesystem
.
writeLock
(
)
;
try
{
for
(
DatanodeDescriptor
dn
:
toRemove
)
{
final
boolean
isHealthy
=
blockManager
.
isNodeHealthyForDecommissionOrMaintenance
(
dn
)
;
if
(
isHealthy
)
{
if
(
dn
.
isDecommissionInProgress
(
)
)
{
dnAdmin
.
setDecommissioned
(
dn
)
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isEnteringMaintenance
(
)
)
{
dnAdmin
.
setInMaintenance
(
dn
)
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isInService
(
)
)
{
for
(
DatanodeDescriptor
dn
:
toRemove
)
{
final
boolean
isHealthy
=
blockManager
.
isNodeHealthyForDecommissionOrMaintenance
(
dn
)
;
if
(
isHealthy
)
{
if
(
dn
.
isDecommissionInProgress
(
)
)
{
dnAdmin
.
setDecommissioned
(
dn
)
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isEnteringMaintenance
(
)
)
{
dnAdmin
.
setInMaintenance
(
dn
)
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isInService
(
)
)
{
LOG
.
info
(
+
,
dn
)
;
pendingRep
.
remove
(
dn
)
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
continue
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isEnteringMaintenance
(
)
)
{
dnAdmin
.
setInMaintenance
(
dn
)
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isInService
(
)
)
{
LOG
.
info
(
+
,
dn
)
;
pendingRep
.
remove
(
dn
)
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
continue
;
}
else
{
LOG
.
error
(
+
,
dn
,
dn
.
getAdminState
(
)
)
;
pendingRep
.
remove
(
dn
)
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
continue
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isEnteringMaintenance
(
)
)
{
dnAdmin
.
setInMaintenance
(
dn
)
;
pendingRep
.
remove
(
dn
)
;
}
else
if
(
dn
.
isInService
(
)
)
{
LOG
.
info
(
+
,
dn
)
;
pendingRep
.
remove
(
dn
)
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
continue
;
}
else
{
LOG
.
error
(
+
,
dn
,
dn
.
getAdminState
(
)
)
;
pendingRep
.
remove
(
dn
)
;
outOfServiceNodeBlocks
.
remove
(
dn
)
;
continue
;
}
LOG
.
info
(
+
,
dn
,
dn
.
getAdminState
(
)
)
;
private
void
checkForCompletedNodes
(
List
<
DatanodeDescriptor
>
removeList
)
{
for
(
DatanodeDescriptor
dn
:
outOfServiceNodeBlocks
.
keySet
(
)
)
{
if
(
dn
.
isInMaintenance
(
)
)
{
DatanodeDescriptor
dn
=
nodeIter
.
next
(
)
;
Iterator
<
BlockInfo
>
blockIt
=
iterators
.
get
(
dn
)
;
while
(
blockIt
.
hasNext
(
)
)
{
if
(
blocksProcessed
>=
blocksPerLock
)
{
blocksProcessed
=
0
;
namesystem
.
writeUnlock
(
)
;
namesystem
.
writeLock
(
)
;
}
blocksProcessed
++
;
if
(
nextBlockAddedToPending
(
blockIt
,
dn
)
)
{
pendingCount
++
;
break
;
}
}
if
(
!
blockIt
.
hasNext
(
)
)
{
nodeIter
.
remove
(
)
;
}
if
(
pendingCount
>=
pendingRepLimit
)
{
break
;
@
Override
protected
void
processConf
(
)
{
numBlocksPerCheck
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_DEFAULT
)
;
if
(
numBlocksPerCheck
<=
0
)
{
if
(
!
namesystem
.
isRunning
(
)
)
{
LOG
.
info
(
+
)
;
return
;
}
numBlocksChecked
=
0
;
numBlocksCheckedPerLock
=
0
;
numNodesChecked
=
0
;
namesystem
.
writeLock
(
)
;
try
{
processPendingNodes
(
)
;
check
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
e
)
;
}
finally
{
namesystem
.
writeUnlock
(
)
;
}
if
(
numBlocksChecked
+
numNodesChecked
>
0
)
{
final
List
<
DatanodeDescriptor
>
toRemove
=
new
ArrayList
<
>
(
)
;
while
(
it
.
hasNext
(
)
&&
!
exceededNumBlocksPerCheck
(
)
&&
namesystem
.
isRunning
(
)
)
{
numNodesChecked
++
;
final
Map
.
Entry
<
DatanodeDescriptor
,
AbstractList
<
BlockInfo
>>
entry
=
it
.
next
(
)
;
final
DatanodeDescriptor
dn
=
entry
.
getKey
(
)
;
try
{
AbstractList
<
BlockInfo
>
blocks
=
entry
.
getValue
(
)
;
boolean
fullScan
=
false
;
if
(
dn
.
isMaintenance
(
)
&&
dn
.
maintenanceExpired
(
)
)
{
dnAdmin
.
stopMaintenance
(
dn
)
;
toRemove
.
add
(
dn
)
;
continue
;
}
if
(
dn
.
isInMaintenance
(
)
)
{
continue
;
}
if
(
blocks
==
null
)
{
try
{
AbstractList
<
BlockInfo
>
blocks
=
entry
.
getValue
(
)
;
boolean
fullScan
=
false
;
if
(
dn
.
isMaintenance
(
)
&&
dn
.
maintenanceExpired
(
)
)
{
dnAdmin
.
stopMaintenance
(
dn
)
;
toRemove
.
add
(
dn
)
;
continue
;
}
if
(
dn
.
isInMaintenance
(
)
)
{
continue
;
}
if
(
blocks
==
null
)
{
LOG
.
debug
(
+
,
dn
)
;
blocks
=
handleInsufficientlyStored
(
dn
)
;
outOfServiceNodeBlocks
.
put
(
dn
,
blocks
)
;
fullScan
=
true
;
}
else
{
dnAdmin
.
stopMaintenance
(
dn
)
;
toRemove
.
add
(
dn
)
;
continue
;
}
if
(
dn
.
isInMaintenance
(
)
)
{
continue
;
}
if
(
blocks
==
null
)
{
LOG
.
debug
(
+
,
dn
)
;
blocks
=
handleInsufficientlyStored
(
dn
)
;
outOfServiceNodeBlocks
.
put
(
dn
,
blocks
)
;
fullScan
=
true
;
}
else
{
LOG
.
debug
(
,
dn
.
getAdminState
(
)
,
dn
)
;
pruneReliableBlocks
(
dn
,
blocks
)
;
}
if
(
blocks
.
size
(
)
==
0
)
{
if
(
!
fullScan
)
{
else
{
LOG
.
debug
(
,
dn
.
getAdminState
(
)
,
dn
)
;
pruneReliableBlocks
(
dn
,
blocks
)
;
}
if
(
blocks
.
size
(
)
==
0
)
{
if
(
!
fullScan
)
{
LOG
.
debug
(
+
,
dn
)
;
blocks
=
handleInsufficientlyStored
(
dn
)
;
outOfServiceNodeBlocks
.
put
(
dn
,
blocks
)
;
}
final
boolean
isHealthy
=
blockManager
.
isNodeHealthyForDecommissionOrMaintenance
(
dn
)
;
if
(
blocks
.
size
(
)
==
0
&&
isHealthy
)
{
if
(
dn
.
isDecommissionInProgress
(
)
)
{
dnAdmin
.
setDecommissioned
(
dn
)
;
toRemove
.
add
(
dn
)
;
}
else
if
(
dn
.
isEnteringMaintenance
(
)
)
{
dnAdmin
.
setInMaintenance
(
dn
)
;
LOG
.
debug
(
,
dn
.
getAdminState
(
)
,
dn
)
;
pruneReliableBlocks
(
dn
,
blocks
)
;
}
if
(
blocks
.
size
(
)
==
0
)
{
if
(
!
fullScan
)
{
LOG
.
debug
(
+
,
dn
)
;
blocks
=
handleInsufficientlyStored
(
dn
)
;
outOfServiceNodeBlocks
.
put
(
dn
,
blocks
)
;
}
final
boolean
isHealthy
=
blockManager
.
isNodeHealthyForDecommissionOrMaintenance
(
dn
)
;
if
(
blocks
.
size
(
)
==
0
&&
isHealthy
)
{
if
(
dn
.
isDecommissionInProgress
(
)
)
{
dnAdmin
.
setDecommissioned
(
dn
)
;
toRemove
.
add
(
dn
)
;
}
else
if
(
dn
.
isEnteringMaintenance
(
)
)
{
dnAdmin
.
setInMaintenance
(
dn
)
;
}
else
{
}
if
(
blocks
.
size
(
)
==
0
)
{
if
(
!
fullScan
)
{
LOG
.
debug
(
+
,
dn
)
;
blocks
=
handleInsufficientlyStored
(
dn
)
;
outOfServiceNodeBlocks
.
put
(
dn
,
blocks
)
;
}
final
boolean
isHealthy
=
blockManager
.
isNodeHealthyForDecommissionOrMaintenance
(
dn
)
;
if
(
blocks
.
size
(
)
==
0
&&
isHealthy
)
{
if
(
dn
.
isDecommissionInProgress
(
)
)
{
dnAdmin
.
setDecommissioned
(
dn
)
;
toRemove
.
add
(
dn
)
;
}
else
if
(
dn
.
isEnteringMaintenance
(
)
)
{
dnAdmin
.
setInMaintenance
(
dn
)
;
}
else
{
Preconditions
.
checkState
(
false
,
+
,
dn
,
dn
.
getAdminState
(
)
,
blocks
.
size
(
)
)
;
}
LOG
.
debug
(
+
,
dn
,
dn
.
getAdminState
(
)
)
;
LOG
.
warn
(
,
deprecatedKey
)
;
LOG
.
warn
(
,
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY
)
;
}
checkArgument
(
blocksPerInterval
>
0
,
+
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY
)
;
final
int
maxConcurrentTrackedNodes
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES
,
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT
)
;
checkArgument
(
maxConcurrentTrackedNodes
>=
0
,
+
+
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES
)
;
Class
cls
=
null
;
try
{
cls
=
conf
.
getClass
(
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_MONITOR_CLASS
,
Class
.
forName
(
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_MONITOR_CLASS_DEFAULT
)
)
;
monitor
=
(
DatanodeAdminMonitorInterface
)
ReflectionUtils
.
newInstance
(
cls
,
conf
)
;
monitor
.
setBlockManager
(
blockManager
)
;
monitor
.
setNameSystem
(
namesystem
)
;
monitor
.
setDatanodeAdminManager
(
this
)
;
}
catch
(
Exception
e
)
{
throw
new
RuntimeException
(
+
+
cls
,
e
)
;
}
executor
.
scheduleAtFixedRate
(
monitor
,
intervalSecs
,
intervalSecs
,
TimeUnit
.
SECONDS
)
;
@
VisibleForTesting
public
void
startDecommission
(
DatanodeDescriptor
node
)
{
if
(
!
node
.
isDecommissionInProgress
(
)
&&
!
node
.
isDecommissioned
(
)
)
{
hbManager
.
startDecommission
(
node
)
;
if
(
node
.
isDecommissionInProgress
(
)
)
{
for
(
DatanodeStorageInfo
storage
:
node
.
getStorageInfos
(
)
)
{
@
VisibleForTesting
public
void
startMaintenance
(
DatanodeDescriptor
node
,
long
maintenanceExpireTimeInMS
)
{
node
.
setMaintenanceExpireTimeInMS
(
maintenanceExpireTimeInMS
)
;
if
(
!
node
.
isMaintenance
(
)
)
{
hbManager
.
startMaintenance
(
node
)
;
if
(
node
.
isEnteringMaintenance
(
)
)
{
for
(
DatanodeStorageInfo
storage
:
node
.
getStorageInfos
(
)
)
{
protected
void
setDecommissioned
(
DatanodeDescriptor
dn
)
{
dn
.
setDecommissioned
(
)
;
protected
void
setInMaintenance
(
DatanodeDescriptor
dn
)
{
dn
.
setInMaintenance
(
)
;
@
Override
public
void
setConf
(
Configuration
conf
)
{
this
.
conf
=
conf
;
this
.
maxConcurrentTrackedNodes
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES
,
DFSConfigKeys
.
DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT
)
;
if
(
this
.
maxConcurrentTrackedNodes
<
0
)
{
private
void
pruneStorageMap
(
final
StorageReport
[
]
reports
)
{
synchronized
(
storageMap
)
{
private
void
pruneStorageMap
(
final
StorageReport
[
]
reports
)
{
synchronized
(
storageMap
)
{
LOG
.
debug
(
+
,
reports
.
length
,
storageMap
.
size
(
)
)
;
HashMap
<
String
,
DatanodeStorageInfo
>
excessStorages
;
excessStorages
=
new
HashMap
<
>
(
storageMap
)
;
for
(
final
StorageReport
report
:
reports
)
{
excessStorages
.
remove
(
report
.
getStorage
(
)
.
getStorageID
(
)
)
;
}
for
(
final
DatanodeStorageInfo
storageInfo
:
excessStorages
.
values
(
)
)
{
if
(
storageInfo
.
numBlocks
(
)
==
0
)
{
DatanodeStorageInfo
info
=
storageMap
.
remove
(
storageInfo
.
getStorageID
(
)
)
;
if
(
!
hasStorageType
(
info
.
getStorageType
(
)
)
)
{
if
(
getParent
(
)
instanceof
DFSTopologyNodeImpl
)
{
(
(
DFSTopologyNodeImpl
)
getParent
(
)
)
.
childRemoveStorage
(
getName
(
)
,
info
.
getStorageType
(
)
)
;
}
}
LOG
.
info
(
,
storageInfo
,
this
)
;
}
else
{
private
void
updateFailedStorage
(
Set
<
DatanodeStorageInfo
>
failedStorageInfos
)
{
for
(
DatanodeStorageInfo
storageInfo
:
failedStorageInfos
)
{
if
(
storageInfo
.
getState
(
)
!=
DatanodeStorage
.
State
.
FAILED
)
{
private
List
<
String
>
getNetworkDependencies
(
DatanodeInfo
node
)
throws
UnresolvedTopologyException
{
List
<
String
>
dependencies
=
Collections
.
emptyList
(
)
;
if
(
dnsToSwitchMapping
instanceof
DNSToSwitchMappingWithDependency
)
{
dependencies
=
(
(
DNSToSwitchMappingWithDependency
)
dnsToSwitchMapping
)
.
getDependency
(
node
.
getHostName
(
)
)
;
if
(
dependencies
==
null
)
{
@
VisibleForTesting
void
checkIfClusterIsNowMultiRack
(
DatanodeDescriptor
node
)
{
if
(
!
hasClusterEverBeenMultiRack
&&
networktopology
.
getNumOfRacks
(
)
>
1
)
{
String
message
=
+
node
+
+
;
if
(
blockManager
.
isPopulatingReplQueues
(
)
)
{
message
+=
+
;
final
boolean
isInMaintenance
=
dn
.
isInMaintenance
(
)
;
if
(
(
(
listLiveNodes
&&
!
isDead
)
||
(
listDeadNodes
&&
isDead
)
||
(
listDecommissioningNodes
&&
isDecommissioning
)
||
(
listEnteringMaintenanceNodes
&&
isEnteringMaintenance
)
||
(
listInMaintenanceNodes
&&
isInMaintenance
)
)
&&
hostConfigManager
.
isIncluded
(
dn
)
)
{
nodes
.
add
(
dn
)
;
}
foundNodes
.
add
(
dn
.
getResolvedAddress
(
)
)
;
}
}
Collections
.
sort
(
nodes
)
;
if
(
listDeadNodes
)
{
for
(
InetSocketAddress
addr
:
includedNodes
)
{
if
(
foundNodes
.
matchedBy
(
addr
)
)
{
continue
;
}
DatanodeDescriptor
dn
=
new
DatanodeDescriptor
(
new
DatanodeID
(
addr
.
getAddress
(
)
.
getHostAddress
(
)
,
addr
.
getHostName
(
)
,
,
addr
.
getPort
(
)
==
0
?
defaultXferPort
:
addr
.
getPort
(
)
,
defaultInfoPort
,
defaultInfoSecurePort
,
defaultIpcPort
)
)
;
setDatanodeDead
(
dn
)
;
if
(
hostConfigManager
.
isExcluded
(
dn
)
)
{
dn
.
setDecommissioned
(
)
;
}
nodes
.
add
(
dn
)
;
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
throw
new
DisallowedDatanodeException
(
nodeinfo
)
;
}
if
(
nodeinfo
==
null
||
!
nodeinfo
.
isRegistered
(
)
)
{
return
new
DatanodeCommand
[
]
{
RegisterCommand
.
REGISTER
}
;
}
heartbeatManager
.
updateHeartbeat
(
nodeinfo
,
reports
,
cacheCapacity
,
cacheUsed
,
xceiverCount
,
failedVolumes
,
volumeFailureSummary
)
;
if
(
namesystem
.
isInSafeMode
(
)
)
{
return
new
DatanodeCommand
[
0
]
;
}
final
BlockRecoveryCommand
brCommand
=
getBlockRecoveryCommand
(
blockPoolId
,
nodeinfo
)
;
if
(
brCommand
!=
null
)
{
return
new
DatanodeCommand
[
]
{
brCommand
}
;
}
final
List
<
DatanodeCommand
>
cmds
=
new
ArrayList
<
>
(
)
;
int
totalReplicateBlocks
=
nodeinfo
.
getNumberOfReplicateBlocks
(
)
;
int
totalECBlocks
=
nodeinfo
.
getNumberOfBlocksToBeErasureCoded
(
)
;
int
totalBlocks
=
totalReplicateBlocks
+
totalECBlocks
;
if
(
totalBlocks
>
0
)
{
int
numReplicationTasks
=
(
int
)
Math
.
ceil
(
(
double
)
(
totalReplicateBlocks
*
maxTransfers
)
/
totalBlocks
)
;
public
void
handleLifeline
(
DatanodeRegistration
nodeReg
,
StorageReport
[
]
reports
,
String
blockPoolId
,
long
cacheCapacity
,
long
cacheUsed
,
int
xceiverCount
,
int
maxTransfers
,
int
failedVolumes
,
VolumeFailureSummary
volumeFailureSummary
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
createReplicationWork
(
int
sourceIndex
,
DatanodeStorageInfo
target
)
{
BlockInfoStriped
stripedBlk
=
(
BlockInfoStriped
)
getBlock
(
)
;
final
byte
blockIndex
=
liveBlockIndicies
[
sourceIndex
]
;
final
DatanodeDescriptor
source
=
getSrcNodes
(
)
[
sourceIndex
]
;
final
long
internBlkLen
=
StripedBlockUtil
.
getInternalBlockLength
(
stripedBlk
.
getNumBytes
(
)
,
stripedBlk
.
getCellSize
(
)
,
stripedBlk
.
getDataBlockNum
(
)
,
blockIndex
)
;
final
Block
targetBlk
=
new
Block
(
stripedBlk
.
getBlockId
(
)
+
blockIndex
,
internBlkLen
,
stripedBlk
.
getGenerationStamp
(
)
)
;
source
.
addBlockToBeReplicated
(
targetBlk
,
new
DatanodeStorageInfo
[
]
{
target
}
)
;
synchronized
void
startDecommission
(
final
DatanodeDescriptor
node
)
{
if
(
!
node
.
isAlive
(
)
)
{
synchronized
void
startMaintenance
(
final
DatanodeDescriptor
node
)
{
if
(
!
node
.
isAlive
(
)
)
{
synchronized
void
stopMaintenance
(
final
DatanodeDescriptor
node
)
{
synchronized
void
stopDecommission
(
final
DatanodeDescriptor
node
)
{
private
boolean
removeNodeFromStaleList
(
DatanodeDescriptor
d
,
boolean
isDead
)
{
boolean
result
=
false
;
result
=
staleDataNodes
.
remove
(
d
)
;
if
(
enableLogStaleNodes
&&
result
)
{
boolean
decrement
(
BlockInfo
block
,
DatanodeStorageInfo
dn
)
{
boolean
removed
=
false
;
synchronized
(
pendingReconstructions
)
{
PendingBlockInfo
found
=
pendingReconstructions
.
get
(
block
)
;
if
(
found
!=
null
)
{
DatanodeStorageInfo
getStorage
(
DatanodeDescriptor
dn
,
DatanodeStorage
s
)
throws
IOException
{
if
(
providedEnabled
&&
storageId
.
equals
(
s
.
getStorageID
(
)
)
)
{
if
(
StorageType
.
PROVIDED
.
equals
(
s
.
getStorageType
(
)
)
)
{
if
(
providedStorageInfo
.
getState
(
)
==
State
.
FAILED
&&
s
.
getState
(
)
==
State
.
NORMAL
)
{
providedStorageInfo
.
setState
(
State
.
NORMAL
)
;
private
static
ECTopologyVerifierResult
verifyECWithTopology
(
final
int
minDN
,
final
int
minRack
,
final
int
numOfRacks
,
final
int
numOfDataNodes
,
String
readablePolicies
)
{
String
resultMessage
;
if
(
numOfDataNodes
<
minDN
)
{
resultMessage
=
String
.
format
(
+
+
,
minDN
,
readablePolicies
,
numOfDataNodes
)
;
user
=
(
user
!=
null
?
user
:
)
;
path
=
(
path
!=
null
?
path
:
)
;
LOG
.
trace
(
,
user
,
remoteIp
,
path
)
;
if
(
remoteIp
==
null
)
{
LOG
.
trace
(
)
;
return
false
;
}
List
<
Rule
>
userRules
=
(
(
userRules
=
rulemap
.
get
(
user
)
)
!=
null
)
?
userRules
:
new
ArrayList
<
Rule
>
(
)
;
List
<
Rule
>
anyRules
=
(
(
anyRules
=
rulemap
.
get
(
)
)
!=
null
)
?
anyRules
:
new
ArrayList
<
Rule
>
(
)
;
List
<
Rule
>
rules
=
Stream
.
of
(
userRules
,
anyRules
)
.
flatMap
(
l
->
l
.
stream
(
)
)
.
collect
(
Collectors
.
toList
(
)
)
;
for
(
Rule
rule
:
rules
)
{
SubnetUtils
.
SubnetInfo
subnet
=
rule
.
getSubnet
(
)
;
String
rulePath
=
rule
.
getPath
(
)
;
LOG
.
trace
(
,
subnet
!=
null
?
subnet
.
getCidrSignature
(
)
:
,
rulePath
)
;
try
{
if
(
(
subnet
==
null
||
subnet
.
isInRange
(
remoteIp
)
)
&&
FilenameUtils
.
directoryContains
(
rulePath
,
path
)
)
{
}
else
{
Pattern
comma_split
=
Pattern
.
compile
(
)
;
Pattern
rule_split
=
Pattern
.
compile
(
)
;
Map
<
Integer
,
List
<
String
[
]
>>
splits
=
rule_split
.
splitAsStream
(
ruleString
)
.
map
(
x
->
comma_split
.
split
(
x
,
3
)
)
.
collect
(
Collectors
.
groupingBy
(
x
->
x
.
length
)
)
;
if
(
!
splits
.
keySet
(
)
.
equals
(
Collections
.
singleton
(
3
)
)
)
{
String
bad_lines
=
rule_split
.
splitAsStream
(
ruleString
)
.
filter
(
x
->
comma_split
.
split
(
x
,
3
)
.
length
!=
3
)
.
collect
(
Collectors
.
joining
(
)
)
;
throw
new
IllegalArgumentException
(
+
bad_lines
)
;
}
int
user
=
0
;
int
cidr
=
1
;
int
path
=
2
;
BiFunction
<
CopyOnWriteArrayList
<
Rule
>
,
CopyOnWriteArrayList
<
Rule
>
,
CopyOnWriteArrayList
<
Rule
>>
arrayListMerge
=
(
v1
,
v2
)
->
{
v1
.
addAll
(
v2
)
;
return
v1
;
}
;
for
(
String
[
]
split
:
splits
.
get
(
3
)
)
{
@
Override
public
void
run
(
)
{
if
(
!
metricsLog
.
isInfoEnabled
(
)
||
!
hasAppenders
(
metricsLog
)
||
objectName
==
null
)
{
return
;
}
metricsLog
.
info
(
+
nodeName
+
)
;
final
MBeanServer
server
=
ManagementFactory
.
getPlatformMBeanServer
(
)
;
for
(
final
ObjectName
mbeanName
:
server
.
queryNames
(
objectName
,
null
)
)
{
try
{
MBeanInfo
mBeanInfo
=
server
.
getMBeanInfo
(
mbeanName
)
;
final
String
mBeanNameName
=
MBeans
.
getMbeanNameName
(
mbeanName
)
;
final
Set
<
String
>
attributeNames
=
getFilteredAttributes
(
mBeanInfo
)
;
final
AttributeList
attributes
=
server
.
getAttributes
(
mbeanName
,
attributeNames
.
toArray
(
new
String
[
attributeNames
.
size
(
)
]
)
)
;
for
(
Object
o
:
attributes
)
{
final
Attribute
attribute
=
(
Attribute
)
o
;
final
Object
value
=
attribute
.
getValue
(
)
;
final
String
valueStr
=
(
value
!=
null
)
?
value
.
toString
(
)
:
;
return
;
}
metricsLog
.
info
(
+
nodeName
+
)
;
final
MBeanServer
server
=
ManagementFactory
.
getPlatformMBeanServer
(
)
;
for
(
final
ObjectName
mbeanName
:
server
.
queryNames
(
objectName
,
null
)
)
{
try
{
MBeanInfo
mBeanInfo
=
server
.
getMBeanInfo
(
mbeanName
)
;
final
String
mBeanNameName
=
MBeans
.
getMbeanNameName
(
mbeanName
)
;
final
Set
<
String
>
attributeNames
=
getFilteredAttributes
(
mBeanInfo
)
;
final
AttributeList
attributes
=
server
.
getAttributes
(
mbeanName
,
attributeNames
.
toArray
(
new
String
[
attributeNames
.
size
(
)
]
)
)
;
for
(
Object
o
:
attributes
)
{
final
Attribute
attribute
=
(
Attribute
)
o
;
final
Object
value
=
attribute
.
getValue
(
)
;
final
String
valueStr
=
(
value
!=
null
)
?
value
.
toString
(
)
:
;
metricsLog
.
info
(
mBeanNameName
+
+
attribute
.
getName
(
)
+
+
trimLine
(
valueStr
)
)
;
}
}
catch
(
Exception
e
)
{
metricsLog
.
info
(
+
nodeName
+
)
;
final
MBeanServer
server
=
ManagementFactory
.
getPlatformMBeanServer
(
)
;
for
(
final
ObjectName
mbeanName
:
server
.
queryNames
(
objectName
,
null
)
)
{
try
{
MBeanInfo
mBeanInfo
=
server
.
getMBeanInfo
(
mbeanName
)
;
final
String
mBeanNameName
=
MBeans
.
getMbeanNameName
(
mbeanName
)
;
final
Set
<
String
>
attributeNames
=
getFilteredAttributes
(
mBeanInfo
)
;
final
AttributeList
attributes
=
server
.
getAttributes
(
mbeanName
,
attributeNames
.
toArray
(
new
String
[
attributeNames
.
size
(
)
]
)
)
;
for
(
Object
o
:
attributes
)
{
final
Attribute
attribute
=
(
Attribute
)
o
;
final
Object
value
=
attribute
.
getValue
(
)
;
final
String
valueStr
=
(
value
!=
null
)
?
value
.
toString
(
)
:
;
metricsLog
.
info
(
mBeanNameName
+
+
attribute
.
getName
(
)
+
+
trimLine
(
valueStr
)
)
;
}
}
catch
(
Exception
e
)
{
metricsLog
.
error
(
+
nodeName
+
+
mbeanName
.
toString
(
)
,
e
)
;
public
static
void
checkVersionUpgradable
(
int
oldVersion
)
throws
IOException
{
if
(
oldVersion
>
LAST_UPGRADABLE_LAYOUT_VERSION
)
{
String
msg
=
+
+
oldVersion
+
+
+
LAST_UPGRADABLE_HADOOP_VERSION
+
+
+
(
oldVersion
==
0
?
:
(
+
oldVersion
)
)
+
+
+
LAST_UPGRADABLE_LAYOUT_VERSION
+
;
throw
new
IOException
(
+
srcFile
+
+
destFile
+
)
;
}
File
parentFile
=
destFile
.
getParentFile
(
)
;
if
(
parentFile
!=
null
)
{
if
(
!
parentFile
.
mkdirs
(
)
&&
!
parentFile
.
isDirectory
(
)
)
{
throw
new
IOException
(
+
parentFile
+
)
;
}
}
if
(
destFile
.
exists
(
)
)
{
if
(
FileUtil
.
canWrite
(
destFile
)
==
false
)
{
throw
new
IOException
(
+
destFile
+
)
;
}
else
{
if
(
destFile
.
delete
(
)
==
false
)
{
throw
new
IOException
(
+
destFile
+
)
;
}
}
}
try
{
NativeIO
.
copyFileUnbuffered
(
srcFile
,
destFile
)
;
}
catch
(
NativeIOException
e
)
{
throw
new
IOException
(
+
srcFile
.
getCanonicalPath
(
)
+
+
destFile
.
getCanonicalPath
(
)
+
+
e
.
toString
(
)
)
;
while
(
num
>
0
)
{
num
=
stream
.
read
(
buf
)
;
if
(
num
>
0
)
{
received
+=
num
;
for
(
FileOutputStream
fos
:
outputStreams
)
{
fos
.
write
(
buf
,
0
,
num
)
;
}
if
(
throttler
!=
null
)
{
throttler
.
throttle
(
num
)
;
}
}
}
finishedReceiving
=
true
;
double
xferSec
=
Math
.
max
(
(
(
float
)
(
Time
.
monotonicNow
(
)
-
startTime
)
)
/
1000.0
,
0.001
)
;
long
xferKb
=
received
/
1024
;
xferCombined
+=
xferSec
;
xferStats
.
append
(
String
.
format
(
,
xferSec
,
xferKb
/
xferSec
)
)
;
}
finally
{
stream
.
close
(
)
;
public
static
boolean
isDiskStatsEnabled
(
int
fileIOSamplingPercentage
)
{
final
boolean
isEnabled
;
if
(
fileIOSamplingPercentage
<=
0
)
{
@
Override
public
Reader
<
FileRegion
>
getReader
(
Reader
.
Options
opts
,
String
blockPoolID
)
throws
IOException
{
InMemoryAliasMapProtocol
aliasMap
=
getAliasMap
(
blockPoolID
)
;
@
Override
public
Writer
<
FileRegion
>
getWriter
(
Writer
.
Options
opts
,
String
blockPoolID
)
throws
IOException
{
InMemoryAliasMapProtocol
aliasMap
=
getAliasMap
(
blockPoolID
)
;
public
BlockMovementStatus
moveBlock
(
BlockMovingInfo
blkMovingInfo
,
SaslDataTransferClient
saslClient
,
ExtendedBlock
eb
,
Socket
sock
,
DataEncryptionKeyFactory
km
,
Token
<
BlockTokenIdentifier
>
accessToken
)
{
public
BlockMovementStatus
moveBlock
(
BlockMovingInfo
blkMovingInfo
,
SaslDataTransferClient
saslClient
,
ExtendedBlock
eb
,
Socket
sock
,
DataEncryptionKeyFactory
km
,
Token
<
BlockTokenIdentifier
>
accessToken
)
{
LOG
.
info
(
+
,
blkMovingInfo
.
getBlock
(
)
,
blkMovingInfo
.
getSource
(
)
,
blkMovingInfo
.
getTarget
(
)
,
blkMovingInfo
.
getSourceStorageType
(
)
,
blkMovingInfo
.
getTargetStorageType
(
)
)
;
DataOutputStream
out
=
null
;
DataInputStream
in
=
null
;
try
{
NetUtils
.
connect
(
sock
,
NetUtils
.
createSocketAddr
(
blkMovingInfo
.
getTarget
(
)
.
getXferAddr
(
connectToDnViaHostname
)
)
,
socketTimeout
)
;
sock
.
setSoTimeout
(
socketTimeout
*
5
)
;
sock
.
setKeepAlive
(
true
)
;
OutputStream
unbufOut
=
sock
.
getOutputStream
(
)
;
InputStream
unbufIn
=
sock
.
getInputStream
(
)
;
DataInputStream
in
=
null
;
try
{
NetUtils
.
connect
(
sock
,
NetUtils
.
createSocketAddr
(
blkMovingInfo
.
getTarget
(
)
.
getXferAddr
(
connectToDnViaHostname
)
)
,
socketTimeout
)
;
sock
.
setSoTimeout
(
socketTimeout
*
5
)
;
sock
.
setKeepAlive
(
true
)
;
OutputStream
unbufOut
=
sock
.
getOutputStream
(
)
;
InputStream
unbufIn
=
sock
.
getInputStream
(
)
;
LOG
.
debug
(
,
blkMovingInfo
.
getTarget
(
)
)
;
IOStreamPair
saslStreams
=
saslClient
.
socketSend
(
sock
,
unbufOut
,
unbufIn
,
km
,
accessToken
,
blkMovingInfo
.
getTarget
(
)
)
;
unbufOut
=
saslStreams
.
out
;
unbufIn
=
saslStreams
.
in
;
out
=
new
DataOutputStream
(
new
BufferedOutputStream
(
unbufOut
,
ioFileBufferSize
)
)
;
in
=
new
DataInputStream
(
new
BufferedInputStream
(
unbufIn
,
ioFileBufferSize
)
)
;
sendRequest
(
out
,
eb
,
accessToken
,
blkMovingInfo
.
getSource
(
)
,
blkMovingInfo
.
getTargetStorageType
(
)
)
;
receiveResponse
(
in
)
;
sock
.
setSoTimeout
(
socketTimeout
*
5
)
;
sock
.
setKeepAlive
(
true
)
;
OutputStream
unbufOut
=
sock
.
getOutputStream
(
)
;
InputStream
unbufIn
=
sock
.
getInputStream
(
)
;
LOG
.
debug
(
,
blkMovingInfo
.
getTarget
(
)
)
;
IOStreamPair
saslStreams
=
saslClient
.
socketSend
(
sock
,
unbufOut
,
unbufIn
,
km
,
accessToken
,
blkMovingInfo
.
getTarget
(
)
)
;
unbufOut
=
saslStreams
.
out
;
unbufIn
=
saslStreams
.
in
;
out
=
new
DataOutputStream
(
new
BufferedOutputStream
(
unbufOut
,
ioFileBufferSize
)
)
;
in
=
new
DataInputStream
(
new
BufferedInputStream
(
unbufIn
,
ioFileBufferSize
)
)
;
sendRequest
(
out
,
eb
,
accessToken
,
blkMovingInfo
.
getSource
(
)
,
blkMovingInfo
.
getTargetStorageType
(
)
)
;
receiveResponse
(
in
)
;
LOG
.
info
(
+
,
blkMovingInfo
.
getBlock
(
)
,
blkMovingInfo
.
getSource
(
)
,
blkMovingInfo
.
getTarget
(
)
,
blkMovingInfo
.
getTargetStorageType
(
)
)
;
return
BlockMovementStatus
.
DN_BLK_STORAGE_MOVEMENT_SUCCESS
;
}
catch
(
BlockPinningException
e
)
{
@
Override
public
void
run
(
)
{
while
(
running
)
{
try
{
Future
<
BlockMovementAttemptFinished
>
future
=
moverCompletionService
.
take
(
)
;
if
(
future
!=
null
)
{
BlockMovementAttemptFinished
result
=
future
.
get
(
)
;
void
verifyAndSetNamespaceInfo
(
BPServiceActor
actor
,
NamespaceInfo
nsInfo
)
throws
IOException
{
writeLock
(
)
;
if
(
nsInfo
.
getState
(
)
==
HAServiceState
.
ACTIVE
&&
bpServiceToActive
==
null
)
{
@
VisibleForTesting
NamespaceInfo
retrieveNamespaceInfo
(
)
throws
IOException
{
NamespaceInfo
nsInfo
=
null
;
while
(
shouldRun
(
)
)
{
try
{
nsInfo
=
bpNamenode
.
versionRequest
(
)
;
cmds
.
add
(
cmd
)
;
}
}
else
{
for
(
int
r
=
0
;
r
<
reports
.
length
;
r
++
)
{
StorageBlockReport
singleReport
[
]
=
{
reports
[
r
]
}
;
DatanodeCommand
cmd
=
bpNamenode
.
blockReport
(
bpRegistration
,
bpos
.
getBlockPoolId
(
)
,
singleReport
,
new
BlockReportContext
(
reports
.
length
,
r
,
reportId
,
fullBrLeaseId
,
true
)
)
;
blockReportSizes
.
add
(
calculateBlockReportPBSize
(
useBlocksBuffer
,
singleReport
)
)
;
numReportsSent
++
;
numRPCs
++
;
if
(
cmd
!=
null
)
{
cmds
.
add
(
cmd
)
;
}
}
}
success
=
true
;
}
finally
{
long
brSendCost
=
monotonicNow
(
)
-
brSendStartTime
;
long
brCreateCost
=
brSendStartTime
-
brCreateStartTime
;
dn
.
getMetrics
(
)
.
addBlockReport
(
brSendCost
,
getRpcMetricSuffix
(
)
)
;
}
DatanodeCommand
cmd
=
null
;
final
long
startTime
=
monotonicNow
(
)
;
if
(
startTime
-
lastCacheReport
>
dnConf
.
cacheReportInterval
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
this
)
;
}
lastCacheReport
=
startTime
;
String
bpid
=
bpos
.
getBlockPoolId
(
)
;
List
<
Long
>
blockIds
=
dn
.
getFSDataset
(
)
.
getCacheReport
(
bpid
)
;
long
createTime
=
monotonicNow
(
)
;
cmd
=
bpNamenode
.
cacheReport
(
bpRegistration
,
bpid
,
blockIds
)
;
long
sendTime
=
monotonicNow
(
)
;
long
createCost
=
createTime
-
startTime
;
long
sendCost
=
sendTime
-
createTime
;
dn
.
getMetrics
(
)
.
addCacheReport
(
sendCost
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
HeartbeatResponse
sendHeartBeat
(
boolean
requestBlockReportLease
)
throws
IOException
{
scheduler
.
scheduleNextHeartbeat
(
)
;
StorageReport
[
]
reports
=
dn
.
getFSDataset
(
)
.
getStorageReports
(
bpos
.
getBlockPoolId
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
handleRollingUpgradeStatus
(
HeartbeatResponse
resp
)
throws
IOException
{
RollingUpgradeStatus
rollingUpgradeStatus
=
resp
.
getRollingUpdateStatus
(
)
;
if
(
rollingUpgradeStatus
!=
null
&&
rollingUpgradeStatus
.
getBlockPoolId
(
)
.
compareTo
(
bpos
.
getBlockPoolId
(
)
)
!=
0
)
{
catch
(
IOException
ioe
)
{
runningState
=
RunningState
.
INIT_FAILED
;
if
(
shouldRetryInit
(
)
)
{
LOG
.
error
(
+
this
+
+
ioe
.
getLocalizedMessage
(
)
)
;
sleepAndLogInterrupts
(
5000
,
)
;
}
else
{
runningState
=
RunningState
.
FAILED
;
LOG
.
error
(
+
this
+
,
ioe
)
;
return
;
}
}
}
runningState
=
RunningState
.
RUNNING
;
if
(
initialRegistrationComplete
!=
null
)
{
initialRegistrationComplete
.
countDown
(
)
;
}
while
(
shouldRun
(
)
)
{
try
{
offerService
(
)
;
List
<
StorageDirectory
>
recoverTransitionRead
(
NamespaceInfo
nsInfo
,
StorageLocation
location
,
StartupOption
startOpt
,
List
<
Callable
<
StorageDirectory
>>
callables
,
Configuration
conf
)
throws
IOException
{
private
void
format
(
StorageDirectory
bpSdir
,
NamespaceInfo
nsInfo
)
throws
IOException
{
void
remove
(
File
absPathToRemove
)
{
Preconditions
.
checkArgument
(
absPathToRemove
.
isAbsolute
(
)
)
;
int
filesRestored
=
0
;
File
[
]
children
=
trashRoot
.
exists
(
)
?
trashRoot
.
listFiles
(
)
:
null
;
if
(
children
==
null
)
{
return
0
;
}
File
restoreDirectory
=
null
;
for
(
File
child
:
children
)
{
if
(
child
.
isDirectory
(
)
)
{
filesRestored
+=
restoreBlockFilesFromTrash
(
child
)
;
continue
;
}
if
(
restoreDirectory
==
null
)
{
restoreDirectory
=
new
File
(
getRestoreDirectory
(
child
)
)
;
if
(
!
restoreDirectory
.
exists
(
)
&&
!
restoreDirectory
.
mkdirs
(
)
)
{
throw
new
IOException
(
+
restoreDirectory
)
;
}
}
final
File
newChild
=
new
File
(
restoreDirectory
,
child
.
getName
(
)
)
;
if
(
newChild
.
exists
(
)
&&
newChild
.
length
(
)
>=
child
.
length
(
)
)
{
private
static
void
linkAllBlocks
(
File
fromDir
,
File
toDir
,
int
diskLayoutVersion
,
Configuration
conf
)
throws
IOException
{
HardLink
hardLink
=
new
HardLink
(
)
;
DataStorage
.
linkBlocks
(
fromDir
,
toDir
,
DataStorage
.
STORAGE_DIR_FINALIZED
,
diskLayoutVersion
,
hardLink
,
conf
)
;
DataStorage
.
linkBlocks
(
fromDir
,
toDir
,
DataStorage
.
STORAGE_DIR_RBW
,
diskLayoutVersion
,
hardLink
,
conf
)
;
@
VisibleForTesting
String
getRestoreDirectory
(
File
blockFile
)
{
Matcher
matcher
=
BLOCK_POOL_TRASH_PATH_PATTERN
.
matcher
(
blockFile
.
getParent
(
)
)
;
String
restoreDirectory
=
matcher
.
replaceFirst
(
+
STORAGE_DIR_CURRENT
+
)
;
boolean
packetSentInTime
(
)
{
final
long
diff
=
Time
.
monotonicNow
(
)
-
this
.
lastSentTime
.
get
(
)
;
final
boolean
allowedIdleTime
=
(
diff
<=
this
.
maxSendIdleTime
)
;
private
void
handleMirrorOutError
(
IOException
ioe
)
throws
IOException
{
String
bpid
=
block
.
getBlockPoolId
(
)
;
private
int
receivePacket
(
)
throws
IOException
{
packetReceiver
.
receiveNextPacket
(
in
)
;
PacketHeader
header
=
packetReceiver
.
getHeader
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
boolean
alignedOnDisk
=
partialChunkSizeOnDisk
==
0
;
boolean
alignedInPacket
=
firstByteInBlock
%
bytesPerChecksum
==
0
;
boolean
overwriteLastCrc
=
!
alignedOnDisk
&&
!
shouldNotWriteChecksum
;
boolean
doCrcRecalc
=
overwriteLastCrc
&&
(
lastChunkBoundary
!=
firstByteInBlock
)
;
if
(
!
alignedInPacket
&&
len
>
bytesPerChecksum
)
{
throw
new
IOException
(
+
block
+
+
inAddr
+
+
+
len
+
+
bytesPerChecksum
+
)
;
}
Checksum
partialCrc
=
null
;
if
(
doCrcRecalc
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
block
+
+
+
onDiskLen
)
;
}
long
offsetInChecksum
=
BlockMetadataHeader
.
getHeaderSize
(
)
+
onDiskLen
/
bytesPerChecksum
*
checksumSize
;
partialCrc
=
computePartialChunkCrc
(
onDiskLen
,
offsetInChecksum
)
;
}
int
startByteToDisk
=
(
int
)
(
onDiskLen
-
firstByteInBlock
)
+
dataBuf
.
arrayOffset
(
)
+
dataBuf
.
position
(
)
;
int
numBytesToDisk
=
(
int
)
(
offsetInBlock
-
onDiskLen
)
;
long
begin
=
Time
.
monotonicNow
(
)
;
while
(
receivePacket
(
)
>=
0
)
{
}
if
(
responder
!=
null
)
{
(
(
PacketResponder
)
responder
.
getRunnable
(
)
)
.
close
(
)
;
responderClosed
=
true
;
}
if
(
isDatanode
||
isTransfer
)
{
try
(
ReplicaHandler
handler
=
claimReplicaHandler
(
)
)
{
close
(
)
;
block
.
setNumBytes
(
replicaInfo
.
getNumBytes
(
)
)
;
if
(
stage
==
BlockConstructionStage
.
TRANSFER_RBW
)
{
datanode
.
data
.
convertTemporaryToRbw
(
block
)
;
}
else
{
datanode
.
data
.
finalizeBlock
(
block
,
dirSyncOnFinalize
)
;
}
}
datanode
.
metrics
.
incrBlocksWritten
(
)
;
}
}
catch
(
IOException
ioe
)
{
replicaInfo
.
releaseAllBytesReserved
(
)
;
if
(
responder
!=
null
)
{
(
(
PacketResponder
)
responder
.
getRunnable
(
)
)
.
close
(
)
;
responderClosed
=
true
;
}
if
(
isDatanode
||
isTransfer
)
{
try
(
ReplicaHandler
handler
=
claimReplicaHandler
(
)
)
{
close
(
)
;
block
.
setNumBytes
(
replicaInfo
.
getNumBytes
(
)
)
;
if
(
stage
==
BlockConstructionStage
.
TRANSFER_RBW
)
{
datanode
.
data
.
convertTemporaryToRbw
(
block
)
;
}
else
{
datanode
.
data
.
finalizeBlock
(
block
,
dirSyncOnFinalize
)
;
}
}
datanode
.
metrics
.
incrBlocksWritten
(
)
;
}
}
catch
(
IOException
ioe
)
{
replicaInfo
.
releaseAllBytesReserved
(
)
;
if
(
datanode
.
isRestarting
(
)
)
{
private
void
initPerfMonitoring
(
DatanodeInfo
[
]
downstreams
)
{
if
(
downstreams
!=
null
&&
downstreams
.
length
>
0
)
{
downstreamDNs
=
downstreams
;
isPenultimateNode
=
(
downstreams
.
length
==
1
)
;
if
(
isPenultimateNode
&&
datanode
.
getPeerMetrics
(
)
!=
null
)
{
mirrorNameForMetrics
=
(
downstreams
[
0
]
.
getInfoSecurePort
(
)
!=
0
?
downstreams
[
0
]
.
getInfoSecureAddr
(
)
:
downstreams
[
0
]
.
getInfoAddr
(
)
)
;
private
Checksum
computePartialChunkCrc
(
long
blkoff
,
long
ckoff
)
throws
IOException
{
int
sizePartialChunk
=
(
int
)
(
blkoff
%
bytesPerChecksum
)
;
blkoff
=
blkoff
-
sizePartialChunk
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
logRecoverBlock
(
String
who
,
RecoveringBlock
rb
)
{
ExtendedBlock
block
=
rb
.
getBlock
(
)
;
DatanodeInfo
[
]
targets
=
rb
.
getLocations
(
)
;
public
synchronized
void
addVolumeScanner
(
FsVolumeReference
ref
)
{
boolean
success
=
false
;
try
{
FsVolumeSpi
volume
=
ref
.
getVolume
(
)
;
if
(
!
isEnabled
(
)
)
{
public
synchronized
void
removeVolumeScanner
(
FsVolumeSpi
volume
)
{
if
(
!
isEnabled
(
)
)
{
synchronized
void
markSuspectBlock
(
String
storageId
,
ExtendedBlock
block
)
{
if
(
!
isEnabled
(
)
)
{
}
}
try
{
if
(
transferTo
)
{
SocketOutputStream
sockOut
=
(
SocketOutputStream
)
out
;
sockOut
.
write
(
buf
,
headerOff
,
dataOff
-
headerOff
)
;
FileChannel
fileCh
=
(
(
FileInputStream
)
ris
.
getDataIn
(
)
)
.
getChannel
(
)
;
LongWritable
waitTime
=
new
LongWritable
(
)
;
LongWritable
transferTime
=
new
LongWritable
(
)
;
fileIoProvider
.
transferToSocketFully
(
ris
.
getVolumeRef
(
)
.
getVolume
(
)
,
sockOut
,
fileCh
,
blockInPosition
,
dataLen
,
waitTime
,
transferTime
)
;
datanode
.
metrics
.
addSendDataPacketBlockedOnNetworkNanos
(
waitTime
.
get
(
)
)
;
datanode
.
metrics
.
addSendDataPacketTransferNanos
(
transferTime
.
get
(
)
)
;
blockInPosition
+=
dataLen
;
}
else
{
out
.
write
(
buf
,
headerOff
,
dataOff
+
dataLen
-
headerOff
)
;
}
}
catch
(
IOException
e
)
{
if
(
e
instanceof
SocketTimeoutException
)
{
pktBufSize
+=
(
chunkSize
+
checksumSize
)
*
maxChunksPerPacket
;
}
ByteBuffer
pktBuf
=
ByteBuffer
.
allocate
(
pktBufSize
)
;
while
(
endOffset
>
offset
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
manageOsCache
(
)
;
long
len
=
sendPacket
(
pktBuf
,
maxChunksPerPacket
,
streamForSendChunks
,
transferTo
,
throttler
)
;
offset
+=
len
;
totalRead
+=
len
+
(
numberOfChunks
(
len
)
*
checksumSize
)
;
seqno
++
;
}
if
(
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
try
{
sendPacket
(
pktBuf
,
maxChunksPerPacket
,
streamForSendChunks
,
transferTo
,
throttler
)
;
out
.
flush
(
)
;
}
catch
(
IOException
e
)
{
throw
ioeToSocketException
(
e
)
;
}
sentEntireByteRange
=
true
;
@
Override
public
String
reconfigurePropertyImpl
(
String
property
,
String
newVal
)
throws
ReconfigurationException
{
switch
(
property
)
{
case
DFS_DATANODE_DATA_DIR_KEY
:
{
IOException
rootException
=
null
;
try
{
catch
(
IOException
e
)
{
rootException
=
e
;
}
finally
{
try
{
triggerBlockReport
(
new
BlockReportOptions
.
Factory
(
)
.
setIncremental
(
false
)
.
build
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
,
property
,
newVal
,
e
)
;
if
(
rootException
==
null
)
{
rootException
=
e
;
}
}
finally
{
if
(
rootException
!=
null
)
{
throw
new
ReconfigurationException
(
property
,
newVal
,
getConf
(
)
.
get
(
property
)
,
rootException
)
;
}
}
}
break
;
}
case
DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY
:
{
ReconfigurationException
rootException
=
null
;
}
ChangedVolumes
results
=
new
ChangedVolumes
(
)
;
results
.
newLocations
.
addAll
(
newStorageLocations
)
;
for
(
Iterator
<
Storage
.
StorageDirectory
>
it
=
storage
.
dirIterator
(
)
;
it
.
hasNext
(
)
;
)
{
Storage
.
StorageDirectory
dir
=
it
.
next
(
)
;
boolean
found
=
false
;
for
(
Iterator
<
StorageLocation
>
newLocationItr
=
results
.
newLocations
.
iterator
(
)
;
newLocationItr
.
hasNext
(
)
;
)
{
StorageLocation
newLocation
=
newLocationItr
.
next
(
)
;
if
(
newLocation
.
matchesStorageDirectory
(
dir
)
)
{
StorageLocation
oldLocation
=
existingStorageLocations
.
get
(
newLocation
.
getNormalizedUri
(
)
.
toString
(
)
)
;
if
(
oldLocation
!=
null
&&
oldLocation
.
getStorageType
(
)
!=
newLocation
.
getStorageType
(
)
)
{
throw
new
IOException
(
)
;
}
newLocationItr
.
remove
(
)
;
results
.
unchangedLocations
.
add
(
newLocation
)
;
found
=
true
;
break
;
throw
new
IOException
(
)
;
}
newLocationItr
.
remove
(
)
;
results
.
unchangedLocations
.
add
(
newLocation
)
;
found
=
true
;
break
;
}
}
if
(
!
found
)
{
LOG
.
info
(
,
dir
.
getRoot
(
)
)
;
results
.
deactivateLocations
.
add
(
StorageLocation
.
parse
(
dir
.
getRoot
(
)
.
toString
(
)
)
)
;
}
}
if
(
getFSDataset
(
)
.
getNumFailedVolumes
(
)
>
0
)
{
for
(
String
failedStorageLocation
:
getFSDataset
(
)
.
getVolumeFailureSummary
(
)
.
getFailedStorageLocations
(
)
)
{
boolean
found
=
false
;
for
(
Iterator
<
StorageLocation
>
newLocationItr
=
results
.
newLocations
.
iterator
(
)
;
newLocationItr
.
hasNext
(
)
;
)
{
StorageLocation
newLocation
=
newLocationItr
.
next
(
)
;
if
(
newLocation
.
getNormalizedUri
(
)
.
toString
(
)
.
equals
(
failedStorageLocation
)
)
{
found
=
true
;
service
=
Executors
.
newFixedThreadPool
(
changedVolumes
.
newLocations
.
size
(
)
)
;
List
<
Future
<
IOException
>>
exceptions
=
Lists
.
newArrayList
(
)
;
for
(
final
StorageLocation
location
:
changedVolumes
.
newLocations
)
{
exceptions
.
add
(
service
.
submit
(
new
Callable
<
IOException
>
(
)
{
@
Override
public
IOException
call
(
)
{
try
{
data
.
addVolume
(
location
,
nsInfos
)
;
}
catch
(
IOException
e
)
{
return
e
;
}
return
null
;
}
}
)
)
;
}
for
(
int
i
=
0
;
i
<
changedVolumes
.
newLocations
.
size
(
)
;
i
++
)
{
StorageLocation
volume
=
changedVolumes
.
newLocations
.
get
(
i
)
;
Future
<
IOException
>
ioExceptionFuture
=
exceptions
.
get
(
i
)
;
try
{
exceptions
.
add
(
service
.
submit
(
new
Callable
<
IOException
>
(
)
{
@
Override
public
IOException
call
(
)
{
try
{
data
.
addVolume
(
location
,
nsInfos
)
;
}
catch
(
IOException
e
)
{
return
e
;
}
return
null
;
}
}
)
)
;
}
for
(
int
i
=
0
;
i
<
changedVolumes
.
newLocations
.
size
(
)
;
i
++
)
{
StorageLocation
volume
=
changedVolumes
.
newLocations
.
get
(
i
)
;
Future
<
IOException
>
ioExceptionFuture
=
exceptions
.
get
(
i
)
;
try
{
IOException
ioe
=
ioExceptionFuture
.
get
(
)
;
if
(
ioe
!=
null
)
{
errorMessageBuilder
.
append
(
String
.
format
(
,
volume
,
ioe
.
getMessage
(
)
)
)
;
data
.
addVolume
(
location
,
nsInfos
)
;
}
catch
(
IOException
e
)
{
return
e
;
}
return
null
;
}
}
)
)
;
}
for
(
int
i
=
0
;
i
<
changedVolumes
.
newLocations
.
size
(
)
;
i
++
)
{
StorageLocation
volume
=
changedVolumes
.
newLocations
.
get
(
i
)
;
Future
<
IOException
>
ioExceptionFuture
=
exceptions
.
get
(
i
)
;
try
{
IOException
ioe
=
ioExceptionFuture
.
get
(
)
;
if
(
ioe
!=
null
)
{
errorMessageBuilder
.
append
(
String
.
format
(
,
volume
,
ioe
.
getMessage
(
)
)
)
;
LOG
.
error
(
,
volume
,
ioe
)
;
}
else
{
effectiveVolumes
.
add
(
volume
.
toString
(
)
)
;
return
null
;
}
}
)
)
;
}
for
(
int
i
=
0
;
i
<
changedVolumes
.
newLocations
.
size
(
)
;
i
++
)
{
StorageLocation
volume
=
changedVolumes
.
newLocations
.
get
(
i
)
;
Future
<
IOException
>
ioExceptionFuture
=
exceptions
.
get
(
i
)
;
try
{
IOException
ioe
=
ioExceptionFuture
.
get
(
)
;
if
(
ioe
!=
null
)
{
errorMessageBuilder
.
append
(
String
.
format
(
,
volume
,
ioe
.
getMessage
(
)
)
)
;
LOG
.
error
(
,
volume
,
ioe
)
;
}
else
{
effectiveVolumes
.
add
(
volume
.
toString
(
)
)
;
LOG
.
info
(
,
volume
)
;
}
}
catch
(
Exception
e
)
{
errorMessageBuilder
.
append
(
String
.
format
(
,
volume
,
e
.
toString
(
)
)
)
;
private
void
initIpcServer
(
)
throws
IOException
{
InetSocketAddress
ipcAddr
=
NetUtils
.
createSocketAddr
(
getConf
(
)
.
getTrimmed
(
DFS_DATANODE_IPC_ADDRESS_KEY
)
)
;
RPC
.
setProtocolEngine
(
getConf
(
)
,
ClientDatanodeProtocolPB
.
class
,
ProtobufRpcEngine2
.
class
)
;
ClientDatanodeProtocolServerSideTranslatorPB
clientDatanodeProtocolXlator
=
new
ClientDatanodeProtocolServerSideTranslatorPB
(
this
)
;
BlockingService
service
=
ClientDatanodeProtocolService
.
newReflectiveBlockingService
(
clientDatanodeProtocolXlator
)
;
ipcServer
=
new
RPC
.
Builder
(
getConf
(
)
)
.
setProtocol
(
ClientDatanodeProtocolPB
.
class
)
.
setInstance
(
service
)
.
setBindAddress
(
ipcAddr
.
getHostName
(
)
)
.
setPort
(
ipcAddr
.
getPort
(
)
)
.
setNumHandlers
(
getConf
(
)
.
getInt
(
DFS_DATANODE_HANDLER_COUNT_KEY
,
DFS_DATANODE_HANDLER_COUNT_DEFAULT
)
)
.
setVerbose
(
false
)
.
setSecretManager
(
blockPoolTokenSecretManager
)
.
build
(
)
;
ReconfigurationProtocolServerSideTranslatorPB
reconfigurationProtocolXlator
=
new
ReconfigurationProtocolServerSideTranslatorPB
(
this
)
;
service
=
ReconfigurationProtocolService
.
newReflectiveBlockingService
(
reconfigurationProtocolXlator
)
;
DFSUtil
.
addPBProtocol
(
getConf
(
)
,
ReconfigurationProtocolPB
.
class
,
service
,
ipcServer
)
;
InterDatanodeProtocolServerSideTranslatorPB
interDatanodeProtocolXlator
=
new
InterDatanodeProtocolServerSideTranslatorPB
(
this
)
;
service
=
InterDatanodeProtocolService
.
newReflectiveBlockingService
(
interDatanodeProtocolXlator
)
;
DFSUtil
.
addPBProtocol
(
getConf
(
)
,
InterDatanodeProtocolPB
.
class
,
service
,
ipcServer
)
;
TraceAdminProtocolServerSideTranslatorPB
traceAdminXlator
=
new
TraceAdminProtocolServerSideTranslatorPB
(
this
)
;
BlockingService
traceAdminService
=
TraceAdminService
.
newReflectiveBlockingService
(
traceAdminXlator
)
;
DFSUtil
.
addPBProtocol
(
getConf
(
)
,
TraceAdminProtocolPB
.
class
,
traceAdminService
,
ipcServer
)
;
}
else
{
int
backlogLength
=
getConf
(
)
.
getInt
(
CommonConfigurationKeysPublic
.
IPC_SERVER_LISTEN_QUEUE_SIZE_KEY
,
CommonConfigurationKeysPublic
.
IPC_SERVER_LISTEN_QUEUE_SIZE_DEFAULT
)
;
tcpPeerServer
=
new
TcpPeerServer
(
dnConf
.
socketWriteTimeout
,
DataNode
.
getStreamingAddr
(
getConf
(
)
)
,
backlogLength
)
;
}
if
(
dnConf
.
getTransferSocketRecvBufferSize
(
)
>
0
)
{
tcpPeerServer
.
setReceiveBufferSize
(
dnConf
.
getTransferSocketRecvBufferSize
(
)
)
;
}
streamingAddr
=
tcpPeerServer
.
getStreamingAddr
(
)
;
LOG
.
info
(
,
streamingAddr
)
;
this
.
threadGroup
=
new
ThreadGroup
(
)
;
xserver
=
new
DataXceiverServer
(
tcpPeerServer
,
getConf
(
)
,
this
)
;
this
.
dataXceiverServer
=
new
Daemon
(
threadGroup
,
xserver
)
;
this
.
threadGroup
.
setDaemon
(
true
)
;
if
(
getConf
(
)
.
getBoolean
(
HdfsClientConfigKeys
.
Read
.
ShortCircuit
.
KEY
,
HdfsClientConfigKeys
.
Read
.
ShortCircuit
.
DEFAULT
)
||
getConf
(
)
.
getBoolean
(
HdfsClientConfigKeys
.
DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC
,
HdfsClientConfigKeys
.
DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC_DEFAULT
)
)
{
DomainPeerServer
domainPeerServer
=
getDomainPeerServer
(
getConf
(
)
,
streamingAddr
.
getPort
(
)
)
;
if
(
domainPeerServer
!=
null
)
{
this
.
localDataXceiverServer
=
new
Daemon
(
threadGroup
,
new
DataXceiverServer
(
domainPeerServer
,
getConf
(
)
,
this
)
)
;
synchronized
void
checkDatanodeUuid
(
)
throws
IOException
{
if
(
storage
.
getDatanodeUuid
(
)
==
null
)
{
storage
.
setDatanodeUuid
(
generateUuid
(
)
)
;
storage
.
writeAll
(
)
;
public
static
InterDatanodeProtocol
createInterDataNodeProtocolProxy
(
DatanodeID
datanodeid
,
final
Configuration
conf
,
final
int
socketTimeout
,
final
boolean
connectToDnViaHostname
)
throws
IOException
{
final
String
dnAddr
=
datanodeid
.
getIpcAddr
(
connectToDnViaHostname
)
;
final
InetSocketAddress
addr
=
NetUtils
.
createSocketAddr
(
dnAddr
)
;
private
void
checkBlockToken
(
ExtendedBlock
block
,
Token
<
BlockTokenIdentifier
>
token
,
AccessMode
accessMode
)
throws
IOException
{
if
(
isBlockTokenEnabled
)
{
BlockTokenIdentifier
id
=
new
BlockTokenIdentifier
(
)
;
ByteArrayInputStream
buf
=
new
ByteArrayInputStream
(
token
.
getIdentifier
(
)
)
;
DataInputStream
in
=
new
DataInputStream
(
buf
)
;
id
.
readFields
(
in
)
;
public
void
shutdown
(
)
{
stopMetricsLogger
(
)
;
if
(
plugins
!=
null
)
{
for
(
ServicePlugin
p
:
plugins
)
{
try
{
p
.
stop
(
)
;
boolean
lengthTooShort
=
false
;
try
{
data
.
checkBlock
(
block
,
block
.
getNumBytes
(
)
,
ReplicaState
.
FINALIZED
)
;
}
catch
(
ReplicaNotFoundException
e
)
{
replicaNotExist
=
true
;
}
catch
(
UnexpectedReplicaStateException
e
)
{
replicaStateNotFinalized
=
true
;
}
catch
(
FileNotFoundException
e
)
{
blockFileNotExist
=
true
;
}
catch
(
EOFException
e
)
{
lengthTooShort
=
true
;
}
catch
(
IOException
e
)
{
blockFileNotExist
=
true
;
}
if
(
replicaNotExist
||
replicaStateNotFinalized
)
{
String
errStr
=
+
block
;
}
catch
(
EOFException
e
)
{
lengthTooShort
=
true
;
}
catch
(
IOException
e
)
{
blockFileNotExist
=
true
;
}
if
(
replicaNotExist
||
replicaStateNotFinalized
)
{
String
errStr
=
+
block
;
LOG
.
info
(
errStr
)
;
bpos
.
trySendErrorReport
(
DatanodeProtocol
.
INVALID_BLOCK
,
errStr
)
;
return
;
}
if
(
blockFileNotExist
)
{
reportBadBlock
(
bpos
,
block
,
+
block
+
)
;
return
;
}
if
(
lengthTooShort
)
{
reportBadBlock
(
bpos
,
block
,
+
block
+
+
data
.
getLength
(
block
)
+
+
block
.
getNumBytes
(
)
)
;
return
;
@
Override
public
void
deleteBlockPool
(
String
blockPoolId
,
boolean
force
)
throws
IOException
{
checkSuperuserPrivilege
(
)
;
@
Override
public
synchronized
void
shutdownDatanode
(
boolean
forUpgrade
)
throws
IOException
{
checkSuperuserPrivilege
(
)
;
public
void
enableTrash
(
String
bpid
)
{
if
(
trashEnabledBpids
.
add
(
bpid
)
)
{
getBPStorage
(
bpid
)
.
stopTrashCleaner
(
)
;
public
void
clearTrash
(
String
bpid
)
{
if
(
trashEnabledBpids
.
contains
(
bpid
)
)
{
getBPStorage
(
bpid
)
.
clearTrash
(
)
;
trashEnabledBpids
.
remove
(
bpid
)
;
final
List
<
UpgradeTask
>
tasks
=
Lists
.
newArrayList
(
)
;
for
(
StorageLocation
dataDir
:
dataDirs
)
{
if
(
!
containsStorageDir
(
dataDir
)
)
{
try
{
final
List
<
Callable
<
StorageDirectory
>>
callables
=
Lists
.
newArrayList
(
)
;
final
StorageDirectory
sd
=
loadStorageDirectory
(
datanode
,
nsInfo
,
dataDir
,
startOpt
,
callables
)
;
if
(
callables
.
isEmpty
(
)
)
{
addStorageDir
(
sd
)
;
success
.
add
(
dataDir
)
;
}
else
{
for
(
Callable
<
StorageDirectory
>
c
:
callables
)
{
tasks
.
add
(
new
UpgradeTask
(
dataDir
,
executor
.
submit
(
c
)
)
)
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
dataDir
,
e
)
;
}
}
else
{
try
{
final
List
<
Callable
<
StorageDirectory
>>
callables
=
Lists
.
newArrayList
(
)
;
final
StorageDirectory
sd
=
loadStorageDirectory
(
datanode
,
nsInfo
,
dataDir
,
startOpt
,
callables
)
;
if
(
callables
.
isEmpty
(
)
)
{
addStorageDir
(
sd
)
;
success
.
add
(
dataDir
)
;
}
else
{
for
(
Callable
<
StorageDirectory
>
c
:
callables
)
{
tasks
.
add
(
new
UpgradeTask
(
dataDir
,
executor
.
submit
(
c
)
)
)
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
dataDir
,
e
)
;
}
}
else
{
LOG
.
info
(
,
dataDir
)
;
success
.
add
(
dataDir
)
;
}
}
if
(
!
tasks
.
isEmpty
(
)
)
{
dataDir
.
makeBlockPoolDir
(
bpid
,
null
)
;
try
{
final
List
<
Callable
<
StorageDirectory
>>
sdCallables
=
Lists
.
newArrayList
(
)
;
final
List
<
StorageDirectory
>
dirs
=
bpStorage
.
recoverTransitionRead
(
nsInfo
,
dataDir
,
startOpt
,
sdCallables
,
datanode
.
getConf
(
)
)
;
if
(
sdCallables
.
isEmpty
(
)
)
{
for
(
StorageDirectory
sd
:
dirs
)
{
success
.
add
(
sd
)
;
}
}
else
{
upgradeCallableMap
.
put
(
dataDir
,
sdCallables
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
dataDir
,
bpid
,
e
)
;
}
}
for
(
Map
.
Entry
<
StorageLocation
,
List
<
Callable
<
StorageDirectory
>>>
entry
:
upgradeCallableMap
.
entrySet
(
)
)
{
for
(
Callable
<
StorageDirectory
>
c
:
entry
.
getValue
(
)
)
{
tasks
.
add
(
new
UpgradeTask
(
entry
.
getKey
(
)
,
executor
.
submit
(
c
)
)
)
;
}
}
if
(
!
tasks
.
isEmpty
(
)
)
{
private
void
doUpgrade
(
final
StorageDirectory
sd
,
final
NamespaceInfo
nsInfo
,
final
File
prevDir
,
final
File
tmpDir
,
final
File
bbwDir
,
final
File
toDir
,
final
int
oldLV
,
Configuration
conf
)
throws
IOException
{
linkAllBlocks
(
tmpDir
,
bbwDir
,
toDir
,
oldLV
,
conf
)
;
clusterID
=
nsInfo
.
getClusterID
(
)
;
upgradeProperties
(
sd
,
conf
)
;
rename
(
tmpDir
,
prevDir
)
;
void
upgradeProperties
(
StorageDirectory
sd
,
Configuration
conf
)
throws
IOException
{
createStorageID
(
sd
,
layoutVersion
,
conf
)
;
private
static
void
linkBlocks
(
File
from
,
File
to
,
int
oldLV
,
HardLink
hl
,
Configuration
conf
)
throws
IOException
{
datanode
.
shortCircuitRegistry
.
registerSlot
(
ExtendedBlockId
.
fromExtendedBlock
(
blk
)
,
slotId
,
isCached
)
;
registeredSlotId
=
slotId
;
}
fis
=
datanode
.
requestShortCircuitFdsForRead
(
blk
,
token
,
maxVersion
)
;
Preconditions
.
checkState
(
fis
!=
null
)
;
bld
.
setStatus
(
SUCCESS
)
;
bld
.
setShortCircuitAccessVersion
(
DataNode
.
CURRENT_BLOCK_FORMAT_VERSION
)
;
}
catch
(
ShortCircuitFdsVersionException
e
)
{
bld
.
setStatus
(
ERROR_UNSUPPORTED
)
;
bld
.
setShortCircuitAccessVersion
(
DataNode
.
CURRENT_BLOCK_FORMAT_VERSION
)
;
bld
.
setMessage
(
e
.
getMessage
(
)
)
;
}
catch
(
ShortCircuitFdsUnsupportedException
e
)
{
bld
.
setStatus
(
ERROR_UNSUPPORTED
)
;
bld
.
setMessage
(
e
.
getMessage
(
)
)
;
}
catch
(
IOException
e
)
{
bld
.
setStatus
(
ERROR
)
;
@
Override
public
void
readBlock
(
final
ExtendedBlock
block
,
final
Token
<
BlockTokenIdentifier
>
blockToken
,
final
String
clientName
,
final
long
blockOffset
,
final
long
length
,
final
boolean
sendChecksum
,
final
CachingStrategy
cachingStrategy
)
throws
IOException
{
previousOpClientName
=
clientName
;
long
read
=
0
;
updateCurrentThreadName
(
+
block
)
;
OutputStream
baseStream
=
getOutputStream
(
)
;
DataOutputStream
out
=
getBufferedOutputStream
(
)
;
checkAccess
(
out
,
true
,
block
,
blockToken
,
Op
.
READ_BLOCK
,
BlockTokenIdentifier
.
AccessMode
.
READ
)
;
BlockSender
blockSender
=
null
;
DatanodeRegistration
dnR
=
datanode
.
getDNRegistrationForBP
(
block
.
getBlockPoolId
(
)
)
;
final
String
clientTraceFmt
=
clientName
.
length
(
)
>
0
&&
ClientTraceLog
.
isInfoEnabled
(
)
?
String
.
format
(
DN_CLIENTTRACE_FORMAT
,
localAddress
,
remoteAddress
,
,
,
clientName
,
,
dnR
.
getDatanodeUuid
(
)
,
block
,
)
:
dnR
+
+
block
+
+
remoteAddress
;
try
{
try
{
blockSender
=
new
BlockSender
(
block
,
blockOffset
,
length
,
true
,
false
,
sendChecksum
,
datanode
,
clientTraceFmt
,
cachingStrategy
)
;
}
catch
(
IOException
e
)
{
String
msg
=
+
block
+
+
e
;
if
(
targetStorageTypes
.
length
>
0
)
{
System
.
arraycopy
(
targetStorageTypes
,
0
,
storageTypes
,
1
,
nst
)
;
}
final
int
nsi
=
targetStorageIds
.
length
;
final
String
[
]
storageIds
;
if
(
nsi
>
0
)
{
storageIds
=
new
String
[
nsi
+
1
]
;
storageIds
[
0
]
=
storageId
;
if
(
targetStorageTypes
.
length
>
0
)
{
System
.
arraycopy
(
targetStorageIds
,
0
,
storageIds
,
1
,
nsi
)
;
}
}
else
{
storageIds
=
new
String
[
0
]
;
}
checkAccess
(
replyOut
,
isClient
,
block
,
blockToken
,
Op
.
WRITE_BLOCK
,
BlockTokenIdentifier
.
AccessMode
.
WRITE
,
storageTypes
,
storageIds
)
;
if
(
isTransfer
&&
targets
.
length
>
0
)
{
throw
new
IOException
(
stage
+
+
Arrays
.
asList
(
targets
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
System
.
arraycopy
(
targetStorageTypes
,
0
,
storageTypes
,
1
,
nst
)
;
}
final
int
nsi
=
targetStorageIds
.
length
;
final
String
[
]
storageIds
;
if
(
nsi
>
0
)
{
storageIds
=
new
String
[
nsi
+
1
]
;
storageIds
[
0
]
=
storageId
;
if
(
targetStorageTypes
.
length
>
0
)
{
System
.
arraycopy
(
targetStorageIds
,
0
,
storageIds
,
1
,
nsi
)
;
}
}
else
{
storageIds
=
new
String
[
0
]
;
}
checkAccess
(
replyOut
,
isClient
,
block
,
blockToken
,
Op
.
WRITE_BLOCK
,
BlockTokenIdentifier
.
AccessMode
.
WRITE
,
storageTypes
,
storageIds
)
;
if
(
isTransfer
&&
targets
.
length
>
0
)
{
throw
new
IOException
(
stage
+
+
Arrays
.
asList
(
targets
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
stage
,
clientname
,
block
,
latestGenerationStamp
,
minBytesRcvd
,
maxBytesRcvd
,
Arrays
.
asList
(
targets
)
,
pipelineSize
,
srcDataNode
,
pinning
)
;
storageIds
[
0
]
=
storageId
;
if
(
targetStorageTypes
.
length
>
0
)
{
System
.
arraycopy
(
targetStorageIds
,
0
,
storageIds
,
1
,
nsi
)
;
}
}
else
{
storageIds
=
new
String
[
0
]
;
}
checkAccess
(
replyOut
,
isClient
,
block
,
blockToken
,
Op
.
WRITE_BLOCK
,
BlockTokenIdentifier
.
AccessMode
.
WRITE
,
storageTypes
,
storageIds
)
;
if
(
isTransfer
&&
targets
.
length
>
0
)
{
throw
new
IOException
(
stage
+
+
Arrays
.
asList
(
targets
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
stage
,
clientname
,
block
,
latestGenerationStamp
,
minBytesRcvd
,
maxBytesRcvd
,
Arrays
.
asList
(
targets
)
,
pipelineSize
,
srcDataNode
,
pinning
)
;
LOG
.
debug
(
,
isDatanode
,
isClient
,
isTransfer
)
;
LOG
.
debug
(
,
peer
.
getReceiveBufferSize
(
)
,
peer
.
getTcpNoDelay
(
)
)
;
}
final
ExtendedBlock
originalBlock
=
new
ExtendedBlock
(
block
)
;
if
(
block
.
getNumBytes
(
)
==
0
)
{
block
.
setNumBytes
(
dataXceiverServer
.
estimateBlockSize
)
;
}
LOG
.
info
(
,
block
,
remoteAddress
,
localAddress
)
;
DataOutputStream
mirrorOut
=
null
;
DataInputStream
mirrorIn
=
null
;
Socket
mirrorSock
=
null
;
String
mirrorNode
=
null
;
String
firstBadLink
=
;
Status
mirrorInStatus
=
SUCCESS
;
final
String
storageUuid
;
final
boolean
isOnTransientStorage
;
try
{
final
Replica
replica
;
if
(
isDatanode
||
stage
!=
BlockConstructionStage
.
PIPELINE_CLOSE_RECOVERY
)
{
setCurrentBlockReceiver
(
getBlockReceiver
(
block
,
storageType
,
in
,
peer
.
getRemoteAddressString
(
)
,
peer
.
getLocalAddressString
(
)
,
stage
,
latestGenerationStamp
,
minBytesRcvd
,
maxBytesRcvd
,
clientname
,
srcDataNode
,
datanode
,
requestedChecksum
,
cachingStrategy
,
allowLazyPersist
,
pinning
,
storageId
)
)
;
replica
=
blockReceiver
.
getReplica
(
)
;
}
else
{
DataEncryptionKeyFactory
keyFactory
=
datanode
.
getDataEncryptionKeyFactoryForBlock
(
block
)
;
SecretKey
secretKey
=
null
;
if
(
dnConf
.
overwriteDownstreamDerivedQOP
)
{
String
bpid
=
block
.
getBlockPoolId
(
)
;
BlockKey
blockKey
=
datanode
.
blockPoolTokenSecretManager
.
get
(
bpid
)
.
getCurrentKey
(
)
;
secretKey
=
blockKey
.
getKey
(
)
;
}
IOStreamPair
saslStreams
=
datanode
.
saslClient
.
socketSend
(
mirrorSock
,
unbufMirrorOut
,
unbufMirrorIn
,
keyFactory
,
blockToken
,
targets
[
0
]
,
secretKey
)
;
unbufMirrorOut
=
saslStreams
.
out
;
unbufMirrorIn
=
saslStreams
.
in
;
mirrorOut
=
new
DataOutputStream
(
new
BufferedOutputStream
(
unbufMirrorOut
,
smallBufferSize
)
)
;
mirrorIn
=
new
DataInputStream
(
unbufMirrorIn
)
;
String
targetStorageId
=
null
;
if
(
targetStorageIds
.
length
>
0
)
{
targetStorageId
=
targetStorageIds
[
0
]
;
}
if
(
targetPinnings
!=
null
&&
targetPinnings
.
length
>
0
)
{
}
if
(
targetPinnings
!=
null
&&
targetPinnings
.
length
>
0
)
{
new
Sender
(
mirrorOut
)
.
writeBlock
(
originalBlock
,
targetStorageTypes
[
0
]
,
blockToken
,
clientname
,
targets
,
targetStorageTypes
,
srcDataNode
,
stage
,
pipelineSize
,
minBytesRcvd
,
maxBytesRcvd
,
latestGenerationStamp
,
requestedChecksum
,
cachingStrategy
,
allowLazyPersist
,
targetPinnings
[
0
]
,
targetPinnings
,
targetStorageId
,
targetStorageIds
)
;
}
else
{
new
Sender
(
mirrorOut
)
.
writeBlock
(
originalBlock
,
targetStorageTypes
[
0
]
,
blockToken
,
clientname
,
targets
,
targetStorageTypes
,
srcDataNode
,
stage
,
pipelineSize
,
minBytesRcvd
,
maxBytesRcvd
,
latestGenerationStamp
,
requestedChecksum
,
cachingStrategy
,
allowLazyPersist
,
false
,
targetPinnings
,
targetStorageId
,
targetStorageIds
)
;
}
mirrorOut
.
flush
(
)
;
DataNodeFaultInjector
.
get
(
)
.
writeBlockAfterFlush
(
)
;
if
(
isClient
)
{
BlockOpResponseProto
connectAck
=
BlockOpResponseProto
.
parseFrom
(
PBHelperClient
.
vintPrefixed
(
mirrorIn
)
)
;
mirrorInStatus
=
connectAck
.
getStatus
(
)
;
firstBadLink
=
connectAck
.
getFirstBadLink
(
)
;
if
(
mirrorInStatus
!=
SUCCESS
)
{
LOG
.
debug
(
+
,
targets
.
length
,
firstBadLink
)
;
}
}
}
catch
(
IOException
e
)
{
if
(
isClient
)
{
BlockOpResponseProto
.
newBuilder
(
)
.
setStatus
(
ERROR
)
.
setFirstBadLink
(
targets
[
0
]
.
getXferAddr
(
)
)
.
build
(
)
.
writeDelimitedTo
(
replyOut
)
;
else
{
new
Sender
(
mirrorOut
)
.
writeBlock
(
originalBlock
,
targetStorageTypes
[
0
]
,
blockToken
,
clientname
,
targets
,
targetStorageTypes
,
srcDataNode
,
stage
,
pipelineSize
,
minBytesRcvd
,
maxBytesRcvd
,
latestGenerationStamp
,
requestedChecksum
,
cachingStrategy
,
allowLazyPersist
,
false
,
targetPinnings
,
targetStorageId
,
targetStorageIds
)
;
}
mirrorOut
.
flush
(
)
;
DataNodeFaultInjector
.
get
(
)
.
writeBlockAfterFlush
(
)
;
if
(
isClient
)
{
BlockOpResponseProto
connectAck
=
BlockOpResponseProto
.
parseFrom
(
PBHelperClient
.
vintPrefixed
(
mirrorIn
)
)
;
mirrorInStatus
=
connectAck
.
getStatus
(
)
;
firstBadLink
=
connectAck
.
getFirstBadLink
(
)
;
if
(
mirrorInStatus
!=
SUCCESS
)
{
LOG
.
debug
(
+
,
targets
.
length
,
firstBadLink
)
;
}
}
}
catch
(
IOException
e
)
{
if
(
isClient
)
{
BlockOpResponseProto
.
newBuilder
(
)
.
setStatus
(
ERROR
)
.
setFirstBadLink
(
targets
[
0
]
.
getXferAddr
(
)
)
.
build
(
)
.
writeDelimitedTo
(
replyOut
)
;
replyOut
.
flush
(
)
;
}
IOUtils
.
closeStream
(
mirrorOut
)
;
catch
(
IOException
e
)
{
if
(
isClient
)
{
BlockOpResponseProto
.
newBuilder
(
)
.
setStatus
(
ERROR
)
.
setFirstBadLink
(
targets
[
0
]
.
getXferAddr
(
)
)
.
build
(
)
.
writeDelimitedTo
(
replyOut
)
;
replyOut
.
flush
(
)
;
}
IOUtils
.
closeStream
(
mirrorOut
)
;
mirrorOut
=
null
;
IOUtils
.
closeStream
(
mirrorIn
)
;
mirrorIn
=
null
;
IOUtils
.
closeSocket
(
mirrorSock
)
;
mirrorSock
=
null
;
if
(
isClient
)
{
LOG
.
error
(
,
datanode
,
block
,
mirrorNode
,
e
)
;
throw
e
;
}
else
{
LOG
.
info
(
+
,
datanode
,
block
,
mirrorNode
,
e
)
;
}
IOUtils
.
closeStream
(
mirrorOut
)
;
mirrorOut
=
null
;
IOUtils
.
closeStream
(
mirrorIn
)
;
mirrorIn
=
null
;
IOUtils
.
closeSocket
(
mirrorSock
)
;
mirrorSock
=
null
;
if
(
isClient
)
{
LOG
.
error
(
,
datanode
,
block
,
mirrorNode
,
e
)
;
throw
e
;
}
else
{
LOG
.
info
(
+
,
datanode
,
block
,
mirrorNode
,
e
)
;
incrDatanodeNetworkErrors
(
)
;
}
}
}
if
(
isClient
&&
!
isTransfer
)
{
if
(
mirrorInStatus
!=
SUCCESS
)
{
LOG
.
debug
(
+
,
targets
.
length
,
firstBadLink
)
;
@
Override
public
void
copyBlock
(
final
ExtendedBlock
block
,
final
Token
<
BlockTokenIdentifier
>
blockToken
)
throws
IOException
{
updateCurrentThreadName
(
+
block
)
;
DataOutputStream
reply
=
getBufferedOutputStream
(
)
;
checkAccess
(
reply
,
true
,
block
,
blockToken
,
Op
.
COPY_BLOCK
,
BlockTokenIdentifier
.
AccessMode
.
COPY
)
;
if
(
datanode
.
data
.
getPinning
(
block
)
)
{
String
msg
=
+
block
.
getBlockId
(
)
+
+
+
peer
.
getRemoteAddressString
(
)
+
;
String
msg
=
+
block
.
getBlockId
(
)
+
+
+
peer
.
getRemoteAddressString
(
)
+
+
;
LOG
.
info
(
msg
)
;
sendResponse
(
ERROR
,
msg
)
;
return
;
}
BlockSender
blockSender
=
null
;
boolean
isOpSuccess
=
true
;
try
{
blockSender
=
new
BlockSender
(
block
,
0
,
-
1
,
false
,
false
,
true
,
datanode
,
null
,
CachingStrategy
.
newDropBehind
(
)
)
;
OutputStream
baseStream
=
getOutputStream
(
)
;
writeSuccessWithChecksumInfo
(
blockSender
,
reply
)
;
long
beginRead
=
Time
.
monotonicNow
(
)
;
long
read
=
blockSender
.
sendBlock
(
reply
,
baseStream
,
dataXceiverServer
.
balanceThrottler
)
;
long
duration
=
Time
.
monotonicNow
(
)
-
beginRead
;
datanode
.
metrics
.
incrBytesRead
(
(
int
)
read
)
;
datanode
.
metrics
.
incrBlocksRead
(
)
;
return
;
}
BlockSender
blockSender
=
null
;
boolean
isOpSuccess
=
true
;
try
{
blockSender
=
new
BlockSender
(
block
,
0
,
-
1
,
false
,
false
,
true
,
datanode
,
null
,
CachingStrategy
.
newDropBehind
(
)
)
;
OutputStream
baseStream
=
getOutputStream
(
)
;
writeSuccessWithChecksumInfo
(
blockSender
,
reply
)
;
long
beginRead
=
Time
.
monotonicNow
(
)
;
long
read
=
blockSender
.
sendBlock
(
reply
,
baseStream
,
dataXceiverServer
.
balanceThrottler
)
;
long
duration
=
Time
.
monotonicNow
(
)
-
beginRead
;
datanode
.
metrics
.
incrBytesRead
(
(
int
)
read
)
;
datanode
.
metrics
.
incrBlocksRead
(
)
;
datanode
.
metrics
.
incrTotalReadTime
(
duration
)
;
LOG
.
info
(
,
block
,
peer
.
getRemoteAddressString
(
)
)
;
}
catch
(
IOException
ioe
)
{
if
(
!
dataXceiverServer
.
balanceThrottler
.
acquire
(
)
)
{
String
msg
=
+
block
.
getBlockId
(
)
+
+
peer
.
getRemoteAddressString
(
)
+
+
;
LOG
.
warn
(
msg
)
;
sendResponse
(
ERROR
,
msg
)
;
return
;
}
Socket
proxySock
=
null
;
DataOutputStream
proxyOut
=
null
;
Status
opStatus
=
SUCCESS
;
String
errMsg
=
null
;
DataInputStream
proxyReply
=
null
;
boolean
IoeDuringCopyBlockOperation
=
false
;
try
{
if
(
proxySource
.
equals
(
datanode
.
getDatanodeId
(
)
)
)
{
ReplicaInfo
oldReplica
=
datanode
.
data
.
moveBlockAcrossStorage
(
block
,
storageType
,
storageId
)
;
if
(
oldReplica
!=
null
)
{
return
;
}
Socket
proxySock
=
null
;
DataOutputStream
proxyOut
=
null
;
Status
opStatus
=
SUCCESS
;
String
errMsg
=
null
;
DataInputStream
proxyReply
=
null
;
boolean
IoeDuringCopyBlockOperation
=
false
;
try
{
if
(
proxySource
.
equals
(
datanode
.
getDatanodeId
(
)
)
)
{
ReplicaInfo
oldReplica
=
datanode
.
data
.
moveBlockAcrossStorage
(
block
,
storageType
,
storageId
)
;
if
(
oldReplica
!=
null
)
{
LOG
.
info
(
,
block
,
oldReplica
.
getVolume
(
)
.
getStorageType
(
)
,
storageType
)
;
}
}
else
{
block
.
setNumBytes
(
dataXceiverServer
.
estimateBlockSize
)
;
final
String
dnAddr
=
proxySource
.
getXferAddr
(
connectToDnViaHostname
)
;
DataEncryptionKeyFactory
keyFactory
=
datanode
.
getDataEncryptionKeyFactoryForBlock
(
block
)
;
IOStreamPair
saslStreams
=
datanode
.
saslClient
.
socketSend
(
proxySock
,
unbufProxyOut
,
unbufProxyIn
,
keyFactory
,
blockToken
,
proxySource
)
;
unbufProxyOut
=
saslStreams
.
out
;
unbufProxyIn
=
saslStreams
.
in
;
proxyOut
=
new
DataOutputStream
(
new
BufferedOutputStream
(
unbufProxyOut
,
smallBufferSize
)
)
;
proxyReply
=
new
DataInputStream
(
new
BufferedInputStream
(
unbufProxyIn
,
ioFileBufferSize
)
)
;
IoeDuringCopyBlockOperation
=
true
;
new
Sender
(
proxyOut
)
.
copyBlock
(
block
,
blockToken
)
;
IoeDuringCopyBlockOperation
=
false
;
BlockOpResponseProto
copyResponse
=
BlockOpResponseProto
.
parseFrom
(
PBHelperClient
.
vintPrefixed
(
proxyReply
)
)
;
String
logInfo
=
+
block
+
+
proxySock
.
getRemoteSocketAddress
(
)
;
DataTransferProtoUtil
.
checkBlockOpStatus
(
copyResponse
,
logInfo
,
true
)
;
ReadOpChecksumInfoProto
checksumInfo
=
copyResponse
.
getReadOpChecksumInfo
(
)
;
DataChecksum
remoteChecksum
=
DataTransferProtoUtil
.
fromProto
(
checksumInfo
.
getChecksum
(
)
)
;
setCurrentBlockReceiver
(
getBlockReceiver
(
block
,
storageType
,
proxyReply
,
proxySock
.
getRemoteSocketAddress
(
)
.
toString
(
)
,
proxySock
.
getLocalSocketAddress
(
)
.
toString
(
)
,
null
,
0
,
0
,
0
,
,
null
,
datanode
,
remoteChecksum
,
CachingStrategy
.
newDropBehind
(
)
,
false
,
false
,
storageId
)
)
;
private
void
checkAccess
(
OutputStream
out
,
final
boolean
reply
,
final
ExtendedBlock
blk
,
final
Token
<
BlockTokenIdentifier
>
t
,
final
Op
op
,
final
BlockTokenIdentifier
.
AccessMode
mode
,
final
StorageType
[
]
storageTypes
,
final
String
[
]
storageIds
)
throws
IOException
{
checkAndWaitForBP
(
blk
)
;
if
(
datanode
.
isBlockTokenEnabled
)
{
try
{
peer
=
peerServer
.
accept
(
)
;
int
curXceiverCount
=
datanode
.
getXceiverCount
(
)
;
if
(
curXceiverCount
>
maxXceiverCount
)
{
throw
new
IOException
(
+
curXceiverCount
+
+
maxXceiverCount
)
;
}
new
Daemon
(
datanode
.
threadGroup
,
DataXceiver
.
create
(
peer
,
datanode
,
this
)
)
.
start
(
)
;
}
catch
(
SocketTimeoutException
ignored
)
{
}
catch
(
AsynchronousCloseException
ace
)
{
if
(
datanode
.
shouldRun
&&
!
datanode
.
shutdownForUpgrade
)
{
LOG
.
warn
(
,
datanode
.
getDisplayName
(
)
,
ace
)
;
}
}
catch
(
IOException
ie
)
{
IOUtils
.
closeQuietly
(
peer
)
;
LOG
.
warn
(
,
datanode
.
getDisplayName
(
)
,
ie
)
;
}
catch
(
OutOfMemoryError
ie
)
{
IOUtils
.
closeQuietly
(
peer
)
;
}
new
Daemon
(
datanode
.
threadGroup
,
DataXceiver
.
create
(
peer
,
datanode
,
this
)
)
.
start
(
)
;
}
catch
(
SocketTimeoutException
ignored
)
{
}
catch
(
AsynchronousCloseException
ace
)
{
if
(
datanode
.
shouldRun
&&
!
datanode
.
shutdownForUpgrade
)
{
LOG
.
warn
(
,
datanode
.
getDisplayName
(
)
,
ace
)
;
}
}
catch
(
IOException
ie
)
{
IOUtils
.
closeQuietly
(
peer
)
;
LOG
.
warn
(
,
datanode
.
getDisplayName
(
)
,
ie
)
;
}
catch
(
OutOfMemoryError
ie
)
{
IOUtils
.
closeQuietly
(
peer
)
;
LOG
.
error
(
,
ie
)
;
try
{
Thread
.
sleep
(
TimeUnit
.
SECONDS
.
toMillis
(
30L
)
)
;
}
catch
(
InterruptedException
e
)
{
}
}
catch
(
Throwable
te
)
{
void
start
(
)
{
shouldRun
.
set
(
true
)
;
long
firstScanTime
=
ThreadLocalRandom
.
current
(
)
.
nextLong
(
scanPeriodMsecs
)
;
if
(
!
shouldRun
.
getAndSet
(
false
)
)
{
LOG
.
warn
(
)
;
}
if
(
masterThread
!=
null
)
{
masterThread
.
shutdown
(
)
;
}
if
(
reportCompileThreadPool
!=
null
)
{
reportCompileThreadPool
.
shutdownNow
(
)
;
}
if
(
masterThread
!=
null
)
{
try
{
masterThread
.
awaitTermination
(
1
,
TimeUnit
.
MINUTES
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
error
(
+
,
e
)
;
}
}
if
(
reportCompileThreadPool
!=
null
)
{
try
{
reportCompileThreadPool
.
awaitTermination
(
1
,
TimeUnit
.
MINUTES
)
;
}
catch
(
InterruptedException
e
)
{
}
else
if
(
memBlock
.
compareWith
(
info
)
!=
0
)
{
statsRecord
.
duplicateBlocks
++
;
addDifference
(
diffRecord
,
statsRecord
,
info
)
;
}
d
++
;
if
(
d
<
blockpoolReport
.
size
(
)
)
{
ScanInfo
nextInfo
=
blockpoolReport
.
get
(
d
)
;
if
(
nextInfo
.
getBlockId
(
)
!=
info
.
getBlockId
(
)
)
{
++
m
;
}
}
else
{
++
m
;
}
}
while
(
m
<
bl
.
size
(
)
)
{
ReplicaInfo
current
=
bl
.
get
(
m
++
)
;
addDifference
(
diffRecord
,
statsRecord
,
current
.
getBlockId
(
)
,
current
.
getVolume
(
)
)
;
}
while
(
d
<
blockpoolReport
.
size
(
)
)
{
if
(
!
dataset
.
isDeletingBlock
(
bpid
,
blockpoolReport
.
get
(
d
)
.
getBlockId
(
)
)
)
{
try
(
FsDatasetSpi
.
FsVolumeReferences
volumes
=
dataset
.
getFsVolumeReferences
(
)
)
{
for
(
final
FsVolumeSpi
volume
:
volumes
)
{
if
(
volume
.
getStorageType
(
)
!=
StorageType
.
PROVIDED
)
{
ReportCompiler
reportCompiler
=
new
ReportCompiler
(
volume
)
;
Future
<
ScanInfoVolumeReport
>
result
=
reportCompileThreadPool
.
submit
(
reportCompiler
)
;
compilersInProgress
.
add
(
result
)
;
}
}
for
(
Future
<
ScanInfoVolumeReport
>
future
:
compilersInProgress
)
{
try
{
final
ScanInfoVolumeReport
result
=
future
.
get
(
)
;
if
(
!
CollectionUtils
.
addIgnoreNull
(
volReports
,
result
)
)
{
volReports
.
clear
(
)
;
break
;
}
}
catch
(
Exception
ex
)
{
LOG
.
warn
(
,
ex
)
;
}
}
}
catch
(
IOException
e
)
{
public
void
cancelPlan
(
String
planID
)
throws
DiskBalancerException
{
lock
.
lock
(
)
;
boolean
needShutdown
=
false
;
try
{
checkDiskBalancerEnabled
(
)
;
if
(
this
.
planID
==
null
||
!
this
.
planID
.
equals
(
planID
)
||
this
.
planID
.
isEmpty
(
)
)
{
private
void
createWorkPlan
(
NodePlan
plan
)
throws
DiskBalancerException
{
Preconditions
.
checkState
(
lock
.
isHeldByCurrentThread
(
)
)
;
workMap
.
clear
(
)
;
Map
<
String
,
String
>
storageIDToVolBasePathMap
=
getStorageIDToVolumeBasePathMap
(
)
;
for
(
Step
step
:
plan
.
getVolumeSetPlans
(
)
)
{
String
sourceVolUuid
=
step
.
getSourceVolume
(
)
.
getUuid
(
)
;
String
destVolUuid
=
step
.
getDestinationVolume
(
)
.
getUuid
(
)
;
String
sourceVolBasePath
=
storageIDToVolBasePathMap
.
get
(
sourceVolUuid
)
;
if
(
sourceVolBasePath
==
null
)
{
final
String
errMsg
=
+
step
.
getSourceVolume
(
)
.
getPath
(
)
+
;
private
void
createWorkPlan
(
NodePlan
plan
)
throws
DiskBalancerException
{
Preconditions
.
checkState
(
lock
.
isHeldByCurrentThread
(
)
)
;
workMap
.
clear
(
)
;
Map
<
String
,
String
>
storageIDToVolBasePathMap
=
getStorageIDToVolumeBasePathMap
(
)
;
for
(
Step
step
:
plan
.
getVolumeSetPlans
(
)
)
{
String
sourceVolUuid
=
step
.
getSourceVolume
(
)
.
getUuid
(
)
;
String
destVolUuid
=
step
.
getDestinationVolume
(
)
.
getUuid
(
)
;
String
sourceVolBasePath
=
storageIDToVolBasePathMap
.
get
(
sourceVolUuid
)
;
if
(
sourceVolBasePath
==
null
)
{
final
String
errMsg
=
+
step
.
getSourceVolume
(
)
.
getPath
(
)
+
;
LOG
.
error
(
errMsg
)
;
throw
new
DiskBalancerException
(
errMsg
,
DiskBalancerException
.
Result
.
INVALID_VOLUME
)
;
}
String
destVolBasePath
=
storageIDToVolBasePathMap
.
get
(
destVolUuid
)
;
if
(
destVolBasePath
==
null
)
{
final
String
errMsg
=
+
step
.
getDestinationVolume
(
)
.
getPath
(
)
+
;
@
Override
public
void
bumpReplicaGS
(
long
newGS
)
throws
IOException
{
long
oldGS
=
getGenerationStamp
(
)
;
final
File
oldmeta
=
getMetaFile
(
)
;
setGenerationStamp
(
newGS
)
;
final
File
newmeta
=
getMetaFile
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
truncateBlock
(
FsVolumeSpi
volume
,
File
blockFile
,
File
metaFile
,
long
oldlen
,
long
newlen
,
FileIoProvider
fileIoProvider
)
throws
IOException
{
if
(
numBlockIters
==
0
)
{
LOG
.
debug
(
,
this
)
;
return
Long
.
MAX_VALUE
;
}
int
curIdx
;
if
(
curBlockIter
==
null
)
{
curIdx
=
0
;
}
else
{
curIdx
=
blockIters
.
indexOf
(
curBlockIter
)
;
Preconditions
.
checkState
(
curIdx
>=
0
)
;
}
long
nowMs
=
Time
.
now
(
)
;
long
minTimeoutMs
=
Long
.
MAX_VALUE
;
for
(
int
i
=
0
;
i
<
numBlockIters
;
i
++
)
{
int
idx
=
(
curIdx
+
i
+
1
)
%
numBlockIters
;
BlockIterator
iter
=
blockIters
.
get
(
idx
)
;
if
(
!
iter
.
atEnd
(
)
)
{
}
else
{
curIdx
=
blockIters
.
indexOf
(
curBlockIter
)
;
Preconditions
.
checkState
(
curIdx
>=
0
)
;
}
long
nowMs
=
Time
.
now
(
)
;
long
minTimeoutMs
=
Long
.
MAX_VALUE
;
for
(
int
i
=
0
;
i
<
numBlockIters
;
i
++
)
{
int
idx
=
(
curIdx
+
i
+
1
)
%
numBlockIters
;
BlockIterator
iter
=
blockIters
.
get
(
idx
)
;
if
(
!
iter
.
atEnd
(
)
)
{
LOG
.
info
(
,
iter
.
getBlockPoolId
(
)
,
volume
)
;
curBlockIter
=
iter
;
return
0L
;
}
long
iterStartMs
=
iter
.
getIterStartMs
(
)
;
long
waitMs
=
(
iterStartMs
+
conf
.
scanPeriodMs
)
-
nowMs
;
if
(
waitMs
<=
0
)
{
long
nowMs
=
Time
.
now
(
)
;
long
minTimeoutMs
=
Long
.
MAX_VALUE
;
for
(
int
i
=
0
;
i
<
numBlockIters
;
i
++
)
{
int
idx
=
(
curIdx
+
i
+
1
)
%
numBlockIters
;
BlockIterator
iter
=
blockIters
.
get
(
idx
)
;
if
(
!
iter
.
atEnd
(
)
)
{
LOG
.
info
(
,
iter
.
getBlockPoolId
(
)
,
volume
)
;
curBlockIter
=
iter
;
return
0L
;
}
long
iterStartMs
=
iter
.
getIterStartMs
(
)
;
long
waitMs
=
(
iterStartMs
+
conf
.
scanPeriodMs
)
-
nowMs
;
if
(
waitMs
<=
0
)
{
iter
.
rewind
(
)
;
LOG
.
info
(
+
,
iter
.
getBlockPoolId
(
)
,
volume
,
TimeUnit
.
HOURS
.
convert
(
conf
.
scanPeriodMs
,
TimeUnit
.
MILLISECONDS
)
)
;
curBlockIter
=
iter
;
private
long
scanBlock
(
ExtendedBlock
cblock
,
long
bytesPerSec
)
{
ExtendedBlock
block
=
null
;
try
{
Block
b
=
volume
.
getDataset
(
)
.
getStoredBlock
(
cblock
.
getBlockPoolId
(
)
,
cblock
.
getBlockId
(
)
)
;
if
(
b
==
null
)
{
block
=
curBlockIter
.
nextBlock
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
this
,
curBlockIter
)
;
return
null
;
}
if
(
block
==
null
)
{
LOG
.
info
(
,
this
,
curBlockIter
.
getBlockPoolId
(
)
)
;
saveBlockIterator
(
curBlockIter
)
;
return
null
;
}
else
if
(
conf
.
skipRecentAccessed
)
{
try
{
BlockLocalPathInfo
blockLocalPathInfo
=
volume
.
getDataset
(
)
.
getBlockLocalPathInfo
(
block
)
;
BasicFileAttributes
attr
=
Files
.
readAttributes
(
new
File
(
blockLocalPathInfo
.
getBlockPath
(
)
)
.
toPath
(
)
,
BasicFileAttributes
.
class
)
;
if
(
System
.
currentTimeMillis
(
)
-
attr
.
lastAccessTime
(
)
.
to
(
TimeUnit
.
MILLISECONDS
)
<
conf
.
scanPeriodMs
)
{
return
null
;
}
}
catch
(
IOException
ioe
)
{
long
timeout
=
findNextUsableBlockIter
(
)
;
if
(
timeout
>
0
)
{
LOG
.
trace
(
+
,
this
,
timeout
)
;
synchronized
(
stats
)
{
stats
.
nextBlockPoolScanStartMs
=
Time
.
monotonicNow
(
)
+
timeout
;
}
return
timeout
;
}
synchronized
(
stats
)
{
stats
.
scansSinceRestart
++
;
stats
.
blocksScannedInCurrentPeriod
=
0
;
stats
.
nextBlockPoolScanStartMs
=
-
1
;
}
return
0L
;
}
block
=
getNextBlockToScan
(
)
;
if
(
block
==
null
)
{
return
0L
;
}
}
if
(
curBlockIter
!=
null
)
{
while
(
true
)
{
ExtendedBlock
suspectBlock
=
null
;
synchronized
(
this
)
{
if
(
stopping
)
{
break
;
}
if
(
timeout
>
0
)
{
LOG
.
debug
(
,
this
,
timeout
)
;
wait
(
timeout
)
;
if
(
stopping
)
{
break
;
}
}
suspectBlock
=
popNextSuspectBlock
(
)
;
}
timeout
=
runLoop
(
suspectBlock
)
;
}
}
catch
(
InterruptedException
e
)
{
LOG
.
trace
(
,
this
)
;
}
catch
(
Throwable
e
)
{
public
synchronized
void
markSuspectBlock
(
ExtendedBlock
block
)
{
if
(
stopping
)
{
LOG
.
trace
(
+
,
gap
,
minDiskCheckGapMs
)
;
return
Collections
.
emptySet
(
)
;
}
final
FsDatasetSpi
.
FsVolumeReferences
references
=
dataset
.
getFsVolumeReferences
(
)
;
if
(
references
.
size
(
)
==
0
)
{
LOG
.
warn
(
)
;
return
Collections
.
emptySet
(
)
;
}
lastAllVolumesCheck
=
timer
.
monotonicNow
(
)
;
final
Set
<
FsVolumeSpi
>
healthyVolumes
=
new
HashSet
<
>
(
)
;
final
Set
<
FsVolumeSpi
>
failedVolumes
=
new
HashSet
<
>
(
)
;
final
Set
<
FsVolumeSpi
>
allVolumes
=
new
HashSet
<
>
(
)
;
final
AtomicLong
numVolumes
=
new
AtomicLong
(
references
.
size
(
)
)
;
final
CountDownLatch
latch
=
new
CountDownLatch
(
1
)
;
for
(
int
i
=
0
;
i
<
references
.
size
(
)
;
++
i
)
{
final
FsVolumeReference
reference
=
references
.
getReference
(
i
)
;
Optional
<
ListenableFuture
<
VolumeCheckResult
>>
olf
=
delegateChecker
.
schedule
(
reference
.
getVolume
(
)
,
IGNORED_CONTEXT
)
;
private
void
initializeStripedBlkReconstructionThreadPool
(
int
numThreads
)
{
@
Override
public
void
setConf
(
Configuration
conf
)
{
balancedSpaceThreshold
=
conf
.
getLongBytes
(
DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY
,
DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_DEFAULT
)
;
balancedPreferencePercent
=
conf
.
getFloat
(
DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY
,
DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT
)
;
private
V
doChooseVolume
(
final
List
<
V
>
volumes
,
long
replicaSize
,
String
storageId
)
throws
IOException
{
AvailableSpaceVolumeList
volumesWithSpaces
=
new
AvailableSpaceVolumeList
(
volumes
)
;
if
(
volumesWithSpaces
.
areAllVolumesWithinFreeSpaceThreshold
(
)
)
{
V
volume
=
roundRobinPolicyBalanced
.
chooseVolume
(
volumes
,
replicaSize
,
storageId
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
volumesWithSpaces
.
areAllVolumesWithinFreeSpaceThreshold
(
)
)
{
V
volume
=
roundRobinPolicyBalanced
.
chooseVolume
(
volumes
,
replicaSize
,
storageId
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
volume
+
+
replicaSize
)
;
}
return
volume
;
}
else
{
V
volume
=
null
;
long
mostAvailableAmongLowVolumes
=
volumesWithSpaces
.
getMostAvailableSpaceAmongVolumesWithLowAvailableSpace
(
)
;
List
<
V
>
highAvailableVolumes
=
extractVolumesFromPairs
(
volumesWithSpaces
.
getVolumesWithHighAvailableSpace
(
)
)
;
List
<
V
>
lowAvailableVolumes
=
extractVolumesFromPairs
(
volumesWithSpaces
.
getVolumesWithLowAvailableSpace
(
)
)
;
float
preferencePercentScaler
=
(
highAvailableVolumes
.
size
(
)
*
balancedPreferencePercent
)
+
(
lowAvailableVolumes
.
size
(
)
*
(
1
-
balancedPreferencePercent
)
)
;
float
scaledPreferencePercent
=
(
highAvailableVolumes
.
size
(
)
*
balancedPreferencePercent
)
/
preferencePercentScaler
;
if
(
mostAvailableAmongLowVolumes
<
replicaSize
||
random
.
nextFloat
(
)
<
scaledPreferencePercent
)
{
volume
=
roundRobinPolicyHighAvailable
.
chooseVolume
(
highAvailableVolumes
,
replicaSize
,
storageId
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
return
volume
;
}
else
{
V
volume
=
null
;
long
mostAvailableAmongLowVolumes
=
volumesWithSpaces
.
getMostAvailableSpaceAmongVolumesWithLowAvailableSpace
(
)
;
List
<
V
>
highAvailableVolumes
=
extractVolumesFromPairs
(
volumesWithSpaces
.
getVolumesWithHighAvailableSpace
(
)
)
;
List
<
V
>
lowAvailableVolumes
=
extractVolumesFromPairs
(
volumesWithSpaces
.
getVolumesWithLowAvailableSpace
(
)
)
;
float
preferencePercentScaler
=
(
highAvailableVolumes
.
size
(
)
*
balancedPreferencePercent
)
+
(
lowAvailableVolumes
.
size
(
)
*
(
1
-
balancedPreferencePercent
)
)
;
float
scaledPreferencePercent
=
(
highAvailableVolumes
.
size
(
)
*
balancedPreferencePercent
)
/
preferencePercentScaler
;
if
(
mostAvailableAmongLowVolumes
<
replicaSize
||
random
.
nextFloat
(
)
<
scaledPreferencePercent
)
{
volume
=
roundRobinPolicyHighAvailable
.
chooseVolume
(
highAvailableVolumes
,
replicaSize
,
storageId
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
volume
+
+
replicaSize
)
;
}
}
else
{
volume
=
roundRobinPolicyLowAvailable
.
chooseVolume
(
lowAvailableVolumes
,
replicaSize
,
storageId
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
VisibleForTesting
static
ReplicaInfo
selectReplicaToDelete
(
final
ReplicaInfo
replica1
,
final
ReplicaInfo
replica2
)
{
ReplicaInfo
replicaToKeep
;
ReplicaInfo
replicaToDelete
;
if
(
replica1
.
getBlockURI
(
)
.
equals
(
replica2
.
getBlockURI
(
)
)
)
{
return
null
;
}
if
(
replica1
.
getGenerationStamp
(
)
!=
replica2
.
getGenerationStamp
(
)
)
{
replicaToKeep
=
replica1
.
getGenerationStamp
(
)
>
replica2
.
getGenerationStamp
(
)
?
replica1
:
replica2
;
}
else
if
(
replica1
.
getNumBytes
(
)
!=
replica2
.
getNumBytes
(
)
)
{
replicaToKeep
=
replica1
.
getNumBytes
(
)
>
replica2
.
getNumBytes
(
)
?
replica1
:
replica2
;
}
else
if
(
replica1
.
getVolume
(
)
.
isTransientStorage
(
)
&&
!
replica2
.
getVolume
(
)
.
isTransientStorage
(
)
)
{
replicaToKeep
=
replica2
;
}
else
{
replicaToKeep
=
replica1
;
}
replicaToDelete
=
(
replicaToKeep
==
replica1
)
?
replica2
:
replica1
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
boolean
readReplicasFromCache
(
ReplicaMap
volumeMap
,
final
RamDiskReplicaTracker
lazyWriteReplicaMap
)
{
ReplicaMap
tmpReplicaMap
=
new
ReplicaMap
(
new
ReentrantReadWriteLock
(
)
)
;
File
replicaFile
=
new
File
(
replicaCacheDir
,
REPLICA_CACHE_FILE
)
;
if
(
!
replicaFile
.
exists
(
)
)
{
FileInputStream
inputStream
=
null
;
try
{
inputStream
=
fileIoProvider
.
getFileInputStream
(
volume
,
replicaFile
)
;
BlockListAsLongs
blocksList
=
BlockListAsLongs
.
readFrom
(
inputStream
,
maxDataLength
)
;
if
(
blocksList
==
null
)
{
return
false
;
}
for
(
BlockReportReplica
replica
:
blocksList
)
{
switch
(
replica
.
getState
(
)
)
{
case
FINALIZED
:
addReplicaToReplicasMap
(
replica
,
tmpReplicaMap
,
lazyWriteReplicaMap
,
true
)
;
break
;
case
RUR
:
case
RBW
:
case
RWR
:
addReplicaToReplicasMap
(
replica
,
tmpReplicaMap
,
lazyWriteReplicaMap
,
false
)
;
break
;
default
:
break
;
}
}
for
(
Iterator
<
ReplicaInfo
>
iter
=
tmpReplicaMap
.
replicas
(
bpid
)
.
iterator
(
)
;
iter
.
hasNext
(
)
;
)
{
ReplicaInfo
info
=
iter
.
next
(
)
;
inputStream
=
fileIoProvider
.
getFileInputStream
(
volume
,
replicaFile
)
;
BlockListAsLongs
blocksList
=
BlockListAsLongs
.
readFrom
(
inputStream
,
maxDataLength
)
;
if
(
blocksList
==
null
)
{
return
false
;
}
for
(
BlockReportReplica
replica
:
blocksList
)
{
switch
(
replica
.
getState
(
)
)
{
case
FINALIZED
:
addReplicaToReplicasMap
(
replica
,
tmpReplicaMap
,
lazyWriteReplicaMap
,
true
)
;
break
;
case
RUR
:
case
RBW
:
case
RWR
:
addReplicaToReplicasMap
(
replica
,
tmpReplicaMap
,
lazyWriteReplicaMap
,
false
)
;
break
;
default
:
break
;
}
}
for
(
Iterator
<
ReplicaInfo
>
iter
=
tmpReplicaMap
.
replicas
(
bpid
)
.
iterator
(
)
;
iter
.
hasNext
(
)
;
)
{
ReplicaInfo
info
=
iter
.
next
(
)
;
iter
.
remove
(
)
;
volumeMap
.
add
(
bpid
,
info
)
;
for
(
BlockReportReplica
replica
:
blocksList
)
{
switch
(
replica
.
getState
(
)
)
{
case
FINALIZED
:
addReplicaToReplicasMap
(
replica
,
tmpReplicaMap
,
lazyWriteReplicaMap
,
true
)
;
break
;
case
RUR
:
case
RBW
:
case
RWR
:
addReplicaToReplicasMap
(
replica
,
tmpReplicaMap
,
lazyWriteReplicaMap
,
false
)
;
break
;
default
:
break
;
}
}
for
(
Iterator
<
ReplicaInfo
>
iter
=
tmpReplicaMap
.
replicas
(
bpid
)
.
iterator
(
)
;
iter
.
hasNext
(
)
;
)
{
ReplicaInfo
info
=
iter
.
next
(
)
;
iter
.
remove
(
)
;
volumeMap
.
add
(
bpid
,
info
)
;
}
LOG
.
info
(
+
replicaFile
.
getPath
(
)
)
;
return
true
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
replicaFile
.
getPath
(
)
,
e
)
;
void
deleteAsync
(
FsVolumeReference
volumeRef
,
ReplicaInfo
replicaToDelete
,
ExtendedBlock
block
,
String
trashDirectory
)
{
void
deleteSync
(
FsVolumeReference
volumeRef
,
ReplicaInfo
replicaToDelete
,
ExtendedBlock
block
,
String
trashDirectory
)
{
synchronized
void
cacheBlock
(
long
blockId
,
String
bpid
,
String
blockFileName
,
long
length
,
long
genstamp
,
Executor
volumeExecutor
)
{
ExtendedBlockId
key
=
new
ExtendedBlockId
(
blockId
,
bpid
)
;
Value
prevValue
=
mappableBlockMap
.
get
(
key
)
;
if
(
prevValue
!=
null
)
{
Value
prevValue
=
mappableBlockMap
.
get
(
key
)
;
boolean
deferred
=
false
;
if
(
cacheLoader
.
isTransientCache
(
)
&&
!
dataset
.
datanode
.
getShortCircuitRegistry
(
)
.
processBlockMunlockRequest
(
key
)
)
{
deferred
=
true
;
}
if
(
prevValue
==
null
)
{
LOG
.
debug
(
+
,
blockId
,
bpid
)
;
numBlocksFailedToUncache
.
incrementAndGet
(
)
;
return
;
}
switch
(
prevValue
.
state
)
{
case
CACHING
:
LOG
.
debug
(
,
blockId
,
bpid
)
;
mappableBlockMap
.
put
(
key
,
new
Value
(
prevValue
.
mappableBlock
,
State
.
CACHING_CANCELLED
)
)
;
break
;
case
CACHED
:
mappableBlockMap
.
put
(
key
,
new
Value
(
prevValue
.
mappableBlock
,
State
.
UNCACHING
)
)
;
if
(
deferred
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
deferred
=
true
;
}
if
(
prevValue
==
null
)
{
LOG
.
debug
(
+
,
blockId
,
bpid
)
;
numBlocksFailedToUncache
.
incrementAndGet
(
)
;
return
;
}
switch
(
prevValue
.
state
)
{
case
CACHING
:
LOG
.
debug
(
,
blockId
,
bpid
)
;
mappableBlockMap
.
put
(
key
,
new
Value
(
prevValue
.
mappableBlock
,
State
.
CACHING_CANCELLED
)
)
;
break
;
case
CACHED
:
mappableBlockMap
.
put
(
key
,
new
Value
(
prevValue
.
mappableBlock
,
State
.
UNCACHING
)
)
;
if
(
deferred
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
key
,
DurationFormatUtils
.
formatDurationHMS
(
revocationPollingMs
)
)
;
}
deferredUncachingExecutor
.
schedule
(
new
UncachingTask
(
key
,
revocationMs
)
,
revocationPollingMs
,
TimeUnit
.
MILLISECONDS
)
;
}
else
{
private
void
activateVolume
(
ReplicaMap
replicaMap
,
Storage
.
StorageDirectory
sd
,
StorageType
storageType
,
FsVolumeReference
ref
)
throws
IOException
{
try
(
AutoCloseableLock
lock
=
datasetWriteLock
.
acquire
(
)
)
{
DatanodeStorage
dnStorage
=
storageMap
.
get
(
sd
.
getStorageUuid
(
)
)
;
if
(
dnStorage
!=
null
)
{
final
String
errorMsg
=
String
.
format
(
,
sd
.
getStorageUuid
(
)
,
sd
.
getVersionFile
(
)
)
;
private
void
addVolume
(
Storage
.
StorageDirectory
sd
)
throws
IOException
{
final
StorageLocation
storageLocation
=
sd
.
getStorageLocation
(
)
;
FsVolumeImpl
fsVolume
=
new
FsVolumeImplBuilder
(
)
.
setDataset
(
this
)
.
setStorageID
(
sd
.
getStorageUuid
(
)
)
.
setStorageDirectory
(
sd
)
.
setFileIoProvider
(
datanode
.
getFileIoProvider
(
)
)
.
setConf
(
this
.
conf
)
.
build
(
)
;
FsVolumeReference
ref
=
fsVolume
.
obtainReference
(
)
;
ReplicaMap
tempVolumeMap
=
new
ReplicaMap
(
datasetReadLock
,
datasetWriteLock
)
;
fsVolume
.
getVolumeMap
(
tempVolumeMap
,
ramDiskReplicaTracker
)
;
activateVolume
(
tempVolumeMap
,
sd
,
storageLocation
.
getStorageType
(
)
,
ref
)
;
@
Override
public
void
removeVolumes
(
final
Collection
<
StorageLocation
>
storageLocsToRemove
,
boolean
clearFailure
)
{
Collection
<
StorageLocation
>
storageLocationsToRemove
=
new
ArrayList
<
>
(
storageLocsToRemove
)
;
Map
<
String
,
List
<
ReplicaInfo
>>
blkToInvalidate
=
new
HashMap
<
>
(
)
;
List
<
String
>
storageToRemove
=
new
ArrayList
<
>
(
)
;
try
(
AutoCloseableLock
lock
=
datasetWriteLock
.
acquire
(
)
)
{
for
(
int
idx
=
0
;
idx
<
dataStorage
.
getNumStorageDirs
(
)
;
idx
++
)
{
Storage
.
StorageDirectory
sd
=
dataStorage
.
getStorageDir
(
idx
)
;
final
StorageLocation
sdLocation
=
sd
.
getStorageLocation
(
)
;
@
Override
public
void
removeVolumes
(
final
Collection
<
StorageLocation
>
storageLocsToRemove
,
boolean
clearFailure
)
{
Collection
<
StorageLocation
>
storageLocationsToRemove
=
new
ArrayList
<
>
(
storageLocsToRemove
)
;
Map
<
String
,
List
<
ReplicaInfo
>>
blkToInvalidate
=
new
HashMap
<
>
(
)
;
List
<
String
>
storageToRemove
=
new
ArrayList
<
>
(
)
;
try
(
AutoCloseableLock
lock
=
datasetWriteLock
.
acquire
(
)
)
{
for
(
int
idx
=
0
;
idx
<
dataStorage
.
getNumStorageDirs
(
)
;
idx
++
)
{
Storage
.
StorageDirectory
sd
=
dataStorage
.
getStorageDir
(
idx
)
;
final
StorageLocation
sdLocation
=
sd
.
getStorageLocation
(
)
;
LOG
.
info
(
+
sdLocation
+
+
sd
.
getStorageUuid
(
)
)
;
if
(
storageLocationsToRemove
.
contains
(
sdLocation
)
)
{
if
(
calculateChecksum
)
{
computeChecksum
(
srcReplica
,
dstMeta
,
smallBufferSize
,
conf
)
;
}
else
{
try
{
srcReplica
.
copyMetadata
(
dstMeta
.
toURI
(
)
)
;
}
catch
(
IOException
e
)
{
throw
new
IOException
(
+
srcReplica
+
+
dstMeta
,
e
)
;
}
}
try
{
srcReplica
.
copyBlockdata
(
dstFile
.
toURI
(
)
)
;
}
catch
(
IOException
e
)
{
throw
new
IOException
(
+
srcReplica
+
+
dstFile
,
e
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
calculateChecksum
)
{
LOG
.
debug
(
+
srcReplica
.
getMetadataURI
(
)
+
+
dstMeta
+
)
;
}
else
{
@
Override
public
ReplicaHandler
recoverAppend
(
ExtendedBlock
b
,
long
newGS
,
long
expectedBlockLen
)
throws
IOException
{
@
Override
public
Replica
recoverClose
(
ExtendedBlock
b
,
long
newGS
,
long
expectedBlockLen
)
throws
IOException
{
@
Override
public
ReplicaHandler
recoverRbw
(
ExtendedBlock
b
,
long
newGS
,
long
minBytesRcvd
,
long
maxBytesRcvd
)
throws
IOException
{
@
Override
public
ReplicaInPipeline
convertTemporaryToRbw
(
final
ExtendedBlock
b
)
throws
IOException
{
long
startTimeMs
=
Time
.
monotonicNow
(
)
;
try
(
AutoCloseableLock
lock
=
datasetWriteLock
.
acquire
(
)
)
{
final
long
blockId
=
b
.
getBlockId
(
)
;
final
long
expectedGs
=
b
.
getGenerationStamp
(
)
;
final
long
visible
=
b
.
getNumBytes
(
)
;
@
Override
public
void
adjustCrcChannelPosition
(
ExtendedBlock
b
,
ReplicaOutputStreams
streams
,
int
checksumSize
)
throws
IOException
{
FileOutputStream
file
=
(
FileOutputStream
)
streams
.
getChecksumOut
(
)
;
FileChannel
channel
=
file
.
getChannel
(
)
;
long
oldPos
=
channel
.
position
(
)
;
long
newPos
=
oldPos
-
checksumSize
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
errors
.
add
(
+
invalidBlks
[
i
]
+
+
Block
.
toString
(
infoByBlockId
)
)
;
}
continue
;
}
v
=
(
FsVolumeImpl
)
info
.
getVolume
(
)
;
if
(
v
==
null
)
{
errors
.
add
(
+
invalidBlks
[
i
]
+
+
info
)
;
continue
;
}
try
{
File
blockFile
=
new
File
(
info
.
getBlockURI
(
)
)
;
if
(
blockFile
!=
null
&&
blockFile
.
getParentFile
(
)
==
null
)
{
errors
.
add
(
+
invalidBlks
[
i
]
+
+
blockFile
)
;
continue
;
}
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
warn
(
+
info
+
)
;
}
removing
=
volumeMap
.
remove
(
bpid
,
invalidBlks
[
i
]
)
;
addDeletingBlock
(
bpid
,
removing
.
getBlockId
(
)
)
;
static
ReplicaRecoveryInfo
initReplicaRecoveryImpl
(
String
bpid
,
ReplicaMap
map
,
Block
block
,
long
recoveryId
)
throws
IOException
,
MustStopExistingWriter
{
final
ReplicaInfo
replica
=
map
.
get
(
bpid
,
block
.
getBlockId
(
)
)
;
if
(
!
rip
.
attemptToSetWriter
(
null
,
Thread
.
currentThread
(
)
)
)
{
throw
new
MustStopExistingWriter
(
rip
)
;
}
if
(
replica
.
getBytesOnDisk
(
)
<
replica
.
getVisibleLength
(
)
)
{
throw
new
IOException
(
+
replica
)
;
}
checkReplicaFiles
(
replica
)
;
}
if
(
replica
.
getGenerationStamp
(
)
<
block
.
getGenerationStamp
(
)
)
{
throw
new
IOException
(
+
block
+
+
replica
)
;
}
if
(
replica
.
getGenerationStamp
(
)
>=
recoveryId
)
{
throw
new
IOException
(
+
+
recoveryId
+
+
block
+
+
replica
)
;
}
final
ReplicaInfo
rur
;
if
(
replica
.
getState
(
)
==
ReplicaState
.
RUR
)
{
rur
=
replica
;
if
(
rur
.
getRecoveryID
(
)
>=
recoveryId
)
{
throw
new
RecoveryInProgressException
(
+
recoveryId
+
+
block
+
+
rur
)
;
}
final
long
oldRecoveryID
=
rur
.
getRecoveryID
(
)
;
throw
new
IOException
(
+
replica
)
;
}
checkReplicaFiles
(
replica
)
;
}
if
(
replica
.
getGenerationStamp
(
)
<
block
.
getGenerationStamp
(
)
)
{
throw
new
IOException
(
+
block
+
+
replica
)
;
}
if
(
replica
.
getGenerationStamp
(
)
>=
recoveryId
)
{
throw
new
IOException
(
+
+
recoveryId
+
+
block
+
+
replica
)
;
}
final
ReplicaInfo
rur
;
if
(
replica
.
getState
(
)
==
ReplicaState
.
RUR
)
{
rur
=
replica
;
if
(
rur
.
getRecoveryID
(
)
>=
recoveryId
)
{
throw
new
RecoveryInProgressException
(
+
recoveryId
+
+
block
+
+
rur
)
;
}
final
long
oldRecoveryID
=
rur
.
getRecoveryID
(
)
;
rur
.
setRecoveryID
(
recoveryId
)
;
LOG
.
info
(
+
block
+
+
oldRecoveryID
+
+
recoveryId
)
;
}
else
{
@
Override
public
void
addBlockPool
(
String
bpid
,
Configuration
conf
)
throws
IOException
{
@
Override
public
void
shutdownBlockPool
(
String
bpid
)
{
try
(
AutoCloseableLock
lock
=
datasetWriteLock
.
acquire
(
)
)
{
@
Override
public
void
onCompleteLazyPersist
(
String
bpId
,
long
blockId
,
long
creationTime
,
File
[
]
savedFiles
,
FsVolumeImpl
targetVolume
)
{
try
(
AutoCloseableLock
lock
=
datasetWriteLock
.
acquire
(
)
)
{
ramDiskReplicaTracker
.
recordEndLazyPersist
(
bpId
,
blockId
,
savedFiles
)
;
targetVolume
.
incDfsUsedAndNumBlocks
(
bpId
,
savedFiles
[
0
]
.
length
(
)
+
savedFiles
[
1
]
.
length
(
)
)
;
datanode
.
getMetrics
(
)
.
incrRamDiskBlocksLazyPersisted
(
)
;
datanode
.
getMetrics
(
)
.
incrRamDiskBytesLazyPersisted
(
savedFiles
[
1
]
.
length
(
)
)
;
datanode
.
getMetrics
(
)
.
addRamDiskBlocksLazyPersistWindowMs
(
Time
.
monotonicNow
(
)
-
creationTime
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
MappableBlock
load
(
long
length
,
FileInputStream
blockIn
,
FileInputStream
metaIn
,
String
blockFileName
,
ExtendedBlockId
key
)
throws
IOException
{
PmemMappedBlock
mappableBlock
=
null
;
String
cachePath
=
null
;
FileChannel
blockChannel
=
null
;
RandomAccessFile
cacheFile
=
null
;
try
{
blockChannel
=
blockIn
.
getChannel
(
)
;
if
(
blockChannel
==
null
)
{
throw
new
IOException
(
)
;
}
cachePath
=
pmemVolumeManager
.
getCachePath
(
key
)
;
cacheFile
=
new
RandomAccessFile
(
cachePath
,
)
;
blockChannel
.
transferTo
(
0
,
length
,
cacheFile
.
getChannel
(
)
)
;
cacheFile
.
getChannel
(
)
.
position
(
0
)
;
verifyChecksum
(
length
,
metaIn
,
cacheFile
.
getChannel
(
)
,
blockFileName
)
;
mappableBlock
=
new
PmemMappedBlock
(
length
,
key
)
;
try
{
blockChannel
=
blockIn
.
getChannel
(
)
;
if
(
blockChannel
==
null
)
{
throw
new
IOException
(
)
;
}
cachePath
=
pmemVolumeManager
.
getCachePath
(
key
)
;
cacheFile
=
new
RandomAccessFile
(
cachePath
,
)
;
blockChannel
.
transferTo
(
0
,
length
,
cacheFile
.
getChannel
(
)
)
;
cacheFile
.
getChannel
(
)
.
position
(
0
)
;
verifyChecksum
(
length
,
metaIn
,
cacheFile
.
getChannel
(
)
,
blockFileName
)
;
mappableBlock
=
new
PmemMappedBlock
(
length
,
key
)
;
LOG
.
info
(
+
,
key
,
cachePath
,
length
)
;
}
finally
{
IOUtils
.
closeQuietly
(
blockChannel
)
;
IOUtils
.
closeQuietly
(
cacheFile
)
;
if
(
mappableBlock
==
null
)
{
@
Override
public
MappableBlock
getRecoveredMappableBlock
(
File
cacheFile
,
String
bpid
,
byte
volumeIndex
)
throws
IOException
{
ExtendedBlockId
key
=
new
ExtendedBlockId
(
getBlockId
(
cacheFile
)
,
bpid
)
;
MappableBlock
mappableBlock
=
new
PmemMappedBlock
(
cacheFile
.
length
(
)
,
key
)
;
PmemVolumeManager
.
getInstance
(
)
.
recoverBlockKeyToVolume
(
key
,
volumeIndex
)
;
String
path
=
PmemVolumeManager
.
getInstance
(
)
.
getCachePath
(
key
)
;
long
length
=
mappableBlock
.
getLength
(
)
;
@
Override
public
void
close
(
)
{
String
cacheFilePath
=
null
;
try
{
cacheFilePath
=
PmemVolumeManager
.
getInstance
(
)
.
getCachePath
(
key
)
;
FsDatasetUtil
.
deleteMappedFile
(
cacheFilePath
)
;
private
void
loadVolumes
(
String
[
]
volumes
)
throws
IOException
{
for
(
byte
n
=
0
;
n
<
volumes
.
length
;
n
++
)
{
try
{
File
pmemDir
=
new
File
(
volumes
[
n
]
)
;
File
realPmemDir
=
verifyIfValidPmemVolume
(
pmemDir
)
;
if
(
!
cacheRecoveryEnabled
)
{
cleanup
(
realPmemDir
)
;
}
this
.
pmemVolumes
.
add
(
realPmemDir
.
getPath
(
)
)
;
long
maxBytes
;
if
(
maxBytesPerPmem
==
-
1
)
{
maxBytes
=
realPmemDir
.
getUsableSpace
(
)
;
}
else
{
maxBytes
=
maxBytesPerPmem
;
}
UsedBytesCount
usedBytesCount
=
new
UsedBytesCount
(
maxBytes
)
;
this
.
usedBytesCounts
.
add
(
usedBytesCount
)
;
try
{
File
pmemDir
=
new
File
(
volumes
[
n
]
)
;
File
realPmemDir
=
verifyIfValidPmemVolume
(
pmemDir
)
;
if
(
!
cacheRecoveryEnabled
)
{
cleanup
(
realPmemDir
)
;
}
this
.
pmemVolumes
.
add
(
realPmemDir
.
getPath
(
)
)
;
long
maxBytes
;
if
(
maxBytesPerPmem
==
-
1
)
{
maxBytes
=
realPmemDir
.
getUsableSpace
(
)
;
}
else
{
maxBytes
=
maxBytesPerPmem
;
}
UsedBytesCount
usedBytesCount
=
new
UsedBytesCount
(
maxBytes
)
;
this
.
usedBytesCounts
.
add
(
usedBytesCount
)
;
LOG
.
info
(
,
volumes
[
n
]
,
maxBytes
)
;
}
catch
(
IllegalArgumentException
e
)
{
if
(
!
cacheRecoveryEnabled
)
{
cleanup
(
realPmemDir
)
;
}
this
.
pmemVolumes
.
add
(
realPmemDir
.
getPath
(
)
)
;
long
maxBytes
;
if
(
maxBytesPerPmem
==
-
1
)
{
maxBytes
=
realPmemDir
.
getUsableSpace
(
)
;
}
else
{
maxBytes
=
maxBytesPerPmem
;
}
UsedBytesCount
usedBytesCount
=
new
UsedBytesCount
(
maxBytes
)
;
this
.
usedBytesCounts
.
add
(
usedBytesCount
)
;
LOG
.
info
(
,
volumes
[
n
]
,
maxBytes
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
error
(
+
volumes
[
n
]
,
e
)
;
continue
;
}
catch
(
IOException
e
)
{
@
VisibleForTesting
protected
static
String
getSuffix
(
final
Path
prefix
,
final
Path
fullPath
)
{
String
prefixStr
=
prefix
.
toString
(
)
;
String
pathStr
=
fullPath
.
toString
(
)
;
if
(
!
pathStr
.
startsWith
(
prefixStr
)
)
{
@
Override
void
addBlockPool
(
String
bpid
,
Configuration
conf
,
Timer
timer
)
throws
IOException
{
@
Override
public
void
compileReport
(
String
bpid
,
Collection
<
ScanInfo
>
report
,
ReportCompiler
reportCompiler
)
throws
InterruptedException
,
IOException
{
void
submitLazyPersistTask
(
String
bpId
,
long
blockId
,
long
genStamp
,
long
creationTime
,
ReplicaInfo
replica
,
FsVolumeReference
target
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
refresh
(
)
{
long
start
=
Time
.
monotonicNow
(
)
;
long
dfsUsed
=
0
;
long
count
=
0
;
FsDatasetSpi
fsDataset
=
volume
.
getDataset
(
)
;
try
{
Collection
<
ReplicaInfo
>
replicaInfos
=
(
Collection
<
ReplicaInfo
>
)
fsDataset
.
deepCopyReplica
(
bpid
)
;
long
cost
=
Time
.
monotonicNow
(
)
-
start
;
if
(
cost
>
DEEP_COPY_REPLICA_THRESHOLD_MS
)
{
FsDatasetSpi
fsDataset
=
volume
.
getDataset
(
)
;
try
{
Collection
<
ReplicaInfo
>
replicaInfos
=
(
Collection
<
ReplicaInfo
>
)
fsDataset
.
deepCopyReplica
(
bpid
)
;
long
cost
=
Time
.
monotonicNow
(
)
-
start
;
if
(
cost
>
DEEP_COPY_REPLICA_THRESHOLD_MS
)
{
LOG
.
debug
(
+
,
bpid
,
replicaInfos
.
size
(
)
,
Time
.
monotonicNow
(
)
-
start
)
;
}
if
(
CollectionUtils
.
isNotEmpty
(
replicaInfos
)
)
{
for
(
ReplicaInfo
replicaInfo
:
replicaInfos
)
{
if
(
Objects
.
equals
(
replicaInfo
.
getVolume
(
)
.
getStorageID
(
)
,
volume
.
getStorageID
(
)
)
)
{
dfsUsed
+=
replicaInfo
.
getBytesOnDisk
(
)
;
dfsUsed
+=
replicaInfo
.
getMetadataLength
(
)
;
count
++
;
}
}
}
this
.
used
.
set
(
dfsUsed
)
;
cost
=
Time
.
monotonicNow
(
)
-
start
;
if
(
cost
>
REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS
)
{
Collection
<
ReplicaInfo
>
replicaInfos
=
(
Collection
<
ReplicaInfo
>
)
fsDataset
.
deepCopyReplica
(
bpid
)
;
long
cost
=
Time
.
monotonicNow
(
)
-
start
;
if
(
cost
>
DEEP_COPY_REPLICA_THRESHOLD_MS
)
{
LOG
.
debug
(
+
,
bpid
,
replicaInfos
.
size
(
)
,
Time
.
monotonicNow
(
)
-
start
)
;
}
if
(
CollectionUtils
.
isNotEmpty
(
replicaInfos
)
)
{
for
(
ReplicaInfo
replicaInfo
:
replicaInfos
)
{
if
(
Objects
.
equals
(
replicaInfo
.
getVolume
(
)
.
getStorageID
(
)
,
volume
.
getStorageID
(
)
)
)
{
dfsUsed
+=
replicaInfo
.
getBytesOnDisk
(
)
;
dfsUsed
+=
replicaInfo
.
getMetadataLength
(
)
;
count
++
;
}
}
}
this
.
used
.
set
(
dfsUsed
)
;
cost
=
Time
.
monotonicNow
(
)
-
start
;
if
(
cost
>
REPLICA_CACHING_GET_SPACE_USED_THRESHOLD_MS
)
{
LOG
.
debug
(
+
,
bpid
,
count
,
used
,
volume
.
getStorageID
(
)
,
Time
.
monotonicNow
(
)
-
start
)
;
}
}
catch
(
Exception
e
)
{
try
{
fsVolumeReferences
=
dn
.
getFSDataset
(
)
.
getFsVolumeReferences
(
)
;
Iterator
<
FsVolumeSpi
>
volumeIterator
=
fsVolumeReferences
.
iterator
(
)
;
while
(
volumeIterator
.
hasNext
(
)
)
{
FsVolumeSpi
volume
=
volumeIterator
.
next
(
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
String
volumeName
=
volume
.
getBaseURI
(
)
.
getPath
(
)
;
metadataOpStats
.
put
(
volumeName
,
metrics
.
getMetadataOperationMean
(
)
)
;
readIoStats
.
put
(
volumeName
,
metrics
.
getReadIoMean
(
)
)
;
writeIoStats
.
put
(
volumeName
,
metrics
.
getWriteIoMean
(
)
)
;
}
}
finally
{
if
(
fsVolumeReferences
!=
null
)
{
try
{
fsVolumeReferences
.
close
(
)
;
}
catch
(
IOException
e
)
{
readIoStats
.
put
(
volumeName
,
metrics
.
getReadIoMean
(
)
)
;
writeIoStats
.
put
(
volumeName
,
metrics
.
getWriteIoMean
(
)
)
;
}
}
finally
{
if
(
fsVolumeReferences
!=
null
)
{
try
{
fsVolumeReferences
.
close
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
}
}
}
if
(
metadataOpStats
.
isEmpty
(
)
&&
readIoStats
.
isEmpty
(
)
&&
writeIoStats
.
isEmpty
(
)
)
{
LOG
.
debug
(
)
;
continue
;
}
detectAndUpdateDiskOutliers
(
metadataOpStats
,
readIoStats
,
writeIoStats
)
;
}
try
{
Thread
.
sleep
(
detectionInterval
)
;
}
catch
(
InterruptedException
e
)
{
public
Map
<
String
,
Double
>
getOutliers
(
Map
<
String
,
Double
>
stats
)
{
if
(
stats
.
size
(
)
<
minNumResources
)
{
@
Override
public
void
exceptionCaught
(
ChannelHandlerContext
ctx
,
Throwable
cause
)
{
@
Override
public
void
exceptionCaught
(
ChannelHandlerContext
ctx
,
Throwable
cause
)
{
p
.
addLast
(
new
HttpRequestEncoder
(
)
,
new
Forwarder
(
uri
,
client
)
)
;
}
}
)
;
ChannelFuture
f
=
proxiedServer
.
connect
(
host
)
;
proxiedChannel
=
f
.
channel
(
)
;
f
.
addListener
(
new
ChannelFutureListener
(
)
{
@
Override
public
void
operationComplete
(
ChannelFuture
future
)
throws
Exception
{
if
(
future
.
isSuccess
(
)
)
{
ctx
.
channel
(
)
.
pipeline
(
)
.
remove
(
HttpResponseEncoder
.
class
)
;
HttpRequest
newReq
=
new
DefaultFullHttpRequest
(
HTTP_1_1
,
req
.
getMethod
(
)
,
req
.
getUri
(
)
)
;
newReq
.
headers
(
)
.
add
(
req
.
headers
(
)
)
;
newReq
.
headers
(
)
.
set
(
CONNECTION
,
Values
.
CLOSE
)
;
future
.
channel
(
)
.
writeAndFlush
(
newReq
)
;
}
else
{
DefaultHttpResponse
resp
=
new
DefaultHttpResponse
(
HTTP_1_1
,
INTERNAL_SERVER_ERROR
)
;
resp
.
headers
(
)
.
set
(
CONNECTION
,
Values
.
CLOSE
)
;
@
Override
public
void
exceptionCaught
(
ChannelHandlerContext
ctx
,
Throwable
cause
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
exceptionCaught
(
ChannelHandlerContext
ctx
,
Throwable
cause
)
{
releaseDfsResources
(
)
;
DefaultHttpResponse
resp
=
ExceptionHandler
.
exceptionCaught
(
cause
)
;
resp
.
headers
(
)
.
set
(
CONNECTION
,
CLOSE
)
;
ctx
.
writeAndFlush
(
resp
)
.
addListener
(
ChannelFutureListener
.
CLOSE
)
;
if
(
LOG
!=
null
&&
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
exceptionCaught
(
ChannelHandlerContext
ctx
,
Throwable
cause
)
{
protected
void
recordOutput
(
final
TextStringBuilder
result
,
final
String
outputLine
)
{
protected
int
parseTopNodes
(
final
CommandLine
cmd
,
final
TextStringBuilder
result
)
throws
IllegalArgumentException
{
String
outputLine
=
;
int
nodes
=
0
;
final
String
topVal
=
cmd
.
getOptionValue
(
DiskBalancerCLI
.
TOP
)
;
if
(
StringUtils
.
isBlank
(
topVal
)
)
{
outputLine
=
String
.
format
(
,
getDefaultTop
(
)
)
;
@
Override
public
void
execute
(
CommandLine
cmd
)
throws
Exception
{
LOG
.
info
(
)
;
Preconditions
.
checkState
(
cmd
.
hasOption
(
DiskBalancerCLI
.
QUERY
)
)
;
verifyCommandOptions
(
DiskBalancerCLI
.
QUERY
,
cmd
)
;
String
nodeName
=
cmd
.
getOptionValue
(
DiskBalancerCLI
.
QUERY
)
;
Preconditions
.
checkNotNull
(
nodeName
)
;
nodeName
=
nodeName
.
trim
(
)
;
String
nodeAddress
=
nodeName
;
if
(
!
nodeName
.
matches
(
)
)
{
int
defaultIPC
=
NetUtils
.
createSocketAddr
(
getConf
(
)
.
getTrimmed
(
DFSConfigKeys
.
DFS_DATANODE_IPC_ADDRESS_KEY
,
DFSConfigKeys
.
DFS_DATANODE_IPC_ADDRESS_DEFAULT
)
)
.
getPort
(
)
;
nodeAddress
=
nodeName
+
+
defaultIPC
;
String
nodeName
=
cmd
.
getOptionValue
(
DiskBalancerCLI
.
QUERY
)
;
Preconditions
.
checkNotNull
(
nodeName
)
;
nodeName
=
nodeName
.
trim
(
)
;
String
nodeAddress
=
nodeName
;
if
(
!
nodeName
.
matches
(
)
)
{
int
defaultIPC
=
NetUtils
.
createSocketAddr
(
getConf
(
)
.
getTrimmed
(
DFSConfigKeys
.
DFS_DATANODE_IPC_ADDRESS_KEY
,
DFSConfigKeys
.
DFS_DATANODE_IPC_ADDRESS_DEFAULT
)
)
.
getPort
(
)
;
nodeAddress
=
nodeName
+
+
defaultIPC
;
LOG
.
debug
(
,
nodeAddress
)
;
}
ClientDatanodeProtocol
dataNode
=
getDataNodeProxy
(
nodeAddress
)
;
try
{
DiskBalancerWorkStatus
workStatus
=
dataNode
.
queryDiskBalancerPlan
(
)
;
System
.
out
.
printf
(
,
workStatus
.
getPlanFile
(
)
,
workStatus
.
getPlanID
(
)
,
workStatus
.
getResult
(
)
.
toString
(
)
)
;
if
(
cmd
.
hasOption
(
DiskBalancerCLI
.
VERBOSE
)
)
{
System
.
out
.
printf
(
,
workStatus
.
currentStateString
(
)
)
;
}
}
catch
(
DiskBalancerException
ex
)
{
public
static
ClusterConnector
getCluster
(
URI
clusterURI
,
Configuration
conf
)
throws
IOException
,
URISyntaxException
{
public
static
ClusterConnector
getCluster
(
URI
clusterURI
,
Configuration
conf
)
throws
IOException
,
URISyntaxException
{
LOG
.
debug
(
,
clusterURI
)
;
@
Override
public
List
<
DiskBalancerDataNode
>
getNodes
(
)
throws
Exception
{
Preconditions
.
checkNotNull
(
this
.
clusterURI
)
;
String
dataFilePath
=
this
.
clusterURI
.
getPath
(
)
;
@
Override
public
List
<
DiskBalancerDataNode
>
getNodes
(
)
throws
Exception
{
Preconditions
.
checkNotNull
(
this
.
clusterURI
)
;
String
dataFilePath
=
this
.
clusterURI
.
getPath
(
)
;
LOG
.
info
(
+
dataFilePath
)
;
DiskBalancerCluster
cluster
=
READER
.
readValue
(
new
File
(
dataFilePath
)
)
;
String
message
=
String
.
format
(
,
cluster
.
getNodes
(
)
.
size
(
)
)
;
public
void
readClusterInfo
(
)
throws
Exception
{
Preconditions
.
checkNotNull
(
clusterConnector
)
;
private
void
skipMisConfiguredVolume
(
DiskBalancerVolume
volume
)
{
String
errMessage
=
String
.
format
(
+
+
+
+
+
+
+
+
,
volume
.
getCapacity
(
)
,
volume
.
getReserved
(
)
,
volume
.
computeEffectiveCapacity
(
)
,
volume
.
getStorageType
(
)
,
volume
.
getUuid
(
)
)
;
@
Override
public
NodePlan
plan
(
DiskBalancerDataNode
node
)
throws
Exception
{
final
long
startTime
=
Time
.
monotonicNow
(
)
;
NodePlan
plan
=
new
NodePlan
(
node
.
getDataNodeName
(
)
,
node
.
getDataNodePort
(
)
)
;
Preconditions
.
checkNotNull
(
vSet
)
;
Preconditions
.
checkNotNull
(
plan
)
;
Preconditions
.
checkNotNull
(
node
)
;
DiskBalancerVolumeSet
currentSet
=
new
DiskBalancerVolumeSet
(
vSet
)
;
while
(
currentSet
.
isBalancingNeeded
(
this
.
threshold
)
)
{
removeSkipVolumes
(
currentSet
)
;
DiskBalancerVolume
lowVolume
=
currentSet
.
getSortedQueue
(
)
.
first
(
)
;
DiskBalancerVolume
highVolume
=
currentSet
.
getSortedQueue
(
)
.
last
(
)
;
Step
nextStep
=
null
;
if
(
!
lowVolume
.
isSkip
(
)
&&
!
highVolume
.
isSkip
(
)
)
{
nextStep
=
computeMove
(
currentSet
,
lowVolume
,
highVolume
)
;
}
else
{
LOG
.
debug
(
,
lowVolume
.
getPath
(
)
,
highVolume
.
getPath
(
)
)
;
}
applyStep
(
nextStep
,
currentSet
,
lowVolume
,
highVolume
)
;
if
(
nextStep
!=
null
)
{
private
Step
computeMove
(
DiskBalancerVolumeSet
currentSet
,
DiskBalancerVolume
lowVolume
,
DiskBalancerVolume
highVolume
)
{
long
maxLowVolumeCanReceive
=
(
long
)
(
(
currentSet
.
getIdealUsed
(
)
*
lowVolume
.
computeEffectiveCapacity
(
)
)
-
lowVolume
.
getUsed
(
)
)
;
if
(
maxLowVolumeCanReceive
<=
0
)
{
private
void
skipVolume
(
DiskBalancerVolumeSet
currentSet
,
DiskBalancerVolume
volume
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
message
=
String
.
format
(
+
+
+
,
volume
.
getPath
(
)
,
volume
.
getStorageType
(
)
,
currentSet
.
getIdealUsed
(
)
*
volume
.
getCapacity
(
)
,
volume
.
getUsed
(
)
)
;
private
void
printQueue
(
TreeSet
<
DiskBalancerVolume
>
queue
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
format
=
String
.
format
(
+
,
queue
.
first
(
)
.
getPath
(
)
,
queue
.
first
(
)
.
getVolumeDataDensity
(
)
,
queue
.
last
(
)
.
getPath
(
)
,
queue
.
last
(
)
.
getVolumeDataDensity
(
)
)
;
public
static
Planner
getPlanner
(
String
plannerName
,
DiskBalancerDataNode
node
,
double
threshold
)
{
if
(
plannerName
.
equals
(
GREEDY_PLANNER
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
message
=
String
.
format
(
,
GREEDY_PLANNER
,
node
.
getDataNodeName
(
)
,
node
.
getDataNodeIP
(
)
,
node
.
getDataNodeUUID
(
)
)
;
static
int
run
(
Map
<
URI
,
List
<
Path
>>
namenodes
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
*
2
+
conf
.
getTimeDuration
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
AtomicInteger
retryCount
=
new
AtomicInteger
(
0
)
;
Map
<
Long
,
Set
<
DatanodeInfo
>>
excludedPinnedBlocks
=
new
HashMap
<
>
(
)
;
private
synchronized
void
setState
(
BNState
newState
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
NamenodeRegistration
nnReg
=
null
;
while
(
!
isStopRequested
(
)
)
{
try
{
nnReg
=
namenode
.
registerSubordinateNamenode
(
getRegistration
(
)
)
;
break
;
}
catch
(
SocketTimeoutException
e
)
{
LOG
.
info
(
+
nnRpcAddress
)
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
LOG
.
warn
(
,
e
)
;
}
}
}
String
msg
=
null
;
if
(
nnReg
==
null
)
msg
=
+
nnRpcAddress
;
else
if
(
!
nnReg
.
isRole
(
NamenodeRole
.
NAMENODE
)
)
{
msg
=
+
nnRpcAddress
+
;
public
final
void
processCacheReport
(
final
DatanodeID
datanodeID
,
final
List
<
Long
>
blockIds
)
throws
IOException
{
if
(
!
enabled
)
{
private
void
initialize
(
Configuration
conf
)
throws
IOException
{
shouldRun
=
true
;
checkpointConf
=
new
CheckpointConf
(
conf
)
;
String
fullInfoAddr
=
conf
.
get
(
DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY
,
DFS_NAMENODE_BACKUP_HTTP_ADDRESS_DEFAULT
)
;
infoBindAddress
=
fullInfoAddr
.
substring
(
0
,
fullInfoAddr
.
indexOf
(
)
)
;
LOG
.
info
(
+
checkpointConf
.
getPeriod
(
)
+
+
+
checkpointConf
.
getPeriod
(
)
/
60
+
)
;
}
while
(
shouldRun
)
{
try
{
long
now
=
monotonicNow
(
)
;
boolean
shouldCheckpoint
=
false
;
if
(
now
>=
lastCheckpointTime
+
checkpointPeriodMSec
)
{
shouldCheckpoint
=
true
;
}
else
if
(
now
>=
lastEditLogCheckTime
+
periodMSec
)
{
long
txns
=
countUncheckpointedTxns
(
)
;
lastEditLogCheckTime
=
now
;
if
(
txns
>=
checkpointConf
.
getTxnCount
(
)
)
shouldCheckpoint
=
true
;
}
if
(
shouldCheckpoint
)
{
doCheckpoint
(
)
;
lastCheckpointTime
=
now
;
lastEditLogCheckTime
=
now
;
}
}
catch
(
IOException
e
)
{
try
{
long
now
=
monotonicNow
(
)
;
boolean
shouldCheckpoint
=
false
;
if
(
now
>=
lastCheckpointTime
+
checkpointPeriodMSec
)
{
shouldCheckpoint
=
true
;
}
else
if
(
now
>=
lastEditLogCheckTime
+
periodMSec
)
{
long
txns
=
countUncheckpointedTxns
(
)
;
lastEditLogCheckTime
=
now
;
if
(
txns
>=
checkpointConf
.
getTxnCount
(
)
)
shouldCheckpoint
=
true
;
}
if
(
shouldCheckpoint
)
{
doCheckpoint
(
)
;
lastCheckpointTime
=
now
;
lastEditLogCheckTime
=
now
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
NNStorage
bnStorage
=
bnImage
.
getStorage
(
)
;
long
startTime
=
monotonicNow
(
)
;
bnImage
.
freezeNamespaceAtNextRoll
(
)
;
NamenodeCommand
cmd
=
getRemoteNamenodeProxy
(
)
.
startCheckpoint
(
backupNode
.
getRegistration
(
)
)
;
CheckpointCommand
cpCmd
=
null
;
switch
(
cmd
.
getAction
(
)
)
{
case
NamenodeProtocol
.
ACT_SHUTDOWN
:
shutdown
(
)
;
throw
new
IOException
(
+
backupNode
.
nnRpcAddress
+
)
;
case
NamenodeProtocol
.
ACT_CHECKPOINT
:
cpCmd
=
(
CheckpointCommand
)
cmd
;
break
;
default
:
throw
new
IOException
(
+
cmd
.
getAction
(
)
)
;
}
bnImage
.
waitUntilNamespaceFrozen
(
)
;
CheckpointSignature
sig
=
cpCmd
.
getSignature
(
)
;
sig
.
validateStorageInfo
(
bnImage
)
;
long
lastApplied
=
bnImage
.
getLastAppliedTxId
(
)
;
case
NamenodeProtocol
.
ACT_SHUTDOWN
:
shutdown
(
)
;
throw
new
IOException
(
+
backupNode
.
nnRpcAddress
+
)
;
case
NamenodeProtocol
.
ACT_CHECKPOINT
:
cpCmd
=
(
CheckpointCommand
)
cmd
;
break
;
default
:
throw
new
IOException
(
+
cmd
.
getAction
(
)
)
;
}
bnImage
.
waitUntilNamespaceFrozen
(
)
;
CheckpointSignature
sig
=
cpCmd
.
getSignature
(
)
;
sig
.
validateStorageInfo
(
bnImage
)
;
long
lastApplied
=
bnImage
.
getLastAppliedTxId
(
)
;
LOG
.
debug
(
+
lastApplied
)
;
RemoteEditLogManifest
manifest
=
getRemoteNamenodeProxy
(
)
.
getEditLogManifest
(
bnImage
.
getLastAppliedTxId
(
)
+
1
)
;
boolean
needReloadImage
=
false
;
if
(
!
manifest
.
getLogs
(
)
.
isEmpty
(
)
)
{
RemoteEditLog
firstRemoteLog
=
manifest
.
getLogs
(
)
.
get
(
0
)
;
if
(
firstRemoteLog
.
getStartTxId
(
)
>
lastApplied
+
1
)
{
LOG
.
debug
(
+
lastApplied
)
;
RemoteEditLogManifest
manifest
=
getRemoteNamenodeProxy
(
)
.
getEditLogManifest
(
bnImage
.
getLastAppliedTxId
(
)
+
1
)
;
boolean
needReloadImage
=
false
;
if
(
!
manifest
.
getLogs
(
)
.
isEmpty
(
)
)
{
RemoteEditLog
firstRemoteLog
=
manifest
.
getLogs
(
)
.
get
(
0
)
;
if
(
firstRemoteLog
.
getStartTxId
(
)
>
lastApplied
+
1
)
{
LOG
.
info
(
+
+
sig
.
mostRecentCheckpointTxId
)
;
MD5Hash
downloadedHash
=
TransferFsImage
.
downloadImageToStorage
(
backupNode
.
nnHttpAddress
,
sig
.
mostRecentCheckpointTxId
,
bnStorage
,
true
,
false
)
;
bnImage
.
saveDigestAndRenameCheckpointImage
(
NameNodeFile
.
IMAGE
,
sig
.
mostRecentCheckpointTxId
,
downloadedHash
)
;
lastApplied
=
sig
.
mostRecentCheckpointTxId
;
needReloadImage
=
true
;
}
if
(
firstRemoteLog
.
getStartTxId
(
)
>
lastApplied
+
1
)
{
throw
new
IOException
(
+
lastApplied
)
;
}
for
(
RemoteEditLog
log
:
manifest
.
getLogs
(
)
)
{
TransferFsImage
.
downloadEditsToStorage
(
backupNode
.
nnHttpAddress
,
log
,
bnStorage
)
;
init
(
true
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
this
,
e
)
;
if
(
skipBrokenEdits
)
{
return
null
;
}
Throwables
.
propagateIfPossible
(
e
,
IOException
.
class
)
;
}
Preconditions
.
checkState
(
state
!=
State
.
UNINIT
)
;
return
nextOpImpl
(
skipBrokenEdits
)
;
case
OPEN
:
op
=
reader
.
readOp
(
skipBrokenEdits
)
;
if
(
(
op
!=
null
)
&&
(
op
.
hasTransactionId
(
)
)
)
{
long
txId
=
op
.
getTransactionId
(
)
;
if
(
(
txId
>=
lastTxId
)
&&
(
lastTxId
!=
HdfsServerConstants
.
INVALID_TXID
)
)
{
long
skipAmt
=
log
.
length
(
)
-
tracker
.
getPos
(
)
;
if
(
skipAmt
>
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
size
=
fc
.
size
(
)
;
int
bufSize
=
doubleBuf
.
getReadyBuf
(
)
.
getLength
(
)
;
long
need
=
bufSize
-
(
size
-
position
)
;
if
(
need
<=
0
)
{
return
;
}
long
oldSize
=
size
;
long
total
=
0
;
long
fillCapacity
=
fill
.
capacity
(
)
;
while
(
need
>
0
)
{
fill
.
position
(
0
)
;
IOUtils
.
writeFully
(
fc
,
fill
,
size
)
;
need
-=
fillCapacity
;
size
+=
fillCapacity
;
total
+=
fillCapacity
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
void
updateCountForQuota
(
int
initThreads
)
{
writeLock
(
)
;
try
{
int
threads
=
(
initThreads
<
1
)
?
1
:
initThreads
;
void
updateCountForQuota
(
int
initThreads
)
{
writeLock
(
)
;
try
{
int
threads
=
(
initThreads
<
1
)
?
1
:
initThreads
;
LOG
.
info
(
+
threads
+
)
;
long
start
=
Time
.
monotonicNow
(
)
;
QuotaCounts
counts
=
new
QuotaCounts
.
Builder
(
)
.
build
(
)
;
ForkJoinPool
p
=
new
ForkJoinPool
(
threads
)
;
RecursiveAction
task
=
new
InitQuotaTask
(
getBlockStoragePolicySuite
(
)
,
rootDir
.
getStoragePolicyID
(
)
,
rootDir
,
counts
)
;
p
.
execute
(
task
)
;
task
.
join
(
)
;
p
.
shutdown
(
)
;
private
void
copyINodeDefaultAcl
(
INode
child
,
FsPermission
modes
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
static
FSEditLog
newInstance
(
Configuration
conf
,
NNStorage
storage
,
List
<
URI
>
editsDirs
)
{
boolean
asyncEditLogging
=
conf
.
getBoolean
(
DFSConfigKeys
.
DFS_NAMENODE_EDITS_ASYNC_LOGGING
,
DFSConfigKeys
.
DFS_NAMENODE_EDITS_ASYNC_LOGGING_DEFAULT
)
;
void
logSyncAll
(
)
{
long
lastWrittenTxId
=
getLastWrittenTxId
(
)
;
void
logSyncAll
(
)
{
long
lastWrittenTxId
=
getLastWrittenTxId
(
)
;
LOG
.
info
(
+
lastWrittenTxId
+
+
synctxid
+
+
txid
)
;
logSync
(
lastWrittenTxId
)
;
lastWrittenTxId
=
getLastWrittenTxId
(
)
;
try
{
wait
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
}
}
if
(
mytxid
<=
synctxid
)
{
return
;
}
editsBatchedInSync
=
txid
-
synctxid
-
1
;
syncStart
=
txid
;
isSyncRunning
=
true
;
sync
=
true
;
try
{
if
(
journalSet
.
isEmpty
(
)
)
{
throw
new
IOException
(
)
;
}
editLogStream
.
setReadyToFlush
(
)
;
}
catch
(
IOException
e
)
{
final
String
msg
=
+
+
e
.
getMessage
(
)
+
+
+
(
txid
-
synctxid
)
;
if
(
journalSet
.
isEmpty
(
)
)
{
throw
new
IOException
(
)
;
}
editLogStream
.
setReadyToFlush
(
)
;
}
catch
(
IOException
e
)
{
final
String
msg
=
+
+
e
.
getMessage
(
)
+
+
+
(
txid
-
synctxid
)
;
LOG
.
error
(
msg
,
new
Exception
(
)
)
;
synchronized
(
journalSetLock
)
{
IOUtils
.
cleanupWithLogger
(
LOG
,
journalSet
)
;
}
terminate
(
1
,
msg
)
;
}
}
finally
{
doneWithAutoSyncScheduling
(
)
;
}
logStream
=
editLogStream
;
}
long
start
=
monotonicNow
(
)
;
try
{
if
(
logStream
!=
null
)
{
public
synchronized
void
startLogSegment
(
long
txid
,
boolean
abortCurrentLogSegment
,
int
layoutVersion
)
throws
IOException
{
synchronized
void
registerBackupNode
(
NamenodeRegistration
bnReg
,
NamenodeRegistration
nnReg
)
throws
IOException
{
if
(
bnReg
.
isRole
(
NamenodeRole
.
CHECKPOINT
)
)
return
;
JournalManager
jas
=
findBackupJournal
(
bnReg
)
;
if
(
jas
!=
null
)
{
synchronized
void
releaseBackupStream
(
NamenodeRegistration
registration
)
throws
IOException
{
BackupJournalManager
bjm
=
this
.
findBackupJournal
(
registration
)
;
if
(
bjm
!=
null
)
{
@
Override
public
void
logSync
(
)
{
Edit
edit
=
THREAD_EDIT
.
get
(
)
;
if
(
edit
!=
null
)
{
THREAD_EDIT
.
set
(
null
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
enqueueEdit
(
Edit
edit
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
terminate
(
Throwable
t
)
{
String
message
=
+
t
.
getMessage
(
)
;
void
format
(
FSNamesystem
fsn
,
String
clusterId
,
boolean
force
)
throws
IOException
{
long
fileCount
=
fsn
.
getFilesTotal
(
)
;
Preconditions
.
checkState
(
fileCount
==
1
,
+
fileCount
+
)
;
NamespaceInfo
ns
=
NNStorage
.
newNamespaceInfo
(
)
;
for
(
Iterator
<
StorageDirectory
>
it
=
storage
.
dirIterator
(
false
)
;
it
.
hasNext
(
)
;
)
{
StorageDirectory
sd
=
it
.
next
(
)
;
if
(
!
NNUpgradeUtil
.
canRollBack
(
sd
,
storage
,
prevState
.
getStorage
(
)
,
HdfsServerConstants
.
NAMENODE_LAYOUT_VERSION
)
)
{
continue
;
}
LOG
.
info
(
+
sd
)
;
canRollback
=
true
;
}
if
(
fsns
.
isHaEnabled
(
)
)
{
editLog
.
initJournalsForWrite
(
)
;
boolean
canRollBackSharedEditLog
=
editLog
.
canRollBackSharedLog
(
prevState
.
getStorage
(
)
,
HdfsServerConstants
.
NAMENODE_LAYOUT_VERSION
)
;
if
(
canRollBackSharedEditLog
)
{
LOG
.
info
(
)
;
canRollback
=
true
;
}
}
if
(
!
canRollback
)
throw
new
IOException
(
+
)
;
for
(
Iterator
<
StorageDirectory
>
it
=
storage
.
dirIterator
(
false
)
;
it
.
hasNext
(
)
;
)
{
StorageDirectory
sd
=
it
.
next
(
)
;
void
reloadFromImageFile
(
File
file
,
FSNamesystem
target
)
throws
IOException
{
target
.
clear
(
)
;
}
else
{
editStreams
=
FSImagePreTransactionalStorageInspector
.
getEditLogStreams
(
storage
)
;
}
int
maxOpSize
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_NAMENODE_MAX_OP_SIZE_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_MAX_OP_SIZE_DEFAULT
)
;
for
(
EditLogInputStream
elis
:
editStreams
)
{
elis
.
setMaxOpSize
(
maxOpSize
)
;
}
for
(
EditLogInputStream
l
:
editStreams
)
{
LOG
.
debug
(
+
l
)
;
}
if
(
!
editStreams
.
iterator
(
)
.
hasNext
(
)
)
{
LOG
.
info
(
)
;
}
FSImageFile
imageFile
=
null
;
for
(
int
i
=
0
;
i
<
imageFiles
.
size
(
)
;
i
++
)
{
try
{
imageFile
=
imageFiles
.
get
(
i
)
;
loadFSImageFile
(
target
,
recovery
,
imageFile
,
startOpt
)
;
break
;
void
loadFSImageFile
(
FSNamesystem
target
,
MetaRecoveryContext
recovery
,
FSImageFile
imageFile
,
StartupOption
startupOption
)
throws
IOException
{
void
saveFSImage
(
SaveNamespaceContext
context
,
StorageDirectory
sd
,
NameNodeFile
dstType
)
throws
IOException
{
long
txid
=
context
.
getTxId
(
)
;
File
newFile
=
NNStorage
.
getStorageFile
(
sd
,
NameNodeFile
.
IMAGE_NEW
,
txid
)
;
File
dstFile
=
NNStorage
.
getStorageFile
(
sd
,
dstType
,
txid
)
;
FSImageFormatProtobuf
.
Saver
saver
=
new
FSImageFormatProtobuf
.
Saver
(
context
,
conf
)
;
FSImageCompression
compression
=
FSImageCompression
.
createCompression
(
conf
)
;
long
numErrors
=
saver
.
save
(
newFile
,
compression
)
;
if
(
numErrors
>
0
)
{
private
void
renameImageFileInDir
(
StorageDirectory
sd
,
NameNodeFile
fromNnf
,
NameNodeFile
toNnf
,
long
txid
,
boolean
renameMD5
)
throws
IOException
{
final
File
fromFile
=
NNStorage
.
getStorageFile
(
sd
,
fromNnf
,
txid
)
;
final
File
toFile
=
NNStorage
.
getStorageFile
(
sd
,
toNnf
,
txid
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
NamenodeCommand
startCheckpoint
(
NamenodeRegistration
bnReg
,
NamenodeRegistration
nnReg
,
int
layoutVersion
)
throws
IOException
{
LOG
.
info
(
+
getEditLog
(
)
.
getLastWrittenTxId
(
)
)
;
String
msg
=
null
;
if
(
bnReg
.
getNamespaceID
(
)
!=
storage
.
getNamespaceID
(
)
)
msg
=
+
bnReg
.
getAddress
(
)
+
+
bnReg
.
getNamespaceID
(
)
+
+
storage
.
getNamespaceID
(
)
;
else
if
(
bnReg
.
isRole
(
NamenodeRole
.
NAMENODE
)
)
msg
=
+
bnReg
.
getAddress
(
)
+
+
bnReg
.
getRole
(
)
+
;
else
if
(
bnReg
.
getLayoutVersion
(
)
<
storage
.
getLayoutVersion
(
)
||
(
bnReg
.
getLayoutVersion
(
)
==
storage
.
getLayoutVersion
(
)
&&
bnReg
.
getCTime
(
)
>
storage
.
getCTime
(
)
)
)
msg
=
+
bnReg
.
getAddress
(
)
+
+
bnReg
.
getLayoutVersion
(
)
+
+
bnReg
.
getCTime
(
)
+
+
storage
.
getLayoutVersion
(
)
+
+
storage
.
getCTime
(
)
;
if
(
msg
!=
null
)
{
private
static
void
setRenameReservedMapInternal
(
String
renameReserved
)
{
Collection
<
String
>
pairs
=
StringUtils
.
getTrimmedStringCollection
(
renameReserved
)
;
for
(
String
p
:
pairs
)
{
String
[
]
pair
=
StringUtils
.
split
(
p
,
'/'
,
'='
)
;
Preconditions
.
checkArgument
(
pair
.
length
==
2
,
+
p
)
;
String
key
=
pair
[
0
]
;
String
value
=
pair
[
1
]
;
Preconditions
.
checkArgument
(
DFSUtil
.
isReservedPathComponent
(
key
)
,
+
key
)
;
Preconditions
.
checkArgument
(
DFSUtil
.
isValidNameForComponent
(
value
)
,
+
key
+
+
value
)
;
static
String
renameReservedPathsOnUpgrade
(
String
path
,
final
int
layoutVersion
)
throws
IllegalReservedPathException
{
final
String
oldPath
=
path
;
if
(
!
NameNodeLayoutVersion
.
supports
(
Feature
.
ADD_INODE_ID
,
layoutVersion
)
)
{
String
[
]
components
=
INode
.
getPathNames
(
path
)
;
if
(
components
.
length
>
1
)
{
components
[
1
]
=
DFSUtil
.
bytes2String
(
renameReservedRootComponentOnUpgrade
(
DFSUtil
.
string2Bytes
(
components
[
1
]
)
,
layoutVersion
)
)
;
path
=
DFSUtil
.
strings2PathString
(
components
)
;
}
}
if
(
!
NameNodeLayoutVersion
.
supports
(
Feature
.
SNAPSHOT
,
layoutVersion
)
)
{
String
[
]
components
=
INode
.
getPathNames
(
path
)
;
if
(
components
.
length
==
0
)
{
return
path
;
}
for
(
int
i
=
0
;
i
<
components
.
length
;
i
++
)
{
components
[
i
]
=
DFSUtil
.
bytes2String
(
renameReservedComponentOnUpgrade
(
DFSUtil
.
string2Bytes
(
components
[
i
]
)
,
layoutVersion
)
)
;
}
path
=
DFSUtil
.
strings2PathString
(
components
)
;
}
if
(
!
path
.
equals
(
oldPath
)
)
{
@
Override
List
<
FSImageFile
>
getLatestImages
(
)
throws
IOException
{
if
(
latestNameSD
==
null
)
throw
new
IOException
(
+
imageDirs
)
;
if
(
latestEditsSD
==
null
)
throw
new
IOException
(
+
editsDirs
)
;
if
(
latestNameCheckpointTime
>
latestEditsCheckpointTime
&&
latestNameSD
!=
latestEditsSD
&&
latestNameSD
.
getStorageDirType
(
)
==
NameNodeDirType
.
IMAGE
&&
latestEditsSD
.
getStorageDirType
(
)
==
NameNodeDirType
.
EDITS
)
{
LOG
.
error
(
)
;
@
Override
public
void
inspectDirectory
(
StorageDirectory
sd
)
throws
IOException
{
if
(
!
sd
.
getVersionFile
(
)
.
exists
(
)
)
{
needToSave
|=
true
;
return
;
}
try
{
maxSeenTxId
=
Math
.
max
(
maxSeenTxId
,
NNStorage
.
readTransactionIdFile
(
sd
)
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
sd
,
ioe
)
;
return
;
}
File
currentDir
=
sd
.
getCurrentDir
(
)
;
File
filesInStorage
[
]
;
try
{
filesInStorage
=
FileUtil
.
listFiles
(
currentDir
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
currentDir
,
ioe
)
;
return
;
}
for
(
File
f
:
filesInStorage
)
{
}
File
currentDir
=
sd
.
getCurrentDir
(
)
;
File
filesInStorage
[
]
;
try
{
filesInStorage
=
FileUtil
.
listFiles
(
currentDir
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
currentDir
,
ioe
)
;
return
;
}
for
(
File
f
:
filesInStorage
)
{
LOG
.
debug
(
+
f
)
;
String
name
=
f
.
getName
(
)
;
Matcher
imageMatch
=
this
.
matchPattern
(
name
)
;
if
(
imageMatch
!=
null
)
{
if
(
sd
.
getStorageDirType
(
)
.
isOfType
(
NameNodeDirType
.
IMAGE
)
)
{
try
{
long
txid
=
Long
.
parseLong
(
imageMatch
.
group
(
1
)
)
;
static
FSNamesystem
loadFromDisk
(
Configuration
conf
)
throws
IOException
{
checkConfiguration
(
conf
)
;
FSImage
fsImage
=
new
FSImage
(
conf
,
FSNamesystem
.
getNamespaceDirs
(
conf
)
,
FSNamesystem
.
getNamespaceEditsDirs
(
conf
)
)
;
FSNamesystem
namesystem
=
new
FSNamesystem
(
conf
,
fsImage
,
false
)
;
StartupOption
startOpt
=
NameNode
.
getStartupOption
(
conf
)
;
if
(
startOpt
==
StartupOption
.
RECOVER
)
{
namesystem
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_ENTER
)
;
}
long
loadStart
=
monotonicNow
(
)
;
try
{
namesystem
.
loadFSImage
(
startOpt
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
,
ioe
)
;
fsImage
.
close
(
)
;
throw
ioe
;
}
long
timeTakenToLoadFSImage
=
monotonicNow
(
)
-
loadStart
;
@
VisibleForTesting
static
RetryCache
initRetryCache
(
Configuration
conf
)
{
boolean
enable
=
conf
.
getBoolean
(
DFS_NAMENODE_ENABLE_RETRY_CACHE_KEY
,
DFS_NAMENODE_ENABLE_RETRY_CACHE_DEFAULT
)
;
@
VisibleForTesting
static
RetryCache
initRetryCache
(
Configuration
conf
)
{
boolean
enable
=
conf
.
getBoolean
(
DFS_NAMENODE_ENABLE_RETRY_CACHE_KEY
,
DFS_NAMENODE_ENABLE_RETRY_CACHE_DEFAULT
)
;
LOG
.
info
(
+
(
enable
?
:
)
)
;
if
(
enable
)
{
float
heapPercent
=
conf
.
getFloat
(
DFS_NAMENODE_RETRY_CACHE_HEAP_PERCENT_KEY
,
DFS_NAMENODE_RETRY_CACHE_HEAP_PERCENT_DEFAULT
)
;
long
entryExpiryMillis
=
conf
.
getLong
(
DFS_NAMENODE_RETRY_CACHE_EXPIRYTIME_MILLIS_KEY
,
DFS_NAMENODE_RETRY_CACHE_EXPIRYTIME_MILLIS_DEFAULT
)
;
List
<
AuditLogger
>
auditLoggers
=
Lists
.
newArrayList
(
)
;
boolean
topAuditLoggerAdded
=
false
;
if
(
alClasses
!=
null
&&
!
alClasses
.
isEmpty
(
)
)
{
for
(
String
className
:
alClasses
)
{
try
{
AuditLogger
logger
;
if
(
DFS_NAMENODE_DEFAULT_AUDIT_LOGGER_NAME
.
equals
(
className
)
)
{
logger
=
new
FSNamesystemAuditLogger
(
)
;
}
else
{
logger
=
(
AuditLogger
)
Class
.
forName
(
className
)
.
newInstance
(
)
;
if
(
TopAuditLogger
.
class
.
getName
(
)
.
equals
(
logger
.
getClass
(
)
.
getName
(
)
)
)
{
topAuditLoggerAdded
=
true
;
}
}
logger
.
initialize
(
conf
)
;
auditLoggers
.
add
(
logger
)
;
}
catch
(
InstantiationException
e
)
{
editLog
.
initJournalsForWrite
(
)
;
editLog
.
recoverUnclosedStreams
(
)
;
LOG
.
info
(
+
)
;
editLogTailer
.
catchupDuringFailover
(
)
;
blockManager
.
setPostponeBlocksFromFuture
(
false
)
;
blockManager
.
getDatanodeManager
(
)
.
markAllDatanodesStale
(
)
;
blockManager
.
clearQueues
(
)
;
blockManager
.
processAllPendingDNMessages
(
)
;
blockManager
.
getBlockIdManager
(
)
.
applyImpendingGenerationStamp
(
)
;
if
(
!
isInSafeMode
(
)
)
{
LOG
.
info
(
)
;
blockManager
.
initializeReplQueues
(
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
metaSaveAsString
(
)
)
;
}
long
nextTxId
=
getFSImage
(
)
.
getLastAppliedTxId
(
)
+
1
;
void
startStandbyServices
(
final
Configuration
conf
,
boolean
isObserver
)
throws
IOException
{
void
stopStandbyServices
(
)
throws
IOException
{
HAServiceState
curState
=
getState
(
)
==
OBSERVER
?
OBSERVER
:
STANDBY
;
CryptoProtocolVersion
chooseProtocolVersion
(
EncryptionZone
zone
,
CryptoProtocolVersion
[
]
supportedVersions
)
throws
UnknownCryptoProtocolVersionException
,
UnresolvedLinkException
,
SnapshotAccessControlException
{
Preconditions
.
checkNotNull
(
zone
)
;
Preconditions
.
checkNotNull
(
supportedVersions
)
;
final
CryptoProtocolVersion
required
=
zone
.
getVersion
(
)
;
for
(
CryptoProtocolVersion
c
:
supportedVersions
)
{
if
(
c
.
equals
(
CryptoProtocolVersion
.
UNKNOWN
)
)
{
private
boolean
checkBlocksComplete
(
String
src
,
boolean
allowCommittedBlock
,
BlockInfo
...
blocks
)
{
final
int
n
=
allowCommittedBlock
?
numCommittedAllowed
:
0
;
for
(
int
i
=
0
;
i
<
blocks
.
length
;
i
++
)
{
final
short
min
=
blockManager
.
getMinStorageNum
(
blocks
[
i
]
)
;
final
String
err
=
INodeFile
.
checkBlockComplete
(
blocks
,
i
,
n
,
min
)
;
if
(
err
!=
null
)
{
final
int
numNodes
=
blocks
[
i
]
.
numNodes
(
)
;
void
commitBlockSynchronization
(
ExtendedBlock
oldBlock
,
long
newgenerationstamp
,
long
newlength
,
boolean
closeFile
,
boolean
deleteblock
,
DatanodeID
[
]
newtargets
,
String
[
]
newtargetstorages
)
throws
IOException
{
void
commitBlockSynchronization
(
ExtendedBlock
oldBlock
,
long
newgenerationstamp
,
long
newlength
,
boolean
closeFile
,
boolean
deleteblock
,
DatanodeID
[
]
newtargets
,
String
[
]
newtargetstorages
)
throws
IOException
{
LOG
.
info
(
+
oldBlock
+
+
newgenerationstamp
+
+
newlength
+
+
Arrays
.
asList
(
newtargets
)
+
+
closeFile
+
+
deleteblock
+
)
;
checkOperation
(
OperationCategory
.
WRITE
)
;
final
String
src
;
writeLock
(
)
;
boolean
copyTruncate
=
false
;
BlockInfo
truncatedBlock
=
null
;
try
{
checkOperation
(
OperationCategory
.
WRITE
)
;
checkNameNodeSafeMode
(
)
;
final
BlockInfo
storedBlock
=
getStoredBlock
(
ExtendedBlock
.
getLocalBlock
(
oldBlock
)
)
;
if
(
storedBlock
==
null
)
{
if
(
deleteblock
)
{
if
(
deleteblock
)
{
LOG
.
debug
(
,
oldBlock
)
;
return
;
}
else
{
throw
new
IOException
(
+
oldBlock
+
)
;
}
}
final
long
oldGenerationStamp
=
storedBlock
.
getGenerationStamp
(
)
;
final
long
oldNumBytes
=
storedBlock
.
getNumBytes
(
)
;
if
(
storedBlock
.
isDeleted
(
)
)
{
throw
new
IOException
(
+
storedBlock
+
+
)
;
}
final
INodeFile
iFile
=
getBlockCollection
(
storedBlock
)
;
src
=
iFile
.
getFullPathName
(
)
;
if
(
isFileDeleted
(
iFile
)
)
{
throw
new
FileNotFoundException
(
+
src
+
)
;
}
if
(
(
!
iFile
.
isUnderConstruction
(
)
||
storedBlock
.
isComplete
(
)
)
&&
iFile
.
getLastBlock
(
)
.
isComplete
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
dsInfos
[
i
]
.
addBlock
(
truncatedBlock
,
truncatedBlock
)
;
}
else
{
Block
bi
=
new
Block
(
storedBlock
)
;
if
(
storedBlock
.
isStriped
(
)
)
{
bi
.
setBlockId
(
bi
.
getBlockId
(
)
+
i
)
;
}
dsInfos
[
i
]
.
addBlock
(
storedBlock
,
bi
)
;
}
}
}
}
if
(
copyTruncate
)
{
iFile
.
convertLastBlockToUC
(
truncatedBlock
,
dsInfos
)
;
}
else
{
iFile
.
convertLastBlockToUC
(
storedBlock
,
dsInfos
)
;
if
(
closeFile
)
{
blockManager
.
markBlockReplicasAsCorrupt
(
oldBlock
.
getLocalBlock
(
)
,
storedBlock
,
oldGenerationStamp
,
oldNumBytes
,
dsInfos
)
;
}
}
}
if
(
closeFile
)
{
if
(
copyTruncate
)
{
closeFileCommitBlocks
(
src
,
iFile
,
truncatedBlock
)
;
else
{
Block
bi
=
new
Block
(
storedBlock
)
;
if
(
storedBlock
.
isStriped
(
)
)
{
bi
.
setBlockId
(
bi
.
getBlockId
(
)
+
i
)
;
}
dsInfos
[
i
]
.
addBlock
(
storedBlock
,
bi
)
;
}
}
}
}
if
(
copyTruncate
)
{
iFile
.
convertLastBlockToUC
(
truncatedBlock
,
dsInfos
)
;
}
else
{
iFile
.
convertLastBlockToUC
(
storedBlock
,
dsInfos
)
;
if
(
closeFile
)
{
blockManager
.
markBlockReplicasAsCorrupt
(
oldBlock
.
getLocalBlock
(
)
,
storedBlock
,
oldGenerationStamp
,
oldNumBytes
,
dsInfos
)
;
}
}
}
if
(
closeFile
)
{
if
(
copyTruncate
)
{
closeFileCommitBlocks
(
src
,
iFile
,
truncatedBlock
)
;
if
(
!
iFile
.
isBlockInLatestSnapshot
(
storedBlock
)
)
{
LinkedHashMap
<
Integer
,
HdfsPartialListing
>
listings
=
Maps
.
newLinkedHashMap
(
)
;
DirectoryListing
lastListing
=
null
;
int
numEntries
=
0
;
for
(
;
srcsIndex
<
srcs
.
length
;
srcsIndex
++
)
{
String
src
=
srcs
[
srcsIndex
]
;
HdfsPartialListing
listing
;
try
{
DirectoryListing
dirListing
=
getListingInt
(
dir
,
pc
,
src
,
indexStartAfter
,
needLocation
)
;
if
(
dirListing
==
null
)
{
throw
new
FileNotFoundException
(
+
src
+
)
;
}
listing
=
new
HdfsPartialListing
(
srcsIndex
,
Lists
.
newArrayList
(
dirListing
.
getPartialListing
(
)
)
)
;
numEntries
+=
listing
.
getPartialListing
(
)
.
size
(
)
;
lastListing
=
dirListing
;
}
catch
(
Exception
e
)
{
if
(
e
instanceof
AccessControlException
)
{
CheckpointSignature
rollEditLog
(
)
throws
IOException
{
String
operationName
=
;
CheckpointSignature
result
=
null
;
checkSuperuserPrivilege
(
operationName
)
;
checkOperation
(
OperationCategory
.
JOURNAL
)
;
writeLock
(
)
;
try
{
checkOperation
(
OperationCategory
.
JOURNAL
)
;
checkNameNodeSafeMode
(
)
;
if
(
Server
.
isRpcInvocation
(
)
)
{
NamenodeCommand
startCheckpoint
(
NamenodeRegistration
backupNode
,
NamenodeRegistration
activeNamenode
)
throws
IOException
{
checkOperation
(
OperationCategory
.
CHECKPOINT
)
;
writeLock
(
)
;
try
{
checkOperation
(
OperationCategory
.
CHECKPOINT
)
;
checkNameNodeSafeMode
(
)
;
void
endCheckpoint
(
NamenodeRegistration
registration
,
CheckpointSignature
sig
)
throws
IOException
{
checkOperation
(
OperationCategory
.
CHECKPOINT
)
;
readLock
(
)
;
try
{
checkOperation
(
OperationCategory
.
CHECKPOINT
)
;
checkNameNodeSafeMode
(
)
;
boolean
disableErasureCodingPolicy
(
String
ecPolicyName
,
final
boolean
logRetryCache
)
throws
IOException
{
final
String
operationName
=
;
checkOperation
(
OperationCategory
.
WRITE
)
;
checkErasureCodingSupported
(
operationName
)
;
boolean
success
=
false
;
void
checkPermission
(
INodesInPath
inodesInPath
,
boolean
doCheckOwner
,
FsAction
ancestorAccess
,
FsAction
parentAccess
,
FsAction
access
,
FsAction
subAccess
,
boolean
ignoreEmptyDir
)
throws
AccessControlException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
checkNotSymlink
(
INode
inode
,
byte
[
]
[
]
components
,
int
i
)
throws
UnresolvedPathException
{
if
(
inode
!=
null
&&
inode
.
isSymlink
(
)
)
{
final
int
last
=
components
.
length
-
1
;
final
String
path
=
getPath
(
components
,
0
,
last
)
;
final
String
preceding
=
getPath
(
components
,
0
,
i
-
1
)
;
final
String
remainder
=
getPath
(
components
,
i
+
1
,
last
)
;
final
String
target
=
inode
.
asSymlink
(
)
.
getSymlinkString
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
final
String
link
=
inode
.
getLocalName
(
)
;
@
Override
synchronized
public
void
finalizeLogSegment
(
long
firstTxId
,
long
lastTxId
)
throws
IOException
{
File
inprogressFile
=
NNStorage
.
getInProgressEditsFile
(
sd
,
firstTxId
)
;
File
dstFile
=
NNStorage
.
getFinalizedEditsFile
(
sd
,
firstTxId
,
lastTxId
)
;
@
Override
public
void
purgeLogsOlderThan
(
long
minTxIdToKeep
)
throws
IOException
{
private
void
discardEditLogSegments
(
long
startTxId
)
throws
IOException
{
File
currentDir
=
sd
.
getCurrentDir
(
)
;
List
<
EditLogFile
>
allLogFiles
=
matchEditLogs
(
currentDir
)
;
List
<
EditLogFile
>
toTrash
=
Lists
.
newArrayList
(
)
;
if
(
editsMatch
.
matches
(
)
)
{
try
{
long
startTxId
=
Long
.
parseLong
(
editsMatch
.
group
(
1
)
)
;
long
endTxId
=
Long
.
parseLong
(
editsMatch
.
group
(
2
)
)
;
ret
.
add
(
new
EditLogFile
(
f
,
startTxId
,
endTxId
)
)
;
continue
;
}
catch
(
NumberFormatException
nfe
)
{
LOG
.
error
(
+
f
+
+
)
;
}
}
Matcher
inProgressEditsMatch
=
EDITS_INPROGRESS_REGEX
.
matcher
(
name
)
;
if
(
inProgressEditsMatch
.
matches
(
)
)
{
try
{
long
startTxId
=
Long
.
parseLong
(
inProgressEditsMatch
.
group
(
1
)
)
;
ret
.
add
(
new
EditLogFile
(
f
,
startTxId
,
HdfsServerConstants
.
INVALID_TXID
,
true
)
)
;
continue
;
}
catch
(
NumberFormatException
nfe
)
{
}
}
Matcher
inProgressEditsMatch
=
EDITS_INPROGRESS_REGEX
.
matcher
(
name
)
;
if
(
inProgressEditsMatch
.
matches
(
)
)
{
try
{
long
startTxId
=
Long
.
parseLong
(
inProgressEditsMatch
.
group
(
1
)
)
;
ret
.
add
(
new
EditLogFile
(
f
,
startTxId
,
HdfsServerConstants
.
INVALID_TXID
,
true
)
)
;
continue
;
}
catch
(
NumberFormatException
nfe
)
{
LOG
.
error
(
+
f
+
+
)
;
}
}
if
(
forPurging
)
{
Matcher
staleInprogressEditsMatch
=
EDITS_INPROGRESS_STALE_REGEX
.
matcher
(
name
)
;
if
(
staleInprogressEditsMatch
.
matches
(
)
)
{
try
{
long
startTxId
=
Long
.
parseLong
(
staleInprogressEditsMatch
.
group
(
1
)
)
;
ret
.
add
(
new
EditLogFile
(
f
,
startTxId
,
HdfsServerConstants
.
INVALID_TXID
,
true
)
)
;
continue
;
@
Override
synchronized
public
void
selectInputStreams
(
Collection
<
EditLogInputStream
>
streams
,
long
fromTxId
,
boolean
inProgressOk
,
boolean
onlyDurableTxns
)
throws
IOException
{
List
<
EditLogFile
>
elfs
=
matchEditLogs
(
sd
.
getCurrentDir
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
synchronized
public
void
recoverUnfinalizedSegments
(
)
throws
IOException
{
File
currentDir
=
sd
.
getCurrentDir
(
)
;
List
<
EditLogFile
>
allLogFiles
=
matchEditLogs
(
currentDir
)
;
for
(
EditLogFile
elf
:
allLogFiles
)
{
if
(
elf
.
getFile
(
)
.
equals
(
currentInProgress
)
)
{
continue
;
}
if
(
elf
.
isInProgress
(
)
)
{
if
(
elf
.
getFile
(
)
.
length
(
)
==
0
)
{
LOG
.
info
(
+
elf
)
;
if
(
!
elf
.
getFile
(
)
.
delete
(
)
)
{
throw
new
IOException
(
+
elf
.
getFile
(
)
)
;
}
continue
;
}
elf
.
scanLog
(
getLastReadableTxId
(
)
,
true
)
;
if
(
elf
.
hasCorruptHeader
(
)
)
{
elf
.
moveAsideCorruptFile
(
)
;
throw
new
CorruptionException
(
+
elf
)
;
}
if
(
elf
.
getLastTxId
(
)
==
HdfsServerConstants
.
INVALID_TXID
)
{
int
run
(
Configuration
conf
,
AtomicInteger
errorCount
)
throws
Exception
{
final
int
initCount
=
errorCount
.
get
(
)
;
LOG
.
info
(
Util
.
memoryInfo
(
)
)
;
initConf
(
conf
)
;
final
FSNamesystem
namesystem
=
checkINodeReference
(
conf
,
errorCount
)
;
INodeMapValidation
.
run
(
namesystem
.
getFSDirectory
(
)
,
errorCount
)
;
FSNamesystem
checkINodeReference
(
Configuration
conf
,
AtomicInteger
errorCount
)
throws
Exception
{
INodeReferenceValidation
.
start
(
)
;
final
FSNamesystem
namesystem
=
loadImage
(
conf
)
;
LOG
.
info
(
Util
.
memoryInfo
(
)
)
;
INodeReferenceValidation
.
end
(
errorCount
)
;
return
false
;
}
Set
<
String
>
validRequestors
=
new
HashSet
<
String
>
(
)
;
validRequestors
.
add
(
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSUtilClient
.
getNNAddress
(
conf
)
.
getHostName
(
)
)
)
;
try
{
validRequestors
.
add
(
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
SecondaryNameNode
.
getHttpAddress
(
conf
)
.
getHostName
(
)
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
debug
(
,
e
)
;
String
msg
=
String
.
format
(
,
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
,
conf
.
get
(
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT
)
)
;
LOG
.
warn
(
msg
)
;
}
if
(
HAUtil
.
isHAEnabled
(
conf
,
DFSUtil
.
getNamenodeNameServiceId
(
conf
)
)
)
{
List
<
Configuration
>
otherNnConfs
=
HAUtil
.
getConfForOtherNodes
(
conf
)
;
for
(
Configuration
otherNnConf
:
otherNnConfs
)
{
validRequestors
.
add
(
SecurityUtil
.
getServerPrincipal
(
otherNnConf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSUtilClient
.
getNNAddress
(
otherNnConf
)
.
getHostName
(
)
)
)
;
}
}
for
(
String
v
:
validRequestors
)
{
if
(
v
!=
null
&&
v
.
equals
(
remoteUser
)
)
{
validRequestors
.
add
(
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSUtilClient
.
getNNAddress
(
conf
)
.
getHostName
(
)
)
)
;
try
{
validRequestors
.
add
(
SecurityUtil
.
getServerPrincipal
(
conf
.
get
(
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
SecondaryNameNode
.
getHttpAddress
(
conf
)
.
getHostName
(
)
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
debug
(
,
e
)
;
String
msg
=
String
.
format
(
,
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
,
conf
.
get
(
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT
)
)
;
LOG
.
warn
(
msg
)
;
}
if
(
HAUtil
.
isHAEnabled
(
conf
,
DFSUtil
.
getNamenodeNameServiceId
(
conf
)
)
)
{
List
<
Configuration
>
otherNnConfs
=
HAUtil
.
getConfForOtherNodes
(
conf
)
;
for
(
Configuration
otherNnConf
:
otherNnConfs
)
{
validRequestors
.
add
(
SecurityUtil
.
getServerPrincipal
(
otherNnConf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSUtilClient
.
getNNAddress
(
otherNnConf
)
.
getHostName
(
)
)
)
;
}
}
for
(
String
v
:
validRequestors
)
{
if
(
v
!=
null
&&
v
.
equals
(
remoteUser
)
)
{
LOG
.
info
(
+
remoteUser
)
;
return
true
;
}
catch
(
Exception
e
)
{
LOG
.
debug
(
,
e
)
;
String
msg
=
String
.
format
(
,
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
,
conf
.
get
(
DFSConfigKeys
.
DFS_SECONDARY_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
conf
.
getTrimmed
(
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_DEFAULT
)
)
;
LOG
.
warn
(
msg
)
;
}
if
(
HAUtil
.
isHAEnabled
(
conf
,
DFSUtil
.
getNamenodeNameServiceId
(
conf
)
)
)
{
List
<
Configuration
>
otherNnConfs
=
HAUtil
.
getConfForOtherNodes
(
conf
)
;
for
(
Configuration
otherNnConf
:
otherNnConfs
)
{
validRequestors
.
add
(
SecurityUtil
.
getServerPrincipal
(
otherNnConf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY
)
,
DFSUtilClient
.
getNNAddress
(
otherNnConf
)
.
getHostName
(
)
)
)
;
}
}
for
(
String
v
:
validRequestors
)
{
if
(
v
!=
null
&&
v
.
equals
(
remoteUser
)
)
{
LOG
.
info
(
+
remoteUser
)
;
return
true
;
}
}
if
(
HttpServer2
.
userHasAdministratorAccess
(
context
,
remoteUser
)
)
{
LOG
.
info
(
+
remoteUser
)
;
return
true
;
response
.
sendError
(
HttpServletResponse
.
SC_CONFLICT
,
+
+
larger
.
last
(
)
)
;
return
null
;
}
if
(
!
currentlyDownloadingCheckpoints
.
add
(
imageRequest
)
)
{
response
.
sendError
(
HttpServletResponse
.
SC_CONFLICT
,
+
+
+
txid
)
;
return
null
;
}
long
now
=
System
.
currentTimeMillis
(
)
;
long
lastCheckpointTime
=
nnImage
.
getStorage
(
)
.
getMostRecentCheckpointTime
(
)
;
long
lastCheckpointTxid
=
nnImage
.
getStorage
(
)
.
getMostRecentCheckpointTxId
(
)
;
long
checkpointPeriod
=
conf
.
getTimeDuration
(
DFS_NAMENODE_CHECKPOINT_PERIOD_KEY
,
DFS_NAMENODE_CHECKPOINT_PERIOD_DEFAULT
,
TimeUnit
.
SECONDS
)
;
checkpointPeriod
=
Math
.
round
(
checkpointPeriod
*
recentImageCheckTimePrecision
)
;
long
checkpointTxnCount
=
conf
.
getLong
(
DFS_NAMENODE_CHECKPOINT_TXNS_KEY
,
DFS_NAMENODE_CHECKPOINT_TXNS_DEFAULT
)
;
long
timeDelta
=
TimeUnit
.
MILLISECONDS
.
toSeconds
(
now
-
lastCheckpointTime
)
;
if
(
checkRecentImageEnable
&&
NameNodeFile
.
IMAGE
.
equals
(
parsedParams
.
getNameNodeFile
(
)
)
&&
timeDelta
<
checkpointPeriod
&&
txid
-
lastCheckpointTxid
<
checkpointTxnCount
)
{
String
message
=
+
+
timeDelta
+
+
checkpointPeriod
+
+
(
txid
-
lastCheckpointTxid
)
+
+
checkpointTxnCount
;
LOG
.
info
(
message
)
;
@
Override
public
void
selectInputStreams
(
Collection
<
EditLogInputStream
>
streams
,
long
fromTxId
,
boolean
inProgressOk
,
boolean
onlyDurableTxns
)
{
final
PriorityQueue
<
EditLogInputStream
>
allStreams
=
new
PriorityQueue
<
EditLogInputStream
>
(
64
,
EDIT_LOG_INPUT_STREAM_COMPARATOR
)
;
for
(
JournalAndStream
jas
:
journals
)
{
if
(
jas
.
isDisabled
(
)
)
{
for
(
JournalAndStream
jas
:
journals
)
{
try
{
closure
.
apply
(
jas
)
;
}
catch
(
Throwable
t
)
{
if
(
jas
.
isRequired
(
)
)
{
final
String
msg
=
+
status
+
+
jas
+
;
LOG
.
error
(
msg
,
t
)
;
abortAllJournals
(
)
;
terminate
(
1
,
msg
)
;
}
else
{
LOG
.
error
(
+
status
+
+
jas
+
,
t
)
;
badJAS
.
add
(
jas
)
;
}
}
}
disableAndReportErrorOnJournals
(
badJAS
)
;
if
(
!
NameNodeResourcePolicy
.
areResourcesAvailable
(
journals
,
minimumRedundantJournals
)
)
{
String
message
=
status
+
;
}
}
}
final
Map
<
Long
,
List
<
RemoteEditLog
>>
logsByStartTxId
=
new
HashMap
<
>
(
)
;
allLogs
.
forEach
(
input
->
{
long
key
=
RemoteEditLog
.
GET_START_TXID
.
apply
(
input
)
;
logsByStartTxId
.
computeIfAbsent
(
key
,
k
->
new
ArrayList
<
>
(
)
)
.
add
(
input
)
;
}
)
;
long
curStartTxId
=
fromTxId
;
List
<
RemoteEditLog
>
logs
=
new
ArrayList
<
>
(
)
;
while
(
true
)
{
List
<
RemoteEditLog
>
logGroup
=
logsByStartTxId
.
getOrDefault
(
curStartTxId
,
Collections
.
emptyList
(
)
)
;
if
(
logGroup
.
isEmpty
(
)
)
{
SortedSet
<
Long
>
startTxIds
=
Sets
.
newTreeSet
(
logsByStartTxId
.
keySet
(
)
)
;
startTxIds
=
startTxIds
.
tailSet
(
curStartTxId
)
;
if
(
startTxIds
.
isEmpty
(
)
)
{
break
;
}
else
{
long
curStartTxId
=
fromTxId
;
List
<
RemoteEditLog
>
logs
=
new
ArrayList
<
>
(
)
;
while
(
true
)
{
List
<
RemoteEditLog
>
logGroup
=
logsByStartTxId
.
getOrDefault
(
curStartTxId
,
Collections
.
emptyList
(
)
)
;
if
(
logGroup
.
isEmpty
(
)
)
{
SortedSet
<
Long
>
startTxIds
=
Sets
.
newTreeSet
(
logsByStartTxId
.
keySet
(
)
)
;
startTxIds
=
startTxIds
.
tailSet
(
curStartTxId
)
;
if
(
startTxIds
.
isEmpty
(
)
)
{
break
;
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
curStartTxId
+
+
)
;
}
logs
.
clear
(
)
;
curStartTxId
=
startTxIds
.
first
(
)
;
continue
;
private
synchronized
void
removeLease
(
Lease
lease
,
long
inodeId
)
{
leasesById
.
remove
(
inodeId
)
;
if
(
!
lease
.
removeFile
(
inodeId
)
)
{
private
void
format
(
StorageDirectory
sd
)
throws
IOException
{
sd
.
clearDirectory
(
)
;
writeProperties
(
sd
)
;
writeTransactionIdFile
(
sd
,
0
)
;
private
void
reportErrorsOnDirectory
(
StorageDirectory
sd
)
{
private
void
reportErrorsOnDirectory
(
StorageDirectory
sd
)
{
LOG
.
error
(
,
sd
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
lsd
=
listStorageDirectories
(
)
;
static
boolean
canRollBack
(
StorageDirectory
sd
,
StorageInfo
storage
,
StorageInfo
prevStorage
,
int
targetLayoutVersion
)
throws
IOException
{
File
prevDir
=
sd
.
getPreviousDir
(
)
;
if
(
!
prevDir
.
exists
(
)
)
{
static
void
doFinalize
(
StorageDirectory
sd
)
throws
IOException
{
File
prevDir
=
sd
.
getPreviousDir
(
)
;
if
(
!
prevDir
.
exists
(
)
)
{
static
void
doFinalize
(
StorageDirectory
sd
)
throws
IOException
{
File
prevDir
=
sd
.
getPreviousDir
(
)
;
if
(
!
prevDir
.
exists
(
)
)
{
LOG
.
info
(
+
prevDir
+
)
;
static
void
doPreUpgrade
(
Configuration
conf
,
StorageDirectory
sd
)
throws
IOException
{
public
static
void
doUpgrade
(
StorageDirectory
sd
,
Storage
storage
)
throws
IOException
{
public
static
void
setServiceAddress
(
Configuration
conf
,
String
address
)
{
void
setRpcLifelineServerAddress
(
Configuration
conf
,
InetSocketAddress
lifelineRPCAddress
)
{
httpServer
.
setAliasMap
(
levelDBAliasMapServer
.
getAliasMap
(
)
)
;
}
}
rpcServer
.
start
(
)
;
try
{
plugins
=
conf
.
getInstances
(
DFS_NAMENODE_PLUGINS_KEY
,
ServicePlugin
.
class
)
;
}
catch
(
RuntimeException
e
)
{
String
pluginsValue
=
conf
.
get
(
DFS_NAMENODE_PLUGINS_KEY
)
;
LOG
.
error
(
+
pluginsValue
,
e
)
;
throw
e
;
}
for
(
ServicePlugin
p
:
plugins
)
{
try
{
p
.
start
(
this
)
;
}
catch
(
Throwable
t
)
{
LOG
.
warn
(
+
p
+
,
t
)
;
}
}
LOG
.
info
(
getRole
(
)
+
+
getNameNodeAddress
(
)
)
;
if
(
rpcServer
.
getServiceRpcAddress
(
)
!=
null
)
{
private
static
boolean
initializeSharedEdits
(
Configuration
conf
,
boolean
force
,
boolean
interactive
)
throws
IOException
{
String
nsId
=
DFSUtil
.
getNamenodeNameServiceId
(
conf
)
;
String
namenodeId
=
HAUtil
.
getNameNodeId
(
conf
,
nsId
)
;
initializeGenericKeys
(
conf
,
nsId
,
namenodeId
)
;
if
(
conf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_SHARED_EDITS_DIR_KEY
)
==
null
)
{
private
static
void
copyEditLogSegmentsToSharedDir
(
FSNamesystem
fsns
,
Collection
<
URI
>
sharedEditsDirs
,
NNStorage
newSharedStorage
,
Configuration
conf
)
throws
IOException
{
Preconditions
.
checkArgument
(
!
sharedEditsDirs
.
isEmpty
(
)
,
)
;
List
<
URI
>
sharedEditsUris
=
new
ArrayList
<
URI
>
(
sharedEditsDirs
)
;
FSEditLog
newSharedEditLog
=
new
FSEditLog
(
conf
,
newSharedStorage
,
sharedEditsUris
)
;
newSharedEditLog
.
initJournalsForWrite
(
)
;
newSharedEditLog
.
recoverUnclosedStreams
(
)
;
FSEditLog
sourceEditLog
=
fsns
.
getFSImage
(
)
.
editLog
;
long
fromTxId
=
fsns
.
getFSImage
(
)
.
getMostRecentCheckpointTxId
(
)
;
Collection
<
EditLogInputStream
>
streams
=
null
;
try
{
streams
=
sourceEditLog
.
selectInputStreams
(
fromTxId
+
1
,
0
)
;
newSharedEditLog
.
setNextTxId
(
fromTxId
+
1
)
;
for
(
EditLogInputStream
stream
:
streams
)
{
try
{
streams
=
sourceEditLog
.
selectInputStreams
(
fromTxId
+
1
,
0
)
;
newSharedEditLog
.
setNextTxId
(
fromTxId
+
1
)
;
for
(
EditLogInputStream
stream
:
streams
)
{
LOG
.
debug
(
,
stream
)
;
FSEditLogOp
op
;
boolean
segmentOpen
=
false
;
while
(
(
op
=
stream
.
readOp
(
)
)
!=
null
)
{
LOG
.
trace
(
,
op
)
;
if
(
!
segmentOpen
)
{
newSharedEditLog
.
startLogSegment
(
op
.
txid
,
false
,
fsns
.
getEffectiveLayoutVersion
(
)
)
;
segmentOpen
=
true
;
}
newSharedEditLog
.
logEdit
(
op
)
;
if
(
op
.
opCode
==
FSEditLogOpCodes
.
OP_END_LOG_SEGMENT
)
{
newSharedEditLog
.
endCurrentLogSegment
(
false
)
;
for
(
EditLogInputStream
stream
:
streams
)
{
LOG
.
debug
(
,
stream
)
;
FSEditLogOp
op
;
boolean
segmentOpen
=
false
;
while
(
(
op
=
stream
.
readOp
(
)
)
!=
null
)
{
LOG
.
trace
(
,
op
)
;
if
(
!
segmentOpen
)
{
newSharedEditLog
.
startLogSegment
(
op
.
txid
,
false
,
fsns
.
getEffectiveLayoutVersion
(
)
)
;
segmentOpen
=
true
;
}
newSharedEditLog
.
logEdit
(
op
)
;
if
(
op
.
opCode
==
FSEditLogOpCodes
.
OP_END_LOG_SEGMENT
)
{
newSharedEditLog
.
endCurrentLogSegment
(
false
)
;
LOG
.
debug
(
,
stream
)
;
segmentOpen
=
false
;
}
}
if
(
segmentOpen
)
{
startOpt
=
StartupOption
.
BACKUP
;
}
else
if
(
StartupOption
.
CHECKPOINT
.
getName
(
)
.
equalsIgnoreCase
(
cmd
)
)
{
startOpt
=
StartupOption
.
CHECKPOINT
;
}
else
if
(
StartupOption
.
OBSERVER
.
getName
(
)
.
equalsIgnoreCase
(
cmd
)
)
{
startOpt
=
StartupOption
.
OBSERVER
;
}
else
if
(
StartupOption
.
UPGRADE
.
getName
(
)
.
equalsIgnoreCase
(
cmd
)
||
StartupOption
.
UPGRADEONLY
.
getName
(
)
.
equalsIgnoreCase
(
cmd
)
)
{
startOpt
=
StartupOption
.
UPGRADE
.
getName
(
)
.
equalsIgnoreCase
(
cmd
)
?
StartupOption
.
UPGRADE
:
StartupOption
.
UPGRADEONLY
;
while
(
i
+
1
<
argsLen
)
{
String
flag
=
args
[
i
+
1
]
;
if
(
flag
.
equalsIgnoreCase
(
StartupOption
.
CLUSTERID
.
getName
(
)
)
)
{
if
(
i
+
2
<
argsLen
)
{
i
+=
2
;
startOpt
.
setClusterId
(
args
[
i
]
)
;
}
else
{
LOG
.
error
(
+
StartupOption
.
CLUSTERID
.
getName
(
)
+
)
;
public
static
NameNode
createNameNode
(
String
argv
[
]
,
Configuration
conf
)
throws
IOException
{
LOG
.
info
(
+
Arrays
.
asList
(
argv
)
)
;
if
(
conf
==
null
)
conf
=
new
HdfsConfiguration
(
)
;
GenericOptionsParser
hParser
=
new
GenericOptionsParser
(
conf
,
argv
)
;
argv
=
hParser
.
getRemainingArgs
(
)
;
StartupOption
startOpt
=
parseArguments
(
argv
)
;
if
(
startOpt
==
null
)
{
printUsage
(
System
.
err
)
;
return
null
;
}
setStartupOption
(
conf
,
startOpt
)
;
boolean
aborted
=
false
;
switch
(
startOpt
)
{
case
FORMAT
:
aborted
=
format
(
conf
,
startOpt
.
getForceFormat
(
)
,
startOpt
.
getInteractiveFormat
(
)
)
;
terminate
(
aborted
?
1
:
0
)
;
return
null
;
case
GENCLUSTERID
:
String
clusterID
=
NNStorage
.
newClusterID
(
)
;
protected
synchronized
void
doImmediateShutdown
(
Throwable
t
)
throws
ExitException
{
try
{
@
Override
public
void
errorReport
(
NamenodeRegistration
registration
,
int
errorCode
,
String
msg
)
throws
IOException
{
checkNNStartup
(
)
;
namesystem
.
checkOperation
(
OperationCategory
.
UNCHECKED
)
;
namesystem
.
checkSuperuserPrivilege
(
)
;
verifyRequest
(
registration
)
;
@
Override
public
HdfsFileStatus
create
(
String
src
,
FsPermission
masked
,
String
clientName
,
EnumSetWritable
<
CreateFlag
>
flag
,
boolean
createParent
,
short
replication
,
long
blockSize
,
CryptoProtocolVersion
[
]
supportedVersions
,
String
ecPolicyName
,
String
storagePolicy
)
throws
IOException
{
checkNNStartup
(
)
;
String
clientMachine
=
getClientMachine
(
)
;
if
(
stateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
LastBlockWithStatus
append
(
String
src
,
String
clientName
,
EnumSetWritable
<
CreateFlag
>
flag
)
throws
IOException
{
checkNNStartup
(
)
;
String
clientMachine
=
getClientMachine
(
)
;
if
(
stateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
LocatedBlock
getAdditionalDatanode
(
final
String
src
,
final
long
fileId
,
final
ExtendedBlock
blk
,
final
DatanodeInfo
[
]
existings
,
final
String
[
]
existingStorageIDs
,
final
DatanodeInfo
[
]
excludes
,
final
int
numAdditionalNodes
,
final
String
clientName
)
throws
IOException
{
checkNNStartup
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Deprecated
@
Override
public
boolean
rename
(
String
src
,
String
dst
)
throws
IOException
{
checkNNStartup
(
)
;
if
(
stateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
void
rename2
(
String
src
,
String
dst
,
Options
.
Rename
...
options
)
throws
IOException
{
checkNNStartup
(
)
;
if
(
stateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
boolean
truncate
(
String
src
,
long
newLength
,
String
clientName
)
throws
IOException
{
checkNNStartup
(
)
;
@
Override
public
boolean
delete
(
String
src
,
boolean
recursive
)
throws
IOException
{
checkNNStartup
(
)
;
if
(
stateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
boolean
mkdirs
(
String
src
,
FsPermission
masked
,
boolean
createParent
)
throws
IOException
{
checkNNStartup
(
)
;
if
(
stateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
RollingUpgradeInfo
rollingUpgrade
(
RollingUpgradeAction
action
)
throws
IOException
{
checkNNStartup
(
)
;
@
Override
public
DatanodeCommand
blockReport
(
final
DatanodeRegistration
nodeReg
,
String
poolId
,
final
StorageBlockReport
[
]
reports
,
final
BlockReportContext
context
)
throws
IOException
{
checkNNStartup
(
)
;
verifyRequest
(
nodeReg
)
;
if
(
blockStateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
DatanodeCommand
cacheReport
(
DatanodeRegistration
nodeReg
,
String
poolId
,
List
<
Long
>
blockIds
)
throws
IOException
{
checkNNStartup
(
)
;
verifyRequest
(
nodeReg
)
;
if
(
blockStateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
void
blockReceivedAndDeleted
(
final
DatanodeRegistration
nodeReg
,
String
poolId
,
StorageReceivedDeletedBlocks
[
]
receivedAndDeletedBlocks
)
throws
IOException
{
checkNNStartup
(
)
;
verifyRequest
(
nodeReg
)
;
metrics
.
incrBlockReceivedAndDeletedOps
(
)
;
if
(
blockStateChangeLog
.
isDebugEnabled
(
)
)
{
@
Override
public
void
errorReport
(
DatanodeRegistration
nodeReg
,
int
errorCode
,
String
msg
)
throws
IOException
{
checkNNStartup
(
)
;
String
dnName
=
(
nodeReg
==
null
)
?
:
nodeReg
.
toString
(
)
;
if
(
errorCode
==
DatanodeProtocol
.
NOTIFY
)
{
@
Override
public
String
[
]
getGroupsForUser
(
String
user
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
dnVersion
=
dnReg
.
getSoftwareVersion
(
)
;
if
(
VersionUtil
.
compareVersions
(
dnVersion
,
minimumDataNodeVersion
)
<
0
)
{
IncorrectVersionException
ive
=
new
IncorrectVersionException
(
minimumDataNodeVersion
,
dnVersion
,
,
)
;
LOG
.
warn
(
ive
.
getMessage
(
)
+
+
dnReg
)
;
throw
ive
;
}
String
nnVersion
=
VersionInfo
.
getVersion
(
)
;
if
(
!
dnVersion
.
equals
(
nnVersion
)
)
{
String
messagePrefix
=
+
dnVersion
+
+
dnReg
+
+
nnVersion
+
;
long
nnCTime
=
nn
.
getFSImage
(
)
.
getStorage
(
)
.
getCTime
(
)
;
long
dnCTime
=
dnReg
.
getStorageInfo
(
)
.
getCTime
(
)
;
if
(
nnCTime
!=
dnCTime
)
{
IncorrectVersionException
ive
=
new
IncorrectVersionException
(
messagePrefix
+
+
dnCTime
+
+
nnCTime
+
)
;
LOG
.
warn
(
ive
.
toString
(
)
,
ive
)
;
throw
ive
;
}
else
{
sb
.
append
(
+
UserGroupInformation
.
getCurrentUser
(
)
+
+
remoteAddress
+
+
new
Date
(
)
)
;
out
.
println
(
sb
)
;
sb
.
append
(
)
;
for
(
String
blk
:
blocks
)
{
if
(
blk
==
null
||
!
blk
.
contains
(
Block
.
BLOCK_FILE_PREFIX
)
)
{
out
.
println
(
+
blk
)
;
continue
;
}
out
.
print
(
)
;
blockIdCK
(
blk
)
;
sb
.
append
(
blk
+
)
;
}
LOG
.
info
(
,
sb
.
toString
(
)
)
;
namenode
.
getNamesystem
(
)
.
logFsckEvent
(
,
remoteAddress
)
;
out
.
flush
(
)
;
return
;
}
String
msg
=
+
UserGroupInformation
.
getCurrentUser
(
)
+
+
remoteAddress
+
+
path
+
+
new
Date
(
)
;
private
void
deleteCorruptedFile
(
String
path
)
{
try
{
namenode
.
getRpcServer
(
)
.
delete
(
path
,
true
)
;
throw
new
IOException
(
+
target
)
;
}
int
chain
=
0
;
boolean
copyError
=
false
;
for
(
LocatedBlock
lBlk
:
blocks
.
getLocatedBlocks
(
)
)
{
LocatedBlock
lblock
=
lBlk
;
DatanodeInfo
[
]
locs
=
lblock
.
getLocations
(
)
;
if
(
locs
==
null
||
locs
.
length
==
0
)
{
if
(
fos
!=
null
)
{
fos
.
flush
(
)
;
fos
.
close
(
)
;
fos
=
null
;
}
continue
;
}
if
(
fos
==
null
)
{
fos
=
dfs
.
create
(
target
+
+
chain
,
true
)
;
chain
++
;
fos
=
null
;
}
continue
;
}
if
(
fos
==
null
)
{
fos
=
dfs
.
create
(
target
+
+
chain
,
true
)
;
chain
++
;
}
try
{
copyBlock
(
dfs
,
lblock
,
fos
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
lblock
.
getBlock
(
)
+
+
target
,
e
)
;
fos
.
flush
(
)
;
fos
.
close
(
)
;
fos
=
null
;
internalError
=
true
;
copyError
=
true
;
}
}
if
(
copyError
)
{
deadNodes
.
clear
(
)
;
failures
++
;
continue
;
}
try
{
String
file
=
BlockReaderFactory
.
getFileName
(
targetAddr
,
block
.
getBlockPoolId
(
)
,
block
.
getBlockId
(
)
)
;
blockReader
=
new
BlockReaderFactory
(
dfs
.
getConf
(
)
)
.
setFileName
(
file
)
.
setBlock
(
block
)
.
setBlockToken
(
lblock
.
getBlockToken
(
)
)
.
setStartOffset
(
0
)
.
setLength
(
block
.
getNumBytes
(
)
)
.
setVerifyChecksum
(
true
)
.
setClientName
(
)
.
setDatanodeInfo
(
chosenNode
)
.
setInetSocketAddress
(
targetAddr
)
.
setCachingStrategy
(
CachingStrategy
.
newDropBehind
(
)
)
.
setClientCacheContext
(
dfs
.
getClientContext
(
)
)
.
setConfiguration
(
namenode
.
getConf
(
)
)
.
setRemotePeerFactory
(
new
RemotePeerFactory
(
)
{
@
Override
public
Peer
newConnectedPeer
(
InetSocketAddress
addr
,
Token
<
BlockTokenIdentifier
>
blockToken
,
DatanodeID
datanodeId
)
throws
IOException
{
Peer
peer
=
null
;
Socket
s
=
NetUtils
.
getDefaultSocketFactory
(
conf
)
.
createSocket
(
)
;
try
{
s
.
connect
(
addr
,
HdfsConstants
.
READ_TIMEOUT
)
;
s
.
setSoTimeout
(
HdfsConstants
.
READ_TIMEOUT
)
;
peer
=
DFSUtilClient
.
peerFromSocketAndKey
(
dfs
.
getSaslDataTransferClient
(
)
,
s
,
NamenodeFsck
.
this
,
blockToken
,
datanodeId
,
HdfsConstants
.
READ_TIMEOUT
)
;
}
finally
{
if
(
peer
==
null
)
{
if
(
op
==
null
)
{
state
=
State
.
EOF
;
if
(
streams
[
curIdx
]
.
getLastTxId
(
)
==
prevTxId
)
{
return
null
;
}
else
{
throw
new
PrematureEOFException
(
+
+
prevTxId
+
+
streams
[
curIdx
]
.
getLastTxId
(
)
)
;
}
}
prevTxId
=
op
.
getTransactionId
(
)
;
return
op
;
}
catch
(
IOException
e
)
{
prevException
=
e
;
state
=
State
.
STREAM_FAILED
;
}
break
;
case
STREAM_FAILED
:
if
(
curIdx
+
1
==
streams
.
length
)
{
throw
prevException
;
}
long
oldLast
=
streams
[
curIdx
]
.
getLastTxId
(
)
;
continue
;
}
LOG
.
info
(
,
zoneId
,
getReencryptionStatus
(
)
)
;
getReencryptionStatus
(
)
.
markZoneStarted
(
zoneId
)
;
resetSubmissionTracker
(
zoneId
)
;
}
finally
{
dir
.
getFSNamesystem
(
)
.
readUnlock
(
)
;
}
try
{
reencryptEncryptionZone
(
zoneId
)
;
}
catch
(
RetriableException
|
SafeModeException
re
)
{
LOG
.
info
(
,
re
)
;
getReencryptionStatus
(
)
.
markZoneForRetry
(
zoneId
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
,
zoneId
,
ioe
)
;
}
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
)
;
final
Future
<
ReencryptionTask
>
completed
=
batchService
.
take
(
)
;
throttle
(
)
;
checkPauseForTesting
(
)
;
if
(
completed
.
isCancelled
(
)
)
{
LOG
.
debug
(
)
;
return
;
}
final
ReencryptionTask
task
=
completed
.
get
(
)
;
boolean
shouldRetry
;
do
{
dir
.
getFSNamesystem
(
)
.
writeLock
(
)
;
try
{
throttleTimerLocked
.
start
(
)
;
processTask
(
task
)
;
shouldRetry
=
false
;
}
catch
(
RetriableException
|
SafeModeException
re
)
{
shouldRun
=
true
;
nameNodeAddr
=
NameNode
.
getServiceAddress
(
conf
,
true
)
;
this
.
conf
=
conf
;
this
.
namenode
=
NameNodeProxies
.
createNonHAProxy
(
conf
,
nameNodeAddr
,
NamenodeProtocol
.
class
,
UserGroupInformation
.
getCurrentUser
(
)
,
true
)
.
getProxy
(
)
;
fsName
=
getInfoServer
(
)
;
checkpointDirs
=
FSImage
.
getCheckpointDirs
(
conf
,
)
;
checkpointEditsDirs
=
FSImage
.
getCheckpointEditsDirs
(
conf
,
)
;
checkpointImage
=
new
CheckpointStorage
(
conf
,
checkpointDirs
,
checkpointEditsDirs
)
;
checkpointImage
.
recoverCreate
(
commandLineOpts
.
shouldFormat
(
)
)
;
checkpointImage
.
deleteTempEdits
(
)
;
namesystem
=
new
FSNamesystem
(
conf
,
checkpointImage
,
true
)
;
namesystem
.
dir
.
disableQuotaChecks
(
)
;
checkpointConf
=
new
CheckpointConf
(
conf
)
;
nameNodeStatusBeanName
=
MBeans
.
register
(
,
,
this
)
;
legacyOivImageDir
=
conf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_LEGACY_OIV_IMAGE_DIR_KEY
)
;
while
(
shouldRun
)
{
try
{
Thread
.
sleep
(
1000
*
period
)
;
}
catch
(
InterruptedException
ie
)
{
}
if
(
!
shouldRun
)
{
break
;
}
try
{
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
UserGroupInformation
.
getCurrentUser
(
)
.
checkTGTAndReloginFromKeytab
(
)
;
final
long
monotonicNow
=
Time
.
monotonicNow
(
)
;
final
long
now
=
Time
.
now
(
)
;
if
(
shouldCheckpointBasedOnCount
(
)
||
monotonicNow
>=
lastCheckpointTime
+
1000
*
checkpointConf
.
getPeriod
(
)
)
{
doCheckpoint
(
)
;
lastCheckpointTime
=
monotonicNow
;
lastCheckpointWallclockTime
=
now
;
}
}
catch
(
IOException
e
)
{
}
catch
(
InterruptedException
ie
)
{
}
if
(
!
shouldRun
)
{
break
;
}
try
{
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
UserGroupInformation
.
getCurrentUser
(
)
.
checkTGTAndReloginFromKeytab
(
)
;
final
long
monotonicNow
=
Time
.
monotonicNow
(
)
;
final
long
now
=
Time
.
now
(
)
;
if
(
shouldCheckpointBasedOnCount
(
)
||
monotonicNow
>=
lastCheckpointTime
+
1000
*
checkpointConf
.
getPeriod
(
)
)
{
doCheckpoint
(
)
;
lastCheckpointTime
=
monotonicNow
;
lastCheckpointWallclockTime
=
now
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
e
.
printStackTrace
(
)
;
if
(
checkpointImage
.
getMergeErrorCount
(
)
>
maxRetries
)
{
if
(
!
shouldRun
)
{
break
;
}
try
{
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
UserGroupInformation
.
getCurrentUser
(
)
.
checkTGTAndReloginFromKeytab
(
)
;
final
long
monotonicNow
=
Time
.
monotonicNow
(
)
;
final
long
now
=
Time
.
now
(
)
;
if
(
shouldCheckpointBasedOnCount
(
)
||
monotonicNow
>=
lastCheckpointTime
+
1000
*
checkpointConf
.
getPeriod
(
)
)
{
doCheckpoint
(
)
;
lastCheckpointTime
=
monotonicNow
;
lastCheckpointWallclockTime
=
now
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
e
.
printStackTrace
(
)
;
if
(
checkpointImage
.
getMergeErrorCount
(
)
>
maxRetries
)
{
LOG
.
error
(
+
checkpointImage
.
getMergeErrorCount
(
)
+
)
;
doCheckpoint
(
)
;
}
else
{
System
.
err
.
println
(
+
count
+
+
+
+
checkpointConf
.
getTxnCount
(
)
+
)
;
System
.
err
.
println
(
)
;
}
break
;
case
GETEDITSIZE
:
long
uncheckpointed
=
countUncheckpointedTxns
(
)
;
System
.
out
.
println
(
+
uncheckpointed
+
)
;
break
;
default
:
throw
new
AssertionError
(
+
opts
.
getCommand
(
)
)
;
}
}
catch
(
RemoteException
e
)
{
exitCode
=
1
;
try
{
String
[
]
content
;
content
=
e
.
getLocalizedMessage
(
)
.
split
(
)
;
LOG
.
error
(
cmd
+
+
content
[
0
]
)
;
System
.
err
.
println
(
+
count
+
+
+
+
checkpointConf
.
getTxnCount
(
)
+
)
;
System
.
err
.
println
(
)
;
}
break
;
case
GETEDITSIZE
:
long
uncheckpointed
=
countUncheckpointedTxns
(
)
;
System
.
out
.
println
(
+
uncheckpointed
+
)
;
break
;
default
:
throw
new
AssertionError
(
+
opts
.
getCommand
(
)
)
;
}
}
catch
(
RemoteException
e
)
{
exitCode
=
1
;
try
{
String
[
]
content
;
content
=
e
.
getLocalizedMessage
(
)
.
split
(
)
;
LOG
.
error
(
cmd
+
+
content
[
0
]
)
;
}
catch
(
Exception
ex
)
{
LOG
.
error
(
cmd
+
+
ex
.
getLocalizedMessage
(
)
)
;
public
static
void
downloadAliasMap
(
URL
fsName
,
File
aliasMap
,
boolean
isBootstrapStandby
)
throws
IOException
{
String
paramString
=
ImageServlet
.
getParamStringForAliasMap
(
isBootstrapStandby
)
;
getFileClient
(
fsName
,
paramString
,
Arrays
.
asList
(
aliasMap
)
,
null
,
false
)
;
}
if
(
CheckpointFaultInjector
.
getInstance
(
)
.
shouldCorruptAByte
(
localfile
)
)
{
LOG
.
warn
(
)
;
buf
[
0
]
++
;
}
out
.
write
(
buf
,
0
,
num
)
;
total
+=
num
;
if
(
throttler
!=
null
)
{
throttler
.
throttle
(
num
,
canceler
)
;
}
}
}
catch
(
EofException
e
)
{
reportStr
+=
;
ioe
=
e
;
out
=
null
;
}
catch
(
IOException
ie
)
{
ioe
=
ie
;
throw
ie
;
}
finally
{
LOG
.
warn
(
)
;
buf
[
0
]
++
;
}
out
.
write
(
buf
,
0
,
num
)
;
total
+=
num
;
if
(
throttler
!=
null
)
{
throttler
.
throttle
(
num
,
canceler
)
;
}
}
}
catch
(
EofException
e
)
{
reportStr
+=
;
ioe
=
e
;
out
=
null
;
}
catch
(
IOException
ie
)
{
ioe
=
ie
;
throw
ie
;
}
finally
{
reportStr
+=
+
total
+
+
num
+
;
static
MD5Hash
getFileClient
(
URL
infoServer
,
String
queryString
,
List
<
File
>
localPaths
,
Storage
dstStorage
,
boolean
getChecksum
)
throws
IOException
{
URL
url
=
new
URL
(
infoServer
,
ImageServlet
.
PATH_SPEC
+
+
queryString
)
;
private
static
void
setTimeout
(
HttpURLConnection
connection
)
{
if
(
timeout
<=
0
)
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
timeout
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_IMAGE_TRANSFER_TIMEOUT_KEY
,
DFSConfigKeys
.
DFS_IMAGE_TRANSFER_TIMEOUT_DEFAULT
)
;
NamenodeProtocol
proxy
=
null
;
NamespaceInfo
nsInfo
=
null
;
boolean
isUpgradeFinalized
=
false
;
RemoteNameNodeInfo
proxyInfo
=
null
;
for
(
int
i
=
0
;
i
<
remoteNNs
.
size
(
)
;
i
++
)
{
proxyInfo
=
remoteNNs
.
get
(
i
)
;
InetSocketAddress
otherIpcAddress
=
proxyInfo
.
getIpcAddress
(
)
;
proxy
=
createNNProtocolProxy
(
otherIpcAddress
)
;
try
{
nsInfo
=
proxy
.
versionRequest
(
)
;
isUpgradeFinalized
=
proxy
.
isUpgradeFinalized
(
)
;
break
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
otherIpcAddress
+
+
ioe
.
getMessage
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
boolean
isUpgradeFinalized
=
false
;
RemoteNameNodeInfo
proxyInfo
=
null
;
for
(
int
i
=
0
;
i
<
remoteNNs
.
size
(
)
;
i
++
)
{
proxyInfo
=
remoteNNs
.
get
(
i
)
;
InetSocketAddress
otherIpcAddress
=
proxyInfo
.
getIpcAddress
(
)
;
proxy
=
createNNProtocolProxy
(
otherIpcAddress
)
;
try
{
nsInfo
=
proxy
.
versionRequest
(
)
;
isUpgradeFinalized
=
proxy
.
isUpgradeFinalized
(
)
;
break
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
otherIpcAddress
+
+
ioe
.
getMessage
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
ioe
)
;
}
}
}
if
(
nsInfo
==
null
)
{
proxyInfo
=
remoteNNs
.
get
(
i
)
;
InetSocketAddress
otherIpcAddress
=
proxyInfo
.
getIpcAddress
(
)
;
proxy
=
createNNProtocolProxy
(
otherIpcAddress
)
;
try
{
nsInfo
=
proxy
.
versionRequest
(
)
;
isUpgradeFinalized
=
proxy
.
isUpgradeFinalized
(
)
;
break
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
otherIpcAddress
+
+
ioe
.
getMessage
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
ioe
)
;
}
}
}
if
(
nsInfo
==
null
)
{
LOG
.
error
(
+
remoteNNs
)
;
return
ERR_CODE_FAILED_CONNECT
;
}
if
(
!
checkLayoutVersion
(
nsInfo
)
)
{
if
(
dataDirStates
.
values
(
)
.
contains
(
StorageState
.
NOT_FORMATTED
)
)
{
isFormatted
=
false
;
System
.
err
.
println
(
)
;
}
}
catch
(
InconsistentFSStateException
e
)
{
LOG
.
warn
(
,
e
)
;
}
finally
{
storage
.
unlockAll
(
)
;
}
if
(
!
isFormatted
&&
!
format
(
storage
,
nsInfo
)
)
{
return
false
;
}
FSImage
.
checkUpgrade
(
storage
)
;
for
(
Iterator
<
StorageDirectory
>
it
=
storage
.
dirIterator
(
false
)
;
it
.
hasNext
(
)
;
)
{
StorageDirectory
sd
=
it
.
next
(
)
;
try
{
NNUpgradeUtil
.
renameCurToTmp
(
sd
)
;
}
catch
(
IOException
e
)
{
private
void
parseConfAndFindOtherNN
(
)
throws
IOException
{
Configuration
conf
=
getConf
(
)
;
nsId
=
DFSUtil
.
getNamenodeNameServiceId
(
conf
)
;
if
(
!
HAUtil
.
isHAEnabled
(
conf
,
nsId
)
)
{
throw
new
HadoopIllegalArgumentException
(
)
;
}
nnId
=
HAUtil
.
getNameNodeId
(
conf
,
nsId
)
;
NameNode
.
initializeGenericKeys
(
conf
,
nsId
,
nnId
)
;
if
(
!
HAUtil
.
usesSharedEditsDir
(
conf
)
)
{
throw
new
HadoopIllegalArgumentException
(
)
;
}
remoteNNs
=
RemoteNameNodeInfo
.
getRemoteNameNodes
(
conf
,
nsId
)
;
List
<
RemoteNameNodeInfo
>
remove
=
new
ArrayList
<
RemoteNameNodeInfo
>
(
remoteNNs
.
size
(
)
)
;
for
(
RemoteNameNodeInfo
info
:
remoteNNs
)
{
InetSocketAddress
address
=
info
.
getIpcAddress
(
)
;
LOG
.
info
(
+
info
.
getNameNodeID
(
)
+
+
info
.
getIpcAddress
(
)
)
;
if
(
address
.
getPort
(
)
==
0
||
address
.
getAddress
(
)
.
isAnyLocalAddress
(
)
)
{
private
int
formatAndDownloadAliasMap
(
String
pathAliasMap
,
RemoteNameNodeInfo
proxyInfo
)
throws
IOException
{
@
VisibleForTesting
public
long
doTailEdits
(
)
throws
IOException
,
InterruptedException
{
namesystem
.
writeLockInterruptibly
(
)
;
try
{
FSImage
image
=
namesystem
.
getFSImage
(
)
;
long
lastTxnId
=
image
.
getLastAppliedTxId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
{
FSImage
image
=
namesystem
.
getFSImage
(
)
;
long
lastTxnId
=
image
.
getLastAppliedTxId
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
lastTxnId
)
;
}
Collection
<
EditLogInputStream
>
streams
;
long
startTime
=
Time
.
monotonicNow
(
)
;
try
{
streams
=
editLog
.
selectInputStreams
(
lastTxnId
+
1
,
0
,
null
,
inProgressOk
,
true
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
,
ioe
)
;
return
0
;
}
finally
{
NameNode
.
getNameNodeMetrics
(
)
.
addEditLogFetchTime
(
Time
.
monotonicNow
(
)
-
startTime
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
streams
=
editLog
.
selectInputStreams
(
lastTxnId
+
1
,
0
,
null
,
inProgressOk
,
true
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
,
ioe
)
;
return
0
;
}
finally
{
NameNode
.
getNameNodeMetrics
(
)
.
addEditLogFetchTime
(
Time
.
monotonicNow
(
)
-
startTime
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
streams
.
size
(
)
)
;
}
long
editsLoaded
=
0
;
try
{
editsLoaded
=
image
.
loadEdits
(
streams
,
namesystem
,
maxTxnsPerLock
,
null
,
null
)
;
}
catch
(
EditLogInputException
elie
)
{
editsLoaded
=
elie
.
getNumEditsLoaded
(
)
;
throw
elie
;
}
finally
{
public
static
void
init
(
int
interval
,
int
maxSkipLevels
,
Logger
log
)
{
DirectoryDiffListFactory
.
skipInterval
=
interval
;
DirectoryDiffListFactory
.
maxLevels
=
maxSkipLevels
;
if
(
maxLevels
>
0
)
{
constructor
=
c
->
new
DiffListBySkipList
(
c
)
;
private
void
gcDeletedSnapshot
(
String
name
)
{
final
Snapshot
.
Root
deleted
;
namesystem
.
readLock
(
)
;
try
{
deleted
=
namesystem
.
getSnapshotManager
(
)
.
chooseDeletedSnapshot
(
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
,
e
)
;
throw
e
;
}
finally
{
namesystem
.
readUnlock
(
)
;
}
if
(
deleted
==
null
)
{
LOG
.
trace
(
,
name
)
;
return
;
}
final
String
snapshotRoot
=
deleted
.
getRootFullPathName
(
)
;
final
String
snapshotName
=
deleted
.
getLocalName
(
)
;
deleted
=
namesystem
.
getSnapshotManager
(
)
.
chooseDeletedSnapshot
(
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
,
e
)
;
throw
e
;
}
finally
{
namesystem
.
readUnlock
(
)
;
}
if
(
deleted
==
null
)
{
LOG
.
trace
(
,
name
)
;
return
;
}
final
String
snapshotRoot
=
deleted
.
getRootFullPathName
(
)
;
final
String
snapshotName
=
deleted
.
getLocalName
(
)
;
LOG
.
info
(
,
name
,
snapshotName
,
snapshotRoot
)
;
try
{
namesystem
.
gcDeletedSnapshot
(
snapshotRoot
,
snapshotName
)
;
}
catch
(
Throwable
e
)
{
if
(
blkLocs
==
null
)
{
return
;
}
for
(
StorageTypeNodePair
dn
:
blkLocs
)
{
boolean
foundDn
=
dn
.
getDatanodeInfo
(
)
.
compareTo
(
reportedDn
)
==
0
?
true
:
false
;
boolean
foundType
=
dn
.
getStorageType
(
)
.
equals
(
type
)
;
if
(
foundDn
&&
foundType
)
{
blkLocs
.
remove
(
dn
)
;
Block
[
]
mFinishedBlocks
=
new
Block
[
1
]
;
mFinishedBlocks
[
0
]
=
reportedBlock
;
context
.
notifyMovementTriedBlocks
(
mFinishedBlocks
)
;
if
(
blkLocs
.
size
(
)
<=
0
)
{
movementFinishedBlocks
.
add
(
reportedBlock
)
;
scheduledBlkLocs
.
remove
(
reportedBlock
)
;
}
return
;
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
VisibleForTesting
void
blocksStorageMovementUnReportedItemsCheck
(
)
{
synchronized
(
storageMovementAttemptedItems
)
{
Iterator
<
AttemptedItemInfo
>
iter
=
storageMovementAttemptedItems
.
iterator
(
)
;
long
now
=
monotonicNow
(
)
;
while
(
iter
.
hasNext
(
)
)
{
AttemptedItemInfo
itemInfo
=
iter
.
next
(
)
;
if
(
now
>
itemInfo
.
getLastAttemptedOrReportedTime
(
)
+
selfRetryTimeout
)
{
long
file
=
itemInfo
.
getFile
(
)
;
ItemInfo
candidate
=
new
ItemInfo
(
itemInfo
.
getStartPath
(
)
,
file
,
itemInfo
.
getRetryCount
(
)
+
1
)
;
blockStorageMovementNeeded
.
add
(
candidate
)
;
iter
.
remove
(
)
;
public
DatanodeMap
getLiveDatanodeStorageReport
(
Context
spsContext
)
throws
IOException
{
long
now
=
Time
.
monotonicNow
(
)
;
long
elapsedTimeMs
=
now
-
lastAccessedTime
;
boolean
refreshNeeded
=
elapsedTimeMs
>=
refreshIntervalMs
;
lastAccessedTime
=
now
;
if
(
refreshNeeded
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
refreshNeeded
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
elapsedTimeMs
,
refreshIntervalMs
)
;
}
datanodeMap
.
reset
(
)
;
DatanodeStorageReport
[
]
liveDns
=
spsContext
.
getLiveDatanodeStorageReport
(
)
;
for
(
DatanodeStorageReport
storage
:
liveDns
)
{
StorageReport
[
]
storageReports
=
storage
.
getStorageReports
(
)
;
List
<
StorageType
>
storageTypes
=
new
ArrayList
<
>
(
)
;
List
<
Long
>
remainingSizeList
=
new
ArrayList
<
>
(
)
;
for
(
StorageReport
t
:
storageReports
)
{
if
(
t
.
getRemaining
(
)
>
0
)
{
storageTypes
.
add
(
t
.
getStorage
(
)
.
getStorageType
(
)
)
;
remainingSizeList
.
add
(
t
.
getRemaining
(
)
)
;
}
}
datanodeMap
.
addTarget
(
storage
.
getDatanodeInfo
(
)
,
storageTypes
,
remainingSizeList
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
synchronized
void
start
(
StoragePolicySatisfierMode
serviceMode
)
{
if
(
serviceMode
==
StoragePolicySatisfierMode
.
NONE
)
{
if
(
itemInfo
.
getRetryCount
(
)
>=
blockMovementMaxRetry
)
{
LOG
.
info
(
+
blockMovementMaxRetry
+
+
itemInfo
.
getFile
(
)
+
)
;
storageMovementNeeded
.
removeItemTrackInfo
(
itemInfo
,
false
)
;
continue
;
}
long
trackId
=
itemInfo
.
getFile
(
)
;
BlocksMovingAnalysis
status
=
null
;
BlockStoragePolicy
existingStoragePolicy
;
HdfsFileStatus
fileStatus
=
ctxt
.
getFileInfo
(
trackId
)
;
if
(
fileStatus
==
null
||
fileStatus
.
isDir
(
)
)
{
storageMovementNeeded
.
removeItemTrackInfo
(
itemInfo
,
true
)
;
}
else
{
byte
existingStoragePolicyID
=
fileStatus
.
getStoragePolicy
(
)
;
existingStoragePolicy
=
ctxt
.
getStoragePolicy
(
existingStoragePolicyID
)
;
HdfsLocatedFileStatus
file
=
(
HdfsLocatedFileStatus
)
fileStatus
;
status
=
analyseBlocksStorageMovementsAndAssignToDN
(
file
,
existingStoragePolicy
)
;
}
long
trackId
=
itemInfo
.
getFile
(
)
;
BlocksMovingAnalysis
status
=
null
;
BlockStoragePolicy
existingStoragePolicy
;
HdfsFileStatus
fileStatus
=
ctxt
.
getFileInfo
(
trackId
)
;
if
(
fileStatus
==
null
||
fileStatus
.
isDir
(
)
)
{
storageMovementNeeded
.
removeItemTrackInfo
(
itemInfo
,
true
)
;
}
else
{
byte
existingStoragePolicyID
=
fileStatus
.
getStoragePolicy
(
)
;
existingStoragePolicy
=
ctxt
.
getStoragePolicy
(
existingStoragePolicyID
)
;
HdfsLocatedFileStatus
file
=
(
HdfsLocatedFileStatus
)
fileStatus
;
status
=
analyseBlocksStorageMovementsAndAssignToDN
(
file
,
existingStoragePolicy
)
;
switch
(
status
.
status
)
{
case
ANALYSIS_SKIPPED_FOR_RETRY
:
case
BLOCKS_TARGETS_PAIRED
:
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
status
.
status
,
fileStatus
.
getFileId
(
)
)
;
}
this
.
storageMovementsMonitor
.
add
(
itemInfo
.
getStartPath
(
)
,
itemInfo
.
getFile
(
)
,
monotonicNow
(
)
,
status
.
assignedBlocks
,
itemInfo
.
getRetryCount
(
)
)
;
HdfsFileStatus
fileStatus
=
ctxt
.
getFileInfo
(
trackId
)
;
if
(
fileStatus
==
null
||
fileStatus
.
isDir
(
)
)
{
storageMovementNeeded
.
removeItemTrackInfo
(
itemInfo
,
true
)
;
}
else
{
byte
existingStoragePolicyID
=
fileStatus
.
getStoragePolicy
(
)
;
existingStoragePolicy
=
ctxt
.
getStoragePolicy
(
existingStoragePolicyID
)
;
HdfsLocatedFileStatus
file
=
(
HdfsLocatedFileStatus
)
fileStatus
;
status
=
analyseBlocksStorageMovementsAndAssignToDN
(
file
,
existingStoragePolicy
)
;
switch
(
status
.
status
)
{
case
ANALYSIS_SKIPPED_FOR_RETRY
:
case
BLOCKS_TARGETS_PAIRED
:
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
status
.
status
,
fileStatus
.
getFileId
(
)
)
;
}
this
.
storageMovementsMonitor
.
add
(
itemInfo
.
getStartPath
(
)
,
itemInfo
.
getFile
(
)
,
monotonicNow
(
)
,
status
.
assignedBlocks
,
itemInfo
.
getRetryCount
(
)
)
;
break
;
case
NO_BLOCKS_TARGETS_PAIRED
:
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
trackId
,
fileStatus
.
getFileId
(
)
)
;
else
{
byte
existingStoragePolicyID
=
fileStatus
.
getStoragePolicy
(
)
;
existingStoragePolicy
=
ctxt
.
getStoragePolicy
(
existingStoragePolicyID
)
;
HdfsLocatedFileStatus
file
=
(
HdfsLocatedFileStatus
)
fileStatus
;
status
=
analyseBlocksStorageMovementsAndAssignToDN
(
file
,
existingStoragePolicy
)
;
switch
(
status
.
status
)
{
case
ANALYSIS_SKIPPED_FOR_RETRY
:
case
BLOCKS_TARGETS_PAIRED
:
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
status
.
status
,
fileStatus
.
getFileId
(
)
)
;
}
this
.
storageMovementsMonitor
.
add
(
itemInfo
.
getStartPath
(
)
,
itemInfo
.
getFile
(
)
,
monotonicNow
(
)
,
status
.
assignedBlocks
,
itemInfo
.
getRetryCount
(
)
)
;
break
;
case
NO_BLOCKS_TARGETS_PAIRED
:
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
trackId
,
fileStatus
.
getFileId
(
)
)
;
}
retryItem
=
true
;
break
;
case
FEW_LOW_REDUNDANCY_BLOCKS
:
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
BlocksMovingAnalysis
analyseBlocksStorageMovementsAndAssignToDN
(
HdfsLocatedFileStatus
fileInfo
,
BlockStoragePolicy
existingStoragePolicy
)
throws
IOException
{
BlocksMovingAnalysis
.
Status
status
=
BlocksMovingAnalysis
.
Status
.
BLOCKS_ALREADY_SATISFIED
;
final
ErasureCodingPolicy
ecPolicy
=
fileInfo
.
getErasureCodingPolicy
(
)
;
final
LocatedBlocks
locatedBlocks
=
fileInfo
.
getLocatedBlocks
(
)
;
final
boolean
lastBlkComplete
=
locatedBlocks
.
isLastBlockComplete
(
)
;
if
(
!
lastBlkComplete
)
{
@
Override
public
void
addFileToProcess
(
ItemInfo
trackInfo
,
boolean
scanCompleted
)
{
storageMovementNeeded
.
add
(
trackInfo
,
scanCompleted
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
start
(
)
{
if
(
!
storagePolicyEnabled
)
{
public
void
changeModeEvent
(
StoragePolicySatisfierMode
newMode
)
{
if
(
!
storagePolicyEnabled
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
mode
,
newMode
)
;
}
switch
(
newMode
)
{
case
EXTERNAL
:
if
(
mode
==
newMode
)
{
LOG
.
info
(
+
,
newMode
)
;
return
;
}
spsService
.
stopGracefully
(
)
;
break
;
case
NONE
:
if
(
mode
==
newMode
)
{
LOG
.
info
(
+
,
newMode
)
;
return
;
}
LOG
.
info
(
,
newMode
)
;
spsService
.
stop
(
true
)
;
clearPathIds
(
)
;
break
;
public
void
verifyOutstandingPathQLimit
(
)
throws
IOException
{
long
size
=
pathsToBeTraveresed
.
size
(
)
;
if
(
outstandingPathsLimit
-
size
<=
0
)
{
private
static
void
logConf
(
Configuration
conf
)
{
LOG
.
info
(
+
DFSConfigKeys
.
NNTOP_BUCKETS_PER_WINDOW_KEY
+
+
conf
.
get
(
DFSConfigKeys
.
NNTOP_BUCKETS_PER_WINDOW_KEY
)
)
;
private
static
void
logConf
(
Configuration
conf
)
{
LOG
.
info
(
+
DFSConfigKeys
.
NNTOP_BUCKETS_PER_WINDOW_KEY
+
+
conf
.
get
(
DFSConfigKeys
.
NNTOP_BUCKETS_PER_WINDOW_KEY
)
)
;
LOG
.
info
(
+
DFSConfigKeys
.
NNTOP_NUM_USERS_KEY
+
+
conf
.
get
(
DFSConfigKeys
.
NNTOP_NUM_USERS_KEY
)
)
;
public
void
report
(
long
currTime
,
String
userName
,
String
cmd
)
{
public
TopWindow
snapshot
(
long
time
)
{
TopWindow
window
=
new
TopWindow
(
windowLenMs
)
;
Set
<
String
>
metricNames
=
metricMap
.
keySet
(
)
;
private
TopN
getTopUsersForMetric
(
long
time
,
String
metricName
,
RollingWindowMap
rollingWindows
)
{
TopN
topN
=
new
TopN
(
topUsersCnt
)
;
Iterator
<
Map
.
Entry
<
String
,
RollingWindow
>>
iterator
=
rollingWindows
.
entrySet
(
)
.
iterator
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
Map
.
Entry
<
String
,
RollingWindow
>
entry
=
iterator
.
next
(
)
;
String
userName
=
entry
.
getKey
(
)
;
RollingWindow
aWindow
=
entry
.
getValue
(
)
;
long
windowSum
=
aWindow
.
getSum
(
time
)
;
if
(
windowSum
==
0
)
{
if
(
fsn
==
null
)
{
throw
new
IOException
(
)
;
}
final
BlockManager
bm
=
fsn
.
getBlockManager
(
)
;
HashSet
<
Node
>
excludes
=
new
HashSet
<
Node
>
(
)
;
if
(
excludeDatanodes
!=
null
)
{
for
(
String
host
:
StringUtils
.
getTrimmedStringCollection
(
excludeDatanodes
)
)
{
int
idx
=
host
.
indexOf
(
)
;
Node
excludeNode
=
null
;
if
(
idx
!=
-
1
)
{
excludeNode
=
bm
.
getDatanodeManager
(
)
.
getDatanodeByXferAddr
(
host
.
substring
(
0
,
idx
)
,
Integer
.
parseInt
(
host
.
substring
(
idx
+
1
)
)
)
;
}
else
{
excludeNode
=
bm
.
getDatanodeManager
(
)
.
getDatanodeByHost
(
host
)
;
}
if
(
excludeNode
!=
null
)
{
excludes
.
add
(
excludeNode
)
;
}
else
{
private
ThreadPoolExecutor
initializeBlockMoverThreadPool
(
int
num
)
{
@
Override
public
void
submitMoveTask
(
BlockMovingInfo
blkMovingInfo
)
throws
IOException
{
public
static
Configuration
addSecurityConfiguration
(
Configuration
conf
)
{
conf
=
new
HdfsConfiguration
(
conf
)
;
String
nameNodePrincipal
=
conf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY
,
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
checkRpcAdminAccess
(
)
throws
IOException
,
AccessControlException
{
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
UserGroupInformation
zkfcUgi
=
UserGroupInformation
.
getLoginUser
(
)
;
if
(
adminAcl
.
isUserAllowed
(
ugi
)
||
ugi
.
getShortUserName
(
)
.
equals
(
zkfcUgi
.
getShortUserName
(
)
)
)
{
isThreadDumpCaptured
=
false
;
int
httpTimeOut
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_HA_ZKFC_NN_HTTP_TIMEOUT_KEY
,
DFSConfigKeys
.
DFS_HA_ZKFC_NN_HTTP_TIMEOUT_KEY_DEFAULT
)
;
if
(
httpTimeOut
==
0
)
{
return
;
}
try
{
String
stacksUrl
=
DFSUtil
.
getInfoServer
(
localNNTarget
.
getAddress
(
)
,
conf
,
DFSUtil
.
getHttpClientScheme
(
conf
)
)
+
;
URL
url
=
new
URL
(
stacksUrl
)
;
HttpURLConnection
conn
=
(
HttpURLConnection
)
url
.
openConnection
(
)
;
conn
.
setReadTimeout
(
httpTimeOut
)
;
conn
.
setConnectTimeout
(
httpTimeOut
)
;
conn
.
connect
(
)
;
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
IOUtils
.
copyBytes
(
conn
.
getInputStream
(
)
,
out
,
4096
,
true
)
;
StringBuilder
localNNThreadDumpContent
=
new
StringBuilder
(
)
;
localNNThreadDumpContent
.
append
(
out
)
.
append
(
)
;
@
VisibleForTesting
static
void
cancelTokens
(
final
Configuration
conf
,
final
Path
tokenFile
)
throws
IOException
,
InterruptedException
{
for
(
Token
<
?
>
token
:
readTokens
(
tokenFile
,
conf
)
)
{
if
(
token
.
isManaged
(
)
)
{
token
.
cancel
(
conf
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
VisibleForTesting
static
void
renewTokens
(
final
Configuration
conf
,
final
Path
tokenFile
)
throws
IOException
,
InterruptedException
{
for
(
Token
<
?
>
token
:
readTokens
(
tokenFile
,
conf
)
)
{
if
(
token
.
isManaged
(
)
)
{
long
result
=
token
.
renew
(
conf
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
VisibleForTesting
static
void
saveDelegationToken
(
Configuration
conf
,
FileSystem
fs
,
final
String
renewer
,
final
Path
tokenFile
)
throws
IOException
{
Token
<
?
>
token
=
fs
.
getDelegationToken
(
renewer
)
;
if
(
null
!=
token
)
{
Credentials
cred
=
new
Credentials
(
)
;
cred
.
addToken
(
token
.
getService
(
)
,
token
)
;
cred
.
writeTokenStorageFile
(
tokenFile
,
conf
,
Credentials
.
SerializedFormat
.
WRITABLE
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
setConf
(
Configuration
conf
)
{
conf
=
new
HdfsConfiguration
(
conf
)
;
String
nameNodePrincipal
=
conf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY
,
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
visitor
.
start
(
inputStream
.
getVersion
(
true
)
)
;
while
(
true
)
{
try
{
FSEditLogOp
op
=
inputStream
.
readOp
(
)
;
if
(
op
==
null
)
break
;
if
(
fixTxIds
)
{
if
(
nextTxId
<=
0
)
{
nextTxId
=
op
.
getTransactionId
(
)
;
if
(
nextTxId
<=
0
)
{
nextTxId
=
1
;
}
}
op
.
setTransactionId
(
nextTxId
)
;
nextTxId
++
;
}
visitor
.
visitOp
(
op
)
;
}
catch
(
IOException
e
)
{
if
(
!
recoveryMode
)
{
nextTxId
=
op
.
getTransactionId
(
)
;
if
(
nextTxId
<=
0
)
{
nextTxId
=
1
;
}
}
op
.
setTransactionId
(
nextTxId
)
;
nextTxId
++
;
}
visitor
.
visitOp
(
op
)
;
}
catch
(
IOException
e
)
{
if
(
!
recoveryMode
)
{
LOG
.
error
(
+
inputStream
.
getPosition
(
)
)
;
visitor
.
close
(
e
)
;
throw
e
;
}
LOG
.
error
(
,
e
)
;
inputStream
.
resync
(
)
;
}
catch
(
RuntimeException
e
)
{
if
(
!
recoveryMode
)
{
}
op
.
setTransactionId
(
nextTxId
)
;
nextTxId
++
;
}
visitor
.
visitOp
(
op
)
;
}
catch
(
IOException
e
)
{
if
(
!
recoveryMode
)
{
LOG
.
error
(
+
inputStream
.
getPosition
(
)
)
;
visitor
.
close
(
e
)
;
throw
e
;
}
LOG
.
error
(
,
e
)
;
inputStream
.
resync
(
)
;
}
catch
(
RuntimeException
e
)
{
if
(
!
recoveryMode
)
{
LOG
.
error
(
+
inputStream
.
getPosition
(
)
)
;
visitor
.
close
(
e
)
;
throw
e
;
switch
(
op
)
{
case
:
content
=
image
.
getFileStatus
(
path
)
;
break
;
case
:
content
=
image
.
listStatus
(
path
)
;
break
;
case
:
content
=
image
.
getAclStatus
(
path
)
;
break
;
case
:
List
<
String
>
names
=
getXattrNames
(
decoder
)
;
String
encoder
=
getEncoder
(
decoder
)
;
content
=
image
.
getXAttrs
(
path
,
names
,
encoder
)
;
break
;
case
:
content
=
image
.
listXAttrs
(
path
)
;
break
;
case
:
content
=
image
.
getContentSummary
(
path
)
;
break
;
ArrayList
<
FsImageProto
.
FileSummary
.
Section
>
sections
=
Lists
.
newArrayList
(
summary
.
getSectionsList
(
)
)
;
Collections
.
sort
(
sections
,
new
Comparator
<
FsImageProto
.
FileSummary
.
Section
>
(
)
{
@
Override
public
int
compare
(
FsImageProto
.
FileSummary
.
Section
s1
,
FsImageProto
.
FileSummary
.
Section
s2
)
{
FSImageFormatProtobuf
.
SectionName
n1
=
FSImageFormatProtobuf
.
SectionName
.
fromString
(
s1
.
getName
(
)
)
;
FSImageFormatProtobuf
.
SectionName
n2
=
FSImageFormatProtobuf
.
SectionName
.
fromString
(
s2
.
getName
(
)
)
;
if
(
n1
==
null
)
{
return
n2
==
null
?
0
:
-
1
;
}
else
if
(
n2
==
null
)
{
return
-
1
;
}
else
{
return
n1
.
ordinal
(
)
-
n2
.
ordinal
(
)
;
}
}
}
)
;
for
(
FsImageProto
.
FileSummary
.
Section
s
:
sections
)
{
fin
.
getChannel
(
)
.
position
(
s
.
getOffset
(
)
)
;
InputStream
is
=
FSImageUtil
.
wrapInputStreamForCompression
(
conf
,
summary
.
getCodec
(
)
,
new
BufferedInputStream
(
new
LimitInputStream
(
fin
,
s
.
getLength
(
)
)
)
)
;
private
static
byte
[
]
[
]
loadINodeSection
(
InputStream
in
)
throws
IOException
{
FsImageProto
.
INodeSection
s
=
FsImageProto
.
INodeSection
.
parseDelimitedFrom
(
in
)
;
static
SerialNumberManager
.
StringTable
loadStringTable
(
InputStream
in
)
throws
IOException
{
FsImageProto
.
StringTableSection
s
=
FsImageProto
.
StringTableSection
.
parseDelimitedFrom
(
in
)
;
}
catch
(
IOException
e
)
{
throw
new
IOException
(
+
+
,
e
)
;
}
Node
version
=
new
Node
(
)
;
loadNodeChildren
(
version
,
)
;
Integer
onDiskVersion
=
version
.
removeChildInt
(
)
;
if
(
onDiskVersion
==
null
)
{
throw
new
IOException
(
+
)
;
}
Integer
layoutVersion
=
version
.
removeChildInt
(
)
;
if
(
layoutVersion
==
null
)
{
throw
new
IOException
(
+
)
;
}
if
(
layoutVersion
.
intValue
(
)
!=
NameNodeLayoutVersion
.
CURRENT_LAYOUT_VERSION
)
{
throw
new
IOException
(
+
+
NameNodeLayoutVersion
.
CURRENT_LAYOUT_VERSION
+
+
+
layoutVersion
+
+
+
+
)
;
}
fileSummaryBld
.
setOndiskVersion
(
onDiskVersion
)
;
fileSummaryBld
.
setLayoutVersion
(
layoutVersion
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
throw
new
IOException
(
+
ev
.
asEndElement
(
)
.
getName
(
)
.
getLocalPart
(
)
+
+
)
;
}
else
if
(
ev
.
getEventType
(
)
!=
XMLStreamConstants
.
START_ELEMENT
)
{
throw
new
IOException
(
+
+
ev
.
getEventType
(
)
)
;
}
String
sectionName
=
ev
.
asStartElement
(
)
.
getName
(
)
.
getLocalPart
(
)
;
if
(
!
unprocessedSections
.
contains
(
sectionName
)
)
{
throw
new
IOException
(
+
sectionName
)
;
}
SectionProcessor
sectionProcessor
=
sections
.
get
(
sectionName
)
;
if
(
sectionProcessor
==
null
)
{
throw
new
IOException
(
+
sectionName
+
+
StringUtils
.
join
(
,
sections
.
keySet
(
)
)
+
)
;
}
unprocessedSections
.
remove
(
sectionName
)
;
sectionProcessor
.
process
(
)
;
}
writeStringTableSection
(
)
;
long
prevOffset
=
out
.
getCount
(
)
;
FileSummary
fileSummary
=
fileSummaryBld
.
build
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
DataInputStream
in
=
null
;
PositionTrackingInputStream
tracker
=
null
;
ImageLoader
fsip
=
null
;
boolean
done
=
false
;
try
{
tracker
=
new
PositionTrackingInputStream
(
new
BufferedInputStream
(
Files
.
newInputStream
(
Paths
.
get
(
inputFile
)
)
)
)
;
in
=
new
DataInputStream
(
tracker
)
;
int
imageVersionFile
=
findImageVersion
(
in
)
;
fsip
=
ImageLoader
.
LoaderFactory
.
getLoader
(
imageVersionFile
)
;
if
(
fsip
==
null
)
throw
new
IOException
(
+
imageVersionFile
+
)
;
fsip
.
loadImage
(
in
,
processor
,
skipBlocks
)
;
done
=
true
;
}
finally
{
if
(
!
done
)
{
if
(
tracker
!=
null
)
{
@
Override
protected
void
buildNamespace
(
InputStream
in
,
List
<
Long
>
refIdList
)
throws
IOException
{
corrChecker
.
saveNodeRefIds
(
refIdList
)
;
count
++
;
if
(
LOG
.
isDebugEnabled
(
)
&&
count
%
10000
==
0
)
{
LOG
.
debug
(
,
count
)
;
}
long
parentId
=
e
.
getParent
(
)
;
if
(
!
corrChecker
.
isNodeIdExist
(
parentId
)
)
{
LOG
.
debug
(
+
)
;
addCorruptedNode
(
parentId
)
;
}
int
numOfCorruption
=
0
;
for
(
int
i
=
0
;
i
<
e
.
getChildrenCount
(
)
;
i
++
)
{
long
childId
=
e
.
getChildren
(
i
)
;
putDirChildToMetadataMap
(
parentId
,
childId
)
;
if
(
!
corrChecker
.
isNodeIdExist
(
childId
)
)
{
addCorruptedNode
(
childId
)
;
numOfCorruption
++
;
}
}
if
(
numOfCorruption
>
0
)
{
@
Override
public
void
afterOutput
(
)
throws
IOException
{
if
(
!
corruptionsMap
.
isEmpty
(
)
)
{
private
void
loadDirectoriesInINodeSection
(
InputStream
in
)
throws
IOException
{
INodeSection
s
=
INodeSection
.
parseDelimitedFrom
(
in
)
;
LOG
.
info
(
)
;
AtomicInteger
numDirs
=
new
AtomicInteger
(
0
)
;
for
(
int
i
=
0
;
i
<
s
.
getNumInodes
(
)
;
++
i
)
{
INode
p
=
INode
.
parseDelimitedFrom
(
in
)
;
if
(
LOG
.
isDebugEnabled
(
)
&&
i
%
10000
==
0
)
{
private
void
outputINodes
(
InputStream
in
)
throws
IOException
{
INodeSection
s
=
INodeSection
.
parseDelimitedFrom
(
in
)
;
LOG
.
info
(
,
s
.
getNumInodes
(
)
)
;
long
ignored
=
0
;
long
ignoredSnapshots
=
0
;
for
(
int
i
=
0
;
i
<
s
.
getNumInodes
(
)
;
++
i
)
{
INode
p
=
INode
.
parseDelimitedFrom
(
in
)
;
try
{
String
parentPath
=
metadataMap
.
getParentPath
(
p
.
getId
(
)
)
;
printIfNotEmpty
(
getEntry
(
parentPath
,
p
)
)
;
}
catch
(
IOException
ioe
)
{
ignored
++
;
if
(
!
(
ioe
instanceof
IgnoreSnapshotException
)
)
{
LOG
.
warn
(
,
p
.
getId
(
)
,
ioe
)
;
}
else
{
ignoredSnapshots
++
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
ignoredSnapshots
=
0
;
for
(
int
i
=
0
;
i
<
s
.
getNumInodes
(
)
;
++
i
)
{
INode
p
=
INode
.
parseDelimitedFrom
(
in
)
;
try
{
String
parentPath
=
metadataMap
.
getParentPath
(
p
.
getId
(
)
)
;
printIfNotEmpty
(
getEntry
(
parentPath
,
p
)
)
;
}
catch
(
IOException
ioe
)
{
ignored
++
;
if
(
!
(
ioe
instanceof
IgnoreSnapshotException
)
)
{
LOG
.
warn
(
,
p
.
getId
(
)
,
ioe
)
;
}
else
{
ignoredSnapshots
++
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
p
.
getId
(
)
,
ioe
)
;
}
}
}
if
(
LOG
.
isDebugEnabled
(
)
&&
i
%
100000
==
0
)
{
private
static
IgnoreSnapshotException
createIgnoredSnapshotException
(
long
inode
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
saveMD5File
(
File
dataFile
,
String
digestString
)
throws
IOException
{
File
md5File
=
getDigestFileForFile
(
dataFile
)
;
String
md5Line
=
digestString
+
+
dataFile
.
getName
(
)
+
;
AtomicFileOutputStream
afos
=
new
AtomicFileOutputStream
(
md5File
)
;
afos
.
write
(
md5Line
.
getBytes
(
Charsets
.
UTF_8
)
)
;
afos
.
close
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
BeforeClass
public
static
void
init
(
)
{
sockDir
=
new
TemporarySocketDirectory
(
)
;
DomainSocket
.
disableBindPathValidation
(
)
;
prevCacheManipulator
=
NativeIO
.
POSIX
.
getCacheManipulator
(
)
;
NativeIO
.
POSIX
.
setCacheManipulator
(
new
CacheManipulator
(
)
{
@
Override
public
void
mlock
(
String
identifier
,
ByteBuffer
mmap
,
long
length
)
throws
IOException
{
private
void
waitForReplicaAnchorStatus
(
final
ShortCircuitCache
cache
,
final
ExtendedBlock
block
,
final
boolean
expectedIsAnchorable
,
final
boolean
expectedIsAnchored
,
final
int
expectedOutstandingMmaps
)
throws
Exception
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
final
MutableBoolean
result
=
new
MutableBoolean
(
false
)
;
cache
.
accept
(
new
CacheVisitor
(
)
{
@
Override
public
void
visit
(
int
numOutstandingMmaps
,
Map
<
ExtendedBlockId
,
ShortCircuitReplica
>
replicas
,
Map
<
ExtendedBlockId
,
InvalidToken
>
failedLoads
,
LinkedMap
evictable
,
LinkedMap
evictableMmapped
)
{
Assert
.
assertEquals
(
expectedOutstandingMmaps
,
numOutstandingMmaps
)
;
ShortCircuitReplica
replica
=
replicas
.
get
(
ExtendedBlockId
.
fromExtendedBlock
(
block
)
)
;
Assert
.
assertNotNull
(
replica
)
;
Slot
slot
=
replica
.
getSlot
(
)
;
if
(
(
expectedIsAnchorable
!=
slot
.
isAnchorable
(
)
)
||
(
expectedIsAnchored
!=
slot
.
isAnchored
(
)
)
)
{
@
Test
public
void
testOpenManyFilesViaTcp
(
)
throws
Exception
{
final
int
NUM_OPENS
=
500
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setBoolean
(
HdfsClientConfigKeys
.
Read
.
ShortCircuit
.
KEY
,
false
)
;
MiniDFSCluster
cluster
=
null
;
FSDataInputStream
[
]
streams
=
new
FSDataInputStream
[
NUM_OPENS
]
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
build
(
)
;
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
final
Path
TEST_PATH
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
dfs
,
TEST_PATH
,
131072
,
(
short
)
1
,
1
)
;
for
(
int
i
=
0
;
i
<
NUM_OPENS
;
i
++
)
{
streams
[
i
]
=
dfs
.
open
(
TEST_PATH
)
;
@
Test
public
void
testStickyBitReset
(
)
throws
Exception
{
Path
sbExplicitTestDir
=
new
Path
(
)
;
Path
sbOmittedTestDir
=
new
Path
(
)
;
hdfs
.
mkdirs
(
sbExplicitTestDir
)
;
hdfs
.
mkdirs
(
sbOmittedTestDir
)
;
assertTrue
(
hdfs
.
exists
(
sbExplicitTestDir
)
)
;
assertTrue
(
hdfs
.
exists
(
sbOmittedTestDir
)
)
;
hdfs
.
setPermission
(
sbExplicitTestDir
,
new
FsPermission
(
(
short
)
01777
)
)
;
@
Test
public
void
testStickyBitReset
(
)
throws
Exception
{
Path
sbExplicitTestDir
=
new
Path
(
)
;
Path
sbOmittedTestDir
=
new
Path
(
)
;
hdfs
.
mkdirs
(
sbExplicitTestDir
)
;
hdfs
.
mkdirs
(
sbOmittedTestDir
)
;
assertTrue
(
hdfs
.
exists
(
sbExplicitTestDir
)
)
;
assertTrue
(
hdfs
.
exists
(
sbOmittedTestDir
)
)
;
hdfs
.
setPermission
(
sbExplicitTestDir
,
new
FsPermission
(
(
short
)
01777
)
)
;
LOG
.
info
(
,
sbExplicitTestDir
.
getName
(
)
,
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
)
;
assertTrue
(
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
.
getStickyBit
(
)
)
;
hdfs
.
setPermission
(
sbOmittedTestDir
,
new
FsPermission
(
(
short
)
0777
)
)
;
@
Test
public
void
testStickyBitReset
(
)
throws
Exception
{
Path
sbExplicitTestDir
=
new
Path
(
)
;
Path
sbOmittedTestDir
=
new
Path
(
)
;
hdfs
.
mkdirs
(
sbExplicitTestDir
)
;
hdfs
.
mkdirs
(
sbOmittedTestDir
)
;
assertTrue
(
hdfs
.
exists
(
sbExplicitTestDir
)
)
;
assertTrue
(
hdfs
.
exists
(
sbOmittedTestDir
)
)
;
hdfs
.
setPermission
(
sbExplicitTestDir
,
new
FsPermission
(
(
short
)
01777
)
)
;
LOG
.
info
(
,
sbExplicitTestDir
.
getName
(
)
,
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
)
;
assertTrue
(
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
.
getStickyBit
(
)
)
;
hdfs
.
setPermission
(
sbOmittedTestDir
,
new
FsPermission
(
(
short
)
0777
)
)
;
LOG
.
info
(
,
sbOmittedTestDir
.
getName
(
)
,
hdfs
.
getFileStatus
(
sbOmittedTestDir
)
.
getPermission
(
)
)
;
assertFalse
(
hdfs
.
getFileStatus
(
sbOmittedTestDir
)
.
getPermission
(
)
.
getStickyBit
(
)
)
;
hdfs
.
setPermission
(
sbExplicitTestDir
,
new
FsPermission
(
(
short
)
00777
)
)
;
hdfs
.
mkdirs
(
sbExplicitTestDir
)
;
hdfs
.
mkdirs
(
sbOmittedTestDir
)
;
assertTrue
(
hdfs
.
exists
(
sbExplicitTestDir
)
)
;
assertTrue
(
hdfs
.
exists
(
sbOmittedTestDir
)
)
;
hdfs
.
setPermission
(
sbExplicitTestDir
,
new
FsPermission
(
(
short
)
01777
)
)
;
LOG
.
info
(
,
sbExplicitTestDir
.
getName
(
)
,
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
)
;
assertTrue
(
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
.
getStickyBit
(
)
)
;
hdfs
.
setPermission
(
sbOmittedTestDir
,
new
FsPermission
(
(
short
)
0777
)
)
;
LOG
.
info
(
,
sbOmittedTestDir
.
getName
(
)
,
hdfs
.
getFileStatus
(
sbOmittedTestDir
)
.
getPermission
(
)
)
;
assertFalse
(
hdfs
.
getFileStatus
(
sbOmittedTestDir
)
.
getPermission
(
)
.
getStickyBit
(
)
)
;
hdfs
.
setPermission
(
sbExplicitTestDir
,
new
FsPermission
(
(
short
)
00777
)
)
;
LOG
.
info
(
,
sbExplicitTestDir
.
getName
(
)
,
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
)
;
assertFalse
(
hdfs
.
getFileStatus
(
sbExplicitTestDir
)
.
getPermission
(
)
.
getStickyBit
(
)
)
;
hdfs
.
setPermission
(
sbOmittedTestDir
,
new
FsPermission
(
(
short
)
01777
)
)
;
hdfs
.
setPermission
(
sbOmittedTestDir
,
new
FsPermission
(
(
short
)
0777
)
)
;
fsTarget
.
createNewFile
(
testBaseFile
)
;
FSDataOutputStream
dataOutputStream
=
fsTarget
.
append
(
testBaseFile
)
;
dataOutputStream
.
write
(
1
)
;
dataOutputStream
.
close
(
)
;
fsTarget
.
createNewFile
(
testLevel2File
)
;
dataOutputStream
=
fsTarget
.
append
(
testLevel2File
)
;
dataOutputStream
.
write
(
.
toString
(
)
.
getBytes
(
)
)
;
dataOutputStream
.
close
(
)
;
String
clusterName
=
;
URI
viewFsUri
=
new
URI
(
FsConstants
.
VIEWFS_SCHEME
,
clusterName
,
,
null
,
null
)
;
Configuration
conf
=
new
Configuration
(
)
;
ConfigUtil
.
addLinkFallback
(
conf
,
clusterName
,
fsTarget
.
getUri
(
)
)
;
FileSystem
vfs
=
FileSystem
.
get
(
viewFsUri
,
conf
)
;
assertEquals
(
ViewFileSystem
.
class
,
vfs
.
getClass
(
)
)
;
FileStatus
baseFileStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testBaseFile
.
toUri
(
)
.
toString
(
)
)
)
;
dataOutputStream
.
write
(
1
)
;
dataOutputStream
.
close
(
)
;
fsTarget
.
createNewFile
(
testLevel2File
)
;
dataOutputStream
=
fsTarget
.
append
(
testLevel2File
)
;
dataOutputStream
.
write
(
.
toString
(
)
.
getBytes
(
)
)
;
dataOutputStream
.
close
(
)
;
String
clusterName
=
;
URI
viewFsUri
=
new
URI
(
FsConstants
.
VIEWFS_SCHEME
,
clusterName
,
,
null
,
null
)
;
Configuration
conf
=
new
Configuration
(
)
;
ConfigUtil
.
addLinkFallback
(
conf
,
clusterName
,
fsTarget
.
getUri
(
)
)
;
FileSystem
vfs
=
FileSystem
.
get
(
viewFsUri
,
conf
)
;
assertEquals
(
ViewFileSystem
.
class
,
vfs
.
getClass
(
)
)
;
FileStatus
baseFileStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testBaseFile
.
toUri
(
)
.
toString
(
)
)
)
;
LOG
.
info
(
+
baseFileStat
)
;
FileStatus
baseFileRelStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testBaseFileRelative
.
toUri
(
)
.
toString
(
)
)
)
;
dataOutputStream
.
write
(
.
toString
(
)
.
getBytes
(
)
)
;
dataOutputStream
.
close
(
)
;
String
clusterName
=
;
URI
viewFsUri
=
new
URI
(
FsConstants
.
VIEWFS_SCHEME
,
clusterName
,
,
null
,
null
)
;
Configuration
conf
=
new
Configuration
(
)
;
ConfigUtil
.
addLinkFallback
(
conf
,
clusterName
,
fsTarget
.
getUri
(
)
)
;
FileSystem
vfs
=
FileSystem
.
get
(
viewFsUri
,
conf
)
;
assertEquals
(
ViewFileSystem
.
class
,
vfs
.
getClass
(
)
)
;
FileStatus
baseFileStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testBaseFile
.
toUri
(
)
.
toString
(
)
)
)
;
LOG
.
info
(
+
baseFileStat
)
;
FileStatus
baseFileRelStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testBaseFileRelative
.
toUri
(
)
.
toString
(
)
)
)
;
LOG
.
info
(
+
baseFileRelStat
)
;
Assert
.
assertEquals
(
+
testBaseFile
,
1
,
baseFileStat
.
getLen
(
)
)
;
Assert
.
assertEquals
(
+
testBaseFileRelative
,
baseFileStat
.
getLen
(
)
,
baseFileRelStat
.
getLen
(
)
)
;
FileStatus
level2FileStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testLevel2File
.
toUri
(
)
.
toString
(
)
)
)
;
FSDataOutputStream
dataOutputStream
=
fsTarget
.
append
(
testLevel2File
)
;
dataOutputStream
.
write
(
.
toString
(
)
.
getBytes
(
)
)
;
dataOutputStream
.
close
(
)
;
String
clusterName
=
;
URI
viewFsUri
=
new
URI
(
FsConstants
.
VIEWFS_SCHEME
,
clusterName
,
,
null
,
null
)
;
Configuration
conf
=
new
Configuration
(
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
System
.
out
.
println
(
+
fsTarget
.
getUri
(
)
)
;
ConfigUtil
.
addLinkFallback
(
conf
,
clusterName
,
targetTestRoot
.
toUri
(
)
)
;
FileSystem
vfs
=
FileSystem
.
get
(
viewFsUri
,
conf
)
;
assertEquals
(
ViewFileSystem
.
class
,
vfs
.
getClass
(
)
)
;
FileStatus
baseFileStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testBaseFile
.
toUri
(
)
.
toString
(
)
)
)
;
String
clusterName
=
;
URI
viewFsUri
=
new
URI
(
FsConstants
.
VIEWFS_SCHEME
,
clusterName
,
,
null
,
null
)
;
Configuration
conf
=
new
Configuration
(
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
ConfigUtil
.
addLink
(
conf
,
clusterName
,
,
new
Path
(
targetTestRoot
,
)
.
toUri
(
)
)
;
System
.
out
.
println
(
+
fsTarget
.
getUri
(
)
)
;
ConfigUtil
.
addLinkFallback
(
conf
,
clusterName
,
targetTestRoot
.
toUri
(
)
)
;
FileSystem
vfs
=
FileSystem
.
get
(
viewFsUri
,
conf
)
;
assertEquals
(
ViewFileSystem
.
class
,
vfs
.
getClass
(
)
)
;
FileStatus
baseFileStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testBaseFile
.
toUri
(
)
.
toString
(
)
)
)
;
LOG
.
info
(
+
baseFileStat
)
;
Assert
.
assertEquals
(
+
testBaseFile
,
0
,
baseFileStat
.
getLen
(
)
)
;
FileStatus
level2FileStat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testLevel2File
.
toUri
(
)
.
toString
(
)
)
)
;
File
infile
=
new
File
(
TEST_DIR
,
testFileName
)
;
final
byte
[
]
content
=
.
getBytes
(
)
;
FileOutputStream
fos
=
null
;
try
{
fos
=
new
FileOutputStream
(
infile
)
;
fos
.
write
(
content
)
;
}
finally
{
if
(
fos
!=
null
)
{
fos
.
close
(
)
;
}
}
assertEquals
(
(
long
)
content
.
length
,
infile
.
length
(
)
)
;
Configuration
conf
=
new
Configuration
(
)
;
ConfigUtil
.
addLinkMergeSlash
(
conf
,
clusterName
,
TEST_DIR
.
toURI
(
)
)
;
FileSystem
vfs
=
FileSystem
.
get
(
viewFsUri
,
conf
)
;
assertEquals
(
ViewFileSystem
.
class
,
vfs
.
getClass
(
)
)
;
FileStatus
stat
=
vfs
.
getFileStatus
(
new
Path
(
viewFsUri
.
toString
(
)
+
testFileName
)
)
;
static
protected
FSDataOutputStream
writeFile
(
FileSystem
fileSys
,
Path
name
,
int
repl
,
int
numOfBlocks
,
boolean
completeFile
)
throws
IOException
{
FSDataOutputStream
stm
=
fileSys
.
create
(
name
,
true
,
fileSys
.
getConf
(
)
.
getInt
(
CommonConfigurationKeys
.
IO_FILE_BUFFER_SIZE_KEY
,
4096
)
,
(
short
)
repl
,
blockSize
)
;
byte
[
]
buffer
=
new
byte
[
blockSize
*
numOfBlocks
]
;
Random
rand
=
new
Random
(
seed
)
;
rand
.
nextBytes
(
buffer
)
;
stm
.
write
(
buffer
)
;
protected
void
putNodeInService
(
int
nnIndex
,
DatanodeInfo
outOfServiceNode
)
throws
IOException
{
protected
void
waitNodeState
(
List
<
DatanodeInfo
>
nodes
,
AdminStates
state
)
{
for
(
DatanodeInfo
node
:
nodes
)
{
boolean
done
=
(
state
==
node
.
getAdminState
(
)
)
;
while
(
!
done
)
{
public
static
byte
[
]
randomBytes
(
long
seed
,
int
size
)
{
public
static
long
verifyExpectedCacheUsage
(
final
long
expectedCacheUsed
,
final
long
expectedBlocks
,
final
FsDatasetSpi
<
?
>
fsd
)
throws
Exception
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
private
int
tries
=
0
;
@
Override
public
Boolean
get
(
)
{
long
curCacheUsed
=
fsd
.
getCacheUsed
(
)
;
long
curBlocks
=
fsd
.
getNumBlocksCached
(
)
;
if
(
(
curCacheUsed
!=
expectedCacheUsed
)
||
(
curBlocks
!=
expectedBlocks
)
)
{
if
(
tries
++
>
10
)
{
public
static
boolean
verifyFileReplicasOnStorageType
(
FileSystem
fs
,
DFSClient
client
,
Path
path
,
StorageType
storageType
)
throws
IOException
{
if
(
!
fs
.
exists
(
path
)
)
{
public
static
HashSet
<
Path
>
closeOpenFiles
(
HashMap
<
Path
,
FSDataOutputStream
>
openFilesMap
,
int
numFilesToClose
)
throws
IOException
{
HashSet
<
Path
>
closedFiles
=
new
HashSet
<
>
(
)
;
for
(
Iterator
<
Entry
<
Path
,
FSDataOutputStream
>>
it
=
openFilesMap
.
entrySet
(
)
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
Entry
<
Path
,
FSDataOutputStream
>
entry
=
it
.
next
(
)
;
public
static
void
verifySnapshotDiffReport
(
DistributedFileSystem
fs
,
Path
dir
,
String
from
,
String
to
,
DiffReportEntry
...
entries
)
throws
IOException
{
SnapshotDiffReport
report
=
fs
.
getSnapshotDiffReport
(
dir
,
from
,
to
)
;
SnapshotDiffReport
inverseReport
=
fs
.
getSnapshotDiffReport
(
dir
,
to
,
from
)
;
public
static
void
verifySnapshotDiffReport
(
DistributedFileSystem
fs
,
Path
dir
,
String
from
,
String
to
,
DiffReportEntry
...
entries
)
throws
IOException
{
SnapshotDiffReport
report
=
fs
.
getSnapshotDiffReport
(
dir
,
from
,
to
)
;
SnapshotDiffReport
inverseReport
=
fs
.
getSnapshotDiffReport
(
dir
,
to
,
from
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
@
Test
public
void
testAppend
(
)
throws
IOException
{
final
int
maxOldFileLen
=
2
*
BLOCK_SIZE
+
1
;
final
int
maxFlushedBytes
=
BLOCK_SIZE
;
byte
[
]
contents
=
AppendTestUtil
.
initBuffer
(
maxOldFileLen
+
2
*
maxFlushedBytes
)
;
for
(
int
oldFileLen
=
0
;
oldFileLen
<=
maxOldFileLen
;
oldFileLen
++
)
{
for
(
int
flushedBytes1
=
0
;
flushedBytes1
<=
maxFlushedBytes
;
flushedBytes1
++
)
{
for
(
int
flushedBytes2
=
0
;
flushedBytes2
<=
maxFlushedBytes
;
flushedBytes2
++
)
{
final
int
fileLen
=
oldFileLen
+
flushedBytes1
+
flushedBytes2
;
final
Path
p
=
new
Path
(
+
oldFileLen
+
+
flushedBytes1
+
+
flushedBytes2
)
;
public
void
shutdown
(
boolean
deleteDfsDir
,
boolean
closeFileSystem
)
{
LOG
.
info
(
)
;
if
(
checkExitOnShutdown
)
{
if
(
ExitUtil
.
terminateCalled
(
)
)
{
public
void
shutdownDataNode
(
int
dnIndex
)
{
public
synchronized
boolean
restartDataNodes
(
boolean
keepPort
)
throws
IOException
{
for
(
int
i
=
dataNodes
.
size
(
)
-
1
;
i
>=
0
;
i
--
)
{
if
(
!
restartDataNode
(
i
,
keepPort
)
)
return
false
;
public
void
printNNs
(
)
{
for
(
int
i
=
0
;
i
<
namenodes
.
size
(
)
;
i
++
)
{
public
static
void
verifyRead
(
DistributedFileSystem
dfs
,
Path
testPath
,
int
length
,
byte
[
]
expected
)
throws
IOException
{
public
static
void
verifyRead
(
DistributedFileSystem
dfs
,
Path
testPath
,
int
length
,
byte
[
]
expected
)
throws
IOException
{
LOG
.
info
(
,
testPath
)
;
byte
[
]
buffer
=
new
byte
[
length
+
100
]
;
public
static
void
verifyRead
(
DistributedFileSystem
dfs
,
Path
testPath
,
int
length
,
byte
[
]
expected
)
throws
IOException
{
LOG
.
info
(
,
testPath
)
;
byte
[
]
buffer
=
new
byte
[
length
+
100
]
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyLength
(
dfs
,
testPath
,
length
)
;
public
static
void
verifyRead
(
DistributedFileSystem
dfs
,
Path
testPath
,
int
length
,
byte
[
]
expected
)
throws
IOException
{
LOG
.
info
(
,
testPath
)
;
byte
[
]
buffer
=
new
byte
[
length
+
100
]
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyLength
(
dfs
,
testPath
,
length
)
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyPread
(
dfs
,
testPath
,
length
,
expected
,
buffer
)
;
public
static
void
verifyRead
(
DistributedFileSystem
dfs
,
Path
testPath
,
int
length
,
byte
[
]
expected
)
throws
IOException
{
LOG
.
info
(
,
testPath
)
;
byte
[
]
buffer
=
new
byte
[
length
+
100
]
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyLength
(
dfs
,
testPath
,
length
)
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyPread
(
dfs
,
testPath
,
length
,
expected
,
buffer
)
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyStatefulRead
(
dfs
,
testPath
,
length
,
expected
,
buffer
)
;
public
static
void
verifyRead
(
DistributedFileSystem
dfs
,
Path
testPath
,
int
length
,
byte
[
]
expected
)
throws
IOException
{
LOG
.
info
(
,
testPath
)
;
byte
[
]
buffer
=
new
byte
[
length
+
100
]
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyLength
(
dfs
,
testPath
,
length
)
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyPread
(
dfs
,
testPath
,
length
,
expected
,
buffer
)
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyStatefulRead
(
dfs
,
testPath
,
length
,
expected
,
buffer
)
;
LOG
.
info
(
,
testPath
)
;
StripedFileTestUtil
.
verifyStatefulRead
(
dfs
,
testPath
,
length
,
expected
,
ByteBuffer
.
allocate
(
length
+
100
)
)
;
public
static
void
testReadWithDNFailure
(
MiniDFSCluster
cluster
,
DistributedFileSystem
dfs
,
int
fileLength
,
int
dnFailureNum
)
throws
Exception
{
String
fileType
=
fileLength
<
(
BLOCK_SIZE
*
NUM_DATA_UNITS
)
?
:
;
String
src
=
+
dnFailureNum
+
+
fileType
;
public
static
void
testReadWithBlockCorrupted
(
MiniDFSCluster
cluster
,
DistributedFileSystem
dfs
,
String
src
,
int
fileNumBytes
,
int
dataBlkDelNum
,
int
parityBlkDelNum
,
boolean
deleteBlockFile
)
throws
IOException
{
public
static
void
corruptBlocks
(
MiniDFSCluster
cluster
,
DistributedFileSystem
dfs
,
Path
srcPath
,
int
dataBlkDelNum
,
int
parityBlkDelNum
,
boolean
deleteBlockFile
)
throws
IOException
{
LOG
.
info
(
,
srcPath
)
;
int
recoverBlkNum
=
dataBlkDelNum
+
parityBlkDelNum
;
LocatedBlocks
locatedBlocks
=
getLocatedBlocks
(
dfs
,
srcPath
)
;
LocatedStripedBlock
lastBlock
=
(
LocatedStripedBlock
)
locatedBlocks
.
getLastLocatedBlock
(
)
;
int
[
]
delDataBlkIndices
=
StripedFileTestUtil
.
randomArray
(
0
,
NUM_DATA_UNITS
,
dataBlkDelNum
)
;
Assert
.
assertNotNull
(
delDataBlkIndices
)
;
int
[
]
delParityBlkIndices
=
StripedFileTestUtil
.
randomArray
(
NUM_DATA_UNITS
,
NUM_DATA_UNITS
+
NUM_PARITY_UNITS
,
parityBlkDelNum
)
;
Assert
.
assertNotNull
(
delParityBlkIndices
)
;
int
[
]
delBlkIndices
=
new
int
[
recoverBlkNum
]
;
System
.
arraycopy
(
delDataBlkIndices
,
0
,
delBlkIndices
,
0
,
delDataBlkIndices
.
length
)
;
System
.
arraycopy
(
delParityBlkIndices
,
0
,
delBlkIndices
,
delDataBlkIndices
.
length
,
delParityBlkIndices
.
length
)
;
ExtendedBlock
[
]
delBlocks
=
new
ExtendedBlock
[
recoverBlkNum
]
;
for
(
int
i
=
0
;
i
<
recoverBlkNum
;
i
++
)
{
delBlocks
[
i
]
=
StripedBlockUtil
.
constructInternalBlock
(
lastBlock
.
getBlock
(
)
,
CELL_SIZE
,
NUM_DATA_UNITS
,
delBlkIndices
[
i
]
)
;
if
(
deleteBlockFile
)
{
LocatedStripedBlock
lastBlock
=
(
LocatedStripedBlock
)
locatedBlocks
.
getLastLocatedBlock
(
)
;
int
[
]
delDataBlkIndices
=
StripedFileTestUtil
.
randomArray
(
0
,
NUM_DATA_UNITS
,
dataBlkDelNum
)
;
Assert
.
assertNotNull
(
delDataBlkIndices
)
;
int
[
]
delParityBlkIndices
=
StripedFileTestUtil
.
randomArray
(
NUM_DATA_UNITS
,
NUM_DATA_UNITS
+
NUM_PARITY_UNITS
,
parityBlkDelNum
)
;
Assert
.
assertNotNull
(
delParityBlkIndices
)
;
int
[
]
delBlkIndices
=
new
int
[
recoverBlkNum
]
;
System
.
arraycopy
(
delDataBlkIndices
,
0
,
delBlkIndices
,
0
,
delDataBlkIndices
.
length
)
;
System
.
arraycopy
(
delParityBlkIndices
,
0
,
delBlkIndices
,
delDataBlkIndices
.
length
,
delParityBlkIndices
.
length
)
;
ExtendedBlock
[
]
delBlocks
=
new
ExtendedBlock
[
recoverBlkNum
]
;
for
(
int
i
=
0
;
i
<
recoverBlkNum
;
i
++
)
{
delBlocks
[
i
]
=
StripedBlockUtil
.
constructInternalBlock
(
lastBlock
.
getBlock
(
)
,
CELL_SIZE
,
NUM_DATA_UNITS
,
delBlkIndices
[
i
]
)
;
if
(
deleteBlockFile
)
{
LOG
.
info
(
,
delBlocks
[
i
]
)
;
cluster
.
corruptBlockOnDataNodesByDeletingBlockFile
(
delBlocks
[
i
]
)
;
}
else
{
public
static
void
waitBlockGroupsReported
(
DistributedFileSystem
fs
,
String
src
,
int
numDeadDNs
)
throws
Exception
{
boolean
success
;
final
int
ATTEMPTS
=
40
;
int
count
=
0
;
final
ErasureCodingPolicy
ecPolicy
=
fs
.
getErasureCodingPolicy
(
new
Path
(
src
)
)
;
do
{
success
=
true
;
count
++
;
LocatedBlocks
lbs
=
fs
.
getClient
(
)
.
getLocatedBlocks
(
src
,
0
)
;
for
(
LocatedBlock
lb
:
lbs
.
getLocatedBlocks
(
)
)
{
short
expected
=
(
short
)
(
getRealTotalBlockNum
(
(
int
)
lb
.
getBlockSize
(
)
,
ecPolicy
)
-
numDeadDNs
)
;
int
reported
=
lb
.
getLocations
(
)
.
length
;
if
(
reported
<
expected
)
{
success
=
false
;
int
count
=
0
;
final
ErasureCodingPolicy
ecPolicy
=
fs
.
getErasureCodingPolicy
(
new
Path
(
src
)
)
;
do
{
success
=
true
;
count
++
;
LocatedBlocks
lbs
=
fs
.
getClient
(
)
.
getLocatedBlocks
(
src
,
0
)
;
for
(
LocatedBlock
lb
:
lbs
.
getLocatedBlocks
(
)
)
{
short
expected
=
(
short
)
(
getRealTotalBlockNum
(
(
int
)
lb
.
getBlockSize
(
)
,
ecPolicy
)
-
numDeadDNs
)
;
int
reported
=
lb
.
getLocations
(
)
.
length
;
if
(
reported
<
expected
)
{
success
=
false
;
LOG
.
info
(
+
lb
.
getBlock
(
)
+
+
src
+
+
reported
+
+
expected
+
+
Joiner
.
on
(
' '
)
.
join
(
lb
.
getLocations
(
)
)
)
;
Thread
.
sleep
(
1000
)
;
break
;
}
}
if
(
success
)
{
List
<
List
<
LocatedBlock
>>
blockGroupList
=
new
ArrayList
<
>
(
)
;
LocatedBlocks
lbs
=
dfs
.
getClient
(
)
.
getLocatedBlocks
(
srcPath
.
toString
(
)
,
0L
,
Long
.
MAX_VALUE
)
;
if
(
length
>
0
)
{
int
expectedNumGroup
=
(
length
-
1
)
/
blkGroupSize
+
1
;
assertEquals
(
expectedNumGroup
,
lbs
.
getLocatedBlocks
(
)
.
size
(
)
)
;
}
final
ErasureCodingPolicy
ecPolicy
=
dfs
.
getErasureCodingPolicy
(
srcPath
)
;
final
int
cellSize
=
ecPolicy
.
getCellSize
(
)
;
final
int
dataBlkNum
=
ecPolicy
.
getNumDataUnits
(
)
;
final
int
parityBlkNum
=
ecPolicy
.
getNumParityUnits
(
)
;
int
index
=
0
;
for
(
LocatedBlock
firstBlock
:
lbs
.
getLocatedBlocks
(
)
)
{
Assert
.
assertTrue
(
firstBlock
instanceof
LocatedStripedBlock
)
;
final
long
gs
=
firstBlock
.
getBlock
(
)
.
getGenerationStamp
(
)
;
final
long
oldGS
=
oldGSList
!=
null
?
oldGSList
.
get
(
index
++
)
:
-
1L
;
final
String
s
=
+
gs
+
+
oldGS
;
final
int
numCellInGroup
=
(
groupSize
-
1
)
/
cellSize
+
1
;
final
int
lastCellIndex
=
(
numCellInGroup
-
1
)
%
dataBlkNum
;
final
int
lastCellSize
=
groupSize
-
(
numCellInGroup
-
1
)
*
cellSize
;
List
<
LocatedBlock
>
blockList
=
blockGroupList
.
get
(
group
)
;
byte
[
]
[
]
dataBlockBytes
=
new
byte
[
dataBlkNum
]
[
]
;
byte
[
]
[
]
parityBlockBytes
=
new
byte
[
parityBlkNum
]
[
]
;
Set
<
Integer
>
checkSet
=
new
HashSet
<
>
(
)
;
for
(
int
i
=
0
;
i
<
blockList
.
size
(
)
;
i
++
)
{
final
int
j
=
i
>=
dataBlkNum
?
0
:
i
;
final
int
numCellInBlock
=
(
numCellInGroup
-
1
)
/
dataBlkNum
+
(
j
<=
lastCellIndex
?
1
:
0
)
;
final
int
blockSize
=
numCellInBlock
*
cellSize
+
(
isLastGroup
&&
j
==
lastCellIndex
?
lastCellSize
-
cellSize
:
0
)
;
final
byte
[
]
blockBytes
=
new
byte
[
blockSize
]
;
if
(
i
<
dataBlkNum
)
{
dataBlockBytes
[
i
]
=
blockBytes
;
}
else
{
public
static
LocatedBlocks
waitForReconstructionFinished
(
Path
file
,
DistributedFileSystem
fs
,
int
groupSize
)
throws
Exception
{
public
static
void
waitForAllReconstructionFinished
(
Path
file
,
DistributedFileSystem
fs
,
long
expectedBlocks
)
throws
Exception
{
void
setErrorState
(
Throwable
t
)
{
checkErrorState
(
)
;
DFSClientFaultInjector
.
set
(
new
DFSClientFaultInjector
(
)
{
public
boolean
skipRollingRestartWait
(
)
{
return
true
;
}
}
)
;
final
DFSOutputStream
out
=
(
DFSOutputStream
)
fileSys
.
append
(
file
)
.
getWrappedStream
(
)
;
final
AtomicBoolean
running
=
new
AtomicBoolean
(
true
)
;
final
AtomicBoolean
failed
=
new
AtomicBoolean
(
false
)
;
Thread
t
=
new
Thread
(
)
{
public
void
run
(
)
{
while
(
running
.
get
(
)
)
{
try
{
out
.
write
(
.
getBytes
(
)
)
;
out
.
hflush
(
)
;
Thread
.
sleep
(
1000
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
}
}
}
)
;
Random
r
=
new
Random
(
)
;
byte
[
]
b
=
new
byte
[
oneWriteSize
]
;
while
(
count
<
totalSize
)
{
r
.
nextBytes
(
b
)
;
o
.
write
(
b
)
;
count
+=
oneWriteSize
;
o
.
hflush
(
)
;
}
assertTrue
(
,
failed
.
get
(
)
)
;
DatanodeInfo
[
]
newNodes
=
dfsO
.
getStreamer
(
)
.
getNodes
(
)
;
o
.
close
(
)
;
for
(
DataNode
d
:
cluster
.
getDataNodes
(
)
)
{
DataNodeTestUtils
.
triggerBlockReport
(
d
)
;
}
List
<
DatanodeInfo
>
pipelineList
=
Arrays
.
asList
(
pipeline
)
;
DatanodeInfo
newNode
=
null
;
private
void
createAFileWithCorruptedBlockReplicas
(
Path
filePath
,
short
repl
,
int
corruptBlockCount
)
throws
IOException
,
AccessControlException
,
FileNotFoundException
,
UnresolvedLinkException
,
InterruptedException
,
TimeoutException
{
DFSTestUtil
.
createFile
(
dfs
,
filePath
,
BLOCK_SIZE
,
repl
,
0
)
;
DFSTestUtil
.
waitReplication
(
dfs
,
filePath
,
repl
)
;
final
LocatedBlocks
locatedblocks
=
dfs
.
dfs
.
getNamenode
(
)
.
getBlockLocations
(
filePath
.
toString
(
)
,
0L
,
BLOCK_SIZE
)
;
Assert
.
assertEquals
(
repl
,
locatedblocks
.
get
(
0
)
.
getLocations
(
)
.
length
)
;
LocatedBlock
lblock
=
locatedblocks
.
get
(
0
)
;
DatanodeInfo
[
]
datanodeinfos
=
lblock
.
getLocations
(
)
;
ExtendedBlock
block
=
lblock
.
getBlock
(
)
;
for
(
int
i
=
0
;
i
<
corruptBlockCount
;
i
++
)
{
DatanodeInfo
dninfo
=
datanodeinfos
[
i
]
;
final
DataNode
dn
=
cluster
.
getDataNode
(
dninfo
.
getIpcPort
(
)
)
;
cluster
.
corruptReplica
(
dn
,
block
)
;
private
static
void
verifyFsckHealth
(
String
expected
)
throws
Exception
{
String
outStr
=
runFsck
(
conf
,
0
,
true
,
)
;
private
static
void
verifyFsckBlockCorrupted
(
)
throws
Exception
{
String
outStr
=
runFsck
(
conf
,
1
,
true
,
)
;
private
static
void
testFsckListCorruptFilesBlocks
(
Path
filePath
,
int
errorCode
)
throws
Exception
{
String
outStr
=
runFsck
(
conf
,
errorCode
,
true
,
filePath
.
toString
(
)
,
)
;
private
void
pread
(
DFSInputStream
in
,
long
pos
,
byte
[
]
buffer
,
int
offset
,
int
length
,
byte
[
]
authenticData
)
throws
IOException
{
Assert
.
assertTrue
(
,
buffer
.
length
>=
offset
+
length
)
;
if
(
pos
>=
0
)
in
.
seek
(
pos
)
;
@
Test
public
void
testReadFromOneDN
(
)
throws
Exception
{
HdfsConfiguration
configuration
=
new
HdfsConfiguration
(
)
;
final
String
contextName
=
;
configuration
.
set
(
HdfsClientConfigKeys
.
DFS_CLIENT_CONTEXT
,
contextName
)
;
configuration
.
setLong
(
HdfsClientConfigKeys
.
DFS_CLIENT_SOCKET_TIMEOUT_KEY
,
100000000L
)
;
BlockReaderTestUtil
util
=
new
BlockReaderTestUtil
(
1
,
configuration
)
;
final
Path
testFile
=
new
Path
(
)
;
byte
authenticData
[
]
=
util
.
writeFile
(
testFile
,
FILE_SIZE
/
1024
)
;
DFSClient
client
=
new
DFSClient
(
new
InetSocketAddress
(
,
util
.
getCluster
(
)
.
getNameNodePort
(
)
)
,
util
.
getConf
(
)
)
;
DFSInputStream
in
=
client
.
open
(
testFile
.
toString
(
)
)
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
numDataNodes
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
util
.
createFiles
(
fs
,
,
replFactor
)
;
util
.
waitReplication
(
fs
,
,
(
short
)
2
)
;
final
int
dnIdx
=
0
;
final
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
dnIdx
)
;
final
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
List
<
ReplicaInfo
>
replicas
=
dn
.
getFSDataset
(
)
.
getFinalizedBlocks
(
bpid
)
;
assertTrue
(
,
!
replicas
.
isEmpty
(
)
)
;
for
(
int
idx
=
0
;
idx
<
replicas
.
size
(
)
;
idx
++
)
{
ReplicaInfo
replica
=
replicas
.
get
(
idx
)
;
ExtendedBlock
eb
=
new
ExtendedBlock
(
bpid
,
replica
)
;
if
(
idx
%
3
==
0
)
{
util
.
createFiles
(
fs
,
,
replFactor
)
;
util
.
waitReplication
(
fs
,
,
(
short
)
2
)
;
final
int
dnIdx
=
0
;
final
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
dnIdx
)
;
final
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
List
<
ReplicaInfo
>
replicas
=
dn
.
getFSDataset
(
)
.
getFinalizedBlocks
(
bpid
)
;
assertTrue
(
,
!
replicas
.
isEmpty
(
)
)
;
for
(
int
idx
=
0
;
idx
<
replicas
.
size
(
)
;
idx
++
)
{
ReplicaInfo
replica
=
replicas
.
get
(
idx
)
;
ExtendedBlock
eb
=
new
ExtendedBlock
(
bpid
,
replica
)
;
if
(
idx
%
3
==
0
)
{
LOG
.
info
(
+
eb
)
;
cluster
.
deleteMeta
(
dnIdx
,
eb
)
;
}
else
if
(
idx
%
3
==
1
)
{
final
int
newSize
=
2
;
Path
file
=
new
Path
(
src
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_BLOCK_SIZE_KEY
,
4096
)
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
NamenodeProtocols
preSpyNN
=
cluster
.
getNameNodeRpc
(
)
;
NamenodeProtocols
spyNN
=
spy
(
preSpyNN
)
;
DFSClient
client
=
new
DFSClient
(
null
,
spyNN
,
conf
,
null
)
;
doAnswer
(
new
Answer
<
LocatedBlock
>
(
)
{
private
int
getBlockCount
(
LocatedBlock
ret
)
throws
IOException
{
LocatedBlocks
lb
=
cluster
.
getNameNodeRpc
(
)
.
getBlockLocations
(
src
,
0
,
Long
.
MAX_VALUE
)
;
assertEquals
(
lb
.
getLastLocatedBlock
(
)
.
getBlock
(
)
,
ret
.
getBlock
(
)
)
;
return
lb
.
getLocatedBlocks
(
)
.
size
(
)
;
}
@
Override
public
LocatedBlock
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
final
int
blockCount
=
getBlockCount
(
ret
)
;
final
LocatedBlock
ret2
;
try
{
ret2
=
(
LocatedBlock
)
invocation
.
callRealMethod
(
)
;
}
catch
(
NotReplicatedYetException
e
)
{
throw
new
AssertionError
(
,
e
)
;
}
final
int
blockCount2
=
getBlockCount
(
ret2
)
;
assertEquals
(
blockCount
,
blockCount2
)
;
return
ret2
;
}
}
)
.
when
(
spyNN
)
.
addBlock
(
Mockito
.
anyString
(
)
,
Mockito
.
anyString
(
)
,
Mockito
.
<
ExtendedBlock
>
any
(
)
,
Mockito
.
<
DatanodeInfo
[
]
>
any
(
)
,
Mockito
.
anyLong
(
)
,
Mockito
.
<
String
[
]
>
any
(
)
,
Mockito
.
<
EnumSet
<
AddBlockFlag
>>
any
(
)
)
;
doAnswer
(
new
Answer
<
Boolean
>
(
)
{
@
Override
public
Boolean
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
LOG
.
info
(
)
;
if
(
!
(
Boolean
)
invocation
.
callRealMethod
(
)
)
{
LOG
.
info
(
)
;
static
void
parseMultipleLinearRandomRetry
(
String
expected
,
String
s
)
{
final
MultipleLinearRandomRetry
r
=
MultipleLinearRandomRetry
.
parseCommaSeparatedString
(
s
)
;
@
Test
public
void
testDefaultSendBufferSize
(
)
throws
IOException
{
final
int
sendBufferSize
=
getSendBufferSize
(
new
Configuration
(
)
)
;
@
Test
public
void
testSpecifiedSendBufferSize
(
)
throws
IOException
{
final
Configuration
conf1
=
new
Configuration
(
)
;
conf1
.
setInt
(
DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY
,
256
*
1024
)
;
final
int
sendBufferSize1
=
getSendBufferSize
(
conf1
)
;
final
Configuration
conf2
=
new
Configuration
(
)
;
conf2
.
setInt
(
DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY
,
1024
)
;
final
int
sendBufferSize2
=
getSendBufferSize
(
conf2
)
;
@
Test
public
void
testAutoTuningSendBufferSize
(
)
throws
IOException
{
final
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
DFS_CLIENT_SOCKET_SEND_BUFFER_SIZE_KEY
,
0
)
;
final
int
sendBufferSize
=
getSendBufferSize
(
conf
)
;
client
.
removeXAttr
(
,
)
;
client
.
setAcl
(
,
AclEntry
.
parseAclSpec
(
,
true
)
)
;
client
.
removeAcl
(
)
;
client
.
rename
(
,
)
;
client
.
truncate
(
,
BLOCK_SIZE
)
;
client
.
create
(
,
false
)
;
EventBatch
batch
=
null
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
long
txid
=
batch
.
getTxid
(
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
RENAME
)
;
Event
.
RenameEvent
re
=
(
Event
.
RenameEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertEquals
(
,
re
.
getDstPath
(
)
)
;
Assert
.
assertEquals
(
,
re
.
getSrcPath
(
)
)
;
Assert
.
assertTrue
(
re
.
getTimestamp
(
)
>
0
)
;
Event
.
RenameEvent
re
=
(
Event
.
RenameEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertEquals
(
,
re
.
getDstPath
(
)
)
;
Assert
.
assertEquals
(
,
re
.
getSrcPath
(
)
)
;
Assert
.
assertTrue
(
re
.
getTimestamp
(
)
>
0
)
;
LOG
.
info
(
re
.
toString
(
)
)
;
Assert
.
assertTrue
(
re
.
toString
(
)
.
startsWith
(
)
)
;
long
eventsBehind
=
eis
.
getTxidsBehindEstimate
(
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
RENAME
)
;
Event
.
RenameEvent
re2
=
(
Event
.
RenameEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
re2
.
getDstPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
re2
.
getSrcPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
re2
.
getTimestamp
(
)
>
0
)
;
LOG
.
info
(
re2
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CREATE
)
;
Event
.
CreateEvent
ce
=
(
Event
.
CreateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce
.
getiNodeType
(
)
==
Event
.
CreateEvent
.
INodeType
.
FILE
)
;
Assert
.
assertTrue
(
ce
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce
.
getCtime
(
)
>
0
)
;
Assert
.
assertTrue
(
ce
.
getReplication
(
)
>
0
)
;
Assert
.
assertTrue
(
ce
.
getSymlinkTarget
(
)
==
null
)
;
Assert
.
assertTrue
(
ce
.
getOverwrite
(
)
)
;
Assert
.
assertEquals
(
BLOCK_SIZE
,
ce
.
getDefaultBlockSize
(
)
)
;
Assert
.
assertTrue
(
ce
.
isErasureCoded
(
)
.
isPresent
(
)
)
;
Assert
.
assertFalse
(
ce
.
isErasureCoded
(
)
.
get
(
)
)
;
Assert
.
assertTrue
(
ce
.
getSymlinkTarget
(
)
==
null
)
;
Assert
.
assertTrue
(
ce
.
getOverwrite
(
)
)
;
Assert
.
assertEquals
(
BLOCK_SIZE
,
ce
.
getDefaultBlockSize
(
)
)
;
Assert
.
assertTrue
(
ce
.
isErasureCoded
(
)
.
isPresent
(
)
)
;
Assert
.
assertFalse
(
ce
.
isErasureCoded
(
)
.
get
(
)
)
;
LOG
.
info
(
ce
.
toString
(
)
)
;
Assert
.
assertTrue
(
ce
.
toString
(
)
.
startsWith
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CLOSE
)
;
Event
.
CloseEvent
ce2
=
(
Event
.
CloseEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce2
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce2
.
getFileSize
(
)
>
0
)
;
Assert
.
assertTrue
(
ce2
.
getTimestamp
(
)
>
0
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CLOSE
)
;
Event
.
CloseEvent
ce2
=
(
Event
.
CloseEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce2
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce2
.
getFileSize
(
)
>
0
)
;
Assert
.
assertTrue
(
ce2
.
getTimestamp
(
)
>
0
)
;
LOG
.
info
(
ce2
.
toString
(
)
)
;
Assert
.
assertTrue
(
ce2
.
toString
(
)
.
startsWith
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
APPEND
)
;
Event
.
AppendEvent
append2
=
(
Event
.
AppendEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertEquals
(
,
append2
.
getPath
(
)
)
;
Assert
.
assertFalse
(
append2
.
toNewBlock
(
)
)
;
Assert
.
assertFalse
(
append2
.
toNewBlock
(
)
)
;
LOG
.
info
(
append2
.
toString
(
)
)
;
Assert
.
assertTrue
(
append2
.
toString
(
)
.
startsWith
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CLOSE
)
;
Assert
.
assertTrue
(
(
(
Event
.
CloseEvent
)
batch
.
getEvents
(
)
[
0
]
)
.
getPath
(
)
.
equals
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
TIMES
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
TIMES
)
;
LOG
.
info
(
mue
.
toString
(
)
)
;
Assert
.
assertTrue
(
mue
.
toString
(
)
.
startsWith
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue2
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue2
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue2
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
REPLICATION
)
;
Assert
.
assertTrue
(
mue2
.
getReplication
(
)
==
1
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue2
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue2
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue2
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
REPLICATION
)
;
Assert
.
assertTrue
(
mue2
.
getReplication
(
)
==
1
)
;
LOG
.
info
(
mue2
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
3
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
APPEND
)
;
Assert
.
assertTrue
(
(
(
Event
.
AppendEvent
)
batch
.
getEvents
(
)
[
0
]
)
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
1
]
.
getEventType
(
)
==
Event
.
EventType
.
UNLINK
)
;
Event
.
UnlinkEvent
ue2
=
(
Event
.
UnlinkEvent
)
batch
.
getEvents
(
)
[
1
]
;
Assert
.
assertTrue
(
ue2
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ue2
.
getTimestamp
(
)
>
0
)
;
Assert
.
assertTrue
(
ue2
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ue2
.
getTimestamp
(
)
>
0
)
;
LOG
.
info
(
ue2
.
toString
(
)
)
;
Assert
.
assertTrue
(
ue2
.
toString
(
)
.
startsWith
(
)
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
2
]
.
getEventType
(
)
==
Event
.
EventType
.
CLOSE
)
;
Event
.
CloseEvent
ce3
=
(
Event
.
CloseEvent
)
batch
.
getEvents
(
)
[
2
]
;
Assert
.
assertTrue
(
ce3
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce3
.
getTimestamp
(
)
>
0
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
UNLINK
)
;
Event
.
UnlinkEvent
ue
=
(
Event
.
UnlinkEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ue
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ue
.
getTimestamp
(
)
>
0
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
UNLINK
)
;
Event
.
UnlinkEvent
ue
=
(
Event
.
UnlinkEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ue
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ue
.
getTimestamp
(
)
>
0
)
;
LOG
.
info
(
ue
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CREATE
)
;
Event
.
CreateEvent
ce4
=
(
Event
.
CreateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce4
.
getiNodeType
(
)
==
Event
.
CreateEvent
.
INodeType
.
DIRECTORY
)
;
Assert
.
assertTrue
(
ce4
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce4
.
getCtime
(
)
>
0
)
;
Assert
.
assertTrue
(
ce4
.
getReplication
(
)
==
0
)
;
Assert
.
assertTrue
(
ce4
.
getSymlinkTarget
(
)
==
null
)
;
Event
.
CreateEvent
ce4
=
(
Event
.
CreateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce4
.
getiNodeType
(
)
==
Event
.
CreateEvent
.
INodeType
.
DIRECTORY
)
;
Assert
.
assertTrue
(
ce4
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce4
.
getCtime
(
)
>
0
)
;
Assert
.
assertTrue
(
ce4
.
getReplication
(
)
==
0
)
;
Assert
.
assertTrue
(
ce4
.
getSymlinkTarget
(
)
==
null
)
;
LOG
.
info
(
ce4
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue3
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue3
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue3
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
PERMS
)
;
Assert
.
assertTrue
(
mue3
.
getPerms
(
)
.
toString
(
)
.
contains
(
)
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue3
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue3
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue3
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
PERMS
)
;
Assert
.
assertTrue
(
mue3
.
getPerms
(
)
.
toString
(
)
.
contains
(
)
)
;
LOG
.
info
(
mue3
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue4
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue4
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue4
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
OWNER
)
;
Assert
.
assertTrue
(
mue4
.
getOwnerName
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue4
.
getGroupName
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue4
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue4
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
OWNER
)
;
Assert
.
assertTrue
(
mue4
.
getOwnerName
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue4
.
getGroupName
(
)
.
equals
(
)
)
;
LOG
.
info
(
mue4
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CREATE
)
;
Event
.
CreateEvent
ce5
=
(
Event
.
CreateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce5
.
getiNodeType
(
)
==
Event
.
CreateEvent
.
INodeType
.
SYMLINK
)
;
Assert
.
assertTrue
(
ce5
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce5
.
getCtime
(
)
>
0
)
;
Assert
.
assertTrue
(
ce5
.
getReplication
(
)
==
0
)
;
Assert
.
assertTrue
(
ce5
.
getSymlinkTarget
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce5
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce5
.
getCtime
(
)
>
0
)
;
Assert
.
assertTrue
(
ce5
.
getReplication
(
)
==
0
)
;
Assert
.
assertTrue
(
ce5
.
getSymlinkTarget
(
)
.
equals
(
)
)
;
LOG
.
info
(
ce5
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue5
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue5
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue5
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
XATTRS
)
;
Assert
.
assertTrue
(
mue5
.
getxAttrs
(
)
.
size
(
)
==
1
)
;
Assert
.
assertTrue
(
mue5
.
getxAttrs
(
)
.
get
(
0
)
.
getName
(
)
.
contains
(
)
)
;
Assert
.
assertTrue
(
!
mue5
.
isxAttrsRemoved
(
)
)
;
Assert
.
assertTrue
(
mue5
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
XATTRS
)
;
Assert
.
assertTrue
(
mue5
.
getxAttrs
(
)
.
size
(
)
==
1
)
;
Assert
.
assertTrue
(
mue5
.
getxAttrs
(
)
.
get
(
0
)
.
getName
(
)
.
contains
(
)
)
;
Assert
.
assertTrue
(
!
mue5
.
isxAttrsRemoved
(
)
)
;
LOG
.
info
(
mue5
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue6
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue6
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue6
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
XATTRS
)
;
Assert
.
assertTrue
(
mue6
.
getxAttrs
(
)
.
size
(
)
==
1
)
;
Assert
.
assertTrue
(
mue6
.
getxAttrs
(
)
.
get
(
0
)
.
getName
(
)
.
contains
(
)
)
;
Assert
.
assertTrue
(
mue6
.
isxAttrsRemoved
(
)
)
;
Event
.
MetadataUpdateEvent
mue6
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue6
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue6
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
XATTRS
)
;
Assert
.
assertTrue
(
mue6
.
getxAttrs
(
)
.
size
(
)
==
1
)
;
Assert
.
assertTrue
(
mue6
.
getxAttrs
(
)
.
get
(
0
)
.
getName
(
)
.
contains
(
)
)
;
Assert
.
assertTrue
(
mue6
.
isxAttrsRemoved
(
)
)
;
LOG
.
info
(
mue6
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue7
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue7
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue7
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
ACLS
)
;
Assert
.
assertTrue
(
mue7
.
getAcls
(
)
.
contains
(
AclEntry
.
parseAclEntry
(
,
true
)
)
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue7
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue7
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue7
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
ACLS
)
;
Assert
.
assertTrue
(
mue7
.
getAcls
(
)
.
contains
(
AclEntry
.
parseAclEntry
(
,
true
)
)
)
;
LOG
.
info
(
mue7
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue8
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue8
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue8
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
ACLS
)
;
Assert
.
assertTrue
(
mue8
.
getAcls
(
)
==
null
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
METADATA
)
;
Event
.
MetadataUpdateEvent
mue8
=
(
Event
.
MetadataUpdateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
mue8
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
mue8
.
getMetadataType
(
)
==
Event
.
MetadataUpdateEvent
.
MetadataType
.
ACLS
)
;
Assert
.
assertTrue
(
mue8
.
getAcls
(
)
==
null
)
;
LOG
.
info
(
mue8
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
RENAME
)
;
Event
.
RenameEvent
re3
=
(
Event
.
RenameEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
re3
.
getDstPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
re3
.
getSrcPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
re3
.
getTimestamp
(
)
>
0
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
RENAME
)
;
Event
.
RenameEvent
re3
=
(
Event
.
RenameEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
re3
.
getDstPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
re3
.
getSrcPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
re3
.
getTimestamp
(
)
>
0
)
;
LOG
.
info
(
re3
.
toString
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
TRUNCATE
)
;
Event
.
TruncateEvent
et
=
(
(
Event
.
TruncateEvent
)
batch
.
getEvents
(
)
[
0
]
)
;
Assert
.
assertTrue
(
et
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
et
.
getFileSize
(
)
==
BLOCK_SIZE
)
;
Assert
.
assertTrue
(
et
.
getTimestamp
(
)
>
0
)
;
Assert
.
assertTrue
(
et
.
toString
(
)
.
startsWith
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
txid
=
checkTxid
(
batch
,
txid
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CREATE
)
;
ce
=
(
Event
.
CreateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce
.
getiNodeType
(
)
==
Event
.
CreateEvent
.
INodeType
.
FILE
)
;
Assert
.
assertTrue
(
ce
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce
.
getCtime
(
)
>
0
)
;
Assert
.
assertTrue
(
ce
.
getReplication
(
)
>
0
)
;
Assert
.
assertTrue
(
ce
.
getSymlinkTarget
(
)
==
null
)
;
Assert
.
assertFalse
(
ce
.
getOverwrite
(
)
)
;
Assert
.
assertEquals
(
BLOCK_SIZE
,
ce
.
getDefaultBlockSize
(
)
)
;
Assert
.
assertTrue
(
ce
.
isErasureCoded
(
)
.
isPresent
(
)
)
;
Assert
.
assertFalse
(
ce
.
isErasureCoded
(
)
.
get
(
)
)
;
batch
=
waitForNextEvents
(
eis
)
;
Assert
.
assertEquals
(
1
,
batch
.
getEvents
(
)
.
length
)
;
long
txid
=
batch
.
getTxid
(
)
;
long
eventsBehind
=
eis
.
getTxidsBehindEstimate
(
)
;
Assert
.
assertTrue
(
batch
.
getEvents
(
)
[
0
]
.
getEventType
(
)
==
Event
.
EventType
.
CREATE
)
;
Event
.
CreateEvent
ce
=
(
Event
.
CreateEvent
)
batch
.
getEvents
(
)
[
0
]
;
Assert
.
assertTrue
(
ce
.
getiNodeType
(
)
==
Event
.
CreateEvent
.
INodeType
.
FILE
)
;
Assert
.
assertTrue
(
ce
.
getPath
(
)
.
equals
(
)
)
;
Assert
.
assertTrue
(
ce
.
getCtime
(
)
>
0
)
;
Assert
.
assertEquals
(
1
,
ce
.
getReplication
(
)
)
;
Assert
.
assertTrue
(
ce
.
getSymlinkTarget
(
)
==
null
)
;
Assert
.
assertTrue
(
ce
.
getOverwrite
(
)
)
;
Assert
.
assertEquals
(
ecPolicy
.
getCellSize
(
)
,
ce
.
getDefaultBlockSize
(
)
)
;
Assert
.
assertTrue
(
ce
.
isErasureCoded
(
)
.
isPresent
(
)
)
;
Assert
.
assertTrue
(
ce
.
isErasureCoded
(
)
.
get
(
)
)
;
@
Test
public
void
testWithKerberizedCluster
(
)
throws
Exception
{
conf
=
new
HdfsConfiguration
(
baseConf
)
;
conf
.
setInt
(
HADOOP_KERBEROS_MIN_SECONDS_BEFORE_RELOGIN
,
3
)
;
conf
.
setInt
(
IPC_CLIENT_CONNECTION_IDLESCANINTERVAL_KEY
,
100
)
;
conf
.
setInt
(
IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY
,
2000
)
;
Client
.
setConnectTimeout
(
conf
,
2000
)
;
cluster
=
new
MiniQJMHACluster
.
Builder
(
conf
)
.
setForceRemoteEditsOnly
(
true
)
.
build
(
)
;
cluster
.
getDfsCluster
(
)
.
waitActive
(
)
;
cluster
.
getDfsCluster
(
)
.
transitionToActive
(
0
)
;
final
UserGroupInformation
ugi
=
UserGroupInformation
.
loginUserFromKeytabAndReturnUGI
(
,
generalHDFSKeytabFile
.
getAbsolutePath
(
)
)
;
UserGroupInformation
.
setShouldRenewImmediatelyForTests
(
true
)
;
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
cluster
=
new
MiniQJMHACluster
.
Builder
(
conf
)
.
setForceRemoteEditsOnly
(
true
)
.
build
(
)
;
cluster
.
getDfsCluster
(
)
.
waitActive
(
)
;
cluster
.
getDfsCluster
(
)
.
transitionToActive
(
0
)
;
final
UserGroupInformation
ugi
=
UserGroupInformation
.
loginUserFromKeytabAndReturnUGI
(
,
generalHDFSKeytabFile
.
getAbsolutePath
(
)
)
;
UserGroupInformation
.
setShouldRenewImmediatelyForTests
(
true
)
;
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
LOG
.
info
(
+
UserGroupInformation
.
getCurrentUser
(
)
+
+
UserGroupInformation
.
getLoginUser
(
)
)
;
Configuration
clientConf
=
new
Configuration
(
cluster
.
getDfsCluster
(
)
.
getConfiguration
(
0
)
)
;
try
(
DistributedFileSystem
clientFs
=
(
DistributedFileSystem
)
FileSystem
.
get
(
clientConf
)
)
{
clientFs
.
mkdirs
(
new
Path
(
)
)
;
LOG
.
info
(
)
;
final
DFSInotifyEventInputStream
eis
=
clientFs
.
getInotifyEventStream
(
)
;
EventBatch
batch
;
while
(
(
batch
=
eis
.
poll
(
)
)
!=
null
)
{
final
Path
file
=
new
Path
(
dir
,
)
;
writeFile
(
dfs
,
file
)
;
final
Path
file2
=
new
Path
(
dir
,
)
;
writeFile
(
dfs
,
file2
)
;
final
Long
fileLength
=
dfs
.
getFileStatus
(
file
)
.
getLen
(
)
;
final
Long
fileDiskUsed
=
fileLength
*
replication
;
final
Long
file2Length
=
dfs
.
getFileStatus
(
file2
)
.
getLen
(
)
;
final
Long
file2DiskUsed
=
file2Length
*
replication
;
int
ret
=
-
1
;
try
{
ret
=
shell
.
run
(
new
String
[
]
{
,
dir
.
toString
(
)
}
)
;
}
catch
(
Exception
e
)
{
System
.
err
.
println
(
+
e
.
getLocalizedMessage
(
)
)
;
}
assertEquals
(
0
,
ret
)
;
String
returnString
=
out
.
toString
(
)
;
}
catch
(
Exception
e
)
{
System
.
err
.
println
(
+
e
.
getLocalizedMessage
(
)
)
;
}
assertEquals
(
0
,
ret
)
;
returnString
=
out
.
toString
(
)
;
LOG
.
info
(
+
returnString
)
;
Long
combinedLength
=
fileLength
+
file2Length
+
newFileLength
;
Long
combinedDiskUsed
=
fileDiskUsed
+
file2DiskUsed
+
newFileDiskUsed
;
assertTrue
(
returnString
.
contains
(
combinedLength
.
toString
(
)
)
)
;
assertTrue
(
returnString
.
contains
(
combinedDiskUsed
.
toString
(
)
)
)
;
out
.
reset
(
)
;
ret
=
-
1
;
try
{
ret
=
shell
.
run
(
new
String
[
]
{
,
parent
.
toString
(
)
}
)
;
}
catch
(
Exception
e
)
{
System
.
err
.
println
(
+
e
.
getLocalizedMessage
(
)
)
;
out
.
reset
(
)
;
ret
=
-
1
;
try
{
ret
=
shell
.
run
(
new
String
[
]
{
,
parent
.
toString
(
)
}
)
;
}
catch
(
Exception
e
)
{
System
.
err
.
println
(
+
e
.
getLocalizedMessage
(
)
)
;
}
assertEquals
(
0
,
ret
)
;
returnString
=
out
.
toString
(
)
;
LOG
.
info
(
+
returnString
)
;
assertTrue
(
returnString
.
contains
(
combinedLength
.
toString
(
)
)
)
;
assertTrue
(
returnString
.
contains
(
combinedDiskUsed
.
toString
(
)
)
)
;
out
.
reset
(
)
;
ret
=
-
1
;
try
{
ret
=
shell
.
run
(
new
String
[
]
{
,
,
,
parent
.
toString
(
)
}
)
;
out
.
reset
(
)
;
ret
=
-
1
;
try
{
ret
=
shell
.
run
(
new
String
[
]
{
,
,
,
parent
.
toString
(
)
}
)
;
}
catch
(
Exception
e
)
{
System
.
err
.
println
(
+
e
.
getLocalizedMessage
(
)
)
;
}
assertEquals
(
0
,
ret
)
;
returnString
=
out
.
toString
(
)
;
LOG
.
info
(
+
returnString
)
;
Long
exludeSnapshotLength
=
file2Length
+
newFileLength
;
Long
excludeSnapshotDiskUsed
=
file2DiskUsed
+
newFileDiskUsed
;
assertTrue
(
returnString
.
contains
(
exludeSnapshotLength
.
toString
(
)
)
)
;
assertTrue
(
returnString
.
contains
(
excludeSnapshotDiskUsed
.
toString
(
)
)
)
;
out
.
reset
(
)
;
ret
=
-
1
;
final
Path
snapshotPath
=
new
Path
(
parent
,
+
snapshotName
)
;
dfs
.
allowSnapshot
(
parent
)
;
assertThat
(
dfs
.
createSnapshot
(
parent
,
snapshotName
)
,
is
(
snapshotPath
)
)
;
rmr
(
dfs
,
file
)
;
rmr
(
dfs
,
dir2
)
;
final
Path
newFile
=
new
Path
(
dir
,
)
;
writeFile
(
dfs
,
newFile
)
;
final
Long
newFileLength
=
dfs
.
getFileStatus
(
newFile
)
.
getLen
(
)
;
int
val
=
-
1
;
try
{
val
=
shell
.
run
(
new
String
[
]
{
,
,
parent
.
toString
(
)
}
)
;
}
catch
(
Exception
e
)
{
System
.
err
.
println
(
+
e
.
getLocalizedMessage
(
)
)
;
}
assertEquals
(
0
,
val
)
;
String
returnString
=
out
.
toString
(
)
;
}
assertEquals
(
0
,
val
)
;
String
returnString
=
out
.
toString
(
)
;
LOG
.
info
(
+
returnString
)
;
Scanner
in
=
new
Scanner
(
returnString
)
;
in
.
nextLine
(
)
;
assertEquals
(
3
,
in
.
nextLong
(
)
)
;
assertEquals
(
3
,
in
.
nextLong
(
)
)
;
assertEquals
(
fileLength
+
file2Length
+
newFileLength
,
in
.
nextLong
(
)
)
;
out
.
reset
(
)
;
val
=
-
1
;
try
{
val
=
shell
.
run
(
new
String
[
]
{
,
,
,
parent
.
toString
(
)
}
)
;
}
catch
(
Exception
e
)
{
System
.
err
.
println
(
+
e
.
getLocalizedMessage
(
)
)
;
}
assertEquals
(
0
,
val
)
;
private
static
int
runCmd
(
FsShell
shell
,
String
...
args
)
throws
IOException
{
StringBuilder
cmdline
=
new
StringBuilder
(
)
;
for
(
String
arg
:
args
)
cmdline
.
append
(
+
arg
)
;
private
static
int
runCmd
(
FsShell
shell
,
String
...
args
)
throws
IOException
{
StringBuilder
cmdline
=
new
StringBuilder
(
)
;
for
(
String
arg
:
args
)
cmdline
.
append
(
+
arg
)
;
LOG
.
info
(
cmdline
.
toString
(
)
)
;
try
{
int
exitCode
;
exitCode
=
shell
.
run
(
args
)
;
shell
.
setConf
(
conf
)
;
try
{
Path
dir
=
new
Path
(
chmodDir
)
;
fs
.
delete
(
dir
,
true
)
;
fs
.
mkdirs
(
dir
)
;
confirmPermissionChange
(
,
,
fs
,
shell
,
dir
)
;
Path
file
=
new
Path
(
chmodDir
,
)
;
TestDFSShell
.
writeFile
(
fs
,
file
)
;
confirmPermissionChange
(
,
,
fs
,
shell
,
file
)
;
runCmd
(
shell
,
,
,
,
chmodDir
)
;
assertEquals
(
,
fs
.
getFileStatus
(
dir
)
.
getPermission
(
)
.
toString
(
)
)
;
assertEquals
(
,
fs
.
getFileStatus
(
file
)
.
getPermission
(
)
.
toString
(
)
)
;
if
(
!
Path
.
WINDOWS
)
{
Path
dir2
=
new
Path
(
dir
,
)
;
fs
.
mkdirs
(
dir2
)
;
private
void
confirmPermissionChange
(
String
toApply
,
String
expected
,
FileSystem
fs
,
FsShell
shell
,
Path
dir2
)
throws
IOException
{
private
void
confirmPermissionChange
(
String
toApply
,
String
expected
,
FileSystem
fs
,
FsShell
shell
,
Path
dir2
)
throws
IOException
{
LOG
.
info
(
+
toApply
+
+
expected
)
;
runCmd
(
shell
,
,
toApply
,
dir2
.
toString
(
)
)
;
String
result
=
fs
.
getFileStatus
(
dir2
)
.
getPermission
(
)
.
toString
(
)
;
@
Test
public
void
testCloseDoesNotAllocateNewBuffer
(
)
throws
Exception
{
final
int
numBlocks
=
2
;
DFSTestUtil
.
createStripedFile
(
cluster
,
filePath
,
null
,
numBlocks
,
stripesPerBlock
,
false
,
ecPolicy
)
;
try
(
DFSInputStream
in
=
fs
.
getClient
(
)
.
open
(
filePath
.
toString
(
)
)
)
{
assertTrue
(
in
instanceof
DFSStripedInputStream
)
;
final
DFSStripedInputStream
stream
=
(
DFSStripedInputStream
)
in
;
final
ElasticByteBufferPool
ebbp
=
(
ElasticByteBufferPool
)
stream
.
getBufferPool
(
)
;
public
void
testMultipleDatanodeFailureRandomLength
(
)
throws
Exception
{
int
lenIndex
=
RANDOM
.
nextInt
(
lengths
.
size
(
)
)
;
final
HdfsConfiguration
conf
=
newHdfsConfiguration
(
)
;
final
int
[
]
fileLengths
=
{
cellSize
*
(
dataBlocks
*
2
-
2
)
,
(
cellSize
*
dataBlocks
)
+
123
}
;
int
[
]
dnIndex
=
null
;
if
(
parityBlocks
>
1
)
{
dnIndex
=
new
int
[
]
{
dataBlocks
-
2
,
dataBlocks
-
1
}
;
}
else
{
dnIndex
=
new
int
[
]
{
dataBlocks
-
1
}
;
}
for
(
int
length
:
fileLengths
)
{
final
int
[
]
killPos
=
getKillPositions
(
length
,
dnIndex
.
length
)
;
try
{
LOG
.
info
(
+
length
+
+
Arrays
.
toString
(
killPos
)
+
+
Arrays
.
toString
(
dnIndex
)
)
;
setup
(
conf
)
;
runTest
(
length
,
killPos
,
dnIndex
,
false
)
;
}
catch
(
Throwable
e
)
{
final
String
err
=
+
Arrays
.
toString
(
killPos
)
+
+
Arrays
.
toString
(
dnIndex
)
+
+
length
;
void
runTest
(
final
int
length
)
{
final
HdfsConfiguration
conf
=
newHdfsConfiguration
(
)
;
for
(
int
dn
=
0
;
dn
<
dataBlocks
+
parityBlocks
;
dn
++
)
{
try
{
void
runTestWithMultipleFailure
(
final
int
length
)
throws
Exception
{
final
HdfsConfiguration
conf
=
newHdfsConfiguration
(
)
;
for
(
int
[
]
dnIndex
:
dnIndexSuite
)
{
int
[
]
killPos
=
getKillPositions
(
length
,
dnIndex
.
length
)
;
try
{
static
long
getGenerationStamp
(
DFSStripedOutputStream
out
)
throws
IOException
{
final
long
gs
=
out
.
getBlock
(
)
.
getGenerationStamp
(
)
;
static
DatanodeInfo
killDatanode
(
MiniDFSCluster
cluster
,
DFSStripedOutputStream
out
,
final
int
dnIndex
,
final
AtomicInteger
pos
)
{
final
StripedDataStreamer
s
=
out
.
getStripedDataStreamer
(
dnIndex
)
;
final
DatanodeInfo
datanode
=
getDatanodes
(
s
)
;
void
checkNameNode
(
String
[
]
baseDirs
,
long
imageTxId
)
throws
IOException
{
for
(
String
baseDir
:
baseDirs
)
{
conf
=
new
HdfsConfiguration
(
)
;
conf
=
UpgradeUtilities
.
initializeStorageStateConf
(
numDirs
,
conf
)
;
String
[
]
nameNodeDirs
=
conf
.
getStrings
(
DFSConfigKeys
.
DFS_NAMENODE_NAME_DIR_KEY
)
;
String
[
]
dataNodeDirs
=
conf
.
getStrings
(
DFSConfigKeys
.
DFS_DATANODE_DATA_DIR_KEY
)
;
conf
.
setBoolean
(
DFSConfigKeys
.
DFS_DATANODE_DUPLICATE_REPLICA_DELETION
,
false
)
;
log
(
,
numDirs
)
;
UpgradeUtilities
.
createNameNodeStorageDirs
(
nameNodeDirs
,
)
;
cluster
=
createCluster
(
)
;
try
{
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
dfs
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_ENTER
)
;
dfs
.
rollingUpgrade
(
RollingUpgradeAction
.
PREPARE
)
;
fail
(
)
;
}
catch
(
RemoteException
re
)
{
assertEquals
(
InconsistentFSStateException
.
class
.
getName
(
)
,
re
.
getClassName
(
)
)
;
private
void
sendRecvData
(
String
testDescription
,
boolean
eofExpected
)
throws
IOException
{
Socket
sock
=
null
;
try
{
if
(
testDescription
!=
null
)
{
sock
.
connect
(
dnAddr
,
HdfsConstants
.
READ_TIMEOUT
)
;
sock
.
setSoTimeout
(
HdfsConstants
.
READ_TIMEOUT
)
;
OutputStream
out
=
sock
.
getOutputStream
(
)
;
byte
[
]
retBuf
=
new
byte
[
recvBuf
.
size
(
)
]
;
DataInputStream
in
=
new
DataInputStream
(
sock
.
getInputStream
(
)
)
;
out
.
write
(
sendBuf
.
toByteArray
(
)
)
;
out
.
flush
(
)
;
try
{
in
.
readFully
(
retBuf
)
;
}
catch
(
EOFException
eof
)
{
if
(
eofExpected
)
{
LOG
.
info
(
)
;
return
;
}
throw
eof
;
}
String
received
=
StringUtils
.
byteToHexString
(
retBuf
)
;
sock
.
setSoTimeout
(
HdfsConstants
.
READ_TIMEOUT
)
;
OutputStream
out
=
sock
.
getOutputStream
(
)
;
byte
[
]
retBuf
=
new
byte
[
recvBuf
.
size
(
)
]
;
DataInputStream
in
=
new
DataInputStream
(
sock
.
getInputStream
(
)
)
;
out
.
write
(
sendBuf
.
toByteArray
(
)
)
;
out
.
flush
(
)
;
try
{
in
.
readFully
(
retBuf
)
;
}
catch
(
EOFException
eof
)
{
if
(
eofExpected
)
{
LOG
.
info
(
)
;
return
;
}
throw
eof
;
}
String
received
=
StringUtils
.
byteToHexString
(
retBuf
)
;
String
expected
=
StringUtils
.
byteToHexString
(
recvBuf
.
toByteArray
(
)
)
;
doReturn
(
HdfsServerConstants
.
DATANODE_LAYOUT_VERSION
)
.
when
(
mockDnReg
)
.
getVersion
(
)
;
doReturn
(
)
.
when
(
mockDnReg
)
.
getIpAddr
(
)
;
doReturn
(
123
)
.
when
(
mockDnReg
)
.
getXferPort
(
)
;
doReturn
(
)
.
when
(
mockDnReg
)
.
getDatanodeUuid
(
)
;
doReturn
(
mockStorageInfo
)
.
when
(
mockDnReg
)
.
getStorageInfo
(
)
;
doReturn
(
)
.
when
(
mockDnReg
)
.
getSoftwareVersion
(
)
;
rpcServer
.
registerDatanode
(
mockDnReg
)
;
doReturn
(
)
.
when
(
mockDnReg
)
.
getSoftwareVersion
(
)
;
rpcServer
.
registerDatanode
(
mockDnReg
)
;
doReturn
(
)
.
when
(
mockDnReg
)
.
getSoftwareVersion
(
)
;
try
{
rpcServer
.
registerDatanode
(
mockDnReg
)
;
fail
(
)
;
}
catch
(
IncorrectVersionException
ive
)
{
GenericTestUtils
.
assertExceptionContains
(
,
ive
)
;
doReturn
(
HdfsServerConstants
.
DATANODE_LAYOUT_VERSION
)
.
when
(
mockDnReg
)
.
getVersion
(
)
;
doReturn
(
)
.
when
(
mockDnReg
)
.
getDatanodeUuid
(
)
;
doReturn
(
mockStorageInfo
)
.
when
(
mockDnReg
)
.
getStorageInfo
(
)
;
doReturn
(
VersionInfo
.
getVersion
(
)
)
.
when
(
mockDnReg
)
.
getSoftwareVersion
(
)
;
doReturn
(
)
.
when
(
mockDnReg
)
.
getIpAddr
(
)
;
doReturn
(
123
)
.
when
(
mockDnReg
)
.
getXferPort
(
)
;
rpcServer
.
registerDatanode
(
mockDnReg
)
;
doReturn
(
nnCTime
+
1
)
.
when
(
mockStorageInfo
)
.
getCTime
(
)
;
rpcServer
.
registerDatanode
(
mockDnReg
)
;
doReturn
(
VersionInfo
.
getVersion
(
)
+
)
.
when
(
mockDnReg
)
.
getSoftwareVersion
(
)
;
try
{
rpcServer
.
registerDatanode
(
mockDnReg
)
;
fail
(
+
)
;
}
catch
(
IncorrectVersionException
ive
)
{
GenericTestUtils
.
assertExceptionContains
(
,
ive
)
;
@
Test
public
void
testDatanodeReport
(
)
throws
Exception
{
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY
,
500
)
;
conf
.
setLong
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
1L
)
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
NUM_OF_DATANODES
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
final
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
final
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
final
DFSClient
client
=
cluster
.
getFileSystem
(
)
.
dfs
;
assertReports
(
NUM_OF_DATANODES
,
DatanodeReportType
.
ALL
,
client
,
datanodes
,
bpid
)
;
assertReports
(
NUM_OF_DATANODES
,
DatanodeReportType
.
LIVE
,
client
,
datanodes
,
bpid
)
;
assertReports
(
0
,
DatanodeReportType
.
DEAD
,
client
,
datanodes
,
bpid
)
;
final
DataNode
last
=
datanodes
.
get
(
datanodes
.
size
(
)
-
1
)
;
final
DatanodeManager
datanodeManager
=
blockManager
.
getDatanodeManager
(
)
;
BlockManagerTestUtil
.
recheckDecommissionState
(
datanodeManager
)
;
DFSClient
client
=
getDfsClient
(
0
)
;
assertEquals
(
,
numDatanodes
,
client
.
datanodeReport
(
DatanodeReportType
.
LIVE
)
.
length
)
;
final
ExtendedBlock
b
=
DFSTestUtil
.
getFirstBlock
(
fileSys
,
file1
)
;
final
String
uuid
=
toDecomUuid
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
BlockInfo
info
=
blockManager
.
getStoredBlock
(
b
.
getLocalBlock
(
)
)
;
int
count
=
0
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
for
(
int
i
=
0
;
i
<
info
.
numNodes
(
)
;
i
++
)
{
DatanodeDescriptor
dn
=
info
.
getDatanode
(
i
)
;
sb
.
append
(
dn
+
)
;
if
(
!
dn
.
getDatanodeUuid
(
)
.
equals
(
uuid
)
)
{
final
DatanodeManager
datanodeManager
=
blockManager
.
getDatanodeManager
(
)
;
BlockManagerTestUtil
.
recheckDecommissionState
(
datanodeManager
)
;
DFSClient
client
=
getDfsClient
(
0
)
;
assertEquals
(
,
numDatanodes
,
client
.
datanodeReport
(
DatanodeReportType
.
LIVE
)
.
length
)
;
final
ExtendedBlock
b
=
DFSTestUtil
.
getFirstBlock
(
fileSys
,
file1
)
;
final
String
uuid
=
toDecomUuid
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
BlockInfo
info
=
blockManager
.
getStoredBlock
(
b
.
getLocalBlock
(
)
)
;
int
count
=
0
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
for
(
int
i
=
0
;
i
<
info
.
numNodes
(
)
;
i
++
)
{
DatanodeDescriptor
dn
=
info
.
getDatanode
(
i
)
;
sb
.
append
(
dn
+
)
;
if
(
!
dn
.
getDatanodeUuid
(
)
.
equals
(
uuid
)
)
{
private
boolean
verifyOpenFilesListing
(
String
message
,
HashSet
<
Path
>
closedFileSet
,
HashMap
<
Path
,
FSDataOutputStream
>
openFilesMap
,
ByteArrayOutputStream
out
,
int
expOpenFilesListSize
)
{
final
String
outStr
=
scanIntoString
(
out
)
;
final
CountDownLatch
decomStarted
=
new
CountDownLatch
(
0
)
;
Thread
decomTh
=
new
Thread
(
)
{
public
void
run
(
)
{
try
{
decomStarted
.
countDown
(
)
;
decommissionNode
(
0
,
decommisionNodes
,
AdminStates
.
DECOMMISSIONED
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
)
;
}
}
}
;
int
deadDecommissioned
=
fsn
.
getNumDecomDeadDataNodes
(
)
;
int
liveDecommissioned
=
fsn
.
getNumDecomLiveDataNodes
(
)
;
decomTh
.
start
(
)
;
decomStarted
.
await
(
5
,
TimeUnit
.
SECONDS
)
;
Thread
.
sleep
(
3000
)
;
public
void
run
(
)
{
try
{
decomStarted
.
countDown
(
)
;
decommissionNode
(
0
,
decommisionNodes
,
AdminStates
.
DECOMMISSIONED
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
)
;
}
}
}
;
int
deadDecommissioned
=
fsn
.
getNumDecomDeadDataNodes
(
)
;
int
liveDecommissioned
=
fsn
.
getNumDecomLiveDataNodes
(
)
;
decomTh
.
start
(
)
;
decomStarted
.
await
(
5
,
TimeUnit
.
SECONDS
)
;
Thread
.
sleep
(
3000
)
;
for
(
DataNodeProperties
dnp
:
stoppedDns
)
{
cluster
.
restartDataNode
(
dnp
)
;
int
writeBytes
=
cellSize
*
dataBlocks
;
writeStripedFile
(
dfs
,
ecFile
,
writeBytes
)
;
Assert
.
assertEquals
(
0
,
bm
.
numOfUnderReplicatedBlocks
(
)
)
;
FileChecksum
fileChecksum1
=
dfs
.
getFileChecksum
(
ecFile
,
writeBytes
)
;
final
List
<
DatanodeInfo
>
decommisionNodes
=
new
ArrayList
<
DatanodeInfo
>
(
)
;
LocatedBlock
lb
=
dfs
.
getClient
(
)
.
getLocatedBlocks
(
ecFile
.
toString
(
)
,
0
)
.
get
(
0
)
;
DatanodeInfo
[
]
dnLocs
=
lb
.
getLocations
(
)
;
assertEquals
(
dataBlocks
+
parityBlocks
,
dnLocs
.
length
)
;
int
decommNodeIndex
=
1
;
decommisionNodes
.
add
(
dnLocs
[
decommNodeIndex
]
)
;
decommissionNode
(
0
,
decommisionNodes
,
AdminStates
.
DECOMMISSIONED
)
;
assertEquals
(
decommisionNodes
.
size
(
)
,
fsn
.
getNumDecomLiveDataNodes
(
)
)
;
assertNull
(
checkFile
(
dfs
,
ecFile
,
9
,
decommisionNodes
,
numDNs
)
)
;
StripedFileTestUtil
.
checkData
(
dfs
,
ecFile
,
writeBytes
,
decommisionNodes
,
null
,
blockGroupSize
)
;
FileChecksum
fileChecksum2
=
dfs
.
getFileChecksum
(
ecFile
,
writeBytes
)
;
writeStripedFile
(
dfs
,
ecFile
,
writeBytes
)
;
Assert
.
assertEquals
(
0
,
bm
.
numOfUnderReplicatedBlocks
(
)
)
;
FileChecksum
fileChecksum1
=
dfs
.
getFileChecksum
(
ecFile
,
writeBytes
)
;
final
List
<
DatanodeInfo
>
decommisionNodes
=
new
ArrayList
<
DatanodeInfo
>
(
)
;
LocatedBlock
lb
=
dfs
.
getClient
(
)
.
getLocatedBlocks
(
ecFile
.
toString
(
)
,
0
)
.
get
(
0
)
;
DatanodeInfo
[
]
dnLocs
=
lb
.
getLocations
(
)
;
assertEquals
(
dataBlocks
+
parityBlocks
,
dnLocs
.
length
)
;
int
decommNodeIndex
=
1
;
decommisionNodes
.
add
(
dnLocs
[
decommNodeIndex
]
)
;
decommissionNode
(
0
,
decommisionNodes
,
AdminStates
.
DECOMMISSIONED
)
;
assertEquals
(
decommisionNodes
.
size
(
)
,
fsn
.
getNumDecomLiveDataNodes
(
)
)
;
assertNull
(
checkFile
(
dfs
,
ecFile
,
9
,
decommisionNodes
,
numDNs
)
)
;
StripedFileTestUtil
.
checkData
(
dfs
,
ecFile
,
writeBytes
,
decommisionNodes
,
null
,
blockGroupSize
)
;
FileChecksum
fileChecksum2
=
dfs
.
getFileChecksum
(
ecFile
,
writeBytes
)
;
LOG
.
info
(
+
fileChecksum1
)
;
private
void
waitNodeState
(
DatanodeInfo
node
,
AdminStates
state
)
{
boolean
done
=
state
==
node
.
getAdminState
(
)
;
while
(
!
done
)
{
private
static
String
checkFile
(
FileSystem
fileSys
,
Path
name
,
int
repl
,
List
<
DatanodeInfo
>
decommissionedNodes
,
int
numDatanodes
)
throws
IOException
{
boolean
isNodeDown
=
decommissionedNodes
.
size
(
)
>
0
;
assertTrue
(
+
fileSys
.
getUri
(
)
,
fileSys
instanceof
DistributedFileSystem
)
;
HdfsDataInputStream
dis
=
(
HdfsDataInputStream
)
fileSys
.
open
(
name
)
;
Collection
<
LocatedBlock
>
dinfo
=
dis
.
getAllBlocks
(
)
;
for
(
LocatedBlock
blk
:
dinfo
)
{
int
hasdown
=
0
;
DatanodeInfo
[
]
nodes
=
blk
.
getLocations
(
)
;
for
(
int
j
=
0
;
j
<
nodes
.
length
;
j
++
)
{
final
ExecutorService
threadPool
=
HadoopExecutors
.
newFixedThreadPool
(
numThreads
)
;
try
{
final
CountDownLatch
allExecutorThreadsReady
=
new
CountDownLatch
(
numThreads
)
;
final
CountDownLatch
startBlocker
=
new
CountDownLatch
(
1
)
;
final
CountDownLatch
allDone
=
new
CountDownLatch
(
numThreads
)
;
final
AtomicReference
<
Throwable
>
childError
=
new
AtomicReference
<
>
(
)
;
for
(
int
i
=
0
;
i
<
numThreads
;
i
++
)
{
threadPool
.
submit
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
allExecutorThreadsReady
.
countDown
(
)
;
try
{
startBlocker
.
await
(
)
;
final
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
fs
.
mkdirs
(
new
Path
(
)
)
;
}
catch
(
Throwable
t
)
{
childError
.
compareAndSet
(
null
,
t
)
;
}
finally
{
allDone
.
countDown
(
)
;
}
}
}
)
;
}
final
long
oldMkdirOpCount
=
getOpStatistics
(
OpType
.
MKDIRS
)
;
allExecutorThreadsReady
.
await
(
)
;
startBlocker
.
countDown
(
)
;
allDone
.
await
(
)
;
assertNull
(
+
childError
.
get
(
)
,
childError
.
get
(
)
)
;
checkStatistics
(
fs
,
0
,
numThreads
,
0
)
;
checkOpStatistics
(
OpType
.
MKDIRS
,
numThreads
+
oldMkdirOpCount
)
;
for
(
Iterator
<
LongStatistic
>
opCountIter
=
FileSystem
.
getGlobalStorageStatistics
(
)
.
get
(
DFSOpsCountStatistics
.
NAME
)
.
getLongStatistics
(
)
;
opCountIter
.
hasNext
(
)
;
)
{
final
LongStatistic
opCount
=
opCountIter
.
next
(
)
;
if
(
OpType
.
MKDIRS
.
getSymbol
(
)
.
equals
(
opCount
.
getName
(
)
)
)
{
assertEquals
(
,
numThreads
+
oldMkdirOpCount
,
opCount
.
getValue
(
)
)
;
@
Test
public
void
testSkewedRack1
(
)
throws
Exception
{
final
int
dataUnits
=
ecPolicy
.
getNumDataUnits
(
)
;
final
int
parityUnits
=
ecPolicy
.
getNumParityUnits
(
)
;
setupCluster
(
dataUnits
+
parityUnits
,
2
,
1
)
;
final
int
filesize
=
ecPolicy
.
getNumDataUnits
(
)
*
ecPolicy
.
getCellSize
(
)
;
byte
[
]
contents
=
new
byte
[
filesize
]
;
final
Path
path
=
new
Path
(
)
;
@
Test
public
void
testSkewedRack2
(
)
throws
Exception
{
final
int
dataUnits
=
ecPolicy
.
getNumDataUnits
(
)
;
final
int
parityUnits
=
ecPolicy
.
getNumParityUnits
(
)
;
setupCluster
(
dataUnits
+
parityUnits
*
2
,
dataUnits
,
dataUnits
-
1
)
;
final
int
filesize
=
ecPolicy
.
getNumDataUnits
(
)
*
ecPolicy
.
getCellSize
(
)
;
byte
[
]
contents
=
new
byte
[
filesize
]
;
final
Path
path
=
new
Path
(
)
;
@
Test
public
void
testSkewedRack3
(
)
throws
Exception
{
final
int
dataUnits
=
ecPolicy
.
getNumDataUnits
(
)
;
final
int
parityUnits
=
ecPolicy
.
getNumParityUnits
(
)
;
int
numRacks
=
dataUnits
-
parityUnits
+
2
;
setupCluster
(
dataUnits
+
parityUnits
*
4
,
numRacks
,
dataUnits
-
parityUnits
)
;
final
int
filesize
=
ecPolicy
.
getNumDataUnits
(
)
*
ecPolicy
.
getCellSize
(
)
;
byte
[
]
contents
=
new
byte
[
filesize
]
;
for
(
int
i
=
0
;
i
<
10
;
++
i
)
{
final
Path
path
=
new
Path
(
+
i
)
;
create
.
close
(
)
;
DFSTestUtil
.
waitReplication
(
fileSystem
,
f
,
(
short
)
2
)
;
LocatedBlocks
lbs
=
fileSystem
.
dfs
.
getNamenode
(
)
.
getBlockLocations
(
,
0
,
Long
.
MAX_VALUE
)
;
List
<
DataNode
>
dnsOfCluster
=
cluster
.
getDataNodes
(
)
;
DatanodeInfo
[
]
dnsWithLocations
=
lbs
.
getLastLocatedBlock
(
)
.
getLocations
(
)
;
for
(
DataNode
dn
:
dnsOfCluster
)
{
for
(
DatanodeInfo
loc
:
dnsWithLocations
)
{
if
(
dn
.
getDatanodeId
(
)
.
equals
(
loc
)
)
{
dn
.
shutdown
(
)
;
DFSTestUtil
.
waitForDatanodeDeath
(
dn
)
;
}
}
}
DFSTestUtil
.
waitReplication
(
fileSystem
,
f
,
(
short
)
0
)
;
try
{
fileSystem
.
append
(
f
)
;
fail
(
)
;
}
catch
(
IOException
e
)
{
private
void
testStripedFileChecksum
(
int
range1
,
int
range2
)
throws
Exception
{
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
range1
,
false
)
;
FileChecksum
stripedFileChecksum2
=
getFileChecksum
(
stripedFile2
,
range1
,
false
)
;
FileChecksum
stripedFileChecksum3
=
getFileChecksum
(
stripedFile2
,
range2
,
false
)
;
private
void
testStripedFileChecksum
(
int
range1
,
int
range2
)
throws
Exception
{
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
range1
,
false
)
;
FileChecksum
stripedFileChecksum2
=
getFileChecksum
(
stripedFile2
,
range1
,
false
)
;
FileChecksum
stripedFileChecksum3
=
getFileChecksum
(
stripedFile2
,
range2
,
false
)
;
LOG
.
info
(
+
stripedFileChecksum1
)
;
private
void
testStripedFileChecksum
(
int
range1
,
int
range2
)
throws
Exception
{
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
range1
,
false
)
;
FileChecksum
stripedFileChecksum2
=
getFileChecksum
(
stripedFile2
,
range1
,
false
)
;
FileChecksum
stripedFileChecksum3
=
getFileChecksum
(
stripedFile2
,
range2
,
false
)
;
LOG
.
info
(
+
stripedFileChecksum1
)
;
LOG
.
info
(
+
stripedFileChecksum2
)
;
@
Test
(
timeout
=
90000
)
public
void
testStripedFileChecksumWithMissedDataBlocks1
(
)
throws
Exception
{
prepareTestFiles
(
fileSize
,
new
String
[
]
{
stripedFile1
}
)
;
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
fileSize
,
false
)
;
FileChecksum
stripedFileChecksumRecon
=
getFileChecksum
(
stripedFile1
,
fileSize
,
true
)
;
@
Test
(
timeout
=
90000
)
public
void
testStripedFileChecksumWithMissedDataBlocks1
(
)
throws
Exception
{
prepareTestFiles
(
fileSize
,
new
String
[
]
{
stripedFile1
}
)
;
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
fileSize
,
false
)
;
FileChecksum
stripedFileChecksumRecon
=
getFileChecksum
(
stripedFile1
,
fileSize
,
true
)
;
LOG
.
info
(
+
stripedFileChecksum1
)
;
@
Test
(
timeout
=
90000
)
public
void
testStripedFileChecksumWithMissedDataBlocks2
(
)
throws
Exception
{
prepareTestFiles
(
fileSize
,
new
String
[
]
{
stripedFile1
,
stripedFile2
}
)
;
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
-
1
,
false
)
;
FileChecksum
stripedFileChecksum2
=
getFileChecksum
(
stripedFile2
,
-
1
,
false
)
;
FileChecksum
stripedFileChecksum2Recon
=
getFileChecksum
(
stripedFile2
,
-
1
,
true
)
;
@
Test
(
timeout
=
90000
)
public
void
testStripedFileChecksumWithMissedDataBlocks2
(
)
throws
Exception
{
prepareTestFiles
(
fileSize
,
new
String
[
]
{
stripedFile1
,
stripedFile2
}
)
;
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
-
1
,
false
)
;
FileChecksum
stripedFileChecksum2
=
getFileChecksum
(
stripedFile2
,
-
1
,
false
)
;
FileChecksum
stripedFileChecksum2Recon
=
getFileChecksum
(
stripedFile2
,
-
1
,
true
)
;
LOG
.
info
(
+
stripedFileChecksum1
)
;
@
Test
(
timeout
=
90000
)
public
void
testStripedFileChecksumWithMissedDataBlocks2
(
)
throws
Exception
{
prepareTestFiles
(
fileSize
,
new
String
[
]
{
stripedFile1
,
stripedFile2
}
)
;
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile1
,
-
1
,
false
)
;
FileChecksum
stripedFileChecksum2
=
getFileChecksum
(
stripedFile2
,
-
1
,
false
)
;
FileChecksum
stripedFileChecksum2Recon
=
getFileChecksum
(
stripedFile2
,
-
1
,
true
)
;
LOG
.
info
(
+
stripedFileChecksum1
)
;
LOG
.
info
(
+
stripedFileChecksum1
)
;
private
void
testStripedFileChecksumWithMissedDataBlocksRangeQuery
(
String
stripedFile
,
int
requestedLen
)
throws
Exception
{
private
void
testStripedFileChecksumWithMissedDataBlocksRangeQuery
(
String
stripedFile
,
int
requestedLen
)
throws
Exception
{
LOG
.
info
(
,
stripedFile
,
requestedLen
)
;
prepareTestFiles
(
fileSize
,
new
String
[
]
{
stripedFile
}
)
;
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile
,
requestedLen
,
false
)
;
FileChecksum
stripedFileChecksumRecon
=
getFileChecksum
(
stripedFile
,
requestedLen
,
true
)
;
private
void
testStripedFileChecksumWithMissedDataBlocksRangeQuery
(
String
stripedFile
,
int
requestedLen
)
throws
Exception
{
LOG
.
info
(
,
stripedFile
,
requestedLen
)
;
prepareTestFiles
(
fileSize
,
new
String
[
]
{
stripedFile
}
)
;
FileChecksum
stripedFileChecksum1
=
getFileChecksum
(
stripedFile
,
requestedLen
,
false
)
;
FileChecksum
stripedFileChecksumRecon
=
getFileChecksum
(
stripedFile
,
requestedLen
,
true
)
;
LOG
.
info
(
+
stripedFileChecksum1
)
;
@
Override
public
void
run
(
)
{
try
{
FSDataOutputStream
outputStream
=
fileSystem
.
create
(
file
)
;
if
(
syncType
==
SyncType
.
APPEND
)
{
outputStream
.
close
(
)
;
outputStream
=
fileSystem
.
append
(
file
)
;
}
try
{
for
(
int
i
=
0
;
!
error
.
get
(
)
&&
i
<
numWrites
;
i
++
)
{
final
byte
[
]
writeBuf
=
DFSTestUtil
.
generateSequentialBytes
(
i
*
writeSize
,
writeSize
)
;
outputStream
.
write
(
writeBuf
)
;
if
(
syncType
==
SyncType
.
SYNC
)
{
outputStream
.
hflush
(
)
;
}
writerStarted
.
set
(
true
)
;
}
}
catch
(
IOException
e
)
{
error
.
set
(
true
)
;
outputStream
=
fileSystem
.
append
(
file
)
;
}
try
{
for
(
int
i
=
0
;
!
error
.
get
(
)
&&
i
<
numWrites
;
i
++
)
{
final
byte
[
]
writeBuf
=
DFSTestUtil
.
generateSequentialBytes
(
i
*
writeSize
,
writeSize
)
;
outputStream
.
write
(
writeBuf
)
;
if
(
syncType
==
SyncType
.
SYNC
)
{
outputStream
.
hflush
(
)
;
}
writerStarted
.
set
(
true
)
;
}
}
catch
(
IOException
e
)
{
error
.
set
(
true
)
;
LOG
.
error
(
,
e
)
;
}
finally
{
outputStream
.
close
(
)
;
}
writerDone
.
set
(
true
)
;
}
catch
(
Exception
e
)
{
catch
(
IOException
e
)
{
error
.
set
(
true
)
;
LOG
.
error
(
,
e
)
;
}
finally
{
outputStream
.
close
(
)
;
}
writerDone
.
set
(
true
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
Thread
tailer
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
long
startPos
=
0
;
while
(
!
writerDone
.
get
(
)
&&
!
error
.
get
(
)
)
{
}
writerDone
.
set
(
true
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
Thread
tailer
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
long
startPos
=
0
;
while
(
!
writerDone
.
get
(
)
&&
!
error
.
get
(
)
)
{
if
(
writerStarted
.
get
(
)
)
{
try
{
startPos
=
tailFile
(
file
,
startPos
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
String
.
format
(
,
file
)
,
e
)
;
private
boolean
validateSequentialBytes
(
byte
[
]
buf
,
int
startPos
,
int
len
)
{
for
(
int
i
=
0
;
i
<
len
;
i
++
)
{
int
expected
=
(
i
+
startPos
)
%
127
;
if
(
buf
[
i
]
%
127
!=
expected
)
{
private
long
tailFile
(
Path
file
,
long
startPos
)
throws
IOException
{
long
numRead
=
0
;
FSDataInputStream
inputStream
=
fileSystem
.
open
(
file
)
;
inputStream
.
seek
(
startPos
)
;
int
len
=
4
*
1024
;
byte
[
]
buf
=
new
byte
[
len
]
;
int
read
;
while
(
(
read
=
inputStream
.
read
(
buf
)
)
>
-
1
)
{
private
long
tailFile
(
Path
file
,
long
startPos
)
throws
IOException
{
long
numRead
=
0
;
FSDataInputStream
inputStream
=
fileSystem
.
open
(
file
)
;
inputStream
.
seek
(
startPos
)
;
int
len
=
4
*
1024
;
byte
[
]
buf
=
new
byte
[
len
]
;
int
read
;
while
(
(
read
=
inputStream
.
read
(
buf
)
)
>
-
1
)
{
LOG
.
info
(
String
.
format
(
,
read
)
)
;
if
(
!
validateSequentialBytes
(
buf
,
(
int
)
(
startPos
+
numRead
)
,
read
)
)
{
@
Test
public
void
testFileCorruption
(
)
throws
Exception
{
MiniDFSCluster
cluster
=
null
;
DFSTestUtil
util
=
new
DFSTestUtil
.
Builder
(
)
.
setName
(
)
.
setNumFiles
(
20
)
.
build
(
)
;
try
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
3
)
.
build
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
util
.
createFiles
(
fs
,
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
2
)
;
Map
<
DatanodeStorage
,
BlockListAsLongs
>
blockReports
=
dn
.
getFSDataset
(
)
.
getBlockReports
(
bpid
)
;
assertTrue
(
,
!
blockReports
.
isEmpty
(
)
)
;
for
(
BlockListAsLongs
report
:
blockReports
.
values
(
)
)
{
for
(
BlockReportReplica
brr
:
report
)
{
@
Test
(
timeout
=
300000
)
public
void
testSecondaryNodePorts
(
)
throws
Exception
{
NameNode
nn
=
null
;
try
{
nn
=
startNameNode
(
)
;
Configuration
conf2
=
new
HdfsConfiguration
(
config
)
;
conf2
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
config
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_HTTP_ADDRESS_KEY
)
)
;
@
Test
(
timeout
=
300000
)
public
void
testSecondaryNodePorts
(
)
throws
Exception
{
NameNode
nn
=
null
;
try
{
nn
=
startNameNode
(
)
;
Configuration
conf2
=
new
HdfsConfiguration
(
config
)
;
conf2
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
config
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_HTTP_ADDRESS_KEY
)
)
;
LOG
.
info
(
+
conf2
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
)
)
;
boolean
started
=
canStartSecondaryNode
(
conf2
)
;
assertFalse
(
started
)
;
conf2
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY
,
THIS_HOST
)
;
@
Test
(
timeout
=
300000
)
public
void
testBackupNodePorts
(
)
throws
Exception
{
NameNode
nn
=
null
;
try
{
nn
=
startNameNode
(
)
;
Configuration
backup_config
=
new
HdfsConfiguration
(
config
)
;
backup_config
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_BACKUP_ADDRESS_KEY
,
THIS_HOST
)
;
backup_config
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY
,
backup_config
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_HTTP_ADDRESS_KEY
)
)
;
@
Test
(
timeout
=
300000
)
public
void
testBackupNodePorts
(
)
throws
Exception
{
NameNode
nn
=
null
;
try
{
nn
=
startNameNode
(
)
;
Configuration
backup_config
=
new
HdfsConfiguration
(
config
)
;
backup_config
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_BACKUP_ADDRESS_KEY
,
THIS_HOST
)
;
backup_config
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY
,
backup_config
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_HTTP_ADDRESS_KEY
)
)
;
LOG
.
info
(
+
backup_config
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY
)
)
;
assertFalse
(
,
canStartBackupNode
(
backup_config
)
)
;
backup_config
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_BACKUP_ADDRESS_KEY
,
THIS_HOST
)
;
backup_config
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY
,
THIS_HOST
)
;
private
void
waitForBlockReplication
(
String
filename
,
ClientProtocol
namenode
,
int
expected
,
long
maxWaitSec
)
throws
IOException
{
long
start
=
Time
.
monotonicNow
(
)
;
private
void
waitForBlockReplication
(
String
filename
,
ClientProtocol
namenode
,
int
expected
,
long
maxWaitSec
)
throws
IOException
{
long
start
=
Time
.
monotonicNow
(
)
;
LOG
.
info
(
+
filename
)
;
LocatedBlocks
blocks
=
namenode
.
getBlockLocations
(
filename
,
0
,
Long
.
MAX_VALUE
)
;
assertEquals
(
numBlocks
,
blocks
.
locatedBlockCount
(
)
)
;
for
(
int
i
=
0
;
i
<
numBlocks
;
++
i
)
{
LOG
.
info
(
+
(
i
+
1
)
)
;
while
(
true
)
{
blocks
=
namenode
.
getBlockLocations
(
filename
,
0
,
Long
.
MAX_VALUE
)
;
assertEquals
(
numBlocks
,
blocks
.
locatedBlockCount
(
)
)
;
LocatedBlock
block
=
blocks
.
get
(
i
)
;
int
actual
=
block
.
getLocations
(
)
.
length
;
if
(
actual
==
expected
)
{
waitForBlockReplication
(
testFile
,
dfsClient
.
getNamenode
(
)
,
numDataNodes
,
20
)
;
List
<
Map
<
DatanodeStorage
,
BlockListAsLongs
>>
blocksList
=
cluster
.
getAllBlockReports
(
bpid
)
;
cluster
.
shutdown
(
)
;
cluster
=
null
;
LOG
.
info
(
)
;
conf
=
new
HdfsConfiguration
(
)
;
SimulatedFSDataset
.
setFactory
(
conf
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY
,
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
numDataNodes
*
2
)
.
format
(
false
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
Set
<
Block
>
uniqueBlocks
=
new
HashSet
<
Block
>
(
)
;
for
(
Map
<
DatanodeStorage
,
BlockListAsLongs
>
map
:
blocksList
)
{
for
(
BlockListAsLongs
blockList
:
map
.
values
(
)
)
{
for
(
Block
b
:
blockList
)
{
uniqueBlocks
.
add
(
new
Block
(
b
)
)
;
static
FSDataOutputStream
createFile
(
FileSystem
fileSys
,
Path
name
,
int
repl
,
final
long
blockSize
)
throws
IOException
{
FSDataOutputStream
stm
=
fileSys
.
create
(
name
,
true
,
fileSys
.
getConf
(
)
.
getInt
(
CommonConfigurationKeys
.
IO_FILE_BUFFER_SIZE_KEY
,
4096
)
,
(
short
)
repl
,
blockSize
)
;
long
bytesToRead
=
fileSize
;
byte
[
]
compb
=
new
byte
[
readSize
]
;
if
(
verifyData
)
{
for
(
int
j
=
0
;
j
<
readSize
;
j
++
)
{
compb
[
j
]
=
pattern
[
j
%
pattern
.
length
]
;
}
}
FSDataInputStream
stm
=
fs
.
open
(
name
)
;
while
(
bytesToRead
>
0
)
{
int
thisread
=
(
int
)
Math
.
min
(
readSize
,
bytesToRead
)
;
stm
.
readFully
(
b
,
0
,
thisread
)
;
if
(
verifyData
)
{
if
(
thisread
==
readSize
)
{
assertTrue
(
+
(
fileSize
-
bytesToRead
)
,
Arrays
.
equals
(
b
,
compb
)
)
;
}
else
{
for
(
int
k
=
0
;
k
<
thisread
;
k
++
)
{
assertTrue
(
+
(
fileSize
-
bytesToRead
)
,
b
[
k
]
==
compb
[
k
]
)
;
byte
[
]
compb
=
new
byte
[
readSize
]
;
if
(
verifyData
)
{
for
(
int
j
=
0
;
j
<
readSize
;
j
++
)
{
compb
[
j
]
=
pattern
[
j
%
pattern
.
length
]
;
}
}
FSDataInputStream
stm
=
fs
.
open
(
name
)
;
while
(
bytesToRead
>
0
)
{
int
thisread
=
(
int
)
Math
.
min
(
readSize
,
bytesToRead
)
;
stm
.
readFully
(
b
,
0
,
thisread
)
;
if
(
verifyData
)
{
if
(
thisread
==
readSize
)
{
assertTrue
(
+
(
fileSize
-
bytesToRead
)
,
Arrays
.
equals
(
b
,
compb
)
)
;
}
else
{
for
(
int
k
=
0
;
k
<
thisread
;
k
++
)
{
assertTrue
(
+
(
fileSize
-
bytesToRead
)
,
b
[
k
]
==
compb
[
k
]
)
;
}
}
}
LOG
.
debug
(
+
bytesToRead
+
+
thisread
)
;
public
void
runTest
(
final
long
blockSize
)
throws
IOException
{
final
long
fileSize
=
blockSize
+
1L
;
Configuration
conf
=
new
Configuration
(
)
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
numDatanodes
)
.
build
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
try
{
Path
file1
=
new
Path
(
,
blockSize
+
)
;
FSDataOutputStream
stm
=
createFile
(
fs
,
file1
,
1
,
blockSize
)
;
public
void
runTest
(
final
long
blockSize
)
throws
IOException
{
final
long
fileSize
=
blockSize
+
1L
;
Configuration
conf
=
new
Configuration
(
)
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
numDatanodes
)
.
build
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
try
{
Path
file1
=
new
Path
(
,
blockSize
+
)
;
FSDataOutputStream
stm
=
createFile
(
fs
,
file1
,
1
,
blockSize
)
;
LOG
.
info
(
+
file1
+
+
fileSize
+
+
blockSize
)
;
assertTrue
(
file1
+
,
fs
.
getFileStatus
(
file1
)
.
isFile
(
)
)
;
writeFile
(
stm
,
fileSize
)
;
public
void
runTest
(
final
long
blockSize
)
throws
IOException
{
final
long
fileSize
=
blockSize
+
1L
;
Configuration
conf
=
new
Configuration
(
)
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
numDatanodes
)
.
build
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
try
{
Path
file1
=
new
Path
(
,
blockSize
+
)
;
FSDataOutputStream
stm
=
createFile
(
fs
,
file1
,
1
,
blockSize
)
;
LOG
.
info
(
+
file1
+
+
fileSize
+
+
blockSize
)
;
assertTrue
(
file1
+
,
fs
.
getFileStatus
(
file1
)
.
isFile
(
)
)
;
writeFile
(
stm
,
fileSize
)
;
LOG
.
info
(
+
file1
+
)
;
stm
.
close
(
)
;
dfs
.
renewLease
(
)
;
}
catch
(
IOException
e
)
{
}
try
{
d_out
.
write
(
buf
,
0
,
1024
)
;
LOG
.
info
(
)
;
}
catch
(
IOException
e
)
{
Assert
.
fail
(
)
;
}
long
hardlimit
=
conf
.
getLong
(
DFSConfigKeys
.
DFS_LEASE_HARDLIMIT_KEY
,
DFSConfigKeys
.
DFS_LEASE_HARDLIMIT_DEFAULT
)
*
1000
;
dfs
.
lastLeaseRenewal
=
Time
.
monotonicNow
(
)
-
hardlimit
-
1000
;
dfs
.
renewLease
(
)
;
try
{
d_out
.
write
(
buf
,
0
,
1024
)
;
d_out
.
close
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IOException
e
)
{
}
long
hardlimit
=
conf
.
getLong
(
DFSConfigKeys
.
DFS_LEASE_HARDLIMIT_KEY
,
DFSConfigKeys
.
DFS_LEASE_HARDLIMIT_DEFAULT
)
*
1000
;
dfs
.
lastLeaseRenewal
=
Time
.
monotonicNow
(
)
-
hardlimit
-
1000
;
dfs
.
renewLease
(
)
;
try
{
d_out
.
write
(
buf
,
0
,
1024
)
;
d_out
.
close
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
info
(
,
e
)
;
}
Thread
.
sleep
(
1000
)
;
Assert
.
assertTrue
(
originalRenewer
.
isEmpty
(
)
)
;
doNothing
(
)
.
when
(
spyNN
)
.
renewLease
(
anyString
(
)
)
;
try
{
int
num
=
c_in
.
read
(
buf
,
0
,
1
)
;
if
(
num
!=
1
)
{
Assert
.
fail
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
info
(
,
e
)
;
}
Thread
.
sleep
(
1000
)
;
Assert
.
assertTrue
(
originalRenewer
.
isEmpty
(
)
)
;
doNothing
(
)
.
when
(
spyNN
)
.
renewLease
(
anyString
(
)
)
;
try
{
int
num
=
c_in
.
read
(
buf
,
0
,
1
)
;
if
(
num
!=
1
)
{
Assert
.
fail
(
)
;
}
c_in
.
close
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
)
;
}
try
{
for
(
DataNode
dn
:
cluster
.
getDataNodes
(
)
)
{
DataNodeTestUtils
.
setHeartbeatsDisabledForTests
(
dn
,
false
)
;
}
cluster
.
waitActive
(
)
;
cluster
.
setLeasePeriod
(
LONG_LEASE_PERIOD
,
SHORT_LEASE_PERIOD
)
;
LocatedBlocks
locatedBlocks
;
do
{
Thread
.
sleep
(
SHORT_LEASE_PERIOD
)
;
locatedBlocks
=
dfs
.
dfs
.
getLocatedBlocks
(
fileStr
,
0L
,
size
)
;
}
while
(
locatedBlocks
.
isUnderConstruction
(
)
)
;
assertEquals
(
size
,
locatedBlocks
.
getFileLength
(
)
)
;
try
{
stm
.
write
(
'b'
)
;
stm
.
hflush
(
)
;
fail
(
)
;
}
catch
(
IOException
e
)
{
private
void
writePartialBlocks
(
int
[
]
blockLengths
)
throws
Exception
{
final
FSDataOutputStream
out
=
dfs
.
create
(
p
)
;
final
DFSStripedOutputStream
stripedOut
=
(
DFSStripedOutputStream
)
out
.
getWrappedStream
(
)
;
int
[
]
posToKill
=
getPosToKill
(
blockLengths
)
;
int
checkingPos
=
nextCheckingPos
(
posToKill
,
0
)
;
Set
<
Integer
>
stoppedStreamerIndexes
=
new
HashSet
<
>
(
)
;
try
{
for
(
int
pos
=
0
;
pos
<
testFileLength
;
pos
++
)
{
out
.
write
(
StripedFileTestUtil
.
getByte
(
pos
)
)
;
if
(
pos
==
checkingPos
)
{
for
(
int
index
:
getIndexToStop
(
posToKill
,
pos
)
)
{
out
.
flush
(
)
;
stripedOut
.
enqueueAllCurrentPackets
(
)
;
stripedOut
.
enqueueAllCurrentPackets
(
)
;
LOG
.
info
(
+
,
index
,
pos
,
blockLengths
[
index
]
)
;
StripedDataStreamer
s
=
stripedOut
.
getStripedDataStreamer
(
index
)
;
waitStreamerAllAcked
(
s
)
;
waitByteSent
(
s
,
blockLengths
[
index
]
)
;
stopBlockStream
(
s
)
;
stoppedStreamerIndexes
.
add
(
index
)
;
}
checkingPos
=
nextCheckingPos
(
posToKill
,
pos
)
;
}
}
}
finally
{
out
.
flush
(
)
;
stripedOut
.
enqueueAllCurrentPackets
(
)
;
for
(
int
i
=
0
;
i
<
blockLengths
.
length
;
i
++
)
{
if
(
stoppedStreamerIndexes
.
contains
(
i
)
)
{
continue
;
}
StripedDataStreamer
s
=
stripedOut
.
getStripedDataStreamer
(
i
)
;
private
void
testFileBlockReplicationImpl
(
int
maintenanceMinRepl
,
int
numDataNodes
,
int
numNewDataNodes
,
int
fileBlockRepl
)
throws
Exception
{
setup
(
)
;
private
void
testFileBlockReplicationImpl
(
int
maintenanceMinRepl
,
int
numDataNodes
,
int
numNewDataNodes
,
int
fileBlockRepl
)
throws
Exception
{
setup
(
)
;
LOG
.
info
(
+
maintenanceMinRepl
+
+
numDataNodes
+
+
numNewDataNodes
+
+
fileBlockRepl
)
;
private
void
testChangeReplicationFactor
(
int
oldFactor
,
int
newFactor
,
int
expectedLiveReplicas
)
throws
IOException
{
setup
(
)
;
static
String
checkFile
(
FSNamesystem
ns
,
FileSystem
fileSys
,
Path
name
,
int
repl
,
DatanodeInfo
expectedExcludedNode
,
DatanodeInfo
expectedMaintenanceNode
)
throws
IOException
{
assertTrue
(
+
fileSys
.
getUri
(
)
,
fileSys
instanceof
DistributedFileSystem
)
;
HdfsDataInputStream
dis
=
(
HdfsDataInputStream
)
fileSys
.
open
(
name
)
;
BlockManager
bm
=
ns
.
getBlockManager
(
)
;
Collection
<
LocatedBlock
>
dinfo
=
dis
.
getAllBlocks
(
)
;
String
output
;
for
(
LocatedBlock
blk
:
dinfo
)
{
DatanodeInfo
[
]
nodes
=
blk
.
getLocations
(
)
;
for
(
int
j
=
0
;
j
<
nodes
.
length
;
j
++
)
{
if
(
expectedExcludedNode
!=
null
&&
nodes
[
j
]
.
equals
(
expectedExcludedNode
)
)
{
output
=
+
blk
.
getBlock
(
)
+
+
nodes
[
j
]
+
;
assertTrue
(
+
fileSys
.
getUri
(
)
,
fileSys
instanceof
DistributedFileSystem
)
;
HdfsDataInputStream
dis
=
(
HdfsDataInputStream
)
fileSys
.
open
(
name
)
;
BlockManager
bm
=
ns
.
getBlockManager
(
)
;
Collection
<
LocatedBlock
>
dinfo
=
dis
.
getAllBlocks
(
)
;
String
output
;
for
(
LocatedBlock
blk
:
dinfo
)
{
DatanodeInfo
[
]
nodes
=
blk
.
getLocations
(
)
;
for
(
int
j
=
0
;
j
<
nodes
.
length
;
j
++
)
{
if
(
expectedExcludedNode
!=
null
&&
nodes
[
j
]
.
equals
(
expectedExcludedNode
)
)
{
output
=
+
blk
.
getBlock
(
)
+
+
nodes
[
j
]
+
;
LOG
.
info
(
output
)
;
return
output
;
}
else
{
if
(
nodes
[
j
]
.
isInMaintenance
(
)
)
{
output
=
+
blk
.
getBlock
(
)
+
+
nodes
[
j
]
+
;
if
(
repl
!=
nodes
.
length
)
{
output
=
+
blk
.
getBlock
(
)
+
+
repl
+
+
nodes
.
length
+
;
for
(
int
j
=
0
;
j
<
nodes
.
length
;
j
++
)
{
output
+=
nodes
[
j
]
+
;
}
output
+=
+
ns
.
getPendingReplicationBlocks
(
)
+
;
output
+=
+
ns
.
getUnderReplicatedBlocks
(
)
+
;
if
(
expectedExcludedNode
!=
null
)
{
output
+=
+
expectedExcludedNode
;
}
LOG
.
info
(
output
)
;
return
output
;
}
Iterator
<
DatanodeStorageInfo
>
storageInfoIter
=
bm
.
getStorages
(
blk
.
getBlock
(
)
.
getLocalBlock
(
)
)
.
iterator
(
)
;
List
<
DatanodeInfo
>
maintenanceNodes
=
new
ArrayList
<
>
(
)
;
while
(
storageInfoIter
.
hasNext
(
)
)
{
DatanodeInfo
node
=
storageInfoIter
.
next
(
)
.
getDatanodeDescriptor
(
)
;
if
(
node
.
isMaintenance
(
)
)
{
output
+=
+
ns
.
getPendingReplicationBlocks
(
)
+
;
output
+=
+
ns
.
getUnderReplicatedBlocks
(
)
+
;
if
(
expectedExcludedNode
!=
null
)
{
output
+=
+
expectedExcludedNode
;
}
LOG
.
info
(
output
)
;
return
output
;
}
Iterator
<
DatanodeStorageInfo
>
storageInfoIter
=
bm
.
getStorages
(
blk
.
getBlock
(
)
.
getLocalBlock
(
)
)
.
iterator
(
)
;
List
<
DatanodeInfo
>
maintenanceNodes
=
new
ArrayList
<
>
(
)
;
while
(
storageInfoIter
.
hasNext
(
)
)
{
DatanodeInfo
node
=
storageInfoIter
.
next
(
)
.
getDatanodeDescriptor
(
)
;
if
(
node
.
isMaintenance
(
)
)
{
maintenanceNodes
.
add
(
node
)
;
}
}
if
(
expectedMaintenanceNode
!=
null
)
{
if
(
!
maintenanceNodes
.
contains
(
expectedMaintenanceNode
)
)
{
output
=
+
expectedMaintenanceNode
;
public
static
void
setupCluster
(
int
replicationFactor
,
HdfsConfiguration
conf
)
throws
Exception
{
util
=
new
BlockReaderTestUtil
(
replicationFactor
,
conf
)
;
dfsClient
=
util
.
getDFSClient
(
)
;
long
seed
=
Time
.
now
(
)
;
@
Test
public
void
pipeline_01
(
)
throws
IOException
{
final
String
METHOD_NAME
=
GenericTestUtils
.
getMethodName
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
static
byte
[
]
writeData
(
final
FSDataOutputStream
out
,
final
int
length
)
throws
IOException
{
int
bytesToWrite
=
length
;
byte
[
]
ret
=
new
byte
[
bytesToWrite
]
;
byte
[
]
toWrite
=
new
byte
[
1024
]
;
int
written
=
0
;
Random
rb
=
new
Random
(
rand
.
nextLong
(
)
)
;
while
(
bytesToWrite
>
0
)
{
rb
.
nextBytes
(
toWrite
)
;
int
bytesToWriteNext
=
(
1024
<
bytesToWrite
)
?
1024
:
bytesToWrite
;
out
.
write
(
toWrite
,
0
,
bytesToWriteNext
)
;
System
.
arraycopy
(
toWrite
,
0
,
ret
,
(
ret
.
length
-
bytesToWrite
)
,
bytesToWriteNext
)
;
written
+=
bytesToWriteNext
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
Path
testFile
=
new
Path
(
dir
,
)
;
FSDataOutputStream
stream
=
dfs
.
create
(
testFile
)
;
final
LeaseRenewer
leaseRenewer
=
dfs
.
getClient
(
)
.
getLeaseRenewer
(
)
;
stream
.
write
(
.
getBytes
(
)
)
;
try
{
stream
.
hflush
(
)
;
fail
(
)
;
}
catch
(
DSQuotaExceededException
expected
)
{
}
try
{
stream
.
close
(
)
;
fail
(
)
;
}
catch
(
DSQuotaExceededException
expected
)
{
}
GenericTestUtils
.
setLogLevel
(
LeaseRenewer
.
LOG
,
Level
.
TRACE
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
@
Test
public
void
testReportBadBlock
(
)
throws
IOException
{
final
Path
file
=
new
Path
(
)
;
final
int
length
=
10
;
final
byte
[
]
bytes
=
StripedFileTestUtil
.
generateBytes
(
length
)
;
DFSTestUtil
.
writeFile
(
dfs
,
file
,
bytes
)
;
int
dnIndex
=
ReadStripedFileWithDecodingHelper
.
findFirstDataNode
(
cluster
,
dfs
,
file
,
CELL_SIZE
*
NUM_DATA_UNITS
)
;
Assert
.
assertNotEquals
(
-
1
,
dnIndex
)
;
LocatedStripedBlock
slb
=
(
LocatedStripedBlock
)
dfs
.
getClient
(
)
.
getLocatedBlocks
(
file
.
toString
(
)
,
0
,
CELL_SIZE
*
NUM_DATA_UNITS
)
.
get
(
0
)
;
final
LocatedBlock
[
]
blks
=
StripedBlockUtil
.
parseStripedBlockGroup
(
slb
,
CELL_SIZE
,
NUM_DATA_UNITS
,
NUM_PARITY_UNITS
)
;
File
storageDir
=
cluster
.
getInstanceStorageDir
(
dnIndex
,
0
)
;
File
blkFile
=
MiniDFSCluster
.
getBlockFile
(
storageDir
,
blks
[
0
]
.
getBlock
(
)
)
;
Assert
.
assertTrue
(
,
blkFile
.
exists
(
)
)
;
private
void
readFileWithMissingBlocks
(
Path
srcPath
,
int
fileLength
,
int
missingDataNum
,
int
missingParityNum
,
byte
[
]
expected
)
throws
Exception
{
private
void
stopDataNodes
(
BlockLocation
[
]
locs
,
int
[
]
datanodes
)
throws
IOException
{
if
(
locs
!=
null
&&
locs
.
length
>
0
)
{
for
(
int
failedDNIdx
:
datanodes
)
{
String
name
=
(
locs
[
0
]
.
getNames
(
)
)
[
failedDNIdx
]
;
for
(
DataNode
dn
:
cluster
.
getDataNodes
(
)
)
{
int
port
=
dn
.
getXferPort
(
)
;
if
(
name
.
contains
(
Integer
.
toString
(
port
)
)
)
{
dn
.
shutdown
(
)
;
cluster
.
setDataNodeDead
(
dn
.
getDatanodeId
(
)
)
;
private
void
assertFileBlocksReconstruction
(
String
fileName
,
int
fileLen
,
ReconstructionType
type
,
int
toRecoverBlockNum
)
throws
Exception
{
if
(
toRecoverBlockNum
<
1
||
toRecoverBlockNum
>
parityBlkNum
)
{
Assert
.
fail
(
+
parityBlkNum
)
;
}
assertTrue
(
,
fileLen
>
0
)
;
Path
file
=
new
Path
(
fileName
)
;
writeFile
(
fs
,
fileName
,
fileLen
)
;
LocatedBlocks
locatedBlocks
=
StripedFileTestUtil
.
getLocatedBlocks
(
file
,
fs
)
;
assertEquals
(
locatedBlocks
.
getFileLength
(
)
,
fileLen
)
;
LocatedStripedBlock
lastBlock
=
(
LocatedStripedBlock
)
locatedBlocks
.
getLastLocatedBlock
(
)
;
DatanodeInfo
[
]
storageInfos
=
lastBlock
.
getLocations
(
)
;
byte
[
]
indices
=
lastBlock
.
getBlockIndices
(
)
;
BitSet
bitset
=
new
BitSet
(
dnNum
)
;
for
(
DatanodeInfo
storageInfo
:
storageInfos
)
{
bitset
.
set
(
dnMap
.
get
(
storageInfo
)
)
;
}
int
[
]
dead
=
generateDeadDnIndices
(
type
,
toRecoverBlockNum
,
indices
)
;
int
[
]
deadDnIndices
=
new
int
[
toRecoverBlockNum
]
;
ExtendedBlock
[
]
blocks
=
new
ExtendedBlock
[
toRecoverBlockNum
]
;
File
[
]
replicas
=
new
File
[
toRecoverBlockNum
]
;
long
[
]
replicaLengths
=
new
long
[
toRecoverBlockNum
]
;
File
[
]
metadatas
=
new
File
[
toRecoverBlockNum
]
;
byte
[
]
[
]
replicaContents
=
new
byte
[
toRecoverBlockNum
]
[
]
;
Map
<
ExtendedBlock
,
DataNode
>
errorMap
=
new
HashMap
<
>
(
dead
.
length
)
;
for
(
int
i
=
0
;
i
<
toRecoverBlockNum
;
i
++
)
{
dataDNs
[
i
]
=
storageInfos
[
dead
[
i
]
]
;
deadDnIndices
[
i
]
=
dnMap
.
get
(
dataDNs
[
i
]
)
;
blocks
[
i
]
=
StripedBlockUtil
.
constructInternalBlock
(
lastBlock
.
getBlock
(
)
,
cellSize
,
dataBlkNum
,
indices
[
dead
[
i
]
]
)
;
errorMap
.
put
(
blocks
[
i
]
,
cluster
.
getDataNodes
(
)
.
get
(
deadDnIndices
[
i
]
)
)
;
replicas
[
i
]
=
cluster
.
getBlockFile
(
deadDnIndices
[
i
]
,
blocks
[
i
]
)
;
replicaLengths
[
i
]
=
replicas
[
i
]
.
length
(
)
;
metadatas
[
i
]
=
cluster
.
getBlockMetadataFile
(
deadDnIndices
[
i
]
,
blocks
[
i
]
)
;
static
void
sleepSeconds
(
final
int
waittime
)
throws
InterruptedException
{
static
void
sleepSeconds
(
final
int
waittime
)
throws
InterruptedException
{
for
(
int
k
=
0
;
k
<
racks
.
length
;
k
++
)
{
if
(
topologyPaths
[
j
]
.
startsWith
(
racks
[
k
]
)
)
{
found
=
true
;
break
;
}
}
assertTrue
(
found
)
;
}
}
boolean
isOnSameRack
=
true
,
isNotOnSameRack
=
true
;
for
(
LocatedBlock
blk
:
locations
.
getLocatedBlocks
(
)
)
{
DatanodeInfo
[
]
datanodes
=
blk
.
getLocations
(
)
;
if
(
datanodes
.
length
<=
1
)
break
;
if
(
datanodes
.
length
==
2
)
{
isNotOnSameRack
=
!
(
datanodes
[
0
]
.
getNetworkLocation
(
)
.
equals
(
datanodes
[
1
]
.
getNetworkLocation
(
)
)
)
;
break
;
}
isOnSameRack
=
false
;
isNotOnSameRack
=
false
;
for
(
int
i
=
0
;
i
<
datanodes
.
length
-
1
;
i
++
)
{
private
void
waitForBlockReplication
(
String
filename
,
ClientProtocol
namenode
,
int
expected
,
long
maxWaitSec
,
boolean
isUnderConstruction
,
boolean
noOverReplication
)
throws
IOException
{
long
start
=
Time
.
monotonicNow
(
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
numDataNodes
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
DFSClient
dfsClient
=
new
DFSClient
(
new
InetSocketAddress
(
,
cluster
.
getNameNodePort
(
)
)
,
conf
)
;
OutputStream
out
=
cluster
.
getFileSystem
(
)
.
create
(
testPath
)
;
out
.
write
(
buffer
)
;
out
.
close
(
)
;
waitForBlockReplication
(
testFile
,
dfsClient
.
getNamenode
(
)
,
numDataNodes
,
-
1
)
;
ExtendedBlock
block
=
dfsClient
.
getNamenode
(
)
.
getBlockLocations
(
testFile
,
0
,
Long
.
MAX_VALUE
)
.
get
(
0
)
.
getBlock
(
)
;
List
<
MaterializedReplica
>
replicas
=
new
ArrayList
<
>
(
)
;
for
(
int
dnIndex
=
0
;
dnIndex
<
3
;
dnIndex
++
)
{
replicas
.
add
(
cluster
.
getMaterializedReplica
(
dnIndex
,
block
)
)
;
}
assertEquals
(
3
,
replicas
.
size
(
)
)
;
cluster
.
shutdown
(
)
;
int
fileCount
=
0
;
for
(
MaterializedReplica
replica
:
replicas
)
{
OutputStream
out
=
cluster
.
getFileSystem
(
)
.
create
(
testPath
)
;
out
.
write
(
buffer
)
;
out
.
close
(
)
;
waitForBlockReplication
(
testFile
,
dfsClient
.
getNamenode
(
)
,
numDataNodes
,
-
1
)
;
ExtendedBlock
block
=
dfsClient
.
getNamenode
(
)
.
getBlockLocations
(
testFile
,
0
,
Long
.
MAX_VALUE
)
.
get
(
0
)
.
getBlock
(
)
;
List
<
MaterializedReplica
>
replicas
=
new
ArrayList
<
>
(
)
;
for
(
int
dnIndex
=
0
;
dnIndex
<
3
;
dnIndex
++
)
{
replicas
.
add
(
cluster
.
getMaterializedReplica
(
dnIndex
,
block
)
)
;
}
assertEquals
(
3
,
replicas
.
size
(
)
)
;
cluster
.
shutdown
(
)
;
int
fileCount
=
0
;
for
(
MaterializedReplica
replica
:
replicas
)
{
if
(
fileCount
==
0
)
{
LOG
.
info
(
+
replica
)
;
replica
.
deleteData
(
)
;
@
Test
(
timeout
=
30000
)
public
void
testRollingUpgradeWithQJM
(
)
throws
Exception
{
String
nnDirPrefix
=
MiniDFSCluster
.
getBaseDirectory
(
)
+
;
final
File
nn1Dir
=
new
File
(
nnDirPrefix
+
)
;
final
File
nn2Dir
=
new
File
(
nnDirPrefix
+
)
;
@
Test
(
timeout
=
30000
)
public
void
testRollingUpgradeWithQJM
(
)
throws
Exception
{
String
nnDirPrefix
=
MiniDFSCluster
.
getBaseDirectory
(
)
+
;
final
File
nn1Dir
=
new
File
(
nnDirPrefix
+
)
;
final
File
nn2Dir
=
new
File
(
nnDirPrefix
+
)
;
LOG
.
info
(
+
nn1Dir
)
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
0
)
.
manageNameDfsDirs
(
false
)
.
checkExitOnShutdown
(
false
)
.
build
(
)
;
cluster
.
shutdown
(
)
;
}
MiniDFSCluster
cluster2
=
null
;
try
{
FileUtil
.
fullyDelete
(
nn2Dir
)
;
FileUtil
.
copy
(
nn1Dir
,
FileSystem
.
getLocal
(
conf
)
.
getRaw
(
)
,
new
Path
(
nn2Dir
.
getAbsolutePath
(
)
)
,
false
,
conf
)
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
0
)
.
format
(
false
)
.
manageNameDfsDirs
(
false
)
.
checkExitOnShutdown
(
false
)
.
build
(
)
;
final
Path
foo
=
new
Path
(
)
;
final
Path
bar
=
new
Path
(
)
;
final
Path
baz
=
new
Path
(
)
;
final
RollingUpgradeInfo
info1
;
{
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
dfs
.
mkdirs
(
foo
)
;
dfs
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_ENTER
)
;
info1
=
dfs
.
rollingUpgrade
(
RollingUpgradeAction
.
PREPARE
)
;
assertEquals
(
+
+
NEWLINE
+
+
+
,
status
)
;
assertFalse
(
+
,
NameNodeAdapter
.
safeModeInitializedReplQueues
(
nn
)
)
;
LOG
.
info
(
)
;
cluster
.
restartDataNode
(
dnprops
.
remove
(
0
)
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
return
getLongCounter
(
,
getMetrics
(
NN_METRICS
)
)
==
cluster
.
getStoragesPerDatanode
(
)
;
}
}
,
10
,
10000
)
;
final
long
safe
=
NameNodeAdapter
.
getSafeModeSafeBlocks
(
nn
)
;
assertTrue
(
,
safe
>
0
)
;
assertTrue
(
,
safe
<
15
)
;
assertTrue
(
NameNodeAdapter
.
safeModeInitializedReplQueues
(
nn
)
)
;
BlockManagerTestUtil
.
updateState
(
nn
.
getNamesystem
(
)
.
getBlockManager
(
)
)
;
long
underReplicatedBlocks
=
nn
.
getNamesystem
(
)
.
getUnderReplicatedBlocks
(
)
;
while
(
underReplicatedBlocks
!=
(
15
-
safe
)
)
{
@
Test
public
void
testCreateZoneAfterAuthTokenExpiry
(
)
throws
Exception
{
final
UserGroupInformation
ugi
=
UserGroupInformation
.
loginUserFromKeytabAndReturnUGI
(
hdfsPrincipal
,
keytab
)
;
@
Test
public
void
testCreateZoneAfterAuthTokenExpiry
(
)
throws
Exception
{
final
UserGroupInformation
ugi
=
UserGroupInformation
.
loginUserFromKeytabAndReturnUGI
(
hdfsPrincipal
,
keytab
)
;
LOG
.
info
(
,
ugi
)
;
ugi
.
doAs
(
(
PrivilegedExceptionAction
<
Object
>
)
(
)
->
{
final
Path
zone
=
new
Path
(
)
;
fsWrapper
.
mkdir
(
zone
,
FsPermission
.
getDirDefault
(
)
,
true
)
;
dfsAdmin
.
createEncryptionZone
(
zone
,
testKey
,
NO_TRASH
)
;
final
Path
zone1
=
new
Path
(
)
;
fsWrapper
.
mkdir
(
zone1
,
FsPermission
.
getDirDefault
(
)
,
true
)
;
final
long
sleepInterval
=
(
AUTH_TOKEN_VALIDITY
+
1
)
*
1000
;
@
Test
public
void
testWriteReadSeq
(
)
throws
IOException
{
useFCOption
=
false
;
positionReadOption
=
false
;
String
fname
=
filenameOption
;
long
rdBeginPos
=
0
;
int
stat
=
testWriteAndRead
(
fname
,
WR_NTIMES
,
WR_CHUNK_SIZE
,
rdBeginPos
)
;
currentPosition
=
in
.
getPos
(
)
;
}
if
(
verboseOption
)
LOG
.
info
(
+
pos
+
+
currentPosition
+
+
buffer
.
length
+
+
fname
)
;
try
{
while
(
byteLeftToRead
>
0
&&
currentPosition
<
visibleLen
)
{
byteToReadThisRound
=
(
int
)
(
byteLeftToRead
>=
buffer
.
length
?
buffer
.
length
:
byteLeftToRead
)
;
if
(
positionReadOption
)
{
byteRead
=
in
.
read
(
currentPosition
,
buffer
,
0
,
byteToReadThisRound
)
;
}
else
{
byteRead
=
in
.
read
(
buffer
,
0
,
byteToReadThisRound
)
;
}
if
(
byteRead
<=
0
)
break
;
chunkNumber
++
;
totalByteRead
+=
byteRead
;
currentPosition
+=
byteRead
;
byteLeftToRead
-=
byteRead
;
if
(
verboseOption
)
{
FSDataOutputStream
out
=
null
;
byte
[
]
outBuffer
=
new
byte
[
BUFFER_SIZE
]
;
byte
[
]
inBuffer
=
new
byte
[
BUFFER_SIZE
]
;
for
(
int
i
=
0
;
i
<
BUFFER_SIZE
;
i
++
)
{
outBuffer
[
i
]
=
(
byte
)
(
i
&
0x00ff
)
;
}
try
{
Path
path
=
getFullyQualifiedPath
(
fname
)
;
long
fileLengthBeforeOpen
=
0
;
if
(
ifExists
(
path
)
)
{
if
(
truncateOption
)
{
out
=
useFCOption
?
mfc
.
create
(
path
,
EnumSet
.
of
(
CreateFlag
.
OVERWRITE
)
)
:
mfs
.
create
(
path
,
truncateOption
)
;
LOG
.
info
(
+
path
)
;
}
else
{
out
=
useFCOption
?
mfc
.
create
(
path
,
EnumSet
.
of
(
CreateFlag
.
APPEND
)
)
:
mfs
.
append
(
path
)
;
fileLengthBeforeOpen
=
getFileLengthFromNN
(
path
)
;
}
}
else
{
out
=
useFCOption
?
mfc
.
create
(
path
,
EnumSet
.
of
(
CreateFlag
.
CREATE
)
)
:
mfs
.
create
(
path
)
;
}
long
totalByteWritten
=
fileLengthBeforeOpen
;
long
totalByteVisible
=
fileLengthBeforeOpen
;
long
totalByteWrittenButNotVisible
=
0
;
boolean
toFlush
;
for
(
int
i
=
0
;
i
<
loopN
;
i
++
)
{
toFlush
=
(
i
%
2
)
==
0
;
writeData
(
out
,
outBuffer
,
chunkSize
)
;
totalByteWritten
+=
chunkSize
;
if
(
toFlush
)
{
out
.
hflush
(
)
;
totalByteVisible
+=
chunkSize
+
totalByteWrittenButNotVisible
;
totalByteWrittenButNotVisible
=
0
;
}
else
{
writeData
(
out
,
outBuffer
,
chunkSize
)
;
totalByteWritten
+=
chunkSize
;
if
(
toFlush
)
{
out
.
hflush
(
)
;
totalByteVisible
+=
chunkSize
+
totalByteWrittenButNotVisible
;
totalByteWrittenButNotVisible
=
0
;
}
else
{
totalByteWrittenButNotVisible
+=
chunkSize
;
}
if
(
verboseOption
)
{
LOG
.
info
(
+
chunkSize
+
+
totalByteWritten
+
+
totalByteVisible
+
+
fname
)
;
}
byteVisibleToRead
=
readData
(
fname
,
inBuffer
,
totalByteVisible
,
readBeginPosition
)
;
String
readmsg
=
+
totalByteWritten
+
+
totalByteVisible
+
+
byteVisibleToRead
+
+
fname
;
if
(
byteVisibleToRead
>=
totalByteVisible
&&
byteVisibleToRead
<=
totalByteWritten
)
{
readmsg
=
+
readmsg
+
;
}
else
{
byteVisibleToRead
=
readData
(
fname
,
inBuffer
,
totalByteVisible
,
readBeginPosition
)
;
String
readmsg
=
+
totalByteWritten
+
+
totalByteVisible
+
+
byteVisibleToRead
+
+
fname
;
if
(
byteVisibleToRead
>=
totalByteVisible
&&
byteVisibleToRead
<=
totalByteWritten
)
{
readmsg
=
+
readmsg
+
;
}
else
{
countOfFailures
++
;
readmsg
=
+
readmsg
+
;
if
(
abortTestOnFailure
)
{
throw
new
IOException
(
readmsg
)
;
}
}
LOG
.
info
(
readmsg
)
;
}
writeData
(
out
,
outBuffer
,
chunkSize
)
;
totalByteWritten
+=
chunkSize
;
totalByteVisible
+=
chunkSize
+
totalByteWrittenButNotVisible
;
totalByteWrittenButNotVisible
+=
0
;
out
.
close
(
)
;
private
void
testOneFileUsingDFSStripedInputStream
(
String
src
,
int
fileLength
,
boolean
withDataNodeFailure
)
throws
Exception
{
final
byte
[
]
expected
=
StripedFileTestUtil
.
generateBytes
(
fileLength
)
;
Path
srcPath
=
new
Path
(
src
)
;
DFSTestUtil
.
writeFile
(
fs
,
srcPath
,
new
String
(
expected
)
)
;
StripedFileTestUtil
.
waitBlockGroupsReported
(
fs
,
src
)
;
StripedFileTestUtil
.
verifyLength
(
fs
,
srcPath
,
fileLength
)
;
if
(
withDataNodeFailure
)
{
int
dnIndex
=
1
;
private
void
writeFileWithDNFailure
(
int
fileLength
,
int
dataDNFailureNum
,
int
parityDNFailureNum
)
throws
IOException
{
String
fileType
=
fileLength
<
(
blockSize
*
dataBlocks
)
?
:
;
String
src
=
+
dataDNFailureNum
+
+
parityDNFailureNum
+
+
fileType
;
Runnable
readerRunnable
=
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
List
<
LocatedBlock
>
locatedBlocks
=
cluster
.
getNameNode
(
)
.
getRpcServer
(
)
.
getBlockLocations
(
TEST_FILE
,
0
,
TEST_FILE_LEN
)
.
getLocatedBlocks
(
)
;
LocatedBlock
lblock
=
locatedBlocks
.
get
(
0
)
;
BlockReader
blockReader
=
null
;
try
{
blockReader
=
BlockReaderTestUtil
.
getBlockReader
(
cluster
.
getFileSystem
(
)
,
lblock
,
0
,
TEST_FILE_LEN
)
;
Assert
.
fail
(
)
;
}
catch
(
Throwable
t
)
{
Assert
.
assertTrue
(
+
+
t
,
t
.
getMessage
(
)
.
contains
(
)
)
;
}
finally
{
if
(
blockReader
!=
null
)
blockReader
.
close
(
)
;
}
gotFailureLatch
.
countDown
(
)
;
shouldRetryLatch
.
await
(
)
;
BlockReader
blockReader
=
null
;
try
{
blockReader
=
BlockReaderTestUtil
.
getBlockReader
(
cluster
.
getFileSystem
(
)
,
lblock
,
0
,
TEST_FILE_LEN
)
;
Assert
.
fail
(
)
;
}
catch
(
Throwable
t
)
{
Assert
.
assertTrue
(
+
+
t
,
t
.
getMessage
(
)
.
contains
(
)
)
;
}
finally
{
if
(
blockReader
!=
null
)
blockReader
.
close
(
)
;
}
gotFailureLatch
.
countDown
(
)
;
shouldRetryLatch
.
await
(
)
;
try
{
blockReader
=
BlockReaderTestUtil
.
getBlockReader
(
cluster
.
getFileSystem
(
)
,
lblock
,
0
,
TEST_FILE_LEN
)
;
}
catch
(
Throwable
t
)
{
LOG
.
error
(
+
,
t
)
;
throw
t
;
try
{
while
(
true
)
{
BlockReader
blockReader
=
null
;
try
{
blockReader
=
BlockReaderTestUtil
.
getBlockReader
(
cluster
.
getFileSystem
(
)
,
lblock
,
0
,
TEST_FILE_LEN
)
;
sem
.
release
(
)
;
try
{
blockReader
.
readAll
(
buf
,
0
,
TEST_FILE_LEN
)
;
}
finally
{
sem
.
acquireUninterruptibly
(
)
;
}
}
catch
(
ClosedByInterruptException
e
)
{
LOG
.
info
(
,
e
)
;
sem
.
release
(
)
;
break
;
}
finally
{
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
totalTrials
=
0
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
StorageType
type
=
StorageType
.
values
(
)
[
i
%
StorageType
.
values
(
)
.
length
]
;
localStart
=
System
.
nanoTime
(
)
;
do
{
totalTrials
+=
1
;
node
=
cluster
.
chooseRandom
(
,
excluded
)
;
assertNotNull
(
node
)
;
if
(
isType
(
node
,
type
)
)
{
break
;
}
excluded
.
add
(
node
)
;
}
while
(
true
)
;
excluded
.
clear
(
)
;
}
while
(
true
)
;
excluded
.
clear
(
)
;
localEnd
=
System
.
nanoTime
(
)
;
records
[
i
]
=
localEnd
-
localStart
;
}
totalEnd
=
System
.
nanoTime
(
)
;
totalMs
=
(
totalEnd
-
totalStart
)
/
NS_TO_MS
;
LOG
.
info
(
,
totalMs
,
totalMs
/
OP_NUM
,
(
float
)
totalTrials
/
OP_NUM
)
;
Thread
.
sleep
(
1000
)
;
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
StorageType
type
=
StorageType
.
values
(
)
[
i
%
StorageType
.
values
(
)
.
length
]
;
localStart
=
System
.
nanoTime
(
)
;
node
=
dfscluster
.
chooseRandomWithStorageType
(
,
excluded
,
type
)
;
assertNotNull
(
node
)
;
totalStart
=
System
.
nanoTime
(
)
;
totalTrials
=
0
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
localStart
=
System
.
nanoTime
(
)
;
do
{
totalTrials
+=
1
;
node
=
cluster
.
chooseRandom
(
,
excluded
)
;
assertNotNull
(
node
)
;
if
(
isType
(
node
,
StorageType
.
ARCHIVE
)
)
{
break
;
}
excluded
.
add
(
node
)
;
}
while
(
true
)
;
excluded
.
clear
(
)
;
localEnd
=
System
.
nanoTime
(
)
;
records
[
i
]
=
localEnd
-
localStart
;
while
(
true
)
;
excluded
.
clear
(
)
;
localEnd
=
System
.
nanoTime
(
)
;
records
[
i
]
=
localEnd
-
localStart
;
}
totalEnd
=
System
.
nanoTime
(
)
;
totalMs
=
(
totalEnd
-
totalStart
)
/
NS_TO_MS
;
LOG
.
info
(
,
totalMs
,
totalMs
/
OP_NUM
,
(
float
)
totalTrials
/
OP_NUM
)
;
Thread
.
sleep
(
1000
)
;
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
localStart
=
System
.
nanoTime
(
)
;
node
=
dfscluster
.
chooseRandomWithStorageType
(
,
excluded
,
StorageType
.
ARCHIVE
)
;
assertNotNull
(
node
)
;
assertTrue
(
isType
(
node
,
StorageType
.
ARCHIVE
)
)
;
Thread
.
sleep
(
1000
)
;
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
totalTrials
=
0
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
do
{
totalTrials
+=
1
;
node
=
cluster
.
chooseRandom
(
,
excluded
)
;
assertNotNull
(
node
)
;
if
(
isType
(
node
,
StorageType
.
DISK
)
)
{
break
;
}
excluded
.
add
(
node
)
;
}
while
(
true
)
;
excluded
.
clear
(
)
;
}
totalEnd
=
System
.
nanoTime
(
)
;
break
;
}
excluded
.
add
(
node
)
;
}
while
(
true
)
;
excluded
.
clear
(
)
;
}
totalEnd
=
System
.
nanoTime
(
)
;
totalMs
=
(
totalEnd
-
totalStart
)
/
NS_TO_MS
;
LOG
.
info
(
,
totalMs
,
totalMs
/
OP_NUM
,
(
float
)
totalTrials
/
OP_NUM
)
;
Thread
.
sleep
(
1000
)
;
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
node
=
dfscluster
.
chooseRandomWithStorageType
(
,
excluded
,
StorageType
.
DISK
)
;
assertNotNull
(
node
)
;
assertTrue
(
isType
(
node
,
StorageType
.
DISK
)
)
;
}
totalEnd
=
System
.
nanoTime
(
)
;
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
totalTrials
=
0
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
localStart
=
System
.
nanoTime
(
)
;
do
{
totalTrials
+=
1
;
node
=
cluster
.
chooseRandom
(
,
excluded
)
;
assertNotNull
(
node
)
;
if
(
isType
(
node
,
StorageType
.
ARCHIVE
)
)
{
break
;
}
excluded
.
add
(
node
)
;
}
while
(
true
)
;
excluded
.
clear
(
)
;
localEnd
=
System
.
nanoTime
(
)
;
}
while
(
true
)
;
excluded
.
clear
(
)
;
localEnd
=
System
.
nanoTime
(
)
;
records
[
i
]
=
localEnd
-
localStart
;
}
totalEnd
=
System
.
nanoTime
(
)
;
totalMs
=
(
totalEnd
-
totalStart
)
/
NS_TO_MS
;
LOG
.
info
(
,
totalMs
,
totalMs
/
OP_NUM
,
(
float
)
totalTrials
/
OP_NUM
)
;
Thread
.
sleep
(
1000
)
;
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
localStart
=
System
.
nanoTime
(
)
;
node
=
dfscluster
.
chooseRandomWithStorageType
(
,
excluded
,
StorageType
.
ARCHIVE
)
;
assertNotNull
(
node
)
;
assertTrue
(
isType
(
node
,
StorageType
.
ARCHIVE
)
)
;
printMemUsage
(
)
;
totalStart
=
System
.
nanoTime
(
)
;
totalTrials
=
0
;
for
(
int
i
=
0
;
i
<
OP_NUM
;
i
++
)
{
localStart
=
System
.
nanoTime
(
)
;
totalTrials
+=
1
;
node
=
cluster
.
chooseRandom
(
,
excluded
)
;
assertNotNull
(
node
)
;
if
(
!
isType
(
node
,
StorageType
.
ARCHIVE
)
)
{
totalTrials
+=
1
;
excluded
.
add
(
node
)
;
node
=
dfscluster
.
chooseRandomWithStorageType
(
,
excluded
,
StorageType
.
ARCHIVE
)
;
}
assertTrue
(
isType
(
node
,
StorageType
.
ARCHIVE
)
)
;
excluded
.
clear
(
)
;
localEnd
=
System
.
nanoTime
(
)
;
private
void
printMemUsage
(
String
message
)
throws
Exception
{
Runtime
runtime
=
Runtime
.
getRuntime
(
)
;
NumberFormat
format
=
NumberFormat
.
getInstance
(
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
message
)
;
long
maxMemory
=
runtime
.
maxMemory
(
)
;
long
allocatedMemory
=
runtime
.
totalMemory
(
)
;
long
freeMemory
=
runtime
.
freeMemory
(
)
;
sb
.
append
(
+
format
.
format
(
freeMemory
/
1024
)
)
;
sb
.
append
(
+
format
.
format
(
allocatedMemory
/
1024
)
)
;
sb
.
append
(
+
format
.
format
(
maxMemory
/
1024
)
)
;
sb
.
append
(
+
format
.
format
(
(
freeMemory
+
(
maxMemory
-
allocatedMemory
)
)
/
1024
)
)
;
qjm
.
createNewUniqueEpoch
(
)
;
assertEquals
(
i
+
1
,
qjm
.
getLoggerSetForTests
(
)
.
getEpoch
(
)
)
;
}
finally
{
qjm
.
close
(
)
;
}
}
long
prevEpoch
=
5
;
for
(
int
i
=
0
;
i
<
20
;
i
++
)
{
long
newEpoch
=
-
1
;
while
(
true
)
{
qjm
=
new
QuorumJournalManager
(
conf
,
uri
,
FAKE_NSINFO
,
new
FaultyLoggerFactory
(
)
)
;
try
{
qjm
.
createNewUniqueEpoch
(
)
;
newEpoch
=
qjm
.
getLoggerSetForTests
(
)
.
getEpoch
(
)
;
break
;
}
catch
(
IOException
ioe
)
{
}
finally
{
MiniJournalCluster
cluster
=
new
MiniJournalCluster
.
Builder
(
conf
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
QuorumJournalManager
qjm
=
null
;
long
ret
;
try
{
qjm
=
createInjectableQJM
(
cluster
)
;
qjm
.
format
(
FAKE_NSINFO
,
false
)
;
doWorkload
(
cluster
,
qjm
)
;
SortedSet
<
Integer
>
ipcCounts
=
Sets
.
newTreeSet
(
)
;
for
(
AsyncLogger
l
:
qjm
.
getLoggerSetForTests
(
)
.
getLoggersForTests
(
)
)
{
InvocationCountingChannel
ch
=
(
InvocationCountingChannel
)
l
;
ch
.
waitForAllPendingCalls
(
)
;
ipcCounts
.
add
(
ch
.
getRpcCount
(
)
)
;
}
assertEquals
(
1
,
ipcCounts
.
size
(
)
)
;
ret
=
ipcCounts
.
first
(
)
;
@
Test
public
void
testRecoverAfterDoubleFailures
(
)
throws
Exception
{
final
long
MAX_IPC_NUMBER
=
determineMaxIpcNumber
(
)
;
for
(
int
failA
=
1
;
failA
<=
MAX_IPC_NUMBER
;
failA
++
)
{
for
(
int
failB
=
1
;
failB
<=
MAX_IPC_NUMBER
;
failB
++
)
{
String
injectionStr
=
+
failA
+
+
failB
+
;
for
(
int
failA
=
1
;
failA
<=
MAX_IPC_NUMBER
;
failA
++
)
{
for
(
int
failB
=
1
;
failB
<=
MAX_IPC_NUMBER
;
failB
++
)
{
String
injectionStr
=
+
failA
+
+
failB
+
;
LOG
.
info
(
+
+
injectionStr
+
+
)
;
MiniJournalCluster
cluster
=
new
MiniJournalCluster
.
Builder
(
conf
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
QuorumJournalManager
qjm
=
null
;
try
{
qjm
=
createInjectableQJM
(
cluster
)
;
qjm
.
format
(
FAKE_NSINFO
,
false
)
;
List
<
AsyncLogger
>
loggers
=
qjm
.
getLoggerSetForTests
(
)
.
getLoggersForTests
(
)
;
failIpcNumber
(
loggers
.
get
(
0
)
,
failA
)
;
failIpcNumber
(
loggers
.
get
(
1
)
,
failB
)
;
int
lastAckedTxn
=
doWorkload
(
cluster
,
qjm
)
;
if
(
lastAckedTxn
<
6
)
{
seed
=
userSpecifiedSeed
;
GenericTestUtils
.
setLogLevel
(
ProtobufRpcEngine2
.
LOG
,
Level
.
ALL
)
;
}
else
{
seed
=
new
Random
(
)
.
nextLong
(
)
;
}
LOG
.
info
(
+
seed
)
;
Random
r
=
new
Random
(
seed
)
;
MiniJournalCluster
cluster
=
new
MiniJournalCluster
.
Builder
(
conf
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
QuorumJournalManager
qjmForInitialFormat
=
createInjectableQJM
(
cluster
)
;
qjmForInitialFormat
.
format
(
FAKE_NSINFO
,
false
)
;
qjmForInitialFormat
.
close
(
)
;
try
{
long
txid
=
0
;
long
lastAcked
=
0
;
for
(
int
i
=
0
;
i
<
NUM_WRITER_ITERS
;
i
++
)
{
QuorumJournalManager
qjm
=
createRandomFaultyQJM
(
cluster
,
r
)
;
try
{
long
recovered
;
try
{
recovered
=
QJMTestUtil
.
recoverAndReturnLastTxn
(
qjm
)
;
}
catch
(
Throwable
t
)
{
LOG
.
info
(
,
t
)
;
checkException
(
t
)
;
continue
;
}
assertTrue
(
+
recovered
+
+
lastAcked
,
recovered
>=
lastAcked
)
;
txid
=
recovered
+
1
;
if
(
txid
>
100
&&
i
%
10
==
1
)
{
qjm
.
purgeLogsOlderThan
(
txid
-
100
)
;
}
Holder
<
Throwable
>
thrown
=
new
Holder
<
Throwable
>
(
null
)
;
for
(
int
j
=
0
;
j
<
SEGMENTS_PER_WRITER
;
j
++
)
{
@
Test
(
timeout
=
300000
)
public
void
testRpcBindHostKey
(
)
throws
IOException
{
LOG
.
info
(
+
DFS_JOURNALNODE_RPC_BIND_HOST_KEY
)
;
jCluster
=
new
MiniJournalCluster
.
Builder
(
conf
)
.
format
(
true
)
.
numJournalNodes
(
NUM_JN
)
.
build
(
)
;
jn
=
jCluster
.
getJournalNode
(
0
)
;
String
address
=
getRpcServerAddress
(
jn
)
;
assertThat
(
,
address
,
not
(
+
WILDCARD_ADDRESS
)
)
;
@
Test
(
timeout
=
300000
)
public
void
testHttpBindHostKey
(
)
throws
IOException
{
LOG
.
info
(
+
DFS_JOURNALNODE_HTTP_BIND_HOST_KEY
)
;
conf
.
set
(
DFS_JOURNALNODE_HTTP_ADDRESS_KEY
,
LOCALHOST_SERVER_ADDRESS
)
;
jCluster
=
new
MiniJournalCluster
.
Builder
(
conf
)
.
format
(
true
)
.
numJournalNodes
(
NUM_JN
)
.
build
(
)
;
jn
=
jCluster
.
getJournalNode
(
0
)
;
String
address
=
jn
.
getHttpAddress
(
)
.
toString
(
)
;
assertFalse
(
,
address
.
startsWith
(
WILDCARD_ADDRESS
)
)
;
@
Test
(
timeout
=
300000
)
public
void
testHttpsBindHostKey
(
)
throws
Exception
{
LOG
.
info
(
+
DFS_JOURNALNODE_HTTPS_BIND_HOST_KEY
)
;
setupSsl
(
)
;
conf
.
set
(
DFS_HTTP_POLICY_KEY
,
HttpConfig
.
Policy
.
HTTPS_ONLY
.
name
(
)
)
;
conf
.
set
(
DFS_JOURNALNODE_HTTPS_ADDRESS_KEY
,
LOCALHOST_SERVER_ADDRESS
)
;
jCluster
=
new
MiniJournalCluster
.
Builder
(
conf
)
.
format
(
true
)
.
numJournalNodes
(
NUM_JN
)
.
build
(
)
;
jn
=
jCluster
.
getJournalNode
(
0
)
;
String
address
=
jn
.
getHttpsAddress
(
)
.
toString
(
)
;
assertFalse
(
,
address
.
startsWith
(
WILDCARD_ADDRESS
)
)
;
private
static
void
configureSuperUserIPAddresses
(
Configuration
conf
,
String
superUserShortName
)
throws
IOException
{
ArrayList
<
String
>
ipList
=
new
ArrayList
<
String
>
(
)
;
Enumeration
<
NetworkInterface
>
netInterfaceList
=
NetworkInterface
.
getNetworkInterfaces
(
)
;
while
(
netInterfaceList
.
hasMoreElements
(
)
)
{
NetworkInterface
inf
=
netInterfaceList
.
nextElement
(
)
;
Enumeration
<
InetAddress
>
addrList
=
inf
.
getInetAddresses
(
)
;
while
(
addrList
.
hasMoreElements
(
)
)
{
InetAddress
addr
=
addrList
.
nextElement
(
)
;
ipList
.
add
(
addr
.
getHostAddress
(
)
)
;
}
}
StringBuilder
builder
=
new
StringBuilder
(
)
;
for
(
String
ip
:
ipList
)
{
builder
.
append
(
ip
)
;
builder
.
append
(
','
)
;
}
builder
.
append
(
)
;
builder
.
append
(
InetAddress
.
getLocalHost
(
)
.
getCanonicalHostName
(
)
)
;
private
void
doTest
(
Configuration
conf
,
long
[
]
capacities
,
String
[
]
racks
,
long
newCapacity
,
String
newRack
,
NewNodeInfo
nodes
,
boolean
useTool
,
boolean
useFile
,
boolean
useNamesystemSpy
,
double
clusterUtilization
)
throws
Exception
{
private
void
doTest
(
Configuration
conf
,
long
[
]
capacities
,
String
[
]
racks
,
long
newCapacity
,
String
newRack
,
NewNodeInfo
nodes
,
boolean
useTool
,
boolean
useFile
,
boolean
useNamesystemSpy
,
double
clusterUtilization
)
throws
Exception
{
LOG
.
info
(
+
long2String
(
capacities
)
)
;
private
void
doTest
(
Configuration
conf
,
long
[
]
capacities
,
String
[
]
racks
,
long
newCapacity
,
String
newRack
,
NewNodeInfo
nodes
,
boolean
useTool
,
boolean
useFile
,
boolean
useNamesystemSpy
,
double
clusterUtilization
)
throws
Exception
{
LOG
.
info
(
+
long2String
(
capacities
)
)
;
LOG
.
info
(
+
Arrays
.
asList
(
racks
)
)
;
private
void
doTest
(
Configuration
conf
,
long
[
]
capacities
,
String
[
]
racks
,
long
newCapacity
,
String
newRack
,
NewNodeInfo
nodes
,
boolean
useTool
,
boolean
useFile
,
boolean
useNamesystemSpy
,
double
clusterUtilization
)
throws
Exception
{
LOG
.
info
(
+
long2String
(
capacities
)
)
;
LOG
.
info
(
+
Arrays
.
asList
(
racks
)
)
;
LOG
.
info
(
+
newCapacity
)
;
private
void
doTest
(
Configuration
conf
,
long
[
]
capacities
,
String
[
]
racks
,
long
newCapacity
,
String
newRack
,
NewNodeInfo
nodes
,
boolean
useTool
,
boolean
useFile
,
boolean
useNamesystemSpy
,
double
clusterUtilization
)
throws
Exception
{
LOG
.
info
(
+
long2String
(
capacities
)
)
;
LOG
.
info
(
+
Arrays
.
asList
(
racks
)
)
;
LOG
.
info
(
+
newCapacity
)
;
LOG
.
info
(
+
newRack
)
;
private
static
int
runBalancer
(
Collection
<
URI
>
namenodes
,
final
BalancerParameters
p
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getLong
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
)
*
2000
+
conf
.
getLong
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
)
*
1000
;
private
static
int
runBalancer
(
Collection
<
URI
>
namenodes
,
final
BalancerParameters
p
,
Configuration
conf
)
throws
IOException
,
InterruptedException
{
final
long
sleeptime
=
conf
.
getLong
(
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_KEY
,
DFSConfigKeys
.
DFS_HEARTBEAT_INTERVAL_DEFAULT
)
*
2000
+
conf
.
getLong
(
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY
,
DFSConfigKeys
.
DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT
)
*
1000
;
LOG
.
info
(
+
namenodes
)
;
cluster
.
getConfiguration
(
0
)
.
setInt
(
DFSConfigKeys
.
DFS_REPLICATION_KEY
,
1
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_REPLICATION_KEY
,
1
)
;
cluster
.
startDataNodes
(
conf
,
1
,
true
,
null
,
null
,
dnCapacities
)
;
cluster
.
waitClusterUp
(
)
;
cluster
.
waitActive
(
)
;
final
Path
path
=
new
Path
(
)
;
DistributedFileSystem
fs
=
cluster
.
getFileSystem
(
)
;
DFSTestUtil
.
createFile
(
fs
,
path
,
4L
*
blockSize
,
rep
,
seed
)
;
cluster
.
startDataNodes
(
conf
,
1
,
true
,
null
,
null
,
dnCapacities
)
;
cluster
.
triggerHeartbeats
(
)
;
List
<
NameNodeConnector
>
connectors
=
Collections
.
emptyList
(
)
;
try
{
BalancerParameters
bParams
=
BalancerParameters
.
DEFAULT
;
connectors
=
NameNodeConnector
.
newNameNodeConnectors
(
DFSUtil
.
getInternalNsRpcUris
(
conf
)
,
Balancer
.
class
.
getSimpleName
(
)
,
Balancer
.
BALANCER_ID_PATH
,
conf
,
bParams
.
getMaxIdleIteration
(
)
)
;
for
(
NameNodeConnector
nnc
:
connectors
)
{
@
Test
(
timeout
=
100000
)
public
void
testManyBalancerSimultaneously
(
)
throws
Exception
{
final
Configuration
conf
=
new
HdfsConfiguration
(
)
;
initConf
(
conf
)
;
long
[
]
capacities
=
new
long
[
]
{
4
*
CAPACITY
}
;
String
[
]
racks
=
new
String
[
]
{
RACK0
}
;
long
newCapacity
=
2
*
CAPACITY
;
String
newRack
=
RACK0
;
@
Test
(
timeout
=
100000
)
public
void
testManyBalancerSimultaneously
(
)
throws
Exception
{
final
Configuration
conf
=
new
HdfsConfiguration
(
)
;
initConf
(
conf
)
;
long
[
]
capacities
=
new
long
[
]
{
4
*
CAPACITY
}
;
String
[
]
racks
=
new
String
[
]
{
RACK0
}
;
long
newCapacity
=
2
*
CAPACITY
;
String
newRack
=
RACK0
;
LOG
.
info
(
+
long2String
(
capacities
)
)
;
@
Test
(
timeout
=
100000
)
public
void
testManyBalancerSimultaneously
(
)
throws
Exception
{
final
Configuration
conf
=
new
HdfsConfiguration
(
)
;
initConf
(
conf
)
;
long
[
]
capacities
=
new
long
[
]
{
4
*
CAPACITY
}
;
String
[
]
racks
=
new
String
[
]
{
RACK0
}
;
long
newCapacity
=
2
*
CAPACITY
;
String
newRack
=
RACK0
;
LOG
.
info
(
+
long2String
(
capacities
)
)
;
LOG
.
info
(
+
Arrays
.
asList
(
racks
)
)
;
@
Test
(
timeout
=
100000
)
public
void
testManyBalancerSimultaneously
(
)
throws
Exception
{
final
Configuration
conf
=
new
HdfsConfiguration
(
)
;
initConf
(
conf
)
;
long
[
]
capacities
=
new
long
[
]
{
4
*
CAPACITY
}
;
String
[
]
racks
=
new
String
[
]
{
RACK0
}
;
long
newCapacity
=
2
*
CAPACITY
;
String
newRack
=
RACK0
;
LOG
.
info
(
+
long2String
(
capacities
)
)
;
LOG
.
info
(
+
Arrays
.
asList
(
racks
)
)
;
LOG
.
info
(
+
newCapacity
)
;
final
long
[
]
lengths
=
{
10
,
10
,
10
,
10
}
;
final
long
[
]
capacities
=
new
long
[
replication
]
;
final
long
totalUsed
=
capacities
.
length
*
sum
(
lengths
)
;
Arrays
.
fill
(
capacities
,
1000
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
capacities
.
length
)
.
simulatedCapacities
(
capacities
)
.
build
(
)
;
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
cluster
.
waitActive
(
)
;
client
=
NameNodeProxies
.
createProxy
(
conf
,
dfs
.
getUri
(
)
,
ClientProtocol
.
class
)
.
getProxy
(
)
;
for
(
int
i
=
0
;
i
<
lengths
.
length
;
i
++
)
{
final
long
size
=
lengths
[
i
]
;
final
Path
p
=
new
Path
(
+
i
+
+
size
)
;
try
(
OutputStream
out
=
dfs
.
create
(
p
)
)
{
for
(
int
j
=
0
;
j
<
size
;
j
++
)
{
out
.
write
(
j
)
;
}
}
}
cluster
.
startDataNodes
(
conf
,
capacities
.
length
,
true
,
null
,
null
,
capacities
)
;
final
long
[
]
capacities
=
new
long
[
replication
]
;
final
long
totalUsed
=
capacities
.
length
*
sum
(
lengths
)
;
Arrays
.
fill
(
capacities
,
1000
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
capacities
.
length
)
.
simulatedCapacities
(
capacities
)
.
build
(
)
;
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
cluster
.
waitActive
(
)
;
client
=
NameNodeProxies
.
createProxy
(
conf
,
dfs
.
getUri
(
)
,
ClientProtocol
.
class
)
.
getProxy
(
)
;
for
(
int
i
=
0
;
i
<
lengths
.
length
;
i
++
)
{
final
long
size
=
lengths
[
i
]
;
final
Path
p
=
new
Path
(
+
i
+
+
size
)
;
try
(
OutputStream
out
=
dfs
.
create
(
p
)
)
{
for
(
int
j
=
0
;
j
<
size
;
j
++
)
{
out
.
write
(
j
)
;
}
}
}
cluster
.
startDataNodes
(
conf
,
capacities
.
length
,
true
,
null
,
null
,
capacities
)
;
LOG
.
info
(
+
Arrays
.
toString
(
capacities
)
)
;
final
long
totalUsed
=
capacities
.
length
*
sum
(
lengths
)
;
Arrays
.
fill
(
capacities
,
1000
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
capacities
.
length
)
.
simulatedCapacities
(
capacities
)
.
build
(
)
;
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
cluster
.
waitActive
(
)
;
client
=
NameNodeProxies
.
createProxy
(
conf
,
dfs
.
getUri
(
)
,
ClientProtocol
.
class
)
.
getProxy
(
)
;
for
(
int
i
=
0
;
i
<
lengths
.
length
;
i
++
)
{
final
long
size
=
lengths
[
i
]
;
final
Path
p
=
new
Path
(
+
i
+
+
size
)
;
try
(
OutputStream
out
=
dfs
.
create
(
p
)
)
{
for
(
int
j
=
0
;
j
<
size
;
j
++
)
{
out
.
write
(
j
)
;
}
}
}
cluster
.
startDataNodes
(
conf
,
capacities
.
length
,
true
,
null
,
null
,
capacities
)
;
LOG
.
info
(
+
Arrays
.
toString
(
capacities
)
)
;
LOG
.
info
(
+
totalUsed
)
;
void
testBalancerRPCDelay
(
int
getBlocksMaxQps
)
throws
Exception
{
final
Configuration
conf
=
new
HdfsConfiguration
(
)
;
initConf
(
conf
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_BALANCER_DISPATCHERTHREADS_KEY
,
30
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_GETBLOCKS_MAX_QPS_KEY
,
getBlocksMaxQps
)
;
int
numDNs
=
20
;
long
[
]
capacities
=
new
long
[
numDNs
]
;
String
[
]
racks
=
new
String
[
numDNs
]
;
for
(
int
i
=
0
;
i
<
numDNs
;
i
++
)
{
capacities
[
i
]
=
CAPACITY
;
racks
[
i
]
=
(
i
<
numDNs
/
2
?
RACK0
:
RACK1
)
;
}
doTest
(
conf
,
capacities
,
racks
,
CAPACITY
,
RACK2
,
new
PortNumberBasedNodes
(
1
,
0
,
0
)
,
false
,
false
,
true
,
0.5
)
;
assertTrue
(
+
getBlocksMaxQps
,
numGetBlocksCalls
.
get
(
)
>=
getBlocksMaxQps
)
;
long
durationMs
=
1
+
endGetBlocksTime
.
get
(
)
-
startGetBlocksTime
.
get
(
)
;
int
durationSec
=
(
int
)
Math
.
ceil
(
durationMs
/
1000.0
)
;
static
void
wait
(
final
ClientProtocol
[
]
clients
,
long
expectedUsedSpace
,
long
expectedTotalSpace
)
throws
IOException
{
static
void
runBalancer
(
Suite
s
,
final
long
totalUsed
,
final
long
totalCapacity
)
throws
Exception
{
double
avg
=
totalUsed
*
100.0
/
totalCapacity
;
private
static
void
compareTotalPoolUsage
(
DatanodeStorageReport
[
]
preReports
,
DatanodeStorageReport
[
]
postReports
)
{
Assert
.
assertNotNull
(
preReports
)
;
Assert
.
assertNotNull
(
postReports
)
;
Assert
.
assertEquals
(
preReports
.
length
,
postReports
.
length
)
;
for
(
DatanodeStorageReport
preReport
:
preReports
)
{
String
dnUuid
=
preReport
.
getDatanodeInfo
(
)
.
getDatanodeUuid
(
)
;
for
(
DatanodeStorageReport
postReport
:
postReports
)
{
if
(
postReport
.
getDatanodeInfo
(
)
.
getDatanodeUuid
(
)
.
equals
(
dnUuid
)
)
{
Assert
.
assertEquals
(
getTotalPoolUsage
(
preReport
)
,
getTotalPoolUsage
(
postReport
)
)
;
private
void
runTest
(
final
int
nNameNodes
,
String
[
]
racks
,
String
[
]
newRack
,
Configuration
conf
,
int
nNameNodestoBalance
,
BalancerParameters
balancerParameters
)
throws
Exception
{
final
int
nDataNodes
=
racks
.
length
;
final
long
[
]
capacities
=
new
long
[
nDataNodes
]
;
Arrays
.
fill
(
capacities
,
CAPACITY
)
;
private
void
runTest
(
final
int
nNameNodes
,
String
[
]
racks
,
String
[
]
newRack
,
Configuration
conf
,
int
nNameNodestoBalance
,
BalancerParameters
balancerParameters
)
throws
Exception
{
final
int
nDataNodes
=
racks
.
length
;
final
long
[
]
capacities
=
new
long
[
nDataNodes
]
;
Arrays
.
fill
(
capacities
,
CAPACITY
)
;
LOG
.
info
(
+
nNameNodes
+
+
nDataNodes
)
;
Assert
.
assertEquals
(
nDataNodes
,
racks
.
length
)
;
private
void
verifyPlacementPolicy
(
final
MiniDFSCluster
cluster
,
final
Path
file
,
boolean
isBlockPlacementSatisfied
)
throws
IOException
{
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
BlockManager
blockManager
=
cluster
.
getNamesystem
(
)
.
getBlockManager
(
)
;
LocatedBlock
lb
=
DFSTestUtil
.
getAllBlocks
(
dfs
,
file
)
.
get
(
0
)
;
BlockInfo
blockInfo
=
blockManager
.
getStoredBlock
(
lb
.
getBlock
(
)
.
getLocalBlock
(
)
)
;
Iterator
<
DatanodeStorageInfo
>
itr
=
blockInfo
.
getStorageInfos
(
)
;
private
static
void
setFailure
(
AtomicReference
<
String
>
failure
,
String
what
)
{
failure
.
compareAndSet
(
,
what
)
;
@
Test
(
timeout
=
180000
)
public
void
testRateLimitingDuringDataNodeStartup
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
DFS_NAMENODE_MAX_FULL_BLOCK_REPORT_LEASES
,
1
)
;
conf
.
setLong
(
DFS_NAMENODE_FULL_BLOCK_REPORT_LEASE_LENGTH_MS
,
20L
*
60L
*
1000L
)
;
final
Semaphore
fbrSem
=
new
Semaphore
(
0
)
;
final
HashSet
<
DatanodeID
>
expectedFbrDns
=
new
HashSet
<
>
(
)
;
final
HashSet
<
DatanodeID
>
fbrDns
=
new
HashSet
<
>
(
)
;
final
AtomicReference
<
String
>
failure
=
new
AtomicReference
<
String
>
(
)
;
final
BlockManagerFaultInjector
injector
=
new
BlockManagerFaultInjector
(
)
{
private
int
numLeases
=
0
;
@
Override
public
void
incomingBlockReportRpc
(
DatanodeID
nodeID
,
BlockReportContext
context
)
throws
IOException
{
LOG
.
info
(
+
nodeID
+
+
Long
.
toHexString
(
context
.
getLeaseId
(
)
)
)
;
if
(
context
.
getLeaseId
(
)
==
0
)
{
setFailure
(
failure
,
+
+
nodeID
)
;
}
fbrSem
.
acquireUninterruptibly
(
)
;
synchronized
(
this
)
{
fbrDns
.
add
(
nodeID
)
;
if
(
!
expectedFbrDns
.
remove
(
nodeID
)
)
{
setFailure
(
failure
,
+
+
nodeID
+
+
Joiner
.
on
(
)
.
join
(
expectedFbrDns
)
)
;
}
LOG
.
info
(
+
nodeID
+
+
Long
.
toHexString
(
context
.
getLeaseId
(
)
)
)
;
}
}
@
Override
public
void
requestBlockReportLease
(
DatanodeDescriptor
node
,
long
leaseId
)
{
if
(
leaseId
==
0
)
{
return
;
}
synchronized
(
this
)
{
numLeases
++
;
expectedFbrDns
.
add
(
node
)
;
fbrSem
.
acquireUninterruptibly
(
)
;
synchronized
(
this
)
{
fbrDns
.
add
(
nodeID
)
;
if
(
!
expectedFbrDns
.
remove
(
nodeID
)
)
{
setFailure
(
failure
,
+
+
nodeID
+
+
Joiner
.
on
(
)
.
join
(
expectedFbrDns
)
)
;
}
LOG
.
info
(
+
nodeID
+
+
Long
.
toHexString
(
context
.
getLeaseId
(
)
)
)
;
}
}
@
Override
public
void
requestBlockReportLease
(
DatanodeDescriptor
node
,
long
leaseId
)
{
if
(
leaseId
==
0
)
{
return
;
}
synchronized
(
this
)
{
numLeases
++
;
expectedFbrDns
.
add
(
node
)
;
LOG
.
info
(
+
node
+
+
Long
.
toHexString
(
leaseId
)
+
+
+
Joiner
.
on
(
)
.
join
(
expectedFbrDns
)
)
;
if
(
numLeases
>
1
)
{
setFailure
(
failure
,
)
;
String
poolId
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
DatanodeRegistration
reg
=
InternalDataNodeTestUtils
.
getDNRegistrationForBP
(
cluster
.
getDataNodes
(
)
.
get
(
3
)
,
poolId
)
;
cluster
.
stopDataNode
(
3
)
;
DFSTestUtil
.
waitForDatanodeState
(
cluster
,
reg
.
getDatanodeUuid
(
)
,
false
,
20000
)
;
final
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
DFSTestUtil
.
createFile
(
fs
,
filePath
,
1L
,
replicationFactor
,
1L
)
;
ExtendedBlock
b
=
DFSTestUtil
.
getFirstBlock
(
fs
,
filePath
)
;
DFSTestUtil
.
waitReplication
(
cluster
.
getFileSystem
(
)
,
filePath
,
replicationFactor
)
;
cluster
.
startDataNodes
(
conf
,
1
,
true
,
null
,
new
String
[
]
{
}
)
;
cluster
.
waitActive
(
)
;
try
{
DFSTestUtil
.
waitForReplication
(
cluster
,
b
,
2
,
replicationFactor
,
0
)
;
fail
(
+
)
;
}
catch
(
TimeoutException
e
)
{
}
String
fsckOp
=
DFSTestUtil
.
runFsck
(
conf
,
0
,
true
,
filePath
.
toString
(
)
,
)
;
@
Test
public
void
testNumVersionsReportedCorrect
(
)
throws
IOException
{
FSNamesystem
fsn
=
Mockito
.
mock
(
FSNamesystem
.
class
)
;
Mockito
.
when
(
fsn
.
hasWriteLock
(
)
)
.
thenReturn
(
true
)
;
DatanodeManager
dm
=
mockDatanodeManager
(
fsn
,
new
Configuration
(
)
)
;
Random
rng
=
new
Random
(
)
;
int
seed
=
rng
.
nextInt
(
)
;
rng
=
new
Random
(
seed
)
;
FSNamesystem
fsn
=
Mockito
.
mock
(
FSNamesystem
.
class
)
;
Mockito
.
when
(
fsn
.
hasWriteLock
(
)
)
.
thenReturn
(
true
)
;
DatanodeManager
dm
=
mockDatanodeManager
(
fsn
,
new
Configuration
(
)
)
;
Random
rng
=
new
Random
(
)
;
int
seed
=
rng
.
nextInt
(
)
;
rng
=
new
Random
(
seed
)
;
LOG
.
info
(
+
seed
+
)
;
HashMap
<
String
,
DatanodeRegistration
>
sIdToDnReg
=
new
HashMap
<
String
,
DatanodeRegistration
>
(
)
;
for
(
int
i
=
0
;
i
<
NUM_ITERATIONS
;
++
i
)
{
if
(
rng
.
nextBoolean
(
)
&&
i
%
3
==
0
&&
sIdToDnReg
.
size
(
)
!=
0
)
{
int
randomIndex
=
rng
.
nextInt
(
)
%
sIdToDnReg
.
size
(
)
;
Iterator
<
Map
.
Entry
<
String
,
DatanodeRegistration
>>
it
=
sIdToDnReg
.
entrySet
(
)
.
iterator
(
)
;
for
(
int
j
=
0
;
j
<
randomIndex
-
1
;
++
j
)
{
it
.
next
(
)
;
}
DatanodeRegistration
toRemove
=
it
.
next
(
)
.
getValue
(
)
;
dm
.
removeDatanode
(
toRemove
)
;
it
.
remove
(
)
;
}
else
{
String
storageID
=
+
rng
.
nextInt
(
5000
)
;
DatanodeRegistration
dr
=
Mockito
.
mock
(
DatanodeRegistration
.
class
)
;
Mockito
.
when
(
dr
.
getDatanodeUuid
(
)
)
.
thenReturn
(
storageID
)
;
if
(
sIdToDnReg
.
containsKey
(
storageID
)
)
{
dr
=
sIdToDnReg
.
get
(
storageID
)
;
if
(
rng
.
nextBoolean
(
)
)
{
dr
.
setIpAddr
(
dr
.
getIpAddr
(
)
+
)
;
}
}
else
{
String
ip
=
+
storageID
;
Mockito
.
when
(
dr
.
getIpAddr
(
)
)
.
thenReturn
(
ip
)
;
Mockito
.
when
(
dr
.
getXferAddr
(
)
)
.
thenReturn
(
ip
+
)
;
Mockito
.
when
(
dr
.
getXferPort
(
)
)
.
thenReturn
(
9000
)
;
if
(
dn
.
getDatanodeUuid
(
)
.
equals
(
datanodeUuid
)
)
{
datanodeToRemoveStorageFrom
=
dn
;
break
;
}
datanodeToRemoveStorageFromIdx
++
;
}
StorageLocation
volumeLocationToRemove
=
null
;
try
(
FsVolumeReferences
volumes
=
datanodeToRemoveStorageFrom
.
getFSDataset
(
)
.
getFsVolumeReferences
(
)
)
{
assertEquals
(
NUM_STORAGES_PER_DN
,
volumes
.
size
(
)
)
;
for
(
FsVolumeSpi
volume
:
volumes
)
{
if
(
volume
.
getStorageID
(
)
.
equals
(
storageIdToRemove
)
)
{
volumeLocationToRemove
=
volume
.
getStorageLocation
(
)
;
}
}
}
;
assertNotNull
(
volumeLocationToRemove
)
;
datanodeToRemoveStorageFrom
.
shutdown
(
)
;
FileUtil
.
fullyDelete
(
new
File
(
volumeLocationToRemove
.
getUri
(
)
)
)
;
FileOutputStream
fos
=
new
FileOutputStream
(
new
File
(
volumeLocationToRemove
.
getUri
(
)
)
)
;
assertEquals
(
NUM_STORAGES_PER_DN
,
volumes
.
size
(
)
)
;
for
(
FsVolumeSpi
volume
:
volumes
)
{
if
(
volume
.
getStorageID
(
)
.
equals
(
storageIdToRemove
)
)
{
volumeLocationToRemove
=
volume
.
getStorageLocation
(
)
;
}
}
}
;
assertNotNull
(
volumeLocationToRemove
)
;
datanodeToRemoveStorageFrom
.
shutdown
(
)
;
FileUtil
.
fullyDelete
(
new
File
(
volumeLocationToRemove
.
getUri
(
)
)
)
;
FileOutputStream
fos
=
new
FileOutputStream
(
new
File
(
volumeLocationToRemove
.
getUri
(
)
)
)
;
try
{
fos
.
write
(
1
)
;
}
finally
{
fos
.
close
(
)
;
}
cluster
.
restartDataNode
(
datanodeToRemoveStorageFromIdx
)
;
LOG
.
info
(
+
storageIdToRemove
)
;
final
String
newStorageId
=
DatanodeStorage
.
generateUuid
(
)
;
try
{
File
currentDir
=
new
File
(
new
File
(
volumeRefs
.
get
(
0
)
.
getStorageLocation
(
)
.
getUri
(
)
)
,
)
;
File
versionFile
=
new
File
(
currentDir
,
)
;
rewriteVersionFile
(
versionFile
,
newStorageId
)
;
}
finally
{
volumeRefs
.
close
(
)
;
}
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
TEST_PATH
)
;
cluster
.
restartDataNodes
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
cluster
.
getNamesystem
(
)
.
writeLock
(
)
;
try
{
Iterator
<
DatanodeStorageInfo
>
storageInfoIter
=
cluster
.
getNamesystem
(
)
.
getBlockManager
(
)
.
getStorages
(
block
.
getLocalBlock
(
)
)
.
iterator
(
)
;
if
(
!
storageInfoIter
.
hasNext
(
)
)
{
rewriteVersionFile
(
versionFile
,
newStorageId
)
;
}
finally
{
volumeRefs
.
close
(
)
;
}
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
TEST_PATH
)
;
cluster
.
restartDataNodes
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
cluster
.
getNamesystem
(
)
.
writeLock
(
)
;
try
{
Iterator
<
DatanodeStorageInfo
>
storageInfoIter
=
cluster
.
getNamesystem
(
)
.
getBlockManager
(
)
.
getStorages
(
block
.
getLocalBlock
(
)
)
.
iterator
(
)
;
if
(
!
storageInfoIter
.
hasNext
(
)
)
{
LOG
.
info
(
+
block
.
getBlockName
(
)
+
+
)
;
return
false
;
}
DatanodeStorageInfo
info
=
storageInfoIter
.
next
(
)
;
if
(
!
newStorageId
.
equals
(
info
.
getStorageID
(
)
)
)
{
volumeRefs
.
close
(
)
;
}
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
TEST_PATH
)
;
cluster
.
restartDataNodes
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
cluster
.
getNamesystem
(
)
.
writeLock
(
)
;
try
{
Iterator
<
DatanodeStorageInfo
>
storageInfoIter
=
cluster
.
getNamesystem
(
)
.
getBlockManager
(
)
.
getStorages
(
block
.
getLocalBlock
(
)
)
.
iterator
(
)
;
if
(
!
storageInfoIter
.
hasNext
(
)
)
{
LOG
.
info
(
+
block
.
getBlockName
(
)
+
+
)
;
return
false
;
}
DatanodeStorageInfo
info
=
storageInfoIter
.
next
(
)
;
if
(
!
newStorageId
.
equals
(
info
.
getStorageID
(
)
)
)
{
LOG
.
info
(
+
block
.
getBlockName
(
)
+
+
+
newStorageId
+
+
+
info
.
getStorageID
(
)
+
+
)
;
return
false
;
final
FSNamesystem
namesystem
=
cluster
.
getNamesystem
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
Path
testPath
=
new
Path
(
,
)
;
out
=
fs
.
create
(
testPath
,
(
short
)
2
)
;
out
.
writeBytes
(
+
testPath
)
;
out
.
hsync
(
)
;
cluster
.
startDataNodes
(
conf
,
1
,
true
,
null
,
null
,
null
)
;
ExtendedBlock
blk
=
DFSTestUtil
.
getFirstBlock
(
fs
,
testPath
)
;
MaterializedReplica
replica
=
cluster
.
getMaterializedReplica
(
0
,
blk
)
;
replica
.
deleteData
(
)
;
replica
.
deleteMeta
(
)
;
out
.
close
(
)
;
int
liveReplicas
=
0
;
while
(
true
)
{
if
(
(
liveReplicas
=
countReplicas
(
namesystem
,
blk
)
.
liveReplicas
(
)
)
<
2
)
{
cluster
.
startDataNodes
(
conf
,
1
,
true
,
null
,
null
,
null
)
;
ExtendedBlock
blk
=
DFSTestUtil
.
getFirstBlock
(
fs
,
testPath
)
;
MaterializedReplica
replica
=
cluster
.
getMaterializedReplica
(
0
,
blk
)
;
replica
.
deleteData
(
)
;
replica
.
deleteMeta
(
)
;
out
.
close
(
)
;
int
liveReplicas
=
0
;
while
(
true
)
{
if
(
(
liveReplicas
=
countReplicas
(
namesystem
,
blk
)
.
liveReplicas
(
)
)
<
2
)
{
LOG
.
info
(
+
liveReplicas
)
;
break
;
}
Thread
.
sleep
(
100
)
;
}
assertEquals
(
+
,
1
,
liveReplicas
)
;
while
(
true
)
{
if
(
(
liveReplicas
=
countReplicas
(
namesystem
,
blk
)
.
liveReplicas
(
)
)
>
1
)
{
@
Test
public
void
testReconstructForNotEnoughRacks
(
)
throws
Exception
{
LOG
.
info
(
,
Arrays
.
asList
(
hosts
)
,
Arrays
.
asList
(
racks
)
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
racks
(
racks
)
.
hosts
(
hosts
)
.
numDataNodes
(
hosts
.
length
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
fs
=
cluster
.
getFileSystem
(
)
;
fs
.
enableErasureCodingPolicy
(
StripedFileTestUtil
.
getDefaultECPolicy
(
)
.
getName
(
)
)
;
fs
.
setErasureCodingPolicy
(
new
Path
(
)
,
StripedFileTestUtil
.
getDefaultECPolicy
(
)
.
getName
(
)
)
;
FSNamesystem
fsn
=
cluster
.
getNamesystem
(
)
;
BlockManager
bm
=
fsn
.
getBlockManager
(
)
;
MiniDFSCluster
.
DataNodeProperties
lastHost
=
stopDataNode
(
hosts
[
hosts
.
length
-
1
]
)
;
final
Path
file
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
file
,
cellSize
*
dataBlocks
*
2
,
(
short
)
1
,
0L
)
;
GenericTestUtils
.
waitFor
(
(
)
->
bm
.
numOfUnderReplicatedBlocks
(
)
==
0
,
100
,
30000
)
;
BlockManager
bm
=
fsn
.
getBlockManager
(
)
;
MiniDFSCluster
.
DataNodeProperties
lastHost
=
stopDataNode
(
hosts
[
hosts
.
length
-
1
]
)
;
final
Path
file
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
file
,
cellSize
*
dataBlocks
*
2
,
(
short
)
1
,
0L
)
;
GenericTestUtils
.
waitFor
(
(
)
->
bm
.
numOfUnderReplicatedBlocks
(
)
==
0
,
100
,
30000
)
;
LOG
.
info
(
,
file
)
;
final
INodeFile
fileNode
=
fsn
.
getFSDirectory
(
)
.
getINode4Write
(
file
.
toString
(
)
)
.
asFile
(
)
;
BlockInfoStriped
blockInfo
=
(
BlockInfoStriped
)
fileNode
.
getLastBlock
(
)
;
Set
<
String
>
rackSet
=
new
HashSet
<
>
(
)
;
for
(
DatanodeStorageInfo
storage
:
blockInfo
.
storages
)
{
rackSet
.
add
(
storage
.
getDatanodeDescriptor
(
)
.
getNetworkLocation
(
)
)
;
}
Assert
.
assertEquals
(
+
rackSet
,
dataBlocks
-
1
,
rackSet
.
size
(
)
)
;
cluster
.
restartDataNode
(
lastHost
)
;
cluster
.
waitActive
(
)
;
NetworkTopology
topology
=
bm
.
getDatanodeManager
(
)
.
getNetworkTopology
(
)
;
@
Test
(
timeout
=
60000
)
public
void
testBlockGroupIdGeneration
(
)
throws
IOException
{
long
blockGroupIdInitialValue
=
blockGrpIdGenerator
.
getCurrentValue
(
)
;
Path
path
=
new
Path
(
ecDir
,
)
;
DFSTestUtil
.
createFile
(
fs
,
path
,
cellSize
,
fileLen
,
blockSize
,
REPLICATION
,
SEED
)
;
List
<
LocatedBlock
>
blocks
=
DFSTestUtil
.
getAllBlocks
(
fs
,
path
)
;
assertThat
(
,
blocks
.
size
(
)
,
is
(
blockGrpCount
)
)
;
blockGrpIdGenerator
.
setCurrentValue
(
blockGroupIdInitialValue
)
;
for
(
int
i
=
0
;
i
<
blocks
.
size
(
)
;
++
i
)
{
blockGrpIdGenerator
.
skipTo
(
(
blockGrpIdGenerator
.
getCurrentValue
(
)
&
~
BLOCK_GROUP_INDEX_MASK
)
+
MAX_BLOCKS_IN_GROUP
)
;
long
nextBlockExpectedId
=
blockGrpIdGenerator
.
getCurrentValue
(
)
;
long
nextBlockGrpId
=
blocks
.
get
(
i
)
.
getBlock
(
)
.
getBlockId
(
)
;
@
Test
public
void
testBlockIdGeneration
(
)
throws
IOException
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_REPLICATION_KEY
,
1
)
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
1
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
Path
path
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
path
,
IO_SIZE
,
BLOCK_SIZE
*
10
,
BLOCK_SIZE
,
REPLICATION
,
SEED
)
;
List
<
LocatedBlock
>
blocks
=
DFSTestUtil
.
getAllBlocks
(
fs
,
path
)
;
LOG
.
info
(
+
blocks
.
get
(
0
)
.
getBlock
(
)
.
getBlockId
(
)
)
;
long
nextBlockExpectedId
=
blocks
.
get
(
0
)
.
getBlock
(
)
.
getBlockId
(
)
+
1
;
for
(
int
i
=
1
;
i
<
blocks
.
size
(
)
;
++
i
)
{
long
nextBlockId
=
blocks
.
get
(
i
)
.
getBlock
(
)
.
getBlockId
(
)
;
private
Set
<
ReportForJson
>
getAndDeserializeJson
(
)
throws
IOException
{
final
String
json
=
tracker
.
getJson
(
)
;
private
void
assertDecommnNodePosition
(
int
blkGrpWidth
,
HashMap
<
Integer
,
List
<
String
>>
decommissionedNodes
,
List
<
LocatedBlock
>
lbs
)
{
for
(
int
i
=
0
;
i
<
lbs
.
size
(
)
;
i
++
)
{
LocatedBlock
blk
=
lbs
.
get
(
i
)
;
DatanodeInfo
[
]
nodes
=
blk
.
getLocations
(
)
;
List
<
String
>
decommissionedNodeList
=
decommissionedNodes
.
get
(
i
)
;
for
(
int
j
=
0
;
j
<
nodes
.
length
;
j
++
)
{
DatanodeInfo
dnInfo
=
nodes
[
j
]
;
@
Test
public
void
testRelativePathAsURI
(
)
throws
IOException
{
URI
u
=
Util
.
stringAsURI
(
RELATIVE_FILE_PATH
)
;
@
Test
public
void
testURI
(
)
throws
IOException
{
LOG
.
info
(
+
URI_UNIX
)
;
URI
u
=
Util
.
stringAsURI
(
URI_UNIX
)
;
@
Test
public
void
testURI
(
)
throws
IOException
{
LOG
.
info
(
+
URI_UNIX
)
;
URI
u
=
Util
.
stringAsURI
(
URI_UNIX
)
;
LOG
.
info
(
+
u
)
;
assertNotNull
(
,
u
)
;
assertEquals
(
URI_FILE_SCHEMA
,
u
.
getScheme
(
)
)
;
assertEquals
(
URI_PATH_UNIX
,
u
.
getPath
(
)
)
;
LOG
.
info
(
+
URI_WINDOWS
)
;
u
=
Util
.
stringAsURI
(
URI_WINDOWS
)
;
Mockito
.
when
(
request
.
getMethod
(
)
)
.
thenReturn
(
)
;
Mockito
.
when
(
request
.
getRequestURI
(
)
)
.
thenReturn
(
new
StringBuffer
(
WebHdfsFileSystem
.
PATH_PREFIX
+
)
.
toString
(
)
)
;
Mockito
.
when
(
request
.
getQueryString
(
)
)
.
thenReturn
(
null
)
;
Mockito
.
when
(
request
.
getRemoteAddr
(
)
)
.
thenReturn
(
)
;
HttpServletResponse
response
=
Mockito
.
mock
(
HttpServletResponse
.
class
)
;
FilterChain
chain
=
new
FilterChain
(
)
{
@
Override
public
void
doFilter
(
ServletRequest
servletRequest
,
ServletResponse
servletResponse
)
throws
IOException
,
ServletException
{
}
}
;
Filter
filter
=
new
HostRestrictingAuthorizationFilter
(
)
;
HashMap
<
String
,
String
>
configs
=
new
HashMap
<
String
,
String
>
(
)
{
}
;
configs
.
put
(
AuthenticationFilter
.
AUTH_TYPE
,
)
;
FilterConfig
fc
=
new
DummyFilterConfig
(
configs
)
;
filter
.
init
(
fc
)
;
filter
.
doFilter
(
request
,
response
,
chain
)
;
boolean
corruptedGs
=
false
;
boolean
corruptedLen
=
false
;
int
reportIndex
=
0
;
for
(
Map
.
Entry
<
DatanodeStorage
,
BlockListAsLongs
>
kvPair
:
perVolumeBlockLists
.
entrySet
(
)
)
{
DatanodeStorage
dnStorage
=
kvPair
.
getKey
(
)
;
BlockListAsLongs
blockList
=
kvPair
.
getValue
(
)
;
BlockListAsLongs
.
Builder
builder
=
BlockListAsLongs
.
builder
(
)
;
for
(
BlockReportReplica
block
:
blockList
)
{
if
(
corruptOneBlockGs
&&
!
corruptedGs
)
{
long
gsOld
=
block
.
getGenerationStamp
(
)
;
long
gsNew
;
do
{
gsNew
=
rand
.
nextInt
(
)
;
}
while
(
gsNew
==
gsOld
)
;
block
.
setGenerationStamp
(
gsNew
)
;
long
gsOld
=
block
.
getGenerationStamp
(
)
;
long
gsNew
;
do
{
gsNew
=
rand
.
nextInt
(
)
;
}
while
(
gsNew
==
gsOld
)
;
block
.
setGenerationStamp
(
gsNew
)
;
LOG
.
info
(
+
block
)
;
corruptedGs
=
true
;
}
else
if
(
corruptOneBlockLen
&&
!
corruptedLen
)
{
long
lenOld
=
block
.
getNumBytes
(
)
;
long
lenNew
;
do
{
lenNew
=
rand
.
nextInt
(
(
int
)
lenOld
-
1
)
;
}
while
(
lenNew
==
lenOld
)
;
block
.
setNumBytes
(
lenNew
)
;
@
Test
(
timeout
=
300000
)
public
void
blockReport_01
(
)
throws
IOException
{
final
String
METHOD_NAME
=
GenericTestUtils
.
getMethodName
(
)
;
Path
filePath
=
new
Path
(
+
METHOD_NAME
+
)
;
ArrayList
<
Block
>
blocks
=
prepareForRide
(
filePath
,
METHOD_NAME
,
FILE_SIZE
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
ArrayList
<
Block
>
blocks
=
prepareForRide
(
filePath
,
METHOD_NAME
,
FILE_SIZE
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
blocks
.
size
(
)
)
;
}
long
[
]
oldLengths
=
new
long
[
blocks
.
size
(
)
]
;
int
tempLen
;
for
(
int
i
=
0
;
i
<
blocks
.
size
(
)
;
i
++
)
{
Block
b
=
blocks
.
get
(
i
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
b
.
getBlockName
(
)
+
+
+
b
.
getNumBytes
(
)
)
;
}
oldLengths
[
i
]
=
b
.
getNumBytes
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
}
tempLen
=
rand
.
nextInt
(
BLOCK_SIZE
)
;
b
.
set
(
b
.
getBlockId
(
)
,
tempLen
,
b
.
getGenerationStamp
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
b
.
getBlockName
(
)
+
+
+
b
.
getNumBytes
(
)
)
;
}
oldLengths
[
i
]
=
b
.
getNumBytes
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
}
tempLen
=
rand
.
nextInt
(
BLOCK_SIZE
)
;
b
.
set
(
b
.
getBlockId
(
)
,
tempLen
,
b
.
getGenerationStamp
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
b
.
getBlockName
(
)
+
+
+
b
.
getNumBytes
(
)
)
;
}
}
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
DN_N0
)
;
String
poolId
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
DatanodeRegistration
dnR
=
dn
.
getDNRegistrationForBP
(
poolId
)
;
StorageBlockReport
[
]
reports
=
getBlockReports
(
dn
,
poolId
,
false
,
false
)
;
sendBlockReports
(
dnR
,
poolId
,
reports
)
;
List
<
LocatedBlock
>
blocksAfterReport
=
DFSTestUtil
.
getAllBlocks
(
fs
.
open
(
filePath
)
)
;
@
Test
(
timeout
=
300000
)
public
void
blockReport_02
(
)
throws
IOException
{
final
String
METHOD_NAME
=
GenericTestUtils
.
getMethodName
(
)
;
final
String
METHOD_NAME
=
GenericTestUtils
.
getMethodName
(
)
;
LOG
.
info
(
+
METHOD_NAME
)
;
Path
filePath
=
new
Path
(
+
METHOD_NAME
+
)
;
DFSTestUtil
.
createFile
(
fs
,
filePath
,
FILE_SIZE
,
REPL_FACTOR
,
rand
.
nextLong
(
)
)
;
File
dataDir
=
new
File
(
cluster
.
getDataDirectory
(
)
)
;
assertTrue
(
dataDir
.
isDirectory
(
)
)
;
List
<
ExtendedBlock
>
blocks2Remove
=
new
ArrayList
<
ExtendedBlock
>
(
)
;
List
<
Integer
>
removedIndex
=
new
ArrayList
<
Integer
>
(
)
;
List
<
LocatedBlock
>
lBlocks
=
cluster
.
getNameNodeRpc
(
)
.
getBlockLocations
(
filePath
.
toString
(
)
,
FILE_START
,
FILE_SIZE
)
.
getLocatedBlocks
(
)
;
while
(
removedIndex
.
size
(
)
!=
2
)
{
int
newRemoveIndex
=
rand
.
nextInt
(
lBlocks
.
size
(
)
)
;
if
(
!
removedIndex
.
contains
(
newRemoveIndex
)
)
removedIndex
.
add
(
newRemoveIndex
)
;
}
for
(
Integer
aRemovedIndex
:
removedIndex
)
{
blocks2Remove
.
add
(
lBlocks
.
get
(
aRemovedIndex
)
.
getBlock
(
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
File
dataDir
=
new
File
(
cluster
.
getDataDirectory
(
)
)
;
assertTrue
(
dataDir
.
isDirectory
(
)
)
;
List
<
ExtendedBlock
>
blocks2Remove
=
new
ArrayList
<
ExtendedBlock
>
(
)
;
List
<
Integer
>
removedIndex
=
new
ArrayList
<
Integer
>
(
)
;
List
<
LocatedBlock
>
lBlocks
=
cluster
.
getNameNodeRpc
(
)
.
getBlockLocations
(
filePath
.
toString
(
)
,
FILE_START
,
FILE_SIZE
)
.
getLocatedBlocks
(
)
;
while
(
removedIndex
.
size
(
)
!=
2
)
{
int
newRemoveIndex
=
rand
.
nextInt
(
lBlocks
.
size
(
)
)
;
if
(
!
removedIndex
.
contains
(
newRemoveIndex
)
)
removedIndex
.
add
(
newRemoveIndex
)
;
}
for
(
Integer
aRemovedIndex
:
removedIndex
)
{
blocks2Remove
.
add
(
lBlocks
.
get
(
aRemovedIndex
)
.
getBlock
(
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
lBlocks
.
size
(
)
)
;
}
final
DataNode
dn0
=
cluster
.
getDataNodes
(
)
.
get
(
DN_N0
)
;
for
(
ExtendedBlock
b
:
blocks2Remove
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
int
newRemoveIndex
=
rand
.
nextInt
(
lBlocks
.
size
(
)
)
;
if
(
!
removedIndex
.
contains
(
newRemoveIndex
)
)
removedIndex
.
add
(
newRemoveIndex
)
;
}
for
(
Integer
aRemovedIndex
:
removedIndex
)
{
blocks2Remove
.
add
(
lBlocks
.
get
(
aRemovedIndex
)
.
getBlock
(
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
lBlocks
.
size
(
)
)
;
}
final
DataNode
dn0
=
cluster
.
getDataNodes
(
)
.
get
(
DN_N0
)
;
for
(
ExtendedBlock
b
:
blocks2Remove
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
b
.
getBlockName
(
)
)
;
}
for
(
File
f
:
findAllFiles
(
dataDir
,
new
MyFileFilter
(
b
.
getBlockName
(
)
,
true
)
)
)
{
DataNodeTestUtils
.
getFSDataset
(
dn0
)
.
unfinalizeBlock
(
b
)
;
if
(
!
f
.
delete
(
)
)
{
LOG
.
warn
(
+
b
.
getBlockName
(
)
)
;
}
else
{
@
Test
(
timeout
=
300000
)
public
void
testInterleavedBlockReports
(
)
throws
IOException
,
ExecutionException
,
InterruptedException
{
int
numConcurrentBlockReports
=
3
;
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
DN_N0
)
;
final
String
poolId
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
private
void
waitForTempReplica
(
Block
bl
,
int
DN_N1
)
throws
IOException
{
final
boolean
tooLongWait
=
false
;
final
int
TIMEOUT
=
40000
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
while
(
cluster
.
getDataNodes
(
)
.
size
(
)
<=
DN_N1
)
{
waitTil
(
20
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
cluster
.
getDataNodes
(
)
.
size
(
)
)
;
}
cluster
.
waitActive
(
)
;
final
DataNode
dn1
=
cluster
.
getDataNodes
(
)
.
get
(
DN_N1
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
Replica
r
=
DataNodeTestUtils
.
fetchReplicaInfo
(
dn1
,
bpid
,
bl
.
getBlockId
(
)
)
;
long
start
=
Time
.
monotonicNow
(
)
;
int
count
=
0
;
while
(
r
==
null
)
{
waitTil
(
5
)
;
r
=
DataNodeTestUtils
.
fetchReplicaInfo
(
dn1
,
bpid
,
bl
.
getBlockId
(
)
)
;
long
waiting_period
=
Time
.
monotonicNow
(
)
-
start
;
if
(
count
++
%
100
==
0
)
if
(
LOG
.
isDebugEnabled
(
)
)
{
Replica
r
=
DataNodeTestUtils
.
fetchReplicaInfo
(
dn1
,
bpid
,
bl
.
getBlockId
(
)
)
;
long
start
=
Time
.
monotonicNow
(
)
;
int
count
=
0
;
while
(
r
==
null
)
{
waitTil
(
5
)
;
r
=
DataNodeTestUtils
.
fetchReplicaInfo
(
dn1
,
bpid
,
bl
.
getBlockId
(
)
)
;
long
waiting_period
=
Time
.
monotonicNow
(
)
-
start
;
if
(
count
++
%
100
==
0
)
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
waiting_period
+
)
;
}
if
(
waiting_period
>
TIMEOUT
)
assertTrue
(
,
tooLongWait
)
;
}
HdfsServerConstants
.
ReplicaState
state
=
r
.
getState
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
state
.
getValue
(
)
)
;
}
start
=
Time
.
monotonicNow
(
)
;
while
(
state
!=
HdfsServerConstants
.
ReplicaState
.
TEMPORARY
)
{
while
(
r
==
null
)
{
waitTil
(
5
)
;
r
=
DataNodeTestUtils
.
fetchReplicaInfo
(
dn1
,
bpid
,
bl
.
getBlockId
(
)
)
;
long
waiting_period
=
Time
.
monotonicNow
(
)
-
start
;
if
(
count
++
%
100
==
0
)
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
waiting_period
+
)
;
}
if
(
waiting_period
>
TIMEOUT
)
assertTrue
(
,
tooLongWait
)
;
}
HdfsServerConstants
.
ReplicaState
state
=
r
.
getState
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
state
.
getValue
(
)
)
;
}
start
=
Time
.
monotonicNow
(
)
;
while
(
state
!=
HdfsServerConstants
.
ReplicaState
.
TEMPORARY
)
{
waitTil
(
5
)
;
state
=
r
.
getState
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
ArrayList
<
Block
>
prepareForRide
(
final
Path
filePath
,
final
String
METHOD_NAME
,
long
fileSize
)
throws
IOException
{
private
ArrayList
<
Block
>
locatedToBlocks
(
final
List
<
LocatedBlock
>
locatedBlks
,
List
<
Integer
>
positionsToRemove
)
{
ArrayList
<
Block
>
newList
=
new
ArrayList
<
Block
>
(
)
;
for
(
int
i
=
0
;
i
<
locatedBlks
.
size
(
)
;
i
++
)
{
if
(
positionsToRemove
!=
null
&&
positionsToRemove
.
contains
(
i
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Test
(
timeout
=
15000
)
public
void
testRefreshLeaseId
(
)
throws
Exception
{
Mockito
.
when
(
mockNN1
.
sendHeartbeat
(
Mockito
.
any
(
DatanodeRegistration
.
class
)
,
Mockito
.
any
(
StorageReport
[
]
.
class
)
,
Mockito
.
anyLong
(
)
,
Mockito
.
anyLong
(
)
,
Mockito
.
anyInt
(
)
,
Mockito
.
anyInt
(
)
,
Mockito
.
anyInt
(
)
,
Mockito
.
any
(
VolumeFailureSummary
.
class
)
,
Mockito
.
anyBoolean
(
)
,
Mockito
.
any
(
SlowPeerReports
.
class
)
,
Mockito
.
any
(
SlowDiskReports
.
class
)
)
)
.
thenAnswer
(
new
HeartbeatAnswer
(
0
)
)
.
thenAnswer
(
new
HeartbeatRegisterAnswer
(
0
)
)
.
thenAnswer
(
new
HeartbeatAnswer
(
0
)
)
;
Mockito
.
when
(
mockNN1
.
blockReport
(
Mockito
.
any
(
DatanodeRegistration
.
class
)
,
Mockito
.
anyString
(
)
,
Mockito
.
any
(
StorageBlockReport
[
]
.
class
)
,
Mockito
.
any
(
BlockReportContext
.
class
)
)
)
.
thenAnswer
(
new
Answer
(
)
{
@
Override
public
Object
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
BlockReportContext
context
=
(
BlockReportContext
)
invocation
.
getArguments
(
)
[
3
]
;
long
leaseId
=
context
.
getLeaseId
(
)
;
}
}
)
;
}
final
ExecutorCompletionService
<
Boolean
>
verifyService
=
new
ExecutorCompletionService
<
>
(
executor
)
;
final
AtomicLong
verifyFileTime
=
new
AtomicLong
(
)
;
for
(
int
i
=
0
;
i
<
NUM_FILES
;
i
++
)
{
final
Path
file
=
createService
.
take
(
)
.
get
(
)
;
verifyService
.
submit
(
new
Callable
<
Boolean
>
(
)
{
@
Override
public
Boolean
call
(
)
throws
Exception
{
final
long
start
=
Time
.
monotonicNow
(
)
;
try
{
return
verifyFile
(
file
,
dfs
)
;
}
finally
{
verifyFileTime
.
addAndGet
(
Time
.
monotonicNow
(
)
-
start
)
;
}
}
}
)
;
}
for
(
int
i
=
0
;
i
<
NUM_FILES
;
i
++
)
{
Assert
.
assertTrue
(
verifyService
.
take
(
)
.
get
(
)
)
;
}
)
;
}
final
ExecutorCompletionService
<
Boolean
>
verifyService
=
new
ExecutorCompletionService
<
>
(
executor
)
;
final
AtomicLong
verifyFileTime
=
new
AtomicLong
(
)
;
for
(
int
i
=
0
;
i
<
NUM_FILES
;
i
++
)
{
final
Path
file
=
createService
.
take
(
)
.
get
(
)
;
verifyService
.
submit
(
new
Callable
<
Boolean
>
(
)
{
@
Override
public
Boolean
call
(
)
throws
Exception
{
final
long
start
=
Time
.
monotonicNow
(
)
;
try
{
return
verifyFile
(
file
,
dfs
)
;
}
finally
{
verifyFileTime
.
addAndGet
(
Time
.
monotonicNow
(
)
-
start
)
;
}
}
}
)
;
}
for
(
int
i
=
0
;
i
<
NUM_FILES
;
i
++
)
{
Assert
.
assertTrue
(
verifyService
.
take
(
)
.
get
(
)
)
;
static
void
logIbrCounts
(
List
<
DataNode
>
datanodes
)
{
final
String
name
=
;
for
(
DataNode
dn
:
datanodes
)
{
final
MetricsRecordBuilder
m
=
MetricsAsserts
.
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
final
long
ibr
=
MetricsAsserts
.
getLongCounter
(
name
,
m
)
;
final
long
seed
;
final
int
numBlocks
;
{
final
String
name
=
f
.
getName
(
)
;
final
int
i
=
name
.
indexOf
(
'_'
)
;
seed
=
Long
.
parseLong
(
name
.
substring
(
0
,
i
)
)
;
numBlocks
=
Integer
.
parseInt
(
name
.
substring
(
i
+
1
)
)
;
}
final
byte
[
]
computed
=
IO_BUF
.
get
(
)
;
final
byte
[
]
expected
=
VERIFY_BUF
.
get
(
)
;
try
(
FSDataInputStream
in
=
dfs
.
open
(
f
)
)
{
for
(
int
i
=
0
;
i
<
numBlocks
;
i
++
)
{
in
.
read
(
computed
)
;
nextBytes
(
i
,
seed
,
expected
)
;
Assert
.
assertArrayEquals
(
expected
,
computed
)
;
}
return
true
;
}
catch
(
Exception
e
)
{
private
void
doLog
(
String
string
)
{
synchronized
(
log
)
{
public
void
getTrashDirectoryForBlockFile
(
String
fileName
,
int
nestingLevel
)
{
final
String
blockFileSubdir
=
makeRandomBlockFileSubdir
(
nestingLevel
)
;
final
String
blockFileName
=
fileName
;
String
testFilePath
=
storage
.
getSingularStorageDir
(
)
.
getRoot
(
)
+
File
.
separator
+
Storage
.
STORAGE_DIR_CURRENT
+
blockFileSubdir
+
blockFileName
;
String
expectedTrashPath
=
storage
.
getSingularStorageDir
(
)
.
getRoot
(
)
+
File
.
separator
+
BlockPoolSliceStorage
.
TRASH_ROOT_DIR
+
blockFileSubdir
.
substring
(
0
,
blockFileSubdir
.
length
(
)
-
1
)
;
public
void
getTrashDirectoryForBlockFile
(
String
fileName
,
int
nestingLevel
)
{
final
String
blockFileSubdir
=
makeRandomBlockFileSubdir
(
nestingLevel
)
;
final
String
blockFileName
=
fileName
;
String
testFilePath
=
storage
.
getSingularStorageDir
(
)
.
getRoot
(
)
+
File
.
separator
+
Storage
.
STORAGE_DIR_CURRENT
+
blockFileSubdir
+
blockFileName
;
String
expectedTrashPath
=
storage
.
getSingularStorageDir
(
)
.
getRoot
(
)
+
File
.
separator
+
BlockPoolSliceStorage
.
TRASH_ROOT_DIR
+
blockFileSubdir
.
substring
(
0
,
blockFileSubdir
.
length
(
)
-
1
)
;
LOG
.
info
(
,
blockFileSubdir
)
;
public
void
getRestoreDirectoryForBlockFile
(
String
fileName
,
int
nestingLevel
)
{
BlockPoolSliceStorage
storage
=
makeBlockPoolStorage
(
)
;
final
String
blockFileSubdir
=
makeRandomBlockFileSubdir
(
nestingLevel
)
;
final
String
blockFileName
=
fileName
;
String
deletedFilePath
=
storage
.
getSingularStorageDir
(
)
.
getRoot
(
)
+
File
.
separator
+
BlockPoolSliceStorage
.
TRASH_ROOT_DIR
+
blockFileSubdir
+
blockFileName
;
String
expectedRestorePath
=
storage
.
getSingularStorageDir
(
)
.
getRoot
(
)
+
File
.
separator
+
Storage
.
STORAGE_DIR_CURRENT
+
blockFileSubdir
.
substring
(
0
,
blockFileSubdir
.
length
(
)
-
1
)
;
final
AtomicReference
<
String
>
failure
=
new
AtomicReference
<
String
>
(
null
)
;
Collection
<
RecoveringBlock
>
recoveringBlocks
=
initRecoveringBlocks
(
)
;
final
RecoveringBlock
recoveringBlock
=
Iterators
.
get
(
recoveringBlocks
.
iterator
(
)
,
0
)
;
final
ExtendedBlock
block
=
recoveringBlock
.
getBlock
(
)
;
Thread
slowWriterThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
LOG
.
debug
(
)
;
ReplicaHandler
replicaHandler
=
spyDN
.
data
.
createRbw
(
StorageType
.
DISK
,
null
,
block
,
false
)
;
replicaHandler
.
close
(
)
;
LOG
.
debug
(
)
;
progressParent
.
sem
.
release
(
)
;
terminateSlowWriter
.
uninterruptiblyAcquire
(
60000
)
;
LOG
.
debug
(
)
;
}
catch
(
Throwable
t
)
{
LOG
.
debug
(
)
;
progressParent
.
sem
.
release
(
)
;
terminateSlowWriter
.
uninterruptiblyAcquire
(
60000
)
;
LOG
.
debug
(
)
;
}
catch
(
Throwable
t
)
{
LOG
.
error
(
,
t
)
;
failure
.
compareAndSet
(
null
,
+
t
.
getMessage
(
)
)
;
}
}
}
)
;
slowWriterThread
.
start
(
)
;
progressParent
.
uninterruptiblyAcquire
(
60000
)
;
Thread
stopWriterThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
LOG
.
debug
(
+
tswr
.
opName
(
)
)
;
tswr
.
run
(
recoveringBlock
)
;
terminateSlowWriter
.
uninterruptiblyAcquire
(
60000
)
;
LOG
.
debug
(
)
;
}
catch
(
Throwable
t
)
{
LOG
.
error
(
,
t
)
;
failure
.
compareAndSet
(
null
,
+
t
.
getMessage
(
)
)
;
}
}
}
)
;
slowWriterThread
.
start
(
)
;
progressParent
.
uninterruptiblyAcquire
(
60000
)
;
Thread
stopWriterThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
LOG
.
debug
(
+
tswr
.
opName
(
)
)
;
tswr
.
run
(
recoveringBlock
)
;
LOG
.
debug
(
+
tswr
.
opName
(
)
)
;
}
catch
(
Throwable
t
)
{
isNewNode
=
false
;
break
;
}
}
if
(
isNewNode
)
{
newNode
=
node
;
break
;
}
}
assertTrue
(
newNode
!=
null
)
;
DatanodeInfo
source
=
null
;
ArrayList
<
DatanodeInfo
>
proxies
=
new
ArrayList
<
DatanodeInfo
>
(
2
)
;
for
(
DatanodeInfo
node
:
datanodes
)
{
if
(
node
!=
newNode
)
{
if
(
node
.
getNetworkLocation
(
)
.
equals
(
newNode
.
getNetworkLocation
(
)
)
)
{
source
=
node
;
}
else
{
proxies
.
add
(
node
)
;
}
}
}
assertTrue
(
source
!=
null
&&
proxies
.
size
(
)
==
2
)
;
}
}
if
(
isNewNode
)
{
newNode
=
node
;
break
;
}
}
assertTrue
(
newNode
!=
null
)
;
DatanodeInfo
source
=
null
;
ArrayList
<
DatanodeInfo
>
proxies
=
new
ArrayList
<
DatanodeInfo
>
(
2
)
;
for
(
DatanodeInfo
node
:
datanodes
)
{
if
(
node
!=
newNode
)
{
if
(
node
.
getNetworkLocation
(
)
.
equals
(
newNode
.
getNetworkLocation
(
)
)
)
{
source
=
node
;
}
else
{
proxies
.
add
(
node
)
;
}
}
}
assertTrue
(
source
!=
null
&&
proxies
.
size
(
)
==
2
)
;
LOG
.
info
(
+
newNode
+
+
b
)
;
assertFalse
(
replaceBlock
(
b
,
source
,
newNode
,
proxies
.
get
(
0
)
)
)
;
if
(
isNewNode
)
{
newNode
=
node
;
break
;
}
}
assertTrue
(
newNode
!=
null
)
;
DatanodeInfo
source
=
null
;
ArrayList
<
DatanodeInfo
>
proxies
=
new
ArrayList
<
DatanodeInfo
>
(
2
)
;
for
(
DatanodeInfo
node
:
datanodes
)
{
if
(
node
!=
newNode
)
{
if
(
node
.
getNetworkLocation
(
)
.
equals
(
newNode
.
getNetworkLocation
(
)
)
)
{
source
=
node
;
}
else
{
proxies
.
add
(
node
)
;
}
}
}
assertTrue
(
source
!=
null
&&
proxies
.
size
(
)
==
2
)
;
LOG
.
info
(
+
newNode
+
+
b
)
;
assertFalse
(
replaceBlock
(
b
,
source
,
newNode
,
proxies
.
get
(
0
)
)
)
;
}
}
assertTrue
(
newNode
!=
null
)
;
DatanodeInfo
source
=
null
;
ArrayList
<
DatanodeInfo
>
proxies
=
new
ArrayList
<
DatanodeInfo
>
(
2
)
;
for
(
DatanodeInfo
node
:
datanodes
)
{
if
(
node
!=
newNode
)
{
if
(
node
.
getNetworkLocation
(
)
.
equals
(
newNode
.
getNetworkLocation
(
)
)
)
{
source
=
node
;
}
else
{
proxies
.
add
(
node
)
;
}
}
}
assertTrue
(
source
!=
null
&&
proxies
.
size
(
)
==
2
)
;
LOG
.
info
(
+
newNode
+
+
b
)
;
assertFalse
(
replaceBlock
(
b
,
source
,
newNode
,
proxies
.
get
(
0
)
)
)
;
LOG
.
info
(
+
proxies
.
get
(
1
)
+
+
b
)
;
assertFalse
(
replaceBlock
(
b
,
source
,
proxies
.
get
(
0
)
,
proxies
.
get
(
1
)
)
)
;
LOG
.
info
(
+
source
+
+
proxies
.
get
(
0
)
+
+
newNode
)
;
LocatedBlock
lb
=
dfs
.
getClient
(
)
.
getLocatedBlocks
(
fileName
,
0
)
.
get
(
0
)
;
DatanodeInfo
[
]
oldNodes
=
lb
.
getLocations
(
)
;
assertEquals
(
,
oldNodes
.
length
,
1
)
;
DatanodeInfo
source
=
oldNodes
[
0
]
;
ExtendedBlock
b
=
lb
.
getBlock
(
)
;
DatanodeInfo
[
]
datanodes
=
dfs
.
getDataNodeStats
(
)
;
DatanodeInfo
destin
=
null
;
for
(
DatanodeInfo
datanodeInfo
:
datanodes
)
{
if
(
!
oldNodes
[
0
]
.
equals
(
datanodeInfo
)
)
{
destin
=
datanodeInfo
;
break
;
}
}
assertNotNull
(
,
destin
)
;
assertFalse
(
,
source
.
equals
(
destin
)
)
;
for
(
int
i
=
0
;
i
<
cluster
.
getDataNodes
(
)
.
size
(
)
;
i
++
)
{
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
i
)
;
do
{
try
{
Thread
.
sleep
(
100
)
;
}
catch
(
InterruptedException
e
)
{
}
List
<
LocatedBlock
>
blocks
=
client
.
getNamenode
(
)
.
getBlockLocations
(
fileName
,
0
,
fileLen
)
.
getLocatedBlocks
(
)
;
assertEquals
(
1
,
blocks
.
size
(
)
)
;
DatanodeInfo
[
]
nodes
=
blocks
.
get
(
0
)
.
getLocations
(
)
;
notDone
=
(
nodes
.
length
!=
replFactor
)
;
if
(
notDone
)
{
LOG
.
info
(
+
replFactor
+
+
nodes
.
length
)
;
}
else
{
List
<
DatanodeInfo
>
nodeLocations
=
Arrays
.
asList
(
nodes
)
;
for
(
DatanodeInfo
node
:
includeNodes
)
{
if
(
!
nodeLocations
.
contains
(
node
)
)
{
notDone
=
true
;
assertEquals
(
1
,
blocks
.
size
(
)
)
;
DatanodeInfo
[
]
nodes
=
blocks
.
get
(
0
)
.
getLocations
(
)
;
notDone
=
(
nodes
.
length
!=
replFactor
)
;
if
(
notDone
)
{
LOG
.
info
(
+
replFactor
+
+
nodes
.
length
)
;
}
else
{
List
<
DatanodeInfo
>
nodeLocations
=
Arrays
.
asList
(
nodes
)
;
for
(
DatanodeInfo
node
:
includeNodes
)
{
if
(
!
nodeLocations
.
contains
(
node
)
)
{
notDone
=
true
;
LOG
.
info
(
+
node
)
;
break
;
}
}
}
if
(
Time
.
monotonicNow
(
)
>
failtime
)
{
String
expectedNodesList
=
;
String
currentNodesList
=
;
DatanodeInfo
[
]
nodes
=
blocks
.
get
(
0
)
.
getLocations
(
)
;
notDone
=
(
nodes
.
length
!=
replFactor
)
;
if
(
notDone
)
{
LOG
.
info
(
+
replFactor
+
+
nodes
.
length
)
;
}
else
{
List
<
DatanodeInfo
>
nodeLocations
=
Arrays
.
asList
(
nodes
)
;
for
(
DatanodeInfo
node
:
includeNodes
)
{
if
(
!
nodeLocations
.
contains
(
node
)
)
{
notDone
=
true
;
LOG
.
info
(
+
node
)
;
break
;
}
}
}
if
(
Time
.
monotonicNow
(
)
>
failtime
)
{
String
expectedNodesList
=
;
String
currentNodesList
=
;
for
(
DatanodeInfo
dn
:
includeNodes
)
expectedNodesList
+=
dn
+
;
Path
fileName
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
fileName
,
10L
,
(
short
)
1
,
1234L
)
;
DFSTestUtil
.
waitReplication
(
fs
,
fileName
,
(
short
)
1
)
;
client
=
new
DFSClient
(
cluster
.
getFileSystem
(
0
)
.
getUri
(
)
,
conf
)
;
List
<
LocatedBlock
>
locatedBlocks
=
client
.
getNamenode
(
)
.
getBlockLocations
(
,
0
,
10L
)
.
getLocatedBlocks
(
)
;
assertTrue
(
locatedBlocks
.
size
(
)
==
1
)
;
assertTrue
(
locatedBlocks
.
get
(
0
)
.
getLocations
(
)
.
length
==
1
)
;
cluster
.
startDataNodes
(
conf
,
1
,
true
,
null
,
null
,
null
,
null
)
;
assertEquals
(
,
2
,
cluster
.
getDataNodes
(
)
.
size
(
)
)
;
DataNode
dn0
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
DataNode
dn1
=
cluster
.
getDataNodes
(
)
.
get
(
1
)
;
String
activeNNBPId
=
cluster
.
getNamesystem
(
0
)
.
getBlockPoolId
(
)
;
DatanodeDescriptor
sourceDnDesc
=
NameNodeAdapter
.
getDatanode
(
cluster
.
getNamesystem
(
0
)
,
dn0
.
getDNRegistrationForBP
(
activeNNBPId
)
)
;
DatanodeDescriptor
destDnDesc
=
NameNodeAdapter
.
getDatanode
(
cluster
.
getNamesystem
(
0
)
,
dn1
.
getDNRegistrationForBP
(
activeNNBPId
)
)
;
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
try
{
List
<
BPOfferService
>
bpos
=
ctx
.
datanode
.
getAllBpOs
(
)
;
assertEquals
(
1
,
bpos
.
size
(
)
)
;
BlockIterator
iter
=
volume
.
newBlockIterator
(
ctx
.
bpids
[
0
]
,
)
;
assertEquals
(
ctx
.
bpids
[
0
]
,
iter
.
getBlockPoolId
(
)
)
;
iter
.
setMaxStalenessMs
(
maxStaleness
)
;
while
(
true
)
{
HashSet
<
ExtendedBlock
>
blocks
=
new
HashSet
<
ExtendedBlock
>
(
)
;
for
(
int
blockIdx
=
0
;
blockIdx
<
numFiles
;
blockIdx
++
)
{
blocks
.
add
(
ctx
.
getFileBlock
(
0
,
blockIdx
)
)
;
}
while
(
true
)
{
ExtendedBlock
block
=
iter
.
nextBlock
(
)
;
if
(
block
==
null
)
{
break
;
}
blocksProcessed
++
;
}
while
(
true
)
{
ExtendedBlock
block
=
iter
.
nextBlock
(
)
;
if
(
block
==
null
)
{
break
;
}
blocksProcessed
++
;
LOG
.
info
(
,
volume
,
block
,
blocksProcessed
)
;
if
(
testedSave
&&
(
savedBlock
==
null
)
)
{
savedBlock
=
block
;
}
if
(
testedLoad
&&
(
loadedBlock
==
null
)
)
{
loadedBlock
=
block
;
assertEquals
(
savedBlock
,
loadedBlock
)
;
}
boolean
blockRemoved
=
blocks
.
remove
(
block
)
;
assertTrue
(
+
block
,
blockRemoved
)
;
if
(
blocksProcessed
>
(
numFiles
/
3
)
)
{
if
(
!
testedSave
)
{
blocksProcessed
++
;
LOG
.
info
(
,
volume
,
block
,
blocksProcessed
)
;
if
(
testedSave
&&
(
savedBlock
==
null
)
)
{
savedBlock
=
block
;
}
if
(
testedLoad
&&
(
loadedBlock
==
null
)
)
{
loadedBlock
=
block
;
assertEquals
(
savedBlock
,
loadedBlock
)
;
}
boolean
blockRemoved
=
blocks
.
remove
(
block
)
;
assertTrue
(
+
block
,
blockRemoved
)
;
if
(
blocksProcessed
>
(
numFiles
/
3
)
)
{
if
(
!
testedSave
)
{
LOG
.
info
(
,
blocksProcessed
,
numFiles
)
;
iter
.
save
(
)
;
testedSave
=
true
;
savedBlocksProcessed
=
blocksProcessed
;
if
(
testedLoad
&&
(
loadedBlock
==
null
)
)
{
loadedBlock
=
block
;
assertEquals
(
savedBlock
,
loadedBlock
)
;
}
boolean
blockRemoved
=
blocks
.
remove
(
block
)
;
assertTrue
(
+
block
,
blockRemoved
)
;
if
(
blocksProcessed
>
(
numFiles
/
3
)
)
{
if
(
!
testedSave
)
{
LOG
.
info
(
,
blocksProcessed
,
numFiles
)
;
iter
.
save
(
)
;
testedSave
=
true
;
savedBlocksProcessed
=
blocksProcessed
;
}
}
if
(
blocksProcessed
>
(
numFiles
/
2
)
)
{
if
(
!
testedRewind
)
{
LOG
.
info
(
,
blocksProcessed
,
numFiles
)
;
iter
.
rewind
(
)
;
}
TestScanResultHandler
.
Info
info
=
TestScanResultHandler
.
getInfo
(
ctx
.
volumes
.
get
(
0
)
)
;
synchronized
(
info
)
{
info
.
shouldRun
=
true
;
info
.
notify
(
)
;
}
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
TestScanResultHandler
.
Info
info
=
TestScanResultHandler
.
getInfo
(
ctx
.
volumes
.
get
(
0
)
)
;
int
numFoundBlocks
=
0
;
StringBuilder
foundBlocksBld
=
new
StringBuilder
(
)
;
String
prefix
=
;
synchronized
(
info
)
{
for
(
ExtendedBlock
block
:
info
.
goodBlocks
)
{
assertTrue
(
expectedBlocks
.
contains
(
block
)
)
;
numFoundBlocks
++
;
foundBlocksBld
.
append
(
prefix
)
.
append
(
block
)
;
assertEquals
(
5
,
info
.
blocksScanned
)
;
info
.
shouldRun
=
false
;
}
ctx
.
datanode
.
shutdown
(
)
;
URI
vURI
=
ctx
.
volumes
.
get
(
0
)
.
getStorageLocation
(
)
.
getUri
(
)
;
File
cursorPath
=
new
File
(
new
File
(
new
File
(
new
File
(
vURI
)
,
)
,
ctx
.
bpids
[
0
]
)
,
)
;
assertTrue
(
+
cursorPath
.
getAbsolutePath
(
)
,
cursorPath
.
exists
(
)
)
;
Set
<
ExtendedBlock
>
prevGoodBlocks
=
new
HashSet
<
ExtendedBlock
>
(
)
;
synchronized
(
info
)
{
info
.
sem
=
new
Semaphore
(
4
)
;
prevGoodBlocks
.
addAll
(
info
.
goodBlocks
)
;
info
.
goodBlocks
.
clear
(
)
;
}
ctx
.
cluster
.
restartDataNode
(
0
)
;
synchronized
(
info
)
{
info
.
shouldRun
=
true
;
info
.
notify
(
)
;
conf
.
setLong
(
DFS_DATANODE_SCAN_PERIOD_HOURS_KEY
,
100L
)
;
conf
.
set
(
INTERNAL_VOLUME_SCANNER_SCAN_RESULT_HANDLER
,
TestScanResultHandler
.
class
.
getName
(
)
)
;
final
TestContext
ctx
=
new
TestContext
(
conf
,
3
)
;
final
int
BYTES_SCANNED_PER_FILE
=
5
;
int
TOTAL_FILES
=
16
;
ctx
.
createFiles
(
0
,
TOTAL_FILES
,
1
)
;
final
TestScanResultHandler
.
Info
info
=
TestScanResultHandler
.
getInfo
(
ctx
.
volumes
.
get
(
0
)
)
;
synchronized
(
info
)
{
info
.
shouldRun
=
true
;
info
.
notify
(
)
;
}
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
synchronized
(
info
)
{
Statistics
stats
=
ctx
.
blockScanner
.
getVolumeStats
(
ctx
.
volumes
.
get
(
0
)
.
getStorageID
(
)
)
;
if
(
stats
.
scansSinceRestart
<
3
)
{
conf
.
setLong
(
INTERNAL_DFS_BLOCK_SCANNER_CURSOR_SAVE_INTERVAL_MS
,
0L
)
;
final
TestContext
ctx
=
new
TestContext
(
conf
,
1
)
;
final
int
NUM_EXPECTED_BLOCKS
=
10
;
ctx
.
createFiles
(
0
,
NUM_EXPECTED_BLOCKS
,
1
)
;
final
TestScanResultHandler
.
Info
info
=
TestScanResultHandler
.
getInfo
(
ctx
.
volumes
.
get
(
0
)
)
;
String
storageID
=
ctx
.
volumes
.
get
(
0
)
.
getStorageID
(
)
;
synchronized
(
info
)
{
info
.
sem
=
new
Semaphore
(
4
)
;
info
.
shouldRun
=
true
;
info
.
notify
(
)
;
}
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
synchronized
(
info
)
{
if
(
info
.
blocksScanned
>=
4
)
{
ctx
.
createFiles
(
0
,
NUM_EXPECTED_BLOCKS
,
1
)
;
final
TestScanResultHandler
.
Info
info
=
TestScanResultHandler
.
getInfo
(
ctx
.
volumes
.
get
(
0
)
)
;
String
storageID
=
ctx
.
volumes
.
get
(
0
)
.
getStorageID
(
)
;
synchronized
(
info
)
{
info
.
sem
=
new
Semaphore
(
4
)
;
info
.
shouldRun
=
true
;
info
.
notify
(
)
;
}
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
synchronized
(
info
)
{
if
(
info
.
blocksScanned
>=
4
)
{
LOG
.
info
(
,
info
)
;
return
true
;
}
else
{
LOG
.
info
(
,
info
)
;
return
false
;
}
}
}
}
,
50
,
30000
)
;
synchronized
(
info
)
{
assertEquals
(
,
4
,
info
.
goodBlocks
.
size
(
)
)
;
info
.
goodBlocks
.
clear
(
)
;
assertEquals
(
,
4
,
info
.
blocksScanned
)
;
assertEquals
(
,
0
,
info
.
badBlocks
.
size
(
)
)
;
info
.
blocksScanned
=
0
;
}
ExtendedBlock
first
=
ctx
.
getFileBlock
(
0
,
0
)
;
ctx
.
datanode
.
getBlockScanner
(
)
.
markSuspectBlock
(
storageID
,
first
)
;
info
.
sem
.
release
(
2
)
;
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
}
}
}
,
50
,
30000
)
;
synchronized
(
info
)
{
assertEquals
(
,
4
,
info
.
goodBlocks
.
size
(
)
)
;
info
.
goodBlocks
.
clear
(
)
;
assertEquals
(
,
4
,
info
.
blocksScanned
)
;
assertEquals
(
,
0
,
info
.
badBlocks
.
size
(
)
)
;
info
.
blocksScanned
=
0
;
}
ExtendedBlock
first
=
ctx
.
getFileBlock
(
0
,
0
)
;
ctx
.
datanode
.
getBlockScanner
(
)
.
markSuspectBlock
(
storageID
,
first
)
;
info
.
sem
.
release
(
2
)
;
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
synchronized
(
info
)
{
if
(
info
.
blocksScanned
>=
2
)
{
synchronized
(
info
)
{
if
(
info
.
blocksScanned
>=
2
)
{
LOG
.
info
(
,
info
)
;
return
true
;
}
else
{
LOG
.
info
(
,
info
)
;
return
false
;
}
}
}
}
,
50
,
30000
)
;
synchronized
(
info
)
{
assertTrue
(
+
first
+
,
info
.
goodBlocks
.
contains
(
first
)
)
;
assertEquals
(
2
,
info
.
goodBlocks
.
size
(
)
)
;
info
.
goodBlocks
.
clear
(
)
;
assertEquals
(
,
0
,
info
.
badBlocks
.
size
(
)
)
;
assertEquals
(
2
,
info
.
blocksScanned
)
;
info
.
blocksScanned
=
0
;
return
true
;
}
else
{
LOG
.
info
(
,
info
)
;
return
false
;
}
}
}
}
,
50
,
30000
)
;
synchronized
(
info
)
{
assertTrue
(
+
first
+
,
info
.
goodBlocks
.
contains
(
first
)
)
;
assertEquals
(
2
,
info
.
goodBlocks
.
size
(
)
)
;
info
.
goodBlocks
.
clear
(
)
;
assertEquals
(
,
0
,
info
.
badBlocks
.
size
(
)
)
;
assertEquals
(
2
,
info
.
blocksScanned
)
;
info
.
blocksScanned
=
0
;
}
ctx
.
datanode
.
getBlockScanner
(
)
.
markSuspectBlock
(
storageID
,
first
)
;
info
.
sem
.
release
(
10
)
;
LOG
.
info
(
)
;
ctx
.
createFiles
(
0
,
NUM_FILES
,
5
)
;
MaterializedReplica
unreachableReplica
=
ctx
.
getMaterializedReplica
(
0
,
1
)
;
ExtendedBlock
unreachableBlock
=
ctx
.
getFileBlock
(
0
,
1
)
;
unreachableReplica
.
makeUnreachable
(
)
;
final
TestScanResultHandler
.
Info
info
=
TestScanResultHandler
.
getInfo
(
ctx
.
volumes
.
get
(
0
)
)
;
String
storageID
=
ctx
.
volumes
.
get
(
0
)
.
getStorageID
(
)
;
synchronized
(
info
)
{
info
.
sem
=
new
Semaphore
(
NUM_FILES
)
;
info
.
shouldRun
=
true
;
info
.
notify
(
)
;
}
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
synchronized
(
info
)
{
if
(
info
.
blocksScanned
>=
NUM_FILES
-
1
)
{
unreachableReplica
.
makeUnreachable
(
)
;
final
TestScanResultHandler
.
Info
info
=
TestScanResultHandler
.
getInfo
(
ctx
.
volumes
.
get
(
0
)
)
;
String
storageID
=
ctx
.
volumes
.
get
(
0
)
.
getStorageID
(
)
;
synchronized
(
info
)
{
info
.
sem
=
new
Semaphore
(
NUM_FILES
)
;
info
.
shouldRun
=
true
;
info
.
notify
(
)
;
}
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
synchronized
(
info
)
{
if
(
info
.
blocksScanned
>=
NUM_FILES
-
1
)
{
LOG
.
info
(
+
info
.
blocksScanned
,
info
)
;
return
true
;
}
else
{
private
void
waitForRescan
(
final
TestScanResultHandler
.
Info
info
,
final
int
numExpectedBlocks
)
throws
TimeoutException
,
InterruptedException
{
LOG
.
info
(
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
synchronized
(
info
)
{
if
(
info
.
blocksScanned
>=
numExpectedBlocks
)
{
private
Scheduler
makeMockScheduler
(
long
now
)
{
FSDataInputStream
fis
=
null
;
long
totalRead
=
0
;
try
{
fis
=
fs
.
open
(
p
)
;
if
(
dropBehind
!=
null
)
{
fis
.
setDropBehind
(
dropBehind
)
;
}
byte
buf
[
]
=
new
byte
[
8196
]
;
while
(
length
>
0
)
{
int
amt
=
(
length
>
buf
.
length
)
?
buf
.
length
:
(
int
)
length
;
int
ret
=
fis
.
read
(
buf
,
0
,
amt
)
;
if
(
ret
==
-
1
)
{
return
totalRead
;
}
totalRead
+=
ret
;
length
-=
ret
;
}
}
catch
(
IOException
e
)
{
private
void
doTest
(
String
fileName
,
int
fileLen
,
int
deadNodeIndex
)
throws
Exception
{
assertTrue
(
fileLen
>
0
)
;
assertTrue
(
deadNodeIndex
>=
0
&&
deadNodeIndex
<
numDNs
)
;
Path
file
=
new
Path
(
fileName
)
;
final
byte
[
]
data
=
StripedFileTestUtil
.
generateBytes
(
fileLen
)
;
DFSTestUtil
.
writeFile
(
fs
,
file
,
data
)
;
StripedFileTestUtil
.
waitBlockGroupsReported
(
fs
,
fileName
)
;
final
LocatedBlocks
locatedBlocks
=
StripedFileTestUtil
.
getLocatedBlocks
(
file
,
fs
)
;
final
LocatedStripedBlock
lastBlock
=
(
LocatedStripedBlock
)
locatedBlocks
.
getLastLocatedBlock
(
)
;
assertTrue
(
lastBlock
.
getLocations
(
)
.
length
>
deadNodeIndex
)
;
final
DataNode
toCorruptDn
=
cluster
.
getDataNode
(
lastBlock
.
getLocations
(
)
[
deadNodeIndex
]
.
getIpcPort
(
)
)
;
dn
.
data
=
Mockito
.
spy
(
data
)
;
final
int
newVolumeCount
=
40
;
List
<
Thread
>
addVolumeDelayedThreads
=
Collections
.
synchronizedList
(
new
ArrayList
<
>
(
)
)
;
AtomicBoolean
addVolumeError
=
new
AtomicBoolean
(
false
)
;
AtomicBoolean
listStorageError
=
new
AtomicBoolean
(
false
)
;
CountDownLatch
addVolumeCompletionLatch
=
new
CountDownLatch
(
newVolumeCount
)
;
final
Thread
listStorageThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
while
(
addVolumeCompletionLatch
.
getCount
(
)
!=
newVolumeCount
)
{
int
i
=
0
;
while
(
i
++
<
1000
)
{
try
{
dn
.
getStorage
(
)
.
listStorageDirectories
(
)
;
}
catch
(
Exception
e
)
{
listStorageError
.
set
(
true
)
;
LOG
.
error
(
+
e
)
;
}
}
}
}
}
)
;
listStorageThread
.
start
(
)
;
doAnswer
(
new
Answer
<
Object
>
(
)
{
@
Override
public
Object
answer
(
InvocationOnMock
invocationOnMock
)
throws
Throwable
{
final
Random
r
=
new
Random
(
)
;
Thread
addVolThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
r
.
setSeed
(
Time
.
now
(
)
)
;
if
(
r
.
nextInt
(
10
)
>
4
)
{
int
s
=
r
.
nextInt
(
10
)
+
1
;
Thread
.
sleep
(
s
*
100
)
;
}
invocationOnMock
.
callRealMethod
(
)
;
}
catch
(
Throwable
throwable
)
{
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
for
(
int
i
=
0
;
i
<
100
;
i
++
)
{
DFSTestUtil
.
writeFile
(
cluster
.
getFileSystem
(
)
,
new
Path
(
+
String
.
valueOf
(
i
)
+
)
,
)
;
}
DataNodeTestUtils
.
triggerBlockReport
(
dn
)
;
MBeanServer
mbs
=
ManagementFactory
.
getPlatformMBeanServer
(
)
;
ObjectName
mxbeanName
=
new
ObjectName
(
)
;
String
bpActorInfo
=
(
String
)
mbs
.
getAttribute
(
mxbeanName
,
)
;
Assert
.
assertEquals
(
dn
.
getBPServiceActorInfo
(
)
,
bpActorInfo
)
;
LOG
.
info
(
+
bpActorInfo
)
;
TypeReference
<
ArrayList
<
Map
<
String
,
String
>>>
typeRef
=
new
TypeReference
<
ArrayList
<
Map
<
String
,
String
>>>
(
)
{
}
;
ArrayList
<
Map
<
String
,
String
>>
bpActorInfoList
=
new
ObjectMapper
(
)
.
readValue
(
bpActorInfo
,
typeRef
)
;
int
maxDataLength
=
Integer
.
valueOf
(
bpActorInfoList
.
get
(
0
)
.
get
(
)
)
;
int
confMaxDataLength
=
dn
.
getConf
(
)
.
getInt
(
CommonConfigurationKeys
.
IPC_MAXIMUM_DATA_LENGTH
,
CommonConfigurationKeys
.
IPC_MAXIMUM_DATA_LENGTH_DEFAULT
)
;
int
maxBlockReportSize
=
Integer
.
valueOf
(
bpActorInfoList
.
get
(
0
)
.
get
(
)
)
;
for
(
int
i
=
0
;
i
<
100
;
i
++
)
{
DFSTestUtil
.
writeFile
(
cluster
.
getFileSystem
(
)
,
new
Path
(
+
String
.
valueOf
(
i
)
+
)
,
)
;
}
DataNodeTestUtils
.
triggerBlockReport
(
dn
)
;
MBeanServer
mbs
=
ManagementFactory
.
getPlatformMBeanServer
(
)
;
ObjectName
mxbeanName
=
new
ObjectName
(
)
;
String
bpActorInfo
=
(
String
)
mbs
.
getAttribute
(
mxbeanName
,
)
;
Assert
.
assertEquals
(
dn
.
getBPServiceActorInfo
(
)
,
bpActorInfo
)
;
LOG
.
info
(
+
bpActorInfo
)
;
TypeReference
<
ArrayList
<
Map
<
String
,
String
>>>
typeRef
=
new
TypeReference
<
ArrayList
<
Map
<
String
,
String
>>>
(
)
{
}
;
ArrayList
<
Map
<
String
,
String
>>
bpActorInfoList
=
new
ObjectMapper
(
)
.
readValue
(
bpActorInfo
,
typeRef
)
;
int
maxDataLength
=
Integer
.
valueOf
(
bpActorInfoList
.
get
(
0
)
.
get
(
)
)
;
int
confMaxDataLength
=
dn
.
getConf
(
)
.
getInt
(
CommonConfigurationKeys
.
IPC_MAXIMUM_DATA_LENGTH
,
CommonConfigurationKeys
.
IPC_MAXIMUM_DATA_LENGTH_DEFAULT
)
;
int
maxBlockReportSize
=
Integer
.
valueOf
(
bpActorInfoList
.
get
(
0
)
.
get
(
)
)
;
LOG
.
info
(
+
maxDataLength
)
;
assertEquals
(
datanodes
.
size
(
)
,
1
)
;
final
DataNode
datanode
=
datanodes
.
get
(
0
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
datanode
.
getMetrics
(
)
.
name
(
)
)
;
final
long
LONG_FILE_LEN
=
1024
*
1024
*
10
;
final
long
startWriteValue
=
getLongCounter
(
,
rb
)
;
final
long
startReadValue
=
getLongCounter
(
,
rb
)
;
final
AtomicInteger
x
=
new
AtomicInteger
(
0
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
x
.
getAndIncrement
(
)
;
try
{
DFSTestUtil
.
createFile
(
fs
,
new
Path
(
+
x
.
get
(
)
)
,
LONG_FILE_LEN
,
(
short
)
1
,
Time
.
monotonicNow
(
)
)
;
DFSTestUtil
.
readFile
(
fs
,
new
Path
(
+
x
.
get
(
)
)
)
;
fs
.
delete
(
new
Path
(
+
x
.
get
(
)
)
,
true
)
;
}
catch
(
IOException
ioe
)
{
try
{
cluster
.
waitActive
(
)
;
NameNode
nn1
=
cluster
.
getNameNode
(
0
)
;
NameNode
nn2
=
cluster
.
getNameNode
(
1
)
;
assertNotNull
(
,
nn1
)
;
assertNotNull
(
,
nn2
)
;
String
bpid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getBlockPoolID
(
)
;
String
bpid2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getBlockPoolID
(
)
;
String
cid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getClusterID
(
)
;
String
cid2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getClusterID
(
)
;
int
lv1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getLayoutVersion
(
)
;
int
lv2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getLayoutVersion
(
)
;
int
ns1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getNamespaceID
(
)
;
int
ns2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getNamespaceID
(
)
;
assertNotSame
(
,
ns1
,
ns2
)
;
cluster
.
waitActive
(
)
;
NameNode
nn1
=
cluster
.
getNameNode
(
0
)
;
NameNode
nn2
=
cluster
.
getNameNode
(
1
)
;
assertNotNull
(
,
nn1
)
;
assertNotNull
(
,
nn2
)
;
String
bpid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getBlockPoolID
(
)
;
String
bpid2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getBlockPoolID
(
)
;
String
cid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getClusterID
(
)
;
String
cid2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getClusterID
(
)
;
int
lv1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getLayoutVersion
(
)
;
int
lv2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getLayoutVersion
(
)
;
int
ns1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getNamespaceID
(
)
;
int
ns2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getNamespaceID
(
)
;
assertNotSame
(
,
ns1
,
ns2
)
;
LOG
.
info
(
+
lv1
+
+
cid1
+
+
bpid1
+
+
nn1
.
getNameNodeAddress
(
)
)
;
int
lv1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getLayoutVersion
(
)
;
int
lv2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getLayoutVersion
(
)
;
int
ns1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getNamespaceID
(
)
;
int
ns2
=
FSImageTestUtil
.
getFSImage
(
nn2
)
.
getNamespaceID
(
)
;
assertNotSame
(
,
ns1
,
ns2
)
;
LOG
.
info
(
+
lv1
+
+
cid1
+
+
bpid1
+
+
nn1
.
getNameNodeAddress
(
)
)
;
LOG
.
info
(
+
lv2
+
+
cid2
+
+
bpid2
+
+
nn2
.
getNameNodeAddress
(
)
)
;
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
final
Map
<
String
,
Object
>
volInfos
=
dn
.
data
.
getVolumeInfoMap
(
)
;
Assert
.
assertTrue
(
,
volInfos
.
size
(
)
>
0
)
;
int
i
=
0
;
for
(
Map
.
Entry
<
String
,
Object
>
e
:
volInfos
.
entrySet
(
)
)
{
LOG
.
info
(
+
i
++
+
+
e
.
getKey
(
)
+
+
e
.
getValue
(
)
)
;
}
assertEquals
(
,
cluster
.
getFsDatasetTestUtils
(
0
)
.
getDefaultNumOfDataDirs
(
)
,
volInfos
.
size
(
)
)
;
for
(
BPOfferService
bpos
:
dn
.
getAllBpOs
(
)
)
{
@
Test
public
void
testFedSingleNN
(
)
throws
IOException
{
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nameNodePort
(
9927
)
.
build
(
)
;
try
{
NameNode
nn1
=
cluster
.
getNameNode
(
)
;
assertNotNull
(
,
nn1
)
;
String
bpid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getBlockPoolID
(
)
;
String
cid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getClusterID
(
)
;
int
lv1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getLayoutVersion
(
)
;
try
{
NameNode
nn1
=
cluster
.
getNameNode
(
)
;
assertNotNull
(
,
nn1
)
;
String
bpid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getBlockPoolID
(
)
;
String
cid1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getClusterID
(
)
;
int
lv1
=
FSImageTestUtil
.
getFSImage
(
nn1
)
.
getLayoutVersion
(
)
;
LOG
.
info
(
+
lv1
+
+
cid1
+
+
bpid1
+
+
nn1
.
getNameNodeAddress
(
)
)
;
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
final
Map
<
String
,
Object
>
volInfos
=
dn
.
data
.
getVolumeInfoMap
(
)
;
Assert
.
assertTrue
(
,
volInfos
.
size
(
)
>
0
)
;
int
i
=
0
;
for
(
Map
.
Entry
<
String
,
Object
>
e
:
volInfos
.
entrySet
(
)
)
{
LOG
.
info
(
+
i
++
+
+
e
.
getKey
(
)
+
+
e
.
getValue
(
)
)
;
}
assertEquals
(
,
cluster
.
getFsDatasetTestUtils
(
0
)
.
getDefaultNumOfDataDirs
(
)
,
volInfos
.
size
(
)
)
;
for
(
BPOfferService
bpos
:
dn
.
getAllBpOs
(
)
)
{
@
Test
public
void
testClusterIdMismatch
(
)
throws
Exception
{
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
MiniDFSNNTopology
.
simpleFederatedTopology
(
2
)
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
List
<
BPOfferService
>
bposs
=
dn
.
getAllBpOs
(
)
;
@
Test
public
void
testClusterIdMismatch
(
)
throws
Exception
{
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
MiniDFSNNTopology
.
simpleFederatedTopology
(
2
)
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
List
<
BPOfferService
>
bposs
=
dn
.
getAllBpOs
(
)
;
LOG
.
info
(
+
bposs
.
size
(
)
)
;
Assert
.
assertEquals
(
,
bposs
.
size
(
)
,
2
)
;
cluster
.
addNameNode
(
conf
,
9938
)
;
Thread
.
sleep
(
500
)
;
bposs
=
dn
.
getAllBpOs
(
)
;
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
List
<
BPOfferService
>
bposs
=
dn
.
getAllBpOs
(
)
;
LOG
.
info
(
+
bposs
.
size
(
)
)
;
Assert
.
assertEquals
(
,
bposs
.
size
(
)
,
2
)
;
cluster
.
addNameNode
(
conf
,
9938
)
;
Thread
.
sleep
(
500
)
;
bposs
=
dn
.
getAllBpOs
(
)
;
LOG
.
info
(
+
bposs
.
size
(
)
)
;
Assert
.
assertEquals
(
,
bposs
.
size
(
)
,
3
)
;
StartupOption
.
FORMAT
.
setClusterId
(
)
;
cluster
.
addNameNode
(
conf
,
9948
)
;
NameNode
nn4
=
cluster
.
getNameNode
(
3
)
;
assertNotNull
(
,
nn4
)
;
Thread
.
sleep
(
500
)
;
bposs
=
dn
.
getAllBpOs
(
)
;
private
void
deleteAndEnsureInTrash
(
Path
pathToDelete
,
File
blockFile
,
File
trashFile
)
throws
Exception
{
assertTrue
(
blockFile
.
exists
(
)
)
;
assertFalse
(
trashFile
.
exists
(
)
)
;
private
void
startNewDataNodeWithDiskFailure
(
File
badDataDir
,
boolean
tolerated
)
throws
Exception
{
final
File
data5
=
new
File
(
dataDir
,
)
;
final
String
newDirs
=
badDataDir
.
toString
(
)
+
+
data5
.
toString
(
)
;
final
Configuration
newConf
=
new
Configuration
(
conf
)
;
newConf
.
set
(
DFSConfigKeys
.
DFS_DATANODE_DATA_DIR_KEY
,
newDirs
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
private
void
verifyDataNodeVolumeMetrics
(
final
FileSystem
fs
,
final
MiniDFSCluster
cluster
,
final
Path
fileName
)
throws
IOException
{
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
List
<
DataNode
>
datanodes
=
cluster
.
getDataNodes
(
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
DataNode
datanode
=
datanodes
.
get
(
0
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fileName
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
final
FsVolumeSpi
volume
=
datanode
.
getFSDataset
(
)
.
getVolume
(
block
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
DataNodeVolumeMetrics
metrics
=
volume
.
getMetrics
(
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
MetricsRecordBuilder
rb
=
getMetrics
(
volume
.
getMetrics
(
)
.
name
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
assertCounter
(
,
metrics
.
getTotalDataFileIos
(
)
,
rb
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalMetadataOperations
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalDataFileIos
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getTotalFileIoErrors
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getMetadataOperationStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFileIoErrorSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getDataFileIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getFlushIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getSyncIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getReadIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoMean
(
)
)
;
LOG
.
info
(
+
metrics
.
getWriteIoStdDev
(
)
)
;
LOG
.
info
(
+
metrics
.
getFileIoErrorSampleCount
(
)
)
;
LOG
.
info
(
+
metrics
.
getFileIoErrorMean
(
)
)
;
@
Test
(
timeout
=
60000
)
public
void
testDatanodeRegistrationRetry
(
)
throws
Exception
{
final
DatanodeProtocolClientSideTranslatorPB
namenode
=
mock
(
DatanodeProtocolClientSideTranslatorPB
.
class
)
;
Mockito
.
doAnswer
(
new
Answer
<
DatanodeRegistration
>
(
)
{
int
i
=
0
;
@
Override
public
DatanodeRegistration
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
i
++
;
if
(
i
>
1
&&
i
<
5
)
{
LOG
.
info
(
+
i
)
;
throw
new
EOFException
(
)
;
}
else
{
DatanodeRegistration
dr
=
(
DatanodeRegistration
)
invocation
.
getArguments
(
)
[
0
]
;
datanodeRegistration
=
new
DatanodeRegistration
(
dr
.
getDatanodeUuid
(
)
,
dr
)
;
LOG
.
info
(
+
datanodeRegistration
)
;
return
datanodeRegistration
;
}
}
}
)
.
when
(
namenode
)
.
registerDatanode
(
Mockito
.
any
(
DatanodeRegistration
.
class
)
)
;
when
(
namenode
.
versionRequest
(
)
)
.
thenReturn
(
new
NamespaceInfo
(
1
,
CLUSTER_ID
,
POOL_ID
,
1L
)
)
;
Mockito
.
doAnswer
(
new
Answer
<
HeartbeatResponse
>
(
)
{
int
i
=
0
;
@
Override
public
HeartbeatResponse
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
i
++
;
HeartbeatResponse
heartbeatResponse
;
if
(
i
==
1
)
{
else
{
DatanodeRegistration
dr
=
(
DatanodeRegistration
)
invocation
.
getArguments
(
)
[
0
]
;
datanodeRegistration
=
new
DatanodeRegistration
(
dr
.
getDatanodeUuid
(
)
,
dr
)
;
LOG
.
info
(
+
datanodeRegistration
)
;
return
datanodeRegistration
;
}
}
}
)
.
when
(
namenode
)
.
registerDatanode
(
Mockito
.
any
(
DatanodeRegistration
.
class
)
)
;
when
(
namenode
.
versionRequest
(
)
)
.
thenReturn
(
new
NamespaceInfo
(
1
,
CLUSTER_ID
,
POOL_ID
,
1L
)
)
;
Mockito
.
doAnswer
(
new
Answer
<
HeartbeatResponse
>
(
)
{
int
i
=
0
;
@
Override
public
HeartbeatResponse
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
i
++
;
HeartbeatResponse
heartbeatResponse
;
if
(
i
==
1
)
{
LOG
.
info
(
+
i
)
;
heartbeatResponse
=
new
HeartbeatResponse
(
new
DatanodeCommand
[
]
{
RegisterCommand
.
REGISTER
}
,
new
NNHAStatusHeartbeat
(
HAServiceState
.
ACTIVE
,
1
)
,
null
,
ThreadLocalRandom
.
current
(
)
.
nextLong
(
)
|
1L
)
;
private
long
truncateBlockFile
(
)
throws
IOException
{
try
(
AutoCloseableLock
lock
=
fds
.
acquireDatasetLock
(
)
)
{
for
(
ReplicaInfo
b
:
FsDatasetTestUtil
.
getReplicas
(
fds
,
bpid
)
)
{
File
f
=
new
File
(
b
.
getBlockURI
(
)
)
;
File
mf
=
new
File
(
b
.
getMetadataURI
(
)
)
;
if
(
f
.
exists
(
)
&&
f
.
length
(
)
!=
0
&&
mf
.
exists
(
)
)
{
FileOutputStream
s
=
null
;
FileChannel
channel
=
null
;
try
{
s
=
new
FileOutputStream
(
f
)
;
channel
=
s
.
getChannel
(
)
;
channel
.
truncate
(
0
)
;
private
long
deleteBlockFile
(
)
{
try
(
AutoCloseableLock
lock
=
fds
.
acquireDatasetLock
(
)
)
{
for
(
ReplicaInfo
b
:
FsDatasetTestUtil
.
getReplicas
(
fds
,
bpid
)
)
{
File
f
=
new
File
(
b
.
getBlockURI
(
)
)
;
File
mf
=
new
File
(
b
.
getMetadataURI
(
)
)
;
if
(
f
.
exists
(
)
&&
mf
.
exists
(
)
&&
f
.
delete
(
)
)
{
private
long
deleteMetaFile
(
)
{
try
(
AutoCloseableLock
lock
=
fds
.
acquireDatasetLock
(
)
)
{
for
(
ReplicaInfo
b
:
FsDatasetTestUtil
.
getReplicas
(
fds
,
bpid
)
)
{
if
(
b
.
metadataExists
(
)
&&
b
.
deleteMetadata
(
)
)
{
for
(
FsVolumeSpi
v
:
volumes
)
{
if
(
v
.
getStorageID
(
)
.
equals
(
b
.
getVolume
(
)
.
getStorageID
(
)
)
)
{
continue
;
}
File
sourceBlock
=
new
File
(
b
.
getBlockURI
(
)
)
;
File
sourceMeta
=
new
File
(
b
.
getMetadataURI
(
)
)
;
URI
sourceRoot
=
b
.
getVolume
(
)
.
getStorageLocation
(
)
.
getUri
(
)
;
URI
destRoot
=
v
.
getStorageLocation
(
)
.
getUri
(
)
;
String
relativeBlockPath
=
sourceRoot
.
relativize
(
sourceBlock
.
toURI
(
)
)
.
getPath
(
)
;
String
relativeMetaPath
=
sourceRoot
.
relativize
(
sourceMeta
.
toURI
(
)
)
.
getPath
(
)
;
File
destBlock
=
new
File
(
new
File
(
destRoot
)
.
toString
(
)
,
relativeBlockPath
)
;
File
destMeta
=
new
File
(
new
File
(
destRoot
)
.
toString
(
)
,
relativeMetaPath
)
;
destBlock
.
getParentFile
(
)
.
mkdirs
(
)
;
FileUtils
.
copyFile
(
sourceBlock
,
destBlock
)
;
FileUtils
.
copyFile
(
sourceMeta
,
destMeta
)
;
if
(
destBlock
.
exists
(
)
&&
destMeta
.
exists
(
)
)
{
if
(
v
.
getStorageID
(
)
.
equals
(
b
.
getVolume
(
)
.
getStorageID
(
)
)
)
{
continue
;
}
File
sourceBlock
=
new
File
(
b
.
getBlockURI
(
)
)
;
File
sourceMeta
=
new
File
(
b
.
getMetadataURI
(
)
)
;
URI
sourceRoot
=
b
.
getVolume
(
)
.
getStorageLocation
(
)
.
getUri
(
)
;
URI
destRoot
=
v
.
getStorageLocation
(
)
.
getUri
(
)
;
String
relativeBlockPath
=
sourceRoot
.
relativize
(
sourceBlock
.
toURI
(
)
)
.
getPath
(
)
;
String
relativeMetaPath
=
sourceRoot
.
relativize
(
sourceMeta
.
toURI
(
)
)
.
getPath
(
)
;
File
destBlock
=
new
File
(
new
File
(
destRoot
)
.
toString
(
)
,
relativeBlockPath
)
;
File
destMeta
=
new
File
(
new
File
(
destRoot
)
.
toString
(
)
,
relativeMetaPath
)
;
destBlock
.
getParentFile
(
)
.
mkdirs
(
)
;
FileUtils
.
copyFile
(
sourceBlock
,
destBlock
)
;
FileUtils
.
copyFile
(
sourceMeta
,
destMeta
)
;
if
(
destBlock
.
exists
(
)
&&
destMeta
.
exists
(
)
)
{
LOG
.
info
(
+
sourceBlock
+
+
destBlock
)
;
private
long
createBlockFile
(
)
throws
IOException
{
long
id
=
getFreeBlockId
(
)
;
try
(
FsDatasetSpi
.
FsVolumeReferences
volumes
=
fds
.
getFsVolumeReferences
(
)
)
{
int
numVolumes
=
volumes
.
size
(
)
;
int
index
=
rand
.
nextInt
(
numVolumes
-
1
)
;
File
finalizedDir
=
(
(
FsVolumeImpl
)
volumes
.
get
(
index
)
)
.
getFinalizedDir
(
bpid
)
;
File
file
=
new
File
(
finalizedDir
,
getBlockFile
(
id
)
)
;
if
(
file
.
createNewFile
(
)
)
{
private
long
createMetaFile
(
)
throws
IOException
{
long
id
=
getFreeBlockId
(
)
;
try
(
FsDatasetSpi
.
FsVolumeReferences
refs
=
fds
.
getFsVolumeReferences
(
)
)
{
int
numVolumes
=
refs
.
size
(
)
;
int
index
=
rand
.
nextInt
(
numVolumes
-
1
)
;
File
finalizedDir
=
(
(
FsVolumeImpl
)
refs
.
get
(
index
)
)
.
getFinalizedDir
(
bpid
)
;
File
file
=
new
File
(
finalizedDir
,
getMetaFile
(
id
)
)
;
if
(
file
.
createNewFile
(
)
)
{
private
long
createBlockMetaFile
(
)
throws
IOException
{
long
id
=
getFreeBlockId
(
)
;
try
(
FsDatasetSpi
.
FsVolumeReferences
refs
=
fds
.
getFsVolumeReferences
(
)
)
{
int
numVolumes
=
refs
.
size
(
)
;
int
index
=
rand
.
nextInt
(
numVolumes
-
1
)
;
File
finalizedDir
=
(
(
FsVolumeImpl
)
refs
.
get
(
index
)
)
.
getFinalizedDir
(
bpid
)
;
File
file
=
new
File
(
finalizedDir
,
getBlockFile
(
id
)
)
;
if
(
file
.
createNewFile
(
)
)
{
private
long
createBlockMetaFile
(
)
throws
IOException
{
long
id
=
getFreeBlockId
(
)
;
try
(
FsDatasetSpi
.
FsVolumeReferences
refs
=
fds
.
getFsVolumeReferences
(
)
)
{
int
numVolumes
=
refs
.
size
(
)
;
int
index
=
rand
.
nextInt
(
numVolumes
-
1
)
;
File
finalizedDir
=
(
(
FsVolumeImpl
)
refs
.
get
(
index
)
)
.
getFinalizedDir
(
bpid
)
;
File
file
=
new
File
(
finalizedDir
,
getBlockFile
(
id
)
)
;
if
(
file
.
createNewFile
(
)
)
{
LOG
.
info
(
+
file
.
getName
(
)
)
;
String
name1
=
file
.
getAbsolutePath
(
)
+
;
String
name2
=
file
.
getAbsolutePath
(
)
+
;
file
=
new
File
(
name1
)
;
if
(
file
.
createNewFile
(
)
)
{
long
id
=
getFreeBlockId
(
)
;
try
(
FsDatasetSpi
.
FsVolumeReferences
refs
=
fds
.
getFsVolumeReferences
(
)
)
{
int
numVolumes
=
refs
.
size
(
)
;
int
index
=
rand
.
nextInt
(
numVolumes
-
1
)
;
File
finalizedDir
=
(
(
FsVolumeImpl
)
refs
.
get
(
index
)
)
.
getFinalizedDir
(
bpid
)
;
File
file
=
new
File
(
finalizedDir
,
getBlockFile
(
id
)
)
;
if
(
file
.
createNewFile
(
)
)
{
LOG
.
info
(
+
file
.
getName
(
)
)
;
String
name1
=
file
.
getAbsolutePath
(
)
+
;
String
name2
=
file
.
getAbsolutePath
(
)
+
;
file
=
new
File
(
name1
)
;
if
(
file
.
createNewFile
(
)
)
{
LOG
.
info
(
+
name1
)
;
}
file
=
new
File
(
name2
)
;
if
(
file
.
createNewFile
(
)
)
{
int
index
=
rand
.
nextInt
(
numVolumes
-
1
)
;
File
finalizedDir
=
(
(
FsVolumeImpl
)
refs
.
get
(
index
)
)
.
getFinalizedDir
(
bpid
)
;
File
file
=
new
File
(
finalizedDir
,
getBlockFile
(
id
)
)
;
if
(
file
.
createNewFile
(
)
)
{
LOG
.
info
(
+
file
.
getName
(
)
)
;
String
name1
=
file
.
getAbsolutePath
(
)
+
;
String
name2
=
file
.
getAbsolutePath
(
)
+
;
file
=
new
File
(
name1
)
;
if
(
file
.
createNewFile
(
)
)
{
LOG
.
info
(
+
name1
)
;
}
file
=
new
File
(
name2
)
;
if
(
file
.
createNewFile
(
)
)
{
LOG
.
info
(
+
name2
)
;
}
file
=
new
File
(
finalizedDir
,
getMetaFile
(
id
)
)
;
if
(
file
.
createNewFile
(
)
)
{
int
retries
=
maxRetries
;
while
(
(
retries
>
0
)
&&
(
(
ratio
<
7f
)
||
(
ratio
>
10f
)
)
)
{
scanner
=
new
DirectoryScanner
(
fds
,
conf
)
;
ratio
=
runThrottleTest
(
blocks
)
;
retries
-=
1
;
}
LOG
.
info
(
+
ratio
)
;
assertTrue
(
,
ratio
<=
10f
)
;
assertTrue
(
,
ratio
>=
7f
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY
,
200
)
;
ratio
=
0.0f
;
retries
=
maxRetries
;
while
(
(
retries
>
0
)
&&
(
(
ratio
<
2.75f
)
||
(
ratio
>
4.5f
)
)
)
{
scanner
=
new
DirectoryScanner
(
fds
,
conf
)
;
ratio
=
runThrottleTest
(
blocks
)
;
retries
-=
1
;
retries
=
maxRetries
;
while
(
(
retries
>
0
)
&&
(
(
ratio
<
2.75f
)
||
(
ratio
>
4.5f
)
)
)
{
scanner
=
new
DirectoryScanner
(
fds
,
conf
)
;
ratio
=
runThrottleTest
(
blocks
)
;
retries
-=
1
;
}
LOG
.
info
(
+
ratio
)
;
assertTrue
(
,
ratio
<=
4.5f
)
;
assertTrue
(
,
ratio
>=
2.75f
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_DATANODE_DIRECTORYSCAN_THREADS_KEY
,
3
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY
,
100
)
;
ratio
=
0.0f
;
retries
=
maxRetries
;
while
(
(
retries
>
0
)
&&
(
(
ratio
<
7f
)
||
(
ratio
>
10f
)
)
)
{
scanner
=
new
DirectoryScanner
(
fds
,
conf
)
;
ratio
=
runThrottleTest
(
blocks
)
;
scanner
=
new
DirectoryScanner
(
fds
,
conf
)
;
scanner
.
setRetainDiffs
(
true
)
;
final
AtomicLong
nowMs
=
new
AtomicLong
(
)
;
interruptor
.
schedule
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
nowMs
.
set
(
Time
.
monotonicNow
(
)
)
;
scanner
.
shutdown
(
)
;
}
}
,
2L
,
TimeUnit
.
SECONDS
)
;
scanner
.
reconcile
(
)
;
assertFalse
(
scanner
.
getRunStatus
(
)
)
;
long
finalMs
=
nowMs
.
get
(
)
;
if
(
finalMs
>
0
)
{
LOG
.
info
(
+
(
Time
.
monotonicNow
(
)
-
finalMs
)
+
)
;
assertTrue
(
,
Time
.
monotonicNow
(
)
-
finalMs
<
1000L
)
;
}
ratio
=
(
float
)
scanner
.
timeWaitingMs
.
get
(
)
/
scanner
.
timeRunningMs
.
get
(
)
;
Configuration
conf
=
getDefaultConf
(
)
;
conf
.
setLong
(
DFSConfigKeys
.
DFS_DATANODE_CACHE_REVOCATION_TIMEOUT_MS
,
250L
)
;
conf
.
setLong
(
DFSConfigKeys
.
DFS_DATANODE_CACHE_REVOCATION_POLLING_MS
,
2L
)
;
MiniDFSCluster
cluster
=
null
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
1
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
final
String
testFile
=
;
DFSTestUtil
.
createFile
(
dfs
,
new
Path
(
testFile
)
,
BLOCK_SIZE
,
(
short
)
1
,
0xcafe
)
;
dfs
.
addCachePool
(
new
CachePoolInfo
(
)
)
;
long
cacheDirectiveId
=
dfs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
new
Path
(
testFile
)
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
FsDatasetSpi
<
?
>
fsd
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
.
getFSDataset
(
)
;
DFSTestUtil
.
verifyExpectedCacheUsage
(
BLOCK_SIZE
,
1
,
fsd
)
;
FSDataInputStream
in
=
dfs
.
open
(
new
Path
(
testFile
)
)
;
ByteBuffer
buf
=
in
.
read
(
null
,
BLOCK_SIZE
,
EnumSet
.
noneOf
(
ReadOption
.
class
)
)
;
conf
.
setLong
(
DFSConfigKeys
.
DFS_DATANODE_CACHE_REVOCATION_POLLING_MS
,
2L
)
;
MiniDFSCluster
cluster
=
null
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
1
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
final
String
testFile
=
;
DFSTestUtil
.
createFile
(
dfs
,
new
Path
(
testFile
)
,
BLOCK_SIZE
,
(
short
)
1
,
0xcafe
)
;
dfs
.
addCachePool
(
new
CachePoolInfo
(
)
)
;
long
cacheDirectiveId
=
dfs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
new
Path
(
testFile
)
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
FsDatasetSpi
<
?
>
fsd
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
.
getFSDataset
(
)
;
DFSTestUtil
.
verifyExpectedCacheUsage
(
BLOCK_SIZE
,
1
,
fsd
)
;
FSDataInputStream
in
=
dfs
.
open
(
new
Path
(
testFile
)
)
;
ByteBuffer
buf
=
in
.
read
(
null
,
BLOCK_SIZE
,
EnumSet
.
noneOf
(
ReadOption
.
class
)
)
;
LOG
.
info
(
,
cacheDirectiveId
)
;
dfs
.
removeCacheDirective
(
cacheDirectiveId
)
;
@
Override
protected
void
sendBlockReports
(
DatanodeRegistration
dnR
,
String
poolId
,
StorageBlockReport
[
]
reports
)
throws
IOException
{
private
static
void
createProvidedReplicas
(
Configuration
conf
)
{
long
numReplicas
=
(
long
)
Math
.
ceil
(
(
double
)
FILE_LEN
/
BLK_LEN
)
;
File
providedFile
=
new
File
(
BASE_DIR
,
FILE_NAME
)
;
replicas
=
new
ArrayList
<
ProvidedReplica
>
(
)
;
private
static
LocalReplicaInPipeline
getReplica
(
final
DataNode
datanode
,
final
String
bpid
,
final
ReplicaState
expectedState
)
throws
InterruptedException
{
final
Collection
<
ReplicaInfo
>
replicas
=
FsDatasetTestUtil
.
getReplicas
(
datanode
.
getFSDataset
(
)
,
bpid
)
;
for
(
int
i
=
0
;
i
<
5
&&
replicas
.
size
(
)
==
0
;
i
++
)
{
@
Test
public
void
testTransferRbw
(
)
throws
Exception
{
final
HdfsConfiguration
conf
=
new
HdfsConfiguration
(
)
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
REPLICATION
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
final
DistributedFileSystem
fs
=
cluster
.
getFileSystem
(
)
;
final
Path
p
=
new
Path
(
)
;
final
int
size
=
(
1
<<
16
)
+
RAN
.
nextInt
(
1
<<
16
)
;
final
FSDataOutputStream
out
=
fs
.
create
(
p
,
REPLICATION
)
;
final
byte
[
]
bytes
=
new
byte
[
1024
]
;
for
(
int
remaining
=
size
;
remaining
>
0
;
)
{
RAN
.
nextBytes
(
bytes
)
;
final
int
len
=
bytes
.
length
<
remaining
?
bytes
.
length
:
remaining
;
out
.
write
(
bytes
,
0
,
len
)
;
out
.
hflush
(
)
;
remaining
-=
len
;
}
final
ReplicaBeingWritten
oldrbw
;
final
DataNode
newnode
;
final
DatanodeInfo
newnodeinfo
;
final
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
{
final
DataNode
oldnode
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
Assert
.
assertNull
(
oldnode
.
xserver
.
getWriteThrottler
(
)
)
;
oldrbw
=
getRbw
(
oldnode
,
bpid
)
;
@
Test
(
timeout
=
10000
)
public
void
testCheckAllVolumes
(
)
throws
Exception
{
LOG
.
info
(
,
testName
.
getMethodName
(
)
)
;
final
List
<
FsVolumeSpi
>
volumes
=
makeVolumes
(
NUM_VOLUMES
,
expectedVolumeHealth
)
;
final
FsDatasetSpi
<
FsVolumeSpi
>
dataset
=
makeDataset
(
volumes
)
;
final
DatasetVolumeChecker
checker
=
new
DatasetVolumeChecker
(
new
HdfsConfiguration
(
)
,
new
FakeTimer
(
)
)
;
checker
.
setDelegateChecker
(
new
DummyChecker
(
)
)
;
Set
<
FsVolumeSpi
>
failedVolumes
=
checker
.
checkAllVolumes
(
dataset
)
;
protected
final
LocatedBlocks
ensureFileReplicasOnStorageType
(
Path
path
,
StorageType
storageType
)
throws
IOException
,
TimeoutException
,
InterruptedException
{
public
static
void
initCacheManipulator
(
)
{
NativeIO
.
POSIX
.
setCacheManipulator
(
new
NativeIO
.
POSIX
.
CacheManipulator
(
)
{
@
Override
public
void
mlock
(
String
identifier
,
ByteBuffer
mmap
,
long
length
)
throws
IOException
{
BlockReaderTestUtil
.
enableHdfsCachingTracing
(
)
;
Assert
.
assertEquals
(
0
,
CACHE_CAPACITY
%
BLOCK_SIZE
)
;
assertEquals
(
CACHE_CAPACITY
,
cacheManager
.
getCacheCapacity
(
)
)
;
assertEquals
(
0L
,
cacheManager
.
getMemCacheCapacity
(
)
)
;
final
Path
testFile
=
new
Path
(
)
;
final
long
testFileLen
=
maxCacheBlocksNum
*
BLOCK_SIZE
;
DFSTestUtil
.
createFile
(
fs
,
testFile
,
testFileLen
,
(
short
)
1
,
0xbeef
)
;
List
<
ExtendedBlockId
>
blockKeys
=
getExtendedBlockId
(
testFile
,
testFileLen
)
;
fs
.
addCachePool
(
new
CachePoolInfo
(
)
)
;
final
long
cacheDirectiveId
=
fs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
testFile
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
long
blocksCached
=
MetricsAsserts
.
getLongCounter
(
,
dnMetrics
)
;
if
(
blocksCached
!=
maxCacheBlocksNum
)
{
}
else
{
fail
(
+
cachePath
)
;
}
}
final
Path
smallTestFile
=
new
Path
(
)
;
final
long
smallTestFileLen
=
BLOCK_SIZE
;
DFSTestUtil
.
createFile
(
fs
,
smallTestFile
,
smallTestFileLen
,
(
short
)
1
,
0xbeef
)
;
final
long
smallFileCacheDirectiveId
=
fs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
smallTestFile
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
Thread
.
sleep
(
10000
)
;
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
long
blocksCached
=
MetricsAsserts
.
getLongCounter
(
,
dnMetrics
)
;
assertTrue
(
blocksCached
==
maxCacheBlocksNum
)
;
assertEquals
(
blockKeyToVolume
.
size
(
)
,
maxCacheBlocksNum
)
;
assertTrue
(
blockKeyToVolume
.
keySet
(
)
.
containsAll
(
blockKeys
)
)
;
fs
.
removeCacheDirective
(
smallFileCacheDirectiveId
)
;
fs
.
removeCacheDirective
(
cacheDirectiveId
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
}
}
final
Path
smallTestFile
=
new
Path
(
)
;
final
long
smallTestFileLen
=
BLOCK_SIZE
;
DFSTestUtil
.
createFile
(
fs
,
smallTestFile
,
smallTestFileLen
,
(
short
)
1
,
0xbeef
)
;
final
long
smallFileCacheDirectiveId
=
fs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
smallTestFile
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
Thread
.
sleep
(
10000
)
;
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
long
blocksCached
=
MetricsAsserts
.
getLongCounter
(
,
dnMetrics
)
;
assertTrue
(
blocksCached
==
maxCacheBlocksNum
)
;
assertEquals
(
blockKeyToVolume
.
size
(
)
,
maxCacheBlocksNum
)
;
assertTrue
(
blockKeyToVolume
.
keySet
(
)
.
containsAll
(
blockKeys
)
)
;
fs
.
removeCacheDirective
(
smallFileCacheDirectiveId
)
;
fs
.
removeCacheDirective
(
cacheDirectiveId
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
@
Test
(
timeout
=
600000
)
public
void
testCacheAndUncacheBlockWithRetries
(
)
throws
Exception
{
NativeIO
.
POSIX
.
setCacheManipulator
(
new
NoMlockCacheManipulator
(
)
{
private
final
Set
<
String
>
seenIdentifiers
=
new
HashSet
<
String
>
(
)
;
@
Override
public
void
mlock
(
String
identifier
,
ByteBuffer
mmap
,
long
length
)
throws
IOException
{
if
(
seenIdentifiers
.
contains
(
identifier
)
)
{
final
int
NUM_BLOCKS
=
5
;
DFSTestUtil
.
verifyExpectedCacheUsage
(
0
,
0
,
fsd
)
;
final
Path
testFile
=
new
Path
(
)
;
final
long
testFileLen
=
BLOCK_SIZE
*
NUM_BLOCKS
;
DFSTestUtil
.
createFile
(
fs
,
testFile
,
testFileLen
,
(
short
)
1
,
0xABBAl
)
;
HdfsBlockLocation
[
]
locs
=
(
HdfsBlockLocation
[
]
)
fs
.
getFileBlockLocations
(
testFile
,
0
,
testFileLen
)
;
assertEquals
(
,
NUM_BLOCKS
,
locs
.
length
)
;
final
long
[
]
blockSizes
=
getBlockSizes
(
locs
)
;
final
long
cacheCapacity
=
fsd
.
getCacheCapacity
(
)
;
long
cacheUsed
=
fsd
.
getCacheUsed
(
)
;
long
current
=
0
;
assertEquals
(
,
CACHE_CAPACITY
,
cacheCapacity
)
;
assertEquals
(
,
current
,
cacheUsed
)
;
NativeIO
.
POSIX
.
setCacheManipulator
(
new
NoMlockCacheManipulator
(
)
{
@
Override
public
void
mlock
(
String
identifier
,
ByteBuffer
mmap
,
long
length
)
throws
IOException
{
final
int
TOTAL_BLOCKS_PER_CACHE
=
Ints
.
checkedCast
(
CACHE_CAPACITY
/
BLOCK_SIZE
)
;
BlockReaderTestUtil
.
enableHdfsCachingTracing
(
)
;
Assert
.
assertEquals
(
0
,
CACHE_CAPACITY
%
BLOCK_SIZE
)
;
final
Path
SMALL_FILE
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
SMALL_FILE
,
BLOCK_SIZE
,
(
short
)
1
,
0xcafe
)
;
final
Path
BIG_FILE
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
BIG_FILE
,
TOTAL_BLOCKS_PER_CACHE
*
BLOCK_SIZE
,
(
short
)
1
,
0xbeef
)
;
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
dfs
.
addCachePool
(
new
CachePoolInfo
(
)
)
;
final
long
bigCacheDirectiveId
=
dfs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
BIG_FILE
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
long
blocksCached
=
MetricsAsserts
.
getLongCounter
(
,
dnMetrics
)
;
if
(
blocksCached
!=
TOTAL_BLOCKS_PER_CACHE
)
{
}
}
,
1000
,
30000
)
;
final
long
shortCacheDirectiveId
=
dfs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
SMALL_FILE
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
Thread
.
sleep
(
10000
)
;
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
Assert
.
assertEquals
(
TOTAL_BLOCKS_PER_CACHE
,
MetricsAsserts
.
getLongCounter
(
,
dnMetrics
)
)
;
dfs
.
removeCacheDirective
(
bigCacheDirectiveId
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
RemoteIterator
<
CacheDirectiveEntry
>
iter
;
try
{
iter
=
dfs
.
listCacheDirectives
(
new
CacheDirectiveInfo
.
Builder
(
)
.
build
(
)
)
;
CacheDirectiveEntry
entry
;
do
{
entry
=
iter
.
next
(
)
;
}
while
(
entry
.
getInfo
(
)
.
getId
(
)
!=
shortCacheDirectiveId
)
;
final
long
shortCacheDirectiveId
=
dfs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
SMALL_FILE
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
Thread
.
sleep
(
10000
)
;
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
Assert
.
assertEquals
(
TOTAL_BLOCKS_PER_CACHE
,
MetricsAsserts
.
getLongCounter
(
,
dnMetrics
)
)
;
dfs
.
removeCacheDirective
(
bigCacheDirectiveId
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
RemoteIterator
<
CacheDirectiveEntry
>
iter
;
try
{
iter
=
dfs
.
listCacheDirectives
(
new
CacheDirectiveInfo
.
Builder
(
)
.
build
(
)
)
;
CacheDirectiveEntry
entry
;
do
{
entry
=
iter
.
next
(
)
;
}
while
(
entry
.
getInfo
(
)
.
getId
(
)
!=
shortCacheDirectiveId
)
;
if
(
entry
.
getStats
(
)
.
getFilesCached
(
)
!=
1
)
{
final
int
numExistingVolumes
=
getNumVolumes
(
)
;
final
int
totalVolumes
=
numNewVolumes
+
numExistingVolumes
;
Set
<
String
>
expectedVolumes
=
new
HashSet
<
String
>
(
)
;
List
<
NamespaceInfo
>
nsInfos
=
Lists
.
newArrayList
(
)
;
for
(
String
bpid
:
BLOCK_POOL_IDS
)
{
nsInfos
.
add
(
new
NamespaceInfo
(
0
,
CLUSTER_ID
,
bpid
,
1
)
)
;
}
for
(
int
i
=
0
;
i
<
numNewVolumes
;
i
++
)
{
String
path
=
BASE_DIR
+
+
i
;
String
pathUri
=
new
Path
(
path
)
.
toUri
(
)
.
toString
(
)
;
expectedVolumes
.
add
(
new
File
(
pathUri
)
.
getAbsolutePath
(
)
)
;
StorageLocation
loc
=
StorageLocation
.
parse
(
pathUri
)
;
Storage
.
StorageDirectory
sd
=
createStorageDirectory
(
new
File
(
path
)
,
conf
)
;
DataStorage
.
VolumeBuilder
builder
=
new
DataStorage
.
VolumeBuilder
(
storage
,
sd
)
;
when
(
storage
.
prepareVolume
(
eq
(
datanode
)
,
eq
(
loc
)
,
anyList
(
)
)
)
.
thenReturn
(
builder
)
;
dataset
.
addVolume
(
loc
,
nsInfos
)
;
String
path
=
BASE_DIR
+
+
i
;
String
pathUri
=
new
Path
(
path
)
.
toUri
(
)
.
toString
(
)
;
expectedVolumes
.
add
(
new
File
(
pathUri
)
.
getAbsolutePath
(
)
)
;
StorageLocation
loc
=
StorageLocation
.
parse
(
pathUri
)
;
Storage
.
StorageDirectory
sd
=
createStorageDirectory
(
new
File
(
path
)
,
conf
)
;
DataStorage
.
VolumeBuilder
builder
=
new
DataStorage
.
VolumeBuilder
(
storage
,
sd
)
;
when
(
storage
.
prepareVolume
(
eq
(
datanode
)
,
eq
(
loc
)
,
anyList
(
)
)
)
.
thenReturn
(
builder
)
;
dataset
.
addVolume
(
loc
,
nsInfos
)
;
LOG
.
info
(
+
i
+
+
new
File
(
pathUri
)
.
getAbsolutePath
(
)
)
;
}
assertEquals
(
totalVolumes
,
getNumVolumes
(
)
)
;
assertEquals
(
totalVolumes
,
dataset
.
storageMap
.
size
(
)
)
;
Set
<
String
>
actualVolumes
=
new
HashSet
<
String
>
(
)
;
try
(
FsDatasetSpi
.
FsVolumeReferences
volumes
=
dataset
.
getFsVolumeReferences
(
)
)
{
for
(
int
i
=
0
;
i
<
numNewVolumes
;
i
++
)
{
String
volumeName
=
volumes
.
get
(
numExistingVolumes
+
i
)
.
toString
(
)
;
volRemoveStartedLatch
.
await
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
e
)
;
}
LOG
.
info
(
)
;
dataset
.
getBlockReports
(
eb
.
getBlockPoolId
(
)
)
;
LOG
.
info
(
)
;
blockReportReceivedLatch
.
countDown
(
)
;
}
}
class
ResponderThread
extends
Thread
{
public
void
run
(
)
{
try
(
ReplicaHandler
replica
=
dataset
.
createRbw
(
StorageType
.
DEFAULT
,
null
,
eb
,
false
)
)
{
LOG
.
info
(
)
;
startFinalizeLatch
.
countDown
(
)
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
public
void
run
(
)
{
try
(
ReplicaHandler
replica
=
dataset
.
createRbw
(
StorageType
.
DEFAULT
,
null
,
eb
,
false
)
)
{
LOG
.
info
(
)
;
startFinalizeLatch
.
countDown
(
)
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
,
ie
)
;
}
blockReportReceivedLatch
.
await
(
)
;
dataset
.
finalizeBlock
(
eb
,
false
)
;
LOG
.
info
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
e
)
;
}
}
}
class
VolRemoveThread
extends
Thread
{
public
void
run
(
)
{
startFinalizeLatch
.
countDown
(
)
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
,
ie
)
;
}
blockReportReceivedLatch
.
await
(
)
;
dataset
.
finalizeBlock
(
eb
,
false
)
;
LOG
.
info
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
e
)
;
}
}
}
class
VolRemoveThread
extends
Thread
{
public
void
run
(
)
{
Set
<
StorageLocation
>
volumesToRemove
=
new
HashSet
<
>
(
)
;
try
{
volumesToRemove
.
add
(
dataset
.
getVolume
(
eb
)
.
getStorageLocation
(
)
)
;
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
,
ie
)
;
}
blockReportReceivedLatch
.
await
(
)
;
dataset
.
finalizeBlock
(
eb
,
false
)
;
LOG
.
info
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
e
)
;
}
}
}
class
VolRemoveThread
extends
Thread
{
public
void
run
(
)
{
Set
<
StorageLocation
>
volumesToRemove
=
new
HashSet
<
>
(
)
;
try
{
volumesToRemove
.
add
(
dataset
.
getVolume
(
eb
)
.
getStorageLocation
(
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
e
)
;
@
Test
(
timeout
=
30000
)
public
void
testMoveBlockFailure
(
)
{
MiniDFSCluster
cluster
=
null
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
1
)
.
storageTypes
(
new
StorageType
[
]
{
StorageType
.
DISK
,
StorageType
.
DISK
}
)
.
storagesPerDatanode
(
2
)
.
build
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
DataNode
dataNode
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
Path
filePath
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
filePath
,
100
,
(
short
)
1
,
0
)
;
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
filePath
)
;
FsDatasetImpl
fsDataSetImpl
=
(
FsDatasetImpl
)
dataNode
.
getFSDataset
(
)
;
ReplicaInfo
newReplicaInfo
=
createNewReplicaObj
(
block
,
fsDataSetImpl
)
;
FSDataOutputStream
out
=
fs
.
append
(
filePath
,
(
short
)
1
)
;
out
.
write
(
100
)
;
out
.
hflush
(
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
1
)
.
storageTypes
(
new
StorageType
[
]
{
StorageType
.
DISK
,
StorageType
.
DISK
}
)
.
storagesPerDatanode
(
2
)
.
build
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
DataNode
dataNode
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
Path
filePath
=
new
Path
(
)
;
DFSTestUtil
.
createFile
(
fs
,
filePath
,
100
,
(
short
)
1
,
0
)
;
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
filePath
)
;
FsDatasetImpl
fsDataSetImpl
=
(
FsDatasetImpl
)
dataNode
.
getFSDataset
(
)
;
ReplicaInfo
newReplicaInfo
=
createNewReplicaObj
(
block
,
fsDataSetImpl
)
;
FSDataOutputStream
out
=
fs
.
append
(
filePath
,
(
short
)
1
)
;
out
.
write
(
100
)
;
out
.
hflush
(
)
;
LOG
.
info
(
,
block
.
getGenerationStamp
(
)
)
;
LOG
.
info
(
,
fsDataSetImpl
.
getReplicaInfo
(
block
.
getBlockPoolId
(
)
,
newReplicaInfo
.
getBlockId
(
)
)
.
getGenerationStamp
(
)
)
;
LambdaTestUtils
.
intercept
(
IOException
.
class
,
+
,
(
)
->
fsDataSetImpl
.
finalizeNewReplica
(
newReplicaInfo
,
block
)
)
;
}
catch
(
Exception
ex
)
{
@
Test
public
void
testConcurrentRead
(
)
throws
Exception
{
getClusterBuilder
(
)
.
setRamDiskReplicaCapacity
(
2
)
.
build
(
)
;
final
String
METHOD_NAME
=
GenericTestUtils
.
getMethodName
(
)
;
final
Path
path1
=
new
Path
(
+
METHOD_NAME
+
)
;
final
int
SEED
=
0xFADED
;
final
int
NUM_TASKS
=
5
;
makeRandomTestFile
(
path1
,
BLOCK_SIZE
,
true
,
SEED
)
;
ensureFileReplicasOnStorageType
(
path1
,
RAM_DISK
)
;
final
CountDownLatch
latch
=
new
CountDownLatch
(
NUM_TASKS
)
;
final
AtomicBoolean
testFailed
=
new
AtomicBoolean
(
false
)
;
Runnable
readerRunnable
=
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
Assert
.
assertTrue
(
verifyReadRandomFile
(
path1
,
BLOCK_SIZE
,
SEED
)
)
;
}
catch
(
Throwable
e
)
{
private
void
waitForLockedBytesUsed
(
final
FsDatasetSpi
<
?
>
fsd
,
final
long
expectedLockedBytes
)
throws
TimeoutException
,
InterruptedException
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
long
cacheUsed
=
fsd
.
getCacheUsed
(
)
;
@
Test
(
timeout
=
60000
)
public
void
testCacheRecovery
(
)
throws
Exception
{
final
int
cacheBlocksNum
=
Ints
.
checkedCast
(
CACHE_AMOUNT
/
BLOCK_SIZE
)
;
BlockReaderTestUtil
.
enableHdfsCachingTracing
(
)
;
Assert
.
assertEquals
(
0
,
CACHE_AMOUNT
%
BLOCK_SIZE
)
;
final
Path
testFile
=
new
Path
(
)
;
final
long
testFileLen
=
cacheBlocksNum
*
BLOCK_SIZE
;
DFSTestUtil
.
createFile
(
fs
,
testFile
,
testFileLen
,
(
short
)
1
,
0xbeef
)
;
List
<
ExtendedBlockId
>
blockKeys
=
getExtendedBlockId
(
testFile
,
testFileLen
)
;
fs
.
addCachePool
(
new
CachePoolInfo
(
)
)
;
final
long
cacheDirectiveId
=
fs
.
addCacheDirective
(
new
CacheDirectiveInfo
.
Builder
(
)
.
setPool
(
)
.
setPath
(
testFile
)
.
setReplication
(
(
short
)
1
)
.
build
(
)
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
MetricsRecordBuilder
dnMetrics
=
getMetrics
(
dn
.
getMetrics
(
)
.
name
(
)
)
;
long
blocksCached
=
MetricsAsserts
.
getLongCounter
(
,
dnMetrics
)
;
if
(
blocksCached
!=
cacheBlocksNum
)
{
@
Test
public
void
testProvidedVolumeImpl
(
)
throws
IOException
{
assertEquals
(
NUM_LOCAL_INIT_VOLUMES
+
NUM_PROVIDED_INIT_VOLUMES
,
getNumVolumes
(
)
)
;
assertEquals
(
NUM_PROVIDED_INIT_VOLUMES
,
providedVolumes
.
size
(
)
)
;
assertEquals
(
0
,
dataset
.
getNumFailedVolumes
(
)
)
;
for
(
int
i
=
0
;
i
<
providedVolumes
.
size
(
)
;
i
++
)
{
assertEquals
(
DFSConfigKeys
.
DFS_PROVIDER_STORAGEUUID_DEFAULT
,
providedVolumes
.
get
(
i
)
.
getStorageID
(
)
)
;
assertEquals
(
StorageType
.
PROVIDED
,
providedVolumes
.
get
(
i
)
.
getStorageType
(
)
)
;
long
space
=
providedVolumes
.
get
(
i
)
.
getBlockPoolUsed
(
BLOCK_POOL_IDS
[
CHOSEN_BP_ID
]
)
;
assertEquals
(
spaceUsed
,
space
)
;
assertEquals
(
NUM_PROVIDED_BLKS
,
providedVolumes
.
get
(
i
)
.
getNumBlocks
(
)
)
;
providedVolumes
.
get
(
i
)
.
shutdownBlockPool
(
BLOCK_POOL_IDS
[
1
-
CHOSEN_BP_ID
]
,
null
)
;
try
{
assertEquals
(
0
,
providedVolumes
.
get
(
i
)
.
getBlockPoolUsed
(
BLOCK_POOL_IDS
[
1
-
CHOSEN_BP_ID
]
)
)
;
assertTrue
(
false
)
;
}
catch
(
IOException
e
)
{
try
(
FSDataOutputStream
fout
=
fs
.
create
(
file
)
)
{
fout
.
write
(
data
)
;
}
PathHandle
pathHandle
=
fs
.
getPathHandle
(
fs
.
getFileStatus
(
file
)
,
Options
.
HandleOpt
.
changed
(
true
)
,
Options
.
HandleOpt
.
moved
(
true
)
)
;
FinalizedProvidedReplica
replica
=
new
FinalizedProvidedReplica
(
0
,
file
.
toUri
(
)
,
0
,
chunkSize
,
0
,
pathHandle
,
null
,
conf
,
fs
)
;
byte
[
]
content
=
new
byte
[
chunkSize
]
;
IOUtils
.
readFully
(
replica
.
getDataInputStream
(
0
)
,
content
,
0
,
chunkSize
)
;
assertArrayEquals
(
data
,
content
)
;
fs
.
rename
(
file
,
new
Path
(
)
)
;
IOUtils
.
readFully
(
replica
.
getDataInputStream
(
0
)
,
content
,
0
,
chunkSize
)
;
assertArrayEquals
(
data
,
content
)
;
replica
.
setPathHandle
(
null
)
;
try
{
replica
.
getDataInputStream
(
0
)
;
fail
(
)
;
}
catch
(
IOException
e
)
{
DataNodeFaultInjector
.
set
(
new
DataNodeFaultInjector
(
)
{
private
int
tries
=
0
;
@
Override
public
void
failMirrorConnection
(
)
throws
IOException
{
if
(
tries
++
==
0
)
{
throw
new
IOException
(
)
;
}
}
}
)
;
FSDataOutputStream
os
=
fs
.
create
(
file
,
replication
)
;
os
.
write
(
new
byte
[
1
]
)
;
os
.
close
(
)
;
cluster
.
triggerBlockReports
(
)
;
for
(
final
DataNode
dn
:
cluster
.
getDataNodes
(
)
)
{
try
(
FsDatasetSpi
.
FsVolumeReferences
volumes
=
dn
.
getFSDataset
(
)
.
getFsVolumeReferences
(
)
)
{
final
FsVolumeImpl
volume
=
(
FsVolumeImpl
)
volumes
.
get
(
0
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
private
void
checkReservedSpace
(
final
long
expectedReserved
)
throws
TimeoutException
,
InterruptedException
,
IOException
{
for
(
final
DataNode
dn
:
cluster
.
getDataNodes
(
)
)
{
try
(
FsDatasetSpi
.
FsVolumeReferences
volumes
=
dn
.
getFSDataset
(
)
.
getFsVolumeReferences
(
)
)
{
final
FsVolumeImpl
volume
=
(
FsVolumeImpl
)
volumes
.
get
(
0
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
final
String
methodName
=
GenericTestUtils
.
getMethodName
(
)
;
final
Path
file
=
new
Path
(
+
methodName
+
)
;
FSDataOutputStream
os
=
fs
.
create
(
file
,
replication
)
;
os
.
write
(
new
byte
[
8192
]
)
;
os
.
hflush
(
)
;
os
.
close
(
)
;
HdfsBlockLocation
blockLocation
=
(
HdfsBlockLocation
)
fs
.
getClient
(
)
.
getBlockLocations
(
file
.
toString
(
)
,
0
,
BLOCK_SIZE
)
[
0
]
;
LocatedBlock
lastBlock
=
blockLocation
.
getLocatedBlock
(
)
;
cluster
.
stopDataNode
(
lastBlock
.
getLocations
(
)
[
2
]
.
getName
(
)
)
;
try
{
os
=
fs
.
append
(
file
)
;
DFSTestUtil
.
setPipeline
(
(
DFSOutputStream
)
os
.
getWrappedStream
(
)
,
lastBlock
)
;
os
.
writeBytes
(
)
;
os
.
hsync
(
)
;
}
catch
(
IOException
e
)
{
public
void
injectFastNodesSamples
(
DataNodePeerMetrics
peerMetrics
)
{
for
(
int
nodeIndex
=
0
;
nodeIndex
<
MIN_OUTLIER_DETECTION_PEERS
;
++
nodeIndex
)
{
final
String
nodeName
=
+
nodeIndex
;
@
Test
public
void
testOutliersFromTestMatrix
(
)
{
for
(
Map
.
Entry
<
Map
<
String
,
Double
>
,
Set
<
String
>>
entry
:
outlierTestMatrix
.
entrySet
(
)
)
{
final
int
dataNodeIndex
=
0
;
final
long
cap
=
blockSize
*
2L
*
blockCount
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_BLOCK_SIZE_KEY
,
blockSize
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_BYTES_PER_CHECKSUM_KEY
,
blockSize
)
;
final
MiniDFSCluster
cluster
=
new
ClusterBuilder
(
)
.
setBlockCount
(
blockCount
)
.
setBlockSize
(
blockSize
)
.
setDiskCount
(
diskCount
)
.
setNumDatanodes
(
dataNodeCount
)
.
setConf
(
conf
)
.
setCapacities
(
new
long
[
]
{
cap
,
cap
}
)
.
build
(
)
;
try
{
DataNode
node
=
cluster
.
getDataNodes
(
)
.
get
(
dataNodeIndex
)
;
final
FsDatasetSpi
<
?
>
fsDatasetSpy
=
Mockito
.
spy
(
node
.
getFSDataset
(
)
)
;
DiskBalancerWorkItem
item
=
Mockito
.
spy
(
new
DiskBalancerWorkItem
(
)
)
;
Mockito
.
doReturn
(
(
long
)
10
)
.
when
(
item
)
.
getBandwidth
(
)
;
doAnswer
(
new
Answer
<
Object
>
(
)
{
public
Object
answer
(
InvocationOnMock
invocation
)
{
try
{
node
.
getFSDataset
(
)
.
moveBlockAcrossVolumes
(
(
ExtendedBlock
)
invocation
.
getArguments
(
)
[
0
]
,
(
FsVolumeSpi
)
invocation
.
getArguments
(
)
[
1
]
)
;
}
catch
(
Exception
e
)
{
Path
dir
=
new
Path
(
)
;
dfs
.
mkdirs
(
dir
)
;
dfs
.
setStoragePolicy
(
dir
,
)
;
final
FSDataOutputStream
out
=
dfs
.
create
(
new
Path
(
file
)
)
;
byte
[
]
fileData
=
StripedFileTestUtil
.
generateBytes
(
DEFAULT_BLOCK_SIZE
*
3
)
;
out
.
write
(
fileData
)
;
out
.
close
(
)
;
LocatedBlock
lb
=
dfs
.
getClient
(
)
.
getLocatedBlocks
(
file
,
0
)
.
get
(
0
)
;
StorageType
[
]
storageTypes
=
lb
.
getStorageTypes
(
)
;
for
(
StorageType
storageType
:
storageTypes
)
{
Assert
.
assertTrue
(
StorageType
.
DISK
==
storageType
)
;
}
StorageType
[
]
[
]
newtypes
=
new
StorageType
[
]
[
]
{
{
StorageType
.
SSD
}
}
;
startAdditionalDNs
(
conf
,
1
,
newtypes
,
cluster
)
;
for
(
int
i
=
0
;
i
<
cluster
.
getDataNodes
(
)
.
size
(
)
;
i
++
)
{
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
i
)
;
InetSocketAddress
[
]
favoredNodes
=
new
InetSocketAddress
[
2
]
;
int
j
=
0
;
for
(
int
i
=
dataNodes
.
size
(
)
-
1
;
i
>=
2
;
i
--
)
{
favoredNodes
[
j
++
]
=
dataNodes
.
get
(
i
)
.
getXferAddress
(
)
;
}
final
String
file
=
;
final
FSDataOutputStream
out
=
dfs
.
create
(
new
Path
(
file
)
,
FsPermission
.
getDefault
(
)
,
true
,
DEFAULT_BLOCK_SIZE
,
(
short
)
2
,
DEFAULT_BLOCK_SIZE
,
null
,
favoredNodes
)
;
byte
[
]
fileData
=
StripedFileTestUtil
.
generateBytes
(
DEFAULT_BLOCK_SIZE
*
2
)
;
out
.
write
(
fileData
)
;
out
.
close
(
)
;
LocatedBlocks
locatedBlocks
=
dfs
.
getClient
(
)
.
getLocatedBlocks
(
file
,
0
)
;
Assert
.
assertEquals
(
,
2
,
locatedBlocks
.
locatedBlockCount
(
)
)
;
LocatedBlock
lb
=
locatedBlocks
.
get
(
0
)
;
DatanodeInfo
datanodeInfo
=
lb
.
getLocations
(
)
[
0
]
;
for
(
DataNode
dn
:
cluster
.
getDataNodes
(
)
)
{
if
(
dn
.
getDatanodeId
(
)
.
getDatanodeUuid
(
)
.
equals
(
datanodeInfo
.
getDatanodeUuid
(
)
)
)
{
static
void
banner
(
String
string
)
{
NamespaceScheme
nsScheme
=
new
NamespaceScheme
(
Arrays
.
asList
(
fooDir
)
,
null
,
BLOCK_SIZE
,
null
,
policyMap
)
;
ClusterScheme
clusterScheme
=
new
ClusterScheme
(
DEFAULT_CONF
,
NUM_DATANODES
,
REPL
,
genStorageTypes
(
NUM_DATANODES
)
,
null
)
;
MigrationTest
test
=
new
MigrationTest
(
clusterScheme
,
nsScheme
)
;
test
.
setupCluster
(
)
;
banner
(
)
;
final
Path
barFile
=
new
Path
(
fooDir
,
)
;
DFSTestUtil
.
createFile
(
test
.
dfs
,
barFile
,
BLOCK_SIZE
,
(
short
)
1
,
0L
)
;
FSDataOutputStream
out
=
test
.
dfs
.
append
(
barFile
)
;
out
.
writeBytes
(
)
;
(
(
DFSOutputStream
)
out
.
getWrappedStream
(
)
)
.
hsync
(
)
;
try
{
banner
(
)
;
test
.
setStoragePolicy
(
)
;
test
.
migrate
(
ExitStatus
.
SUCCESS
)
;
LocatedBlocks
lbs
=
test
.
dfs
.
getClient
(
)
.
getLocatedBlocks
(
barFile
.
toString
(
)
,
BLOCK_SIZE
)
;
(
(
DFSOutputStream
)
out
.
getWrappedStream
(
)
)
.
hsync
(
)
;
try
{
banner
(
)
;
test
.
setStoragePolicy
(
)
;
test
.
migrate
(
ExitStatus
.
SUCCESS
)
;
LocatedBlocks
lbs
=
test
.
dfs
.
getClient
(
)
.
getLocatedBlocks
(
barFile
.
toString
(
)
,
BLOCK_SIZE
)
;
LOG
.
info
(
+
lbs
)
;
List
<
LocatedBlock
>
blks
=
lbs
.
getLocatedBlocks
(
)
;
Assert
.
assertEquals
(
1
,
blks
.
size
(
)
)
;
Assert
.
assertEquals
(
1
,
blks
.
get
(
0
)
.
getLocations
(
)
.
length
)
;
banner
(
)
;
out
.
writeBytes
(
)
;
(
(
DFSOutputStream
)
out
.
getWrappedStream
(
)
)
.
hsync
(
)
;
IOUtils
.
cleanupWithLogger
(
LOG
,
out
)
;
lbs
=
test
.
dfs
.
getClient
(
)
.
getLocatedBlocks
(
barFile
.
toString
(
)
,
BLOCK_SIZE
)
;
private
void
waitForAllReplicas
(
int
expectedReplicaNum
,
Path
file
,
DistributedFileSystem
dfs
,
int
retryCount
)
throws
Exception
{
private
void
setVolumeFull
(
DataNode
dn
,
StorageType
type
)
{
try
(
FsDatasetSpi
.
FsVolumeReferences
refs
=
dn
.
getFSDataset
(
)
.
getFsVolumeReferences
(
)
)
{
for
(
FsVolumeSpi
fvs
:
refs
)
{
FsVolumeImpl
volume
=
(
FsVolumeImpl
)
fvs
;
if
(
volume
.
getStorageType
(
)
==
type
)
{
public
static
void
assertNNHasCheckpoints
(
MiniDFSCluster
cluster
,
int
nnIdx
,
List
<
Integer
>
txids
)
{
for
(
File
nameDir
:
getNameNodeCurrentDirs
(
cluster
,
nnIdx
)
)
{
LOG
.
info
(
+
Joiner
.
on
(
)
.
join
(
nameDir
.
listFiles
(
)
)
)
;
public
static
void
logStorageContents
(
Logger
log
,
NNStorage
storage
)
{
log
.
info
(
)
;
for
(
StorageDirectory
sd
:
storage
.
dirIterable
(
null
)
)
{
File
curDir
=
sd
.
getCurrentDir
(
)
;
public
static
void
logStorageContents
(
Logger
log
,
NNStorage
storage
)
{
log
.
info
(
)
;
for
(
StorageDirectory
sd
:
storage
.
dirIterable
(
null
)
)
{
File
curDir
=
sd
.
getCurrentDir
(
)
;
log
.
info
(
+
curDir
)
;
File
[
]
files
=
curDir
.
listFiles
(
)
;
Arrays
.
sort
(
files
)
;
for
(
File
f
:
files
)
{
static
void
setNameNodeLoggingLevel
(
Level
logLevel
)
{
@
Test
public
void
testRetryAddBlockWhileInChooseTarget
(
)
throws
Exception
{
final
String
src
=
;
final
FSNamesystem
ns
=
cluster
.
getNamesystem
(
)
;
final
NamenodeProtocols
nn
=
cluster
.
getNameNodeRpc
(
)
;
nn
.
create
(
src
,
FsPermission
.
getFileDefault
(
)
,
,
new
EnumSetWritable
<
CreateFlag
>
(
EnumSet
.
of
(
CreateFlag
.
CREATE
)
)
,
true
,
(
short
)
3
,
1024
,
null
,
null
,
null
)
;
final
FSNamesystem
ns
=
cluster
.
getNamesystem
(
)
;
final
NamenodeProtocols
nn
=
cluster
.
getNameNodeRpc
(
)
;
nn
.
create
(
src
,
FsPermission
.
getFileDefault
(
)
,
,
new
EnumSetWritable
<
CreateFlag
>
(
EnumSet
.
of
(
CreateFlag
.
CREATE
)
)
,
true
,
(
short
)
3
,
1024
,
null
,
null
,
null
)
;
LOG
.
info
(
+
src
)
;
LocatedBlock
[
]
onRetryBlock
=
new
LocatedBlock
[
1
]
;
ns
.
readLock
(
)
;
FSDirWriteFileOp
.
ValidateAddBlockResult
r
;
FSPermissionChecker
pc
=
Mockito
.
mock
(
FSPermissionChecker
.
class
)
;
try
{
r
=
FSDirWriteFileOp
.
validateAddBlock
(
ns
,
pc
,
src
,
HdfsConstants
.
GRANDFATHER_INODE_ID
,
,
null
,
onRetryBlock
)
;
}
finally
{
ns
.
readUnlock
(
)
;
;
}
DatanodeStorageInfo
targets
[
]
=
FSDirWriteFileOp
.
chooseTargetForNewBlock
(
ns
.
getBlockManager
(
)
,
src
,
null
,
null
,
null
,
r
)
;
assertNotNull
(
,
targets
)
;
@
Test
public
void
testAddBlockRetryShouldReturnBlockWithLocations
(
)
throws
Exception
{
final
String
src
=
;
NamenodeProtocols
nameNodeRpc
=
cluster
.
getNameNodeRpc
(
)
;
nameNodeRpc
.
create
(
src
,
FsPermission
.
getFileDefault
(
)
,
,
new
EnumSetWritable
<
CreateFlag
>
(
EnumSet
.
of
(
CreateFlag
.
CREATE
)
)
,
true
,
(
short
)
3
,
1024
,
null
,
null
,
null
)
;
config
.
setBoolean
(
DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY
,
true
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
config
)
.
manageDataDfsDirs
(
false
)
.
manageNameDfsDirs
(
false
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
assertNotNull
(
cluster
)
;
nn
=
cluster
.
getNameNode
(
)
;
assertNotNull
(
nn
)
;
LOG
.
info
(
)
;
LOG
.
info
(
)
;
config
.
setBoolean
(
DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY
,
false
)
;
try
{
cluster
.
shutdown
(
)
;
NameNode
.
format
(
config
)
;
fail
(
)
;
}
catch
(
IOException
e
)
{
assertTrue
(
,
e
.
getMessage
(
)
.
startsWith
(
+
DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY
)
)
;
@
Test
public
void
testAuditLoggerWithCallContext
(
)
throws
IOException
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
conf
.
setBoolean
(
HADOOP_CALLER_CONTEXT_ENABLED_KEY
,
true
)
;
conf
.
setInt
(
HADOOP_CALLER_CONTEXT_MAX_SIZE_KEY
,
128
)
;
conf
.
setInt
(
HADOOP_CALLER_CONTEXT_SIGNATURE_MAX_SIZE_KEY
,
40
)
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
build
(
)
;
LogCapturer
auditlog
=
LogCapturer
.
captureLogs
(
FSNamesystem
.
auditLog
)
;
try
{
cluster
.
waitClusterUp
(
)
;
final
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
final
long
time
=
System
.
currentTimeMillis
(
)
;
final
Path
p
=
new
Path
(
)
;
assertNull
(
CallerContext
.
getCurrent
(
)
)
;
CallerContext
context
=
new
CallerContext
.
Builder
(
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LogCapturer
auditlog
=
LogCapturer
.
captureLogs
(
FSNamesystem
.
auditLog
)
;
try
{
cluster
.
waitClusterUp
(
)
;
final
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
final
long
time
=
System
.
currentTimeMillis
(
)
;
final
Path
p
=
new
Path
(
)
;
assertNull
(
CallerContext
.
getCurrent
(
)
)
;
CallerContext
context
=
new
CallerContext
.
Builder
(
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
setTimes
(
p
,
time
,
time
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
CallerContext
context
=
new
CallerContext
.
Builder
(
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
setTimes
(
p
,
time
,
time
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
setTimes
(
p
,
time
,
time
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
final
String
longContext
=
StringUtils
.
repeat
(
,
100
)
;
context
=
new
CallerContext
.
Builder
(
longContext
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
final
String
longContext
=
StringUtils
.
repeat
(
,
100
)
;
context
=
new
CallerContext
.
Builder
(
longContext
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
setTimes
(
p
,
time
,
time
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
,
longContext
.
substring
(
0
,
128
)
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
)
;
fs
.
setTimes
(
p
,
time
,
time
)
;
assertFalse
(
auditlog
.
getOutput
(
)
.
contains
(
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
Thread
child
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
fs
.
setTimes
(
p
,
time
,
time
)
;
}
catch
(
IOException
e
)
{
fail
(
+
e
)
;
}
}
}
)
;
child
.
start
(
)
;
try
{
child
.
join
(
)
;
}
catch
(
InterruptedException
ignored
)
{
}
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
final
CallerContext
childContext
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
}
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
final
CallerContext
childContext
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
.
getBytes
(
CallerContext
.
SIGNATURE_ENCODING
)
)
.
build
(
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
child
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
CallerContext
.
setCurrent
(
childContext
)
;
fs
.
setPermission
(
p
,
new
FsPermission
(
(
short
)
777
)
)
;
}
catch
(
IOException
e
)
{
fail
(
+
e
)
;
}
}
}
)
;
child
.
start
(
)
;
try
{
child
.
join
(
)
;
@
Override
public
void
run
(
)
{
try
{
CallerContext
.
setCurrent
(
childContext
)
;
fs
.
setPermission
(
p
,
new
FsPermission
(
(
short
)
777
)
)
;
}
catch
(
IOException
e
)
{
fail
(
+
e
)
;
}
}
}
)
;
child
.
start
(
)
;
try
{
child
.
join
(
)
;
}
catch
(
InterruptedException
ignored
)
{
}
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
CallerContext
.
getCurrent
(
)
.
getSignature
(
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
fail
(
+
e
)
;
}
}
}
)
;
child
.
start
(
)
;
try
{
child
.
join
(
)
;
}
catch
(
InterruptedException
ignored
)
{
}
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
CallerContext
.
getCurrent
(
)
.
getSignature
(
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
mkdirs
(
new
Path
(
)
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
new
byte
[
41
]
)
.
build
(
)
;
try
{
child
.
join
(
)
;
}
catch
(
InterruptedException
ignored
)
{
}
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
CallerContext
.
getCurrent
(
)
.
getSignature
(
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
mkdirs
(
new
Path
(
)
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
new
byte
[
41
]
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
setTimes
(
p
,
time
,
time
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
CallerContext
.
getCurrent
(
)
.
getSignature
(
)
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
mkdirs
(
new
Path
(
)
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
new
byte
[
41
]
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
LOG
.
info
(
,
CallerContext
.
getCurrent
(
)
)
;
fs
.
setTimes
(
p
,
time
,
time
)
;
assertTrue
(
auditlog
.
getOutput
(
)
.
endsWith
(
String
.
format
(
)
)
)
;
auditlog
.
clearOutput
(
)
;
context
=
new
CallerContext
.
Builder
(
)
.
setSignature
(
null
)
.
build
(
)
;
CallerContext
.
setCurrent
(
context
)
;
void
waitCheckpointDone
(
MiniDFSCluster
cluster
,
long
txid
)
{
long
thisCheckpointTxId
;
do
{
try
{
try
{
Configuration
nnconf
=
new
HdfsConfiguration
(
c
)
;
DFSTestUtil
.
formatNameNode
(
nnconf
)
;
nn
=
NameNode
.
createNameNode
(
new
String
[
]
{
}
,
nnconf
)
;
}
catch
(
IOException
e
)
{
LOG
.
info
(
)
;
throw
e
;
}
c
.
set
(
CommonConfigurationKeysPublic
.
HADOOP_SECURITY_AUTHENTICATION
,
)
;
c
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_KEYTAB_FILE_KEY
,
)
;
BackupNode
bn
=
null
;
try
{
bn
=
(
BackupNode
)
NameNode
.
createNameNode
(
new
String
[
]
{
startupOpt
.
getName
(
)
}
,
c
)
;
assertTrue
(
,
bn
.
getNamesystem
(
)
==
null
)
;
fail
(
)
;
}
catch
(
IOException
e
)
{
private
void
testBNInSync
(
MiniDFSCluster
cluster
,
final
BackupNode
backup
,
int
testIdx
)
throws
Exception
{
final
NameNode
nn
=
cluster
.
getNameNode
(
)
;
final
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
for
(
int
i
=
0
;
i
<
10
;
i
++
)
{
final
String
src
=
+
testIdx
+
+
i
;
private
void
testBNInSync
(
MiniDFSCluster
cluster
,
final
BackupNode
backup
,
int
testIdx
)
throws
Exception
{
final
NameNode
nn
=
cluster
.
getNameNode
(
)
;
final
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
for
(
int
i
=
0
;
i
<
10
;
i
++
)
{
final
String
src
=
+
testIdx
+
+
i
;
LOG
.
info
(
+
src
+
)
;
Path
p
=
new
Path
(
src
)
;
assertTrue
(
fs
.
mkdirs
(
p
)
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
conf
.
setInt
(
DFSConfigKeys
.
DFS_DATANODE_SCAN_PERIOD_HOURS_KEY
,
-
1
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_CHECKPOINT_TXNS_KEY
,
1
)
;
MiniDFSCluster
cluster
=
null
;
FileSystem
fileSys
=
null
;
BackupNode
backup
=
null
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
0
)
.
build
(
)
;
fileSys
=
cluster
.
getFileSystem
(
)
;
assertTrue
(
!
fileSys
.
exists
(
file1
)
)
;
assertTrue
(
!
fileSys
.
exists
(
file2
)
)
;
assertTrue
(
fileSys
.
mkdirs
(
file1
)
)
;
long
txid
=
cluster
.
getNameNodeRpc
(
)
.
getTransactionID
(
)
;
backup
=
startBackupNode
(
conf
,
op
,
1
)
;
waitCheckpointDone
(
cluster
,
txid
)
;
}
catch
(
IOException
e
)
{
fileSys
.
mkdirs
(
file2
)
;
long
txid
=
cluster
.
getNameNodeRpc
(
)
.
getTransactionID
(
)
;
backup
=
startBackupNode
(
conf
,
op
,
1
)
;
waitCheckpointDone
(
cluster
,
txid
)
;
for
(
int
i
=
0
;
i
<
10
;
i
++
)
{
fileSys
.
mkdirs
(
new
Path
(
+
i
)
)
;
}
txid
=
cluster
.
getNameNodeRpc
(
)
.
getTransactionID
(
)
;
backup
.
doCheckpoint
(
)
;
waitCheckpointDone
(
cluster
,
txid
)
;
txid
=
cluster
.
getNameNodeRpc
(
)
.
getTransactionID
(
)
;
backup
.
doCheckpoint
(
)
;
waitCheckpointDone
(
cluster
,
txid
)
;
InetSocketAddress
add
=
backup
.
getNameNodeAddress
(
)
;
FileSystem
bnFS
=
FileSystem
.
get
(
new
Path
(
+
NetUtils
.
getHostPortString
(
add
)
)
.
toUri
(
)
,
conf
)
;
boolean
canWrite
=
true
;
txid
=
cluster
.
getNameNodeRpc
(
)
.
getTransactionID
(
)
;
backup
.
doCheckpoint
(
)
;
waitCheckpointDone
(
cluster
,
txid
)
;
txid
=
cluster
.
getNameNodeRpc
(
)
.
getTransactionID
(
)
;
backup
.
doCheckpoint
(
)
;
waitCheckpointDone
(
cluster
,
txid
)
;
InetSocketAddress
add
=
backup
.
getNameNodeAddress
(
)
;
FileSystem
bnFS
=
FileSystem
.
get
(
new
Path
(
+
NetUtils
.
getHostPortString
(
add
)
)
.
toUri
(
)
,
conf
)
;
boolean
canWrite
=
true
;
try
{
DFSTestUtil
.
createFile
(
bnFS
,
file3
,
fileSize
,
fileSize
,
blockSize
,
replication
,
seed
)
;
}
catch
(
IOException
eio
)
{
LOG
.
info
(
+
backup
.
getRole
(
)
+
,
eio
)
;
canWrite
=
false
;
}
assertFalse
(
,
canWrite
)
;
}
catch
(
IOException
eio
)
{
LOG
.
info
(
+
backup
.
getRole
(
)
+
,
eio
)
;
canRead
=
false
;
}
assertEquals
(
,
canRead
,
backup
.
isRole
(
NamenodeRole
.
BACKUP
)
)
;
DFSTestUtil
.
createFile
(
fileSys
,
file3
,
fileSize
,
fileSize
,
blockSize
,
replication
,
seed
)
;
TestCheckpoint
.
checkFile
(
fileSys
,
file3
,
replication
)
;
assertTrue
(
,
op
!=
StartupOption
.
BACKUP
||
backup
.
getNamesystem
(
)
.
getFileInfo
(
file3
.
toUri
(
)
.
getPath
(
)
,
false
,
false
,
false
)
!=
null
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
AssertionError
(
e
)
;
}
finally
{
if
(
backup
!=
null
)
backup
.
stop
(
)
;
if
(
fileSys
!=
null
)
fileSys
.
close
(
)
;
if
(
cluster
!=
null
)
cluster
.
shutdown
(
)
;
}
FSImageTestUtil
.
assertParallelFilesAreIdentical
(
ImmutableList
.
of
(
bnCurDir
,
nnCurDir
)
,
ImmutableSet
.
<
String
>
of
(
)
)
;
String
nnAddr
=
cluster
.
getNameNode
(
)
.
getNameNodeAddressHostPortString
(
)
;
conf
.
get
(
DFSConfigKeys
.
DFS_NAMENODE_RPC_ADDRESS_KEY
)
;
String
bnAddr
=
backup
.
getNameNodeAddressHostPortString
(
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_NAMESERVICES
,
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_NAMESERVICE_ID
,
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_HA_NAMENODES_KEY_PREFIX
+
,
)
;
conf
.
set
(
rpcAddrKeyPreffix
+
,
nnAddr
)
;
conf
.
set
(
rpcAddrKeyPreffix
+
,
bnAddr
)
;
cluster
.
startDataNodes
(
conf
,
3
,
true
,
StartupOption
.
REGULAR
,
null
)
;
DFSTestUtil
.
createFile
(
fileSys
,
file1
,
fileSize
,
fileSize
,
blockSize
,
(
short
)
3
,
seed
)
;
FileSystem
bnFS
=
FileSystem
.
get
(
new
Path
(
+
bnAddr
)
.
toUri
(
)
,
conf
)
;
String
nnData
=
DFSTestUtil
.
readFile
(
fileSys
,
file1
)
;
String
bnData
=
DFSTestUtil
.
readFile
(
bnFS
,
file1
)
;
assertEquals
(
,
nnData
,
bnData
)
;
}
catch
(
IOException
e
)
{
private
static
void
waitForCachedBlocks
(
NameNode
nn
,
final
int
expectedCachedBlocks
,
final
int
expectedCachedReplicas
,
final
String
logString
)
throws
Exception
{
final
FSNamesystem
namesystem
=
nn
.
getNamesystem
(
)
;
final
CacheManager
cacheManager
=
namesystem
.
getCacheManager
(
)
;
private
static
void
waitForCacheDirectiveStats
(
final
DistributedFileSystem
dfs
,
final
long
targetBytesNeeded
,
final
long
targetBytesCached
,
final
long
targetFilesNeeded
,
final
long
targetFilesCached
,
final
CacheDirectiveInfo
filter
,
final
String
infoString
)
throws
Exception
{
LOG
.
info
(
+
(
(
filter
==
null
)
?
:
filter
.
toString
(
)
)
+
+
targetBytesNeeded
+
+
targetBytesCached
+
+
targetFilesNeeded
+
+
targetFilesCached
+
)
;
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
RemoteIterator
<
CacheDirectiveEntry
>
iter
=
null
;
CacheDirectiveEntry
entry
=
null
;
try
{
iter
=
dfs
.
listCacheDirectives
(
filter
)
;
entry
=
iter
.
next
(
)
;
}
catch
(
IOException
e
)
{
fail
(
+
+
e
.
getMessage
(
)
)
;
}
Assert
.
assertNotNull
(
entry
)
;
CacheDirectiveStats
stats
=
entry
.
getStats
(
)
;
if
(
(
targetBytesNeeded
==
stats
.
getBytesNeeded
(
)
)
&&
(
targetBytesCached
==
stats
.
getBytesCached
(
)
)
&&
(
targetFilesNeeded
==
stats
.
getFilesNeeded
(
)
)
&&
(
targetFilesCached
==
stats
.
getFilesCached
(
)
)
)
{
return
true
;
}
else
{
private
static
void
waitForCachePoolStats
(
final
DistributedFileSystem
dfs
,
final
long
targetBytesNeeded
,
final
long
targetBytesCached
,
final
long
targetFilesNeeded
,
final
long
targetFilesCached
,
final
CachePoolInfo
pool
,
final
String
infoString
)
throws
Exception
{
}
while
(
true
)
{
CachePoolEntry
entry
=
null
;
try
{
if
(
!
iter
.
hasNext
(
)
)
{
break
;
}
entry
=
iter
.
next
(
)
;
}
catch
(
IOException
e
)
{
fail
(
+
+
e
.
getMessage
(
)
)
;
}
if
(
entry
==
null
)
{
break
;
}
if
(
!
entry
.
getInfo
(
)
.
getPoolName
(
)
.
equals
(
pool
.
getPoolName
(
)
)
)
{
continue
;
}
CachePoolStats
stats
=
entry
.
getStats
(
)
;
if
(
(
targetBytesNeeded
==
stats
.
getBytesNeeded
(
)
)
&&
(
targetBytesCached
==
stats
.
getBytesCached
(
)
)
&&
(
targetFilesNeeded
==
stats
.
getFilesNeeded
(
)
)
&&
(
targetFilesCached
==
stats
.
getFilesCached
(
)
)
)
{
return
true
;
private
static
void
checkNumCachedReplicas
(
final
DistributedFileSystem
dfs
,
final
List
<
Path
>
paths
,
final
int
expectedBlocks
,
final
int
expectedReplicas
)
throws
Exception
{
int
numCachedBlocks
=
0
;
int
numCachedReplicas
=
0
;
for
(
Path
p
:
paths
)
{
final
FileStatus
f
=
dfs
.
getFileStatus
(
p
)
;
final
long
len
=
f
.
getLen
(
)
;
final
long
blockSize
=
f
.
getBlockSize
(
)
;
final
long
numBlocks
=
(
len
+
blockSize
-
1
)
/
blockSize
;
BlockLocation
[
]
locs
=
dfs
.
getFileBlockLocations
(
p
,
0
,
len
)
;
assertEquals
(
+
p
,
numBlocks
,
locs
.
length
)
;
for
(
BlockLocation
l
:
locs
)
{
if
(
l
.
getCachedHosts
(
)
.
length
>
0
)
{
numCachedBlocks
++
;
}
numCachedReplicas
+=
l
.
getCachedHosts
(
)
.
length
;
}
}
LOG
.
info
(
+
numCachedBlocks
+
+
expectedBlocks
+
)
;
Configuration
conf
=
new
HdfsConfiguration
(
)
;
MiniDFSCluster
cluster
=
null
;
SecondaryNameNode
secondary
=
null
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
0
)
.
build
(
)
;
StorageDirectory
savedSd
=
null
;
secondary
=
startSecondaryNameNode
(
conf
)
;
NNStorage
storage
=
secondary
.
getFSImage
(
)
.
getStorage
(
)
;
for
(
StorageDirectory
sd
:
storage
.
dirIterable
(
null
)
)
{
assertLockFails
(
sd
)
;
savedSd
=
sd
;
}
LOG
.
info
(
)
;
secondary
.
shutdown
(
)
;
secondary
=
null
;
LOG
.
info
(
)
;
Configuration
conf
=
new
HdfsConfiguration
(
)
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
numDatanodes
)
.
format
(
true
)
.
build
(
)
;
secondary
=
startSecondaryNameNode
(
conf
)
;
Mockito
.
doThrow
(
new
IOException
(
)
)
.
when
(
faultInjector
)
.
afterSecondaryCallsRollEditLog
(
)
;
try
{
secondary
.
doCheckpoint
(
)
;
fail
(
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
info
(
,
ioe
)
;
assertTrue
(
ioe
.
toString
(
)
.
contains
(
)
)
;
}
try
{
secondary
.
doCheckpoint
(
)
;
fail
(
)
;
}
catch
(
IOException
ioe
)
{
Configuration
snnConf
=
new
Configuration
(
conf
)
;
File
checkpointDir
=
new
File
(
MiniDFSCluster
.
getBaseDirectory
(
)
,
)
;
snnConf
.
set
(
DFSConfigKeys
.
DFS_NAMENODE_CHECKPOINT_DIR_KEY
,
checkpointDir
.
getAbsolutePath
(
)
)
;
secondary
=
startSecondaryNameNode
(
snnConf
)
;
secondary
.
doCheckpoint
(
)
;
cluster
.
shutdown
(
)
;
cluster
=
null
;
try
{
Thread
.
sleep
(
100
)
;
}
catch
(
InterruptedException
ie
)
{
}
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
0
)
.
nameNodePort
(
origPort
)
.
nameNodeHttpPort
(
origHttpPort
)
.
format
(
true
)
.
build
(
)
;
try
{
secondary
.
doCheckpoint
(
)
;
fail
(
)
;
}
catch
(
IOException
ioe
)
{
private
void
testDeleteAndCommitBlockSynchronizationRace
(
boolean
hasSnapshot
)
throws
Exception
{
testList
.
add
(
new
AbstractMap
.
SimpleImmutableEntry
<
String
,
Boolean
>
(
,
true
)
)
;
final
Path
rootPath
=
new
Path
(
)
;
final
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setBoolean
(
DFSConfigKeys
.
DFS_PERMISSIONS_ENABLED_KEY
,
false
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_BLOCK_SIZE_KEY
,
BLOCK_SIZE
)
;
FSDataOutputStream
stm
=
null
;
Map
<
DataNode
,
DatanodeProtocolClientSideTranslatorPB
>
dnMap
=
new
HashMap
<
DataNode
,
DatanodeProtocolClientSideTranslatorPB
>
(
)
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
3
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
DistributedFileSystem
fs
=
cluster
.
getFileSystem
(
)
;
int
stId
=
0
;
for
(
AbstractMap
.
SimpleImmutableEntry
<
String
,
Boolean
>
stest
:
testList
)
{
String
testPath
=
stest
.
getKey
(
)
;
Boolean
mkSameDir
=
stest
.
getValue
(
)
;
Map
<
DataNode
,
DatanodeProtocolClientSideTranslatorPB
>
dnMap
=
new
HashMap
<
DataNode
,
DatanodeProtocolClientSideTranslatorPB
>
(
)
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
3
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
DistributedFileSystem
fs
=
cluster
.
getFileSystem
(
)
;
int
stId
=
0
;
for
(
AbstractMap
.
SimpleImmutableEntry
<
String
,
Boolean
>
stest
:
testList
)
{
String
testPath
=
stest
.
getKey
(
)
;
Boolean
mkSameDir
=
stest
.
getValue
(
)
;
LOG
.
info
(
+
testPath
+
+
mkSameDir
+
+
hasSnapshot
)
;
Path
fPath
=
new
Path
(
testPath
)
;
Path
grandestNonRootParent
=
fPath
;
while
(
!
grandestNonRootParent
.
getParent
(
)
.
equals
(
rootPath
)
)
{
grandestNonRootParent
=
grandestNonRootParent
.
getParent
(
)
;
}
stm
=
fs
.
create
(
fPath
)
;
LOG
.
info
(
+
testPath
+
+
mkSameDir
+
+
hasSnapshot
)
;
Path
fPath
=
new
Path
(
testPath
)
;
Path
grandestNonRootParent
=
fPath
;
while
(
!
grandestNonRootParent
.
getParent
(
)
.
equals
(
rootPath
)
)
{
grandestNonRootParent
=
grandestNonRootParent
.
getParent
(
)
;
}
stm
=
fs
.
create
(
fPath
)
;
LOG
.
info
(
+
testPath
+
+
fPath
)
;
AppendTestUtil
.
write
(
stm
,
0
,
BLOCK_SIZE
/
2
)
;
stm
.
hflush
(
)
;
if
(
hasSnapshot
)
{
SnapshotTestHelper
.
createSnapshot
(
fs
,
rootPath
,
+
String
.
valueOf
(
stId
)
)
;
++
stId
;
}
NameNode
nn
=
cluster
.
getNameNode
(
)
;
ExtendedBlock
blk
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fPath
)
;
DatanodeDescriptor
expectedPrimary
=
DFSTestUtil
.
getExpectedPrimaryNode
(
nn
,
blk
)
;
NameNode
nn
=
cluster
.
getNameNode
(
)
;
ExtendedBlock
blk
=
DFSTestUtil
.
getFirstBlock
(
fs
,
fPath
)
;
DatanodeDescriptor
expectedPrimary
=
DFSTestUtil
.
getExpectedPrimaryNode
(
nn
,
blk
)
;
LOG
.
info
(
+
expectedPrimary
)
;
DataNode
primaryDN
=
cluster
.
getDataNode
(
expectedPrimary
.
getIpcPort
(
)
)
;
DatanodeProtocolClientSideTranslatorPB
nnSpy
=
dnMap
.
get
(
primaryDN
)
;
if
(
nnSpy
==
null
)
{
nnSpy
=
InternalDataNodeTestUtils
.
spyOnBposToNN
(
primaryDN
,
nn
)
;
dnMap
.
put
(
primaryDN
,
nnSpy
)
;
}
DelayAnswer
delayer
=
new
DelayAnswer
(
LOG
)
;
Mockito
.
doAnswer
(
delayer
)
.
when
(
nnSpy
)
.
commitBlockSynchronization
(
Mockito
.
eq
(
blk
)
,
Mockito
.
anyLong
(
)
,
Mockito
.
anyLong
(
)
,
Mockito
.
eq
(
true
)
,
Mockito
.
eq
(
false
)
,
Mockito
.
any
(
)
,
Mockito
.
any
(
)
)
;
fs
.
recoverLease
(
fPath
)
;
LOG
.
info
(
)
;
delayer
.
waitForCall
(
)
;
LOG
.
info
(
+
grandestNonRootParent
)
;
@
Test
public
void
testDeleteAndLeaseRecoveryHardLimitSnapshot
(
)
throws
Exception
{
final
Path
rootPath
=
new
Path
(
)
;
final
Configuration
config
=
new
Configuration
(
)
;
config
.
setBoolean
(
DFSConfigKeys
.
DFS_PERMISSIONS_ENABLED_KEY
,
false
)
;
config
.
setInt
(
DFSConfigKeys
.
DFS_BLOCK_SIZE_KEY
,
BLOCK_SIZE
)
;
FSDataOutputStream
stm
=
null
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
config
)
.
numDataNodes
(
3
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
final
DistributedFileSystem
fs
=
cluster
.
getFileSystem
(
)
;
final
Path
testPath
=
new
Path
(
)
;
stm
=
fs
.
create
(
testPath
)
;
fileSys
=
cluster
.
getFileSystem
(
)
;
final
FSNamesystem
namesystem
=
cluster
.
getNamesystem
(
)
;
FSImage
fsimage
=
namesystem
.
getFSImage
(
)
;
final
FSEditLog
editLog
=
fsimage
.
getEditLog
(
)
;
fileSys
.
mkdirs
(
new
Path
(
)
)
;
Iterator
<
StorageDirectory
>
iter
=
fsimage
.
getStorage
(
)
.
dirIterator
(
NameNodeDirType
.
EDITS
)
;
LinkedList
<
StorageDirectory
>
sds
=
new
LinkedList
<
StorageDirectory
>
(
)
;
while
(
iter
.
hasNext
(
)
)
{
sds
.
add
(
iter
.
next
(
)
)
;
}
editLog
.
close
(
)
;
cluster
.
shutdown
(
)
;
for
(
StorageDirectory
sd
:
sds
)
{
File
editFile
=
NNStorage
.
getFinalizedEditsFile
(
sd
,
1
,
3
)
;
assertTrue
(
editFile
.
exists
(
)
)
;
long
fileLen
=
editFile
.
length
(
)
;
stream
.
create
(
NameNodeLayoutVersion
.
CURRENT_LAYOUT_VERSION
)
;
if
(
!
inBothDirs
)
{
break
;
}
NNStorage
storage
=
new
NNStorage
(
conf
,
Collections
.
<
URI
>
emptyList
(
)
,
Lists
.
newArrayList
(
uri
)
)
;
if
(
updateTransactionIdFile
)
{
storage
.
writeTransactionIdFileToStorage
(
3
)
;
}
storage
.
close
(
)
;
}
finally
{
stream
.
close
(
)
;
}
}
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
NUM_DATA_NODES
)
.
format
(
false
)
.
build
(
)
;
if
(
!
shouldSucceed
)
{
fail
(
)
;
}
}
catch
(
IOException
ioe
)
{
if
(
shouldSucceed
)
{
@
Test
public
void
testAlternatingJournalFailure
(
)
throws
IOException
{
File
f1
=
new
File
(
TEST_DIR
+
)
;
File
f2
=
new
File
(
TEST_DIR
+
)
;
List
<
URI
>
editUris
=
ImmutableList
.
of
(
f1
.
toURI
(
)
,
f2
.
toURI
(
)
)
;
NNStorage
storage
=
setupEdits
(
editUris
,
10
,
new
AbortSpec
(
1
,
0
)
,
new
AbortSpec
(
2
,
1
)
,
new
AbortSpec
(
3
,
0
)
,
new
AbortSpec
(
4
,
1
)
,
new
AbortSpec
(
5
,
0
)
,
new
AbortSpec
(
6
,
1
)
,
new
AbortSpec
(
7
,
0
)
,
new
AbortSpec
(
8
,
1
)
,
new
AbortSpec
(
9
,
0
)
,
new
AbortSpec
(
10
,
1
)
)
;
long
totaltxnread
=
0
;
FSEditLog
editlog
=
getFSEditLog
(
storage
)
;
editlog
.
initJournalsForWrite
(
)
;
long
startTxId
=
1
;
Iterable
<
EditLogInputStream
>
editStreams
=
editlog
.
selectInputStreams
(
startTxId
,
TXNS_PER_ROLL
*
11
)
;
for
(
EditLogInputStream
edits
:
editStreams
)
{
FSEditLogLoader
.
EditLogValidation
val
=
FSEditLogLoader
.
scanEditLog
(
edits
,
Long
.
MAX_VALUE
)
;
long
read
=
(
val
.
getEndTxId
(
)
-
edits
.
getFirstTxId
(
)
)
+
1
;
@
Override
public
boolean
accept
(
File
dir
,
String
name
)
{
if
(
name
.
startsWith
(
NNStorage
.
getFinalizedEditsFileName
(
startErrorTxId
,
endErrorTxId
)
)
)
{
return
true
;
}
return
false
;
}
}
)
;
assertEquals
(
1
,
files
.
length
)
;
assertTrue
(
files
[
0
]
.
delete
(
)
)
;
FSEditLog
editlog
=
getFSEditLog
(
storage
)
;
editlog
.
initJournalsForWrite
(
)
;
long
startTxId
=
1
;
Collection
<
EditLogInputStream
>
streams
=
null
;
try
{
streams
=
editlog
.
selectInputStreams
(
startTxId
,
4
*
TXNS_PER_ROLL
)
;
readAllEdits
(
streams
,
startTxId
)
;
}
catch
(
IOException
e
)
{
@
Test
public
void
testEditLogFailOverFromCorrupt
(
)
throws
IOException
{
File
f1
=
new
File
(
TEST_DIR
+
)
;
File
f2
=
new
File
(
TEST_DIR
+
)
;
List
<
URI
>
editUris
=
ImmutableList
.
of
(
f1
.
toURI
(
)
,
f2
.
toURI
(
)
)
;
NNStorage
storage
=
setupEdits
(
editUris
,
3
)
;
final
long
startErrorTxId
=
1
*
TXNS_PER_ROLL
+
1
;
final
long
endErrorTxId
=
2
*
TXNS_PER_ROLL
;
File
[
]
files
=
new
File
(
f1
,
)
.
listFiles
(
new
FilenameFilter
(
)
{
@
Override
public
boolean
accept
(
File
dir
,
String
name
)
{
if
(
name
.
startsWith
(
NNStorage
.
getFinalizedEditsFileName
(
startErrorTxId
,
endErrorTxId
)
)
)
{
return
true
;
}
return
false
;
}
}
)
;
assertEquals
(
1
,
files
.
length
)
;
long
fileLen
=
files
[
0
]
.
length
(
)
;
editLog
.
rollEditLog
(
NameNodeLayoutVersion
.
CURRENT_LAYOUT_VERSION
)
;
assertExistsInStorageDirs
(
cluster
,
NameNodeDirType
.
EDITS
,
NNStorage
.
getFinalizedEditsFileName
(
(
i
*
3
)
+
1
,
(
i
*
3
)
+
3
)
)
;
}
editLog
.
close
(
)
;
}
finally
{
if
(
fileSys
!=
null
)
fileSys
.
close
(
)
;
if
(
cluster
!=
null
)
cluster
.
shutdown
(
)
;
}
long
startTime
=
Time
.
now
(
)
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
NUM_DATA_NODES
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
}
finally
{
if
(
cluster
!=
null
)
{
cluster
.
shutdown
(
)
;
}
}
long
endTime
=
Time
.
now
(
)
;
double
delta
=
(
(
float
)
(
endTime
-
startTime
)
)
/
1000.0
;
int
retryCount
=
0
;
while
(
true
)
{
try
{
int
basePort
=
10060
+
random
.
nextInt
(
100
)
*
2
;
MiniDFSNNTopology
topology
=
new
MiniDFSNNTopology
(
)
.
addNameservice
(
new
MiniDFSNNTopology
.
NSConf
(
)
.
addNN
(
new
MiniDFSNNTopology
.
NNConf
(
)
.
setHttpPort
(
basePort
)
)
.
addNN
(
new
MiniDFSNNTopology
.
NNConf
(
)
.
setHttpPort
(
basePort
+
1
)
)
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
topology
)
.
numDataNodes
(
0
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
nn0
=
cluster
.
getNameNode
(
0
)
;
fs
=
HATestUtil
.
configureFailoverFs
(
cluster
,
conf
)
;
cluster
.
transitionToActive
(
0
)
;
fs
=
cluster
.
getFileSystem
(
0
)
;
editLog
=
nn0
.
getNamesystem
(
)
.
getEditLog
(
)
;
++
retryCount
;
break
;
}
catch
(
BindException
e
)
{
@
Test
(
timeout
=
60000
)
public
void
testScanCorruptEditLog
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
File
editLog
=
new
File
(
GenericTestUtils
.
getTempPath
(
)
)
;
PermissionStatus
perms
=
PermissionStatus
.
createImmutable
(
,
,
FsPermission
.
createImmutable
(
(
short
)
0777
)
)
;
mkdirOp
.
setPermissionStatus
(
perms
)
;
elos
.
write
(
mkdirOp
)
;
mkdirOp
.
reset
(
)
;
mkdirOp
.
setRpcCallId
(
456
)
;
mkdirOp
.
setTransactionId
(
2
)
;
mkdirOp
.
setInodeId
(
123L
)
;
mkdirOp
.
setPath
(
)
;
perms
=
PermissionStatus
.
createImmutable
(
,
,
FsPermission
.
createImmutable
(
(
short
)
0666
)
)
;
mkdirOp
.
setPermissionStatus
(
perms
)
;
elos
.
write
(
mkdirOp
)
;
elos
.
setReadyToFlush
(
)
;
elos
.
flushAndSync
(
false
)
;
elos
.
close
(
)
;
long
fileLen
=
editLog
.
length
(
)
;
elos
.
write
(
mkdirOp
)
;
elos
.
setReadyToFlush
(
)
;
elos
.
flushAndSync
(
false
)
;
elos
.
close
(
)
;
long
fileLen
=
editLog
.
length
(
)
;
LOG
.
debug
(
+
editLog
+
+
fileLen
)
;
RandomAccessFile
rwf
=
new
RandomAccessFile
(
editLog
,
)
;
rwf
.
seek
(
fileLen
-
4
)
;
int
b
=
rwf
.
readInt
(
)
;
rwf
.
seek
(
fileLen
-
4
)
;
rwf
.
writeInt
(
b
+
1
)
;
rwf
.
close
(
)
;
EditLogFileInputStream
elis
=
new
EditLogFileInputStream
(
editLog
)
;
Assert
.
assertEquals
(
NameNodeLayoutVersion
.
CURRENT_LAYOUT_VERSION
,
elis
.
getVersion
(
true
)
)
;
Assert
.
assertEquals
(
1
,
elis
.
scanNextOp
(
)
)
;
LOG
.
debug
(
+
editLog
+
+
fileLen
)
;
RandomAccessFile
rwf
=
new
RandomAccessFile
(
editLog
,
)
;
rwf
.
seek
(
fileLen
-
4
)
;
int
b
=
rwf
.
readInt
(
)
;
rwf
.
seek
(
fileLen
-
4
)
;
rwf
.
writeInt
(
b
+
1
)
;
rwf
.
close
(
)
;
EditLogFileInputStream
elis
=
new
EditLogFileInputStream
(
editLog
)
;
Assert
.
assertEquals
(
NameNodeLayoutVersion
.
CURRENT_LAYOUT_VERSION
,
elis
.
getVersion
(
true
)
)
;
Assert
.
assertEquals
(
1
,
elis
.
scanNextOp
(
)
)
;
LOG
.
debug
(
+
editLog
)
;
try
{
elis
.
scanNextOp
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IOException
e
)
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
NUM_DATA_NODES
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
fileSys
=
cluster
.
getFileSystem
(
)
;
final
FSNamesystem
namesystem
=
cluster
.
getNamesystem
(
)
;
FSImage
fsimage
=
namesystem
.
getFSImage
(
)
;
FSEditLog
editLog
=
fsimage
.
getEditLog
(
)
;
startTransactionWorkers
(
cluster
,
caughtErr
)
;
for
(
int
i
=
0
;
i
<
NUM_SAVE_IMAGE
&&
caughtErr
.
get
(
)
==
null
;
i
++
)
{
try
{
Thread
.
sleep
(
20
)
;
}
catch
(
InterruptedException
ignored
)
{
}
LOG
.
info
(
+
i
+
)
;
namesystem
.
enterSafeMode
(
false
)
;
long
logStartTxId
=
fsimage
.
getStorage
(
)
.
getMostRecentCheckpointTxId
(
)
+
1
;
verifyEditLogs
(
namesystem
,
fsimage
,
NNStorage
.
getInProgressEditsFileName
(
logStartTxId
)
,
logStartTxId
)
;
fileSys
=
cluster
.
getFileSystem
(
)
;
final
FSNamesystem
namesystem
=
cluster
.
getNamesystem
(
)
;
FSImage
fsimage
=
namesystem
.
getFSImage
(
)
;
FSEditLog
editLog
=
fsimage
.
getEditLog
(
)
;
startTransactionWorkers
(
cluster
,
caughtErr
)
;
for
(
int
i
=
0
;
i
<
NUM_SAVE_IMAGE
&&
caughtErr
.
get
(
)
==
null
;
i
++
)
{
try
{
Thread
.
sleep
(
20
)
;
}
catch
(
InterruptedException
ignored
)
{
}
LOG
.
info
(
+
i
+
)
;
namesystem
.
enterSafeMode
(
false
)
;
long
logStartTxId
=
fsimage
.
getStorage
(
)
.
getMostRecentCheckpointTxId
(
)
+
1
;
verifyEditLogs
(
namesystem
,
fsimage
,
NNStorage
.
getInProgressEditsFileName
(
logStartTxId
)
,
logStartTxId
)
;
LOG
.
info
(
+
i
+
)
;
namesystem
.
saveNamespace
(
0
,
0
)
;
for
(
int
i
=
0
;
i
<
NUM_SAVE_IMAGE
&&
caughtErr
.
get
(
)
==
null
;
i
++
)
{
try
{
Thread
.
sleep
(
20
)
;
}
catch
(
InterruptedException
ignored
)
{
}
LOG
.
info
(
+
i
+
)
;
namesystem
.
enterSafeMode
(
false
)
;
long
logStartTxId
=
fsimage
.
getStorage
(
)
.
getMostRecentCheckpointTxId
(
)
+
1
;
verifyEditLogs
(
namesystem
,
fsimage
,
NNStorage
.
getInProgressEditsFileName
(
logStartTxId
)
,
logStartTxId
)
;
LOG
.
info
(
+
i
+
)
;
namesystem
.
saveNamespace
(
0
,
0
)
;
LOG
.
info
(
+
i
+
)
;
long
savedImageTxId
=
fsimage
.
getStorage
(
)
.
getMostRecentCheckpointTxId
(
)
;
verifyEditLogs
(
namesystem
,
fsimage
,
NNStorage
.
getFinalizedEditsFileName
(
logStartTxId
,
savedImageTxId
)
,
logStartTxId
)
;
assertEquals
(
fsimage
.
getStorage
(
)
.
getMostRecentCheckpointTxId
(
)
,
editLog
.
getLastWrittenTxId
(
)
-
1
)
;
namesystem
.
leaveSafeMode
(
false
)
;
try
{
FSImage
fsimage
=
namesystem
.
getFSImage
(
)
;
FSEditLog
editLog
=
fsimage
.
getEditLog
(
)
;
JournalAndStream
jas
=
editLog
.
getJournals
(
)
.
get
(
0
)
;
EditLogFileOutputStream
spyElos
=
spy
(
(
EditLogFileOutputStream
)
jas
.
getCurrentStream
(
)
)
;
jas
.
setCurrentStreamForTests
(
spyElos
)
;
final
AtomicReference
<
Throwable
>
deferredException
=
new
AtomicReference
<
Throwable
>
(
)
;
final
CountDownLatch
waitToEnterFlush
=
new
CountDownLatch
(
1
)
;
final
Thread
doAnEditThread
=
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
LOG
.
info
(
)
;
namesystem
.
mkdirs
(
,
new
PermissionStatus
(
,
,
new
FsPermission
(
(
short
)
00755
)
)
,
true
)
;
LOG
.
info
(
)
;
}
catch
(
Throwable
ioe
)
{
@
Override
public
Void
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
LOG
.
info
(
)
;
if
(
useAsyncEditLog
||
Thread
.
currentThread
(
)
==
doAnEditThread
)
{
LOG
.
info
(
)
;
waitToEnterFlush
.
countDown
(
)
;
LOG
.
info
(
+
BLOCK_TIME
+
)
;
Thread
.
sleep
(
BLOCK_TIME
*
1000
)
;
LOG
.
info
(
)
;
}
invocation
.
callRealMethod
(
)
;
LOG
.
info
(
)
;
return
null
;
}
}
;
doAnswer
(
blockingFlush
)
.
when
(
spyElos
)
.
flush
(
)
;
doAnEditThread
.
start
(
)
;
LOG
.
info
(
)
;
final
Thread
doAnEditThread
=
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
LOG
.
info
(
)
;
namesystem
.
writeLock
(
)
;
try
{
editLog
.
logSetOwner
(
,
,
)
;
}
finally
{
namesystem
.
writeUnlock
(
)
;
}
sleepingBeforeSync
.
countDown
(
)
;
LOG
.
info
(
+
BLOCK_TIME
+
)
;
Thread
.
sleep
(
BLOCK_TIME
*
1000
)
;
editLog
.
logSync
(
)
;
LOG
.
info
(
)
;
}
catch
(
Throwable
ioe
)
{
final
FSEditLog
editLog
=
namesystem
.
getEditLog
(
)
;
FSEditLogOp
.
OpInstanceCache
cache
=
editLog
.
cache
.
get
(
)
;
final
FSEditLogOp
op
=
FSEditLogOp
.
SetOwnerOp
.
getInstance
(
cache
)
.
setSource
(
)
.
setUser
(
)
.
setGroup
(
)
;
final
FSEditLogOp
reuseOp
=
Mockito
.
spy
(
op
)
;
Mockito
.
doNothing
(
)
.
when
(
reuseOp
)
.
reset
(
)
;
Future
[
]
logSpammers
=
new
Future
[
16
]
;
for
(
int
i
=
0
;
i
<
logSpammers
.
length
;
i
++
)
{
final
int
ii
=
i
;
logSpammers
[
i
]
=
executor
.
submit
(
new
Callable
(
)
{
@
Override
public
Void
call
(
)
throws
Exception
{
Thread
.
currentThread
(
)
.
setName
(
+
ii
)
;
startSpamLatch
.
await
(
)
;
for
(
int
i
=
0
;
!
done
.
get
(
)
&&
i
<
1000000
;
i
++
)
{
editLog
.
logEdit
(
reuseOp
)
;
if
(
i
%
2048
==
0
)
{
@
Test
(
timeout
=
300000
)
public
void
testXAttrMultiSetRemove
(
)
throws
Exception
{
List
<
XAttr
>
existingXAttrs
=
Lists
.
newArrayListWithCapacity
(
0
)
;
final
Random
rand
=
new
Random
(
0xFEEDA
)
;
int
numExpectedXAttrs
=
0
;
while
(
numExpectedXAttrs
<
numGeneratedXAttrs
)
{
@
Test
(
timeout
=
300000
)
public
void
testXAttrMultiSetRemove
(
)
throws
Exception
{
List
<
XAttr
>
existingXAttrs
=
Lists
.
newArrayListWithCapacity
(
0
)
;
final
Random
rand
=
new
Random
(
0xFEEDA
)
;
int
numExpectedXAttrs
=
0
;
while
(
numExpectedXAttrs
<
numGeneratedXAttrs
)
{
LOG
.
info
(
+
numExpectedXAttrs
+
)
;
final
int
numToAdd
=
rand
.
nextInt
(
5
)
+
1
;
List
<
XAttr
>
toAdd
=
Lists
.
newArrayListWithCapacity
(
numToAdd
)
;
for
(
int
i
=
0
;
i
<
numToAdd
;
i
++
)
{
if
(
numExpectedXAttrs
>=
numGeneratedXAttrs
)
{
break
;
}
toAdd
.
add
(
generatedXAttrs
.
get
(
numExpectedXAttrs
)
)
;
numExpectedXAttrs
++
;
}
LOG
.
info
(
+
toAdd
.
size
(
)
+
)
;
for
(
int
i
=
0
;
i
<
toAdd
.
size
(
)
;
i
++
)
{
LOG
.
info
(
+
numExpectedXAttrs
+
)
;
final
int
numToAdd
=
rand
.
nextInt
(
5
)
+
1
;
List
<
XAttr
>
toAdd
=
Lists
.
newArrayListWithCapacity
(
numToAdd
)
;
for
(
int
i
=
0
;
i
<
numToAdd
;
i
++
)
{
if
(
numExpectedXAttrs
>=
numGeneratedXAttrs
)
{
break
;
}
toAdd
.
add
(
generatedXAttrs
.
get
(
numExpectedXAttrs
)
)
;
numExpectedXAttrs
++
;
}
LOG
.
info
(
+
toAdd
.
size
(
)
+
)
;
for
(
int
i
=
0
;
i
<
toAdd
.
size
(
)
;
i
++
)
{
LOG
.
info
(
+
toAdd
.
get
(
i
)
)
;
}
List
<
XAttr
>
newXAttrs
=
FSDirXAttrOp
.
setINodeXAttrs
(
fsdir
,
existingXAttrs
,
toAdd
,
EnumSet
.
of
(
XAttrSetFlag
.
CREATE
)
)
;
verifyXAttrsPresent
(
newXAttrs
,
numExpectedXAttrs
)
;
existingXAttrs
=
newXAttrs
;
}
while
(
numExpectedXAttrs
>
0
)
{
@
Test
public
void
testBasicTruncate
(
)
throws
IOException
{
int
startingFileSize
=
3
*
BLOCK_SIZE
;
fs
.
mkdirs
(
parent
)
;
fs
.
setQuota
(
parent
,
100
,
1000
)
;
byte
[
]
contents
=
AppendTestUtil
.
initBuffer
(
startingFileSize
)
;
for
(
int
fileLength
=
startingFileSize
;
fileLength
>
0
;
fileLength
-=
BLOCK_SIZE
-
1
)
{
for
(
int
toTruncate
=
0
;
toTruncate
<=
fileLength
;
toTruncate
++
)
{
final
Path
p
=
new
Path
(
parent
,
+
fileLength
)
;
writeContents
(
contents
,
fileLength
,
p
)
;
int
newLength
=
fileLength
-
toTruncate
;
boolean
isReady
=
fs
.
truncate
(
p
,
newLength
)
;
@
Test
public
void
testMultipleTruncate
(
)
throws
IOException
{
Path
dir
=
new
Path
(
)
;
fs
.
mkdirs
(
dir
)
;
final
Path
p
=
new
Path
(
dir
,
)
;
final
byte
[
]
data
=
new
byte
[
100
*
BLOCK_SIZE
]
;
ThreadLocalRandom
.
current
(
)
.
nextBytes
(
data
)
;
writeContents
(
data
,
data
.
length
,
p
)
;
for
(
int
n
=
data
.
length
;
n
>
0
;
)
{
final
int
newLength
=
ThreadLocalRandom
.
current
(
)
.
nextInt
(
n
)
;
final
boolean
isReady
=
fs
.
truncate
(
p
,
newLength
)
;
static
void
runTestToCommaSeparatedNumber
(
long
n
)
{
final
String
s
=
FsImageValidation
.
Util
.
toCommaSeparatedNumber
(
n
)
;
public
static
String
runFsck
(
Configuration
conf
,
int
expectedErrCode
,
boolean
checkErrorCode
,
String
...
path
)
throws
Exception
{
ByteArrayOutputStream
bStream
=
new
ByteArrayOutputStream
(
)
;
PrintStream
out
=
new
PrintStream
(
bStream
,
true
)
;
GenericTestUtils
.
setLogLevel
(
FSPermissionChecker
.
LOG
,
org
.
slf4j
.
event
.
Level
.
TRACE
)
;
int
errCode
=
ToolRunner
.
run
(
new
DFSck
(
conf
,
out
)
,
path
)
;
if
(
m
.
matches
(
)
)
{
numMissing
=
m
.
group
(
1
)
;
}
m
=
NUM_CORRUPT_BLOCKS_PATTERN
.
matcher
(
line
)
;
if
(
m
.
matches
(
)
)
{
numCorrupt
=
m
.
group
(
1
)
;
}
if
(
numMissing
!=
null
&&
numCorrupt
!=
null
)
{
break
;
}
}
if
(
numMissing
==
null
||
numCorrupt
==
null
)
{
throw
new
IOException
(
+
)
;
}
if
(
numMissing
.
equals
(
Integer
.
toString
(
totalMissingBlocks
)
)
)
{
assertTrue
(
numCorrupt
.
equals
(
Integer
.
toString
(
0
)
)
)
;
assertTrue
(
outStr
.
contains
(
NamenodeFsck
.
CORRUPT_STATUS
)
)
;
break
;
}
try
{
Thread
.
sleep
(
100
)
;
final
int
dfsBlockSize
=
512
*
1024
;
final
int
numDatanodes
=
1
;
final
int
replication
=
1
;
conf
.
setLong
(
DFSConfigKeys
.
DFS_BLOCK_SIZE_KEY
,
dfsBlockSize
)
;
conf
.
setLong
(
DFSConfigKeys
.
DFS_BLOCKREPORT_INTERVAL_MSEC_KEY
,
1000L
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY
,
1
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_REPLICATION_KEY
,
replication
)
;
File
builderBaseDir
=
new
File
(
GenericTestUtils
.
getRandomizedTempPath
(
)
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
,
builderBaseDir
)
.
build
(
)
;
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
cluster
.
waitActive
(
)
;
final
String
srcDir
=
;
final
DFSTestUtil
util
=
new
DFSTestUtil
.
Builder
(
)
.
setName
(
)
.
setMinSize
(
dfsBlockSize
*
2
)
.
setMaxSize
(
dfsBlockSize
*
3
)
.
setNumFiles
(
1
)
.
build
(
)
;
util
.
createFiles
(
dfs
,
srcDir
,
(
short
)
replication
)
;
final
String
[
]
fileNames
=
util
.
getFileNames
(
srcDir
)
;
@
Override
public
Boolean
get
(
)
{
try
{
final
String
str
=
runFsck
(
conf
,
1
,
false
,
)
;
String
numCorrupt
=
null
;
for
(
String
line
:
str
.
split
(
LINE_SEPARATOR
)
)
{
Matcher
m
=
NUM_CORRUPT_BLOCKS_PATTERN
.
matcher
(
line
)
;
if
(
m
.
matches
(
)
)
{
numCorrupt
=
m
.
group
(
1
)
;
break
;
}
}
if
(
numCorrupt
==
null
)
{
Assert
.
fail
(
)
;
}
if
(
Integer
.
parseInt
(
numCorrupt
)
==
ctf
.
getTotalMissingBlocks
(
)
)
{
assertTrue
(
str
.
contains
(
NamenodeFsck
.
CORRUPT_STATUS
)
)
;
return
true
;
}
}
catch
(
Exception
e
)
{
Assert
.
fail
(
)
;
}
if
(
Integer
.
parseInt
(
numCorrupt
)
==
ctf
.
getTotalMissingBlocks
(
)
)
{
assertTrue
(
str
.
contains
(
NamenodeFsck
.
CORRUPT_STATUS
)
)
;
return
true
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
)
;
}
return
false
;
}
}
,
1000
,
60000
)
;
runFsck
(
conf
,
1
,
true
,
,
,
,
)
;
LOG
.
info
(
)
;
runFsck
(
conf
,
1
,
false
,
,
)
;
final
List
<
LocatedFileStatus
>
retVal
=
new
ArrayList
<
>
(
)
;
final
RemoteIterator
<
LocatedFileStatus
>
iter
=
dfs
.
listFiles
(
new
Path
(
)
,
true
)
;
while
(
iter
.
hasNext
(
)
)
{
private
void
runTest
(
final
int
nNameNodes
,
final
int
nDataNodes
,
Configuration
conf
)
throws
Exception
{
LOG
.
info
(
+
nNameNodes
+
+
nDataNodes
)
;
LOG
.
info
(
)
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
MiniDFSNNTopology
.
simpleFederatedTopology
(
nNameNodes
)
)
.
numDataNodes
(
nDataNodes
)
.
build
(
)
;
LOG
.
info
(
)
;
DFSTestUtil
.
setFederatedConfiguration
(
cluster
,
conf
)
;
try
{
cluster
.
waitActive
(
)
;
LOG
.
info
(
)
;
final
Suite
s
=
new
Suite
(
cluster
,
nNameNodes
,
nDataNodes
)
;
for
(
int
i
=
0
;
i
<
nNameNodes
;
i
++
)
{
s
.
createFile
(
i
,
1024
)
;
}
LOG
.
info
(
)
;
final
String
[
]
urls
=
new
String
[
nNameNodes
]
;
for
(
int
i
=
0
;
i
<
urls
.
length
;
i
++
)
{
urls
[
i
]
=
cluster
.
getFileSystem
(
i
)
.
getUri
(
)
+
FILE_NAME
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
MiniDFSNNTopology
.
simpleFederatedTopology
(
nNameNodes
)
)
.
numDataNodes
(
nDataNodes
)
.
build
(
)
;
LOG
.
info
(
)
;
DFSTestUtil
.
setFederatedConfiguration
(
cluster
,
conf
)
;
try
{
cluster
.
waitActive
(
)
;
LOG
.
info
(
)
;
final
Suite
s
=
new
Suite
(
cluster
,
nNameNodes
,
nDataNodes
)
;
for
(
int
i
=
0
;
i
<
nNameNodes
;
i
++
)
{
s
.
createFile
(
i
,
1024
)
;
}
LOG
.
info
(
)
;
final
String
[
]
urls
=
new
String
[
nNameNodes
]
;
for
(
int
i
=
0
;
i
<
urls
.
length
;
i
++
)
{
urls
[
i
]
=
cluster
.
getFileSystem
(
i
)
.
getUri
(
)
+
FILE_NAME
;
LOG
.
info
(
+
i
+
+
urls
[
i
]
)
;
final
String
result
=
TestFsck
.
runFsck
(
conf
,
0
,
false
,
urls
[
i
]
)
;
}
LOG
.
info
(
)
;
final
String
[
]
urls
=
new
String
[
nNameNodes
]
;
for
(
int
i
=
0
;
i
<
urls
.
length
;
i
++
)
{
urls
[
i
]
=
cluster
.
getFileSystem
(
i
)
.
getUri
(
)
+
FILE_NAME
;
LOG
.
info
(
+
i
+
+
urls
[
i
]
)
;
final
String
result
=
TestFsck
.
runFsck
(
conf
,
0
,
false
,
urls
[
i
]
)
;
LOG
.
info
(
+
result
)
;
Assert
.
assertTrue
(
result
.
contains
(
)
)
;
}
LOG
.
info
(
)
;
final
String
[
]
vurls
=
new
String
[
nNameNodes
]
;
for
(
int
i
=
0
;
i
<
vurls
.
length
;
i
++
)
{
String
link
=
+
i
+
FILE_NAME
;
ConfigUtil
.
addLink
(
conf
,
link
,
new
URI
(
urls
[
i
]
)
)
;
vurls
[
i
]
=
+
link
;
}
for
(
int
i
=
0
;
i
<
vurls
.
length
;
i
++
)
{
final
String
[
]
urls
=
new
String
[
nNameNodes
]
;
for
(
int
i
=
0
;
i
<
urls
.
length
;
i
++
)
{
urls
[
i
]
=
cluster
.
getFileSystem
(
i
)
.
getUri
(
)
+
FILE_NAME
;
LOG
.
info
(
+
i
+
+
urls
[
i
]
)
;
final
String
result
=
TestFsck
.
runFsck
(
conf
,
0
,
false
,
urls
[
i
]
)
;
LOG
.
info
(
+
result
)
;
Assert
.
assertTrue
(
result
.
contains
(
)
)
;
}
LOG
.
info
(
)
;
final
String
[
]
vurls
=
new
String
[
nNameNodes
]
;
for
(
int
i
=
0
;
i
<
vurls
.
length
;
i
++
)
{
String
link
=
+
i
+
FILE_NAME
;
ConfigUtil
.
addLink
(
conf
,
link
,
new
URI
(
urls
[
i
]
)
)
;
vurls
[
i
]
=
+
link
;
}
for
(
int
i
=
0
;
i
<
vurls
.
length
;
i
++
)
{
LOG
.
info
(
+
i
+
+
vurls
[
i
]
)
;
Configuration
conf
=
getConf
(
)
;
short
REPLICATION_FACTOR
=
2
;
final
Path
filePath
=
new
Path
(
)
;
HostsFileWriter
hostsFileWriter
=
new
HostsFileWriter
(
)
;
hostsFileWriter
.
initialize
(
conf
,
)
;
String
racks
[
]
=
{
,
,
,
}
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
racks
.
length
)
.
racks
(
racks
)
.
build
(
)
;
final
FSNamesystem
ns
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
;
try
{
final
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
DFSTestUtil
.
createFile
(
fs
,
filePath
,
1L
,
REPLICATION_FACTOR
,
1L
)
;
ExtendedBlock
b
=
DFSTestUtil
.
getFirstBlock
(
fs
,
filePath
)
;
DFSTestUtil
.
waitForReplication
(
cluster
,
b
,
2
,
REPLICATION_FACTOR
,
0
)
;
BlockLocation
locs
[
]
=
fs
.
getFileBlockLocations
(
fs
.
getFileStatus
(
filePath
)
,
0
,
Long
.
MAX_VALUE
)
;
String
name
=
locs
[
0
]
.
getNames
(
)
[
0
]
;
private
void
verifyFileStatus
(
UserGroupInformation
ugi
)
throws
IOException
{
FileSystem
fs
=
FileSystem
.
get
(
miniDFS
.
getConfiguration
(
0
)
)
;
FileStatus
status
=
fs
.
getFileStatus
(
new
Path
(
)
)
;
INodeFile
inodeFile
;
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
new
Short
(
(
short
)
3
)
,
StripedFileTestUtil
.
getDefaultECPolicy
(
)
.
getId
(
)
,
preferredBlockSize
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
CONTIGUOUS
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
null
,
null
,
preferredBlockSize
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
CONTIGUOUS
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
Short
.
MAX_VALUE
,
null
,
preferredBlockSize
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
CONTIGUOUS
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
null
,
null
,
preferredBlockSize
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
CONTIGUOUS
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
Short
.
MAX_VALUE
,
null
,
preferredBlockSize
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
CONTIGUOUS
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
final
Short
replication
=
new
Short
(
(
short
)
3
)
;
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
replication
,
null
,
preferredBlockSize
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
STRIPED
)
;
fail
(
+
)
;
private
Path
getInodePath
(
long
inodeId
,
String
remainingPath
)
{
StringBuilder
b
=
new
StringBuilder
(
)
;
b
.
append
(
Path
.
SEPARATOR
)
.
append
(
FSDirectory
.
DOT_RESERVED_STRING
)
.
append
(
Path
.
SEPARATOR
)
.
append
(
FSDirectory
.
DOT_INODES_STRING
)
.
append
(
Path
.
SEPARATOR
)
.
append
(
inodeId
)
.
append
(
Path
.
SEPARATOR
)
.
append
(
remainingPath
)
;
Path
p
=
new
Path
(
b
.
toString
(
)
)
;
catch
(
IOException
ex
)
{
LOG
.
info
(
,
ex
)
;
break
;
}
}
}
}
;
threads
[
1
]
=
new
TestThread
(
)
{
@
Override
protected
void
execute
(
)
throws
Throwable
{
while
(
live
)
{
try
{
int
blockcount
=
getBlockCount
(
)
;
if
(
blockcount
<
TOTAL_BLOCKS
&&
blockcount
>
0
)
{
mc
.
getNamesystem
(
)
.
writeLock
(
)
;
try
{
lockOps
++
;
}
finally
{
mc
.
getNamesystem
(
)
.
writeUnlock
(
)
;
int
blockcount
=
getBlockCount
(
)
;
if
(
blockcount
<
TOTAL_BLOCKS
&&
blockcount
>
0
)
{
mc
.
getNamesystem
(
)
.
writeLock
(
)
;
try
{
lockOps
++
;
}
finally
{
mc
.
getNamesystem
(
)
.
writeUnlock
(
)
;
}
Thread
.
sleep
(
1
)
;
}
}
catch
(
InterruptedException
ex
)
{
LOG
.
info
(
,
ex
)
;
break
;
}
}
}
}
;
threads
[
0
]
.
start
(
)
;
threads
[
1
]
.
start
(
)
;
final
long
start
=
Time
.
now
(
)
;
assertCorruptFilesCount
(
cluster
,
badFiles
.
size
(
)
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
File
storageDir
=
cluster
.
getInstanceStorageDir
(
0
,
1
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
assertTrue
(
,
data_dir
.
exists
(
)
)
;
List
<
File
>
metaFiles
=
MiniDFSCluster
.
getAllBlockFiles
(
data_dir
)
;
assertTrue
(
+
,
metaFiles
!=
null
&&
!
metaFiles
.
isEmpty
(
)
)
;
File
metaFile
=
metaFiles
.
get
(
0
)
;
RandomAccessFile
file
=
new
RandomAccessFile
(
metaFile
,
)
;
FileChannel
channel
=
file
.
getChannel
(
)
;
long
position
=
channel
.
size
(
)
-
corruptionLength
;
byte
[
]
buffer
=
new
byte
[
corruptionLength
]
;
new
Random
(
13L
)
.
nextBytes
(
buffer
)
;
channel
.
write
(
ByteBuffer
.
wrap
(
buffer
)
,
position
)
;
file
.
close
(
)
;
RandomAccessFile
file
=
new
RandomAccessFile
(
metaFile
,
)
;
FileChannel
channel
=
file
.
getChannel
(
)
;
long
position
=
channel
.
size
(
)
-
corruptionLength
;
byte
[
]
buffer
=
new
byte
[
corruptionLength
]
;
new
Random
(
13L
)
.
nextBytes
(
buffer
)
;
channel
.
write
(
ByteBuffer
.
wrap
(
buffer
)
,
position
)
;
file
.
close
(
)
;
LOG
.
info
(
+
metaFile
.
getName
(
)
+
+
position
+
+
corruptionLength
)
;
try
{
util
.
checkFiles
(
fs
,
)
;
}
catch
(
BlockMissingException
e
)
{
System
.
out
.
println
(
)
;
}
catch
(
IOException
e
)
{
assertTrue
(
+
+
e
,
false
)
;
}
badFiles
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
assertEquals
(
+
badFiles
.
size
(
)
+
,
0
,
badFiles
.
size
(
)
)
;
assertCorruptFilesCount
(
cluster
,
badFiles
.
size
(
)
)
;
File
storageDir
=
cluster
.
getInstanceStorageDir
(
0
,
0
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
)
;
assertTrue
(
,
data_dir
.
exists
(
)
)
;
List
<
File
>
metaFiles
=
MiniDFSCluster
.
getAllBlockFiles
(
data_dir
)
;
assertTrue
(
+
,
metaFiles
!=
null
&&
!
metaFiles
.
isEmpty
(
)
)
;
File
metaFile
=
metaFiles
.
get
(
0
)
;
RandomAccessFile
file
=
new
RandomAccessFile
(
metaFile
,
)
;
FileChannel
channel
=
file
.
getChannel
(
)
;
long
position
=
channel
.
size
(
)
-
corruptionLength
;
byte
[
]
buffer
=
new
byte
[
corruptionLength
]
;
new
Random
(
13L
)
.
nextBytes
(
buffer
)
;
channel
.
write
(
ByteBuffer
.
wrap
(
buffer
)
,
position
)
;
file
.
close
(
)
;
RandomAccessFile
file
=
new
RandomAccessFile
(
metaFile
,
)
;
FileChannel
channel
=
file
.
getChannel
(
)
;
long
position
=
channel
.
size
(
)
-
corruptionLength
;
byte
[
]
buffer
=
new
byte
[
corruptionLength
]
;
new
Random
(
13L
)
.
nextBytes
(
buffer
)
;
channel
.
write
(
ByteBuffer
.
wrap
(
buffer
)
,
position
)
;
file
.
close
(
)
;
LOG
.
info
(
+
metaFile
.
getName
(
)
+
+
position
+
+
corruptionLength
)
;
try
{
util
.
checkFiles
(
fs
,
)
;
}
catch
(
BlockMissingException
e
)
{
System
.
out
.
println
(
)
;
}
catch
(
IOException
e
)
{
assertTrue
(
+
+
+
e
,
false
)
;
}
badFiles
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
badFiles
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
LOG
.
info
(
+
badFiles
.
size
(
)
)
;
assertEquals
(
+
badFiles
.
size
(
)
+
+
,
1
,
badFiles
.
size
(
)
)
;
assertCorruptFilesCount
(
cluster
,
badFiles
.
size
(
)
)
;
cluster
.
restartNameNode
(
0
)
;
fs
=
cluster
.
getFileSystem
(
)
;
while
(
!
cluster
.
getNameNode
(
)
.
namesystem
.
getBlockManager
(
)
.
isPopulatingReplQueues
(
)
)
{
try
{
LOG
.
info
(
)
;
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ignore
)
{
}
}
try
{
util
.
checkFiles
(
fs
,
)
;
}
catch
(
BlockMissingException
e
)
{
System
.
out
.
println
(
)
;
try
{
LOG
.
info
(
)
;
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ignore
)
{
}
}
try
{
util
.
checkFiles
(
fs
,
)
;
}
catch
(
BlockMissingException
e
)
{
System
.
out
.
println
(
)
;
}
catch
(
IOException
e
)
{
assertTrue
(
+
+
+
e
,
false
)
;
}
badFiles
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
LOG
.
info
(
+
badFiles
.
size
(
)
)
;
assertEquals
(
+
badFiles
.
size
(
)
+
+
,
1
,
badFiles
.
size
(
)
)
;
assertCorruptFilesCount
(
cluster
,
badFiles
.
size
(
)
)
;
assertTrue
(
,
cluster
.
getNameNode
(
)
.
isInSafeMode
(
)
)
;
util
.
createFiles
(
fs
,
)
;
final
NameNode
namenode
=
cluster
.
getNameNode
(
)
;
Collection
<
FSNamesystem
.
CorruptFileBlockInfo
>
corruptFileBlocks
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
int
numCorrupt
=
corruptFileBlocks
.
size
(
)
;
assertEquals
(
0
,
numCorrupt
)
;
assertCorruptFilesCount
(
cluster
,
numCorrupt
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
for
(
int
i
=
0
;
i
<
4
;
i
++
)
{
for
(
int
j
=
0
;
j
<=
1
;
j
++
)
{
File
storageDir
=
cluster
.
getInstanceStorageDir
(
i
,
j
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
List
<
File
>
metadataFiles
=
MiniDFSCluster
.
getAllBlockMetadataFiles
(
data_dir
)
;
if
(
metadataFiles
==
null
)
continue
;
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
Collection
<
FSNamesystem
.
CorruptFileBlockInfo
>
corruptFileBlocks
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
int
numCorrupt
=
corruptFileBlocks
.
size
(
)
;
assertEquals
(
0
,
numCorrupt
)
;
assertCorruptFilesCount
(
cluster
,
numCorrupt
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
for
(
int
i
=
0
;
i
<
4
;
i
++
)
{
for
(
int
j
=
0
;
j
<=
1
;
j
++
)
{
File
storageDir
=
cluster
.
getInstanceStorageDir
(
i
,
j
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
List
<
File
>
metadataFiles
=
MiniDFSCluster
.
getAllBlockMetadataFiles
(
data_dir
)
;
if
(
metadataFiles
==
null
)
continue
;
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
LOG
.
info
(
+
blockFile
.
getName
(
)
)
;
assertTrue
(
,
blockFile
.
delete
(
)
)
;
DistributedFileSystem
dfs
=
(
DistributedFileSystem
)
fs
;
DFSTestUtil
util
=
new
DFSTestUtil
.
Builder
(
)
.
setName
(
)
.
setNumFiles
(
3
)
.
setMaxLevels
(
1
)
.
setMaxSize
(
1024
)
.
build
(
)
;
util
.
createFiles
(
fs
,
)
;
RemoteIterator
<
Path
>
corruptFileBlocks
=
dfs
.
listCorruptFileBlocks
(
new
Path
(
)
)
;
int
numCorrupt
=
countPaths
(
corruptFileBlocks
)
;
assertEquals
(
0
,
numCorrupt
)
;
assertCorruptFilesCount
(
cluster
,
numCorrupt
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
File
storageDir
=
cluster
.
getInstanceStorageDir
(
0
,
i
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
List
<
File
>
metadataFiles
=
MiniDFSCluster
.
getAllBlockMetadataFiles
(
data_dir
)
;
if
(
metadataFiles
==
null
)
continue
;
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
util
.
createFiles
(
fs
,
)
;
RemoteIterator
<
Path
>
corruptFileBlocks
=
dfs
.
listCorruptFileBlocks
(
new
Path
(
)
)
;
int
numCorrupt
=
countPaths
(
corruptFileBlocks
)
;
assertEquals
(
0
,
numCorrupt
)
;
assertCorruptFilesCount
(
cluster
,
numCorrupt
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
File
storageDir
=
cluster
.
getInstanceStorageDir
(
0
,
i
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
List
<
File
>
metadataFiles
=
MiniDFSCluster
.
getAllBlockMetadataFiles
(
data_dir
)
;
if
(
metadataFiles
==
null
)
continue
;
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
LOG
.
info
(
+
blockFile
.
getName
(
)
)
;
assertTrue
(
,
blockFile
.
delete
(
)
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
build
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
final
int
maxCorruptFileBlocks
=
conf
.
getInt
(
DFSConfigKeys
.
DFS_NAMENODE_MAX_CORRUPT_FILE_BLOCKS_RETURNED_KEY
,
100
)
;
DFSTestUtil
util
=
new
DFSTestUtil
.
Builder
(
)
.
setName
(
)
.
setNumFiles
(
maxCorruptFileBlocks
*
3
)
.
setMaxLevels
(
1
)
.
setMaxSize
(
512
)
.
build
(
)
;
util
.
createFiles
(
fs
,
,
(
short
)
1
)
;
util
.
waitReplication
(
fs
,
,
(
short
)
1
)
;
final
NameNode
namenode
=
cluster
.
getNameNode
(
)
;
Collection
<
FSNamesystem
.
CorruptFileBlockInfo
>
badFiles
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
assertEquals
(
+
badFiles
.
size
(
)
+
,
0
,
badFiles
.
size
(
)
)
;
assertCorruptFilesCount
(
cluster
,
badFiles
.
size
(
)
)
;
final
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
for
(
int
i
=
0
;
i
<
4
;
i
++
)
{
for
(
int
j
=
0
;
j
<=
1
;
j
++
)
{
File
storageDir
=
cluster
.
getInstanceStorageDir
(
i
,
j
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
LOG
.
info
(
+
data_dir
)
;
List
<
File
>
metadataFiles
=
MiniDFSCluster
.
getAllBlockMetadataFiles
(
data_dir
)
;
if
(
metadataFiles
==
null
)
continue
;
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
assertTrue
(
,
blockFile
.
delete
(
)
)
;
assertTrue
(
,
metadataFile
.
delete
(
)
)
;
}
}
}
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
DataNodeTestUtils
.
runDirectoryScanner
(
dn
)
;
LOG
.
info
(
)
;
cluster
.
restartDataNodes
(
)
;
cluster
.
waitActive
(
)
;
badFiles
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
while
(
badFiles
.
size
(
)
<
maxCorruptFileBlocks
)
{
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
assertTrue
(
,
blockFile
.
delete
(
)
)
;
assertTrue
(
,
metadataFile
.
delete
(
)
)
;
}
}
}
DataNode
dn
=
cluster
.
getDataNodes
(
)
.
get
(
0
)
;
DataNodeTestUtils
.
runDirectoryScanner
(
dn
)
;
LOG
.
info
(
)
;
cluster
.
restartDataNodes
(
)
;
cluster
.
waitActive
(
)
;
badFiles
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
while
(
badFiles
.
size
(
)
<
maxCorruptFileBlocks
)
{
LOG
.
info
(
+
badFiles
.
size
(
)
)
;
Thread
.
sleep
(
10000
)
;
badFiles
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
}
badFiles
=
namenode
.
getNamesystem
(
)
.
listCorruptFileBlocks
(
,
null
)
;
fs
.
setWorkingDirectory
(
baseDir
)
;
DFSTestUtil
util
=
new
DFSTestUtil
.
Builder
(
)
.
setName
(
)
.
setNumFiles
(
3
)
.
setMaxLevels
(
1
)
.
setMaxSize
(
1024
)
.
build
(
)
;
util
.
createFiles
(
fs
,
)
;
RemoteIterator
<
Path
>
corruptFileBlocks
=
dfs
.
listCorruptFileBlocks
(
new
Path
(
)
)
;
int
numCorrupt
=
countPaths
(
corruptFileBlocks
)
;
assertEquals
(
0
,
numCorrupt
)
;
assertCorruptFilesCount
(
cluster
,
numCorrupt
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
File
storageDir
=
cluster
.
getInstanceStorageDir
(
0
,
i
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
List
<
File
>
metadataFiles
=
MiniDFSCluster
.
getAllBlockMetadataFiles
(
data_dir
)
;
if
(
metadataFiles
==
null
)
continue
;
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
util
.
createFiles
(
fs
,
)
;
RemoteIterator
<
Path
>
corruptFileBlocks
=
dfs
.
listCorruptFileBlocks
(
new
Path
(
)
)
;
int
numCorrupt
=
countPaths
(
corruptFileBlocks
)
;
assertEquals
(
0
,
numCorrupt
)
;
assertCorruptFilesCount
(
cluster
,
numCorrupt
)
;
String
bpid
=
cluster
.
getNamesystem
(
)
.
getBlockPoolId
(
)
;
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
File
storageDir
=
cluster
.
getInstanceStorageDir
(
0
,
i
)
;
File
data_dir
=
MiniDFSCluster
.
getFinalizedDir
(
storageDir
,
bpid
)
;
List
<
File
>
metadataFiles
=
MiniDFSCluster
.
getAllBlockMetadataFiles
(
data_dir
)
;
if
(
metadataFiles
==
null
)
continue
;
for
(
File
metadataFile
:
metadataFiles
)
{
File
blockFile
=
Block
.
metaToBlockFile
(
metadataFile
)
;
LOG
.
info
(
+
blockFile
.
getName
(
)
)
;
assertTrue
(
,
blockFile
.
delete
(
)
)
;
ThreadLocalRandom
.
current
(
)
.
nextBytes
(
data
)
;
DFSTestUtil
.
createOpenFiles
(
fileSystem
,
,
(
(
BATCH_SIZE
*
4
)
+
(
BATCH_SIZE
/
2
)
)
)
;
final
DFSAdmin
dfsAdmin
=
new
DFSAdmin
(
haConf
)
;
final
AtomicBoolean
failoverCompleted
=
new
AtomicBoolean
(
false
)
;
final
AtomicBoolean
listOpenFilesError
=
new
AtomicBoolean
(
false
)
;
final
int
listingIntervalMsec
=
250
;
Thread
clientThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
while
(
!
failoverCompleted
.
get
(
)
)
{
try
{
assertEquals
(
0
,
ToolRunner
.
run
(
dfsAdmin
,
new
String
[
]
{
}
)
)
;
assertEquals
(
0
,
ToolRunner
.
run
(
dfsAdmin
,
new
String
[
]
{
,
}
)
)
;
Thread
.
sleep
(
listingIntervalMsec
)
;
}
catch
(
Exception
e
)
{
listOpenFilesError
.
set
(
true
)
;
MiniDFSCluster
cluster
=
null
;
HostsFileWriter
hostsFileWriter
=
new
HostsFileWriter
(
)
;
hostsFileWriter
.
initialize
(
conf
,
)
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
3
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
FSNamesystem
fsn
=
cluster
.
getNameNode
(
)
.
namesystem
;
MBeanServer
mbs
=
ManagementFactory
.
getPlatformMBeanServer
(
)
;
ObjectName
mxbeanName
=
new
ObjectName
(
)
;
List
<
String
>
hosts
=
new
ArrayList
<
>
(
)
;
for
(
DataNode
dn
:
cluster
.
getDataNodes
(
)
)
{
hosts
.
add
(
dn
.
getDisplayName
(
)
)
;
}
hostsFileWriter
.
initIncludeHosts
(
hosts
.
toArray
(
new
String
[
hosts
.
size
(
)
]
)
)
;
fsn
.
getBlockManager
(
)
.
getDatanodeManager
(
)
.
refreshNodes
(
conf
)
;
String
liveNodesInfo
=
(
String
)
(
mbs
.
getAttribute
(
mxbeanName
,
)
)
;
for
(
Map
<
String
,
Object
>
liveNode
:
liveNodes
.
values
(
)
)
{
assertTrue
(
liveNode
.
containsKey
(
)
)
;
assertTrue
(
liveNode
.
containsKey
(
)
)
;
}
Map
<
String
,
Long
>
maintenanceNodes
=
new
HashMap
<
>
(
)
;
maintenanceNodes
.
put
(
cluster
.
getDataNodes
(
)
.
get
(
0
)
.
getDisplayName
(
)
,
Time
.
now
(
)
+
expirationInMs
)
;
hostsFileWriter
.
initOutOfServiceHosts
(
null
,
maintenanceNodes
)
;
fsn
.
getBlockManager
(
)
.
getDatanodeManager
(
)
.
refreshNodes
(
conf
)
;
boolean
recheck
=
true
;
while
(
recheck
)
{
String
enteringMaintenanceNodesInfo
=
(
String
)
(
mbs
.
getAttribute
(
mxbeanName
,
)
)
;
Map
<
String
,
Map
<
String
,
Object
>>
enteringMaintenanceNodes
=
(
Map
<
String
,
Map
<
String
,
Object
>>
)
JSON
.
parse
(
enteringMaintenanceNodesInfo
)
;
if
(
enteringMaintenanceNodes
.
size
(
)
<=
0
)
{
LOG
.
info
(
)
;
Uninterruptibles
.
sleepUninterruptibly
(
1
,
TimeUnit
.
SECONDS
)
;
continue
;
if
(
!
finalize
)
{
FSEditLog
spyLog
=
spy
(
cluster
.
getNameNode
(
)
.
getFSImage
(
)
.
getEditLog
(
)
)
;
doNothing
(
)
.
when
(
spyLog
)
.
endCurrentLogSegment
(
true
)
;
DFSTestUtil
.
setEditLogForTesting
(
cluster
.
getNamesystem
(
)
,
spyLog
)
;
}
fileSys
=
cluster
.
getFileSystem
(
)
;
final
FSNamesystem
namesystem
=
cluster
.
getNamesystem
(
)
;
FSImage
fsimage
=
namesystem
.
getFSImage
(
)
;
fileSys
.
mkdirs
(
new
Path
(
TEST_PATH
)
)
;
fileSys
.
mkdirs
(
new
Path
(
TEST_PATH2
)
)
;
sd
=
fsimage
.
getStorage
(
)
.
dirIterator
(
NameNodeDirType
.
EDITS
)
.
next
(
)
;
}
finally
{
if
(
cluster
!=
null
)
{
cluster
.
shutdown
(
)
;
}
}
File
editFile
=
FSImageTestUtil
.
findLatestEditsLog
(
sd
)
.
getFile
(
)
;
assertTrue
(
+
editFile
,
editFile
.
exists
(
)
)
;
finally
{
if
(
cluster
!=
null
)
{
cluster
.
shutdown
(
)
;
}
}
File
editFile
=
FSImageTestUtil
.
findLatestEditsLog
(
sd
)
.
getFile
(
)
;
assertTrue
(
+
editFile
,
editFile
.
exists
(
)
)
;
LOG
.
info
(
+
editFile
+
)
;
corruptor
.
corrupt
(
editFile
)
;
cluster
=
null
;
try
{
LOG
.
debug
(
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
0
)
.
enableManagedDfsDirsRedundancy
(
false
)
.
format
(
false
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
cluster
.
shutdown
(
)
;
if
(
needRecovery
)
{
fail
(
)
;
}
catch
(
IOException
e
)
{
if
(
!
needRecovery
)
{
LOG
.
error
(
+
corruptor
.
getName
(
)
+
corruptor
,
e
)
;
fail
(
+
e
.
getMessage
(
)
)
;
}
}
finally
{
if
(
cluster
!=
null
)
{
cluster
.
shutdown
(
)
;
}
}
cluster
=
null
;
try
{
LOG
.
debug
(
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
0
)
.
enableManagedDfsDirsRedundancy
(
false
)
.
format
(
false
)
.
startupOption
(
recoverStartOpt
)
.
build
(
)
;
}
catch
(
IOException
e
)
{
fail
(
+
+
e
.
getMessage
(
)
+
+
StringUtils
.
stringifyException
(
e
)
)
;
}
finally
{
if
(
cluster
!=
null
)
{
final
List
<
DatanodeDescriptor
>
live
=
new
ArrayList
<
DatanodeDescriptor
>
(
)
;
final
List
<
DatanodeDescriptor
>
dead
=
new
ArrayList
<
DatanodeDescriptor
>
(
)
;
dm
.
fetchDatanodes
(
live
,
dead
,
false
)
;
assertTrue
(
live
.
size
(
)
==
1
)
;
long
used
,
remaining
,
configCapacity
,
nonDFSUsed
,
bpUsed
;
float
percentUsed
,
percentRemaining
,
percentBpUsed
;
for
(
final
DatanodeDescriptor
datanode
:
live
)
{
used
=
datanode
.
getDfsUsed
(
)
;
remaining
=
datanode
.
getRemaining
(
)
;
nonDFSUsed
=
datanode
.
getNonDfsUsed
(
)
;
configCapacity
=
datanode
.
getCapacity
(
)
;
percentUsed
=
datanode
.
getDfsUsedPercent
(
)
;
percentRemaining
=
datanode
.
getRemainingPercent
(
)
;
bpUsed
=
datanode
.
getBlockPoolUsed
(
)
;
percentBpUsed
=
datanode
.
getBlockPoolUsedPercent
(
)
;
assertTrue
(
percentUsed
==
DFSUtilClient
.
getPercentUsed
(
used
,
configCapacity
)
)
;
assertTrue
(
percentRemaining
==
DFSUtilClient
.
getPercentRemaining
(
remaining
,
configCapacity
)
)
;
assertTrue
(
percentBpUsed
==
DFSUtilClient
.
getPercentUsed
(
bpUsed
,
configCapacity
)
)
;
}
final
FsDatasetTestUtils
utils
=
cluster
.
getFsDatasetTestUtils
(
0
)
;
int
numOfDataDirs
=
utils
.
getDefaultNumOfDataDirs
(
)
;
long
diskCapacity
=
numOfDataDirs
*
utils
.
getRawCapacity
(
)
;
reserved
*=
numOfDataDirs
;
configCapacity
=
namesystem
.
getCapacityTotal
(
)
;
used
=
namesystem
.
getCapacityUsed
(
)
;
nonDFSUsed
=
namesystem
.
getNonDfsUsedSpace
(
)
;
remaining
=
namesystem
.
getCapacityRemaining
(
)
;
percentUsed
=
namesystem
.
getPercentUsed
(
)
;
percentRemaining
=
namesystem
.
getPercentRemaining
(
)
;
bpUsed
=
namesystem
.
getBlockPoolUsedSpace
(
)
;
percentBpUsed
=
namesystem
.
getPercentBlockPoolUsed
(
)
;
assertTrue
(
percentRemaining
==
DFSUtilClient
.
getPercentRemaining
(
remaining
,
configCapacity
)
)
;
assertTrue
(
percentBpUsed
==
DFSUtilClient
.
getPercentUsed
(
bpUsed
,
configCapacity
)
)
;
}
final
FsDatasetTestUtils
utils
=
cluster
.
getFsDatasetTestUtils
(
0
)
;
int
numOfDataDirs
=
utils
.
getDefaultNumOfDataDirs
(
)
;
long
diskCapacity
=
numOfDataDirs
*
utils
.
getRawCapacity
(
)
;
reserved
*=
numOfDataDirs
;
configCapacity
=
namesystem
.
getCapacityTotal
(
)
;
used
=
namesystem
.
getCapacityUsed
(
)
;
nonDFSUsed
=
namesystem
.
getNonDfsUsedSpace
(
)
;
remaining
=
namesystem
.
getCapacityRemaining
(
)
;
percentUsed
=
namesystem
.
getPercentUsed
(
)
;
percentRemaining
=
namesystem
.
getPercentRemaining
(
)
;
bpUsed
=
namesystem
.
getBlockPoolUsedSpace
(
)
;
percentBpUsed
=
namesystem
.
getPercentBlockPoolUsed
(
)
;
LOG
.
info
(
+
cluster
.
getDataDirectory
(
)
)
;
@
Test
public
void
testDelete
(
)
throws
Throwable
{
for
(
TestMatrixEntry
testMatrixEntry
:
createTestMatrix
(
)
)
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
MiniDFSCluster
cluster
=
setupTestCase
(
conf
,
testMatrixEntry
.
getProtectedPaths
(
)
,
testMatrixEntry
.
getUnprotectedPaths
(
)
)
;
try
{
@
Test
public
void
testMoveToTrash
(
)
throws
Throwable
{
for
(
TestMatrixEntry
testMatrixEntry
:
createTestMatrix
(
)
)
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
conf
.
setInt
(
DFSConfigKeys
.
FS_TRASH_INTERVAL_KEY
,
3600
)
;
MiniDFSCluster
cluster
=
setupTestCase
(
conf
,
testMatrixEntry
.
getProtectedPaths
(
)
,
testMatrixEntry
.
getUnprotectedPaths
(
)
)
;
try
{
@
Test
public
void
testRename
(
)
throws
Throwable
{
for
(
TestMatrixEntry
testMatrixEntry
:
createTestMatrix
(
)
)
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
MiniDFSCluster
cluster
=
setupTestCase
(
conf
,
testMatrixEntry
.
getProtectedPaths
(
)
,
testMatrixEntry
.
getUnprotectedPaths
(
)
)
;
try
{
@
Test
public
void
testRenameProtectSubDirs
(
)
throws
Throwable
{
for
(
TestMatrixEntry
testMatrixEntry
:
createTestMatrixForProtectSubDirs
(
)
)
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
conf
.
setBoolean
(
DFS_PROTECTED_SUBDIRECTORIES_ENABLE
,
true
)
;
MiniDFSCluster
cluster
=
setupTestCase
(
conf
,
testMatrixEntry
.
getProtectedPaths
(
)
,
testMatrixEntry
.
getUnprotectedPaths
(
)
)
;
try
{
@
Test
public
void
testMoveProtectedSubDirsToTrash
(
)
throws
Throwable
{
for
(
TestMatrixEntry
testMatrixEntry
:
createTestMatrixForProtectSubDirs
(
)
)
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
conf
.
setBoolean
(
DFS_PROTECTED_SUBDIRECTORIES_ENABLE
,
true
)
;
conf
.
setInt
(
DFSConfigKeys
.
FS_TRASH_INTERVAL_KEY
,
3600
)
;
MiniDFSCluster
cluster
=
setupTestCase
(
conf
,
testMatrixEntry
.
getProtectedPaths
(
)
,
testMatrixEntry
.
getUnprotectedPaths
(
)
)
;
try
{
@
Test
public
void
testDeleteProtectSubDirs
(
)
throws
Throwable
{
for
(
TestMatrixEntry
testMatrixEntry
:
createTestMatrixForProtectSubDirs
(
)
)
{
Configuration
conf
=
new
HdfsConfiguration
(
)
;
conf
.
setBoolean
(
DFS_PROTECTED_SUBDIRECTORIES_ENABLE
,
true
)
;
MiniDFSCluster
cluster
=
setupTestCase
(
conf
,
testMatrixEntry
.
getProtectedPaths
(
)
,
testMatrixEntry
.
getUnprotectedPaths
(
)
)
;
try
{
dfs
.
setStoragePolicy
(
bar
,
HdfsConstants
.
ONESSD_STORAGE_POLICY_NAME
)
;
dfs
.
setQuotaByStorageType
(
foo
,
StorageType
.
SSD
,
BLOCKSIZE
*
4
)
;
dfs
.
setQuotaByStorageType
(
bar
,
StorageType
.
SSD
,
BLOCKSIZE
*
2
)
;
INode
fnode
=
fsdir
.
getINode4Write
(
foo
.
toString
(
)
)
;
assertTrue
(
fnode
.
isDirectory
(
)
)
;
assertTrue
(
fnode
.
isQuotaSet
(
)
)
;
long
file1Len
=
BLOCKSIZE
*
3
;
int
bufLen
=
BLOCKSIZE
/
16
;
DFSTestUtil
.
createFile
(
dfs
,
createdFile1foo
,
bufLen
,
file1Len
,
BLOCKSIZE
,
REPLICATION
,
seed
)
;
long
ssdConsumed
=
fnode
.
asDirectory
(
)
.
getDirectoryWithQuotaFeature
(
)
.
getSpaceConsumed
(
)
.
getTypeSpaces
(
)
.
get
(
StorageType
.
SSD
)
;
assertEquals
(
file1Len
,
ssdConsumed
)
;
try
{
dfs
.
rename
(
createdFile1foo
,
createdFile1bar
)
;
fail
(
)
;
}
catch
(
Throwable
t
)
{
int
bufLen
=
BLOCKSIZE
/
16
;
DFSTestUtil
.
createFile
(
dfs
,
createdFile1
,
bufLen
,
file1Len
,
BLOCKSIZE
,
REPLICATION
,
seed
)
;
long
currentSSDConsumed
=
fnode
.
asDirectory
(
)
.
getDirectoryWithQuotaFeature
(
)
.
getSpaceConsumed
(
)
.
getTypeSpaces
(
)
.
get
(
StorageType
.
SSD
)
;
assertEquals
(
file1Len
,
currentSSDConsumed
)
;
Path
createdFile2
=
new
Path
(
foo
,
)
;
long
file2Len
=
BLOCKSIZE
+
BLOCKSIZE
/
2
;
DFSTestUtil
.
createFile
(
dfs
,
createdFile2
,
bufLen
,
file2Len
,
BLOCKSIZE
,
REPLICATION
,
seed
)
;
currentSSDConsumed
=
fnode
.
asDirectory
(
)
.
getDirectoryWithQuotaFeature
(
)
.
getSpaceConsumed
(
)
.
getTypeSpaces
(
)
.
get
(
StorageType
.
SSD
)
;
assertEquals
(
file1Len
+
file2Len
,
currentSSDConsumed
)
;
Path
createdFile3
=
new
Path
(
foo
,
)
;
long
file3Len
=
BLOCKSIZE
;
try
{
DFSTestUtil
.
createFile
(
dfs
,
createdFile3
,
bufLen
,
file3Len
,
BLOCKSIZE
,
REPLICATION
,
seed
)
;
fail
(
)
;
}
catch
(
Throwable
t
)
{
Path
createdFile1
=
new
Path
(
child
,
)
;
long
file1Len
=
BLOCKSIZE
*
2
+
BLOCKSIZE
/
2
;
int
bufLen
=
BLOCKSIZE
/
16
;
DFSTestUtil
.
createFile
(
dfs
,
createdFile1
,
bufLen
,
file1Len
,
BLOCKSIZE
,
replication
,
seed
)
;
INode
fnode
=
fsdir
.
getINode4Write
(
parent
.
toString
(
)
)
;
assertTrue
(
fnode
.
isDirectory
(
)
)
;
assertTrue
(
fnode
.
isQuotaSet
(
)
)
;
long
currentSSDConsumed
=
fnode
.
asDirectory
(
)
.
getDirectoryWithQuotaFeature
(
)
.
getSpaceConsumed
(
)
.
getTypeSpaces
(
)
.
get
(
StorageType
.
SSD
)
;
assertEquals
(
file1Len
,
currentSSDConsumed
)
;
Path
createdFile2
=
new
Path
(
child
,
)
;
long
file2Len
=
BLOCKSIZE
;
try
{
DFSTestUtil
.
createFile
(
dfs
,
createdFile2
,
bufLen
,
file2Len
,
BLOCKSIZE
,
replication
,
seed
)
;
fail
(
)
;
}
catch
(
Throwable
t
)
{
@
Test
(
timeout
=
60000
)
public
void
testQuotaByStorageTypeParentOnChildOn
(
)
throws
Exception
{
final
Path
parent
=
new
Path
(
dir
,
)
;
final
Path
child
=
new
Path
(
parent
,
)
;
dfs
.
mkdirs
(
parent
)
;
dfs
.
mkdirs
(
child
)
;
dfs
.
setStoragePolicy
(
parent
,
HdfsConstants
.
ONESSD_STORAGE_POLICY_NAME
)
;
dfs
.
setQuotaByStorageType
(
parent
,
StorageType
.
SSD
,
2
*
BLOCKSIZE
)
;
dfs
.
setQuotaByStorageType
(
child
,
StorageType
.
SSD
,
3
*
BLOCKSIZE
)
;
Path
createdFile1
=
new
Path
(
child
,
)
;
long
file1Len
=
BLOCKSIZE
*
2
+
BLOCKSIZE
/
2
;
int
bufLen
=
BLOCKSIZE
/
16
;
try
{
DFSTestUtil
.
createFile
(
dfs
,
createdFile1
,
bufLen
,
file1Len
,
BLOCKSIZE
,
REPLICATION
,
seed
)
;
fail
(
)
;
}
catch
(
Throwable
t
)
{
dfs
.
mkdirs
(
testDir
)
;
dfs
.
setStoragePolicy
(
testDir
,
HdfsConstants
.
ONESSD_STORAGE_POLICY_NAME
)
;
final
long
ssdQuota
=
BLOCKSIZE
*
ssdQuotaInBlocks
;
final
long
storageSpaceQuota
=
BLOCKSIZE
*
storageSpaceQuotaInBlocks
;
dfs
.
setQuota
(
testDir
,
Long
.
MAX_VALUE
-
1
,
storageSpaceQuota
)
;
dfs
.
setQuotaByStorageType
(
testDir
,
StorageType
.
SSD
,
ssdQuota
)
;
INode
testDirNode
=
fsdir
.
getINode4Write
(
testDir
.
toString
(
)
)
;
assertTrue
(
testDirNode
.
isDirectory
(
)
)
;
assertTrue
(
testDirNode
.
isQuotaSet
(
)
)
;
Path
createdFile
=
new
Path
(
testDir
,
)
;
long
fileLen
=
testFileLenInBlocks
*
BLOCKSIZE
;
try
{
DFSTestUtil
.
createFile
(
dfs
,
createdFile
,
BLOCKSIZE
/
16
,
fileLen
,
BLOCKSIZE
,
replication
,
seed
)
;
fail
(
+
)
;
}
catch
(
Throwable
t
)
{
final
long
storageTypeSpaceQuota
=
BLOCKSIZE
*
1
;
dfs
.
setQuota
(
testDir
,
HdfsConstants
.
QUOTA_DONT_SET
,
storageSpaceQuota
)
;
Path
createdFile
;
final
long
fileLen
=
BLOCKSIZE
;
createdFile
=
new
Path
(
testDir
,
)
;
DFSTestUtil
.
createFile
(
dfs
,
createdFile
,
BLOCKSIZE
/
16
,
fileLen
,
BLOCKSIZE
,
REPLICATION
,
seed
)
;
assertTrue
(
dfs
.
exists
(
createdFile
)
)
;
assertTrue
(
dfs
.
isFile
(
createdFile
)
)
;
dfs
.
setQuotaByStorageType
(
testDir
,
StorageType
.
DISK
,
storageTypeSpaceQuota
)
;
dfs
.
setStoragePolicy
(
testDir
,
HdfsConstants
.
WARM_STORAGE_POLICY_NAME
)
;
try
{
createdFile
=
new
Path
(
testDir
,
)
;
DFSTestUtil
.
createFile
(
dfs
,
createdFile
,
BLOCKSIZE
/
16
,
fileLen
,
BLOCKSIZE
,
REPLICATION
,
seed
)
;
fail
(
)
;
}
catch
(
QuotaByStorageTypeExceededException
e
)
{
assertNotEquals
(
fei0
.
getEzKeyVersionName
(
)
,
zs
.
getEzKeyVersionName
(
)
)
;
assertEquals
(
fei1
.
getEzKeyVersionName
(
)
,
zs
.
getEzKeyVersionName
(
)
)
;
assertEquals
(
10
,
zs
.
getFilesReencrypted
(
)
)
;
dfsAdmin
.
reencryptEncryptionZone
(
zone
,
ReencryptAction
.
START
)
;
waitForReencryptedZones
(
3
)
;
assertKeyVersionEquals
(
encFile1
,
fei1
)
;
try
{
dfsAdmin
.
reencryptEncryptionZone
(
subdir
,
ReencryptAction
.
START
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertExceptionContains
(
,
expected
)
;
}
try
{
dfsAdmin
.
reencryptEncryptionZone
(
new
Path
(
zone
,
)
,
ReencryptAction
.
START
)
;
fail
(
)
;
try
{
dfsAdmin
.
reencryptEncryptionZone
(
subdir
,
ReencryptAction
.
START
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertExceptionContains
(
,
expected
)
;
}
try
{
dfsAdmin
.
reencryptEncryptionZone
(
new
Path
(
zone
,
)
,
ReencryptAction
.
START
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertTrue
(
expected
.
unwrapRemoteException
(
)
instanceof
FileNotFoundException
)
;
}
try
{
dfsAdmin
.
reencryptEncryptionZone
(
encFile1
,
ReencryptAction
.
START
)
;
fail
(
)
;
try
{
dfsAdmin
.
reencryptEncryptionZone
(
new
Path
(
zone
,
)
,
ReencryptAction
.
START
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertTrue
(
expected
.
unwrapRemoteException
(
)
instanceof
FileNotFoundException
)
;
}
try
{
dfsAdmin
.
reencryptEncryptionZone
(
encFile1
,
ReencryptAction
.
START
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertExceptionContains
(
,
expected
)
;
}
getEzManager
(
)
.
pauseReencryptForTesting
(
)
;
dfsAdmin
.
reencryptEncryptionZone
(
zone
,
ReencryptAction
.
START
)
;
waitForQueuedZones
(
1
)
;
DFSTestUtil
.
createFile
(
fs
,
new
Path
(
zone
,
Integer
.
toString
(
i
)
)
,
len
,
(
short
)
1
,
0xFEED
)
;
}
final
Path
subdir
=
new
Path
(
)
;
fsWrapper
.
mkdir
(
subdir
,
FsPermission
.
getDirDefault
(
)
,
true
)
;
DFSTestUtil
.
createFile
(
fs
,
new
Path
(
subdir
,
)
,
len
,
(
short
)
1
,
0xFEED
)
;
final
Path
zoneSnap
=
fs
.
createSnapshot
(
zone
)
;
fsWrapper
.
rename
(
new
Path
(
zone
,
)
,
new
Path
(
zone
,
)
)
;
fsWrapper
.
rename
(
new
Path
(
zone
,
)
,
new
Path
(
zone
,
)
)
;
fsWrapper
.
delete
(
new
Path
(
zone
,
)
,
true
)
;
final
Path
encFile1
=
new
Path
(
zone
,
)
;
final
FileEncryptionInfo
fei0
=
getFileEncryptionInfo
(
encFile1
)
;
rollKey
(
TEST_KEY
)
;
try
{
dfsAdmin
.
reencryptEncryptionZone
(
zoneSnap
,
ReencryptAction
.
START
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
private
void
waitForReencryptedZones
(
final
int
expected
)
throws
TimeoutException
,
InterruptedException
{
private
void
waitForQueuedZones
(
final
int
expected
)
throws
TimeoutException
,
InterruptedException
{
private
void
waitForTotalZones
(
final
int
expected
)
throws
TimeoutException
,
InterruptedException
{
private
void
waitForZoneCompletes
(
final
String
zone
)
throws
TimeoutException
,
InterruptedException
{
private
void
waitForReencryptedFiles
(
final
String
zone
,
final
int
expected
)
throws
TimeoutException
,
InterruptedException
{
catch
(
RemoteException
expected
)
{
assertExceptionContains
(
,
expected
)
;
}
rollKey
(
TEST_KEY
)
;
getEzManager
(
)
.
pauseForTestingAfterNthSubmission
(
1
)
;
getEzManager
(
)
.
resumeReencryptForTesting
(
)
;
dfsAdmin
.
reencryptEncryptionZone
(
zone
,
ReencryptAction
.
START
)
;
waitForReencryptedFiles
(
zone
.
toString
(
)
,
5
)
;
dfsAdmin
.
reencryptEncryptionZone
(
zone
,
ReencryptAction
.
CANCEL
)
;
getEzManager
(
)
.
resumeReencryptForTesting
(
)
;
waitForZoneCompletes
(
zone
.
toString
(
)
)
;
assertEquals
(
5
,
getZoneStatus
(
zone
.
toString
(
)
)
.
getFilesReencrypted
(
)
)
;
assertNull
(
getEzManager
(
)
.
getZoneStatus
(
zone
.
toString
(
)
)
.
getLastCheckpointFile
(
)
)
;
assertNull
(
getReencryptionStatus
(
)
.
getNextUnprocessedZone
(
)
)
;
try
{
dfsAdmin
.
reencryptEncryptionZone
(
subdir
,
ReencryptAction
.
CANCEL
)
;
dfsAdmin
.
reencryptEncryptionZone
(
zone
,
ReencryptAction
.
START
)
;
waitForReencryptedFiles
(
zone
.
toString
(
)
,
5
)
;
dfsAdmin
.
reencryptEncryptionZone
(
zone
,
ReencryptAction
.
CANCEL
)
;
getEzManager
(
)
.
resumeReencryptForTesting
(
)
;
waitForZoneCompletes
(
zone
.
toString
(
)
)
;
assertEquals
(
5
,
getZoneStatus
(
zone
.
toString
(
)
)
.
getFilesReencrypted
(
)
)
;
assertNull
(
getEzManager
(
)
.
getZoneStatus
(
zone
.
toString
(
)
)
.
getLastCheckpointFile
(
)
)
;
assertNull
(
getReencryptionStatus
(
)
.
getNextUnprocessedZone
(
)
)
;
try
{
dfsAdmin
.
reencryptEncryptionZone
(
subdir
,
ReencryptAction
.
CANCEL
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertExceptionContains
(
,
expected
)
;
}
try
{
assertNull
(
getReencryptionStatus
(
)
.
getNextUnprocessedZone
(
)
)
;
try
{
dfsAdmin
.
reencryptEncryptionZone
(
subdir
,
ReencryptAction
.
CANCEL
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertExceptionContains
(
,
expected
)
;
}
try
{
dfsAdmin
.
reencryptEncryptionZone
(
new
Path
(
zone
,
)
,
ReencryptAction
.
CANCEL
)
;
fail
(
)
;
}
catch
(
RemoteException
expected
)
{
LOG
.
info
(
,
expected
)
;
assertTrue
(
expected
.
unwrapRemoteException
(
)
instanceof
FileNotFoundException
)
;
}
final
Path
encFile
=
new
Path
(
zone
,
)
;
try
{
shouldFail
=
false
;
break
;
default
:
fail
(
)
;
break
;
}
try
{
doAnEdit
(
fsn
,
1
)
;
fsn
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_ENTER
)
;
try
{
fsn
.
saveNamespace
(
0
,
0
)
;
if
(
shouldFail
)
{
fail
(
)
;
}
}
catch
(
Exception
e
)
{
if
(
!
shouldFail
)
{
throw
e
;
}
else
{
originalImage
.
close
(
)
;
fsn
.
close
(
)
;
fsn
=
null
;
LOG
.
info
(
)
;
fsn
=
FSNamesystem
.
loadFromDisk
(
conf
)
;
LOG
.
info
(
)
;
checkEditExists
(
fsn
,
1
)
;
LOG
.
info
(
)
;
}
finally
{
if
(
rootDir
.
exists
(
)
)
{
fs
.
setPermission
(
rootPath
,
permissionAll
)
;
}
if
(
fsn
!=
null
)
{
try
{
fsn
.
close
(
)
;
}
catch
(
Throwable
t
)
{
NNStorage
storage
=
originalImage
.
getStorage
(
)
;
storage
.
close
(
)
;
NNStorage
spyStorage
=
spy
(
storage
)
;
originalImage
.
storage
=
spyStorage
;
FSImage
spyImage
=
spy
(
originalImage
)
;
Whitebox
.
setInternalState
(
fsn
,
,
spyImage
)
;
spyImage
.
storage
.
setStorageDirectories
(
FSNamesystem
.
getNamespaceDirs
(
conf
)
,
FSNamesystem
.
getNamespaceEditsDirs
(
conf
)
)
;
doThrow
(
new
IOException
(
)
)
.
when
(
spyImage
)
.
saveFSImage
(
any
(
)
,
any
(
)
,
any
(
)
)
;
try
{
doAnEdit
(
fsn
,
1
)
;
fsn
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_ENTER
)
;
try
{
fsn
.
saveNamespace
(
0
,
0
)
;
fail
(
)
;
}
catch
(
IOException
ioe
)
{
public
void
createCheckPoint
(
int
count
)
throws
IOException
{
LOG
.
info
(
)
;
MiniDFSCluster
cluster
=
null
;
SecondaryNameNode
sn
=
null
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
config
)
.
manageDataDfsDirs
(
false
)
.
manageNameDfsDirs
(
false
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
LOG
.
info
(
)
;
sn
=
new
SecondaryNameNode
(
config
)
;
assertNotNull
(
sn
)
;
for
(
int
i
=
0
;
i
<
count
;
i
++
)
{
FileSystem
fileSys
=
cluster
.
getFileSystem
(
)
;
Path
p
=
new
Path
(
+
i
)
;
DFSTestUtil
.
createFile
(
fileSys
,
p
,
fileSize
,
fileSize
,
blockSize
,
(
short
)
1
,
seed
)
;
public
void
invalidateStorage
(
FSImage
fi
,
Set
<
File
>
filesToInvalidate
)
throws
IOException
{
ArrayList
<
StorageDirectory
>
al
=
new
ArrayList
<
StorageDirectory
>
(
2
)
;
Iterator
<
StorageDirectory
>
it
=
fi
.
getStorage
(
)
.
dirIterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
StorageDirectory
sd
=
it
.
next
(
)
;
if
(
filesToInvalidate
.
contains
(
sd
.
getRoot
(
)
)
)
{
@
Test
public
void
testDfsAdminCmd
(
)
throws
Exception
{
cluster
=
new
MiniDFSCluster
.
Builder
(
config
)
.
numDataNodes
(
2
)
.
manageNameDfsDirs
(
false
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
try
{
FSImage
fsi
=
cluster
.
getNameNode
(
)
.
getFSImage
(
)
;
boolean
restore
=
fsi
.
getStorage
(
)
.
getRestoreFailedStorage
(
)
;
INodeFile
inodeFile
;
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
new
Short
(
(
short
)
3
)
,
StripedFileTestUtil
.
getDefaultECPolicy
(
)
.
getId
(
)
,
1024L
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
STRIPED
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
null
,
null
,
1024L
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
STRIPED
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
null
,
Byte
.
MAX_VALUE
,
1024L
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
STRIPED
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
null
,
null
,
1024L
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
STRIPED
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
null
,
Byte
.
MAX_VALUE
,
1024L
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
STRIPED
)
;
fail
(
+
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
info
(
,
iae
)
;
}
final
Byte
ecPolicyID
=
StripedFileTestUtil
.
getDefaultECPolicy
(
)
.
getId
(
)
;
try
{
new
INodeFile
(
HdfsConstants
.
GRANDFATHER_INODE_ID
,
null
,
perm
,
0L
,
0L
,
null
,
null
,
ecPolicyID
,
1024L
,
HdfsConstants
.
WARM_STORAGE_POLICY_ID
,
CONTIGUOUS
)
;
fail
(
+
)
;
@
Test
public
void
testDownloadingLaterCheckpoint
(
)
throws
Exception
{
nn0
.
getRpcServer
(
)
.
rollEditLog
(
)
;
nn0
.
getRpcServer
(
)
.
rollEditLog
(
)
;
NameNodeAdapter
.
enterSafeMode
(
nn0
,
false
)
;
NameNodeAdapter
.
saveNamespace
(
nn0
)
;
NameNodeAdapter
.
leaveSafeMode
(
nn0
)
;
long
expectedCheckpointTxId
=
NameNodeAdapter
.
getNamesystem
(
nn0
)
.
getFSImage
(
)
.
getMostRecentCheckpointTxId
(
)
;
assertEquals
(
6
,
expectedCheckpointTxId
)
;
cluster
.
getFileSystem
(
0
)
.
create
(
new
Path
(
)
,
(
short
)
1
)
.
close
(
)
;
URI
editsUri
=
cluster
.
getSharedEditsDir
(
0
,
maxNNCount
-
1
)
;
long
seen_txid_shared
=
FSImageTestUtil
.
getStorageTxId
(
nn0
,
editsUri
)
;
for
(
int
i
=
1
;
i
<
maxNNCount
;
i
++
)
{
assertEquals
(
0
,
forceBootstrap
(
i
)
)
;
private
void
removeStandbyNameDirs
(
)
{
for
(
int
i
=
1
;
i
<
maxNNCount
;
i
++
)
{
for
(
URI
u
:
cluster
.
getNameDirs
(
i
)
)
{
assertTrue
(
u
.
getScheme
(
)
.
equals
(
)
)
;
File
dir
=
new
File
(
u
.
getPath
(
)
)
;
@
Test
public
void
testStartingWithUpgradeInProgressSucceeds
(
)
throws
Exception
{
MiniDFSCluster
cluster
=
null
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
MiniDFSNNTopology
.
simpleHATopology
(
)
)
.
numDataNodes
(
0
)
.
build
(
)
;
for
(
int
i
=
0
;
i
<
2
;
i
++
)
{
for
(
URI
uri
:
cluster
.
getNameDirs
(
i
)
)
{
File
prevTmp
=
new
File
(
new
File
(
uri
)
,
Storage
.
STORAGE_TMP_PREVIOUS
)
;
AppendTestUtil
.
write
(
out
,
10
,
10
)
;
out
.
hflush
(
)
;
cluster
.
triggerBlockReports
(
)
;
numQueued
+=
numDN
*
2
;
}
finally
{
IOUtils
.
closeStream
(
out
)
;
cluster
.
triggerHeartbeats
(
)
;
numQueued
+=
numDN
;
}
assertEquals
(
numQueued
,
cluster
.
getNameNode
(
1
)
.
getNamesystem
(
)
.
getPendingDataNodeMessageCount
(
)
)
;
try
{
out
=
fs
.
append
(
TEST_FILE_PATH
)
;
AppendTestUtil
.
write
(
out
,
20
,
10
)
;
}
finally
{
IOUtils
.
closeStream
(
out
)
;
numQueued
+=
numDN
;
private
void
banner
(
String
string
)
{
identifier
.
readFields
(
new
DataInputStream
(
new
ByteArrayInputStream
(
tokenId
)
)
)
;
LOG
.
info
(
+
)
;
assertTrue
(
null
!=
dtSecretManager
.
retrievePassword
(
identifier
)
)
;
dtSecretManager
.
renewToken
(
token
,
)
;
cluster
.
transitionToStandby
(
0
)
;
try
{
cluster
.
getNameNodeRpc
(
0
)
.
renewDelegationToken
(
token
)
;
fail
(
)
;
}
catch
(
StandbyException
e
)
{
GenericTestUtils
.
assertExceptionContains
(
HAServiceState
.
STANDBY
.
toString
(
)
,
e
)
;
}
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
cluster
.
transitionToActive
(
1
)
;
}
catch
(
Exception
e
)
{
}
catch
(
StandbyException
e
)
{
GenericTestUtils
.
assertExceptionContains
(
HAServiceState
.
STANDBY
.
toString
(
)
,
e
)
;
}
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
cluster
.
transitionToActive
(
1
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
}
}
.
start
(
)
;
Thread
.
sleep
(
1000
)
;
try
{
nn1
.
getNamesystem
(
)
.
verifyToken
(
token
.
decodeIdentifier
(
)
,
token
.
getPassword
(
)
)
;
fail
(
)
;
}
catch
(
IOException
e
)
{
assertTrue
(
e
instanceof
StandbyException
||
e
instanceof
RetriableException
)
;
private
void
testFailoverFinalizesAndReadsInProgress
(
boolean
partialTxAtEnd
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
MiniDFSNNTopology
.
simpleHATopology
(
)
)
.
numDataNodes
(
0
)
.
build
(
)
;
try
{
URI
sharedUri
=
cluster
.
getSharedEditsDir
(
0
,
1
)
;
File
sharedDir
=
new
File
(
sharedUri
.
getPath
(
)
,
)
;
FSNamesystem
fsn
=
cluster
.
getNamesystem
(
0
)
;
FSImageTestUtil
.
createAbortedLogWithMkdirs
(
sharedDir
,
NUM_DIRS_IN_LOG
,
1
,
fsn
.
getFSDirectory
(
)
.
getLastInodeId
(
)
+
1
)
;
assertEditFiles
(
Collections
.
singletonList
(
sharedUri
)
,
NNStorage
.
getInProgressEditsFileName
(
1
)
)
;
if
(
partialTxAtEnd
)
{
FileOutputStream
outs
=
null
;
try
{
File
editLogFile
=
new
File
(
sharedDir
,
NNStorage
.
getInProgressEditsFileName
(
1
)
)
;
outs
=
new
FileOutputStream
(
editLogFile
,
true
)
;
outs
.
write
(
new
byte
[
]
{
0x18
,
0x00
,
0x00
,
0x00
}
)
;
private
void
assertEditFiles
(
Iterable
<
URI
>
dirs
,
String
...
files
)
throws
IOException
{
for
(
URI
u
:
dirs
)
{
File
editDirRoot
=
new
File
(
u
.
getPath
(
)
)
;
File
editDir
=
new
File
(
editDirRoot
,
)
;
GenericTestUtils
.
assertExists
(
editDir
)
;
if
(
files
.
length
==
0
)
{
@
Before
public
void
setUpCluster
(
)
throws
Exception
{
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_CHECKPOINT_CHECK_PERIOD_KEY
,
1
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_CHECKPOINT_TXNS_KEY
,
1
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED_KEY
,
10
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_HA_TAILEDITS_PERIOD_KEY
,
1
)
;
conf
.
setBoolean
(
DFSConfigKeys
.
DFS_NAMENODE_EDITS_ASYNC_LOGGING
,
useAsyncEditLogging
)
;
HAUtil
.
setAllowStandbyReads
(
conf
,
true
)
;
if
(
clusterType
==
TestType
.
SHARED_DIR_HA
)
{
int
basePort
=
10000
;
int
retryCount
=
0
;
while
(
true
)
{
try
{
basePort
=
10000
+
RANDOM
.
nextInt
(
1000
)
*
4
;
if
(
clusterType
==
TestType
.
SHARED_DIR_HA
)
{
int
basePort
=
10000
;
int
retryCount
=
0
;
while
(
true
)
{
try
{
basePort
=
10000
+
RANDOM
.
nextInt
(
1000
)
*
4
;
LOG
.
info
(
+
basePort
)
;
MiniDFSNNTopology
topology
=
MiniQJMHACluster
.
createDefaultTopology
(
basePort
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
topology
)
.
numDataNodes
(
0
)
.
checkExitOnShutdown
(
false
)
.
build
(
)
;
break
;
}
catch
(
BindException
e
)
{
if
(
cluster
!=
null
)
{
cluster
.
shutdown
(
true
)
;
cluster
=
null
;
}
++
retryCount
;
@
Test
(
timeout
=
300000
)
public
void
testClientRetrySafeMode
(
)
throws
Exception
{
final
Map
<
Path
,
Boolean
>
results
=
Collections
.
synchronizedMap
(
new
HashMap
<
Path
,
Boolean
>
(
)
)
;
final
Path
test
=
new
Path
(
)
;
cluster
.
getConfiguration
(
0
)
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY
,
3
)
;
NameNodeAdapter
.
enterSafeMode
(
nn0
,
false
)
;
Whitebox
.
setInternalState
(
nn0
.
getNamesystem
(
)
,
,
false
)
;
BlockManagerTestUtil
.
setStartupSafeModeForTest
(
nn0
.
getNamesystem
(
)
.
getBlockManager
(
)
)
;
assertTrue
(
nn0
.
getNamesystem
(
)
.
isInStartupSafeMode
(
)
)
;
LOG
.
info
(
)
;
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
boolean
mkdir
=
fs
.
mkdirs
(
test
)
;
cluster
.
getConfiguration
(
0
)
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY
,
3
)
;
NameNodeAdapter
.
enterSafeMode
(
nn0
,
false
)
;
Whitebox
.
setInternalState
(
nn0
.
getNamesystem
(
)
,
,
false
)
;
BlockManagerTestUtil
.
setStartupSafeModeForTest
(
nn0
.
getNamesystem
(
)
.
getBlockManager
(
)
)
;
assertTrue
(
nn0
.
getNamesystem
(
)
.
isInStartupSafeMode
(
)
)
;
LOG
.
info
(
)
;
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
boolean
mkdir
=
fs
.
mkdirs
(
test
)
;
LOG
.
info
(
+
mkdir
)
;
synchronized
(
TestHASafeMode
.
this
)
{
results
.
put
(
test
,
mkdir
)
;
TestHASafeMode
.
this
.
notifyAll
(
)
;
}
}
catch
(
Exception
e
)
{
static
void
banner
(
String
string
)
{
private
void
testManualFailoverFailback
(
MiniDFSCluster
cluster
,
Configuration
conf
,
int
nsIndex
)
throws
Exception
{
int
nn0
=
2
*
nsIndex
,
nn1
=
2
*
nsIndex
+
1
;
cluster
.
transitionToActive
(
nn0
)
;
private
void
testManualFailoverFailback
(
MiniDFSCluster
cluster
,
Configuration
conf
,
int
nsIndex
)
throws
Exception
{
int
nn0
=
2
*
nsIndex
,
nn1
=
2
*
nsIndex
+
1
;
cluster
.
transitionToActive
(
nn0
)
;
LOG
.
info
(
+
nsIndex
)
;
FileSystem
fs
=
HATestUtil
.
configureFailoverFs
(
cluster
,
conf
)
;
fs
.
mkdirs
(
TEST_DIR
)
;
private
void
testManualFailoverFailback
(
MiniDFSCluster
cluster
,
Configuration
conf
,
int
nsIndex
)
throws
Exception
{
int
nn0
=
2
*
nsIndex
,
nn1
=
2
*
nsIndex
+
1
;
cluster
.
transitionToActive
(
nn0
)
;
LOG
.
info
(
+
nsIndex
)
;
FileSystem
fs
=
HATestUtil
.
configureFailoverFs
(
cluster
,
conf
)
;
fs
.
mkdirs
(
TEST_DIR
)
;
LOG
.
info
(
+
nsIndex
)
;
cluster
.
transitionToStandby
(
nn0
)
;
cluster
.
transitionToActive
(
nn1
)
;
assertTrue
(
fs
.
exists
(
TEST_DIR
)
)
;
DFSTestUtil
.
writeFile
(
fs
,
TEST_FILE_PATH
,
TEST_FILE_DATA
)
;
FileSystem
fs
=
HATestUtil
.
configureFailoverFs
(
cluster
,
conf
)
;
fs
.
mkdirs
(
TEST_DIR
)
;
LOG
.
info
(
+
nsIndex
)
;
cluster
.
transitionToStandby
(
nn0
)
;
cluster
.
transitionToActive
(
nn1
)
;
assertTrue
(
fs
.
exists
(
TEST_DIR
)
)
;
DFSTestUtil
.
writeFile
(
fs
,
TEST_FILE_PATH
,
TEST_FILE_DATA
)
;
LOG
.
info
(
+
nsIndex
)
;
cluster
.
transitionToStandby
(
nn1
)
;
cluster
.
transitionToActive
(
nn0
)
;
assertTrue
(
fs
.
exists
(
TEST_DIR
)
)
;
assertEquals
(
TEST_FILE_DATA
,
DFSTestUtil
.
readFile
(
fs
,
TEST_FILE_PATH
)
)
;
LOG
.
info
(
)
;
fs
.
delete
(
TEST_DIR
,
true
)
;
assertFalse
(
fs
.
exists
(
TEST_DIR
)
)
;
static
void
banner
(
String
string
)
{
@
Test
public
void
testFsckWithObserver
(
)
throws
Exception
{
setObserverRead
(
true
)
;
dfs
.
create
(
testPath
,
(
short
)
1
)
.
close
(
)
;
assertSentTo
(
0
)
;
final
String
result
=
TestFsck
.
runFsck
(
conf
,
0
,
true
,
)
;
@
Test
public
void
testFsckDelete
(
)
throws
Exception
{
setObserverRead
(
true
)
;
DFSTestUtil
.
createFile
(
dfs
,
testPath
,
512
,
(
short
)
1
,
0
)
;
DFSTestUtil
.
waitForReplication
(
dfs
,
testPath
,
(
short
)
1
,
5000
)
;
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
dfs
,
testPath
)
;
int
dnToCorrupt
=
DFSTestUtil
.
firstDnWithBlock
(
dfsCluster
,
block
)
;
FSNamesystem
ns
=
dfsCluster
.
getNameNode
(
0
)
.
getNamesystem
(
)
;
dfsCluster
.
corruptReplica
(
dnToCorrupt
,
block
)
;
dfsCluster
.
restartDataNode
(
dnToCorrupt
)
;
DFSTestUtil
.
waitCorruptReplicas
(
dfs
,
ns
,
testPath
,
block
,
1
)
;
final
String
result
=
TestFsck
.
runFsck
(
conf
,
1
,
true
,
,
)
;
MiniDFSCluster
cluster
=
newMiniCluster
(
conf
,
5
)
;
try
{
cluster
.
waitActive
(
)
;
cluster
.
transitionToActive
(
0
)
;
Thread
.
sleep
(
500
)
;
LOG
.
info
(
)
;
FileSystem
fs
=
HATestUtil
.
configureFailoverFs
(
cluster
,
conf
)
;
stm
=
fs
.
create
(
TEST_PATH
)
;
AppendTestUtil
.
write
(
stm
,
0
,
BLOCK_AND_A_HALF
)
;
stm
.
hflush
(
)
;
int
nextActive
=
failover
(
cluster
,
scenario
)
;
assertTrue
(
fs
.
exists
(
TEST_PATH
)
)
;
cluster
.
stopDataNode
(
0
)
;
AppendTestUtil
.
write
(
stm
,
BLOCK_AND_A_HALF
,
BLOCK_AND_A_HALF
)
;
stm
.
hflush
(
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_BLOCK_SIZE_KEY
,
BLOCK_SIZE
)
;
FSDataOutputStream
stm
=
null
;
final
MiniDFSCluster
cluster
=
newMiniCluster
(
conf
,
3
)
;
try
{
cluster
.
waitActive
(
)
;
cluster
.
transitionToActive
(
0
)
;
Thread
.
sleep
(
500
)
;
LOG
.
info
(
)
;
FileSystem
fs
=
HATestUtil
.
configureFailoverFs
(
cluster
,
conf
)
;
stm
=
fs
.
create
(
TEST_PATH
)
;
AppendTestUtil
.
write
(
stm
,
0
,
BLOCK_SIZE
/
2
)
;
stm
.
hflush
(
)
;
NameNode
nn0
=
cluster
.
getNameNode
(
0
)
;
ExtendedBlock
blk
=
DFSTestUtil
.
getFirstBlock
(
fs
,
TEST_PATH
)
;
DatanodeDescriptor
expectedPrimary
=
DFSTestUtil
.
getExpectedPrimaryNode
(
nn0
,
blk
)
;
@
Test
(
timeout
=
STRESS_RUNTIME
*
3
)
public
void
testPipelineRecoveryStress
(
)
throws
Exception
{
LOG
.
info
(
)
;
String
[
]
[
]
scmds
=
new
String
[
]
[
]
{
{
,
,
}
,
{
}
,
{
,
}
}
;
for
(
String
[
]
scmd
:
scmds
)
{
String
scmd_str
=
StringUtils
.
join
(
,
scmd
)
;
try
{
ShellCommandExecutor
sce
=
new
ShellCommandExecutor
(
scmd
)
;
sce
.
execute
(
)
;
public
void
testClientRetryWithFailover
(
final
AtMostOnceOp
op
)
throws
Exception
{
final
Map
<
String
,
Object
>
results
=
new
ConcurrentHashMap
<
>
(
)
;
op
.
prepare
(
)
;
DummyRetryInvocationHandler
.
block
.
set
(
true
)
;
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
op
.
invoke
(
)
;
Object
result
=
op
.
getResult
(
)
;
op
.
invoke
(
)
;
Object
result
=
op
.
getResult
(
)
;
LOG
.
info
(
+
op
.
name
+
)
;
results
.
put
(
op
.
name
,
result
==
null
?
:
result
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
op
.
name
,
e
)
;
}
finally
{
IOUtils
.
cleanup
(
null
,
op
.
client
)
;
}
}
}
.
start
(
)
;
assertTrue
(
+
op
.
name
+
,
op
.
checkNamenodeBeforeReturn
(
)
)
;
cluster
.
transitionToStandby
(
0
)
;
cluster
.
transitionToActive
(
1
)
;
LOG
.
info
(
)
;
DummyRetryInvocationHandler
.
block
.
set
(
false
)
;
GenericTestUtils
.
waitFor
(
(
)
->
results
.
containsKey
(
op
.
name
)
,
5
,
10000
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED_KEY
,
1
)
;
conf
.
setInt
(
DFSConfigKeys
.
DFS_NAMENODE_NUM_EXTRA_EDITS_RETAINED_KEY
,
0
)
;
int
retryCount
=
0
;
while
(
true
)
{
try
{
int
basePort
=
10060
+
random
.
nextInt
(
100
)
*
2
;
MiniDFSNNTopology
topology
=
new
MiniDFSNNTopology
(
)
.
addNameservice
(
new
MiniDFSNNTopology
.
NSConf
(
)
.
addNN
(
new
MiniDFSNNTopology
.
NNConf
(
)
.
setHttpPort
(
basePort
)
)
.
addNN
(
new
MiniDFSNNTopology
.
NNConf
(
)
.
setHttpPort
(
basePort
+
1
)
)
.
addNN
(
new
MiniDFSNNTopology
.
NNConf
(
)
.
setHttpPort
(
basePort
+
2
)
)
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nnTopology
(
topology
)
.
numDataNodes
(
1
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
setNNs
(
)
;
fs
=
HATestUtil
.
configureFailoverFs
(
cluster
,
conf
)
;
cluster
.
transitionToActive
(
0
)
;
++
retryCount
;
break
;
}
catch
(
BindException
e
)
{
private
static
void
assertEditFiles
(
Iterable
<
URI
>
dirs
,
String
...
files
)
throws
IOException
{
for
(
URI
u
:
dirs
)
{
File
editDirRoot
=
new
File
(
u
.
getPath
(
)
)
;
File
editDir
=
new
File
(
editDirRoot
,
)
;
GenericTestUtils
.
assertExists
(
editDir
)
;
if
(
files
.
length
==
0
)
{
public
static
Path
createSnapshot
(
DistributedFileSystem
hdfs
,
Path
snapshotRoot
,
String
snapshotName
)
throws
Exception
{
@
Test
(
timeout
=
600000
)
public
void
testOpenFileDeletionAndNNRestart
(
)
throws
Exception
{
final
Path
snapRootDir
=
new
Path
(
)
;
final
String
hbaseFileName
=
;
final
String
snap1Name
=
;
final
Path
hbaseFile
=
new
Path
(
snapRootDir
,
hbaseFileName
)
;
createFile
(
hbaseFile
)
;
FSDataOutputStream
hbaseOutputStream
=
fs
.
append
(
hbaseFile
)
;
int
newWriteLength
=
(
int
)
(
BLOCKSIZE
*
1.5
)
;
byte
[
]
buf
=
new
byte
[
newWriteLength
]
;
Random
random
=
new
Random
(
)
;
random
.
nextBytes
(
buf
)
;
writeToStream
(
hbaseOutputStream
,
buf
)
;
final
Path
snap1Dir
=
SnapshotTestHelper
.
createSnapshot
(
fs
,
snapRootDir
,
snap1Name
)
;
flumeOutputStream
.
write
(
bytes
)
;
if
(
hbaseOutputStream
!=
null
)
{
hbaseOutputStream
.
write
(
bytes
)
;
}
if
(
i
==
50000
)
{
startLatch
.
countDown
(
)
;
}
else
if
(
i
==
100000
)
{
deleteLatch
.
countDown
(
)
;
}
else
if
(
i
==
150000
)
{
hbaseOutputStream
.
hsync
(
)
;
fs
.
delete
(
hbaseFile
,
true
)
;
try
{
hbaseOutputStream
.
close
(
)
;
}
catch
(
Exception
e
)
{
}
hbaseOutputStream
=
null
;
}
else
if
(
i
%
5000
==
0
)
{
else
if
(
i
==
150000
)
{
hbaseOutputStream
.
hsync
(
)
;
fs
.
delete
(
hbaseFile
,
true
)
;
try
{
hbaseOutputStream
.
close
(
)
;
}
catch
(
Exception
e
)
{
}
hbaseOutputStream
=
null
;
}
else
if
(
i
%
5000
==
0
)
{
LOG
.
info
(
+
flumeOutputStream
.
getPos
(
)
+
+
fs
.
getFileStatus
(
flumeFile
)
.
getLen
(
)
+
+
(
i
+
1
)
)
;
}
}
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
e
)
;
writerError
.
set
(
true
)
;
}
}
}
)
;
t
.
start
(
)
;
startLatch
.
await
(
)
;
fs
.
delete
(
hbaseFile
,
true
)
;
try
{
hbaseOutputStream
.
close
(
)
;
}
catch
(
Exception
e
)
{
}
hbaseOutputStream
=
null
;
}
else
if
(
i
%
5000
==
0
)
{
LOG
.
info
(
+
flumeOutputStream
.
getPos
(
)
+
+
fs
.
getFileStatus
(
flumeFile
)
.
getLen
(
)
+
+
(
i
+
1
)
)
;
}
}
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
e
)
;
writerError
.
set
(
true
)
;
}
}
}
)
;
t
.
start
(
)
;
startLatch
.
await
(
)
;
final
Path
snap1Dir
=
SnapshotTestHelper
.
createSnapshot
(
fs
,
snapRootDir
,
snap1Name
)
;
final
Path
flumeS1Path
=
new
Path
(
snap1Dir
,
flumeFileName
)
;
try
{
hbaseOutputStream
.
close
(
)
;
}
catch
(
Exception
e
)
{
}
hbaseOutputStream
=
null
;
}
else
if
(
i
%
5000
==
0
)
{
LOG
.
info
(
+
flumeOutputStream
.
getPos
(
)
+
+
fs
.
getFileStatus
(
flumeFile
)
.
getLen
(
)
+
+
(
i
+
1
)
)
;
}
}
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
e
)
;
writerError
.
set
(
true
)
;
}
}
}
)
;
t
.
start
(
)
;
startLatch
.
await
(
)
;
final
Path
snap1Dir
=
SnapshotTestHelper
.
createSnapshot
(
fs
,
snapRootDir
,
snap1Name
)
;
final
Path
flumeS1Path
=
new
Path
(
snap1Dir
,
flumeFileName
)
;
LOG
.
info
(
+
fs
.
getFileStatus
(
flumeS1Path
)
)
;
hbaseOutputStream
.
close
(
)
;
}
catch
(
Exception
e
)
{
}
hbaseOutputStream
=
null
;
}
else
if
(
i
%
5000
==
0
)
{
LOG
.
info
(
+
flumeOutputStream
.
getPos
(
)
+
+
fs
.
getFileStatus
(
flumeFile
)
.
getLen
(
)
+
+
(
i
+
1
)
)
;
}
}
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
e
)
;
writerError
.
set
(
true
)
;
}
}
}
)
;
t
.
start
(
)
;
startLatch
.
await
(
)
;
final
Path
snap1Dir
=
SnapshotTestHelper
.
createSnapshot
(
fs
,
snapRootDir
,
snap1Name
)
;
final
Path
flumeS1Path
=
new
Path
(
snap1Dir
,
flumeFileName
)
;
LOG
.
info
(
+
fs
.
getFileStatus
(
flumeS1Path
)
)
;
LOG
.
info
(
+
fs
.
getFileStatus
(
flumeFile
)
)
;
@
Test
(
timeout
=
900000
)
public
void
testRandomOperationsWithSnapshots
(
)
throws
IOException
,
InterruptedException
,
TimeoutException
{
long
seed
=
System
.
currentTimeMillis
(
)
;
@
Test
(
timeout
=
900000
)
public
void
testRandomOperationsWithSnapshots
(
)
throws
IOException
,
InterruptedException
,
TimeoutException
{
long
seed
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
seed
)
;
generator
=
new
Random
(
seed
)
;
int
fileLen
=
generator
.
nextInt
(
MAX_NUM_FILE_LENGTH
)
;
createFiles
(
TESTDIRSTRING
,
fileLen
)
;
SnapshottableDirectoryStatus
[
]
snapshottableDirectoryStatus
=
hdfs
.
getSnapshottableDirListing
(
)
;
for
(
SnapshottableDirectoryStatus
ssds
:
snapshottableDirectoryStatus
)
{
snapshottableDirectories
.
add
(
ssds
.
getFullPath
(
)
)
;
}
if
(
snapshottableDirectories
.
size
(
)
==
0
)
{
hdfs
.
allowSnapshot
(
hdfs
.
getHomeDirectory
(
)
)
;
snapshottableDirectories
.
add
(
hdfs
.
getHomeDirectory
(
)
)
;
}
int
numberOfIterations
=
generator
.
nextInt
(
MAX_NUM_ITERATIONS
)
;
LOG
.
info
(
+
numberOfIterations
)
;
int
numberFileSystemOperations
=
generator
.
nextInt
(
MAX_NUM_FILESYSTEM_OPERATIONS
-
MIN_NUM_OPERATIONS
+
1
)
+
MIN_NUM_OPERATIONS
;
LOG
.
info
(
+
seed
)
;
generator
=
new
Random
(
seed
)
;
int
fileLen
=
generator
.
nextInt
(
MAX_NUM_FILE_LENGTH
)
;
createFiles
(
TESTDIRSTRING
,
fileLen
)
;
SnapshottableDirectoryStatus
[
]
snapshottableDirectoryStatus
=
hdfs
.
getSnapshottableDirListing
(
)
;
for
(
SnapshottableDirectoryStatus
ssds
:
snapshottableDirectoryStatus
)
{
snapshottableDirectories
.
add
(
ssds
.
getFullPath
(
)
)
;
}
if
(
snapshottableDirectories
.
size
(
)
==
0
)
{
hdfs
.
allowSnapshot
(
hdfs
.
getHomeDirectory
(
)
)
;
snapshottableDirectories
.
add
(
hdfs
.
getHomeDirectory
(
)
)
;
}
int
numberOfIterations
=
generator
.
nextInt
(
MAX_NUM_ITERATIONS
)
;
LOG
.
info
(
+
numberOfIterations
)
;
int
numberFileSystemOperations
=
generator
.
nextInt
(
MAX_NUM_FILESYSTEM_OPERATIONS
-
MIN_NUM_OPERATIONS
+
1
)
+
MIN_NUM_OPERATIONS
;
LOG
.
info
(
+
numberFileSystemOperations
)
;
int
numberSnapshotOperations
=
generator
.
nextInt
(
MAX_NUM_SNAPSHOT_OPERATIONS
-
MIN_NUM_OPERATIONS
)
+
MIN_NUM_OPERATIONS
;
public
void
randomOperationsWithSnapshots
(
int
numberOfIterations
,
int
numberFileSystemOperations
,
int
numberSnapshotOperations
)
throws
IOException
,
InterruptedException
,
TimeoutException
{
for
(
int
i
=
0
;
i
<
numberOfIterations
;
i
++
)
{
for
(
int
j
=
0
;
j
<
numberFileSystemOperations
;
j
++
)
{
Operations
fsOperation
=
Operations
.
getRandomOperation
(
OperationType
.
FileSystem
)
;
break
;
case
FileSystem_DeleteDir
:
deleteTestDir
(
)
;
break
;
case
FileSystem_RenameDir
:
renameTestDir
(
)
;
break
;
case
FileSystem_CreateFile
:
createTestFile
(
)
;
break
;
case
FileSystem_DeleteFile
:
deleteTestFile
(
)
;
break
;
case
FileSystem_RenameFile
:
renameTestFile
(
)
;
break
;
default
:
assertNull
(
,
fsOperation
)
;
break
;
}
}
for
(
int
k
=
0
;
k
<
numberSnapshotOperations
;
k
++
)
{
Operations
snapshotOperation
=
Operations
.
getRandomOperation
(
OperationType
.
Snapshot
)
;
private
void
createSnapshot
(
)
throws
IOException
{
if
(
snapshottableDirectories
.
size
(
)
>
0
)
{
int
index
=
generator
.
nextInt
(
snapshottableDirectories
.
size
(
)
)
;
Path
randomDir
=
snapshottableDirectories
.
get
(
index
)
;
String
snapshotName
=
Integer
.
toString
(
generator
.
nextInt
(
)
)
+
;
hdfs
.
createSnapshot
(
randomDir
,
snapshotName
)
;
private
void
deleteSnapshot
(
)
throws
IOException
{
if
(
!
pathToSnapshotsMap
.
isEmpty
(
)
)
{
int
index
=
generator
.
nextInt
(
pathToSnapshotsMap
.
size
(
)
)
;
Object
[
]
snapshotPaths
=
pathToSnapshotsMap
.
keySet
(
)
.
toArray
(
)
;
Path
snapshotPath
=
(
Path
)
snapshotPaths
[
index
]
;
ArrayList
<
String
>
snapshotNameList
=
pathToSnapshotsMap
.
get
(
snapshotPath
)
;
String
snapshotNameToBeDeleted
=
snapshotNameList
.
get
(
generator
.
nextInt
(
snapshotNameList
.
size
(
)
)
)
;
hdfs
.
deleteSnapshot
(
snapshotPath
,
snapshotNameToBeDeleted
)
;
private
void
renameSnapshot
(
)
throws
IOException
{
if
(
!
pathToSnapshotsMap
.
isEmpty
(
)
)
{
int
index
=
generator
.
nextInt
(
pathToSnapshotsMap
.
size
(
)
)
;
Object
[
]
snapshotPaths
=
pathToSnapshotsMap
.
keySet
(
)
.
toArray
(
)
;
Path
snapshotPath
=
(
Path
)
snapshotPaths
[
index
]
;
ArrayList
<
String
>
snapshotNameList
=
pathToSnapshotsMap
.
get
(
snapshotPath
)
;
String
snapshotOldName
=
snapshotNameList
.
get
(
generator
.
nextInt
(
snapshotNameList
.
size
(
)
)
)
;
String
snapshotOldNameNoExt
=
snapshotOldName
.
substring
(
0
,
snapshotOldName
.
lastIndexOf
(
'.'
)
-
1
)
;
String
snapshotNewName
=
snapshotOldNameNoExt
+
;
hdfs
.
renameSnapshot
(
snapshotPath
,
snapshotOldName
,
snapshotNewName
)
;
if
(
snapshottableDirectories
.
size
(
)
>
0
)
{
int
index
=
generator
.
nextInt
(
snapshottableDirectories
.
size
(
)
)
;
Path
randomDir
=
snapshottableDirectories
.
get
(
index
)
;
FileStatus
[
]
fileStatusList
=
hdfs
.
listStatus
(
randomDir
)
;
for
(
FileStatus
fsEntry
:
fileStatusList
)
{
if
(
fsEntry
.
isFile
(
)
)
{
Path
oldFile
=
fsEntry
.
getPath
(
)
;
Path
newFile
=
oldFile
.
suffix
(
)
;
for
(
OperationDirectories
dir
:
OperationDirectories
.
values
(
)
)
{
if
(
dir
==
OperationDirectories
.
WitnessDir
)
{
oldFile
=
new
Path
(
getNewPathString
(
oldFile
.
toString
(
)
,
TESTDIRSTRING
,
WITNESSDIRSTRING
)
)
;
newFile
=
new
Path
(
getNewPathString
(
newFile
.
toString
(
)
,
TESTDIRSTRING
,
WITNESSDIRSTRING
)
)
;
}
hdfs
.
rename
(
oldFile
,
newFile
,
Options
.
Rename
.
OVERWRITE
)
;
assertTrue
(
,
hdfs
.
exists
(
newFile
)
)
;
assertFalse
(
,
hdfs
.
exists
(
oldFile
)
)
;
private
void
createFiles
(
String
rootDir
,
int
fileLength
)
throws
IOException
{
if
(
!
rootDir
.
endsWith
(
)
)
{
rootDir
+=
;
}
for
(
int
i
=
0
;
i
<
TOTAL_FILECOUNT
;
i
++
)
{
String
filename
=
rootDir
;
int
dirs
=
generator
.
nextInt
(
MAX_NUM_SUB_DIRECTORIES_LEVEL
)
;
for
(
int
j
=
i
;
j
>=
(
i
-
dirs
)
;
j
--
)
{
filename
+=
j
+
;
}
filename
+=
+
i
;
createFile
(
filename
,
fileLength
,
true
)
;
assertTrue
(
,
hdfs
.
exists
(
new
Path
(
filename
)
)
)
;
LOG
.
info
(
+
filename
+
)
;
String
witnessFile
=
filename
.
replaceAll
(
TESTDIRSTRING
,
WITNESSDIRSTRING
)
;
createFile
(
witnessFile
,
fileLength
,
false
)
;
assertTrue
(
,
hdfs
.
exists
(
new
Path
(
witnessFile
)
)
)
;
private
String
getNewPathString
(
String
originalString
,
String
targetString
,
String
replacementString
)
{
String
str
=
originalString
.
replaceAll
(
targetString
,
replacementString
)
;
private
String
getNewPathString
(
String
originalString
,
String
targetString
,
String
replacementString
)
{
String
str
=
originalString
.
replaceAll
(
targetString
,
replacementString
)
;
LOG
.
info
(
+
originalString
)
;
@
Test
(
timeout
=
60000
)
public
void
testRenameTwiceInSnapshot
(
)
throws
Exception
{
hdfs
.
mkdirs
(
sub1
)
;
hdfs
.
allowSnapshot
(
sub1
)
;
DFSTestUtil
.
createFile
(
hdfs
,
file1
,
BLOCKSIZE
,
REPL
,
SEED
)
;
hdfs
.
createSnapshot
(
sub1
,
snap1
)
;
hdfs
.
rename
(
file1
,
file2
)
;
hdfs
.
createSnapshot
(
sub1
,
snap2
)
;
hdfs
.
rename
(
file2
,
file3
)
;
SnapshotDiffReport
diffReport
;
diffReport
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
snap1
,
snap2
)
;
hdfs
.
mkdirs
(
sub1
)
;
hdfs
.
allowSnapshot
(
sub1
)
;
DFSTestUtil
.
createFile
(
hdfs
,
file1
,
BLOCKSIZE
,
REPL
,
SEED
)
;
hdfs
.
createSnapshot
(
sub1
,
snap1
)
;
hdfs
.
rename
(
file1
,
file2
)
;
hdfs
.
createSnapshot
(
sub1
,
snap2
)
;
hdfs
.
rename
(
file2
,
file3
)
;
SnapshotDiffReport
diffReport
;
diffReport
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
snap1
,
snap2
)
;
LOG
.
info
(
+
diffReport
.
toString
(
)
)
;
List
<
DiffReportEntry
>
entries
=
diffReport
.
getDiffList
(
)
;
assertTrue
(
entries
.
size
(
)
==
2
)
;
assertTrue
(
existsInDiffReport
(
entries
,
DiffType
.
MODIFY
,
,
null
)
)
;
assertTrue
(
existsInDiffReport
(
entries
,
DiffType
.
RENAME
,
file1
.
getName
(
)
,
file2
.
getName
(
)
)
)
;
diffReport
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
snap2
,
)
;
hdfs
.
rename
(
file2
,
file3
)
;
SnapshotDiffReport
diffReport
;
diffReport
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
snap1
,
snap2
)
;
LOG
.
info
(
+
diffReport
.
toString
(
)
)
;
List
<
DiffReportEntry
>
entries
=
diffReport
.
getDiffList
(
)
;
assertTrue
(
entries
.
size
(
)
==
2
)
;
assertTrue
(
existsInDiffReport
(
entries
,
DiffType
.
MODIFY
,
,
null
)
)
;
assertTrue
(
existsInDiffReport
(
entries
,
DiffType
.
RENAME
,
file1
.
getName
(
)
,
file2
.
getName
(
)
)
)
;
diffReport
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
snap2
,
)
;
LOG
.
info
(
+
diffReport
.
toString
(
)
)
;
entries
=
diffReport
.
getDiffList
(
)
;
assertTrue
(
entries
.
size
(
)
==
2
)
;
assertTrue
(
existsInDiffReport
(
entries
,
DiffType
.
MODIFY
,
,
null
)
)
;
assertTrue
(
existsInDiffReport
(
entries
,
DiffType
.
RENAME
,
file2
.
getName
(
)
,
file3
.
getName
(
)
)
)
;
diffReport
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
snap1
,
)
;
hdfs
.
mkdirs
(
st
)
;
hdfs
.
allowSnapshot
(
st
)
;
Path
[
]
files
=
new
Path
[
3
]
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
files
[
i
]
=
new
Path
(
st
,
i
+
)
;
}
Path
dest
=
new
Path
(
st
,
)
;
hdfs
.
createNewFile
(
dest
)
;
hdfs
.
createSnapshot
(
st
,
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
for
(
int
j
=
0
;
j
<
3
;
j
++
)
{
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
DFSTestUtil
.
createFile
(
fs
,
files
[
j
]
,
false
,
1024
,
1024
,
512
,
(
short
)
1
,
RandomUtils
.
nextLong
(
1
,
512
)
,
true
)
;
}
hdfs
.
concat
(
dest
,
files
)
;
hdfs
.
createSnapshot
(
st
,
+
i
)
;
SnapshotDiffReport
sdr
=
hdfs
.
getSnapshotDiffReport
(
st
,
+
i
,
)
;
Path
subsub1
=
new
Path
(
sub1
,
)
;
Path
subsubsub1
=
new
Path
(
subsub1
,
)
;
hdfs
.
mkdirs
(
subsubsub1
)
;
modifyAndCreateSnapshot
(
sub1
,
new
Path
[
]
{
sub1
,
subsubsub1
}
)
;
modifyAndCreateSnapshot
(
subsubsub1
,
new
Path
[
]
{
sub1
,
subsubsub1
}
)
;
final
String
invalidName
=
;
try
{
hdfs
.
getSnapshotDiffReport
(
sub1
,
invalidName
,
invalidName
)
;
fail
(
+
)
;
}
catch
(
IOException
e
)
{
GenericTestUtils
.
assertExceptionContains
(
+
sub1
+
+
invalidName
,
e
)
;
}
SnapshotDiffReport
report
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
,
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
assertEquals
(
0
,
report
.
getDiffList
(
)
.
size
(
)
)
;
report
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
,
)
;
fail
(
+
)
;
}
catch
(
IOException
e
)
{
GenericTestUtils
.
assertExceptionContains
(
+
sub1
+
+
invalidName
,
e
)
;
}
SnapshotDiffReport
report
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
,
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
assertEquals
(
0
,
report
.
getDiffList
(
)
.
size
(
)
)
;
report
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
,
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
assertEquals
(
0
,
report
.
getDiffList
(
)
.
size
(
)
)
;
try
{
report
=
hdfs
.
getSnapshotDiffReport
(
subsubsub1
,
null
,
)
;
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
GenericTestUtils
.
assertExceptionContains
(
,
e
)
;
}
report
=
hdfs
.
getSnapshotDiffReport
(
subsubsub1
,
,
)
;
GenericTestUtils
.
assertExceptionContains
(
+
sub1
+
+
invalidName
,
e
)
;
}
SnapshotDiffReport
report
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
,
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
assertEquals
(
0
,
report
.
getDiffList
(
)
.
size
(
)
)
;
report
=
hdfs
.
getSnapshotDiffReport
(
sub1
,
,
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
assertEquals
(
0
,
report
.
getDiffList
(
)
.
size
(
)
)
;
try
{
report
=
hdfs
.
getSnapshotDiffReport
(
subsubsub1
,
null
,
)
;
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
GenericTestUtils
.
assertExceptionContains
(
,
e
)
;
}
report
=
hdfs
.
getSnapshotDiffReport
(
subsubsub1
,
,
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
assertEquals
(
0
,
report
.
getDiffList
(
)
.
size
(
)
)
;
@
Test
public
void
testSnapshotDiffInfo
(
)
throws
Exception
{
Path
snapshotRootDirPath
=
dir
;
Path
snapshotDirDescendantPath
=
new
Path
(
snapshotRootDirPath
,
)
;
Path
snapshotDirNonDescendantPath
=
new
Path
(
)
;
hdfs
.
mkdirs
(
snapshotDirDescendantPath
)
;
hdfs
.
mkdirs
(
snapshotDirNonDescendantPath
)
;
hdfs
.
allowSnapshot
(
snapshotRootDirPath
)
;
hdfs
.
createSnapshot
(
snapshotRootDirPath
,
)
;
hdfs
.
createSnapshot
(
snapshotRootDirPath
,
)
;
INodeDirectory
snapshotRootDir
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
getFSDirectory
(
)
.
getINode
(
snapshotRootDirPath
.
toUri
(
)
.
getPath
(
)
)
.
asDirectory
(
)
;
INodeDirectory
snapshotRootDescendantDir
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
getFSDirectory
(
)
.
getINode
(
snapshotDirDescendantPath
.
toUri
(
)
.
getPath
(
)
)
.
asDirectory
(
)
;
INodeDirectory
snapshotRootNonDescendantDir
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
getFSDirectory
(
)
.
getINode
(
snapshotDirNonDescendantPath
.
toUri
(
)
.
getPath
(
)
)
.
asDirectory
(
)
;
try
{
SnapshotDiffInfo
sdi
=
new
SnapshotDiffInfo
(
snapshotRootDir
,
snapshotRootDescendantDir
,
new
Snapshot
(
0
,
,
snapshotRootDescendantDir
)
,
new
Snapshot
(
0
,
,
snapshotRootDescendantDir
)
)
;
hdfs
.
mkdirs
(
snapshotDirDescendantPath
)
;
hdfs
.
mkdirs
(
snapshotDirNonDescendantPath
)
;
hdfs
.
allowSnapshot
(
snapshotRootDirPath
)
;
hdfs
.
createSnapshot
(
snapshotRootDirPath
,
)
;
hdfs
.
createSnapshot
(
snapshotRootDirPath
,
)
;
INodeDirectory
snapshotRootDir
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
getFSDirectory
(
)
.
getINode
(
snapshotRootDirPath
.
toUri
(
)
.
getPath
(
)
)
.
asDirectory
(
)
;
INodeDirectory
snapshotRootDescendantDir
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
getFSDirectory
(
)
.
getINode
(
snapshotDirDescendantPath
.
toUri
(
)
.
getPath
(
)
)
.
asDirectory
(
)
;
INodeDirectory
snapshotRootNonDescendantDir
=
cluster
.
getNameNode
(
)
.
getNamesystem
(
)
.
getFSDirectory
(
)
.
getINode
(
snapshotDirNonDescendantPath
.
toUri
(
)
.
getPath
(
)
)
.
asDirectory
(
)
;
try
{
SnapshotDiffInfo
sdi
=
new
SnapshotDiffInfo
(
snapshotRootDir
,
snapshotRootDescendantDir
,
new
Snapshot
(
0
,
,
snapshotRootDescendantDir
)
,
new
Snapshot
(
0
,
,
snapshotRootDescendantDir
)
)
;
LOG
.
info
(
+
sdi
.
getFrom
(
)
+
+
sdi
.
getTo
(
)
)
;
}
catch
(
IllegalArgumentException
iae
)
{
fail
(
+
iae
)
;
}
try
{
SnapshotDiffInfo
sdi
=
new
SnapshotDiffInfo
(
snapshotRootDir
,
snapshotRootNonDescendantDir
,
new
Snapshot
(
0
,
,
snapshotRootNonDescendantDir
)
,
new
Snapshot
(
0
,
,
snapshotRootNonDescendantDir
)
)
;
private
void
printAtime
(
Path
path
,
Path
ssRoot
,
String
ssName
)
throws
IOException
{
Path
ssPath
=
getSSpath
(
path
,
ssRoot
,
ssName
)
;
private
void
verifyDiffReportForGivenReport
(
Path
dirPath
,
String
from
,
String
to
,
SnapshotDiffReport
report
,
DiffReportEntry
...
entries
)
throws
IOException
{
SnapshotDiffReport
inverseReport
=
hdfs
.
getSnapshotDiffReport
(
dirPath
,
to
,
from
)
;
private
void
verifyDiffReportForGivenReport
(
Path
dirPath
,
String
from
,
String
to
,
SnapshotDiffReport
report
,
DiffReportEntry
...
entries
)
throws
IOException
{
SnapshotDiffReport
inverseReport
=
hdfs
.
getSnapshotDiffReport
(
dirPath
,
to
,
from
)
;
LOG
.
info
(
report
.
toString
(
)
)
;
private
void
waitExpectedStorageType
(
MiniDFSCluster
cluster
,
final
String
fileName
,
long
fileLen
,
final
StorageType
expectedStorageType
,
int
expectedStorageCount
,
int
expectedBlkLocationCount
,
int
timeout
)
throws
Exception
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
int
actualStorageCount
=
0
;
try
{
LocatedBlocks
locatedBlocks
=
cluster
.
getFileSystem
(
)
.
getClient
(
)
.
getLocatedBlocks
(
fileName
,
0
,
fileLen
)
;
for
(
LocatedBlock
lb
:
locatedBlocks
.
getLocatedBlocks
(
)
)
{
@
Override
public
Boolean
get
(
)
{
int
actualStorageCount
=
0
;
try
{
LocatedBlocks
locatedBlocks
=
cluster
.
getFileSystem
(
)
.
getClient
(
)
.
getLocatedBlocks
(
fileName
,
0
,
fileLen
)
;
for
(
LocatedBlock
lb
:
locatedBlocks
.
getLocatedBlocks
(
)
)
{
LOG
.
info
(
,
lb
.
getLocations
(
)
.
length
,
lb
)
;
if
(
lb
.
getLocations
(
)
.
length
>
expectedBlkLocationCount
)
{
return
false
;
}
for
(
StorageType
storageType
:
lb
.
getStorageTypes
(
)
)
{
if
(
expectedStorageType
==
storageType
)
{
actualStorageCount
++
;
}
else
{
LOG
.
info
(
,
expectedStorageType
,
storageType
)
;
}
}
}
LOG
.
info
(
expectedStorageType
+
,
expectedStorageCount
,
actualStorageCount
)
;
}
catch
(
IOException
e
)
{
private
void
waitForAttemptedItems
(
long
expectedBlkMovAttemptedCount
,
int
timeout
)
throws
TimeoutException
,
InterruptedException
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
@
Test
public
void
testDataLocality
(
)
throws
Exception
{
final
Configuration
conf
=
WebHdfsTestUtil
.
createConf
(
)
;
final
String
[
]
racks
=
{
RACK0
,
RACK0
,
RACK1
,
RACK1
,
RACK2
,
RACK2
}
;
final
int
nDataNodes
=
racks
.
length
;
@
Test
public
void
testDataLocality
(
)
throws
Exception
{
final
Configuration
conf
=
WebHdfsTestUtil
.
createConf
(
)
;
final
String
[
]
racks
=
{
RACK0
,
RACK0
,
RACK1
,
RACK1
,
RACK2
,
RACK2
}
;
final
int
nDataNodes
=
racks
.
length
;
LOG
.
info
(
+
nDataNodes
+
+
Arrays
.
asList
(
racks
)
)
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
nDataNodes
)
.
racks
(
racks
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
final
NameNode
namenode
=
cluster
.
getNameNode
(
)
;
final
DatanodeManager
dm
=
namenode
.
getNamesystem
(
)
.
getBlockManager
(
)
.
getDatanodeManager
(
)
;
@
Test
public
void
testExcludeDataNodes
(
)
throws
Exception
{
final
Configuration
conf
=
WebHdfsTestUtil
.
createConf
(
)
;
final
String
[
]
racks
=
{
RACK0
,
RACK0
,
RACK1
,
RACK1
,
RACK2
,
RACK2
}
;
final
String
[
]
hosts
=
{
,
,
,
,
,
}
;
final
int
nDataNodes
=
hosts
.
length
;
@
Test
public
void
testExcludeDataNodes
(
)
throws
Exception
{
final
Configuration
conf
=
WebHdfsTestUtil
.
createConf
(
)
;
final
String
[
]
racks
=
{
RACK0
,
RACK0
,
RACK1
,
RACK1
,
RACK2
,
RACK2
}
;
final
String
[
]
hosts
=
{
,
,
,
,
,
}
;
final
int
nDataNodes
=
hosts
.
length
;
LOG
.
info
(
+
nDataNodes
+
+
Arrays
.
asList
(
racks
)
+
+
Arrays
.
asList
(
hosts
)
)
;
final
MiniDFSCluster
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
hosts
(
hosts
)
.
numDataNodes
(
nDataNodes
)
.
racks
(
racks
)
.
build
(
)
;
try
{
cluster
.
waitActive
(
)
;
final
DistributedFileSystem
dfs
=
cluster
.
getFileSystem
(
)
;
final
NameNode
namenode
=
cluster
.
getNameNode
(
)
;
final
DatanodeManager
dm
=
namenode
.
getNamesystem
(
)
.
getBlockManager
(
)
.
getDatanodeManager
(
)
;
final
String
file1
=
;
InetSocketAddress
[
]
favoredNodes
=
new
InetSocketAddress
[
favoredNodesCount
]
;
for
(
int
i
=
0
;
i
<
favoredNodesCount
;
i
++
)
{
favoredNodes
[
i
]
=
dns
.
get
(
i
)
.
getXferAddress
(
)
;
}
DFSTestUtil
.
createFile
(
dfs
,
new
Path
(
file1
)
,
false
,
1024
,
100
,
DEFAULT_BLOCK_SIZE
,
(
short
)
3
,
0
,
false
,
favoredNodes
)
;
LocatedBlocks
locatedBlocks
=
dfs
.
getClient
(
)
.
getLocatedBlocks
(
file1
,
0
)
;
Assert
.
assertEquals
(
,
1
,
locatedBlocks
.
locatedBlockCount
(
)
)
;
LocatedBlock
lb
=
locatedBlocks
.
get
(
0
)
;
StorageType
[
]
storageTypes
=
lb
.
getStorageTypes
(
)
;
for
(
StorageType
storageType
:
storageTypes
)
{
Assert
.
assertTrue
(
StorageType
.
DISK
==
storageType
)
;
}
DatanodeInfo
[
]
locations
=
lb
.
getLocations
(
)
;
Assert
.
assertEquals
(
3
,
locations
.
length
)
;
Assert
.
assertTrue
(
favoredNodesCount
<
locations
.
length
)
;
for
(
DatanodeInfo
dnInfo
:
locations
)
{
public
void
waitForAttemptedItems
(
long
expectedBlkMovAttemptedCount
,
int
timeout
)
throws
TimeoutException
,
InterruptedException
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
public
void
waitForBlocksMovementAttemptReport
(
long
expectedMovementFinishedBlocksCount
,
int
timeout
)
throws
TimeoutException
,
InterruptedException
{
GenericTestUtils
.
waitFor
(
new
Supplier
<
Boolean
>
(
)
{
@
Override
public
Boolean
get
(
)
{
int
actualCount
=
externalSps
.
getAttemptedItemsMonitor
(
)
.
getAttemptedItemsCount
(
)
;
resetStream
(
)
;
assertEquals
(
0
,
ToolRunner
.
run
(
dfsAdmin
,
new
String
[
]
{
}
)
)
;
verifyNodesAndCorruptBlocks
(
numDn
,
numDn
,
0
,
0
,
client
,
0L
,
0L
)
;
final
short
replFactor
=
1
;
final
long
fileLength
=
512L
;
final
DistributedFileSystem
fs
=
miniCluster
.
getFileSystem
(
)
;
final
Path
file
=
new
Path
(
baseDir
,
)
;
fs
.
enableErasureCodingPolicy
(
ecPolicy
.
getName
(
)
)
;
DFSTestUtil
.
createFile
(
fs
,
file
,
fileLength
,
replFactor
,
12345L
)
;
DFSTestUtil
.
waitReplication
(
fs
,
file
,
replFactor
)
;
final
ExtendedBlock
block
=
DFSTestUtil
.
getFirstBlock
(
fs
,
file
)
;
LocatedBlocks
lbs
=
miniCluster
.
getFileSystem
(
)
.
getClient
(
)
.
getNamenode
(
)
.
getBlockLocations
(
file
.
toString
(
)
,
0
,
fileLength
)
;
assertTrue
(
+
lbs
.
get
(
0
)
,
lbs
.
get
(
0
)
instanceof
LocatedBlock
)
;
LocatedBlock
locatedBlock
=
lbs
.
get
(
0
)
;
DatanodeInfo
locatedDataNode
=
locatedBlock
.
getLocations
(
)
[
0
]
;
int
cellSize
=
ecPolicy
.
getCellSize
(
)
;
int
blockSize
=
stripesPerBlock
*
cellSize
;
int
blockGroupSize
=
ecPolicy
.
getNumDataUnits
(
)
*
blockSize
;
int
totalBlockGroups
=
1
;
DFSTestUtil
.
createStripedFile
(
miniCluster
,
ecFile
,
ecDir
,
totalBlockGroups
,
stripesPerBlock
,
false
,
ecPolicy
)
;
resetStream
(
)
;
assertEquals
(
0
,
ToolRunner
.
run
(
dfsAdmin
,
new
String
[
]
{
}
)
)
;
verifyNodesAndCorruptBlocks
(
numDn
,
numDn
,
0
,
0
,
client
,
0L
,
0L
)
;
final
List
<
DataNode
>
datanodes
=
miniCluster
.
getDataNodes
(
)
;
DataNode
dataNodeToShutdown
=
null
;
for
(
DataNode
dn
:
datanodes
)
{
if
(
!
dn
.
getDatanodeId
(
)
.
getDatanodeUuid
(
)
.
equals
(
locatedDataNode
.
getDatanodeUuid
(
)
)
)
{
dataNodeToShutdown
=
dn
;
break
;
}
}
assertTrue
(
,
dataNodeToShutdown
!=
null
)
;
private
void
verifyOpenFilesListing
(
HashSet
<
Path
>
closedFileSet
,
HashMap
<
Path
,
FSDataOutputStream
>
openFilesMap
)
{
final
String
outStr
=
scanIntoString
(
out
)
;
private
Object
runTool
(
String
...
args
)
throws
Exception
{
errOutBytes
.
reset
(
)
;
outBytes
.
reset
(
)
;
LOG
.
info
(
+
Joiner
.
on
(
)
.
join
(
args
)
)
;
int
ret
=
tool
.
run
(
args
)
;
errOutput
=
new
String
(
errOutBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
output
=
new
String
(
outBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
private
int
runTool
(
String
...
args
)
throws
Exception
{
errOutBytes
.
reset
(
)
;
LOG
.
info
(
+
Joiner
.
on
(
)
.
join
(
args
)
)
;
int
ret
=
tool
.
run
(
args
)
;
errOutput
=
new
String
(
errOutBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
try
{
cluster
.
waitActive
(
)
;
DistributedFileSystem
fs
=
cluster
.
getFileSystem
(
)
;
LocalFileSystem
localFileSystem
=
FileSystem
.
getLocal
(
conf
)
;
Path
p
=
new
Path
(
f
.
getRoot
(
)
.
getAbsolutePath
(
)
,
tokenFile
)
;
p
=
localFileSystem
.
makeQualified
(
p
)
;
DelegationTokenFetcher
.
saveDelegationToken
(
conf
,
fs
,
null
,
p
)
;
Credentials
creds
=
Credentials
.
readTokenStorageFile
(
p
,
conf
)
;
Iterator
<
Token
<
?
>>
itr
=
creds
.
getAllTokens
(
)
.
iterator
(
)
;
assertTrue
(
,
itr
.
hasNext
(
)
)
;
final
Token
token
=
itr
.
next
(
)
;
assertNotNull
(
,
token
)
;
String
expectedNonVerbose
=
+
System
.
getProperty
(
)
+
;
String
resNonVerbose
=
DelegationTokenFetcher
.
printTokensToString
(
conf
,
p
,
false
)
;
assertTrue
(
+
expectedNonVerbose
+
,
resNonVerbose
.
startsWith
(
expectedNonVerbose
)
)
;
cluster
.
waitActive
(
)
;
DistributedFileSystem
fs
=
cluster
.
getFileSystem
(
)
;
LocalFileSystem
localFileSystem
=
FileSystem
.
getLocal
(
conf
)
;
Path
p
=
new
Path
(
f
.
getRoot
(
)
.
getAbsolutePath
(
)
,
tokenFile
)
;
p
=
localFileSystem
.
makeQualified
(
p
)
;
DelegationTokenFetcher
.
saveDelegationToken
(
conf
,
fs
,
null
,
p
)
;
Credentials
creds
=
Credentials
.
readTokenStorageFile
(
p
,
conf
)
;
Iterator
<
Token
<
?
>>
itr
=
creds
.
getAllTokens
(
)
.
iterator
(
)
;
assertTrue
(
,
itr
.
hasNext
(
)
)
;
final
Token
token
=
itr
.
next
(
)
;
assertNotNull
(
,
token
)
;
String
expectedNonVerbose
=
+
System
.
getProperty
(
)
+
;
String
resNonVerbose
=
DelegationTokenFetcher
.
printTokensToString
(
conf
,
p
,
false
)
;
assertTrue
(
+
expectedNonVerbose
+
,
resNonVerbose
.
startsWith
(
expectedNonVerbose
)
)
;
LOG
.
info
(
resNonVerbose
)
;
@
Test
public
void
testGenerated
(
)
throws
IOException
{
String
edits
=
nnHelper
.
generateEdits
(
)
;
@
Test
public
void
testGenerated
(
)
throws
IOException
{
String
edits
=
nnHelper
.
generateEdits
(
)
;
LOG
.
info
(
+
edits
)
;
String
editsParsedXml
=
folder
.
newFile
(
)
.
getAbsolutePath
(
)
;
String
editsReparsed
=
folder
.
newFile
(
)
.
getAbsolutePath
(
)
;
String
editsParsedXML_caseInSensitive
=
folder
.
newFile
(
)
.
getAbsolutePath
(
)
;
assertEquals
(
0
,
runOev
(
edits
,
editsParsedXml
,
,
false
)
)
;
assertEquals
(
0
,
runOev
(
edits
,
editsParsedXML_caseInSensitive
,
,
false
)
)
;
assertEquals
(
0
,
runOev
(
editsParsedXml
,
editsReparsed
,
,
false
)
)
;
assertEquals
(
0
,
runOev
(
editsParsedXML_caseInSensitive
,
editsReparsed
,
,
false
)
)
;
assertTrue
(
+
edits
+
,
hasAllOpCodes
(
edits
)
)
;
private
int
runOev
(
String
inFilename
,
String
outFilename
,
String
processor
,
boolean
recovery
)
throws
IOException
{
private
boolean
hasAllOpCodes
(
String
inFilename
)
throws
IOException
{
String
outFilename
=
inFilename
+
;
FileOutputStream
fout
=
new
FileOutputStream
(
outFilename
)
;
StatisticsEditsVisitor
visitor
=
new
StatisticsEditsVisitor
(
fout
)
;
OfflineEditsViewer
oev
=
new
OfflineEditsViewer
(
)
;
if
(
oev
.
go
(
inFilename
,
outFilename
,
,
new
Flags
(
)
,
visitor
)
!=
0
)
return
false
;
private
boolean
hasAllOpCodes
(
String
inFilename
)
throws
IOException
{
String
outFilename
=
inFilename
+
;
FileOutputStream
fout
=
new
FileOutputStream
(
outFilename
)
;
StatisticsEditsVisitor
visitor
=
new
StatisticsEditsVisitor
(
fout
)
;
OfflineEditsViewer
oev
=
new
OfflineEditsViewer
(
)
;
if
(
oev
.
go
(
inFilename
,
outFilename
,
,
new
Flags
(
)
,
visitor
)
!=
0
)
return
false
;
LOG
.
info
(
+
inFilename
+
+
visitor
.
getStatisticsString
(
)
)
;
boolean
hasAllOpCodes
=
true
;
for
(
FSEditLogOpCodes
opCode
:
FSEditLogOpCodes
.
values
(
)
)
{
if
(
skippedOps
.
contains
(
opCode
)
)
continue
;
Long
count
=
visitor
.
getStatistics
(
)
.
get
(
opCode
)
;
if
(
(
count
==
null
)
||
(
count
==
0
)
)
{
hasAllOpCodes
=
false
;
@
Test
public
void
testProcessorWithSameTypeFormatFile
(
)
throws
IOException
{
String
edits
=
nnHelper
.
generateEdits
(
)
;
hdfs
.
create
(
emptyECFile
)
.
close
(
)
;
writtenFiles
.
put
(
emptyECFile
.
toString
(
)
,
pathToFileEntry
(
hdfs
,
emptyECFile
.
toString
(
)
)
)
;
filesECCount
++
;
Path
smallECFile
=
new
Path
(
ecDir
,
)
;
FSDataOutputStream
out
=
hdfs
.
create
(
smallECFile
)
;
Random
r
=
new
Random
(
)
;
byte
[
]
bytes
=
new
byte
[
1024
*
10
]
;
r
.
nextBytes
(
bytes
)
;
out
.
write
(
bytes
)
;
writtenFiles
.
put
(
smallECFile
.
toString
(
)
,
pathToFileEntry
(
hdfs
,
smallECFile
.
toString
(
)
)
)
;
filesECCount
++
;
hdfs
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_ENTER
,
false
)
;
hdfs
.
saveNamespace
(
)
;
hdfs
.
setSafeMode
(
SafeModeAction
.
SAFEMODE_LEAVE
,
false
)
;
originalFsimage
=
FSImageTestUtil
.
findLatestImageFile
(
FSImageTestUtil
.
getFSImage
(
cluster
.
getNameNode
(
)
)
.
getStorage
(
)
.
getStorageDir
(
0
)
)
;
@
Test
public
void
testReverseXmlRoundTrip
(
)
throws
Throwable
{
GenericTestUtils
.
setLogLevel
(
OfflineImageReconstructor
.
LOG
,
Level
.
TRACE
)
;
File
reverseImageXml
=
new
File
(
tempDir
,
)
;
File
reverseImage
=
new
File
(
tempDir
,
)
;
File
reverseImage2Xml
=
new
File
(
tempDir
,
)
;
}
try
(
FSDataOutputStream
o
=
hdfs
.
create
(
new
Path
(
parentDir
,
)
)
)
{
o
.
write
(
.
getBytes
(
)
)
;
}
Path
link1
=
new
Path
(
)
;
Path
link2
=
new
Path
(
)
;
hdfs
.
createSymlink
(
new
Path
(
)
,
link1
,
true
)
;
summaryFromDFS
=
hdfs
.
getContentSummary
(
parentDir
)
;
emptyDirSummaryFromDFS
=
hdfs
.
getContentSummary
(
childDir2
)
;
fileSummaryFromDFS
=
hdfs
.
getContentSummary
(
file1OnParentDir
)
;
symLinkSummaryFromDFS
=
hdfs
.
getContentSummary
(
link1
)
;
hdfs
.
createSymlink
(
childDir1
,
link2
,
true
)
;
symLinkSummaryForDirContainsFromDFS
=
hdfs
.
getContentSummary
(
new
Path
(
)
)
;
hdfs
.
setSafeMode
(
HdfsConstants
.
SafeModeAction
.
SAFEMODE_ENTER
,
false
)
;
hdfs
.
saveNamespace
(
)
;
originalFsimage
=
FSImageTestUtil
.
findLatestImageFile
(
FSImageTestUtil
.
getFSImage
(
cluster
.
getNameNode
(
)
)
.
getStorage
(
)
.
getStorageDir
(
0
)
)
;
if
(
originalFsimage
==
null
)
{
hdfs
.
mkdirs
(
dir
)
;
hdfs
.
setStoragePolicy
(
dir
,
HdfsConstants
.
ALLSSD_STORAGE_POLICY_NAME
)
;
dir
=
new
Path
(
)
;
hdfs
.
mkdirs
(
dir
)
;
file
=
new
Path
(
)
;
try
(
FSDataOutputStream
o
=
hdfs
.
create
(
file
)
)
{
o
.
write
(
123
)
;
o
.
close
(
)
;
}
dir
=
new
Path
(
)
;
hdfs
.
mkdirs
(
dir
)
;
hdfs
.
setStoragePolicy
(
dir
,
HdfsConstants
.
HOT_STORAGE_POLICY_NAME
)
;
hdfs
.
setSafeMode
(
HdfsConstants
.
SafeModeAction
.
SAFEMODE_ENTER
,
false
)
;
hdfs
.
saveNamespace
(
)
;
originalFsimage
=
FSImageTestUtil
.
findLatestImageFile
(
FSImageTestUtil
.
getFSImage
(
cluster
.
getNameNode
(
)
)
.
getStorage
(
)
.
getStorageDir
(
0
)
)
;
if
(
originalFsimage
==
null
)
{
static
void
verifySeek
(
FileSystem
fs
,
Path
p
,
long
offset
,
long
length
,
byte
[
]
buf
,
byte
[
]
expected
)
throws
IOException
{
long
remaining
=
length
-
offset
;
long
checked
=
0
;
static
void
verifyPread
(
FileSystem
fs
,
Path
p
,
long
offset
,
long
length
,
byte
[
]
buf
,
byte
[
]
expected
)
throws
IOException
{
long
remaining
=
length
-
offset
;
long
checked
=
0
;
private
void
checkResponseContainsLocation
(
URL
url
,
String
TYPE
)
throws
JSONException
,
IOException
{
HttpURLConnection
conn
=
(
HttpURLConnection
)
url
.
openConnection
(
)
;
conn
.
setRequestMethod
(
TYPE
)
;
conn
.
setInstanceFollowRedirects
(
false
)
;
String
response
=
IOUtils
.
toString
(
conn
.
getInputStream
(
)
)
;
@
Test
public
void
testWebHdfsNoRedirect
(
)
throws
Exception
{
final
Configuration
conf
=
WebHdfsTestUtil
.
createConf
(
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
3
)
.
build
(
)
;
LOG
.
info
(
)
;
InetSocketAddress
addr
=
cluster
.
getNameNode
(
)
.
getHttpAddress
(
)
;
URL
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
@
Test
public
void
testWebHdfsNoRedirect
(
)
throws
Exception
{
final
Configuration
conf
=
WebHdfsTestUtil
.
createConf
(
)
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
3
)
.
build
(
)
;
LOG
.
info
(
)
;
InetSocketAddress
addr
=
cluster
.
getNameNode
(
)
.
getHttpAddress
(
)
;
URL
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
LOG
.
info
(
+
url
)
;
checkResponseContainsLocation
(
url
,
)
;
final
WebHdfsFileSystem
fs
=
WebHdfsTestUtil
.
getWebHdfsFileSystem
(
conf
,
WebHdfsConstants
.
WEBHDFS_SCHEME
)
;
final
String
PATH
=
;
byte
[
]
CONTENTS
=
new
byte
[
1024
]
;
RANDOM
.
nextBytes
(
CONTENTS
)
;
try
(
OutputStream
os
=
fs
.
create
(
new
Path
(
PATH
)
)
)
{
os
.
write
(
CONTENTS
)
;
}
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
LOG
.
info
(
)
;
InetSocketAddress
addr
=
cluster
.
getNameNode
(
)
.
getHttpAddress
(
)
;
URL
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
LOG
.
info
(
+
url
)
;
checkResponseContainsLocation
(
url
,
)
;
final
WebHdfsFileSystem
fs
=
WebHdfsTestUtil
.
getWebHdfsFileSystem
(
conf
,
WebHdfsConstants
.
WEBHDFS_SCHEME
)
;
final
String
PATH
=
;
byte
[
]
CONTENTS
=
new
byte
[
1024
]
;
RANDOM
.
nextBytes
(
CONTENTS
)
;
try
(
OutputStream
os
=
fs
.
create
(
new
Path
(
PATH
)
)
)
{
os
.
write
(
CONTENTS
)
;
}
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
LOG
.
info
(
+
url
)
;
checkResponseContainsLocation
(
url
,
)
;
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
LOG
.
info
(
+
url
)
;
checkResponseContainsLocation
(
url
,
)
;
final
WebHdfsFileSystem
fs
=
WebHdfsTestUtil
.
getWebHdfsFileSystem
(
conf
,
WebHdfsConstants
.
WEBHDFS_SCHEME
)
;
final
String
PATH
=
;
byte
[
]
CONTENTS
=
new
byte
[
1024
]
;
RANDOM
.
nextBytes
(
CONTENTS
)
;
try
(
OutputStream
os
=
fs
.
create
(
new
Path
(
PATH
)
)
)
{
os
.
write
(
CONTENTS
)
;
}
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
LOG
.
info
(
+
url
)
;
checkResponseContainsLocation
(
url
,
)
;
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
LOG
.
info
(
+
url
)
;
checkResponseContainsLocation
(
url
,
)
;
url
=
new
URL
(
,
addr
.
getHostString
(
)
,
addr
.
getPort
(
)
,
WebHdfsFileSystem
.
PATH_PREFIX
+
+
+
Param
.
toSortedString
(
,
new
NoRedirectParam
(
true
)
)
)
;
try
{
clientSocket
=
serverSocket
.
accept
(
)
;
fs
.
connectionFactory
=
connectionFactory
;
if
(
consumeConnectionBacklog
)
{
consumeConnectionBacklog
(
)
;
}
in
=
clientSocket
.
getInputStream
(
)
;
isr
=
new
InputStreamReader
(
in
)
;
br
=
new
BufferedReader
(
isr
)
;
for
(
;
;
)
{
String
line
=
br
.
readLine
(
)
;
if
(
line
==
null
||
line
.
isEmpty
(
)
)
{
break
;
}
}
out
=
clientSocket
.
getOutputStream
(
)
;
out
.
write
(
temporaryRedirect
(
)
.
getBytes
(
)
)
;
}
catch
(
IOException
e
)
{
private
static
void
setupCluster
(
final
int
nNameNodes
,
final
int
nDataNodes
)
throws
Exception
{
private
static
String
[
]
createStrings
(
String
prefix
,
String
name
)
{
final
String
[
]
strings
=
new
String
[
webhdfs
.
length
]
;
for
(
int
i
=
0
;
i
<
webhdfs
.
length
;
i
++
)
{
strings
[
i
]
=
createString
(
prefix
,
i
)
;
@
Test
public
void
testPermissionParam
(
)
{
final
PermissionParam
p
=
new
PermissionParam
(
PermissionParam
.
DEFAULT
)
;
Assert
.
assertEquals
(
new
FsPermission
(
(
short
)
0755
)
,
p
.
getDirFsPermission
(
)
)
;
Assert
.
assertEquals
(
new
FsPermission
(
(
short
)
0644
)
,
p
.
getFileFsPermission
(
)
)
;
new
PermissionParam
(
)
;
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
new
PermissionParam
(
)
;
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
new
PermissionParam
(
)
;
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
new
PermissionParam
(
)
;
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
PermissionParam
(
)
;
Assert
.
fail
(
)
;
@
Test
public
void
testAclPermissionParam
(
)
{
final
AclPermissionParam
p
=
new
AclPermissionParam
(
)
;
List
<
AclEntry
>
setAclList
=
AclEntry
.
parseAclSpec
(
,
true
)
;
Assert
.
assertEquals
(
setAclList
.
toString
(
)
,
p
.
getAclPermission
(
true
)
.
toString
(
)
)
;
new
AclPermissionParam
(
)
;
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
new
AclPermissionParam
(
)
;
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
new
AclPermissionParam
(
)
;
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
new
AclPermissionParam
(
)
;
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
AclPermissionParam
(
)
;
Assert
.
fail
(
)
;
new
FsActionParam
(
)
;
new
FsActionParam
(
)
;
new
FsActionParam
(
)
;
new
FsActionParam
(
)
;
new
FsActionParam
(
)
;
new
FsActionParam
(
)
;
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
new
FsActionParam
(
)
;
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
new
FsActionParam
(
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
info
(
+
e
)
;
String
hosts
[
]
=
{
,
}
;
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
numDataNodes
(
2
)
.
racks
(
racks
)
.
hosts
(
hosts
)
.
build
(
)
;
cluster
.
waitActive
(
)
;
NamenodeProtocols
nn
=
cluster
.
getNameNodeRpc
(
)
;
Assert
.
assertNotNull
(
nn
)
;
DatanodeInfo
[
]
info
;
while
(
true
)
{
info
=
nn
.
getDatanodeReport
(
DatanodeReportType
.
LIVE
)
;
Assert
.
assertFalse
(
info
.
length
==
2
)
;
if
(
info
.
length
==
1
)
{
break
;
}
Thread
.
sleep
(
1000
)
;
}
int
validIdx
=
info
[
0
]
.
getHostName
(
)
.
equals
(
hosts
[
0
]
)
?
0
:
1
;
int
invalidIdx
=
validIdx
==
1
?
0
:
1
;
StaticMapping
.
addNodeToRack
(
hosts
[
invalidIdx
]
,
racks
[
validIdx
]
)
;
private
void
verifyResults
(
int
upperbound
,
Set
<
Node
>
excludedNodes
,
Map
<
Node
,
Integer
>
frequency
)
{
private
void
verifyResults
(
int
upperbound
,
Set
<
Node
>
excludedNodes
,
Map
<
Node
,
Integer
>
frequency
)
{
LOG
.
info
(
,
excludedNodes
)
;
for
(
int
i
=
0
;
i
<
upperbound
;
++
i
)
{
final
Node
n
=
dataNodes
[
i
]
;
static
FsPermission
checkPermission
(
FileSystem
fs
,
String
path
,
FsPermission
expected
)
throws
IOException
{
FileStatus
s
=
fs
.
getFileStatus
(
new
Path
(
path
)
)
;
out
.
write
(
123
)
;
out
.
close
(
)
;
checkPermission
(
fs
,
,
inheritPerm
)
;
checkPermission
(
fs
,
,
inheritPerm
)
;
checkPermission
(
fs
,
,
filePerm
)
;
conf
.
set
(
FsPermission
.
UMASK_LABEL
,
)
;
permission
=
FsPermission
.
createImmutable
(
(
short
)
0666
)
;
FileSystem
.
mkdirs
(
fs
,
new
Path
(
)
,
new
FsPermission
(
permission
)
)
;
FileSystem
.
create
(
fs
,
new
Path
(
)
,
new
FsPermission
(
permission
)
)
;
checkPermission
(
fs
,
,
permission
)
;
checkPermission
(
fs
,
,
permission
)
;
}
finally
{
try
{
if
(
fs
!=
null
)
fs
.
close
(
)
;
}
catch
(
Exception
e
)
{
checkPermission
(
fs
,
,
filePerm
)
;
conf
.
set
(
FsPermission
.
UMASK_LABEL
,
)
;
permission
=
FsPermission
.
createImmutable
(
(
short
)
0666
)
;
FileSystem
.
mkdirs
(
fs
,
new
Path
(
)
,
new
FsPermission
(
permission
)
)
;
FileSystem
.
create
(
fs
,
new
Path
(
)
,
new
FsPermission
(
permission
)
)
;
checkPermission
(
fs
,
,
permission
)
;
checkPermission
(
fs
,
,
permission
)
;
}
finally
{
try
{
if
(
fs
!=
null
)
fs
.
close
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
StringUtils
.
stringifyException
(
e
)
)
;
}
try
{
if
(
cluster
!=
null
)
cluster
.
shutdown
(
)
;
}
catch
(
Exception
e
)
{
@
Test
public
void
testGroupMappingRefresh
(
)
throws
Exception
{
DFSAdmin
admin
=
new
DFSAdmin
(
config
)
;
String
[
]
args
=
new
String
[
]
{
}
;
Groups
groups
=
Groups
.
getUserToGroupsMappingService
(
config
)
;
String
user
=
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
;
LOG
.
debug
(
)
;
List
<
String
>
g1
=
groups
.
getGroups
(
user
)
;
@
Test
public
void
testGroupMappingRefresh
(
)
throws
Exception
{
DFSAdmin
admin
=
new
DFSAdmin
(
config
)
;
String
[
]
args
=
new
String
[
]
{
}
;
Groups
groups
=
Groups
.
getUserToGroupsMappingService
(
config
)
;
String
user
=
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
;
LOG
.
debug
(
)
;
List
<
String
>
g1
=
groups
.
getGroups
(
user
)
;
LOG
.
debug
(
g1
.
toString
(
)
)
;
LOG
.
debug
(
)
;
List
<
String
>
g2
=
groups
.
getGroups
(
user
)
;
DFSAdmin
admin
=
new
DFSAdmin
(
config
)
;
String
[
]
args
=
new
String
[
]
{
}
;
Groups
groups
=
Groups
.
getUserToGroupsMappingService
(
config
)
;
String
user
=
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
;
LOG
.
debug
(
)
;
List
<
String
>
g1
=
groups
.
getGroups
(
user
)
;
LOG
.
debug
(
g1
.
toString
(
)
)
;
LOG
.
debug
(
)
;
List
<
String
>
g2
=
groups
.
getGroups
(
user
)
;
LOG
.
debug
(
g2
.
toString
(
)
)
;
for
(
int
i
=
0
;
i
<
g2
.
size
(
)
;
i
++
)
{
assertEquals
(
,
g1
.
get
(
i
)
,
g2
.
get
(
i
)
)
;
}
admin
.
run
(
args
)
;
LOG
.
debug
(
)
;
List
<
String
>
g3
=
groups
.
getGroups
(
user
)
;
public
void
start
(
)
throws
IOException
,
FileNotFoundException
{
dfs
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nameNodePort
(
nameNodePort
)
.
nameNodeHttpPort
(
nameNodeHttpPort
)
.
numDataNodes
(
numDataNodes
)
.
startupOption
(
dfsOpts
)
.
format
(
format
)
.
build
(
)
;
dfs
.
waitActive
(
)
;
private
boolean
parseArguments
(
String
[
]
args
)
{
Options
options
=
makeOptions
(
)
;
CommandLine
cli
;
try
{
CommandLineParser
parser
=
new
GnuParser
(
)
;
cli
=
parser
.
parse
(
options
,
args
)
;
}
catch
(
ParseException
e
)
{
LOG
.
warn
(
,
e
)
;
new
HelpFormatter
(
)
.
printHelp
(
,
options
)
;
return
false
;
}
if
(
cli
.
hasOption
(
)
)
{
new
HelpFormatter
(
)
.
printHelp
(
,
options
)
;
return
false
;
}
if
(
cli
.
getArgs
(
)
.
length
>
0
)
{
for
(
String
arg
:
cli
.
getArgs
(
)
)
{
public
void
serviceStart
(
)
throws
Exception
{
taskRunner
=
HadoopExecutors
.
newSingleThreadExecutor
(
new
ThreadFactoryBuilder
(
)
.
setDaemon
(
true
)
.
setNameFormat
(
)
.
build
(
)
)
;
eventHandler
=
new
Thread
(
new
EventHandler
(
)
,
)
;
if
(
jobClassLoader
!=
null
)
{
@
VisibleForTesting
protected
static
MapOutputFile
renameMapOutputForReduce
(
JobConf
conf
,
TaskAttemptId
mapId
,
MapOutputFile
subMapOutputFile
)
throws
IOException
{
FileSystem
localFs
=
FileSystem
.
getLocal
(
conf
)
;
Path
mapOut
=
subMapOutputFile
.
getOutputFile
(
)
;
FileStatus
mStatus
=
localFs
.
getFileStatus
(
mapOut
)
;
Path
reduceIn
=
subMapOutputFile
.
getInputFileForWrite
(
TypeConverter
.
fromYarn
(
mapId
)
.
getTaskID
(
)
,
mStatus
.
getLen
(
)
)
;
Path
mapOutIndex
=
subMapOutputFile
.
getOutputIndexFile
(
)
;
Path
reduceInIndex
=
new
Path
(
reduceIn
.
toString
(
)
+
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
boolean
canCommit
(
TaskAttemptID
taskAttemptID
)
throws
IOException
{
@
Override
public
void
commitPending
(
TaskAttemptID
taskAttemptID
,
TaskStatus
taskStatsu
)
throws
IOException
,
InterruptedException
{
@
Override
public
void
preempted
(
TaskAttemptID
taskAttemptID
,
TaskStatus
taskStatus
)
throws
IOException
,
InterruptedException
{
@
Override
public
void
done
(
TaskAttemptID
taskAttemptID
)
throws
IOException
{
@
Override
public
void
fatalError
(
TaskAttemptID
taskAttemptID
,
String
msg
,
boolean
fastFail
)
throws
IOException
{
@
Override
public
void
fsError
(
TaskAttemptID
taskAttemptID
,
String
message
)
throws
IOException
{
@
Override
public
MapTaskCompletionEventsUpdate
getMapCompletionEvents
(
JobID
jobIdentifier
,
int
startIndex
,
int
maxEvents
,
TaskAttemptID
taskAttemptID
)
throws
IOException
{
@
Override
public
void
reportDiagnosticInfo
(
TaskAttemptID
taskAttemptID
,
String
diagnosticInfo
)
throws
IOException
{
diagnosticInfo
=
StringInterner
.
weakIntern
(
diagnosticInfo
)
;
@
Override
public
AMFeedback
statusUpdate
(
TaskAttemptID
taskAttemptID
,
TaskStatus
taskStatus
)
throws
IOException
,
InterruptedException
{
org
.
apache
.
hadoop
.
mapreduce
.
v2
.
api
.
records
.
TaskAttemptId
yarnAttemptID
=
TypeConverter
.
toYarn
(
taskAttemptID
)
;
AMFeedback
feedback
=
new
AMFeedback
(
)
;
feedback
.
setTaskFound
(
true
)
;
AtomicReference
<
TaskAttemptStatus
>
lastStatusRef
=
attemptIdToStatus
.
get
(
yarnAttemptID
)
;
if
(
lastStatusRef
==
null
)
{
if
(
!
taskHeartbeatHandler
.
hasRecentlyUnregistered
(
yarnAttemptID
)
)
{
@
Override
public
AMFeedback
statusUpdate
(
TaskAttemptID
taskAttemptID
,
TaskStatus
taskStatus
)
throws
IOException
,
InterruptedException
{
org
.
apache
.
hadoop
.
mapreduce
.
v2
.
api
.
records
.
TaskAttemptId
yarnAttemptID
=
TypeConverter
.
toYarn
(
taskAttemptID
)
;
AMFeedback
feedback
=
new
AMFeedback
(
)
;
feedback
.
setTaskFound
(
true
)
;
AtomicReference
<
TaskAttemptStatus
>
lastStatusRef
=
attemptIdToStatus
.
get
(
yarnAttemptID
)
;
if
(
lastStatusRef
==
null
)
{
if
(
!
taskHeartbeatHandler
.
hasRecentlyUnregistered
(
yarnAttemptID
)
)
{
LOG
.
error
(
+
yarnAttemptID
)
;
feedback
.
setTaskFound
(
false
)
;
}
return
feedback
;
}
if
(
getConfig
(
)
.
getBoolean
(
MRJobConfig
.
TASK_PREEMPTION
,
false
)
&&
preemptionPolicy
.
isPreempted
(
yarnAttemptID
)
)
{
feedback
.
setPreemption
(
true
)
;
LOG
.
info
(
+
yarnAttemptID
+
+
yarnAttemptID
.
getTaskId
(
)
.
getTaskType
(
)
)
;
}
if
(
taskStatus
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
JvmTask
getTask
(
JvmContext
context
)
throws
IOException
{
JVMId
jvmId
=
context
.
jvmId
;
@
Override
public
JvmTask
getTask
(
JvmContext
context
)
throws
IOException
{
JVMId
jvmId
=
context
.
jvmId
;
LOG
.
info
(
+
jvmId
+
)
;
JvmTask
jvmTask
=
null
;
WrappedJvmID
wJvmID
=
new
WrappedJvmID
(
jvmId
.
getJobId
(
)
,
jvmId
.
isMap
,
jvmId
.
getId
(
)
)
;
if
(
!
jvmIDToActiveAttemptMap
.
containsKey
(
wJvmID
)
)
{
@
Override
public
JvmTask
getTask
(
JvmContext
context
)
throws
IOException
{
JVMId
jvmId
=
context
.
jvmId
;
LOG
.
info
(
+
jvmId
+
)
;
JvmTask
jvmTask
=
null
;
WrappedJvmID
wJvmID
=
new
WrappedJvmID
(
jvmId
.
getJobId
(
)
,
jvmId
.
isMap
,
jvmId
.
getId
(
)
)
;
if
(
!
jvmIDToActiveAttemptMap
.
containsKey
(
wJvmID
)
)
{
LOG
.
info
(
+
jvmId
+
)
;
jvmTask
=
TASK_FOR_INVALID_JVM
;
}
else
{
if
(
!
launchedJVMs
.
contains
(
wJvmID
)
)
{
jvmTask
=
null
;
LOG
.
info
(
+
jvmId
+
)
;
}
else
{
org
.
apache
.
hadoop
.
mapred
.
Task
task
=
jvmIDToActiveAttemptMap
.
remove
(
wJvmID
)
;
launchedJVMs
.
remove
(
wJvmID
)
;
private
void
coalesceStatusUpdate
(
TaskAttemptId
yarnAttemptID
,
TaskAttemptStatus
taskAttemptStatus
,
AtomicReference
<
TaskAttemptStatus
>
lastStatusRef
)
{
List
<
TaskAttemptId
>
fetchFailedMaps
=
taskAttemptStatus
.
fetchFailedMaps
;
TaskAttemptStatus
lastStatus
=
null
;
boolean
done
=
false
;
while
(
!
done
)
{
lastStatus
=
lastStatusRef
.
get
(
)
;
if
(
lastStatus
!=
null
&&
lastStatus
.
fetchFailedMaps
!=
null
)
{
if
(
taskAttemptStatus
.
fetchFailedMaps
==
null
)
{
taskAttemptStatus
.
fetchFailedMaps
=
lastStatus
.
fetchFailedMaps
;
}
else
{
taskAttemptStatus
.
fetchFailedMaps
=
new
ArrayList
<
>
(
lastStatus
.
fetchFailedMaps
.
size
(
)
+
fetchFailedMaps
.
size
(
)
)
;
taskAttemptStatus
.
fetchFailedMaps
.
addAll
(
lastStatus
.
fetchFailedMaps
)
;
taskAttemptStatus
.
fetchFailedMaps
.
addAll
(
fetchFailedMaps
)
;
}
}
done
=
lastStatusRef
.
compareAndSet
(
lastStatus
,
taskAttemptStatus
)
;
if
(
!
done
)
{
Thread
.
setDefaultUncaughtExceptionHandler
(
new
YarnUncaughtExceptionHandler
(
)
)
;
LOG
.
debug
(
)
;
final
JobConf
job
=
new
JobConf
(
MRJobConfig
.
JOB_CONF_FILE
)
;
Limits
.
init
(
job
)
;
UserGroupInformation
.
setConfiguration
(
job
)
;
SecurityUtil
.
setConfiguration
(
job
)
;
String
host
=
args
[
0
]
;
int
port
=
Integer
.
parseInt
(
args
[
1
]
)
;
final
InetSocketAddress
address
=
NetUtils
.
createSocketAddrForHost
(
host
,
port
)
;
final
TaskAttemptID
firstTaskid
=
TaskAttemptID
.
forName
(
args
[
2
]
)
;
long
jvmIdLong
=
Long
.
parseLong
(
args
[
3
]
)
;
JVMId
jvmId
=
new
JVMId
(
firstTaskid
.
getJobID
(
)
,
firstTaskid
.
getTaskType
(
)
==
TaskType
.
MAP
,
jvmIdLong
)
;
CallerContext
.
setCurrent
(
new
CallerContext
.
Builder
(
+
firstTaskid
.
toString
(
)
)
.
build
(
)
)
;
DefaultMetricsSystem
.
initialize
(
StringUtils
.
camelize
(
firstTaskid
.
getTaskType
(
)
.
name
(
)
)
+
)
;
Credentials
credentials
=
UserGroupInformation
.
getCurrentUser
(
)
.
getCredentials
(
)
;
taskOwner
.
addToken
(
jt
)
;
final
TaskUmbilicalProtocol
umbilical
=
taskOwner
.
doAs
(
new
PrivilegedExceptionAction
<
TaskUmbilicalProtocol
>
(
)
{
@
Override
public
TaskUmbilicalProtocol
run
(
)
throws
Exception
{
return
(
TaskUmbilicalProtocol
)
RPC
.
getProxy
(
TaskUmbilicalProtocol
.
class
,
TaskUmbilicalProtocol
.
versionID
,
address
,
job
)
;
}
}
)
;
JvmContext
context
=
new
JvmContext
(
jvmId
,
)
;
LOG
.
debug
(
+
System
.
getenv
(
)
.
get
(
)
)
;
Task
task
=
null
;
UserGroupInformation
childUGI
=
null
;
ScheduledExecutorService
logSyncer
=
null
;
try
{
int
idleLoopCount
=
0
;
JvmTask
myTask
=
null
;
for
(
int
idle
=
0
;
null
==
myTask
;
++
idle
)
{
long
sleepTimeMilliSecs
=
Math
.
min
(
idle
*
500
,
1500
)
;
return
null
;
}
}
)
;
}
catch
(
FSError
e
)
{
LOG
.
error
(
,
e
)
;
if
(
!
ShutdownHookManager
.
get
(
)
.
isShutdownInProgress
(
)
)
{
umbilical
.
fsError
(
taskid
,
e
.
getMessage
(
)
)
;
}
}
catch
(
Exception
exception
)
{
LOG
.
warn
(
+
StringUtils
.
stringifyException
(
exception
)
)
;
try
{
if
(
task
!=
null
)
{
if
(
childUGI
==
null
)
{
task
.
taskCleanup
(
umbilical
)
;
}
else
{
final
Task
taskFinal
=
task
;
childUGI
.
doAs
(
new
PrivilegedExceptionAction
<
Object
>
(
)
{
private
static
void
configureLocalDirs
(
Task
task
,
JobConf
job
)
throws
IOException
{
String
[
]
localSysDirs
=
StringUtils
.
getTrimmedStrings
(
System
.
getenv
(
Environment
.
LOCAL_DIRS
.
name
(
)
)
)
;
job
.
setStrings
(
MRConfig
.
LOCAL_DIR
,
localSysDirs
)
;
private
static
void
configureTask
(
JobConf
job
,
Task
task
,
Credentials
credentials
,
Token
<
JobTokenIdentifier
>
jt
)
throws
IOException
{
job
.
setCredentials
(
credentials
)
;
ApplicationAttemptId
appAttemptId
=
ContainerId
.
fromString
(
System
.
getenv
(
Environment
.
CONTAINER_ID
.
name
(
)
)
)
.
getApplicationAttemptId
(
)
;
public
static
FSDataInputStream
getPreviousJobHistoryFileStream
(
Configuration
conf
,
ApplicationAttemptId
applicationAttemptId
)
throws
IOException
{
FSDataInputStream
in
=
null
;
Path
historyFile
=
null
;
String
jobId
=
TypeConverter
.
fromYarn
(
applicationAttemptId
.
getApplicationId
(
)
)
.
toString
(
)
;
String
jobhistoryDir
=
JobHistoryUtils
.
getConfiguredHistoryStagingDirPrefix
(
conf
,
jobId
)
;
Path
histDirPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
jobhistoryDir
)
)
;
FileContext
fc
=
FileContext
.
getFileContext
(
histDirPath
.
toUri
(
)
,
conf
)
;
historyFile
=
fc
.
makeQualified
(
JobHistoryUtils
.
getStagingJobHistoryFile
(
histDirPath
,
jobId
,
(
applicationAttemptId
.
getAttemptId
(
)
-
1
)
)
)
;
String
stagingDirStr
=
null
;
String
doneDirStr
=
null
;
String
userDoneDirStr
=
null
;
try
{
stagingDirStr
=
JobHistoryUtils
.
getConfiguredHistoryStagingDirPrefix
(
conf
,
jobId
)
;
doneDirStr
=
JobHistoryUtils
.
getConfiguredHistoryIntermediateDoneDirPrefix
(
conf
)
;
userDoneDirStr
=
JobHistoryUtils
.
getHistoryIntermediateDoneDirForUser
(
conf
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
YarnRuntimeException
(
e
)
;
}
try
{
stagingDirPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
stagingDirStr
)
)
;
stagingDirFS
=
FileSystem
.
get
(
stagingDirPath
.
toUri
(
)
,
conf
)
;
mkdir
(
stagingDirFS
,
stagingDirPath
,
new
FsPermission
(
JobHistoryUtils
.
HISTORY_STAGING_DIR_PERMISSIONS
)
)
;
}
catch
(
IOException
e
)
{
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
YarnRuntimeException
(
e
)
;
}
try
{
stagingDirPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
stagingDirStr
)
)
;
stagingDirFS
=
FileSystem
.
get
(
stagingDirPath
.
toUri
(
)
,
conf
)
;
mkdir
(
stagingDirFS
,
stagingDirPath
,
new
FsPermission
(
JobHistoryUtils
.
HISTORY_STAGING_DIR_PERMISSIONS
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
stagingDirPath
+
,
e
)
;
throw
new
YarnRuntimeException
(
e
)
;
}
Path
doneDirPath
=
null
;
try
{
doneDirPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
doneDirStr
)
)
;
doneDirFS
=
FileSystem
.
get
(
doneDirPath
.
toUri
(
)
,
conf
)
;
if
(
!
doneDirFS
.
exists
(
doneDirPath
)
)
{
mkdir
(
stagingDirFS
,
stagingDirPath
,
new
FsPermission
(
JobHistoryUtils
.
HISTORY_STAGING_DIR_PERMISSIONS
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
stagingDirPath
+
,
e
)
;
throw
new
YarnRuntimeException
(
e
)
;
}
Path
doneDirPath
=
null
;
try
{
doneDirPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
doneDirStr
)
)
;
doneDirFS
=
FileSystem
.
get
(
doneDirPath
.
toUri
(
)
,
conf
)
;
if
(
!
doneDirFS
.
exists
(
doneDirPath
)
)
{
if
(
JobHistoryUtils
.
shouldCreateNonUserDirectory
(
conf
)
)
{
LOG
.
info
(
+
doneDirPath
+
+
MRJobConfig
.
MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR
)
;
mkdir
(
doneDirFS
,
doneDirPath
,
new
FsPermission
(
JobHistoryUtils
.
HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS
.
toShort
(
)
)
)
;
}
else
{
String
message
=
+
doneDirPath
+
+
MRJobConfig
.
MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR
+
+
;
LOG
.
error
(
message
)
;
Path
doneDirPath
=
null
;
try
{
doneDirPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
doneDirStr
)
)
;
doneDirFS
=
FileSystem
.
get
(
doneDirPath
.
toUri
(
)
,
conf
)
;
if
(
!
doneDirFS
.
exists
(
doneDirPath
)
)
{
if
(
JobHistoryUtils
.
shouldCreateNonUserDirectory
(
conf
)
)
{
LOG
.
info
(
+
doneDirPath
+
+
MRJobConfig
.
MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR
)
;
mkdir
(
doneDirFS
,
doneDirPath
,
new
FsPermission
(
JobHistoryUtils
.
HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS
.
toShort
(
)
)
)
;
}
else
{
String
message
=
+
doneDirPath
+
+
MRJobConfig
.
MR_AM_CREATE_JH_INTERMEDIATE_BASE_DIR
+
+
;
LOG
.
error
(
message
)
;
throw
new
YarnRuntimeException
(
message
)
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
+
doneDirPath
+
)
;
throw
new
YarnRuntimeException
(
e
)
;
LOG
.
error
(
+
+
doneDirPath
+
)
;
throw
new
YarnRuntimeException
(
e
)
;
}
try
{
doneDirPrefixPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
userDoneDirStr
)
)
;
mkdir
(
doneDirFS
,
doneDirPrefixPath
,
JobHistoryUtils
.
getConfiguredHistoryIntermediateUserDoneDirPermissions
(
conf
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
doneDirPrefixPath
+
,
e
)
;
throw
new
YarnRuntimeException
(
e
)
;
}
maxUnflushedCompletionEvents
=
conf
.
getInt
(
MRJobConfig
.
MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS
,
MRJobConfig
.
DEFAULT_MR_AM_HISTORY_MAX_UNFLUSHED_COMPLETE_EVENTS
)
;
postJobCompletionMultiplier
=
conf
.
getInt
(
MRJobConfig
.
MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER
,
MRJobConfig
.
DEFAULT_MR_AM_HISTORY_JOB_COMPLETE_UNFLUSHED_MULTIPLIER
)
;
flushTimeout
=
conf
.
getLong
(
MRJobConfig
.
MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS
,
MRJobConfig
.
DEFAULT_MR_AM_HISTORY_COMPLETE_EVENT_FLUSH_TIMEOUT_MS
)
;
minQueueSizeForBatchingFlushes
=
conf
.
getInt
(
MRJobConfig
.
MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD
,
MRJobConfig
.
DEFAULT_MR_AM_HISTORY_USE_BATCHED_FLUSH_QUEUE_SIZE_THRESHOLD
)
;
if
(
conf
.
getBoolean
(
MRJobConfig
.
MAPREDUCE_JOB_EMIT_TIMELINE_DATA
,
MRJobConfig
.
DEFAULT_MAPREDUCE_JOB_EMIT_TIMELINE_DATA
)
)
{
LOG
.
info
(
)
;
if
(
YarnConfiguration
.
timelineServiceEnabled
(
conf
)
)
{
private
void
mkdir
(
FileSystem
fs
,
Path
path
,
FsPermission
fsp
)
throws
IOException
{
if
(
!
fs
.
exists
(
path
)
)
{
try
{
fs
.
mkdirs
(
path
,
fsp
)
;
FileStatus
fsStatus
=
fs
.
getFileStatus
(
path
)
;
LOG
.
info
(
+
fsStatus
.
getPermission
(
)
.
toShort
(
)
+
+
fsp
.
toShort
(
)
)
;
if
(
fsStatus
.
getPermission
(
)
.
toShort
(
)
!=
fsp
.
toShort
(
)
)
{
synchronized
(
lock
)
{
if
(
eventHandlingThread
!=
null
)
{
LOG
.
debug
(
)
;
eventHandlingThread
.
interrupt
(
)
;
}
else
{
LOG
.
debug
(
)
;
}
}
try
{
if
(
eventHandlingThread
!=
null
)
{
LOG
.
debug
(
)
;
eventHandlingThread
.
join
(
)
;
}
}
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
,
ie
)
;
}
for
(
MetaInfo
mi
:
fileMap
.
values
(
)
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
eventHandlingThread
.
interrupt
(
)
;
}
else
{
LOG
.
debug
(
)
;
}
}
try
{
if
(
eventHandlingThread
!=
null
)
{
LOG
.
debug
(
)
;
eventHandlingThread
.
join
(
)
;
}
}
catch
(
InterruptedException
ie
)
{
LOG
.
info
(
,
ie
)
;
}
for
(
MetaInfo
mi
:
fileMap
.
values
(
)
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
mi
)
;
}
mi
.
shutDownTimer
(
)
;
}
catch
(
IOException
e
)
{
catch
(
IOException
e
)
{
LOG
.
info
(
+
+
e
.
getMessage
(
)
)
;
}
}
Iterator
<
JobHistoryEvent
>
it
=
eventQueue
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
JobHistoryEvent
ev
=
it
.
next
(
)
;
LOG
.
info
(
+
ev
.
getType
(
)
)
;
handleEvent
(
ev
)
;
}
if
(
forceJobCompletion
)
{
for
(
Map
.
Entry
<
JobId
,
MetaInfo
>
jobIt
:
fileMap
.
entrySet
(
)
)
{
JobId
toClose
=
jobIt
.
getKey
(
)
;
MetaInfo
mi
=
jobIt
.
getValue
(
)
;
if
(
mi
!=
null
&&
mi
.
isWriterActive
(
)
)
{
LOG
.
warn
(
+
toClose
+
)
;
final
Job
job
=
context
.
getJob
(
toClose
)
;
int
successfulMaps
=
job
.
getCompletedMaps
(
)
-
job
.
getFailedMaps
(
)
-
job
.
getKilledMaps
(
)
;
if
(
stagingDirPath
==
null
)
{
LOG
.
error
(
)
;
throw
new
IOException
(
)
;
}
MetaInfo
oldFi
=
fileMap
.
get
(
jobId
)
;
Configuration
conf
=
getConfig
(
)
;
Path
historyFile
=
JobHistoryUtils
.
getStagingJobHistoryFile
(
stagingDirPath
,
jobId
,
startCount
)
;
String
user
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
if
(
user
==
null
)
{
throw
new
IOException
(
)
;
}
String
jobName
=
context
.
getJob
(
jobId
)
.
getName
(
)
;
EventWriter
writer
=
(
oldFi
==
null
)
?
null
:
oldFi
.
writer
;
Path
logDirConfPath
=
JobHistoryUtils
.
getStagingConfFile
(
stagingDirPath
,
jobId
,
startCount
)
;
if
(
writer
==
null
)
{
try
{
writer
=
createEventWriter
(
historyFile
)
;
throw
new
IOException
(
)
;
}
MetaInfo
oldFi
=
fileMap
.
get
(
jobId
)
;
Configuration
conf
=
getConfig
(
)
;
Path
historyFile
=
JobHistoryUtils
.
getStagingJobHistoryFile
(
stagingDirPath
,
jobId
,
startCount
)
;
String
user
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
if
(
user
==
null
)
{
throw
new
IOException
(
)
;
}
String
jobName
=
context
.
getJob
(
jobId
)
.
getName
(
)
;
EventWriter
writer
=
(
oldFi
==
null
)
?
null
:
oldFi
.
writer
;
Path
logDirConfPath
=
JobHistoryUtils
.
getStagingConfFile
(
stagingDirPath
,
jobId
,
startCount
)
;
if
(
writer
==
null
)
{
try
{
writer
=
createEventWriter
(
historyFile
)
;
LOG
.
info
(
+
jobId
+
+
historyFile
)
;
}
catch
(
IOException
ioe
)
{
try
{
AMStartedEvent
amStartedEvent
=
(
AMStartedEvent
)
event
.
getHistoryEvent
(
)
;
setupEventWriter
(
event
.
getJobID
(
)
,
amStartedEvent
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
error
(
+
event
,
ioe
)
;
throw
new
YarnRuntimeException
(
ioe
)
;
}
}
MetaInfo
mi
=
fileMap
.
get
(
event
.
getJobID
(
)
)
;
try
{
HistoryEvent
historyEvent
=
event
.
getHistoryEvent
(
)
;
if
(
!
(
historyEvent
instanceof
NormalizedResourceEvent
)
)
{
mi
.
writeEvent
(
historyEvent
)
;
}
processEventForJobSummary
(
event
.
getHistoryEvent
(
)
,
mi
.
getJobSummary
(
)
,
event
.
getJobID
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
event
.
getHistoryEvent
(
)
.
getEventType
(
)
)
;
}
}
catch
(
IOException
e
)
{
tEvent
.
addEventInfo
(
,
ase
.
getStartTime
(
)
)
;
tEvent
.
addEventInfo
(
,
ase
.
getSubmitTime
(
)
)
;
tEntity
.
addEvent
(
tEvent
)
;
tEntity
.
setEntityId
(
jobId
.
toString
(
)
)
;
tEntity
.
setEntityType
(
MAPREDUCE_JOB_ENTITY_TYPE
)
;
break
;
default
:
break
;
}
try
{
TimelinePutResponse
response
=
timelineClient
.
putEntities
(
tEntity
)
;
List
<
TimelinePutResponse
.
TimelinePutError
>
errors
=
response
.
getErrors
(
)
;
if
(
errors
.
size
(
)
==
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
event
.
getEventType
(
)
)
;
}
}
else
{
for
(
TimelinePutResponse
.
TimelinePutError
error
:
errors
)
{
tEntity
.
addEvent
(
tEvent
)
;
tEntity
.
setEntityId
(
jobId
.
toString
(
)
)
;
tEntity
.
setEntityType
(
MAPREDUCE_JOB_ENTITY_TYPE
)
;
break
;
default
:
break
;
}
try
{
TimelinePutResponse
response
=
timelineClient
.
putEntities
(
tEntity
)
;
List
<
TimelinePutResponse
.
TimelinePutError
>
errors
=
response
.
getErrors
(
)
;
if
(
errors
.
size
(
)
==
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
event
.
getEventType
(
)
)
;
}
}
else
{
for
(
TimelinePutResponse
.
TimelinePutError
error
:
errors
)
{
LOG
.
error
(
+
error
.
getEntityType
(
)
+
+
error
.
getEntityId
(
)
+
+
error
.
getErrorCode
(
)
)
;
}
}
}
catch
(
YarnException
|
IOException
|
ClientHandlerException
ex
)
{
int
size
=
entry
.
getKey
(
)
.
length
(
)
+
entry
.
getValue
(
)
.
length
(
)
;
configSize
+=
size
;
if
(
configSize
>
JobHistoryEventUtils
.
ATS_CONFIG_PUBLISH_SIZE_BYTES
)
{
if
(
jobEntityForConfigs
.
getConfigs
(
)
.
size
(
)
>
0
)
{
timelineV2Client
.
putEntities
(
jobEntityForConfigs
)
;
timelineV2Client
.
putEntities
(
appEntityForConfigs
)
;
jobEntityForConfigs
=
createJobEntity
(
jobId
)
;
appEntityForConfigs
=
new
ApplicationEntity
(
)
;
appEntityForConfigs
.
setId
(
appId
)
;
}
configSize
=
size
;
}
jobEntityForConfigs
.
addConfig
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
appEntityForConfigs
.
addConfig
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
if
(
configSize
>
0
)
{
timelineV2Client
.
putEntities
(
jobEntityForConfigs
)
;
timelineV2Client
.
putEntities
(
appEntityForConfigs
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEntity
appEntityWithJobMetrics
=
null
;
if
(
taskId
==
null
)
{
tEntity
=
createJobEntity
(
event
,
timestamp
,
jobId
,
MAPREDUCE_JOB_ENTITY_TYPE
,
setCreatedTime
)
;
if
(
event
.
getEventType
(
)
==
EventType
.
JOB_FINISHED
&&
event
.
getTimelineMetrics
(
)
!=
null
)
{
appEntityWithJobMetrics
=
createAppEntityWithJobMetrics
(
event
,
jobId
)
;
}
}
else
{
if
(
taskAttemptId
==
null
)
{
tEntity
=
createTaskEntity
(
event
,
timestamp
,
taskId
,
MAPREDUCE_TASK_ENTITY_TYPE
,
MAPREDUCE_JOB_ENTITY_TYPE
,
jobId
,
setCreatedTime
,
taskIdPrefix
)
;
}
else
{
tEntity
=
createTaskAttemptEntity
(
event
,
timestamp
,
taskAttemptId
,
MAPREDUCE_TASK_ATTEMPT_ENTITY_TYPE
,
MAPREDUCE_TASK_ENTITY_TYPE
,
taskId
,
setCreatedTime
,
taskAttemptIdPrefix
)
;
}
}
try
{
if
(
appEntityWithJobMetrics
==
null
)
{
timelineV2Client
.
putEntitiesAsync
(
tEntity
)
;
}
else
{
timelineV2Client
.
putEntities
(
tEntity
,
appEntityWithJobMetrics
)
;
throw
new
IOException
(
+
jobId
+
)
;
}
if
(
mi
.
getHistoryFile
(
)
==
null
)
{
LOG
.
warn
(
+
jobId
+
)
;
}
if
(
mi
.
getConfFile
(
)
==
null
)
{
LOG
.
warn
(
+
jobId
+
)
;
}
Path
qualifiedSummaryDoneFile
=
null
;
FSDataOutputStream
summaryFileOut
=
null
;
try
{
String
doneSummaryFileName
=
getTempFileName
(
JobHistoryUtils
.
getIntermediateSummaryFileName
(
jobId
)
)
;
qualifiedSummaryDoneFile
=
doneDirFS
.
makeQualified
(
new
Path
(
doneDirPrefixPath
,
doneSummaryFileName
)
)
;
summaryFileOut
=
doneDirFS
.
create
(
qualifiedSummaryDoneFile
,
true
)
;
summaryFileOut
.
writeUTF
(
mi
.
getJobSummary
(
)
.
getJobSummaryString
(
)
)
;
summaryFileOut
.
close
(
)
;
doneDirFS
.
setPermission
(
qualifiedSummaryDoneFile
,
new
FsPermission
(
JobHistoryUtils
.
HISTORY_INTERMEDIATE_FILE_PERMISSIONS
)
)
;
}
catch
(
IOException
e
)
{
Path
historyFile
=
mi
.
getHistoryFile
(
)
;
Path
qualifiedLogFile
=
stagingDirFS
.
makeQualified
(
historyFile
)
;
int
jobNameLimit
=
getConfig
(
)
.
getInt
(
JHAdminConfig
.
MR_HS_JOBNAME_LIMIT
,
JHAdminConfig
.
DEFAULT_MR_HS_JOBNAME_LIMIT
)
;
String
doneJobHistoryFileName
=
getTempFileName
(
FileNameIndexUtils
.
getDoneFileName
(
mi
.
getJobIndexInfo
(
)
,
jobNameLimit
)
)
;
qualifiedDoneFile
=
doneDirFS
.
makeQualified
(
new
Path
(
doneDirPrefixPath
,
doneJobHistoryFileName
)
)
;
if
(
moveToDoneNow
(
qualifiedLogFile
,
qualifiedDoneFile
)
)
{
String
historyUrl
=
MRWebAppUtil
.
getApplicationWebURLOnJHSWithScheme
(
getConfig
(
)
,
context
.
getApplicationID
(
)
)
;
context
.
setHistoryUrl
(
historyUrl
)
;
LOG
.
info
(
+
historyUrl
)
;
}
}
Path
qualifiedConfDoneFile
=
null
;
if
(
mi
.
getConfFile
(
)
!=
null
)
{
Path
confFile
=
mi
.
getConfFile
(
)
;
Path
qualifiedConfFile
=
stagingDirFS
.
makeQualified
(
confFile
)
;
String
doneConfFileName
=
getTempFileName
(
JobHistoryUtils
.
getIntermediateConfFileName
(
jobId
)
)
;
qualifiedConfDoneFile
=
doneDirFS
.
makeQualified
(
new
Path
(
doneDirPrefixPath
,
doneConfFileName
)
)
;
protected
void
moveTmpToDone
(
Path
tmpPath
)
throws
IOException
{
if
(
tmpPath
!=
null
)
{
String
tmpFileName
=
tmpPath
.
getName
(
)
;
String
fileName
=
getFileNameFromTmpFN
(
tmpFileName
)
;
Path
path
=
new
Path
(
tmpPath
.
getParent
(
)
,
fileName
)
;
doneDirFS
.
rename
(
tmpPath
,
path
)
;
protected
boolean
moveToDoneNow
(
Path
fromPath
,
Path
toPath
)
throws
IOException
{
boolean
success
=
false
;
if
(
stagingDirFS
.
exists
(
fromPath
)
)
{
protected
boolean
moveToDoneNow
(
Path
fromPath
,
Path
toPath
)
throws
IOException
{
boolean
success
=
false
;
if
(
stagingDirFS
.
exists
(
fromPath
)
)
{
LOG
.
info
(
+
fromPath
.
toString
(
)
+
+
toPath
.
toString
(
)
)
;
doneDirFS
.
delete
(
toPath
,
true
)
;
boolean
copied
=
FileUtil
.
copy
(
stagingDirFS
,
fromPath
,
doneDirFS
,
toPath
,
false
,
getConfig
(
)
)
;
doneDirFS
.
setPermission
(
toPath
,
new
FsPermission
(
JobHistoryUtils
.
HISTORY_INTERMEDIATE_FILE_PERMISSIONS
)
)
;
if
(
copied
)
{
public
void
setForcejobCompletion
(
boolean
forceJobCompletion
)
{
this
.
forceJobCompletion
=
forceJobCompletion
;
}
boolean
copyHistory
=
false
;
committer
=
createOutputCommitter
(
conf
)
;
try
{
String
user
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
Path
stagingDir
=
MRApps
.
getStagingAreaDir
(
conf
,
user
)
;
FileSystem
fs
=
getFileSystem
(
conf
)
;
boolean
stagingExists
=
fs
.
exists
(
stagingDir
)
;
Path
startCommitFile
=
MRApps
.
getStartJobCommitFile
(
conf
,
user
,
jobId
)
;
boolean
commitStarted
=
fs
.
exists
(
startCommitFile
)
;
Path
endCommitSuccessFile
=
MRApps
.
getEndJobCommitSuccessFile
(
conf
,
user
,
jobId
)
;
boolean
commitSuccess
=
fs
.
exists
(
endCommitSuccessFile
)
;
Path
endCommitFailureFile
=
MRApps
.
getEndJobCommitFailureFile
(
conf
,
user
,
jobId
)
;
boolean
commitFailure
=
fs
.
exists
(
endCommitFailureFile
)
;
if
(
!
stagingExists
)
{
isLastAMRetry
=
true
;
Path
startCommitFile
=
MRApps
.
getStartJobCommitFile
(
conf
,
user
,
jobId
)
;
boolean
commitStarted
=
fs
.
exists
(
startCommitFile
)
;
Path
endCommitSuccessFile
=
MRApps
.
getEndJobCommitSuccessFile
(
conf
,
user
,
jobId
)
;
boolean
commitSuccess
=
fs
.
exists
(
endCommitSuccessFile
)
;
Path
endCommitFailureFile
=
MRApps
.
getEndJobCommitFailureFile
(
conf
,
user
,
jobId
)
;
boolean
commitFailure
=
fs
.
exists
(
endCommitFailureFile
)
;
if
(
!
stagingExists
)
{
isLastAMRetry
=
true
;
LOG
.
info
(
+
appAttemptID
.
getAttemptId
(
)
+
+
isLastAMRetry
+
)
;
errorHappenedShutDown
=
true
;
forcedState
=
JobStateInternal
.
ERROR
;
shutDownMessage
=
+
stagingDir
;
LOG
.
error
(
shutDownMessage
)
;
}
else
if
(
commitStarted
)
{
errorHappenedShutDown
=
true
;
private
OutputCommitter
createOutputCommitter
(
Configuration
conf
)
{
return
callWithJobClassLoader
(
conf
,
new
Action
<
OutputCommitter
>
(
)
{
public
OutputCommitter
call
(
Configuration
conf
)
{
OutputCommitter
committer
=
null
;
return
callWithJobClassLoader
(
conf
,
new
Action
<
Speculator
>
(
)
{
public
Speculator
call
(
Configuration
conf
)
{
Class
<
?
extends
Speculator
>
speculatorClass
;
try
{
speculatorClass
=
conf
.
getClass
(
MRJobConfig
.
MR_AM_JOB_SPECULATOR
,
DefaultSpeculator
.
class
,
Speculator
.
class
)
;
Constructor
<
?
extends
Speculator
>
speculatorConstructor
=
speculatorClass
.
getConstructor
(
Configuration
.
class
,
AppContext
.
class
)
;
Speculator
result
=
speculatorConstructor
.
newInstance
(
conf
,
context
)
;
return
result
;
}
catch
(
InstantiationException
ex
)
{
LOG
.
error
(
+
MRJobConfig
.
MR_AM_JOB_SPECULATOR
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
IllegalAccessException
ex
)
{
LOG
.
error
(
+
MRJobConfig
.
MR_AM_JOB_SPECULATOR
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
InvocationTargetException
ex
)
{
try
{
speculatorClass
=
conf
.
getClass
(
MRJobConfig
.
MR_AM_JOB_SPECULATOR
,
DefaultSpeculator
.
class
,
Speculator
.
class
)
;
Constructor
<
?
extends
Speculator
>
speculatorConstructor
=
speculatorClass
.
getConstructor
(
Configuration
.
class
,
AppContext
.
class
)
;
Speculator
result
=
speculatorConstructor
.
newInstance
(
conf
,
context
)
;
return
result
;
}
catch
(
InstantiationException
ex
)
{
LOG
.
error
(
+
MRJobConfig
.
MR_AM_JOB_SPECULATOR
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
IllegalAccessException
ex
)
{
LOG
.
error
(
+
MRJobConfig
.
MR_AM_JOB_SPECULATOR
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
InvocationTargetException
ex
)
{
LOG
.
error
(
+
MRJobConfig
.
MR_AM_JOB_SPECULATOR
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
NoSuchMethodException
ex
)
{
cleanUpPreviousJobOutput
(
)
;
AMInfo
amInfo
=
MRBuilderUtils
.
newAMInfo
(
appAttemptID
,
startTime
,
containerID
,
nmHost
,
nmPort
,
nmHttpPort
)
;
job
=
createJob
(
getConfig
(
)
,
forcedState
,
shutDownMessage
)
;
for
(
AMInfo
info
:
amInfos
)
{
dispatcher
.
getEventHandler
(
)
.
handle
(
new
JobHistoryEvent
(
job
.
getID
(
)
,
new
AMStartedEvent
(
info
.
getAppAttemptId
(
)
,
info
.
getStartTime
(
)
,
info
.
getContainerId
(
)
,
info
.
getNodeManagerHost
(
)
,
info
.
getNodeManagerPort
(
)
,
info
.
getNodeManagerHttpPort
(
)
,
appSubmitTime
)
)
)
;
}
dispatcher
.
getEventHandler
(
)
.
handle
(
new
JobHistoryEvent
(
job
.
getID
(
)
,
new
AMStartedEvent
(
amInfo
.
getAppAttemptId
(
)
,
amInfo
.
getStartTime
(
)
,
amInfo
.
getContainerId
(
)
,
amInfo
.
getNodeManagerHost
(
)
,
amInfo
.
getNodeManagerPort
(
)
,
amInfo
.
getNodeManagerHttpPort
(
)
,
this
.
forcedState
==
null
?
null
:
this
.
forcedState
.
toString
(
)
,
appSubmitTime
)
)
)
;
amInfos
.
add
(
amInfo
)
;
DefaultMetricsSystem
.
initialize
(
)
;
boolean
initFailed
=
false
;
if
(
!
errorHappenedShutDown
)
{
JobEvent
initJobEvent
=
new
JobEvent
(
job
.
getID
(
)
,
JobEventType
.
JOB_INIT
)
;
jobEventDispatcher
.
handle
(
initJobEvent
)
;
initFailed
=
(
(
(
JobImpl
)
job
)
.
getInternalState
(
)
!=
JobStateInternal
.
INITED
)
;
if
(
job
.
isUber
(
)
)
{
speculatorEventDispatcher
.
disableSpeculation
(
)
;
for
(
AMInfo
info
:
amInfos
)
{
dispatcher
.
getEventHandler
(
)
.
handle
(
new
JobHistoryEvent
(
job
.
getID
(
)
,
new
AMStartedEvent
(
info
.
getAppAttemptId
(
)
,
info
.
getStartTime
(
)
,
info
.
getContainerId
(
)
,
info
.
getNodeManagerHost
(
)
,
info
.
getNodeManagerPort
(
)
,
info
.
getNodeManagerHttpPort
(
)
,
appSubmitTime
)
)
)
;
}
dispatcher
.
getEventHandler
(
)
.
handle
(
new
JobHistoryEvent
(
job
.
getID
(
)
,
new
AMStartedEvent
(
amInfo
.
getAppAttemptId
(
)
,
amInfo
.
getStartTime
(
)
,
amInfo
.
getContainerId
(
)
,
amInfo
.
getNodeManagerHost
(
)
,
amInfo
.
getNodeManagerPort
(
)
,
amInfo
.
getNodeManagerHttpPort
(
)
,
this
.
forcedState
==
null
?
null
:
this
.
forcedState
.
toString
(
)
,
appSubmitTime
)
)
)
;
amInfos
.
add
(
amInfo
)
;
DefaultMetricsSystem
.
initialize
(
)
;
boolean
initFailed
=
false
;
if
(
!
errorHappenedShutDown
)
{
JobEvent
initJobEvent
=
new
JobEvent
(
job
.
getID
(
)
,
JobEventType
.
JOB_INIT
)
;
jobEventDispatcher
.
handle
(
initJobEvent
)
;
initFailed
=
(
(
(
JobImpl
)
job
)
.
getInternalState
(
)
!=
JobStateInternal
.
INITED
)
;
if
(
job
.
isUber
(
)
)
{
speculatorEventDispatcher
.
disableSpeculation
(
)
;
LOG
.
info
(
+
job
.
getID
(
)
+
+
nmHost
+
+
nmPort
+
)
;
}
else
{
dispatcher
.
getEventHandler
(
)
.
handle
(
new
SpeculatorEvent
(
job
.
getID
(
)
,
clock
.
getTime
(
)
)
)
;
private
static
FSDataInputStream
getPreviousJobHistoryStream
(
Configuration
conf
,
ApplicationAttemptId
appAttemptId
)
throws
IOException
{
Path
historyFile
=
JobHistoryUtils
.
getPreviousJobHistoryPath
(
conf
,
appAttemptId
)
;
private
void
parsePreviousJobHistory
(
)
throws
IOException
{
FSDataInputStream
in
=
getPreviousJobHistoryStream
(
getConfig
(
)
,
appAttemptID
)
;
JobHistoryParser
parser
=
new
JobHistoryParser
(
in
)
;
JobInfo
jobInfo
=
parser
.
parse
(
)
;
Exception
parseException
=
parser
.
getParseException
(
)
;
if
(
parseException
!=
null
)
{
FSDataInputStream
in
=
getPreviousJobHistoryStream
(
getConfig
(
)
,
appAttemptID
)
;
JobHistoryParser
parser
=
new
JobHistoryParser
(
in
)
;
JobInfo
jobInfo
=
parser
.
parse
(
)
;
Exception
parseException
=
parser
.
getParseException
(
)
;
if
(
parseException
!=
null
)
{
LOG
.
info
(
+
,
parseException
)
;
}
Map
<
org
.
apache
.
hadoop
.
mapreduce
.
TaskID
,
TaskInfo
>
taskInfos
=
jobInfo
.
getAllTasks
(
)
;
for
(
TaskInfo
taskInfo
:
taskInfos
.
values
(
)
)
{
if
(
TaskState
.
SUCCEEDED
.
toString
(
)
.
equals
(
taskInfo
.
getTaskStatus
(
)
)
)
{
Iterator
<
Entry
<
TaskAttemptID
,
TaskAttemptInfo
>>
taskAttemptIterator
=
taskInfo
.
getAllTaskAttempts
(
)
.
entrySet
(
)
.
iterator
(
)
;
while
(
taskAttemptIterator
.
hasNext
(
)
)
{
Map
.
Entry
<
TaskAttemptID
,
TaskAttemptInfo
>
currentEntry
=
taskAttemptIterator
.
next
(
)
;
if
(
!
jobInfo
.
getAllCompletedTaskAttempts
(
)
.
containsKey
(
currentEntry
.
getKey
(
)
)
)
{
taskAttemptIterator
.
remove
(
)
;
}
}
completedTasksFromPreviousRun
.
put
(
TypeConverter
.
toYarn
(
taskInfo
.
getTaskId
(
)
)
,
taskInfo
)
;
private
static
void
validateInputParam
(
String
value
,
String
param
)
throws
IOException
{
if
(
value
==
null
)
{
String
msg
=
param
+
;
validateInputParam
(
nodePortString
,
Environment
.
NM_PORT
.
name
(
)
)
;
validateInputParam
(
nodeHttpPortString
,
Environment
.
NM_HTTP_PORT
.
name
(
)
)
;
validateInputParam
(
appSubmitTimeStr
,
ApplicationConstants
.
APP_SUBMIT_TIME_ENV
)
;
ContainerId
containerId
=
ContainerId
.
fromString
(
containerIdStr
)
;
ApplicationAttemptId
applicationAttemptId
=
containerId
.
getApplicationAttemptId
(
)
;
if
(
applicationAttemptId
!=
null
)
{
CallerContext
.
setCurrent
(
new
CallerContext
.
Builder
(
+
applicationAttemptId
.
toString
(
)
)
.
build
(
)
)
;
}
long
appSubmitTime
=
Long
.
parseLong
(
appSubmitTimeStr
)
;
MRAppMaster
appMaster
=
new
MRAppMaster
(
applicationAttemptId
,
containerId
,
nodeHostString
,
Integer
.
parseInt
(
nodePortString
)
,
Integer
.
parseInt
(
nodeHttpPortString
)
,
appSubmitTime
)
;
ShutdownHookManager
.
get
(
)
.
addShutdownHook
(
new
MRAppMasterShutdownHook
(
appMaster
)
,
SHUTDOWN_HOOK_PRIORITY
)
;
JobConf
conf
=
new
JobConf
(
new
YarnConfiguration
(
)
)
;
conf
.
addResource
(
new
Path
(
MRJobConfig
.
JOB_CONF_FILE
)
)
;
MRWebAppUtil
.
initialize
(
conf
)
;
String
systemPropsToLog
=
MRApps
.
getSystemPropertiesToLog
(
conf
)
;
if
(
systemPropsToLog
!=
null
)
{
if
(
applicationAttemptId
!=
null
)
{
CallerContext
.
setCurrent
(
new
CallerContext
.
Builder
(
+
applicationAttemptId
.
toString
(
)
)
.
build
(
)
)
;
}
long
appSubmitTime
=
Long
.
parseLong
(
appSubmitTimeStr
)
;
MRAppMaster
appMaster
=
new
MRAppMaster
(
applicationAttemptId
,
containerId
,
nodeHostString
,
Integer
.
parseInt
(
nodePortString
)
,
Integer
.
parseInt
(
nodeHttpPortString
)
,
appSubmitTime
)
;
ShutdownHookManager
.
get
(
)
.
addShutdownHook
(
new
MRAppMasterShutdownHook
(
appMaster
)
,
SHUTDOWN_HOOK_PRIORITY
)
;
JobConf
conf
=
new
JobConf
(
new
YarnConfiguration
(
)
)
;
conf
.
addResource
(
new
Path
(
MRJobConfig
.
JOB_CONF_FILE
)
)
;
MRWebAppUtil
.
initialize
(
conf
)
;
String
systemPropsToLog
=
MRApps
.
getSystemPropertiesToLog
(
conf
)
;
if
(
systemPropsToLog
!=
null
)
{
LOG
.
info
(
systemPropsToLog
)
;
}
String
jobUserName
=
System
.
getenv
(
ApplicationConstants
.
Environment
.
USER
.
name
(
)
)
;
conf
.
set
(
MRJobConfig
.
USER_NAME
,
jobUserName
)
;
initAndStartAppMaster
(
appMaster
,
conf
,
jobUserName
)
;
}
catch
(
Throwable
t
)
{
public
void
notifyIsLastAMRetry
(
boolean
isLastAMRetry
)
{
if
(
containerAllocator
instanceof
ContainerAllocatorRouter
)
{
protected
static
void
initAndStartAppMaster
(
final
MRAppMaster
appMaster
,
final
JobConf
conf
,
String
jobUserName
)
throws
IOException
,
InterruptedException
{
UserGroupInformation
.
setConfiguration
(
conf
)
;
SecurityUtil
.
setConfiguration
(
conf
)
;
Credentials
credentials
=
UserGroupInformation
.
getCurrentUser
(
)
.
getCredentials
(
)
;
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
super
.
serviceInit
(
conf
)
;
taskTimeOut
=
conf
.
getLong
(
MRJobConfig
.
TASK_TIMEOUT
,
MRJobConfig
.
DEFAULT_TASK_TIMEOUT_MILLIS
)
;
unregisterTimeOut
=
conf
.
getLong
(
MRJobConfig
.
TASK_EXIT_TIMEOUT
,
MRJobConfig
.
TASK_EXIT_TIMEOUT_DEFAULT
)
;
taskStuckTimeOut
=
conf
.
getLong
(
MRJobConfig
.
TASK_STUCK_TIMEOUT_MS
,
MRJobConfig
.
DEFAULT_TASK_STUCK_TIMEOUT_MS
)
;
long
taskProgressReportIntervalMillis
=
MRJobConfUtil
.
getTaskProgressReportInterval
(
conf
)
;
long
minimumTaskTimeoutAllowed
=
taskProgressReportIntervalMillis
*
2
;
if
(
taskTimeOut
<
minimumTaskTimeoutAllowed
)
{
taskTimeOut
=
minimumTaskTimeoutAllowed
;
protected
void
serviceStart
(
)
throws
Exception
{
Configuration
conf
=
getConfig
(
)
;
YarnRPC
rpc
=
YarnRPC
.
create
(
conf
)
;
InetSocketAddress
address
=
new
InetSocketAddress
(
0
)
;
server
=
rpc
.
getServer
(
MRClientProtocol
.
class
,
protocolHandler
,
address
,
conf
,
appContext
.
getClientToAMTokenSecretManager
(
)
,
conf
.
getInt
(
MRJobConfig
.
MR_AM_JOB_CLIENT_THREAD_COUNT
,
MRJobConfig
.
DEFAULT_MR_AM_JOB_CLIENT_THREAD_COUNT
)
,
MRJobConfig
.
MR_AM_JOB_CLIENT_PORT_RANGE
)
;
if
(
conf
.
getBoolean
(
CommonConfigurationKeysPublic
.
HADOOP_SECURITY_AUTHORIZATION
,
false
)
)
{
refreshServiceAcls
(
conf
,
new
MRAMPolicyProvider
(
)
)
;
}
server
.
start
(
)
;
this
.
bindAddress
=
NetUtils
.
createSocketAddrForHost
(
appContext
.
getNMHostname
(
)
,
server
.
getListenerAddress
(
)
.
getPort
(
)
)
;
LOG
.
info
(
+
this
.
bindAddress
)
;
try
{
HttpConfig
.
Policy
httpPolicy
=
conf
.
getBoolean
(
MRJobConfig
.
MR_AM_WEBAPP_HTTPS_ENABLED
,
MRJobConfig
.
DEFAULT_MR_AM_WEBAPP_HTTPS_ENABLED
)
?
Policy
.
HTTPS_ONLY
:
Policy
.
HTTP_ONLY
;
boolean
needsClientAuth
=
conf
.
getBoolean
(
MRJobConfig
.
MR_AM_WEBAPP_HTTPS_CLIENT_AUTH
,
MRJobConfig
.
DEFAULT_MR_AM_WEBAPP_HTTPS_CLIENT_AUTH
)
;
webApp
=
WebApps
.
$for
(
,
AppContext
.
class
,
appContext
,
)
.
withHttpPolicy
(
conf
,
httpPolicy
)
.
withPortRange
(
conf
,
MRJobConfig
.
MR_AM_WEBAPP_PORT_RANGE
)
.
needsClientAuth
(
needsClientAuth
)
.
start
(
new
AMWebApp
(
)
)
;
}
catch
(
Exception
e
)
{
Thread
thread
=
new
Thread
(
r
)
;
thread
.
setContextClassLoader
(
jobClassLoader
)
;
return
thread
;
}
}
;
tfBuilder
.
setThreadFactory
(
backingTf
)
;
}
ThreadFactory
tf
=
tfBuilder
.
build
(
)
;
launcherPool
=
new
HadoopThreadPoolExecutor
(
5
,
5
,
1
,
TimeUnit
.
HOURS
,
new
LinkedBlockingQueue
<
Runnable
>
(
)
,
tf
)
;
eventHandlingThread
=
new
Thread
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
CommitterEvent
event
=
null
;
while
(
!
stopped
.
get
(
)
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
try
{
event
=
eventQueue
.
take
(
)
;
}
catch
(
InterruptedException
e
)
{
if
(
!
stopped
.
get
(
)
)
{
@
Override
public
void
handle
(
JobEvent
event
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
boolean
smallInput
=
(
dataInputLength
<=
sysMaxBytes
)
;
long
requiredMapMB
=
conf
.
getLong
(
MRJobConfig
.
MAP_MEMORY_MB
,
0
)
;
long
requiredReduceMB
=
conf
.
getLong
(
MRJobConfig
.
REDUCE_MEMORY_MB
,
0
)
;
long
requiredMB
=
Math
.
max
(
requiredMapMB
,
requiredReduceMB
)
;
int
requiredMapCores
=
conf
.
getInt
(
MRJobConfig
.
MAP_CPU_VCORES
,
MRJobConfig
.
DEFAULT_MAP_CPU_VCORES
)
;
int
requiredReduceCores
=
conf
.
getInt
(
MRJobConfig
.
REDUCE_CPU_VCORES
,
MRJobConfig
.
DEFAULT_REDUCE_CPU_VCORES
)
;
int
requiredCores
=
Math
.
max
(
requiredMapCores
,
requiredReduceCores
)
;
if
(
numReduceTasks
==
0
)
{
requiredMB
=
requiredMapMB
;
requiredCores
=
requiredMapCores
;
}
boolean
smallMemory
=
(
requiredMB
<=
sysMemSizeForUberSlot
)
||
(
sysMemSizeForUberSlot
==
JobConf
.
DISABLED_MEMORY_LIMIT
)
;
boolean
smallCpu
=
requiredCores
<=
sysCPUSizeForUberSlot
;
boolean
notChainJob
=
!
isChainJob
(
conf
)
;
isUber
=
uberEnabled
&&
smallNumMapTasks
&&
smallNumReduceTasks
&&
smallInput
&&
smallMemory
&&
smallCpu
&&
notChainJob
;
if
(
isUber
)
{
conf
.
setFloat
(
MRJobConfig
.
COMPLETED_MAPS_FOR_REDUCE_SLOWSTART
,
1.0f
)
;
conf
.
setInt
(
MRJobConfig
.
MAP_MAX_ATTEMPTS
,
1
)
;
conf
.
setInt
(
MRJobConfig
.
REDUCE_MAX_ATTEMPTS
,
1
)
;
conf
.
setBoolean
(
MRJobConfig
.
MAP_SPECULATIVE
,
false
)
;
conf
.
setBoolean
(
MRJobConfig
.
REDUCE_SPECULATIVE
,
false
)
;
}
else
{
StringBuilder
msg
=
new
StringBuilder
(
)
;
msg
.
append
(
)
.
append
(
jobId
)
.
append
(
)
;
if
(
!
uberEnabled
)
msg
.
append
(
)
;
if
(
!
smallNumMapTasks
)
msg
.
append
(
)
;
if
(
!
smallNumReduceTasks
)
msg
.
append
(
)
;
if
(
!
smallInput
)
msg
.
append
(
)
;
if
(
!
smallCpu
)
msg
.
append
(
)
;
if
(
!
smallMemory
)
msg
.
append
(
)
;
if
(
!
smallCpu
)
msg
.
append
(
)
;
private
void
actOnUnusableNode
(
NodeId
nodeId
,
NodeState
nodeState
)
{
if
(
getInternalState
(
)
==
JobStateInternal
.
RUNNING
&&
!
allReducersComplete
(
)
)
{
List
<
TaskAttemptId
>
taskAttemptIdList
=
nodesToSucceededTaskAttempts
.
get
(
nodeId
)
;
if
(
taskAttemptIdList
!=
null
)
{
String
mesg
=
+
nodeId
;
for
(
TaskAttemptId
id
:
taskAttemptIdList
)
{
if
(
TaskType
.
MAP
==
id
.
getTaskId
(
)
.
getTaskType
(
)
)
{
private
static
ByteBuffer
configureTokens
(
Token
<
JobTokenIdentifier
>
jobToken
,
Credentials
credentials
,
Map
<
String
,
ByteBuffer
>
serviceData
)
throws
IOException
{
private
static
ByteBuffer
configureTokens
(
Token
<
JobTokenIdentifier
>
jobToken
,
Credentials
credentials
,
Map
<
String
,
ByteBuffer
>
serviceData
)
throws
IOException
{
LOG
.
info
(
+
credentials
.
numberOfTokens
(
)
+
+
credentials
.
numberOfSecretKeys
(
)
+
)
;
Credentials
taskCredentials
=
new
Credentials
(
credentials
)
;
TokenCache
.
setJobToken
(
jobToken
,
taskCredentials
)
;
DataOutputBuffer
containerTokens_dob
=
new
DataOutputBuffer
(
)
;
@
SuppressWarnings
(
)
@
Override
public
void
handle
(
TaskAttemptEvent
event
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
SuppressWarnings
(
)
@
Override
public
void
handle
(
TaskAttemptEvent
event
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
event
.
getTaskAttemptID
(
)
+
+
event
.
getType
(
)
)
;
}
writeLock
.
lock
(
)
;
try
{
final
TaskAttemptStateInternal
oldState
=
getInternalState
(
)
;
try
{
stateMachine
.
doTransition
(
event
.
getType
(
)
,
event
)
;
}
catch
(
InvalidStateTransitionException
e
)
{
LOG
.
error
(
+
this
.
attemptId
,
e
)
;
eventHandler
.
handle
(
new
JobDiagnosticsUpdateEvent
(
this
.
attemptId
.
getTaskId
(
)
.
getJobId
(
)
,
+
event
.
getType
(
)
+
+
this
.
attemptId
)
)
;
eventHandler
.
handle
(
new
JobEvent
(
this
.
attemptId
.
getTaskId
(
)
.
getJobId
(
)
,
JobEventType
.
INTERNAL_ERROR
)
)
;
}
if
(
oldState
!=
getInternalState
(
)
)
{
if
(
getInternalState
(
)
==
TaskAttemptStateInternal
.
FAILED
)
{
String
nodeId
=
null
==
this
.
container
?
:
this
.
container
.
getNodeId
(
)
.
toString
(
)
;
LOG
.
debug
(
+
event
.
getTaskAttemptID
(
)
+
+
event
.
getType
(
)
)
;
}
writeLock
.
lock
(
)
;
try
{
final
TaskAttemptStateInternal
oldState
=
getInternalState
(
)
;
try
{
stateMachine
.
doTransition
(
event
.
getType
(
)
,
event
)
;
}
catch
(
InvalidStateTransitionException
e
)
{
LOG
.
error
(
+
this
.
attemptId
,
e
)
;
eventHandler
.
handle
(
new
JobDiagnosticsUpdateEvent
(
this
.
attemptId
.
getTaskId
(
)
.
getJobId
(
)
,
+
event
.
getType
(
)
+
+
this
.
attemptId
)
)
;
eventHandler
.
handle
(
new
JobEvent
(
this
.
attemptId
.
getTaskId
(
)
.
getJobId
(
)
,
JobEventType
.
INTERNAL_ERROR
)
)
;
}
if
(
oldState
!=
getInternalState
(
)
)
{
if
(
getInternalState
(
)
==
TaskAttemptStateInternal
.
FAILED
)
{
String
nodeId
=
null
==
this
.
container
?
:
this
.
container
.
getNodeId
(
)
.
toString
(
)
;
LOG
.
info
(
attemptId
+
+
oldState
+
+
getInternalState
(
)
+
+
event
.
getType
(
)
+
+
nodeId
)
;
}
else
{
reportedStatus
.
id
=
attemptId
;
reportedStatus
.
progress
=
1.0f
;
reportedStatus
.
counters
=
taInfo
.
getCounters
(
)
;
reportedStatus
.
stateString
=
taInfo
.
getState
(
)
;
reportedStatus
.
phase
=
Phase
.
CLEANUP
;
reportedStatus
.
mapFinishTime
=
taInfo
.
getMapFinishTime
(
)
;
reportedStatus
.
shuffleFinishTime
=
taInfo
.
getShuffleFinishTime
(
)
;
reportedStatus
.
sortFinishTime
=
taInfo
.
getSortFinishTime
(
)
;
addDiagnosticInfo
(
taInfo
.
getError
(
)
)
;
boolean
needToClean
=
false
;
String
recoveredState
=
taInfo
.
getTaskStatus
(
)
;
if
(
recoverOutput
&&
TaskAttemptState
.
SUCCEEDED
.
toString
(
)
.
equals
(
recoveredState
)
)
{
TaskAttemptContext
tac
=
new
TaskAttemptContextImpl
(
conf
,
TypeConverter
.
fromYarn
(
attemptId
)
)
;
try
{
committer
.
recoverTask
(
tac
)
;
reportedStatus
.
counters
=
taInfo
.
getCounters
(
)
;
reportedStatus
.
stateString
=
taInfo
.
getState
(
)
;
reportedStatus
.
phase
=
Phase
.
CLEANUP
;
reportedStatus
.
mapFinishTime
=
taInfo
.
getMapFinishTime
(
)
;
reportedStatus
.
shuffleFinishTime
=
taInfo
.
getShuffleFinishTime
(
)
;
reportedStatus
.
sortFinishTime
=
taInfo
.
getSortFinishTime
(
)
;
addDiagnosticInfo
(
taInfo
.
getError
(
)
)
;
boolean
needToClean
=
false
;
String
recoveredState
=
taInfo
.
getTaskStatus
(
)
;
if
(
recoverOutput
&&
TaskAttemptState
.
SUCCEEDED
.
toString
(
)
.
equals
(
recoveredState
)
)
{
TaskAttemptContext
tac
=
new
TaskAttemptContextImpl
(
conf
,
TypeConverter
.
fromYarn
(
attemptId
)
)
;
try
{
committer
.
recoverTask
(
tac
)
;
LOG
.
info
(
+
attemptId
)
;
}
catch
(
Exception
e
)
{
reportedStatus
.
stateString
=
taInfo
.
getState
(
)
;
reportedStatus
.
phase
=
Phase
.
CLEANUP
;
reportedStatus
.
mapFinishTime
=
taInfo
.
getMapFinishTime
(
)
;
reportedStatus
.
shuffleFinishTime
=
taInfo
.
getShuffleFinishTime
(
)
;
reportedStatus
.
sortFinishTime
=
taInfo
.
getSortFinishTime
(
)
;
addDiagnosticInfo
(
taInfo
.
getError
(
)
)
;
boolean
needToClean
=
false
;
String
recoveredState
=
taInfo
.
getTaskStatus
(
)
;
if
(
recoverOutput
&&
TaskAttemptState
.
SUCCEEDED
.
toString
(
)
.
equals
(
recoveredState
)
)
{
TaskAttemptContext
tac
=
new
TaskAttemptContextImpl
(
conf
,
TypeConverter
.
fromYarn
(
attemptId
)
)
;
try
{
committer
.
recoverTask
(
tac
)
;
LOG
.
info
(
+
attemptId
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
attemptId
,
e
)
;
needToClean
=
true
;
}
}
TaskAttemptStateInternal
attemptState
;
if
(
TaskAttemptState
.
SUCCEEDED
.
toString
(
)
.
equals
(
recoveredState
)
)
{
attemptState
=
TaskAttemptStateInternal
.
SUCCEEDED
;
reportedStatus
.
taskState
=
TaskAttemptState
.
SUCCEEDED
;
eventHandler
.
handle
(
createJobCounterUpdateEventTASucceeded
(
this
)
)
;
logAttemptFinishedEvent
(
attemptState
)
;
}
else
if
(
TaskAttemptState
.
FAILED
.
toString
(
)
.
equals
(
recoveredState
)
)
{
attemptState
=
TaskAttemptStateInternal
.
FAILED
;
reportedStatus
.
taskState
=
TaskAttemptState
.
FAILED
;
eventHandler
.
handle
(
createJobCounterUpdateEventTAFailed
(
this
,
false
)
)
;
TaskAttemptUnsuccessfulCompletionEvent
tauce
=
createTaskAttemptUnsuccessfulCompletionEvent
(
this
,
TaskAttemptStateInternal
.
FAILED
)
;
eventHandler
.
handle
(
new
JobHistoryEvent
(
attemptId
.
getTaskId
(
)
.
getJobId
(
)
,
tauce
)
)
;
}
else
{
if
(
!
TaskAttemptState
.
KILLED
.
toString
(
)
.
equals
(
recoveredState
)
)
{
@
SuppressWarnings
(
)
private
void
sendLaunchedEvents
(
)
{
JobCounterUpdateEvent
jce
=
new
JobCounterUpdateEvent
(
attemptId
.
getTaskId
(
)
.
getJobId
(
)
)
;
jce
.
addCounterUpdate
(
attemptId
.
getTaskId
(
)
.
getTaskType
(
)
==
TaskType
.
MAP
?
JobCounter
.
TOTAL_LAUNCHED_MAPS
:
JobCounter
.
TOTAL_LAUNCHED_REDUCES
,
1
)
;
eventHandler
.
handle
(
jce
)
;
@
Override
public
boolean
canCommit
(
TaskAttemptId
taskAttemptID
)
{
readLock
.
lock
(
)
;
boolean
canCommit
=
false
;
try
{
if
(
commitAttempt
!=
null
)
{
canCommit
=
taskAttemptID
.
equals
(
commitAttempt
)
;
private
TaskAttemptImpl
addAttempt
(
Avataar
avataar
)
{
TaskAttemptImpl
attempt
=
createAttempt
(
)
;
attempt
.
setAvataar
(
avataar
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
handle
(
TaskEvent
event
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
void
internalError
(
TaskEventType
type
)
{
private
TaskStateInternal
recover
(
TaskInfo
taskInfo
,
OutputCommitter
committer
,
boolean
recoverTaskOutput
)
{
Set
<
String
>
allNodes
=
new
HashSet
<
String
>
(
)
;
while
(
!
stopped
.
get
(
)
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
try
{
event
=
eventQueue
.
take
(
)
;
}
catch
(
InterruptedException
e
)
{
if
(
!
stopped
.
get
(
)
)
{
LOG
.
error
(
+
e
)
;
}
return
;
}
allNodes
.
add
(
event
.
getContainerMgrAddress
(
)
)
;
int
poolSize
=
launcherPool
.
getCorePoolSize
(
)
;
if
(
poolSize
!=
limitOnPoolSize
)
{
int
numNodes
=
allNodes
.
size
(
)
;
int
idealPoolSize
=
Math
.
min
(
limitOnPoolSize
,
numNodes
)
;
if
(
poolSize
<
idealPoolSize
)
{
int
newPoolSize
=
Math
.
min
(
limitOnPoolSize
,
idealPoolSize
+
initialPoolSize
)
;
@
SuppressWarnings
(
)
void
sendContainerLaunchFailedMsg
(
TaskAttemptId
taskAttemptID
,
String
message
)
{
AllocateRequest
allocateRequest
=
AllocateRequest
.
newInstance
(
this
.
lastResponseID
,
super
.
getApplicationProgress
(
)
,
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
,
null
)
;
AllocateResponse
allocateResponse
=
null
;
try
{
allocateResponse
=
scheduler
.
allocate
(
allocateRequest
)
;
retrystartTime
=
System
.
currentTimeMillis
(
)
;
}
catch
(
ApplicationAttemptNotFoundException
e
)
{
LOG
.
info
(
)
;
eventHandler
.
handle
(
new
JobEvent
(
this
.
getJob
(
)
.
getID
(
)
,
JobEventType
.
JOB_AM_REBOOT
)
)
;
throw
new
YarnRuntimeException
(
+
this
.
getContext
(
)
.
getApplicationID
(
)
,
e
)
;
}
catch
(
ApplicationMasterNotRegisteredException
e
)
{
LOG
.
info
(
+
)
;
this
.
lastResponseID
=
0
;
register
(
)
;
}
catch
(
Exception
e
)
{
if
(
System
.
currentTimeMillis
(
)
-
retrystartTime
>=
retryInterval
)
{
@
SuppressWarnings
(
)
@
Override
public
void
handle
(
ContainerAllocatorEvent
event
)
{
if
(
event
.
getType
(
)
==
ContainerAllocator
.
EventType
.
CONTAINER_REQ
)
{
if
(
clientService
!=
null
)
{
serviceAddr
=
clientService
.
getBindAddress
(
)
;
}
try
{
RegisterApplicationMasterRequest
request
=
recordFactory
.
newRecordInstance
(
RegisterApplicationMasterRequest
.
class
)
;
if
(
serviceAddr
!=
null
)
{
request
.
setHost
(
serviceAddr
.
getHostName
(
)
)
;
request
.
setRpcPort
(
serviceAddr
.
getPort
(
)
)
;
request
.
setTrackingUrl
(
MRWebAppUtil
.
getAMWebappScheme
(
getConfig
(
)
)
+
serviceAddr
.
getHostName
(
)
+
+
clientService
.
getHttpPort
(
)
)
;
}
RegisterApplicationMasterResponse
response
=
scheduler
.
registerApplicationMaster
(
request
)
;
isApplicationMasterRegistered
=
true
;
maxContainerCapability
=
response
.
getMaximumResourceCapability
(
)
;
this
.
context
.
getClusterInfo
(
)
.
setMaxContainerCapability
(
maxContainerCapability
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
setClientToAMToken
(
response
.
getClientToAMTokenMasterKey
(
)
)
;
}
this
.
applicationACLs
=
response
.
getApplicationACLs
(
)
;
}
try
{
RegisterApplicationMasterRequest
request
=
recordFactory
.
newRecordInstance
(
RegisterApplicationMasterRequest
.
class
)
;
if
(
serviceAddr
!=
null
)
{
request
.
setHost
(
serviceAddr
.
getHostName
(
)
)
;
request
.
setRpcPort
(
serviceAddr
.
getPort
(
)
)
;
request
.
setTrackingUrl
(
MRWebAppUtil
.
getAMWebappScheme
(
getConfig
(
)
)
+
serviceAddr
.
getHostName
(
)
+
+
clientService
.
getHttpPort
(
)
)
;
}
RegisterApplicationMasterResponse
response
=
scheduler
.
registerApplicationMaster
(
request
)
;
isApplicationMasterRegistered
=
true
;
maxContainerCapability
=
response
.
getMaximumResourceCapability
(
)
;
this
.
context
.
getClusterInfo
(
)
.
setMaxContainerCapability
(
maxContainerCapability
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
setClientToAMToken
(
response
.
getClientToAMTokenMasterKey
(
)
)
;
}
this
.
applicationACLs
=
response
.
getApplicationACLs
(
)
;
LOG
.
info
(
+
maxContainerCapability
)
;
String
queue
=
response
.
getQueue
(
)
;
public
void
setShouldUnregister
(
boolean
shouldUnregister
)
{
this
.
shouldUnregister
=
shouldUnregister
;
public
void
setSignalled
(
boolean
isSignalled
)
{
this
.
isSignalled
=
isSignalled
;
@
Override
public
void
handle
(
ContainerAllocatorEvent
event
)
{
int
qSize
=
eventQueue
.
size
(
)
;
if
(
qSize
!=
0
&&
qSize
%
1000
==
0
)
{
}
else
{
handleReduceContainerRequest
(
reqEvent
)
;
}
}
else
if
(
event
.
getType
(
)
==
ContainerAllocator
.
EventType
.
CONTAINER_DEALLOCATE
)
{
LOG
.
info
(
+
event
.
toString
(
)
)
;
TaskAttemptId
aId
=
event
.
getAttemptID
(
)
;
boolean
removed
=
scheduledRequests
.
remove
(
aId
)
;
if
(
!
removed
)
{
ContainerId
containerId
=
assignedRequests
.
get
(
aId
)
;
if
(
containerId
!=
null
)
{
removed
=
true
;
assignedRequests
.
remove
(
aId
)
;
containersReleased
++
;
pendingRelease
.
add
(
containerId
)
;
release
(
containerId
)
;
}
}
if
(
!
removed
)
{
@
SuppressWarnings
(
{
}
)
private
void
handleReduceContainerRequest
(
ContainerRequestEvent
reqEvent
)
{
assert
(
reqEvent
.
getAttemptID
(
)
.
getTaskId
(
)
.
getTaskType
(
)
.
equals
(
TaskType
.
REDUCE
)
)
;
Resource
supportedMaxContainerCapability
=
getMaxContainerCapability
(
)
;
JobId
jobId
=
getJob
(
)
.
getID
(
)
;
if
(
reduceResourceRequest
.
equals
(
Resources
.
none
(
)
)
)
{
reduceResourceRequest
=
reqEvent
.
getCapability
(
)
;
eventHandler
.
handle
(
new
JobHistoryEvent
(
jobId
,
new
NormalizedResourceEvent
(
org
.
apache
.
hadoop
.
mapreduce
.
TaskType
.
REDUCE
,
reduceResourceRequest
.
getMemorySize
(
)
)
)
)
;
reduceResourceRequest
=
reqEvent
.
getCapability
(
)
;
eventHandler
.
handle
(
new
JobHistoryEvent
(
jobId
,
new
NormalizedResourceEvent
(
org
.
apache
.
hadoop
.
mapreduce
.
TaskType
.
REDUCE
,
reduceResourceRequest
.
getMemorySize
(
)
)
)
)
;
LOG
.
info
(
+
reduceResourceRequest
)
;
}
boolean
reduceContainerRequestAccepted
=
true
;
if
(
reduceResourceRequest
.
getMemorySize
(
)
>
supportedMaxContainerCapability
.
getMemorySize
(
)
||
reduceResourceRequest
.
getVirtualCores
(
)
>
supportedMaxContainerCapability
.
getVirtualCores
(
)
)
{
reduceContainerRequestAccepted
=
false
;
}
if
(
reduceContainerRequestAccepted
)
{
reqEvent
.
getCapability
(
)
.
setVirtualCores
(
reduceResourceRequest
.
getVirtualCores
(
)
)
;
reqEvent
.
getCapability
(
)
.
setMemorySize
(
reduceResourceRequest
.
getMemorySize
(
)
)
;
if
(
reqEvent
.
getEarlierAttemptFailed
(
)
)
{
pendingReduces
.
addFirst
(
new
ContainerRequest
(
reqEvent
,
PRIORITY_REDUCE
,
reduceNodeLabelExpression
)
)
;
}
else
{
pendingReduces
.
add
(
new
ContainerRequest
(
reqEvent
,
PRIORITY_REDUCE
,
reduceNodeLabelExpression
)
)
;
}
}
else
{
String
diagMsg
=
+
+
+
reduceResourceRequest
+
+
supportedMaxContainerCapability
;
@
SuppressWarnings
(
{
}
)
private
void
handleMapContainerRequest
(
ContainerRequestEvent
reqEvent
)
{
assert
(
reqEvent
.
getAttemptID
(
)
.
getTaskId
(
)
.
getTaskType
(
)
.
equals
(
TaskType
.
MAP
)
)
;
Resource
supportedMaxContainerCapability
=
getMaxContainerCapability
(
)
;
JobId
jobId
=
getJob
(
)
.
getID
(
)
;
if
(
mapResourceRequest
.
equals
(
Resources
.
none
(
)
)
)
{
mapResourceRequest
=
reqEvent
.
getCapability
(
)
;
eventHandler
.
handle
(
new
JobHistoryEvent
(
jobId
,
new
NormalizedResourceEvent
(
org
.
apache
.
hadoop
.
mapreduce
.
TaskType
.
MAP
,
mapResourceRequest
.
getMemorySize
(
)
)
)
)
;
Resource
supportedMaxContainerCapability
=
getMaxContainerCapability
(
)
;
JobId
jobId
=
getJob
(
)
.
getID
(
)
;
if
(
mapResourceRequest
.
equals
(
Resources
.
none
(
)
)
)
{
mapResourceRequest
=
reqEvent
.
getCapability
(
)
;
eventHandler
.
handle
(
new
JobHistoryEvent
(
jobId
,
new
NormalizedResourceEvent
(
org
.
apache
.
hadoop
.
mapreduce
.
TaskType
.
MAP
,
mapResourceRequest
.
getMemorySize
(
)
)
)
)
;
LOG
.
info
(
+
mapResourceRequest
)
;
}
boolean
mapContainerRequestAccepted
=
true
;
if
(
mapResourceRequest
.
getMemorySize
(
)
>
supportedMaxContainerCapability
.
getMemorySize
(
)
||
mapResourceRequest
.
getVirtualCores
(
)
>
supportedMaxContainerCapability
.
getVirtualCores
(
)
)
{
mapContainerRequestAccepted
=
false
;
}
if
(
mapContainerRequestAccepted
)
{
reqEvent
.
getCapability
(
)
.
setMemorySize
(
mapResourceRequest
.
getMemorySize
(
)
)
;
reqEvent
.
getCapability
(
)
.
setVirtualCores
(
mapResourceRequest
.
getVirtualCores
(
)
)
;
scheduledRequests
.
addMap
(
reqEvent
)
;
}
else
{
String
diagMsg
=
+
+
+
mapResourceRequest
+
+
supportedMaxContainerCapability
;
private
void
preemptReducer
(
int
hangingMapRequests
)
{
clearAllPendingReduceRequests
(
)
;
int
preemptionReduceNumForOneMap
=
ResourceCalculatorUtils
.
divideAndCeilContainers
(
mapResourceRequest
,
reduceResourceRequest
,
getSchedulerResourceTypes
(
)
)
;
int
preemptionReduceNumForPreemptionLimit
=
ResourceCalculatorUtils
.
divideAndCeilContainers
(
Resources
.
multiply
(
getResourceLimit
(
)
,
maxReducePreemptionLimit
)
,
reduceResourceRequest
,
getSchedulerResourceTypes
(
)
)
;
int
preemptionReduceNumForAllMaps
=
ResourceCalculatorUtils
.
divideAndCeilContainers
(
Resources
.
multiply
(
mapResourceRequest
,
hangingMapRequests
)
,
reduceResourceRequest
,
getSchedulerResourceTypes
(
)
)
;
int
toPreempt
=
Math
.
min
(
Math
.
max
(
preemptionReduceNumForOneMap
,
preemptionReduceNumForPreemptionLimit
)
,
preemptionReduceNumForAllMaps
)
;
float
completedMapPercent
=
0f
;
if
(
totalMaps
!=
0
)
{
completedMapPercent
=
(
float
)
completedMaps
/
totalMaps
;
}
else
{
completedMapPercent
=
1
;
}
Resource
netScheduledMapResource
=
Resources
.
multiply
(
mapResourceReqt
,
(
scheduledMaps
+
assignedMaps
)
)
;
Resource
netScheduledReduceResource
=
Resources
.
multiply
(
reduceResourceReqt
,
(
scheduledReduces
+
assignedReduces
)
)
;
Resource
finalMapResourceLimit
;
Resource
finalReduceResourceLimit
;
Resource
totalResourceLimit
=
getResourceLimit
(
)
;
Resource
idealReduceResourceLimit
=
Resources
.
multiply
(
totalResourceLimit
,
Math
.
min
(
completedMapPercent
,
maxReduceRampupLimit
)
)
;
Resource
ideaMapResourceLimit
=
Resources
.
subtract
(
totalResourceLimit
,
idealReduceResourceLimit
)
;
if
(
ResourceCalculatorUtils
.
computeAvailableContainers
(
ideaMapResourceLimit
,
mapResourceReqt
,
getSchedulerResourceTypes
(
)
)
>=
(
scheduledMaps
+
assignedMaps
)
)
{
Resource
unusedMapResourceLimit
=
Resources
.
subtract
(
ideaMapResourceLimit
,
netScheduledMapResource
)
;
finalReduceResourceLimit
=
Resources
.
add
(
idealReduceResourceLimit
,
unusedMapResourceLimit
)
;
AllocateResponse
response
;
try
{
response
=
makeRemoteRequest
(
)
;
retrystartTime
=
System
.
currentTimeMillis
(
)
;
}
catch
(
ApplicationAttemptNotFoundException
e
)
{
eventHandler
.
handle
(
new
JobEvent
(
this
.
getJob
(
)
.
getID
(
)
,
JobEventType
.
JOB_AM_REBOOT
)
)
;
throw
new
RMContainerAllocationException
(
+
this
.
getContext
(
)
.
getApplicationAttemptId
(
)
,
e
)
;
}
catch
(
ApplicationMasterNotRegisteredException
e
)
{
LOG
.
info
(
+
)
;
lastResponseID
=
0
;
register
(
)
;
addOutstandingRequestOnResync
(
)
;
return
null
;
}
catch
(
InvalidLabelResourceRequestException
e
)
{
String
diagMsg
=
+
StringUtils
.
stringifyException
(
e
)
;
if
(
System
.
currentTimeMillis
(
)
-
retrystartTime
>=
retryInterval
)
{
LOG
.
error
(
+
retryInterval
+
)
;
eventHandler
.
handle
(
new
JobEvent
(
this
.
getJob
(
)
.
getID
(
)
,
JobEventType
.
JOB_AM_REBOOT
)
)
;
throw
new
RMContainerAllocationException
(
+
retryInterval
+
)
;
}
throw
e
;
}
Resource
newHeadRoom
=
getAvailableResources
(
)
;
List
<
Container
>
newContainers
=
response
.
getAllocatedContainers
(
)
;
if
(
response
.
getNMTokens
(
)
!=
null
)
{
for
(
NMToken
nmToken
:
response
.
getNMTokens
(
)
)
{
NMTokenCache
.
setNMToken
(
nmToken
.
getNodeId
(
)
.
toString
(
)
,
nmToken
.
getToken
(
)
)
;
}
}
if
(
response
.
getAMRMToken
(
)
!=
null
)
{
updateAMRMToken
(
response
.
getAMRMToken
(
)
)
;
}
List
<
ContainerStatus
>
finishedContainers
=
response
.
getCompletedContainersStatuses
(
)
;
final
PreemptionMessage
preemptReq
=
response
.
getPreemptionMessage
(
)
;
if
(
preemptReq
!=
null
)
{
@
SuppressWarnings
(
)
@
VisibleForTesting
void
processFinishedContainer
(
ContainerStatus
container
)
{
@
SuppressWarnings
(
)
@
VisibleForTesting
void
processFinishedContainer
(
ContainerStatus
container
)
{
LOG
.
info
(
+
container
.
getContainerId
(
)
)
;
TaskAttemptId
attemptID
=
assignedRequests
.
get
(
container
.
getContainerId
(
)
)
;
if
(
attemptID
==
null
)
{
@
SuppressWarnings
(
)
private
void
handleUpdatedNodes
(
AllocateResponse
response
)
{
List
<
NodeReport
>
updatedNodes
=
response
.
getUpdatedNodes
(
)
;
if
(
!
updatedNodes
.
isEmpty
(
)
)
{
eventHandler
.
handle
(
new
JobUpdatedNodesEvent
(
getJob
(
)
.
getID
(
)
,
updatedNodes
)
)
;
HashSet
<
NodeId
>
unusableNodes
=
new
HashSet
<
NodeId
>
(
)
;
for
(
NodeReport
nr
:
updatedNodes
)
{
NodeState
nodeState
=
nr
.
getNodeState
(
)
;
if
(
nodeState
.
isUnusable
(
)
)
{
unusableNodes
.
add
(
nr
.
getNodeId
(
)
)
;
}
}
for
(
int
i
=
0
;
i
<
2
;
++
i
)
{
HashMap
<
TaskAttemptId
,
Container
>
taskSet
=
i
==
0
?
assignedRequests
.
maps
:
assignedRequests
.
reduces
;
for
(
Map
.
Entry
<
TaskAttemptId
,
Container
>
entry
:
taskSet
.
entrySet
(
)
)
{
TaskAttemptId
tid
=
entry
.
getKey
(
)
;
NodeId
taskAttemptNodeId
=
entry
.
getValue
(
)
.
getNodeId
(
)
;
if
(
unusableNodes
.
contains
(
taskAttemptNodeId
)
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
super
.
serviceInit
(
conf
)
;
nodeBlacklistingEnabled
=
conf
.
getBoolean
(
MRJobConfig
.
MR_AM_JOB_NODE_BLACKLISTING_ENABLE
,
true
)
;
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
super
.
serviceInit
(
conf
)
;
nodeBlacklistingEnabled
=
conf
.
getBoolean
(
MRJobConfig
.
MR_AM_JOB_NODE_BLACKLISTING_ENABLE
,
true
)
;
LOG
.
info
(
+
nodeBlacklistingEnabled
)
;
maxTaskFailuresPerNode
=
conf
.
getInt
(
MRJobConfig
.
MAX_TASK_FAILURES_PER_TRACKER
,
3
)
;
blacklistDisablePercent
=
conf
.
getInt
(
MRJobConfig
.
MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERECENT
,
MRJobConfig
.
DEFAULT_MR_AM_IGNORE_BLACKLISTING_BLACKLISTED_NODE_PERCENT
)
;
protected
AllocateResponse
makeRemoteRequest
(
)
throws
YarnException
,
IOException
{
applyRequestLimits
(
)
;
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
String
>
(
blacklistAdditions
)
,
new
ArrayList
<
String
>
(
blacklistRemovals
)
)
;
AllocateRequest
allocateRequest
=
AllocateRequest
.
newInstance
(
lastResponseID
,
super
.
getApplicationProgress
(
)
,
new
ArrayList
<
ResourceRequest
>
(
ask
)
,
new
ArrayList
<
ContainerId
>
(
release
)
,
blacklistRequest
)
;
AllocateResponse
allocateResponse
=
scheduler
.
allocate
(
allocateRequest
)
;
lastResponseID
=
allocateResponse
.
getResponseId
(
)
;
availableResources
=
allocateResponse
.
getAvailableResources
(
)
;
lastClusterNmCount
=
clusterNmCount
;
clusterNmCount
=
allocateResponse
.
getNumClusterNodes
(
)
;
int
numCompletedContainers
=
allocateResponse
.
getCompletedContainersStatuses
(
)
.
size
(
)
;
if
(
ask
.
size
(
)
>
0
||
release
.
size
(
)
>
0
)
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
String
>
(
blacklistAdditions
)
,
new
ArrayList
<
String
>
(
blacklistRemovals
)
)
;
AllocateRequest
allocateRequest
=
AllocateRequest
.
newInstance
(
lastResponseID
,
super
.
getApplicationProgress
(
)
,
new
ArrayList
<
ResourceRequest
>
(
ask
)
,
new
ArrayList
<
ContainerId
>
(
release
)
,
blacklistRequest
)
;
AllocateResponse
allocateResponse
=
scheduler
.
allocate
(
allocateRequest
)
;
lastResponseID
=
allocateResponse
.
getResponseId
(
)
;
availableResources
=
allocateResponse
.
getAvailableResources
(
)
;
lastClusterNmCount
=
clusterNmCount
;
clusterNmCount
=
allocateResponse
.
getNumClusterNodes
(
)
;
int
numCompletedContainers
=
allocateResponse
.
getCompletedContainersStatuses
(
)
.
size
(
)
;
if
(
ask
.
size
(
)
>
0
||
release
.
size
(
)
>
0
)
{
LOG
.
info
(
+
applicationId
+
+
+
ask
.
size
(
)
+
+
release
.
size
(
)
+
+
allocateResponse
.
getAllocatedContainers
(
)
.
size
(
)
+
+
numCompletedContainers
+
+
availableResources
+
+
clusterNmCount
)
;
}
ask
.
clear
(
)
;
release
.
clear
(
)
;
if
(
numCompletedContainers
>
0
)
{
requestLimitsToUpdate
.
addAll
(
requestLimits
.
keySet
(
)
)
;
}
if
(
blacklistAdditions
.
size
(
)
>
0
||
blacklistRemovals
.
size
(
)
>
0
)
{
if
(
!
nodeBlacklistingEnabled
)
{
return
;
}
if
(
blacklistDisablePercent
!=
-
1
&&
(
blacklistedNodeCount
!=
blacklistedNodes
.
size
(
)
||
clusterNmCount
!=
lastClusterNmCount
)
)
{
blacklistedNodeCount
=
blacklistedNodes
.
size
(
)
;
if
(
clusterNmCount
==
0
)
{
LOG
.
info
(
)
;
return
;
}
int
val
=
(
int
)
(
(
float
)
blacklistedNodes
.
size
(
)
/
clusterNmCount
*
100
)
;
if
(
val
>=
blacklistDisablePercent
)
{
if
(
ignoreBlacklisting
.
compareAndSet
(
false
,
true
)
)
{
LOG
.
info
(
+
clusterNmCount
+
+
blacklistedNodeCount
+
+
val
+
)
;
blacklistAdditions
.
clear
(
)
;
blacklistRemovals
.
addAll
(
blacklistedNodes
)
;
}
}
else
{
if
(
ignoreBlacklisting
.
compareAndSet
(
true
,
false
)
)
{
private
void
addResourceRequest
(
Priority
priority
,
String
resourceName
,
Resource
capability
,
String
nodeLabelExpression
,
ExecutionType
executionType
)
{
Map
<
String
,
Map
<
Resource
,
ResourceRequest
>>
remoteRequests
=
this
.
remoteRequestsTable
.
get
(
priority
)
;
if
(
remoteRequests
==
null
)
{
remoteRequests
=
new
HashMap
<
String
,
Map
<
Resource
,
ResourceRequest
>>
(
)
;
this
.
remoteRequestsTable
.
put
(
priority
,
remoteRequests
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
Map
<
Resource
,
ResourceRequest
>
reqMap
=
remoteRequests
.
get
(
resourceName
)
;
if
(
reqMap
==
null
)
{
reqMap
=
new
HashMap
<
Resource
,
ResourceRequest
>
(
)
;
remoteRequests
.
put
(
resourceName
,
reqMap
)
;
}
ResourceRequest
remoteRequest
=
reqMap
.
get
(
capability
)
;
if
(
remoteRequest
==
null
)
{
remoteRequest
=
recordFactory
.
newRecordInstance
(
ResourceRequest
.
class
)
;
remoteRequest
.
setPriority
(
priority
)
;
remoteRequest
.
setResourceName
(
resourceName
)
;
remoteRequest
.
setCapability
(
capability
)
;
remoteRequest
.
setNumContainers
(
0
)
;
remoteRequest
.
setNodeLabelExpression
(
nodeLabelExpression
)
;
remoteRequest
.
setExecutionTypeRequest
(
ExecutionTypeRequest
.
newInstance
(
executionType
,
true
)
)
;
reqMap
.
put
(
capability
,
remoteRequest
)
;
}
remoteRequest
.
setNumContainers
(
remoteRequest
.
getNumContainers
(
)
+
1
)
;
private
void
decResourceRequest
(
Priority
priority
,
String
resourceName
,
Resource
capability
)
{
Map
<
String
,
Map
<
Resource
,
ResourceRequest
>>
remoteRequests
=
this
.
remoteRequestsTable
.
get
(
priority
)
;
Map
<
Resource
,
ResourceRequest
>
reqMap
=
remoteRequests
.
get
(
resourceName
)
;
if
(
reqMap
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
resourceName
+
)
;
}
return
;
}
ResourceRequest
remoteRequest
=
reqMap
.
get
(
capability
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
applicationId
.
getId
(
)
+
+
priority
.
getPriority
(
)
+
+
resourceName
+
+
remoteRequest
.
getNumContainers
(
)
+
+
ask
.
size
(
)
)
;
}
if
(
remoteRequest
.
getNumContainers
(
)
>
0
)
{
remoteRequest
.
setNumContainers
(
remoteRequest
.
getNumContainers
(
)
-
1
)
;
}
if
(
remoteRequest
.
getNumContainers
(
)
==
0
)
{
reqMap
.
remove
(
capability
)
;
if
(
reqMap
.
size
(
)
==
0
)
{
remoteRequests
.
remove
(
resourceName
)
;
}
if
(
remoteRequests
.
size
(
)
==
0
)
{
remoteRequestsTable
.
remove
(
priority
)
;
}
}
addResourceRequestToAsk
(
remoteRequest
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
preempt
(
Context
ctxt
,
PreemptionMessage
preemptionRequests
)
{
if
(
preemptionRequests
!=
null
)
{
StrictPreemptionContract
cStrict
=
preemptionRequests
.
getStrictContract
(
)
;
if
(
cStrict
!=
null
&&
cStrict
.
getContainers
(
)
!=
null
&&
cStrict
.
getContainers
(
)
.
size
(
)
>
0
)
{
LOG
.
info
(
+
preemptionRequests
.
getStrictContract
(
)
.
getContainers
(
)
.
size
(
)
+
)
;
for
(
PreemptionContainer
c
:
preemptionRequests
.
getStrictContract
(
)
.
getContainers
(
)
)
{
ContainerId
reqCont
=
c
.
getId
(
)
;
TaskAttemptId
reqTask
=
ctxt
.
getTaskAttempt
(
reqCont
)
;
if
(
reqTask
!=
null
)
{
if
(
org
.
apache
.
hadoop
.
mapreduce
.
v2
.
api
.
records
.
TaskType
.
REDUCE
.
equals
(
reqTask
.
getTaskId
(
)
.
getTaskType
(
)
)
)
{
toBePreempted
.
add
(
reqTask
)
;
else
{
LOG
.
info
(
+
reqCont
+
+
reqTask
)
;
}
}
}
}
PreemptionContract
cNegot
=
preemptionRequests
.
getContract
(
)
;
if
(
cNegot
!=
null
&&
cNegot
.
getResourceRequest
(
)
!=
null
&&
cNegot
.
getResourceRequest
(
)
.
size
(
)
>
0
&&
cNegot
.
getContainers
(
)
!=
null
&&
cNegot
.
getContainers
(
)
.
size
(
)
>
0
)
{
LOG
.
info
(
+
preemptionRequests
.
getContract
(
)
.
getResourceRequest
(
)
.
size
(
)
+
+
preemptionRequests
.
getContract
(
)
.
getContainers
(
)
.
size
(
)
+
)
;
List
<
PreemptionResourceRequest
>
reqResources
=
preemptionRequests
.
getContract
(
)
.
getResourceRequest
(
)
;
int
pendingPreemptionRam
=
0
;
int
pendingPreemptionCores
=
0
;
for
(
Resource
r
:
pendingFlexiblePreemptions
.
values
(
)
)
{
pendingPreemptionRam
+=
r
.
getMemorySize
(
)
;
pendingPreemptionCores
+=
r
.
getVirtualCores
(
)
;
}
for
(
PreemptionResourceRequest
rr
:
reqResources
)
{
ResourceRequest
reqRsrc
=
rr
.
getResourceRequest
(
)
;
if
(
!
ResourceRequest
.
ANY
.
equals
(
reqRsrc
.
getResourceName
(
)
)
)
{
continue
;
totalMemoryToRelease
-=
pendingPreemptionRam
;
pendingPreemptionRam
-=
totalMemoryToRelease
;
}
if
(
pendingPreemptionCores
>
0
)
{
totalCoresToRelease
-=
pendingPreemptionCores
;
pendingPreemptionCores
-=
totalCoresToRelease
;
}
List
<
Container
>
listOfCont
=
ctxt
.
getContainers
(
TaskType
.
REDUCE
)
;
Collections
.
sort
(
listOfCont
,
new
Comparator
<
Container
>
(
)
{
@
Override
public
int
compare
(
final
Container
o1
,
final
Container
o2
)
{
return
o2
.
getId
(
)
.
compareTo
(
o1
.
getId
(
)
)
;
}
}
)
;
for
(
Container
cont
:
listOfCont
)
{
if
(
totalMemoryToRelease
<=
0
&&
totalCoresToRelease
<=
0
)
{
break
;
}
TaskAttemptId
reduceId
=
ctxt
.
getTaskAttempt
(
cont
.
getId
(
)
)
;
int
cMem
=
(
int
)
cont
.
getResource
(
)
.
getMemorySize
(
)
;
@
Override
public
void
handleCompletedContainer
(
TaskAttemptId
attemptID
)
{
@
SuppressWarnings
(
)
private
void
killContainer
(
Context
ctxt
,
PreemptionContainer
c
)
{
ContainerId
reqCont
=
c
.
getId
(
)
;
TaskAttemptId
reqTask
=
ctxt
.
getTaskAttempt
(
reqCont
)
;
try
{
Class
<
?
extends
TaskRuntimeEstimator
>
estimatorClass
=
conf
.
getClass
(
MRJobConfig
.
MR_AM_TASK_ESTIMATOR
,
LegacyTaskRuntimeEstimator
.
class
,
TaskRuntimeEstimator
.
class
)
;
Constructor
<
?
extends
TaskRuntimeEstimator
>
estimatorConstructor
=
estimatorClass
.
getConstructor
(
)
;
estimator
=
estimatorConstructor
.
newInstance
(
)
;
estimator
.
contextualize
(
conf
,
context
)
;
}
catch
(
InstantiationException
ex
)
{
LOG
.
error
(
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
IllegalAccessException
ex
)
{
LOG
.
error
(
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
InvocationTargetException
ex
)
{
LOG
.
error
(
,
ex
)
;
throw
new
YarnRuntimeException
(
ex
)
;
}
catch
(
NoSuchMethodException
ex
)
{
@
Override
protected
void
serviceStart
(
)
throws
Exception
{
Runnable
speculationBackgroundCore
=
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
while
(
!
stopped
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
long
backgroundRunStartTime
=
clock
.
getTime
(
)
;
try
{
int
speculations
=
computeSpeculations
(
)
;
long
mininumRecomp
=
speculations
>
0
?
soonestRetryAfterSpeculate
:
soonestRetryAfterNoSpeculate
;
long
wait
=
Math
.
max
(
mininumRecomp
,
clock
.
getTime
(
)
-
backgroundRunStartTime
)
;
if
(
speculations
>
0
)
{
requireJob
(
)
;
}
catch
(
Exception
e
)
{
renderText
(
e
.
getMessage
(
)
)
;
return
;
}
if
(
app
.
getJob
(
)
!=
null
)
{
try
{
String
taskType
=
$
(
TASK_TYPE
)
;
if
(
taskType
.
isEmpty
(
)
)
{
throw
new
RuntimeException
(
)
;
}
String
attemptState
=
$
(
ATTEMPT_STATE
)
;
if
(
attemptState
.
isEmpty
(
)
)
{
throw
new
RuntimeException
(
)
;
}
setTitle
(
join
(
attemptState
,
,
MRApps
.
taskType
(
taskType
)
.
toString
(
)
,
,
$
(
JOB_ID
)
)
)
;
render
(
attemptsPage
(
)
)
;
}
catch
(
Exception
e
)
{
Path
confPath
=
job
.
getConfFile
(
)
;
try
{
ConfInfo
info
=
new
ConfInfo
(
job
)
;
html
.
div
(
)
.
a
(
+
jid
,
confPath
.
toString
(
)
)
.
__
(
)
;
TBODY
<
TABLE
<
Hamlet
>>
tbody
=
html
.
table
(
)
.
thead
(
)
.
tr
(
)
.
th
(
_TH
,
)
.
th
(
_TH
,
)
.
th
(
_TH
,
)
.
__
(
)
.
__
(
)
.
tbody
(
)
;
for
(
ConfEntryInfo
entry
:
info
.
getProperties
(
)
)
{
StringBuffer
buffer
=
new
StringBuffer
(
)
;
String
[
]
sources
=
entry
.
getSource
(
)
;
boolean
first
=
true
;
for
(
int
i
=
(
sources
.
length
-
2
)
;
i
>=
0
;
i
--
)
{
if
(
!
first
)
{
buffer
.
append
(
)
;
}
first
=
false
;
buffer
.
append
(
sources
[
i
]
)
;
}
tbody
.
tr
(
)
.
td
(
entry
.
getName
(
)
)
.
td
(
entry
.
getValue
(
)
)
.
td
(
buffer
.
toString
(
)
)
.
__
(
)
;
@
SuppressWarnings
(
)
@
Test
(
timeout
=
10000
)
public
void
testKillJob
(
)
throws
Exception
{
JobConf
conf
=
new
JobConf
(
)
;
AppContext
context
=
mock
(
AppContext
.
class
)
;
final
CountDownLatch
isDone
=
new
CountDownLatch
(
1
)
;
EventHandler
<
Event
>
handler
=
new
EventHandler
<
Event
>
(
)
{
@
Override
public
void
handle
(
Event
event
)
{
@
Test
public
void
testEscapeJobSummary
(
)
{
summary
.
setJobName
(
)
;
String
out
=
summary
.
getJobSummaryString
(
)
;
public
void
verifyCompleted
(
)
{
for
(
Job
job
:
getContext
(
)
.
getAllJobs
(
)
.
values
(
)
)
{
JobReport
jobReport
=
job
.
getReport
(
)
;
public
void
verifyCompleted
(
)
{
for
(
Job
job
:
getContext
(
)
.
getAllJobs
(
)
.
values
(
)
)
{
JobReport
jobReport
=
job
.
getReport
(
)
;
LOG
.
info
(
,
jobReport
.
getStartTime
(
)
)
;
public
void
verifyCompleted
(
)
{
for
(
Job
job
:
getContext
(
)
.
getAllJobs
(
)
.
values
(
)
)
{
JobReport
jobReport
=
job
.
getReport
(
)
;
LOG
.
info
(
,
jobReport
.
getStartTime
(
)
)
;
LOG
.
info
(
,
jobReport
.
getFinishTime
(
)
)
;
Assert
.
assertTrue
(
,
jobReport
.
getStartTime
(
)
<=
jobReport
.
getFinishTime
(
)
)
;
Assert
.
assertTrue
(
,
jobReport
.
getFinishTime
(
)
<=
System
.
currentTimeMillis
(
)
)
;
for
(
Task
task
:
job
.
getTasks
(
)
.
values
(
)
)
{
TaskReport
taskReport
=
task
.
getReport
(
)
;
public
void
verifyCompleted
(
)
{
for
(
Job
job
:
getContext
(
)
.
getAllJobs
(
)
.
values
(
)
)
{
JobReport
jobReport
=
job
.
getReport
(
)
;
LOG
.
info
(
,
jobReport
.
getStartTime
(
)
)
;
LOG
.
info
(
,
jobReport
.
getFinishTime
(
)
)
;
Assert
.
assertTrue
(
,
jobReport
.
getStartTime
(
)
<=
jobReport
.
getFinishTime
(
)
)
;
Assert
.
assertTrue
(
,
jobReport
.
getFinishTime
(
)
<=
System
.
currentTimeMillis
(
)
)
;
for
(
Task
task
:
job
.
getTasks
(
)
.
values
(
)
)
{
TaskReport
taskReport
=
task
.
getReport
(
)
;
LOG
.
info
(
,
task
.
getID
(
)
,
taskReport
.
getStartTime
(
)
)
;
String
userName
=
;
JobConf
conf
=
new
JobConf
(
)
;
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
stagingDir
)
;
conf
.
setInt
(
org
.
apache
.
hadoop
.
mapreduce
.
lib
.
output
.
FileOutputCommitter
.
FILEOUTPUTCOMMITTER_ALGORITHM_VERSION
,
1
)
;
ApplicationAttemptId
applicationAttemptId
=
ApplicationAttemptId
.
fromString
(
applicationAttemptIdStr
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
TypeConverter
.
fromYarn
(
applicationAttemptId
.
getApplicationId
(
)
)
)
;
Path
start
=
MRApps
.
getStartJobCommitFile
(
conf
,
userName
,
jobId
)
;
FileSystem
fs
=
FileSystem
.
get
(
conf
)
;
fs
.
create
(
start
)
.
close
(
)
;
ContainerId
containerId
=
ContainerId
.
fromString
(
containerIdStr
)
;
MRAppMaster
appMaster
=
new
MRAppMasterTest
(
applicationAttemptId
,
containerId
,
,
-
1
,
-
1
,
System
.
currentTimeMillis
(
)
,
false
,
false
)
;
boolean
caught
=
false
;
try
{
MRAppMaster
.
initAndStartAppMaster
(
appMaster
,
conf
,
userName
)
;
}
catch
(
IOException
e
)
{
JobConf
conf
=
new
JobConf
(
)
;
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
stagingDir
)
;
ApplicationAttemptId
applicationAttemptId
=
ApplicationAttemptId
.
fromString
(
applicationAttemptIdStr
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
TypeConverter
.
fromYarn
(
applicationAttemptId
.
getApplicationId
(
)
)
)
;
Path
start
=
MRApps
.
getStartJobCommitFile
(
conf
,
userName
,
jobId
)
;
Path
end
=
MRApps
.
getEndJobCommitSuccessFile
(
conf
,
userName
,
jobId
)
;
FileSystem
fs
=
FileSystem
.
get
(
conf
)
;
fs
.
create
(
start
)
.
close
(
)
;
fs
.
create
(
end
)
.
close
(
)
;
ContainerId
containerId
=
ContainerId
.
fromString
(
containerIdStr
)
;
MRAppMaster
appMaster
=
new
MRAppMasterTest
(
applicationAttemptId
,
containerId
,
,
-
1
,
-
1
,
System
.
currentTimeMillis
(
)
,
false
,
false
)
;
boolean
caught
=
false
;
try
{
MRAppMaster
.
initAndStartAppMaster
(
appMaster
,
conf
,
userName
)
;
}
catch
(
IOException
e
)
{
JobConf
conf
=
new
JobConf
(
)
;
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
stagingDir
)
;
ApplicationAttemptId
applicationAttemptId
=
ApplicationAttemptId
.
fromString
(
applicationAttemptIdStr
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
TypeConverter
.
fromYarn
(
applicationAttemptId
.
getApplicationId
(
)
)
)
;
Path
start
=
MRApps
.
getStartJobCommitFile
(
conf
,
userName
,
jobId
)
;
Path
end
=
MRApps
.
getEndJobCommitFailureFile
(
conf
,
userName
,
jobId
)
;
FileSystem
fs
=
FileSystem
.
get
(
conf
)
;
fs
.
create
(
start
)
.
close
(
)
;
fs
.
create
(
end
)
.
close
(
)
;
ContainerId
containerId
=
ContainerId
.
fromString
(
containerIdStr
)
;
MRAppMaster
appMaster
=
new
MRAppMasterTest
(
applicationAttemptId
,
containerId
,
,
-
1
,
-
1
,
System
.
currentTimeMillis
(
)
,
false
,
false
)
;
boolean
caught
=
false
;
try
{
MRAppMaster
.
initAndStartAppMaster
(
appMaster
,
conf
,
userName
)
;
}
catch
(
IOException
e
)
{
String
applicationAttemptIdStr
=
;
String
containerIdStr
=
;
String
userName
=
;
JobConf
conf
=
new
JobConf
(
)
;
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
stagingDir
)
;
ApplicationAttemptId
applicationAttemptId
=
ApplicationAttemptId
.
fromString
(
applicationAttemptIdStr
)
;
File
dir
=
new
File
(
stagingDir
)
;
if
(
dir
.
exists
(
)
)
{
FileUtils
.
deleteDirectory
(
dir
)
;
}
ContainerId
containerId
=
ContainerId
.
fromString
(
containerIdStr
)
;
MRAppMaster
appMaster
=
new
MRAppMasterTest
(
applicationAttemptId
,
containerId
,
,
-
1
,
-
1
,
System
.
currentTimeMillis
(
)
,
false
,
false
)
;
boolean
caught
=
false
;
try
{
MRAppMaster
.
initAndStartAppMaster
(
appMaster
,
conf
,
userName
)
;
}
catch
(
IOException
e
)
{
Task
mapTask2
=
it
.
next
(
)
;
Task
reduceTask
=
it
.
next
(
)
;
app
.
waitForState
(
mapTask1
,
TaskState
.
RUNNING
)
;
app
.
waitForState
(
mapTask2
,
TaskState
.
RUNNING
)
;
app
.
getContext
(
)
.
getEventHandler
(
)
.
handle
(
new
TaskEvent
(
mapTask1
.
getID
(
)
,
TaskEventType
.
T_ADD_SPEC_ATTEMPT
)
)
;
int
timeOut
=
0
;
while
(
mapTask1
.
getAttempts
(
)
.
size
(
)
!=
2
&&
timeOut
++
<
10
)
{
Thread
.
sleep
(
1000
)
;
LOG
.
info
(
)
;
}
Iterator
<
TaskAttempt
>
t1it
=
mapTask1
.
getAttempts
(
)
.
values
(
)
.
iterator
(
)
;
TaskAttempt
task1Attempt1
=
t1it
.
next
(
)
;
TaskAttempt
task1Attempt2
=
t1it
.
next
(
)
;
TaskAttempt
task2Attempt
=
mapTask2
.
getAttempts
(
)
.
values
(
)
.
iterator
(
)
.
next
(
)
;
waitForContainerAssignment
(
task1Attempt2
)
;
ContainerId
t1a2contId
=
task1Attempt2
.
getAssignedContainerID
(
)
;
assertThat
(
containerLauncher
.
initialPoolSize
)
.
isEqualTo
(
MRJobConfig
.
DEFAULT_MR_AM_CONTAINERLAUNCHER_THREADPOOL_INITIAL_SIZE
)
;
Assert
.
assertEquals
(
0
,
threadPool
.
getPoolSize
(
)
)
;
Assert
.
assertEquals
(
containerLauncher
.
initialPoolSize
,
threadPool
.
getCorePoolSize
(
)
)
;
Assert
.
assertNull
(
containerLauncher
.
foundErrors
)
;
containerLauncher
.
expectedCorePoolSize
=
containerLauncher
.
initialPoolSize
;
for
(
int
i
=
0
;
i
<
10
;
i
++
)
{
ContainerId
containerId
=
ContainerId
.
newContainerId
(
appAttemptId
,
i
)
;
TaskAttemptId
taskAttemptId
=
MRBuilderUtils
.
newTaskAttemptId
(
taskId
,
i
)
;
containerLauncher
.
handle
(
new
ContainerLauncherEvent
(
taskAttemptId
,
containerId
,
+
i
+
,
null
,
ContainerLauncher
.
EventType
.
CONTAINER_REMOTE_LAUNCH
)
)
;
}
waitForEvents
(
containerLauncher
,
10
)
;
Assert
.
assertEquals
(
10
,
threadPool
.
getPoolSize
(
)
)
;
Assert
.
assertNull
(
containerLauncher
.
foundErrors
)
;
containerLauncher
.
finishEventHandling
=
true
;
int
timeOut
=
0
;
while
(
containerLauncher
.
numEventsProcessed
.
get
(
)
<
10
&&
timeOut
++
<
200
)
{
private
void
waitForEvents
(
CustomContainerLauncher
containerLauncher
,
int
expectedNumEvents
)
throws
InterruptedException
{
int
timeOut
=
0
;
while
(
containerLauncher
.
numEventsProcessing
.
get
(
)
<
expectedNumEvents
&&
timeOut
++
<
20
)
{
server
.
start
(
)
;
MRApp
app
=
new
MRAppWithSlowNM
(
tokenSecretManager
)
;
try
{
Job
job
=
app
.
submit
(
conf
)
;
app
.
waitForState
(
job
,
JobState
.
RUNNING
)
;
Map
<
TaskId
,
Task
>
tasks
=
job
.
getTasks
(
)
;
Assert
.
assertEquals
(
,
1
,
tasks
.
size
(
)
)
;
Task
task
=
tasks
.
values
(
)
.
iterator
(
)
.
next
(
)
;
app
.
waitForState
(
task
,
TaskState
.
SCHEDULED
)
;
Map
<
TaskAttemptId
,
TaskAttempt
>
attempts
=
tasks
.
values
(
)
.
iterator
(
)
.
next
(
)
.
getAttempts
(
)
;
Assert
.
assertEquals
(
,
maxAttempts
,
attempts
.
size
(
)
)
;
TaskAttempt
attempt
=
attempts
.
values
(
)
.
iterator
(
)
.
next
(
)
;
app
.
waitForInternalState
(
(
TaskAttemptImpl
)
attempt
,
TaskAttemptStateInternal
.
ASSIGNED
)
;
app
.
waitForState
(
job
,
JobState
.
FAILED
)
;
String
diagnostics
=
attempt
.
getDiagnostics
(
)
.
toString
(
)
;
assertBlacklistAdditionsAndRemovals
(
0
,
0
,
rm
)
;
Assert
.
assertEquals
(
,
0
,
assigned
.
size
(
)
)
;
LOG
.
info
(
)
;
assigned
=
allocator
.
schedule
(
)
;
rm
.
drainEvents
(
)
;
assertBlacklistAdditionsAndRemovals
(
0
,
0
,
rm
)
;
Assert
.
assertEquals
(
,
0
,
assigned
.
size
(
)
)
;
LOG
.
info
(
)
;
nodeManager3
.
nodeHeartbeat
(
true
)
;
rm
.
drainEvents
(
)
;
LOG
.
info
(
)
;
assigned
=
allocator
.
schedule
(
)
;
assertBlacklistAdditionsAndRemovals
(
0
,
0
,
rm
)
;
rm
.
drainEvents
(
)
;
for
(
TaskAttemptContainerAssignedEvent
assig
:
assigned
)
{
}
catch
(
InterruptedException
e
)
{
throw
new
IOException
(
e
)
;
}
catch
(
ExecutionException
e
)
{
throw
new
IOException
(
e
)
;
}
String
pathString
=
path
.
toUri
(
)
.
toString
(
)
;
String
link
=
entry
.
getKey
(
)
;
String
target
=
new
File
(
path
.
toUri
(
)
)
.
getPath
(
)
;
symlink
(
workDir
,
target
,
link
)
;
if
(
resource
.
getType
(
)
==
LocalResourceType
.
ARCHIVE
)
{
localArchives
.
add
(
pathString
)
;
}
else
if
(
resource
.
getType
(
)
==
LocalResourceType
.
FILE
)
{
localFiles
.
add
(
pathString
)
;
}
else
if
(
resource
.
getType
(
)
==
LocalResourceType
.
PATTERN
)
{
throw
new
IllegalArgumentException
(
+
+
resource
.
getResource
(
)
)
;
}
Path
resourcePath
;
private
void
symlink
(
File
workDir
,
String
target
,
String
link
)
throws
IOException
{
if
(
link
!=
null
)
{
link
=
workDir
.
toString
(
)
+
Path
.
SEPARATOR
+
link
;
File
flink
=
new
File
(
link
)
;
if
(
!
flink
.
exists
(
)
)
{
protected
MRClientProtocol
instantiateHistoryProxy
(
final
Configuration
conf
,
final
InetSocketAddress
hsAddress
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
setClassLoader
(
ClassLoader
classLoader
,
Configuration
conf
)
{
if
(
classLoader
!=
null
)
{
static
String
readOutput
(
Path
outDir
,
Configuration
conf
)
throws
IOException
{
FileSystem
fs
=
outDir
.
getFileSystem
(
conf
)
;
StringBuffer
result
=
new
StringBuffer
(
)
;
Path
[
]
fileList
=
FileUtil
.
stat2Paths
(
fs
.
listStatus
(
outDir
,
new
Utils
.
OutputFileUtils
.
OutputFilesFilter
(
)
)
)
;
for
(
Path
outputFile
:
fileList
)
{
assert
(
readSegmentIndex
!=
0
)
;
assert
(
currentKVOffset
!=
0
)
;
readSegmentIndex
--
;
}
int
i
=
0
;
Iterator
<
Segment
<
K
,
V
>>
itr
=
segmentList
.
iterator
(
)
;
while
(
itr
.
hasNext
(
)
)
{
Segment
<
K
,
V
>
s
=
itr
.
next
(
)
;
if
(
i
==
readSegmentIndex
)
{
break
;
}
s
.
close
(
)
;
itr
.
remove
(
)
;
i
++
;
LOG
.
debug
(
)
;
}
firstSegmentOffset
=
currentKVOffset
;
readSegmentIndex
=
0
;
}
}
inReset
=
true
;
for
(
int
i
=
0
;
i
<
segmentList
.
size
(
)
;
i
++
)
{
Segment
<
K
,
V
>
s
=
segmentList
.
get
(
i
)
;
if
(
s
.
inMemory
(
)
)
{
int
offset
=
(
i
==
0
)
?
firstSegmentOffset
:
0
;
s
.
getReader
(
)
.
reset
(
offset
)
;
}
else
{
s
.
closeReader
(
)
;
if
(
i
==
0
)
{
s
.
reinitReader
(
firstSegmentOffset
)
;
s
.
getReader
(
)
.
disableChecksumValidation
(
)
;
}
}
}
currentKVOffset
=
firstSegmentOffset
;
nextKVOffset
=
-
1
;
readSegmentIndex
=
0
;
hasMore
=
false
;
protected
static
boolean
deletePath
(
PathDeletionContext
context
)
throws
IOException
{
context
.
enablePathForCleanup
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
log
(
Logger
log
)
{
log
.
info
(
+
size
(
)
)
;
for
(
Group
group
:
this
)
{
public
void
log
(
Logger
log
)
{
log
.
info
(
+
size
(
)
)
;
for
(
Group
group
:
this
)
{
log
.
info
(
+
group
.
getDisplayName
(
)
)
;
for
(
Counter
counter
:
group
)
{
int
numThreads
=
job
.
getInt
(
org
.
apache
.
hadoop
.
mapreduce
.
lib
.
input
.
FileInputFormat
.
LIST_STATUS_NUM_THREADS
,
org
.
apache
.
hadoop
.
mapreduce
.
lib
.
input
.
FileInputFormat
.
DEFAULT_LIST_STATUS_NUM_THREADS
)
;
StopWatch
sw
=
new
StopWatch
(
)
.
start
(
)
;
if
(
numThreads
==
1
)
{
List
<
FileStatus
>
locatedFiles
=
singleThreadedListStatus
(
job
,
dirs
,
inputFilter
,
recursive
)
;
result
=
locatedFiles
.
toArray
(
new
FileStatus
[
locatedFiles
.
size
(
)
]
)
;
}
else
{
Iterable
<
FileStatus
>
locatedFiles
=
null
;
try
{
LocatedFileStatusFetcher
locatedFileStatusFetcher
=
new
LocatedFileStatusFetcher
(
job
,
dirs
,
recursive
,
inputFilter
,
false
)
;
locatedFiles
=
locatedFileStatusFetcher
.
getFileStatuses
(
)
;
}
catch
(
InterruptedException
e
)
{
throw
(
IOException
)
new
InterruptedIOException
(
)
.
initCause
(
e
)
;
}
result
=
Iterables
.
toArray
(
locatedFiles
,
FileStatus
.
class
)
;
}
sw
.
stop
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
BlockLocation
[
]
blkLocations
;
if
(
file
instanceof
LocatedFileStatus
)
{
blkLocations
=
(
(
LocatedFileStatus
)
file
)
.
getBlockLocations
(
)
;
}
else
{
blkLocations
=
fs
.
getFileBlockLocations
(
file
,
0
,
length
)
;
}
if
(
isSplitable
(
fs
,
path
)
)
{
long
blockSize
=
file
.
getBlockSize
(
)
;
long
splitSize
=
computeSplitSize
(
goalSize
,
minSize
,
blockSize
)
;
long
bytesRemaining
=
length
;
while
(
(
(
double
)
bytesRemaining
)
/
splitSize
>
SPLIT_SLOP
)
{
String
[
]
[
]
splitHosts
=
getSplitHostsAndCachedHosts
(
blkLocations
,
length
-
bytesRemaining
,
splitSize
,
clusterMap
)
;
splits
.
add
(
makeSplit
(
path
,
length
-
bytesRemaining
,
splitSize
,
splitHosts
[
0
]
,
splitHosts
[
1
]
)
)
;
bytesRemaining
-=
splitSize
;
}
if
(
bytesRemaining
!=
0
)
{
String
[
]
[
]
splitHosts
=
getSplitHostsAndCachedHosts
(
blkLocations
,
length
-
bytesRemaining
,
bytesRemaining
,
clusterMap
)
;
if
(
isSplitable
(
fs
,
path
)
)
{
long
blockSize
=
file
.
getBlockSize
(
)
;
long
splitSize
=
computeSplitSize
(
goalSize
,
minSize
,
blockSize
)
;
long
bytesRemaining
=
length
;
while
(
(
(
double
)
bytesRemaining
)
/
splitSize
>
SPLIT_SLOP
)
{
String
[
]
[
]
splitHosts
=
getSplitHostsAndCachedHosts
(
blkLocations
,
length
-
bytesRemaining
,
splitSize
,
clusterMap
)
;
splits
.
add
(
makeSplit
(
path
,
length
-
bytesRemaining
,
splitSize
,
splitHosts
[
0
]
,
splitHosts
[
1
]
)
)
;
bytesRemaining
-=
splitSize
;
}
if
(
bytesRemaining
!=
0
)
{
String
[
]
[
]
splitHosts
=
getSplitHostsAndCachedHosts
(
blkLocations
,
length
-
bytesRemaining
,
bytesRemaining
,
clusterMap
)
;
splits
.
add
(
makeSplit
(
path
,
length
-
bytesRemaining
,
bytesRemaining
,
splitHosts
[
0
]
,
splitHosts
[
1
]
)
)
;
}
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
length
>
Math
.
min
(
file
.
getBlockSize
(
)
,
minSize
)
)
{
LOG
.
debug
(
+
+
file
.
getPath
(
)
)
;
public
boolean
checkAccess
(
UserGroupInformation
callerUGI
,
JobACL
jobOperation
,
String
jobOwner
,
AccessControlList
jobACL
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
int
getMemoryRequiredHelper
(
String
configName
,
int
defaultValue
,
int
heapSize
,
float
heapRatio
)
{
int
memory
=
getInt
(
configName
,
-
1
)
;
if
(
memory
<=
0
)
{
if
(
heapSize
>
0
)
{
memory
=
(
int
)
Math
.
ceil
(
heapSize
/
heapRatio
)
;
if
(
notification
!=
null
)
{
do
{
try
{
int
code
=
httpNotification
(
notification
.
getUri
(
)
,
notification
.
getTimeout
(
)
)
;
if
(
code
!=
200
)
{
throw
new
IOException
(
+
code
)
;
}
else
{
break
;
}
}
catch
(
IOException
ioex
)
{
LOG
.
error
(
+
notification
.
getUri
(
)
+
,
ioex
)
;
}
catch
(
Exception
ex
)
{
LOG
.
error
(
+
notification
.
getUri
(
)
+
,
ex
)
;
}
try
{
Thread
.
sleep
(
notification
.
getRetryInterval
(
)
)
;
}
catch
(
InterruptedException
iex
)
{
public
Iterable
<
FileStatus
>
getFileStatuses
(
)
throws
InterruptedException
,
IOException
{
runningTasks
.
incrementAndGet
(
)
;
for
(
Path
p
:
inputDirs
)
{
private
void
registerError
(
Throwable
t
)
{
@
SuppressWarnings
(
)
private
<
INKEY
,
INVALUE
,
OUTKEY
,
OUTVALUE
>
void
runOldMapper
(
final
JobConf
job
,
final
TaskSplitIndex
splitIndex
,
final
TaskUmbilicalProtocol
umbilical
,
TaskReporter
reporter
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
InputSplit
inputSplit
=
getSplitDetails
(
new
Path
(
splitIndex
.
getSplitLocation
(
)
)
,
splitIndex
.
getStartOffset
(
)
)
;
updateJobWithSplit
(
job
,
inputSplit
)
;
reporter
.
setInputSplit
(
inputSplit
)
;
RecordReader
<
INKEY
,
INVALUE
>
in
=
isSkipping
(
)
?
new
SkippingRecordReader
<
INKEY
,
INVALUE
>
(
umbilical
,
reporter
,
job
)
:
new
TrackedRecordReader
<
INKEY
,
INVALUE
>
(
reporter
,
job
)
;
job
.
setBoolean
(
JobContext
.
SKIP_RECORDS
,
isSkipping
(
)
)
;
int
numReduceTasks
=
conf
.
getNumReduceTasks
(
)
;
@
SuppressWarnings
(
)
private
<
INKEY
,
INVALUE
,
OUTKEY
,
OUTVALUE
>
void
runNewMapper
(
final
JobConf
job
,
final
TaskSplitIndex
splitIndex
,
final
TaskUmbilicalProtocol
umbilical
,
TaskReporter
reporter
)
throws
IOException
,
ClassNotFoundException
,
InterruptedException
{
org
.
apache
.
hadoop
.
mapreduce
.
TaskAttemptContext
taskContext
=
new
org
.
apache
.
hadoop
.
mapreduce
.
task
.
TaskAttemptContextImpl
(
job
,
getTaskID
(
)
,
reporter
)
;
org
.
apache
.
hadoop
.
mapreduce
.
Mapper
<
INKEY
,
INVALUE
,
OUTKEY
,
OUTVALUE
>
mapper
=
(
org
.
apache
.
hadoop
.
mapreduce
.
Mapper
<
INKEY
,
INVALUE
,
OUTKEY
,
OUTVALUE
>
)
ReflectionUtils
.
newInstance
(
taskContext
.
getMapperClass
(
)
,
job
)
;
org
.
apache
.
hadoop
.
mapreduce
.
InputFormat
<
INKEY
,
INVALUE
>
inputFormat
=
(
org
.
apache
.
hadoop
.
mapreduce
.
InputFormat
<
INKEY
,
INVALUE
>
)
ReflectionUtils
.
newInstance
(
taskContext
.
getInputFormatClass
(
)
,
job
)
;
org
.
apache
.
hadoop
.
mapreduce
.
InputSplit
split
=
null
;
split
=
getSplitDetails
(
new
Path
(
splitIndex
.
getSplitLocation
(
)
)
,
splitIndex
.
getStartOffset
(
)
)
;
JobQueueInfo
getJobQueueInfo
(
)
{
JobQueueInfo
queueInfo
=
new
JobQueueInfo
(
)
;
queueInfo
.
setQueueName
(
name
)
;
return
false
;
}
if
(
!
(
name
.
equals
(
newState
.
getName
(
)
)
)
)
{
LOG
.
info
(
+
name
+
+
newState
.
getName
(
)
)
;
return
false
;
}
if
(
children
==
null
||
children
.
size
(
)
==
0
)
{
if
(
newState
.
getChildren
(
)
!=
null
&&
newState
.
getChildren
(
)
.
size
(
)
>
0
)
{
LOG
.
info
(
newState
+
)
;
return
false
;
}
}
else
if
(
children
.
size
(
)
>
0
)
{
if
(
newState
.
getChildren
(
)
==
null
)
{
LOG
.
error
(
+
getName
(
)
+
+
children
.
size
(
)
+
)
;
return
false
;
}
int
childrenSize
=
children
.
size
(
)
;
int
newChildrenSize
=
newState
.
getChildren
(
)
.
size
(
)
;
if
(
childrenSize
!=
newChildrenSize
)
{
LOG
.
info
(
newState
+
)
;
return
false
;
}
}
else
if
(
children
.
size
(
)
>
0
)
{
if
(
newState
.
getChildren
(
)
==
null
)
{
LOG
.
error
(
+
getName
(
)
+
+
children
.
size
(
)
+
)
;
return
false
;
}
int
childrenSize
=
children
.
size
(
)
;
int
newChildrenSize
=
newState
.
getChildren
(
)
.
size
(
)
;
if
(
childrenSize
!=
newChildrenSize
)
{
LOG
.
error
(
+
newState
.
getName
(
)
+
+
newChildrenSize
+
+
childrenSize
+
)
;
return
false
;
}
Iterator
<
Queue
>
itr1
=
children
.
iterator
(
)
;
Iterator
<
Queue
>
itr2
=
newState
.
getChildren
(
)
.
iterator
(
)
;
while
(
itr1
.
hasNext
(
)
)
{
Queue
q
=
itr1
.
next
(
)
;
private
void
initialize
(
QueueConfigurationParser
cp
)
{
this
.
root
=
cp
.
getRoot
(
)
;
leafQueues
.
clear
(
)
;
allQueues
.
clear
(
)
;
leafQueues
=
getRoot
(
)
.
getLeafQueues
(
)
;
allQueues
.
putAll
(
getRoot
(
)
.
getInnerQueues
(
)
)
;
allQueues
.
putAll
(
leafQueues
)
;
public
synchronized
boolean
hasAccess
(
String
queueName
,
QueueACL
qACL
,
UserGroupInformation
ugi
)
{
Queue
q
=
leafQueues
.
get
(
queueName
)
;
if
(
q
==
null
)
{
if
(
jobCleanup
)
{
runJobCleanupTask
(
umbilical
,
reporter
)
;
return
;
}
if
(
jobSetup
)
{
runJobSetupTask
(
umbilical
,
reporter
)
;
return
;
}
if
(
taskCleanup
)
{
runTaskCleanupTask
(
umbilical
,
reporter
)
;
return
;
}
codec
=
initCodec
(
)
;
RawKeyValueIterator
rIter
=
null
;
ShuffleConsumerPlugin
shuffleConsumerPlugin
=
null
;
Class
combinerClass
=
conf
.
getCombinerClass
(
)
;
CombineOutputCollector
combineCollector
=
(
null
!=
combinerClass
)
?
new
CombineOutputCollector
(
reduceCombineOutputCounter
,
reporter
,
conf
)
:
null
;
Class
<
?
extends
ShuffleConsumerPlugin
>
clazz
=
job
.
getClass
(
MRConfig
.
SHUFFLE_CONSUMER_PLUGIN
,
Shuffle
.
class
,
ShuffleConsumerPlugin
.
class
)
;
return
;
}
long
startIndex
=
range
.
getStartIndex
(
)
;
long
endIndex
=
range
.
getEndIndex
(
)
;
SortedSet
<
Range
>
headSet
=
ranges
.
headSet
(
range
)
;
if
(
headSet
.
size
(
)
>
0
)
{
Range
previousRange
=
headSet
.
last
(
)
;
LOG
.
debug
(
+
previousRange
)
;
if
(
startIndex
<
previousRange
.
getEndIndex
(
)
)
{
if
(
ranges
.
remove
(
previousRange
)
)
{
indicesCount
-=
previousRange
.
getLength
(
)
;
}
startIndex
=
previousRange
.
getStartIndex
(
)
;
endIndex
=
endIndex
>=
previousRange
.
getEndIndex
(
)
?
endIndex
:
previousRange
.
getEndIndex
(
)
;
}
}
Iterator
<
Range
>
tailSetIt
=
ranges
.
tailSet
(
range
)
.
iterator
(
)
;
while
(
tailSetIt
.
hasNext
(
)
)
{
Range
nextRange
=
tailSetIt
.
next
(
)
;
long
startIndex
=
range
.
getStartIndex
(
)
;
long
endIndex
=
range
.
getEndIndex
(
)
;
SortedSet
<
Range
>
headSet
=
ranges
.
headSet
(
range
)
;
if
(
headSet
.
size
(
)
>
0
)
{
Range
previousRange
=
headSet
.
last
(
)
;
LOG
.
debug
(
+
previousRange
)
;
if
(
startIndex
<
previousRange
.
getEndIndex
(
)
)
{
if
(
ranges
.
remove
(
previousRange
)
)
{
indicesCount
-=
previousRange
.
getLength
(
)
;
LOG
.
debug
(
+
previousRange
)
;
}
add
(
previousRange
.
getStartIndex
(
)
,
startIndex
)
;
if
(
endIndex
<=
previousRange
.
getEndIndex
(
)
)
{
add
(
endIndex
,
previousRange
.
getEndIndex
(
)
)
;
}
}
}
Iterator
<
Range
>
tailSetIt
=
ranges
.
tailSet
(
range
)
.
iterator
(
)
;
while
(
tailSetIt
.
hasNext
(
)
)
{
private
void
add
(
long
start
,
long
end
)
{
if
(
end
>
start
)
{
Range
recRange
=
new
Range
(
start
,
end
-
start
)
;
ranges
.
add
(
recRange
)
;
indicesCount
+=
recRange
.
getLength
(
)
;
protected
void
reportFatalError
(
TaskAttemptID
id
,
Throwable
throwable
,
String
logMsg
,
boolean
fastFail
)
{
protected
void
reportNextRecordRange
(
final
TaskUmbilicalProtocol
umbilical
,
long
nextRecIndex
)
throws
IOException
{
long
len
=
nextRecIndex
-
currentRecStartIndex
+
1
;
SortedRanges
.
Range
range
=
new
SortedRanges
.
Range
(
currentRecStartIndex
,
len
)
;
taskStatus
.
setNextRecordRange
(
range
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
setState
(
TaskStatus
.
State
.
COMMIT_PENDING
)
;
while
(
true
)
{
try
{
umbilical
.
commitPending
(
taskId
,
taskStatus
)
;
break
;
}
catch
(
InterruptedException
ie
)
{
}
catch
(
IOException
ie
)
{
LOG
.
warn
(
+
StringUtils
.
stringifyException
(
ie
)
)
;
if
(
--
retries
==
0
)
{
System
.
exit
(
67
)
;
}
}
}
commit
(
umbilical
,
reporter
,
committer
)
;
}
taskDone
.
set
(
true
)
;
reporter
.
stopCommunicationThread
(
)
;
updateCounters
(
)
;
sendLastUpdate
(
umbilical
)
;
int
retries
=
MAX_RETRIES
;
while
(
true
)
{
try
{
while
(
!
umbilical
.
canCommit
(
taskId
)
)
{
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ie
)
{
}
reporter
.
setProgressFlag
(
)
;
}
break
;
}
catch
(
IOException
ie
)
{
LOG
.
warn
(
+
StringUtils
.
stringifyException
(
ie
)
)
;
if
(
--
retries
==
0
)
{
discardOutput
(
taskContext
)
;
System
.
exit
(
68
)
;
}
}
}
try
{
protected
void
runJobCleanupTask
(
TaskUmbilicalProtocol
umbilical
,
TaskReporter
reporter
)
throws
IOException
,
InterruptedException
{
setPhase
(
TaskStatus
.
Phase
.
CLEANUP
)
;
getProgress
(
)
.
setStatus
(
)
;
statusUpdate
(
umbilical
)
;
LOG
.
info
(
)
;
if
(
jobRunStateForCleanup
==
JobStatus
.
State
.
FAILED
||
jobRunStateForCleanup
==
JobStatus
.
State
.
KILLED
)
{
public
void
setDiagnosticInfo
(
String
info
)
{
if
(
diagnosticInfo
!=
null
&&
diagnosticInfo
.
length
(
)
==
getMaxStringSize
(
)
)
{
@
SuppressWarnings
(
)
public
void
configure
(
JobConf
jobConf
)
{
int
numberOfThreads
=
jobConf
.
getInt
(
MultithreadedMapper
.
NUM_THREADS
,
10
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
run
(
RecordReader
<
K1
,
V1
>
input
,
OutputCollector
<
K2
,
V2
>
output
,
Reporter
reporter
)
throws
IOException
{
try
{
K1
key
=
input
.
createKey
(
)
;
V1
value
=
input
.
createValue
(
)
;
while
(
input
.
next
(
key
,
value
)
)
{
executorService
.
execute
(
new
MapperInvokeRunable
(
key
,
value
,
output
,
reporter
)
)
;
checkForExceptionsFromProcessingThreads
(
)
;
key
=
input
.
createKey
(
)
;
value
=
input
.
createValue
(
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
job
.
getJobName
(
)
)
;
}
executorService
.
shutdown
(
)
;
try
{
while
(
!
executorService
.
awaitTermination
(
100
,
TimeUnit
.
MILLISECONDS
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
void
abort
(
Throwable
t
)
throws
IOException
{
public
void
authenticate
(
String
digest
,
String
challenge
)
throws
IOException
{
private
void
initialize
(
InetSocketAddress
jobTrackAddr
,
Configuration
conf
)
throws
IOException
{
initProviderList
(
)
;
final
IOException
initEx
=
new
IOException
(
+
MRConfig
.
FRAMEWORK_NAME
+
)
;
if
(
jobTrackAddr
!=
null
)
{
ClientProtocol
clientProtocol
=
null
;
try
{
if
(
jobTrackAddr
==
null
)
{
clientProtocol
=
provider
.
create
(
conf
)
;
}
else
{
clientProtocol
=
provider
.
create
(
jobTrackAddr
,
conf
)
;
}
if
(
clientProtocol
!=
null
)
{
clientProtocolProvider
=
provider
;
client
=
clientProtocol
;
LOG
.
debug
(
+
provider
.
getClass
(
)
.
getName
(
)
+
)
;
break
;
}
else
{
LOG
.
debug
(
+
provider
.
getClass
(
)
.
getName
(
)
+
)
;
}
}
catch
(
Exception
e
)
{
final
String
errMsg
=
+
provider
.
getClass
(
)
.
getName
(
)
+
;
public
static
FSDataOutputStream
wrapIfNecessary
(
Configuration
conf
,
FSDataOutputStream
out
,
boolean
closeOutputStream
)
throws
IOException
{
if
(
isEncryptedSpillEnabled
(
conf
)
)
{
out
.
write
(
ByteBuffer
.
allocate
(
8
)
.
putLong
(
out
.
getPos
(
)
)
.
array
(
)
)
;
byte
[
]
iv
=
createIV
(
conf
)
;
out
.
write
(
iv
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
FSDataInputStream
wrapIfNecessary
(
Configuration
conf
,
FSDataInputStream
in
)
throws
IOException
{
if
(
isEncryptedSpillEnabled
(
conf
)
)
{
CryptoCodec
cryptoCodec
=
CryptoCodec
.
getInstance
(
conf
)
;
int
bufferSize
=
getBufferSize
(
conf
)
;
IOUtils
.
readFully
(
in
,
new
byte
[
8
]
,
0
,
8
)
;
byte
[
]
iv
=
new
byte
[
cryptoCodec
.
getCipherSuite
(
)
.
getAlgorithmBlockSize
(
)
]
;
IOUtils
.
readFully
(
in
,
iv
,
0
,
cryptoCodec
.
getCipherSuite
(
)
.
getAlgorithmBlockSize
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
Map
<
String
,
Boolean
>
getSharedCacheUploadPolicies
(
Configuration
conf
,
boolean
areFiles
)
{
String
confParam
=
areFiles
?
MRJobConfig
.
CACHE_FILES_SHARED_CACHE_UPLOAD_POLICIES
:
MRJobConfig
.
CACHE_ARCHIVES_SHARED_CACHE_UPLOAD_POLICIES
;
Collection
<
String
>
policies
=
conf
.
getStringCollection
(
confParam
)
;
String
[
]
policy
;
Map
<
String
,
Boolean
>
policyMap
=
new
LinkedHashMap
<
String
,
Boolean
>
(
)
;
for
(
String
s
:
policies
)
{
policy
=
s
.
split
(
DELIM
)
;
if
(
policy
.
length
!=
2
)
{
public
boolean
monitorAndPrintJob
(
)
throws
IOException
,
InterruptedException
{
String
lastReport
=
null
;
Job
.
TaskStatusFilter
filter
;
Configuration
clientConf
=
getConfiguration
(
)
;
filter
=
Job
.
getTaskOutputFilter
(
clientConf
)
;
JobID
jobId
=
getJobID
(
)
;
int
progMonitorPollIntervalMillis
=
Job
.
getProgressPollInterval
(
clientConf
)
;
boolean
reportedAfterCompletion
=
false
;
boolean
reportedUberMode
=
false
;
while
(
!
isComplete
(
)
||
!
reportedAfterCompletion
)
{
if
(
isComplete
(
)
)
{
reportedAfterCompletion
=
true
;
}
else
{
Thread
.
sleep
(
progMonitorPollIntervalMillis
)
;
}
if
(
status
.
getState
(
)
==
JobStatus
.
State
.
PREP
)
{
continue
;
}
if
(
!
reportedUberMode
)
{
reportedUberMode
=
true
;
LOG
.
info
(
+
jobId
+
+
isUber
(
)
)
;
}
String
report
=
(
+
StringUtils
.
formatPercent
(
mapProgress
(
)
,
0
)
+
+
StringUtils
.
formatPercent
(
reduceProgress
(
)
,
0
)
)
;
if
(
!
report
.
equals
(
lastReport
)
)
{
else
{
Thread
.
sleep
(
progMonitorPollIntervalMillis
)
;
}
if
(
status
.
getState
(
)
==
JobStatus
.
State
.
PREP
)
{
continue
;
}
if
(
!
reportedUberMode
)
{
reportedUberMode
=
true
;
LOG
.
info
(
+
jobId
+
+
isUber
(
)
)
;
}
String
report
=
(
+
StringUtils
.
formatPercent
(
mapProgress
(
)
,
0
)
+
+
StringUtils
.
formatPercent
(
reduceProgress
(
)
,
0
)
)
;
if
(
!
report
.
equals
(
lastReport
)
)
{
LOG
.
info
(
report
)
;
lastReport
=
report
;
}
TaskCompletionEvent
[
]
events
=
getTaskCompletionEvents
(
eventCounter
,
10
)
;
eventCounter
+=
events
.
length
;
printTaskEvents
(
events
,
filter
,
profiling
,
mapRanges
,
reduceRanges
)
;
}
boolean
success
=
isSuccessful
(
)
;
}
if
(
status
.
getState
(
)
==
JobStatus
.
State
.
PREP
)
{
continue
;
}
if
(
!
reportedUberMode
)
{
reportedUberMode
=
true
;
LOG
.
info
(
+
jobId
+
+
isUber
(
)
)
;
}
String
report
=
(
+
StringUtils
.
formatPercent
(
mapProgress
(
)
,
0
)
+
+
StringUtils
.
formatPercent
(
reduceProgress
(
)
,
0
)
)
;
if
(
!
report
.
equals
(
lastReport
)
)
{
LOG
.
info
(
report
)
;
lastReport
=
report
;
}
TaskCompletionEvent
[
]
events
=
getTaskCompletionEvents
(
eventCounter
,
10
)
;
eventCounter
+=
events
.
length
;
printTaskEvents
(
events
,
filter
,
profiling
,
mapRanges
,
reduceRanges
)
;
}
boolean
success
=
isSuccessful
(
)
;
if
(
success
)
{
LOG
.
info
(
+
jobId
+
)
;
private
void
printTaskEvents
(
TaskCompletionEvent
[
]
events
,
Job
.
TaskStatusFilter
filter
,
boolean
profiling
,
IntegerRanges
mapRanges
,
IntegerRanges
reduceRanges
)
throws
IOException
,
InterruptedException
{
for
(
TaskCompletionEvent
event
:
events
)
{
switch
(
filter
)
{
case
NONE
:
break
;
case
SUCCEEDED
:
if
(
event
.
getStatus
(
)
==
TaskCompletionEvent
.
Status
.
SUCCEEDED
)
{
for
(
TaskCompletionEvent
event
:
events
)
{
switch
(
filter
)
{
case
NONE
:
break
;
case
SUCCEEDED
:
if
(
event
.
getStatus
(
)
==
TaskCompletionEvent
.
Status
.
SUCCEEDED
)
{
LOG
.
info
(
event
.
toString
(
)
)
;
}
break
;
case
FAILED
:
if
(
event
.
getStatus
(
)
==
TaskCompletionEvent
.
Status
.
FAILED
)
{
LOG
.
info
(
event
.
toString
(
)
)
;
TaskAttemptID
taskId
=
event
.
getTaskAttemptId
(
)
;
String
[
]
taskDiagnostics
=
getTaskDiagnostics
(
taskId
)
;
if
(
taskDiagnostics
!=
null
)
{
for
(
String
diagnostics
:
taskDiagnostics
)
{
System
.
err
.
println
(
diagnostics
)
;
}
}
}
break
;
case
KILLED
:
if
(
event
.
getStatus
(
)
==
TaskCompletionEvent
.
Status
.
KILLED
)
{
private
void
disableErasureCodingForPath
(
Path
path
)
throws
IOException
{
try
{
if
(
jtFs
instanceof
DistributedFileSystem
)
{
Path
jobStagingArea
=
JobSubmissionFiles
.
getStagingDir
(
cluster
,
conf
)
;
InetAddress
ip
=
InetAddress
.
getLocalHost
(
)
;
if
(
ip
!=
null
)
{
submitHostAddress
=
ip
.
getHostAddress
(
)
;
submitHostName
=
ip
.
getHostName
(
)
;
conf
.
set
(
MRJobConfig
.
JOB_SUBMITHOST
,
submitHostName
)
;
conf
.
set
(
MRJobConfig
.
JOB_SUBMITHOSTADDR
,
submitHostAddress
)
;
}
JobID
jobId
=
submitClient
.
getNewJobID
(
)
;
job
.
setJobID
(
jobId
)
;
Path
submitJobDir
=
new
Path
(
jobStagingArea
,
jobId
.
toString
(
)
)
;
JobStatus
status
=
null
;
try
{
conf
.
set
(
MRJobConfig
.
USER_NAME
,
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
)
;
conf
.
set
(
,
)
;
conf
.
set
(
MRJobConfig
.
MAPREDUCE_JOB_DIR
,
submitJobDir
.
toString
(
)
)
;
TokenCache
.
obtainTokensForNamenodes
(
job
.
getCredentials
(
)
,
new
Path
[
]
{
submitJobDir
}
,
conf
)
;
populateTokenCache
(
conf
,
job
.
getCredentials
(
)
)
;
if
(
TokenCache
.
getShuffleSecretKey
(
job
.
getCredentials
(
)
)
==
null
)
{
KeyGenerator
keyGen
;
try
{
keyGen
=
KeyGenerator
.
getInstance
(
SHUFFLE_KEYGEN_ALGORITHM
)
;
keyGen
.
init
(
SHUFFLE_KEY_LENGTH
)
;
}
catch
(
NoSuchAlgorithmException
e
)
{
throw
new
IOException
(
,
e
)
;
}
SecretKey
shuffleKey
=
keyGen
.
generateKey
(
)
;
TokenCache
.
setShuffleSecretKey
(
shuffleKey
.
getEncoded
(
)
,
job
.
getCredentials
(
)
)
;
}
if
(
CryptoUtils
.
isEncryptedSpillEnabled
(
conf
)
)
{
conf
.
setInt
(
MRJobConfig
.
MR_AM_MAX_ATTEMPTS
,
1
)
;
LOG
.
warn
(
+
)
;
}
copyAndConfigureFiles
(
job
,
submitJobDir
)
;
KeyGenerator
keyGen
;
try
{
keyGen
=
KeyGenerator
.
getInstance
(
SHUFFLE_KEYGEN_ALGORITHM
)
;
keyGen
.
init
(
SHUFFLE_KEY_LENGTH
)
;
}
catch
(
NoSuchAlgorithmException
e
)
{
throw
new
IOException
(
,
e
)
;
}
SecretKey
shuffleKey
=
keyGen
.
generateKey
(
)
;
TokenCache
.
setShuffleSecretKey
(
shuffleKey
.
getEncoded
(
)
,
job
.
getCredentials
(
)
)
;
}
if
(
CryptoUtils
.
isEncryptedSpillEnabled
(
conf
)
)
{
conf
.
setInt
(
MRJobConfig
.
MR_AM_MAX_ATTEMPTS
,
1
)
;
LOG
.
warn
(
+
)
;
}
copyAndConfigureFiles
(
job
,
submitJobDir
)
;
Path
submitJobFile
=
JobSubmissionFiles
.
getJobConfPath
(
submitJobDir
)
;
LOG
.
debug
(
+
jtFs
.
makeQualified
(
submitJobDir
)
)
;
int
maps
=
writeSplits
(
job
,
submitJobDir
)
;
String
queue
=
conf
.
get
(
MRJobConfig
.
QUEUE_NAME
,
JobConf
.
DEFAULT_QUEUE_NAME
)
;
AccessControlList
acl
=
submitClient
.
getQueueAdmins
(
queue
)
;
conf
.
set
(
toFullPropertyName
(
queue
,
QueueACL
.
ADMINISTER_JOBS
.
getAclName
(
)
)
,
acl
.
getAclString
(
)
)
;
TokenCache
.
cleanUpTokenReferral
(
conf
)
;
if
(
conf
.
getBoolean
(
MRJobConfig
.
JOB_TOKEN_TRACKING_IDS_ENABLED
,
MRJobConfig
.
DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED
)
)
{
ArrayList
<
String
>
trackingIds
=
new
ArrayList
<
String
>
(
)
;
for
(
Token
<
?
extends
TokenIdentifier
>
t
:
job
.
getCredentials
(
)
.
getAllTokens
(
)
)
{
trackingIds
.
add
(
t
.
decodeIdentifier
(
)
.
getTrackingId
(
)
)
;
}
conf
.
setStrings
(
MRJobConfig
.
JOB_TOKEN_TRACKING_IDS
,
trackingIds
.
toArray
(
new
String
[
trackingIds
.
size
(
)
]
)
)
;
}
ReservationId
reservationId
=
job
.
getReservationId
(
)
;
if
(
reservationId
!=
null
)
{
conf
.
set
(
MRJobConfig
.
RESERVATION_ID
,
reservationId
.
toString
(
)
)
;
}
writeConf
(
conf
,
submitJobFile
)
;
printTokens
(
jobId
,
job
.
getCredentials
(
)
)
;
status
=
submitClient
.
submitJob
(
jobId
,
submitJobDir
.
toString
(
)
,
job
.
getCredentials
(
)
)
;
private
void
printTokens
(
JobID
jobId
,
Credentials
credentials
)
throws
IOException
{
private
void
printTokens
(
JobID
jobId
,
Credentials
credentials
)
throws
IOException
{
LOG
.
info
(
+
jobId
)
;
private
void
populateTokenCache
(
Configuration
conf
,
Credentials
credentials
)
throws
IOException
{
readTokensFromFiles
(
conf
,
credentials
)
;
String
[
]
nameNodes
=
conf
.
getStrings
(
MRJobConfig
.
JOB_NAMENODES
)
;
DBSplitter
splitter
=
getSplitter
(
sqlDataType
)
;
if
(
null
==
splitter
)
{
throw
new
IOException
(
+
sqlDataType
)
;
}
return
splitter
.
split
(
job
.
getConfiguration
(
)
,
results
,
getDBConf
(
)
.
getInputOrderBy
(
)
)
;
}
catch
(
SQLException
e
)
{
throw
new
IOException
(
e
.
getMessage
(
)
)
;
}
finally
{
try
{
if
(
null
!=
results
)
{
results
.
close
(
)
;
}
}
catch
(
SQLException
se
)
{
LOG
.
debug
(
+
se
.
toString
(
)
)
;
}
try
{
if
(
null
!=
statement
)
{
statement
.
close
(
)
;
}
catch
(
SQLException
e
)
{
throw
new
IOException
(
e
.
getMessage
(
)
)
;
}
finally
{
try
{
if
(
null
!=
results
)
{
results
.
close
(
)
;
}
}
catch
(
SQLException
se
)
{
LOG
.
debug
(
+
se
.
toString
(
)
)
;
}
try
{
if
(
null
!=
statement
)
{
statement
.
close
(
)
;
}
}
catch
(
SQLException
se
)
{
LOG
.
debug
(
+
se
.
toString
(
)
)
;
}
try
{
connection
.
commit
(
)
;
protected
RecordReader
<
LongWritable
,
T
>
createDBRecordReader
(
DBInputSplit
split
,
Configuration
conf
)
throws
IOException
{
DBConfiguration
dbConf
=
getDBConf
(
)
;
@
SuppressWarnings
(
)
Class
<
T
>
inputClass
=
(
Class
<
T
>
)
(
dbConf
.
getInputClass
(
)
)
;
String
dbProductName
=
getDBProductName
(
)
;
query
.
append
(
)
;
for
(
int
i
=
0
;
i
<
fieldNames
.
length
;
i
++
)
{
query
.
append
(
fieldNames
[
i
]
)
;
if
(
i
!=
fieldNames
.
length
-
1
)
{
query
.
append
(
)
;
}
}
query
.
append
(
)
.
append
(
tableName
)
;
if
(
!
dbProductName
.
startsWith
(
)
)
{
query
.
append
(
)
.
append
(
tableName
)
;
}
query
.
append
(
)
;
if
(
conditions
!=
null
&&
conditions
.
length
(
)
>
0
)
{
query
.
append
(
)
.
append
(
conditions
)
.
append
(
)
;
}
query
.
append
(
conditionClauses
.
toString
(
)
)
;
}
else
{
String
inputQuery
=
dbConf
.
getInputQuery
(
)
;
if
(
inputQuery
.
indexOf
(
DataDrivenDBInputFormat
.
SUBSTITUTE_TOKEN
)
==
-
1
)
{
method
=
conn
.
getClass
(
)
.
getMethod
(
,
new
Class
[
]
{
String
.
class
}
)
;
}
catch
(
Exception
ex
)
{
LOG
.
error
(
+
conn
.
getClass
(
)
.
getName
(
)
,
ex
)
;
throw
new
SQLException
(
ex
)
;
}
String
clientTimeZone
=
conf
.
get
(
SESSION_TIMEZONE_KEY
,
)
;
try
{
method
.
setAccessible
(
true
)
;
method
.
invoke
(
conn
,
clientTimeZone
)
;
LOG
.
info
(
+
clientTimeZone
)
;
}
catch
(
Exception
ex
)
{
LOG
.
warn
(
+
clientTimeZone
+
)
;
LOG
.
warn
(
)
;
try
{
method
.
invoke
(
conn
,
)
;
}
catch
(
Exception
ex2
)
{
public
void
setup
(
Context
context
)
throws
IOException
,
InterruptedException
{
Configuration
conf
=
context
.
getConfiguration
(
)
;
this
.
fieldSeparator
=
conf
.
get
(
FieldSelectionHelper
.
DATA_FIELD_SEPARATOR
,
)
;
this
.
reduceOutputKeyValueSpec
=
conf
.
get
(
FieldSelectionHelper
.
REDUCE_OUTPUT_KEY_VALUE_SPEC
,
)
;
allReduceValueFieldsFrom
=
FieldSelectionHelper
.
parseOutputKeyValueSpec
(
reduceOutputKeyValueSpec
,
reduceOutputKeyFieldList
,
reduceOutputValueFieldList
)
;
List
<
FileStatus
>
result
=
null
;
int
numThreads
=
job
.
getConfiguration
(
)
.
getInt
(
LIST_STATUS_NUM_THREADS
,
DEFAULT_LIST_STATUS_NUM_THREADS
)
;
StopWatch
sw
=
new
StopWatch
(
)
.
start
(
)
;
if
(
numThreads
==
1
)
{
result
=
singleThreadedListStatus
(
job
,
dirs
,
inputFilter
,
recursive
)
;
}
else
{
Iterable
<
FileStatus
>
locatedFiles
=
null
;
try
{
LocatedFileStatusFetcher
locatedFileStatusFetcher
=
new
LocatedFileStatusFetcher
(
job
.
getConfiguration
(
)
,
dirs
,
recursive
,
inputFilter
,
true
)
;
locatedFiles
=
locatedFileStatusFetcher
.
getFileStatuses
(
)
;
}
catch
(
InterruptedException
e
)
{
throw
(
IOException
)
new
InterruptedIOException
(
)
.
initCause
(
e
)
;
}
result
=
Lists
.
newArrayList
(
locatedFiles
)
;
}
sw
.
stop
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
else
{
FileSystem
fs
=
path
.
getFileSystem
(
job
.
getConfiguration
(
)
)
;
blkLocations
=
fs
.
getFileBlockLocations
(
file
,
0
,
length
)
;
}
if
(
isSplitable
(
job
,
path
)
)
{
long
blockSize
=
file
.
getBlockSize
(
)
;
long
splitSize
=
computeSplitSize
(
blockSize
,
minSize
,
maxSize
)
;
long
bytesRemaining
=
length
;
while
(
(
(
double
)
bytesRemaining
)
/
splitSize
>
SPLIT_SLOP
)
{
int
blkIndex
=
getBlockIndex
(
blkLocations
,
length
-
bytesRemaining
)
;
splits
.
add
(
makeSplit
(
path
,
length
-
bytesRemaining
,
splitSize
,
blkLocations
[
blkIndex
]
.
getHosts
(
)
,
blkLocations
[
blkIndex
]
.
getCachedHosts
(
)
)
)
;
bytesRemaining
-=
splitSize
;
}
if
(
bytesRemaining
!=
0
)
{
int
blkIndex
=
getBlockIndex
(
blkLocations
,
length
-
bytesRemaining
)
;
splits
.
add
(
makeSplit
(
path
,
length
-
bytesRemaining
,
bytesRemaining
,
blkLocations
[
blkIndex
]
.
getHosts
(
)
,
blkLocations
[
blkIndex
]
.
getCachedHosts
(
)
)
)
;
}
}
else
{
long
bytesRemaining
=
length
;
while
(
(
(
double
)
bytesRemaining
)
/
splitSize
>
SPLIT_SLOP
)
{
int
blkIndex
=
getBlockIndex
(
blkLocations
,
length
-
bytesRemaining
)
;
splits
.
add
(
makeSplit
(
path
,
length
-
bytesRemaining
,
splitSize
,
blkLocations
[
blkIndex
]
.
getHosts
(
)
,
blkLocations
[
blkIndex
]
.
getCachedHosts
(
)
)
)
;
bytesRemaining
-=
splitSize
;
}
if
(
bytesRemaining
!=
0
)
{
int
blkIndex
=
getBlockIndex
(
blkLocations
,
length
-
bytesRemaining
)
;
splits
.
add
(
makeSplit
(
path
,
length
-
bytesRemaining
,
bytesRemaining
,
blkLocations
[
blkIndex
]
.
getHosts
(
)
,
blkLocations
[
blkIndex
]
.
getCachedHosts
(
)
)
)
;
}
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
length
>
Math
.
min
(
file
.
getBlockSize
(
)
,
minSize
)
)
{
LOG
.
debug
(
+
+
file
.
getPath
(
)
)
;
}
}
splits
.
add
(
makeSplit
(
path
,
0
,
length
,
blkLocations
[
0
]
.
getHosts
(
)
,
blkLocations
[
0
]
.
getCachedHosts
(
)
)
)
;
}
}
else
{
splits
.
add
(
makeSplit
(
path
,
0
,
length
,
new
String
[
0
]
)
)
;
isCompressedInput
=
true
;
decompressor
=
CodecPool
.
getDecompressor
(
codec
)
;
CompressionInputStream
cIn
=
codec
.
createInputStream
(
fileIn
,
decompressor
)
;
filePosition
=
cIn
;
inputStream
=
cIn
;
numRecordsRemainingInSplit
=
Long
.
MAX_VALUE
;
LOG
.
info
(
)
;
}
else
{
fileIn
.
seek
(
start
)
;
filePosition
=
fileIn
;
inputStream
=
fileIn
;
long
splitSize
=
end
-
start
-
numBytesToSkip
;
numRecordsRemainingInSplit
=
(
splitSize
+
recordLength
-
1
)
/
recordLength
;
if
(
numRecordsRemainingInSplit
<
0
)
{
numRecordsRemainingInSplit
=
0
;
}
for
(
ControlledJob
n
:
jobList
)
{
if
(
!
hasInComingEdge
(
n
,
jobList
,
processedMap
)
)
{
SourceSet
.
add
(
n
)
;
}
}
while
(
!
SourceSet
.
isEmpty
(
)
)
{
ControlledJob
controlledJob
=
SourceSet
.
iterator
(
)
.
next
(
)
;
SourceSet
.
remove
(
controlledJob
)
;
if
(
controlledJob
.
getDependentJobs
(
)
!=
null
)
{
for
(
int
i
=
0
;
i
<
controlledJob
.
getDependentJobs
(
)
.
size
(
)
;
i
++
)
{
ControlledJob
depenControlledJob
=
controlledJob
.
getDependentJobs
(
)
.
get
(
i
)
;
processedMap
.
get
(
controlledJob
)
.
add
(
depenControlledJob
)
;
if
(
!
hasInComingEdge
(
controlledJob
,
jobList
,
processedMap
)
)
{
SourceSet
.
add
(
depenControlledJob
)
;
}
}
}
}
for
(
ControlledJob
controlledJob
:
jobList
)
{
if
(
controlledJob
.
getDependentJobs
(
)
!=
null
&&
controlledJob
.
getDependentJobs
(
)
.
size
(
)
!=
processedMap
.
get
(
controlledJob
)
.
size
(
)
)
{
cyclePresent
=
true
;
@
Override
public
void
run
(
Context
context
)
throws
IOException
,
InterruptedException
{
outer
=
context
;
int
numberOfThreads
=
getNumberOfThreads
(
context
)
;
mapClass
=
getMapperClass
(
context
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
setupJob
(
JobContext
context
)
throws
IOException
{
if
(
hasOutputPath
(
)
)
{
Path
jobAttemptPath
=
getJobAttemptPath
(
context
)
;
FileSystem
fs
=
jobAttemptPath
.
getFileSystem
(
context
.
getConfiguration
(
)
)
;
if
(
!
fs
.
mkdirs
(
jobAttemptPath
)
)
{
private
void
mergePaths
(
FileSystem
fs
,
final
FileStatus
from
,
final
Path
to
,
JobContext
context
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
FileStatus
taskAttemptDirStatus
;
try
{
taskAttemptDirStatus
=
fs
.
getFileStatus
(
taskAttemptPath
)
;
}
catch
(
FileNotFoundException
e
)
{
taskAttemptDirStatus
=
null
;
}
if
(
taskAttemptDirStatus
!=
null
)
{
if
(
algorithmVersion
==
1
)
{
Path
committedTaskPath
=
getCommittedTaskPath
(
context
)
;
if
(
fs
.
exists
(
committedTaskPath
)
)
{
if
(
!
fs
.
delete
(
committedTaskPath
,
true
)
)
{
throw
new
IOException
(
+
committedTaskPath
)
;
}
}
if
(
!
fs
.
rename
(
taskAttemptPath
,
committedTaskPath
)
)
{
throw
new
IOException
(
+
taskAttemptPath
+
+
committedTaskPath
)
;
}
LOG
.
info
(
+
attemptId
+
+
committedTaskPath
)
;
}
else
{
taskAttemptDirStatus
=
fs
.
getFileStatus
(
taskAttemptPath
)
;
}
catch
(
FileNotFoundException
e
)
{
taskAttemptDirStatus
=
null
;
}
if
(
taskAttemptDirStatus
!=
null
)
{
if
(
algorithmVersion
==
1
)
{
Path
committedTaskPath
=
getCommittedTaskPath
(
context
)
;
if
(
fs
.
exists
(
committedTaskPath
)
)
{
if
(
!
fs
.
delete
(
committedTaskPath
,
true
)
)
{
throw
new
IOException
(
+
committedTaskPath
)
;
}
}
if
(
!
fs
.
rename
(
taskAttemptPath
,
committedTaskPath
)
)
{
throw
new
IOException
(
+
taskAttemptPath
+
+
committedTaskPath
)
;
}
LOG
.
info
(
+
attemptId
+
+
committedTaskPath
)
;
}
else
{
mergePaths
(
fs
,
taskAttemptDirStatus
,
outputPath
,
context
)
;
LOG
.
info
(
+
attemptId
+
+
outputPath
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
previousCommittedTaskPath
)
;
}
if
(
algorithmVersion
==
1
)
{
if
(
fs
.
exists
(
previousCommittedTaskPath
)
)
{
Path
committedTaskPath
=
getCommittedTaskPath
(
context
)
;
if
(
!
fs
.
delete
(
committedTaskPath
,
true
)
&&
fs
.
exists
(
committedTaskPath
)
)
{
throw
new
IOException
(
+
committedTaskPath
)
;
}
Path
committedParent
=
committedTaskPath
.
getParent
(
)
;
fs
.
mkdirs
(
committedParent
)
;
if
(
!
fs
.
rename
(
previousCommittedTaskPath
,
committedTaskPath
)
)
{
throw
new
IOException
(
+
previousCommittedTaskPath
+
+
committedTaskPath
)
;
}
}
else
{
LOG
.
warn
(
attemptId
+
)
;
}
}
else
{
try
{
if
(
fs
.
exists
(
previousCommittedTaskPath
)
)
{
Path
committedTaskPath
=
getCommittedTaskPath
(
context
)
;
if
(
!
fs
.
delete
(
committedTaskPath
,
true
)
&&
fs
.
exists
(
committedTaskPath
)
)
{
throw
new
IOException
(
+
committedTaskPath
)
;
}
Path
committedParent
=
committedTaskPath
.
getParent
(
)
;
fs
.
mkdirs
(
committedParent
)
;
if
(
!
fs
.
rename
(
previousCommittedTaskPath
,
committedTaskPath
)
)
{
throw
new
IOException
(
+
previousCommittedTaskPath
+
+
committedTaskPath
)
;
}
}
else
{
LOG
.
warn
(
attemptId
+
)
;
}
}
else
{
try
{
FileStatus
from
=
fs
.
getFileStatus
(
previousCommittedTaskPath
)
;
LOG
.
info
(
+
previousCommittedTaskPath
+
+
outputPath
)
;
mergePaths
(
fs
,
from
,
outputPath
,
context
)
;
public
static
Path
getWorkOutputPath
(
TaskInputOutputContext
<
?
,
?
,
?
,
?
>
context
)
throws
IOException
,
InterruptedException
{
PathOutputCommitter
committer
=
(
PathOutputCommitter
)
context
.
getOutputCommitter
(
)
;
Path
workPath
=
committer
.
getWorkPath
(
)
;
public
Path
getDefaultWorkFile
(
TaskAttemptContext
context
,
String
extension
)
throws
IOException
{
OutputCommitter
c
=
getOutputCommitter
(
context
)
;
Preconditions
.
checkState
(
c
instanceof
PathOutputCommitter
,
,
c
)
;
Path
workPath
=
(
(
PathOutputCommitter
)
c
)
.
getWorkPath
(
)
;
Preconditions
.
checkNotNull
(
workPath
,
,
c
)
;
Path
workFile
=
new
Path
(
workPath
,
getUniqueFile
(
context
,
getOutputName
(
context
)
,
extension
)
)
;
@
SuppressWarnings
(
)
@
Override
public
PathOutputCommitter
createOutputCommitter
(
Path
outputPath
,
TaskAttemptContext
context
)
throws
IOException
{
Class
<
?
extends
PathOutputCommitter
>
clazz
=
loadCommitterClass
(
context
)
;
protected
final
PathOutputCommitter
createFileOutputCommitter
(
Path
outputPath
,
TaskAttemptContext
context
)
throws
IOException
{
public
static
PathOutputCommitterFactory
getCommitterFactory
(
Path
outputPath
,
Configuration
conf
)
{
public
static
PathOutputCommitterFactory
getCommitterFactory
(
Path
outputPath
,
Configuration
conf
)
{
LOG
.
debug
(
,
outputPath
)
;
String
key
=
COMMITTER_FACTORY_CLASS
;
if
(
StringUtils
.
isEmpty
(
conf
.
getTrimmed
(
key
)
)
&&
outputPath
!=
null
)
{
String
scheme
=
outputPath
.
toUri
(
)
.
getScheme
(
)
;
String
schemeKey
=
String
.
format
(
COMMITTER_FACTORY_SCHEME_PATTERN
,
scheme
)
;
if
(
StringUtils
.
isNotEmpty
(
conf
.
getTrimmed
(
schemeKey
)
)
)
{
if
(
StringUtils
.
isEmpty
(
conf
.
getTrimmed
(
key
)
)
&&
outputPath
!=
null
)
{
String
scheme
=
outputPath
.
toUri
(
)
.
getScheme
(
)
;
String
schemeKey
=
String
.
format
(
COMMITTER_FACTORY_SCHEME_PATTERN
,
scheme
)
;
if
(
StringUtils
.
isNotEmpty
(
conf
.
getTrimmed
(
schemeKey
)
)
)
{
LOG
.
debug
(
,
outputPath
)
;
key
=
schemeKey
;
}
else
{
LOG
.
debug
(
,
schemeKey
)
;
}
}
Class
<
?
extends
PathOutputCommitterFactory
>
factory
;
String
trimmedValue
=
conf
.
getTrimmed
(
key
,
)
;
if
(
StringUtils
.
isEmpty
(
trimmedValue
)
)
{
LOG
.
debug
(
+
)
;
factory
=
FileOutputCommitterFactory
.
class
;
}
else
{
factory
=
conf
.
getClass
(
key
,
FileOutputCommitterFactory
.
class
,
PathOutputCommitterFactory
.
class
)
;
@
SuppressWarnings
(
)
public
static
<
K
,
V
>
void
writePartitionFile
(
Job
job
,
Sampler
<
K
,
V
>
sampler
)
throws
IOException
,
ClassNotFoundException
,
InterruptedException
{
Configuration
conf
=
job
.
getConfiguration
(
)
;
final
InputFormat
inf
=
ReflectionUtils
.
newInstance
(
job
.
getInputFormatClass
(
)
,
conf
)
;
int
numPartitions
=
job
.
getNumReduceTasks
(
)
;
K
[
]
samples
=
(
K
[
]
)
sampler
.
getSample
(
inf
,
job
)
;
@
InterfaceAudience
.
Private
@
Deprecated
public
static
Credentials
loadTokens
(
String
jobTokenFile
,
JobConf
conf
)
throws
IOException
{
Path
localJobTokenFile
=
new
Path
(
+
jobTokenFile
)
;
Credentials
ts
=
Credentials
.
readTokenStorageFile
(
localJobTokenFile
,
conf
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
run
(
)
{
int
failures
=
0
;
LOG
.
info
(
reduce
+
+
getName
(
)
)
;
try
{
while
(
!
stopped
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
try
{
int
numNewMaps
=
getMapCompletionEvents
(
)
;
failures
=
0
;
if
(
numNewMaps
>
0
)
{
LOG
.
info
(
reduce
+
+
getName
(
)
)
;
try
{
while
(
!
stopped
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
try
{
int
numNewMaps
=
getMapCompletionEvents
(
)
;
failures
=
0
;
if
(
numNewMaps
>
0
)
{
LOG
.
info
(
reduce
+
+
+
numNewMaps
+
)
;
}
LOG
.
debug
(
+
SLEEP_TIME
)
;
if
(
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
Thread
.
sleep
(
SLEEP_TIME
)
;
}
}
catch
(
InterruptedException
e
)
{
LOG
.
info
(
)
;
return
;
}
catch
(
IOException
ie
)
{
if
(
rc
==
TOO_MANY_REQ_STATUS_CODE
)
{
long
backoff
=
connection
.
getHeaderFieldLong
(
FETCH_RETRY_AFTER_HEADER
,
FETCH_RETRY_DELAY_DEFAULT
)
;
if
(
backoff
<
0
)
{
backoff
=
FETCH_RETRY_DELAY_DEFAULT
;
LOG
.
warn
(
+
+
FETCH_RETRY_DELAY_DEFAULT
)
;
}
throw
new
TryAgainLaterException
(
backoff
,
url
.
getHost
(
)
)
;
}
if
(
rc
!=
HttpURLConnection
.
HTTP_OK
)
{
throw
new
IOException
(
+
rc
+
+
url
+
+
connection
.
getResponseMessage
(
)
)
;
}
if
(
!
ShuffleHeader
.
DEFAULT_HTTP_HEADER_NAME
.
equals
(
connection
.
getHeaderField
(
ShuffleHeader
.
HTTP_HEADER_NAME
)
)
||
!
ShuffleHeader
.
DEFAULT_HTTP_HEADER_VERSION
.
equals
(
connection
.
getHeaderField
(
ShuffleHeader
.
HTTP_HEADER_VERSION
)
)
)
{
throw
new
IOException
(
)
;
}
String
replyHash
=
connection
.
getHeaderField
(
SecureShuffleUtils
.
HTTP_HEADER_REPLY_URL_HASH
)
;
if
(
replyHash
==
null
)
{
throw
new
IOException
(
)
;
}
LOG
.
debug
(
+
msgToEncode
+
+
encHash
+
+
replyHash
)
;
SecureShuffleUtils
.
verifyReply
(
replyHash
,
encHash
,
shuffleSecretKey
)
;
mapId
=
TaskAttemptID
.
forName
(
header
.
mapId
)
;
compressedLength
=
header
.
compressedLength
;
decompressedLength
=
header
.
uncompressedLength
;
forReduce
=
header
.
forReduce
;
}
catch
(
IllegalArgumentException
e
)
{
badIdErrs
.
increment
(
1
)
;
LOG
.
warn
(
,
e
)
;
return
remaining
.
toArray
(
new
TaskAttemptID
[
remaining
.
size
(
)
]
)
;
}
InputStream
is
=
input
;
is
=
CryptoUtils
.
wrapIfNecessary
(
jobConf
,
is
,
compressedLength
)
;
compressedLength
-=
CryptoUtils
.
cryptoPadding
(
jobConf
)
;
decompressedLength
-=
CryptoUtils
.
cryptoPadding
(
jobConf
)
;
if
(
!
verifySanity
(
compressedLength
,
decompressedLength
,
forReduce
,
remaining
,
mapId
)
)
{
return
new
TaskAttemptID
[
]
{
mapId
}
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
return
remaining
.
toArray
(
new
TaskAttemptID
[
remaining
.
size
(
)
]
)
;
}
InputStream
is
=
input
;
is
=
CryptoUtils
.
wrapIfNecessary
(
jobConf
,
is
,
compressedLength
)
;
compressedLength
-=
CryptoUtils
.
cryptoPadding
(
jobConf
)
;
decompressedLength
-=
CryptoUtils
.
cryptoPadding
(
jobConf
)
;
if
(
!
verifySanity
(
compressedLength
,
decompressedLength
,
forReduce
,
remaining
,
mapId
)
)
{
return
new
TaskAttemptID
[
]
{
mapId
}
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
mapId
+
+
compressedLength
+
+
decompressedLength
)
;
}
try
{
mapOutput
=
merger
.
reserve
(
mapId
,
decompressedLength
,
id
)
;
}
catch
(
IOException
ioe
)
{
ioErrs
.
increment
(
1
)
;
scheduler
.
reportLocalError
(
ioe
)
;
return
EMPTY_ATTEMPT_ID_ARRAY
;
is
=
CryptoUtils
.
wrapIfNecessary
(
jobConf
,
is
,
compressedLength
)
;
compressedLength
-=
CryptoUtils
.
cryptoPadding
(
jobConf
)
;
decompressedLength
-=
CryptoUtils
.
cryptoPadding
(
jobConf
)
;
if
(
!
verifySanity
(
compressedLength
,
decompressedLength
,
forReduce
,
remaining
,
mapId
)
)
{
return
new
TaskAttemptID
[
]
{
mapId
}
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
mapId
+
+
compressedLength
+
+
decompressedLength
)
;
}
try
{
mapOutput
=
merger
.
reserve
(
mapId
,
decompressedLength
,
id
)
;
}
catch
(
IOException
ioe
)
{
ioErrs
.
increment
(
1
)
;
scheduler
.
reportLocalError
(
ioe
)
;
return
EMPTY_ATTEMPT_ID_ARRAY
;
}
if
(
mapOutput
==
null
)
{
LOG
.
info
(
+
id
+
)
;
}
long
startTime
=
Time
.
monotonicNow
(
)
;
long
lastTime
=
startTime
;
int
attempts
=
0
;
connection
.
setConnectTimeout
(
unit
)
;
while
(
true
)
{
try
{
attempts
++
;
connection
.
connect
(
)
;
break
;
}
catch
(
IOException
ioe
)
{
long
currentTime
=
Time
.
monotonicNow
(
)
;
long
retryTime
=
currentTime
-
startTime
;
long
leftTime
=
connectionTimeout
-
retryTime
;
long
timeSinceLastIteration
=
currentTime
-
lastTime
;
if
(
leftTime
<=
0
)
{
private
void
doCopy
(
Set
<
TaskAttemptID
>
maps
)
throws
IOException
{
Iterator
<
TaskAttemptID
>
iter
=
maps
.
iterator
(
)
;
while
(
iter
.
hasNext
(
)
)
{
TaskAttemptID
map
=
iter
.
next
(
)
;
private
boolean
copyMapOutput
(
TaskAttemptID
mapTaskId
)
throws
IOException
{
Path
mapOutputFileName
=
localMapFiles
.
get
(
mapTaskId
)
.
getOutputFile
(
)
;
Path
indexFileName
=
mapOutputFileName
.
suffix
(
)
;
SpillRecord
sr
=
new
SpillRecord
(
indexFileName
,
job
)
;
IndexRecord
ir
=
sr
.
getIndex
(
reduce
)
;
long
compressedLength
=
ir
.
partLength
;
long
decompressedLength
=
ir
.
rawLength
;
compressedLength
-=
CryptoUtils
.
cryptoPadding
(
job
)
;
decompressedLength
-=
CryptoUtils
.
cryptoPadding
(
job
)
;
MapOutput
<
K
,
V
>
mapOutput
=
merger
.
reserve
(
mapTaskId
,
decompressedLength
,
id
)
;
if
(
mapOutput
==
null
)
{
@
Override
public
synchronized
MapOutput
<
K
,
V
>
reserve
(
TaskAttemptID
mapId
,
long
requestedSize
,
int
fetcher
)
throws
IOException
{
if
(
requestedSize
>
maxSingleShuffleLimit
)
{
public
synchronized
void
closeInMemoryFile
(
InMemoryMapOutput
<
K
,
V
>
mapOutput
)
{
inMemoryMapOutputs
.
add
(
mapOutput
)
;
public
synchronized
void
closeInMemoryFile
(
InMemoryMapOutput
<
K
,
V
>
mapOutput
)
{
inMemoryMapOutputs
.
add
(
mapOutput
)
;
LOG
.
info
(
+
mapOutput
.
getSize
(
)
+
+
inMemoryMapOutputs
.
size
(
)
+
+
commitMemory
+
+
usedMemory
)
;
commitMemory
+=
mapOutput
.
getSize
(
)
;
if
(
commitMemory
>=
mergeThreshold
)
{
public
synchronized
void
closeInMemoryMergedFile
(
InMemoryMapOutput
<
K
,
V
>
mapOutput
)
{
inMemoryMergedMapOutputs
.
add
(
mapOutput
)
;
private
RawKeyValueIterator
finalMerge
(
JobConf
job
,
FileSystem
fs
,
List
<
InMemoryMapOutput
<
K
,
V
>>
inMemoryMapOutputs
,
List
<
CompressAwarePath
>
onDiskMapOutputs
)
throws
IOException
{
onDiskMapOutputs
.
add
(
new
CompressAwarePath
(
outputPath
,
writer
.
getRawLength
(
)
,
writer
.
getCompressedLength
(
)
)
)
;
writer
=
null
;
}
catch
(
IOException
e
)
{
if
(
null
!=
outputPath
)
{
try
{
fs
.
delete
(
outputPath
,
true
)
;
}
catch
(
IOException
ie
)
{
}
}
throw
e
;
}
finally
{
if
(
null
!=
writer
)
{
writer
.
close
(
)
;
}
}
LOG
.
info
(
+
numMemDiskSegments
+
+
inMemToDiskBytes
+
+
)
;
inMemToDiskBytes
=
0
;
memDiskSegments
.
clear
(
)
;
}
else
if
(
inMemToDiskBytes
!=
0
)
{
}
}
throw
e
;
}
finally
{
if
(
null
!=
writer
)
{
writer
.
close
(
)
;
}
}
LOG
.
info
(
+
numMemDiskSegments
+
+
inMemToDiskBytes
+
+
)
;
inMemToDiskBytes
=
0
;
memDiskSegments
.
clear
(
)
;
}
else
if
(
inMemToDiskBytes
!=
0
)
{
LOG
.
info
(
+
numMemDiskSegments
+
+
inMemToDiskBytes
+
+
)
;
}
}
List
<
Segment
<
K
,
V
>>
diskSegments
=
new
ArrayList
<
Segment
<
K
,
V
>>
(
)
;
long
onDiskBytes
=
inMemToDiskBytes
;
long
rawBytes
=
inMemToDiskBytes
;
CompressAwarePath
[
]
onDisk
=
onDiskMapOutputs
.
toArray
(
new
CompressAwarePath
[
onDiskMapOutputs
.
size
(
)
]
)
;
for
(
CompressAwarePath
file
:
onDisk
)
{
long
fileLength
=
fs
.
getFileStatus
(
file
)
.
getLen
(
)
;
}
finally
{
if
(
null
!=
writer
)
{
writer
.
close
(
)
;
}
}
LOG
.
info
(
+
numMemDiskSegments
+
+
inMemToDiskBytes
+
+
)
;
inMemToDiskBytes
=
0
;
memDiskSegments
.
clear
(
)
;
}
else
if
(
inMemToDiskBytes
!=
0
)
{
LOG
.
info
(
+
numMemDiskSegments
+
+
inMemToDiskBytes
+
+
)
;
}
}
List
<
Segment
<
K
,
V
>>
diskSegments
=
new
ArrayList
<
Segment
<
K
,
V
>>
(
)
;
long
onDiskBytes
=
inMemToDiskBytes
;
long
rawBytes
=
inMemToDiskBytes
;
CompressAwarePath
[
]
onDisk
=
onDiskMapOutputs
.
toArray
(
new
CompressAwarePath
[
onDiskMapOutputs
.
size
(
)
]
)
;
for
(
CompressAwarePath
file
:
onDisk
)
{
long
fileLength
=
fs
.
getFileStatus
(
file
)
.
getLen
(
)
;
onDiskBytes
+=
fileLength
;
@
Override
public
void
resolve
(
TaskCompletionEvent
event
)
{
switch
(
event
.
getTaskStatus
(
)
)
{
case
SUCCEEDED
:
URI
u
=
getBaseURI
(
reduceId
,
event
.
getTaskTrackerHttp
(
)
)
;
addKnownMapOutput
(
u
.
getHost
(
)
+
+
u
.
getPort
(
)
,
u
.
toString
(
)
,
event
.
getTaskAttemptId
(
)
)
;
maxMapRuntime
=
Math
.
max
(
maxMapRuntime
,
event
.
getTaskRunTime
(
)
)
;
break
;
case
FAILED
:
case
KILLED
:
case
OBSOLETE
:
obsoleteMapOutput
(
event
.
getTaskAttemptId
(
)
)
;
@
Override
public
void
resolve
(
TaskCompletionEvent
event
)
{
switch
(
event
.
getTaskStatus
(
)
)
{
case
SUCCEEDED
:
URI
u
=
getBaseURI
(
reduceId
,
event
.
getTaskTrackerHttp
(
)
)
;
addKnownMapOutput
(
u
.
getHost
(
)
+
+
u
.
getPort
(
)
,
u
.
toString
(
)
,
event
.
getTaskAttemptId
(
)
)
;
maxMapRuntime
=
Math
.
max
(
maxMapRuntime
,
event
.
getTaskRunTime
(
)
)
;
break
;
case
FAILED
:
case
KILLED
:
case
OBSOLETE
:
obsoleteMapOutput
(
event
.
getTaskAttemptId
(
)
)
;
LOG
.
info
(
+
event
.
getTaskStatus
(
)
+
+
event
.
getTaskAttemptId
(
)
+
)
;
break
;
case
TIPFAILED
:
tipFailed
(
event
.
getTaskAttemptId
(
)
.
getTaskID
(
)
)
;
output
.
commit
(
)
;
finishedMaps
[
mapIndex
]
=
true
;
shuffledMapsCounter
.
increment
(
1
)
;
if
(
--
remainingMaps
==
0
)
{
notifyAll
(
)
;
}
long
copyMillis
=
(
endMillis
-
startMillis
)
;
if
(
copyMillis
==
0
)
copyMillis
=
1
;
float
bytesPerMillis
=
(
float
)
bytes
/
copyMillis
;
float
transferRate
=
bytesPerMillis
*
BYTES_PER_MILLIS_TO_MBS
;
String
individualProgress
=
+
mapId
+
+
+
mbpsFormat
.
format
(
transferRate
)
+
;
copyTimeTracker
.
add
(
startMillis
,
endMillis
)
;
totalBytesShuffledTillNow
+=
bytes
;
updateStatus
(
individualProgress
)
;
reduceShuffleBytes
.
increment
(
bytes
)
;
lastProgressTime
=
Time
.
monotonicNow
(
)
;
private
void
checkAndInformMRAppMaster
(
int
failures
,
TaskAttemptID
mapId
,
boolean
readError
,
boolean
connectExcpt
,
boolean
hostFailed
)
{
if
(
connectExcpt
||
(
reportReadErrorImmediately
&&
readError
)
||
(
(
failures
%
maxFetchFailuresBeforeReporting
)
==
0
)
||
hostFailed
)
{
Iterator
<
TaskAttemptID
>
itr
=
list
.
iterator
(
)
;
List
<
TaskAttemptID
>
result
=
new
ArrayList
<
TaskAttemptID
>
(
)
;
int
includedMaps
=
0
;
int
totalSize
=
list
.
size
(
)
;
while
(
itr
.
hasNext
(
)
)
{
TaskAttemptID
id
=
itr
.
next
(
)
;
if
(
!
obsoleteMaps
.
contains
(
id
)
&&
!
finishedMaps
[
id
.
getTaskID
(
)
.
getId
(
)
]
)
{
result
.
add
(
id
)
;
if
(
++
includedMaps
>=
MAX_MAPS_AT_ONCE
)
{
break
;
}
}
}
while
(
itr
.
hasNext
(
)
)
{
TaskAttemptID
id
=
itr
.
next
(
)
;
if
(
!
obsoleteMaps
.
contains
(
id
)
&&
!
finishedMaps
[
id
.
getTaskID
(
)
.
getId
(
)
]
)
{
host
.
addKnownMap
(
id
)
;
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
String
run
(
HistoryViewerPrinter
printer
)
throws
Exception
{
ByteArrayOutputStream
boas
=
new
ByteArrayOutputStream
(
)
;
PrintStream
out
=
new
PrintStream
(
boas
,
true
)
;
printer
.
print
(
out
)
;
out
.
close
(
)
;
String
outStr
=
boas
.
toString
(
)
;
private
Job
loadJob
(
JobId
jobId
)
throws
RuntimeException
,
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
report
.
setUser
(
jobInfo
.
getUsername
(
)
)
;
report
.
setDiagnostics
(
jobInfo
.
getErrorInfo
(
)
)
;
if
(
getTotalMaps
(
)
==
0
)
{
report
.
setMapProgress
(
1.0f
)
;
}
else
{
report
.
setMapProgress
(
(
float
)
getCompletedMaps
(
)
/
getTotalMaps
(
)
)
;
}
if
(
getTotalReduces
(
)
==
0
)
{
report
.
setReduceProgress
(
1.0f
)
;
}
else
{
report
.
setReduceProgress
(
(
float
)
getCompletedReduces
(
)
/
getTotalReduces
(
)
)
;
}
report
.
setJobFile
(
getConfFile
(
)
.
toString
(
)
)
;
String
historyUrl
=
;
try
{
historyUrl
=
MRWebAppUtil
.
getApplicationWebURLOnJHSWithScheme
(
conf
,
jobId
.
getAppId
(
)
)
;
}
catch
(
UnknownHostException
e
)
{
protected
synchronized
void
loadFullHistoryData
(
boolean
loadTasks
,
Path
historyFileAbsolute
)
throws
IOException
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
private
void
mkdir
(
FileContext
fc
,
Path
path
,
FsPermission
fsp
)
throws
IOException
{
if
(
!
fc
.
util
(
)
.
exists
(
path
)
)
{
try
{
fc
.
mkdir
(
path
,
fsp
,
true
)
;
FileStatus
fsStatus
=
fc
.
getFileStatus
(
path
)
;
LOG
.
info
(
+
fsStatus
.
getPermission
(
)
.
toShort
(
)
+
+
fsp
.
toShort
(
)
)
;
if
(
fsStatus
.
getPermission
(
)
.
toShort
(
)
!=
fsp
.
toShort
(
)
)
{
@
SuppressWarnings
(
)
void
initExisting
(
)
throws
IOException
{
LOG
.
info
(
)
;
List
<
FileStatus
>
timestampedDirList
=
findTimestampedDirectories
(
)
;
Collections
.
sort
(
timestampedDirList
)
;
List
<
FileStatus
>
timestampedDirList
=
findTimestampedDirectories
(
)
;
Collections
.
sort
(
timestampedDirList
)
;
LOG
.
info
(
+
timestampedDirList
.
size
(
)
+
)
;
for
(
FileStatus
fs
:
timestampedDirList
)
{
addDirectoryToSerialNumberIndex
(
fs
.
getPath
(
)
)
;
}
final
double
maxCacheSize
=
(
double
)
jobListCache
.
maxSize
;
int
prevCacheSize
=
jobListCache
.
size
(
)
;
for
(
int
i
=
timestampedDirList
.
size
(
)
-
1
;
i
>=
0
&&
!
jobListCache
.
isFull
(
)
;
i
--
)
{
FileStatus
fs
=
timestampedDirList
.
get
(
i
)
;
addDirectoryToJobListCache
(
fs
.
getPath
(
)
)
;
int
currCacheSize
=
jobListCache
.
size
(
)
;
if
(
(
currCacheSize
-
prevCacheSize
)
/
maxCacheSize
>=
0.05
)
{
LOG
.
info
(
currCacheSize
*
100.0
/
maxCacheSize
+
)
;
}
prevCacheSize
=
currCacheSize
;
}
final
double
loadedPercent
=
maxCacheSize
==
0.0
?
100
:
prevCacheSize
*
100.0
/
maxCacheSize
;
private
void
addDirectoryToSerialNumberIndex
(
Path
serialDirPath
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
addDirectoryToJobListCache
(
Path
path
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
scanIntermediateDirectory
(
final
Path
absPath
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
fs
.
getPath
(
)
)
;
}
JobIndexInfo
jobIndexInfo
=
FileNameIndexUtils
.
getIndexInfo
(
fs
.
getPath
(
)
.
getName
(
)
)
;
String
confFileName
=
JobHistoryUtils
.
getIntermediateConfFileName
(
jobIndexInfo
.
getJobId
(
)
)
;
String
summaryFileName
=
JobHistoryUtils
.
getIntermediateSummaryFileName
(
jobIndexInfo
.
getJobId
(
)
)
;
HistoryFileInfo
fileInfo
=
createHistoryFileInfo
(
fs
.
getPath
(
)
,
new
Path
(
fs
.
getPath
(
)
.
getParent
(
)
,
confFileName
)
,
new
Path
(
fs
.
getPath
(
)
.
getParent
(
)
,
summaryFileName
)
,
jobIndexInfo
,
false
)
;
final
HistoryFileInfo
old
=
jobListCache
.
addIfAbsent
(
fileInfo
)
;
if
(
old
==
null
||
old
.
didMoveFail
(
)
)
{
final
HistoryFileInfo
found
=
(
old
==
null
)
?
fileInfo
:
old
;
long
cutoff
=
System
.
currentTimeMillis
(
)
-
maxHistoryAge
;
if
(
found
.
getJobIndexInfo
(
)
.
getFinishTime
(
)
<=
cutoff
)
{
try
{
found
.
delete
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
e
)
;
HistoryFileInfo
fileInfo
=
createHistoryFileInfo
(
fs
.
getPath
(
)
,
new
Path
(
fs
.
getPath
(
)
.
getParent
(
)
,
confFileName
)
,
new
Path
(
fs
.
getPath
(
)
.
getParent
(
)
,
summaryFileName
)
,
jobIndexInfo
,
false
)
;
final
HistoryFileInfo
old
=
jobListCache
.
addIfAbsent
(
fileInfo
)
;
if
(
old
==
null
||
old
.
didMoveFail
(
)
)
{
final
HistoryFileInfo
found
=
(
old
==
null
)
?
fileInfo
:
old
;
long
cutoff
=
System
.
currentTimeMillis
(
)
-
maxHistoryAge
;
if
(
found
.
getJobIndexInfo
(
)
.
getFinishTime
(
)
<=
cutoff
)
{
try
{
found
.
delete
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
e
)
;
}
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
found
)
;
}
moveToDoneExecutor
.
execute
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
private
void
moveToDoneNow
(
final
Path
src
,
final
Path
target
)
throws
IOException
{
@
Override
public
void
storeToken
(
MRDelegationTokenIdentifier
tokenId
,
Long
renewDate
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
updateToken
(
MRDelegationTokenIdentifier
tokenId
,
Long
renewDate
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
removeToken
(
MRDelegationTokenIdentifier
tokenId
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
storeTokenMasterKey
(
DelegationKey
key
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
removeTokenMasterKey
(
DelegationKey
key
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
loadTokenState
(
HistoryServerState
state
)
throws
IOException
{
int
numKeys
=
loadKeys
(
state
)
;
int
numTokens
=
loadTokens
(
state
)
;
@
Override
protected
void
startStorage
(
)
throws
IOException
{
Path
storeRoot
=
createStorageDir
(
getConfig
(
)
)
;
Options
options
=
new
Options
(
)
;
options
.
createIfMissing
(
false
)
;
@
Override
public
HistoryServerState
loadState
(
)
throws
IOException
{
HistoryServerState
state
=
new
HistoryServerState
(
)
;
int
numKeys
=
loadTokenMasterKeys
(
state
)
;
@
Override
public
HistoryServerState
loadState
(
)
throws
IOException
{
HistoryServerState
state
=
new
HistoryServerState
(
)
;
int
numKeys
=
loadTokenMasterKeys
(
state
)
;
LOG
.
info
(
+
numKeys
+
)
;
int
numTokens
=
loadTokens
(
state
)
;
@
Override
public
void
storeToken
(
MRDelegationTokenIdentifier
tokenId
,
Long
renewDate
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
storeTokenMasterKey
(
DelegationKey
masterKey
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
removeTokenMasterKey
(
DelegationKey
masterKey
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
checkVersion
(
)
throws
IOException
{
Version
loadedVersion
=
loadVersion
(
)
;
@
Override
protected
void
storeNewMasterKey
(
DelegationKey
key
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
removeStoredMasterKey
(
DelegationKey
key
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
storeNewToken
(
MRDelegationTokenIdentifier
tokenId
,
long
renewDate
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
removeStoredToken
(
MRDelegationTokenIdentifier
tokenId
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
updateStoredToken
(
MRDelegationTokenIdentifier
tokenId
,
long
renewDate
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
Map
<
JobId
,
Job
>
getAllJobs
(
ApplicationId
appID
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
amStartTimeEst
=
System
.
currentTimeMillis
(
)
;
conf
.
setClass
(
NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY
,
MyResolver
.
class
,
DNSToSwitchMapping
.
class
)
;
RackResolver
.
init
(
conf
)
;
MRApp
app
=
new
MRAppWithHistory
(
numMaps
,
numReduces
,
true
,
this
.
getClass
(
)
.
getName
(
)
,
true
)
;
app
.
submit
(
conf
)
;
Job
job
=
app
.
getContext
(
)
.
getAllJobs
(
)
.
values
(
)
.
iterator
(
)
.
next
(
)
;
JobId
jobId
=
job
.
getID
(
)
;
LOG
.
info
(
+
TypeConverter
.
fromYarn
(
jobId
)
.
toString
(
)
)
;
app
.
waitForState
(
job
,
JobState
.
SUCCEEDED
)
;
app
.
waitForState
(
Service
.
STATE
.
STOPPED
)
;
String
jobhistoryDir
=
JobHistoryUtils
.
getHistoryIntermediateDoneDirForUser
(
conf
)
;
FileContext
fc
=
null
;
try
{
fc
=
FileContext
.
getFileContext
(
conf
)
;
}
catch
(
IOException
ioe
)
{
Assert
.
assertTrue
(
,
Long
.
parseLong
(
jobSummaryElements
.
get
(
)
)
!=
0
)
;
Assert
.
assertEquals
(
,
numSuccessfulMaps
,
Integer
.
parseInt
(
jobSummaryElements
.
get
(
)
)
)
;
Assert
.
assertEquals
(
,
numReduces
,
Integer
.
parseInt
(
jobSummaryElements
.
get
(
)
)
)
;
Assert
.
assertEquals
(
,
System
.
getProperty
(
)
,
jobSummaryElements
.
get
(
)
)
;
Assert
.
assertEquals
(
,
,
jobSummaryElements
.
get
(
)
)
;
Assert
.
assertEquals
(
,
,
jobSummaryElements
.
get
(
)
)
;
}
JobHistory
jobHistory
=
new
JobHistory
(
)
;
jobHistory
.
init
(
conf
)
;
HistoryFileInfo
fileInfo
=
jobHistory
.
getJobFileInfo
(
jobId
)
;
JobInfo
jobInfo
;
long
numFinishedMaps
;
synchronized
(
fileInfo
)
{
Path
historyFilePath
=
fileInfo
.
getHistoryFile
(
)
;
FSDataInputStream
in
=
null
;
LOG
.
info
(
+
historyFilePath
)
;
app
.
waitForState
(
job
,
JobState
.
SUCCEEDED
)
;
app
.
waitForState
(
Service
.
STATE
.
STOPPED
)
;
JobHistory
jobHistory
=
new
JobHistory
(
)
;
jobHistory
.
init
(
conf
)
;
HistoryFileInfo
fileInfo
=
jobHistory
.
getJobFileInfo
(
jobId
)
;
JobHistoryParser
parser
;
JobInfo
jobInfo
;
synchronized
(
fileInfo
)
{
Path
historyFilePath
=
fileInfo
.
getHistoryFile
(
)
;
FSDataInputStream
in
=
null
;
FileContext
fc
=
null
;
try
{
fc
=
FileContext
.
getFileContext
(
conf
)
;
in
=
fc
.
open
(
fc
.
makeQualified
(
historyFilePath
)
)
;
}
catch
(
IOException
ioe
)
{
app
.
waitForState
(
job
,
JobState
.
SUCCEEDED
)
;
app
.
waitForState
(
Service
.
STATE
.
STOPPED
)
;
jobHistory
=
new
JobHistory
(
)
;
jobHistory
.
init
(
conf
)
;
HistoryFileInfo
fileInfo
=
jobHistory
.
getJobFileInfo
(
jobId
)
;
JobHistoryParser
parser
;
JobInfo
jobInfo
;
synchronized
(
fileInfo
)
{
Path
historyFilePath
=
fileInfo
.
getHistoryFile
(
)
;
FSDataInputStream
in
=
null
;
FileContext
fc
=
null
;
try
{
fc
=
FileContext
.
getFileContext
(
conf
)
;
in
=
fc
.
open
(
fc
.
makeQualified
(
historyFilePath
)
)
;
}
catch
(
IOException
ioe
)
{
app
.
waitForState
(
job
,
JobState
.
FAILED
)
;
app
.
waitForState
(
Service
.
STATE
.
STOPPED
)
;
JobHistory
jobHistory
=
new
JobHistory
(
)
;
jobHistory
.
init
(
conf
)
;
HistoryFileInfo
fileInfo
=
jobHistory
.
getJobFileInfo
(
jobId
)
;
JobHistoryParser
parser
;
JobInfo
jobInfo
;
synchronized
(
fileInfo
)
{
Path
historyFilePath
=
fileInfo
.
getHistoryFile
(
)
;
FSDataInputStream
in
=
null
;
FileContext
fc
=
null
;
try
{
fc
=
FileContext
.
getFileContext
(
conf
)
;
in
=
fc
.
open
(
fc
.
makeQualified
(
historyFilePath
)
)
;
}
catch
(
IOException
ioe
)
{
app
.
waitForState
(
job
,
JobState
.
KILLED
)
;
app
.
waitForState
(
Service
.
STATE
.
STOPPED
)
;
JobHistory
jobHistory
=
new
JobHistory
(
)
;
jobHistory
.
init
(
conf
)
;
HistoryFileInfo
fileInfo
=
jobHistory
.
getJobFileInfo
(
jobId
)
;
JobHistoryParser
parser
;
JobInfo
jobInfo
;
synchronized
(
fileInfo
)
{
Path
historyFilePath
=
fileInfo
.
getHistoryFile
(
)
;
FSDataInputStream
in
=
null
;
FileContext
fc
=
null
;
try
{
fc
=
FileContext
.
getFileContext
(
conf
)
;
in
=
fc
.
open
(
fc
.
makeQualified
(
historyFilePath
)
)
;
}
catch
(
IOException
ioe
)
{
@
Test
public
void
testTaskAttemptUnsuccessfulCompletionWithoutCounters203
(
)
throws
IOException
{
Path
histPath
=
new
Path
(
getClass
(
)
.
getClassLoader
(
)
.
getResource
(
)
.
getFile
(
)
)
;
JobHistoryParser
parser
=
new
JobHistoryParser
(
FileSystem
.
getLocal
(
new
Configuration
(
)
)
,
histPath
)
;
JobInfo
jobInfo
=
parser
.
parse
(
)
;
@
Test
public
void
testTaskAttemptUnsuccessfulCompletionWithoutCounters240
(
)
throws
IOException
{
Path
histPath
=
new
Path
(
getClass
(
)
.
getClassLoader
(
)
.
getResource
(
)
.
getFile
(
)
)
;
JobHistoryParser
parser
=
new
JobHistoryParser
(
FileSystem
.
getLocal
(
new
Configuration
(
)
)
,
histPath
)
;
JobInfo
jobInfo
=
parser
.
parse
(
)
;
@
Test
public
void
testTaskAttemptUnsuccessfulCompletionWithoutCounters0239
(
)
throws
IOException
{
Path
histPath
=
new
Path
(
getClass
(
)
.
getClassLoader
(
)
.
getResource
(
)
.
getFile
(
)
)
;
JobHistoryParser
parser
=
new
JobHistoryParser
(
FileSystem
.
getLocal
(
new
Configuration
(
)
)
,
histPath
)
;
JobInfo
jobInfo
=
parser
.
parse
(
)
;
application
=
rm
.
getApplicationReport
(
appId
)
;
}
catch
(
ApplicationNotFoundException
e
)
{
application
=
null
;
}
catch
(
YarnException
e2
)
{
throw
new
IOException
(
e2
)
;
}
if
(
application
!=
null
)
{
trackingUrl
=
application
.
getTrackingUrl
(
)
;
}
InetSocketAddress
serviceAddr
=
null
;
while
(
application
==
null
||
YarnApplicationState
.
RUNNING
==
application
.
getYarnApplicationState
(
)
)
{
if
(
application
==
null
)
{
LOG
.
info
(
+
jobId
+
)
;
return
checkAndGetHSProxy
(
null
,
JobState
.
NEW
)
;
}
try
{
if
(
application
.
getHost
(
)
==
null
||
.
equals
(
application
.
getHost
(
)
)
)
{
LOG
.
debug
(
)
;
catch
(
YarnException
e2
)
{
throw
new
IOException
(
e2
)
;
}
if
(
application
!=
null
)
{
trackingUrl
=
application
.
getTrackingUrl
(
)
;
}
InetSocketAddress
serviceAddr
=
null
;
while
(
application
==
null
||
YarnApplicationState
.
RUNNING
==
application
.
getYarnApplicationState
(
)
)
{
if
(
application
==
null
)
{
LOG
.
info
(
+
jobId
+
)
;
return
checkAndGetHSProxy
(
null
,
JobState
.
NEW
)
;
}
try
{
if
(
application
.
getHost
(
)
==
null
||
.
equals
(
application
.
getHost
(
)
)
)
{
LOG
.
debug
(
)
;
Thread
.
sleep
(
2000
)
;
LOG
.
debug
(
+
application
.
getYarnApplicationState
(
)
)
;
application
=
rm
.
getApplicationReport
(
appId
)
;
}
try
{
if
(
application
.
getHost
(
)
==
null
||
.
equals
(
application
.
getHost
(
)
)
)
{
LOG
.
debug
(
)
;
Thread
.
sleep
(
2000
)
;
LOG
.
debug
(
+
application
.
getYarnApplicationState
(
)
)
;
application
=
rm
.
getApplicationReport
(
appId
)
;
continue
;
}
else
if
(
UNAVAILABLE
.
equals
(
application
.
getHost
(
)
)
)
{
if
(
!
amAclDisabledStatusLogged
)
{
LOG
.
info
(
+
jobId
+
+
)
;
amAclDisabledStatusLogged
=
true
;
}
return
getNotRunningJob
(
application
,
JobState
.
RUNNING
)
;
}
if
(
!
conf
.
getBoolean
(
MRJobConfig
.
JOB_AM_ACCESS_DISABLED
,
false
)
)
{
UserGroupInformation
newUgi
=
UserGroupInformation
.
createRemoteUser
(
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
)
;
serviceAddr
=
NetUtils
.
createSocketAddrForHost
(
application
.
getHost
(
)
,
application
.
getRpcPort
(
)
)
;
continue
;
}
else
if
(
UNAVAILABLE
.
equals
(
application
.
getHost
(
)
)
)
{
if
(
!
amAclDisabledStatusLogged
)
{
LOG
.
info
(
+
jobId
+
+
)
;
amAclDisabledStatusLogged
=
true
;
}
return
getNotRunningJob
(
application
,
JobState
.
RUNNING
)
;
}
if
(
!
conf
.
getBoolean
(
MRJobConfig
.
JOB_AM_ACCESS_DISABLED
,
false
)
)
{
UserGroupInformation
newUgi
=
UserGroupInformation
.
createRemoteUser
(
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
)
;
serviceAddr
=
NetUtils
.
createSocketAddrForHost
(
application
.
getHost
(
)
,
application
.
getRpcPort
(
)
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
clientToAMToken
=
application
.
getClientToAMToken
(
)
;
Token
<
ClientToAMTokenIdentifier
>
token
=
ConverterUtils
.
convertFromYarn
(
clientToAMToken
,
serviceAddr
)
;
newUgi
.
addToken
(
token
)
;
}
LOG
.
debug
(
+
serviceAddr
)
;
final
InetSocketAddress
finalServiceAddr
=
serviceAddr
;
amAclDisabledStatusLogged
=
true
;
}
return
getNotRunningJob
(
application
,
JobState
.
RUNNING
)
;
}
if
(
!
conf
.
getBoolean
(
MRJobConfig
.
JOB_AM_ACCESS_DISABLED
,
false
)
)
{
UserGroupInformation
newUgi
=
UserGroupInformation
.
createRemoteUser
(
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
)
;
serviceAddr
=
NetUtils
.
createSocketAddrForHost
(
application
.
getHost
(
)
,
application
.
getRpcPort
(
)
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
clientToAMToken
=
application
.
getClientToAMToken
(
)
;
Token
<
ClientToAMTokenIdentifier
>
token
=
ConverterUtils
.
convertFromYarn
(
clientToAMToken
,
serviceAddr
)
;
newUgi
.
addToken
(
token
)
;
}
LOG
.
debug
(
+
serviceAddr
)
;
final
InetSocketAddress
finalServiceAddr
=
serviceAddr
;
realProxy
=
newUgi
.
doAs
(
new
PrivilegedExceptionAction
<
MRClientProtocol
>
(
)
{
@
Override
public
MRClientProtocol
run
(
)
throws
IOException
{
return
instantiateAMProxy
(
finalServiceAddr
)
;
}
}
)
;
}
LOG
.
debug
(
+
serviceAddr
)
;
final
InetSocketAddress
finalServiceAddr
=
serviceAddr
;
realProxy
=
newUgi
.
doAs
(
new
PrivilegedExceptionAction
<
MRClientProtocol
>
(
)
{
@
Override
public
MRClientProtocol
run
(
)
throws
IOException
{
return
instantiateAMProxy
(
finalServiceAddr
)
;
}
}
)
;
}
else
{
if
(
!
amAclDisabledStatusLogged
)
{
LOG
.
info
(
+
jobId
+
)
;
amAclDisabledStatusLogged
=
true
;
}
return
getNotRunningJob
(
null
,
JobState
.
RUNNING
)
;
}
return
realProxy
;
}
catch
(
IOException
e
)
{
LOG
.
info
(
+
serviceAddr
+
)
;
try
{
Method
methodOb
=
null
;
try
{
methodOb
=
MRClientProtocol
.
class
.
getMethod
(
method
,
argClass
)
;
}
catch
(
SecurityException
e
)
{
throw
new
YarnRuntimeException
(
e
)
;
}
catch
(
NoSuchMethodException
e
)
{
throw
new
YarnRuntimeException
(
,
e
)
;
}
maxClientRetry
=
this
.
conf
.
getInt
(
MRJobConfig
.
MR_CLIENT_MAX_RETRIES
,
MRJobConfig
.
DEFAULT_MR_CLIENT_MAX_RETRIES
)
;
IOException
lastException
=
null
;
while
(
maxClientRetry
>
0
)
{
MRClientProtocol
MRClientProxy
=
null
;
try
{
MRClientProxy
=
getProxy
(
)
;
return
methodOb
.
invoke
(
MRClientProxy
,
args
)
;
}
catch
(
InvocationTargetException
e
)
{
try
{
MRClientProxy
=
getProxy
(
)
;
return
methodOb
.
invoke
(
MRClientProxy
,
args
)
;
}
catch
(
InvocationTargetException
e
)
{
LOG
.
debug
(
+
jobId
+
,
e
.
getTargetException
(
)
)
;
realProxy
=
null
;
if
(
e
.
getCause
(
)
instanceof
AuthorizationException
)
{
throw
new
IOException
(
e
.
getTargetException
(
)
)
;
}
if
(
!
usingAMProxy
.
get
(
)
)
{
maxClientRetry
--
;
}
usingAMProxy
.
set
(
false
)
;
lastException
=
new
IOException
(
e
.
getTargetException
(
)
)
;
try
{
Thread
.
sleep
(
100
)
;
}
catch
(
InterruptedException
ie
)
{
public
String
getStagingAreaDir
(
)
throws
IOException
,
InterruptedException
{
String
user
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
Path
path
=
MRApps
.
getStagingAreaDir
(
conf
,
user
)
;
private
Map
<
String
,
LocalResource
>
setupLocalResources
(
Configuration
jobConf
,
String
jobSubmitDir
)
throws
IOException
{
Map
<
String
,
LocalResource
>
localResources
=
new
HashMap
<
>
(
)
;
Path
jobConfPath
=
new
Path
(
jobSubmitDir
,
MRJobConfig
.
JOB_CONF_FILE
)
;
URL
yarnUrlForJobSubmitDir
=
URL
.
fromPath
(
defaultFileContext
.
getDefaultFileSystem
(
)
.
resolvePath
(
defaultFileContext
.
makeQualified
(
new
Path
(
jobSubmitDir
)
)
)
)
;
if
(
regex
!=
null
&&
!
regex
.
isEmpty
(
)
)
{
setTokenRenewerConf
(
amContainer
,
conf
,
regex
)
;
}
Collection
<
String
>
tagsFromConf
=
jobConf
.
getTrimmedStringCollection
(
MRJobConfig
.
JOB_TAGS
)
;
ApplicationSubmissionContext
appContext
=
recordFactory
.
newRecordInstance
(
ApplicationSubmissionContext
.
class
)
;
appContext
.
setApplicationId
(
applicationId
)
;
appContext
.
setQueue
(
jobConf
.
get
(
JobContext
.
QUEUE_NAME
,
YarnConfiguration
.
DEFAULT_QUEUE_NAME
)
)
;
ReservationId
reservationID
=
null
;
try
{
reservationID
=
ReservationId
.
parseReservationId
(
jobConf
.
get
(
JobContext
.
RESERVATION_ID
)
)
;
}
catch
(
NumberFormatException
e
)
{
String
errMsg
=
+
jobConf
.
get
(
JobContext
.
RESERVATION_ID
)
+
+
applicationId
;
LOG
.
warn
(
errMsg
)
;
throw
new
IOException
(
errMsg
)
;
}
if
(
reservationID
!=
null
)
{
appContext
.
setReservationID
(
reservationID
)
;
LOG
.
warn
(
+
MR_AM_RESOURCE_PREFIX
+
resourceName
+
+
resourceReq
.
getValue
(
)
+
resourceReq
.
getUnits
(
)
+
+
MRJobConfig
.
MR_AM_VMEM_MB
+
+
conf
.
get
(
MRJobConfig
.
MR_AM_VMEM_MB
)
+
)
;
}
}
else
if
(
MRJobConfig
.
RESOURCE_TYPE_NAME_VCORE
.
equals
(
resourceName
)
)
{
capability
.
setVirtualCores
(
(
int
)
UnitsConversionUtil
.
convert
(
resourceReq
.
getUnits
(
)
,
,
resourceReq
.
getValue
(
)
)
)
;
cpuVcoresSet
=
true
;
if
(
conf
.
get
(
MRJobConfig
.
MR_AM_CPU_VCORES
)
!=
null
)
{
LOG
.
warn
(
+
MR_AM_RESOURCE_PREFIX
+
resourceName
+
+
resourceReq
.
getValue
(
)
+
resourceReq
.
getUnits
(
)
+
+
MRJobConfig
.
MR_AM_CPU_VCORES
+
+
conf
.
get
(
MRJobConfig
.
MR_AM_CPU_VCORES
)
+
)
;
}
}
else
if
(
!
MRJobConfig
.
MR_AM_VMEM_MB
.
equals
(
MR_AM_RESOURCE_PREFIX
+
resourceName
)
&&
!
MRJobConfig
.
MR_AM_CPU_VCORES
.
equals
(
MR_AM_RESOURCE_PREFIX
+
resourceName
)
)
{
ResourceInformation
resourceInformation
=
capability
.
getResourceInformation
(
resourceName
)
;
resourceInformation
.
setUnits
(
resourceReq
.
getUnits
(
)
)
;
resourceInformation
.
setValue
(
resourceReq
.
getValue
(
)
)
;
capability
.
setResourceInformation
(
resourceName
,
resourceInformation
)
;
}
}
if
(
!
memorySet
)
{
capability
.
setMemorySize
(
conf
.
getInt
(
MRJobConfig
.
MR_AM_VMEM_MB
,
MRJobConfig
.
DEFAULT_MR_AM_VMEM_MB
)
)
;
}
if
(
!
cpuVcoresSet
)
{
capability
.
setVirtualCores
(
conf
.
getInt
(
MRJobConfig
.
MR_AM_CPU_VCORES
,
MRJobConfig
.
DEFAULT_MR_AM_CPU_VCORES
)
)
;
Configuration
copy
=
new
Configuration
(
false
)
;
copy
.
clear
(
)
;
int
count
=
0
;
for
(
Map
.
Entry
<
String
,
String
>
map
:
conf
)
{
String
key
=
map
.
getKey
(
)
;
String
val
=
map
.
getValue
(
)
;
if
(
key
.
matches
(
regex
)
)
{
copy
.
set
(
key
,
val
)
;
count
++
;
}
}
copy
.
write
(
dob
)
;
ByteBuffer
appConf
=
ByteBuffer
.
wrap
(
dob
.
getData
(
)
,
0
,
dob
.
getLength
(
)
)
;
LOG
.
info
(
+
regex
+
+
count
+
+
dob
.
getLength
(
)
+
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
for
(
Iterator
<
Map
.
Entry
<
String
,
String
>>
itor
=
copy
.
iterator
(
)
;
itor
.
hasNext
(
)
;
)
{
Map
.
Entry
<
String
,
String
>
entry
=
itor
.
next
(
)
;
protected
static
float
getProbability
(
final
String
klass
)
{
String
newProbName
=
FPROB_NAME
+
klass
;
String
newValue
=
System
.
getProperty
(
newProbName
,
conf
.
get
(
ALL_PROBABILITIES
)
)
;
if
(
newValue
!=
null
&&
!
newValue
.
equals
(
conf
.
get
(
newProbName
)
)
)
conf
.
set
(
newProbName
,
newValue
)
;
float
ret
=
conf
.
getFloat
(
newProbName
,
conf
.
getFloat
(
ALL_PROBABILITIES
,
DEFAULT_PROB
)
)
;
private
static
void
createControlFile
(
FileSystem
fs
,
int
fileSize
,
int
nrFiles
)
throws
IOException
{
testType
=
TEST_TYPE_READ
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
testType
=
TEST_TYPE_WRITE
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
testType
=
TEST_TYPE_CLEANUP
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
isSequential
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
nrFiles
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fileSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
resFileName
=
args
[
++
i
]
;
testType
=
TEST_TYPE_READ
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
testType
=
TEST_TYPE_WRITE
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
testType
=
TEST_TYPE_CLEANUP
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
isSequential
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
nrFiles
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fileSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
resFileName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
testType
=
TEST_TYPE_WRITE
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
testType
=
TEST_TYPE_CLEANUP
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
isSequential
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
nrFiles
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fileSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
resFileName
=
args
[
++
i
]
;
}
}
LOG
.
info
(
+
nrFiles
)
;
float
sqrate
=
0
;
String
line
;
while
(
(
line
=
lines
.
readLine
(
)
)
!=
null
)
{
StringTokenizer
tokens
=
new
StringTokenizer
(
line
,
)
;
String
attr
=
tokens
.
nextToken
(
)
;
if
(
attr
.
endsWith
(
)
)
tasks
=
Long
.
parseLong
(
tokens
.
nextToken
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
size
=
Long
.
parseLong
(
tokens
.
nextToken
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
time
=
Long
.
parseLong
(
tokens
.
nextToken
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
rate
=
Float
.
parseFloat
(
tokens
.
nextToken
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
sqrate
=
Float
.
parseFloat
(
tokens
.
nextToken
(
)
)
;
}
double
med
=
rate
/
1000
/
tasks
;
double
stdDev
=
Math
.
sqrt
(
Math
.
abs
(
sqrate
/
1000
/
tasks
-
med
*
med
)
)
;
String
resultLines
[
]
=
{
+
(
(
testType
==
TEST_TYPE_WRITE
)
?
:
(
testType
==
TEST_TYPE_READ
)
?
:
)
,
+
new
Date
(
System
.
currentTimeMillis
(
)
)
,
+
tasks
,
+
size
/
MEGA
,
+
size
*
1000.0
/
(
time
*
MEGA
)
,
+
med
,
+
stdDev
,
+
(
float
)
execTime
/
1000
,
}
;
PrintStream
res
=
new
PrintStream
(
new
FileOutputStream
(
new
File
(
resFileName
)
,
true
)
)
;
for
(
int
i
=
0
;
i
<
resultLines
.
length
;
i
++
)
{
if
(
args
.
length
==
1
&&
args
[
0
]
.
startsWith
(
)
)
{
System
.
err
.
println
(
usage
)
;
System
.
exit
(
-
1
)
;
}
for
(
int
i
=
0
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
.
equals
(
)
)
{
rootName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
testType
=
TEST_TYPE_CLEANUP
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
resFileName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
startsWith
(
)
)
{
viewStats
=
true
;
}
}
LOG
.
info
(
+
rootName
)
;
try
{
for
(
int
i
=
start
;
i
<
end
;
i
++
)
{
String
name
=
getFileName
(
i
)
;
Path
controlFile
=
new
Path
(
INPUT_DIR
,
+
name
)
;
SequenceFile
.
Writer
writer
=
null
;
try
{
writer
=
SequenceFile
.
createWriter
(
fs
,
fs
.
getConf
(
)
,
controlFile
,
Text
.
class
,
LongWritable
.
class
,
CompressionType
.
NONE
)
;
String
logFile
=
jhLogFiles
[
i
]
.
getPath
(
)
.
toString
(
)
;
writer
.
append
(
new
Text
(
logFile
)
,
new
LongWritable
(
0
)
)
;
}
catch
(
Exception
e
)
{
throw
new
IOException
(
e
)
;
}
finally
{
if
(
writer
!=
null
)
writer
.
close
(
)
;
writer
=
null
;
}
}
}
catch
(
IOException
ex
)
{
int
start
=
0
;
int
step
=
jhLogFiles
.
length
/
NUM_CREATE_THREADS
+
(
(
jhLogFiles
.
length
%
NUM_CREATE_THREADS
)
>
0
?
1
:
0
)
;
FileCreateDaemon
[
]
daemons
=
new
FileCreateDaemon
[
NUM_CREATE_THREADS
]
;
numRunningThreads
=
0
;
for
(
int
tIdx
=
0
;
tIdx
<
NUM_CREATE_THREADS
&&
start
<
jhLogFiles
.
length
;
tIdx
++
)
{
int
end
=
Math
.
min
(
start
+
step
,
jhLogFiles
.
length
)
;
daemons
[
tIdx
]
=
new
FileCreateDaemon
(
fs
,
start
,
end
)
;
start
+=
step
;
numRunningThreads
++
;
}
for
(
int
tIdx
=
0
;
tIdx
<
numRunningThreads
;
tIdx
++
)
{
daemons
[
tIdx
]
.
start
(
)
;
}
}
finally
{
int
prevValue
=
0
;
while
(
numFinishedThreads
<
numRunningThreads
)
{
if
(
prevValue
<
numFinishedThreads
)
{
private
static
void
createControlFile
(
FileSystem
fs
,
Path
jhLogDir
)
throws
IOException
{
private
static
void
createControlFile
(
FileSystem
fs
,
Path
jhLogDir
)
throws
IOException
{
LOG
.
info
(
+
jhLogDir
)
;
FileCreateDaemon
.
createControlFile
(
fs
,
jhLogDir
)
;
@
SuppressWarnings
(
)
private
void
createControlFile
(
FileSystem
fs
,
long
nrBytes
,
int
nrFiles
)
throws
IOException
{
@
Override
public
int
run
(
String
[
]
args
)
throws
IOException
{
TestType
testType
=
null
;
int
bufferSize
=
DEFAULT_BUFFER_SIZE
;
long
nrBytes
=
1
*
MEGA
;
String
erasureCodePolicyName
=
null
;
int
nrFiles
=
1
;
long
skipSize
=
0
;
String
resFileName
=
DEFAULT_RES_FILE_NAME
;
String
compressionClass
=
null
;
String
storagePolicy
=
null
;
boolean
isSequential
=
false
;
String
version
=
TestDFSIO
.
class
.
getSimpleName
(
)
+
;
compressionClass
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
nrFiles
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
||
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
nrBytes
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
skipSize
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
resFileName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
storagePolicy
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
erasureCodePolicyName
=
args
[
++
i
]
;
compressionClass
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
nrFiles
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
||
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
nrBytes
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
skipSize
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
resFileName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
storagePolicy
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
erasureCodePolicyName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
nrFiles
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
||
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
nrBytes
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
skipSize
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
resFileName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
storagePolicy
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
erasureCodePolicyName
=
args
[
++
i
]
;
}
else
{
nrFiles
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
||
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
nrBytes
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
skipSize
=
parseSize
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
bufferSize
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
resFileName
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
storagePolicy
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equalsIgnoreCase
(
)
)
{
erasureCodePolicyName
=
args
[
++
i
]
;
}
else
{
System
.
err
.
println
(
+
args
[
i
]
)
;
boolean
isValid
=
false
;
for
(
ErasureCodingPolicyInfo
ec
:
list
)
{
if
(
erasureCodePolicyName
.
equals
(
ec
.
getPolicy
(
)
.
getName
(
)
)
)
{
isValid
=
true
;
break
;
}
}
if
(
!
isValid
)
{
System
.
out
.
println
(
+
erasureCodePolicyName
)
;
System
.
out
.
println
(
)
;
for
(
ErasureCodingPolicyInfo
ec
:
list
)
{
System
.
out
.
println
(
ec
.
getPolicy
(
)
.
getName
(
)
)
;
}
return
false
;
}
if
(
testType
==
TestType
.
TEST_TYPE_APPEND
||
testType
==
TestType
.
TEST_TYPE_TRUNCATE
)
{
System
.
out
.
println
(
+
)
;
return
false
;
}
config
.
set
(
ERASURE_CODE_POLICY_NAME_KEY
,
erasureCodePolicyName
)
;
Collection
<
BlockStoragePolicy
>
storagePolicies
=
(
(
DistributedFileSystem
)
fs
)
.
getAllStoragePolicies
(
)
;
try
{
for
(
BlockStoragePolicy
policy
:
storagePolicies
)
{
if
(
policy
.
getName
(
)
.
equals
(
storagePolicy
)
)
{
isValid
=
true
;
break
;
}
}
}
catch
(
Exception
e
)
{
throw
new
IOException
(
,
e
)
;
}
if
(
!
isValid
)
{
System
.
out
.
println
(
+
storagePolicy
)
;
System
.
out
.
println
(
)
;
for
(
BlockStoragePolicy
policy
:
storagePolicies
)
{
System
.
out
.
println
(
policy
.
getName
(
)
)
;
}
return
false
;
}
config
.
set
(
STORAGE_POLICY_NAME_KEY
,
storagePolicy
)
;
void
createAndEnableECOnPath
(
FileSystem
fs
,
Path
path
)
throws
IOException
{
String
erasureCodePolicyName
=
getConf
(
)
.
get
(
ERASURE_CODE_POLICY_NAME_KEY
,
null
)
;
fs
.
mkdirs
(
path
)
;
Collection
<
ErasureCodingPolicyInfo
>
list
=
(
(
DistributedFileSystem
)
fs
)
.
getAllErasureCodingPolicies
(
)
;
for
(
ErasureCodingPolicyInfo
info
:
list
)
{
final
ErasureCodingPolicy
ec
=
info
.
getPolicy
(
)
;
if
(
erasureCodePolicyName
.
equals
(
ec
.
getName
(
)
)
)
{
(
(
DistributedFileSystem
)
fs
)
.
setErasureCodingPolicy
(
path
,
ec
.
getName
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
size
=
Long
.
parseLong
(
tokens
.
nextToken
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
time
=
Long
.
parseLong
(
tokens
.
nextToken
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
rate
=
Float
.
parseFloat
(
tokens
.
nextToken
(
)
)
;
else
if
(
attr
.
endsWith
(
)
)
sqrate
=
Float
.
parseFloat
(
tokens
.
nextToken
(
)
)
;
}
}
finally
{
if
(
in
!=
null
)
in
.
close
(
)
;
if
(
lines
!=
null
)
lines
.
close
(
)
;
}
double
med
=
rate
/
1000
/
tasks
;
double
stdDev
=
Math
.
sqrt
(
Math
.
abs
(
sqrate
/
1000
/
tasks
-
med
*
med
)
)
;
DecimalFormat
df
=
new
DecimalFormat
(
)
;
String
resultLines
[
]
=
{
+
testType
,
+
new
Date
(
System
.
currentTimeMillis
(
)
)
,
+
tasks
,
+
df
.
format
(
toMB
(
size
)
)
,
+
df
.
format
(
toMB
(
size
)
/
msToSecs
(
time
)
)
,
+
df
.
format
(
med
)
,
+
df
.
format
(
stdDev
)
,
+
df
.
format
(
msToSecs
(
execTime
)
)
,
}
;
PrintStream
res
=
null
;
try
{
res
=
new
PrintStream
(
new
FileOutputStream
(
new
File
(
resFileName
)
,
true
)
)
;
for
(
int
i
=
0
;
i
<
resultLines
.
length
;
i
++
)
{
public
static
void
testFs
(
long
megaBytes
,
int
numFiles
,
long
seed
)
throws
Exception
{
FileSystem
fs
=
FileSystem
.
get
(
conf
)
;
if
(
seed
==
0
)
seed
=
new
Random
(
)
.
nextLong
(
)
;
public
static
void
createControlFile
(
FileSystem
fs
,
long
megaBytes
,
int
numFiles
,
long
seed
)
throws
Exception
{
System
.
exit
(
-
1
)
;
}
for
(
int
i
=
0
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
.
equals
(
)
)
{
files
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
megaBytes
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
noRead
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
noWrite
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
noSeek
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fastCheck
=
true
;
}
}
LOG
.
info
(
+
seed
)
;
}
for
(
int
i
=
0
;
i
<
args
.
length
;
i
++
)
{
if
(
args
[
i
]
.
equals
(
)
)
{
files
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
megaBytes
=
Integer
.
parseInt
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
noRead
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
noWrite
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
noSeek
=
true
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
fastCheck
=
true
;
}
}
LOG
.
info
(
+
seed
)
;
LOG
.
info
(
+
files
)
;
static
void
runTestCache
(
int
port
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
MiniDFSCluster
cluster
=
null
;
try
{
cluster
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
nameNodePort
(
port
)
.
numDataNodes
(
2
)
.
build
(
)
;
URI
uri
=
cluster
.
getFileSystem
(
)
.
getUri
(
)
;
long
timeTaken
=
0
,
bytesAppended
=
0
;
DataWriter
writer
=
new
DataWriter
(
getRandom
(
)
)
;
LOG
.
info
(
+
fn
+
+
Helper
.
toByteInfo
(
appendSize
)
)
;
{
long
startTime
=
Timer
.
now
(
)
;
os
=
fs
.
append
(
fn
)
;
timeTaken
+=
Timer
.
elapsed
(
startTime
)
;
GenerateOutput
stats
=
writer
.
writeSegment
(
appendSize
,
os
)
;
timeTaken
+=
stats
.
getTimeTaken
(
)
;
bytesAppended
+=
stats
.
getBytesWritten
(
)
;
startTime
=
Timer
.
now
(
)
;
os
.
close
(
)
;
os
=
null
;
timeTaken
+=
Timer
.
elapsed
(
startTime
)
;
}
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
BYTES_WRITTEN
,
bytesAppended
)
)
;
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
OK_TIME_TAKEN
,
timeTaken
)
)
;
if
(
cfg
==
null
)
{
return
;
}
LOG
.
info
(
+
cfg
.
getBaseDirectory
(
)
)
;
LOG
.
info
(
+
cfg
.
getDataPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getOutputPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getResultFile
(
)
)
;
LOG
.
info
(
+
cfg
.
getQueueName
(
)
)
;
LOG
.
info
(
+
cfg
.
shouldExitOnFirstError
(
)
)
;
{
String
duration
=
;
if
(
cfg
.
getDurationMilliseconds
(
)
==
Integer
.
MAX_VALUE
)
{
duration
+=
;
}
else
{
duration
+=
cfg
.
getDurationMilliseconds
(
)
+
;
}
LOG
.
info
(
duration
)
;
}
LOG
.
info
(
+
cfg
.
getMapAmount
(
)
)
;
return
;
}
LOG
.
info
(
+
cfg
.
getBaseDirectory
(
)
)
;
LOG
.
info
(
+
cfg
.
getDataPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getOutputPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getResultFile
(
)
)
;
LOG
.
info
(
+
cfg
.
getQueueName
(
)
)
;
LOG
.
info
(
+
cfg
.
shouldExitOnFirstError
(
)
)
;
{
String
duration
=
;
if
(
cfg
.
getDurationMilliseconds
(
)
==
Integer
.
MAX_VALUE
)
{
duration
+=
;
}
else
{
duration
+=
cfg
.
getDurationMilliseconds
(
)
+
;
}
LOG
.
info
(
duration
)
;
}
LOG
.
info
(
+
cfg
.
getMapAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getReducerAmount
(
)
)
;
}
LOG
.
info
(
+
cfg
.
getBaseDirectory
(
)
)
;
LOG
.
info
(
+
cfg
.
getDataPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getOutputPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getResultFile
(
)
)
;
LOG
.
info
(
+
cfg
.
getQueueName
(
)
)
;
LOG
.
info
(
+
cfg
.
shouldExitOnFirstError
(
)
)
;
{
String
duration
=
;
if
(
cfg
.
getDurationMilliseconds
(
)
==
Integer
.
MAX_VALUE
)
{
duration
+=
;
}
else
{
duration
+=
cfg
.
getDurationMilliseconds
(
)
+
;
}
LOG
.
info
(
duration
)
;
}
LOG
.
info
(
+
cfg
.
getMapAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getReducerAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getOpCount
(
)
)
;
LOG
.
info
(
+
cfg
.
getBaseDirectory
(
)
)
;
LOG
.
info
(
+
cfg
.
getDataPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getOutputPath
(
)
)
;
LOG
.
info
(
+
cfg
.
getResultFile
(
)
)
;
LOG
.
info
(
+
cfg
.
getQueueName
(
)
)
;
LOG
.
info
(
+
cfg
.
shouldExitOnFirstError
(
)
)
;
{
String
duration
=
;
if
(
cfg
.
getDurationMilliseconds
(
)
==
Integer
.
MAX_VALUE
)
{
duration
+=
;
}
else
{
duration
+=
cfg
.
getDurationMilliseconds
(
)
+
;
}
LOG
.
info
(
duration
)
;
}
LOG
.
info
(
+
cfg
.
getMapAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getReducerAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getOpCount
(
)
)
;
String
duration
=
;
if
(
cfg
.
getDurationMilliseconds
(
)
==
Integer
.
MAX_VALUE
)
{
duration
+=
;
}
else
{
duration
+=
cfg
.
getDurationMilliseconds
(
)
+
;
}
LOG
.
info
(
duration
)
;
}
LOG
.
info
(
+
cfg
.
getMapAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getReducerAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getOpCount
(
)
)
;
LOG
.
info
(
+
cfg
.
getTotalFiles
(
)
)
;
LOG
.
info
(
+
cfg
.
getDirSize
(
)
)
;
{
String
read
=
;
if
(
cfg
.
shouldReadFullFile
(
)
)
{
read
+=
;
}
else
{
}
LOG
.
info
(
duration
)
;
}
LOG
.
info
(
+
cfg
.
getMapAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getReducerAmount
(
)
)
;
LOG
.
info
(
+
cfg
.
getOpCount
(
)
)
;
LOG
.
info
(
+
cfg
.
getTotalFiles
(
)
)
;
LOG
.
info
(
+
cfg
.
getDirSize
(
)
)
;
{
String
read
=
;
if
(
cfg
.
shouldReadFullFile
(
)
)
{
read
+=
;
}
else
{
read
+=
cfg
.
getReadSize
(
)
+
;
}
LOG
.
info
(
read
)
;
}
{
String
write
=
;
if
(
cfg
.
shouldWriteUseBlockSize
(
)
)
{
write
+=
;
LOG
.
info
(
+
cfg
.
getTotalFiles
(
)
)
;
LOG
.
info
(
+
cfg
.
getDirSize
(
)
)
;
{
String
read
=
;
if
(
cfg
.
shouldReadFullFile
(
)
)
{
read
+=
;
}
else
{
read
+=
cfg
.
getReadSize
(
)
+
;
}
LOG
.
info
(
read
)
;
}
{
String
write
=
;
if
(
cfg
.
shouldWriteUseBlockSize
(
)
)
{
write
+=
;
}
else
{
write
+=
cfg
.
getWriteSize
(
)
+
;
}
LOG
.
info
(
write
)
;
}
{
String
append
=
;
if
(
cfg
.
shouldReadFullFile
(
)
)
{
read
+=
;
}
else
{
read
+=
cfg
.
getReadSize
(
)
+
;
}
LOG
.
info
(
read
)
;
}
{
String
write
=
;
if
(
cfg
.
shouldWriteUseBlockSize
(
)
)
{
write
+=
;
}
else
{
write
+=
cfg
.
getWriteSize
(
)
+
;
}
LOG
.
info
(
write
)
;
}
{
String
append
=
;
if
(
cfg
.
shouldAppendUseBlockSize
(
)
)
{
append
+=
;
}
else
{
}
else
{
read
+=
cfg
.
getReadSize
(
)
+
;
}
LOG
.
info
(
read
)
;
}
{
String
write
=
;
if
(
cfg
.
shouldWriteUseBlockSize
(
)
)
{
write
+=
;
}
else
{
write
+=
cfg
.
getWriteSize
(
)
+
;
}
LOG
.
info
(
write
)
;
}
{
String
append
=
;
if
(
cfg
.
shouldAppendUseBlockSize
(
)
)
{
append
+=
;
}
else
{
append
+=
cfg
.
getAppendSize
(
)
+
;
}
LOG
.
info
(
append
)
;
read
+=
cfg
.
getReadSize
(
)
+
;
}
LOG
.
info
(
read
)
;
}
{
String
write
=
;
if
(
cfg
.
shouldWriteUseBlockSize
(
)
)
{
write
+=
;
}
else
{
write
+=
cfg
.
getWriteSize
(
)
+
;
}
LOG
.
info
(
write
)
;
}
{
String
append
=
;
if
(
cfg
.
shouldAppendUseBlockSize
(
)
)
{
append
+=
;
}
else
{
append
+=
cfg
.
getAppendSize
(
)
+
;
}
LOG
.
info
(
append
)
;
}
{
String
bsize
=
;
FSDataOutputStream
os
=
null
;
try
{
Path
fn
=
getCreateFile
(
)
;
Range
<
Long
>
writeSizeRange
=
getConfig
(
)
.
getWriteSize
(
)
;
long
writeSize
=
0
;
long
blockSize
=
determineBlockSize
(
)
;
short
replicationAmount
=
determineReplication
(
)
;
if
(
getConfig
(
)
.
shouldWriteUseBlockSize
(
)
)
{
writeSizeRange
=
getConfig
(
)
.
getBlockSize
(
)
;
}
writeSize
=
Range
.
betweenPositive
(
getRandom
(
)
,
writeSizeRange
)
;
long
bytesWritten
=
0
;
long
timeTaken
=
0
;
int
bufSize
=
getBufferSize
(
)
;
boolean
overWrite
=
false
;
DataWriter
writer
=
new
DataWriter
(
getRandom
(
)
)
;
long
timeTaken
=
0
;
int
bufSize
=
getBufferSize
(
)
;
boolean
overWrite
=
false
;
DataWriter
writer
=
new
DataWriter
(
getRandom
(
)
)
;
LOG
.
info
(
+
fn
+
+
Helper
.
toByteInfo
(
writeSize
)
+
+
Helper
.
toByteInfo
(
blockSize
)
+
+
replicationAmount
)
;
{
long
startTime
=
Timer
.
now
(
)
;
os
=
fs
.
create
(
fn
,
overWrite
,
bufSize
,
replicationAmount
,
blockSize
)
;
timeTaken
+=
Timer
.
elapsed
(
startTime
)
;
GenerateOutput
stats
=
writer
.
writeSegment
(
writeSize
,
os
)
;
bytesWritten
+=
stats
.
getBytesWritten
(
)
;
timeTaken
+=
stats
.
getTimeTaken
(
)
;
startTime
=
Timer
.
now
(
)
;
os
.
close
(
)
;
os
=
null
;
timeTaken
+=
Timer
.
elapsed
(
startTime
)
;
@
Override
List
<
OperationOutput
>
run
(
FileSystem
fs
)
{
List
<
OperationOutput
>
out
=
super
.
run
(
fs
)
;
try
{
Path
fn
=
getDeleteFile
(
)
;
long
timeTaken
=
0
;
boolean
deleteStatus
=
false
;
{
long
startTime
=
Timer
.
now
(
)
;
deleteStatus
=
fs
.
delete
(
fn
,
false
)
;
timeTaken
=
Timer
.
elapsed
(
startTime
)
;
}
if
(
!
deleteStatus
)
{
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
FAILURES
,
1L
)
)
;
LOG
.
info
(
+
fn
)
;
}
else
{
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
OK_TIME_TAKEN
,
timeTaken
)
)
;
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
SUCCESSES
,
1L
)
)
;
Range
<
Long
>
readSizeRange
=
getConfig
(
)
.
getReadSize
(
)
;
long
readSize
=
0
;
String
readStrAm
=
;
if
(
getConfig
(
)
.
shouldReadFullFile
(
)
)
{
readSize
=
Long
.
MAX_VALUE
;
readStrAm
=
;
}
else
{
readSize
=
Range
.
betweenPositive
(
getRandom
(
)
,
readSizeRange
)
;
readStrAm
=
Helper
.
toByteInfo
(
readSize
)
;
}
long
timeTaken
=
0
;
long
chunkSame
=
0
;
long
chunkDiff
=
0
;
long
bytesRead
=
0
;
long
startTime
=
0
;
DataVerifier
vf
=
new
DataVerifier
(
)
;
{
startTime
=
Timer
.
now
(
)
;
is
=
fs
.
open
(
fn
)
;
timeTaken
+=
Timer
.
elapsed
(
startTime
)
;
VerifyOutput
vo
=
vf
.
verifyFile
(
readSize
,
is
)
;
timeTaken
+=
vo
.
getReadTime
(
)
;
chunkSame
+=
vo
.
getChunksSame
(
)
;
chunkDiff
+=
vo
.
getChunksDifferent
(
)
;
bytesRead
+=
vo
.
getBytesRead
(
)
;
startTime
=
Timer
.
now
(
)
;
is
.
close
(
)
;
is
=
null
;
timeTaken
+=
Timer
.
elapsed
(
startTime
)
;
}
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
OK_TIME_TAKEN
,
timeTaken
)
)
;
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
BYTES_READ
,
bytesRead
)
)
;
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
SUCCESSES
,
1L
)
)
;
private
void
writeMessage
(
String
msg
,
PrintWriter
os
)
{
private
void
logAndSetStatus
(
Reporter
r
,
String
msg
)
{
r
.
setStatus
(
msg
)
;
private
void
logAndSetStatus
(
Reporter
r
,
String
msg
)
{
r
.
setStatus
(
msg
)
;
parsedOpts
=
argHolder
.
parse
(
)
;
if
(
parsedOpts
.
shouldOutputHelp
(
)
)
{
parsedOpts
.
outputHelp
(
)
;
return
1
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
return
1
;
}
LOG
.
info
(
+
Helper
.
stringifyArray
(
args
,
)
)
;
ConfigExtractor
config
=
null
;
try
{
ConfigMerger
cfgMerger
=
new
ConfigMerger
(
)
;
Configuration
cfg
=
cfgMerger
.
getMerged
(
parsedOpts
,
new
Configuration
(
base
)
)
;
if
(
cfg
!=
null
)
{
config
=
new
ConfigExtractor
(
cfg
)
;
}
}
catch
(
Exception
e
)
{
}
LOG
.
info
(
+
Helper
.
stringifyArray
(
args
,
)
)
;
ConfigExtractor
config
=
null
;
try
{
ConfigMerger
cfgMerger
=
new
ConfigMerger
(
)
;
Configuration
cfg
=
cfgMerger
.
getMerged
(
parsedOpts
,
new
Configuration
(
base
)
)
;
if
(
cfg
!=
null
)
{
config
=
new
ConfigExtractor
(
cfg
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
return
1
;
}
if
(
config
==
null
)
{
LOG
.
error
(
)
;
return
1
;
}
try
{
LOG
.
info
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
return
1
;
}
if
(
config
==
null
)
{
LOG
.
error
(
)
;
return
1
;
}
try
{
LOG
.
info
(
)
;
ConfigExtractor
.
dumpOptions
(
config
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
return
1
;
}
boolean
jobOk
=
false
;
try
{
LOG
.
info
(
)
;
if
(
config
==
null
)
{
LOG
.
error
(
)
;
return
1
;
}
try
{
LOG
.
info
(
)
;
ConfigExtractor
.
dumpOptions
(
config
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
return
1
;
}
boolean
jobOk
=
false
;
try
{
LOG
.
info
(
)
;
runJob
(
config
)
;
jobOk
=
true
;
}
catch
(
Exception
e
)
{
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
return
1
;
}
boolean
jobOk
=
false
;
try
{
LOG
.
info
(
)
;
runJob
(
config
)
;
jobOk
=
true
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
if
(
jobOk
)
{
try
{
LOG
.
info
(
)
;
writeReport
(
config
)
;
}
catch
(
Exception
e
)
{
private
void
writeReport
(
ConfigExtractor
cfg
)
throws
Exception
{
Path
dn
=
cfg
.
getOutputPath
(
)
;
List
<
OperationOutput
>
opList
=
splitTypes
.
get
(
op
)
;
if
(
opList
==
null
)
{
opList
=
new
ArrayList
<
OperationOutput
>
(
)
;
}
opList
.
add
(
data
)
;
splitTypes
.
put
(
op
,
opList
)
;
}
else
{
noOperations
.
add
(
data
)
;
}
}
else
{
throw
new
IOException
(
+
line
)
;
}
}
fileReader
.
close
(
)
;
fileReader
=
null
;
}
File
resFile
=
null
;
if
(
cfg
.
getResultFile
(
)
!=
null
)
{
resFile
=
new
File
(
cfg
.
getResultFile
(
)
)
;
}
if
(
resFile
!=
null
)
{
private
void
cleanup
(
ConfigExtractor
cfg
)
throws
IOException
{
Path
base
=
cfg
.
getBaseDirectory
(
)
;
if
(
base
!=
null
)
{
private
void
rDelete
(
File
place
)
throws
Exception
{
if
(
place
.
isFile
(
)
)
{
@
Test
public
void
testDataWriting
(
)
throws
Exception
{
long
byteAm
=
100
;
File
fn
=
getTestFile
(
)
;
DataWriter
writer
=
new
DataWriter
(
rnd
)
;
FileOutputStream
fs
=
new
FileOutputStream
(
fn
)
;
GenerateOutput
ostat
=
writer
.
writeSegment
(
byteAm
,
fs
)
;
@
Test
public
void
testDataWriting
(
)
throws
Exception
{
long
byteAm
=
100
;
File
fn
=
getTestFile
(
)
;
DataWriter
writer
=
new
DataWriter
(
rnd
)
;
FileOutputStream
fs
=
new
FileOutputStream
(
fn
)
;
GenerateOutput
ostat
=
writer
.
writeSegment
(
byteAm
,
fs
)
;
LOG
.
info
(
ostat
.
toString
(
)
)
;
fs
.
close
(
)
;
assertTrue
(
ostat
.
getBytesWritten
(
)
==
byteAm
)
;
DataVerifier
vf
=
new
DataVerifier
(
)
;
FileInputStream
fin
=
new
FileInputStream
(
fn
)
;
VerifyOutput
vfout
=
vf
.
verifyFile
(
byteAm
,
new
DataInputStream
(
fin
)
)
;
boolean
waitOnTruncate
=
getConfig
(
)
.
shouldWaitOnTruncate
(
)
;
long
currentSize
=
fs
.
getFileStatus
(
fn
)
.
getLen
(
)
;
Range
<
Long
>
truncateSizeRange
=
getConfig
(
)
.
getTruncateSize
(
)
;
if
(
getConfig
(
)
.
shouldTruncateUseBlockSize
(
)
)
{
truncateSizeRange
=
getConfig
(
)
.
getBlockSize
(
)
;
}
long
truncateSize
=
Math
.
max
(
0L
,
currentSize
-
Range
.
betweenPositive
(
getRandom
(
)
,
truncateSizeRange
)
)
;
long
timeTaken
=
0
;
LOG
.
info
(
+
fn
+
+
Helper
.
toByteInfo
(
truncateSize
)
)
;
{
long
startTime
=
Timer
.
now
(
)
;
boolean
completed
=
fs
.
truncate
(
fn
,
truncateSize
)
;
if
(
!
completed
&&
waitOnTruncate
)
waitForRecovery
(
fs
,
fn
,
truncateSize
)
;
timeTaken
+=
Timer
.
elapsed
(
startTime
)
;
}
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
BYTES_WRITTEN
,
0
)
)
;
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
OK_TIME_TAKEN
,
timeTaken
)
)
;
out
.
add
(
new
OperationOutput
(
OutputType
.
LONG
,
getType
(
)
,
ReportWriter
.
SUCCESSES
,
1L
)
)
;
private
void
waitForRecovery
(
FileSystem
fs
,
Path
fn
,
long
newLength
)
throws
IOException
{
private
void
configureOperations
(
ConfigExtractor
cfg
)
{
operations
=
new
TreeMap
<
OperationType
,
OperationInfo
>
(
)
;
Map
<
OperationType
,
OperationData
>
opinfo
=
cfg
.
getOperations
(
)
;
int
totalAm
=
cfg
.
getOpCount
(
)
;
int
opsLeft
=
totalAm
;
NumberFormat
formatter
=
Formatter
.
getPercentFormatter
(
)
;
for
(
final
OperationType
type
:
opinfo
.
keySet
(
)
)
{
OperationData
opData
=
opinfo
.
get
(
type
)
;
OperationInfo
info
=
new
OperationInfo
(
)
;
info
.
distribution
=
opData
.
getDistribution
(
)
;
int
amLeft
=
determineHowMany
(
totalAm
,
opData
,
type
)
;
opsLeft
-=
amLeft
;
opsLeft
-=
amLeft
;
LOG
.
info
(
type
.
name
(
)
+
+
amLeft
+
+
totalAm
+
+
formatter
.
format
(
opData
.
getPercent
(
)
)
)
;
info
.
amountLeft
=
amLeft
;
Operation
op
=
factory
.
getOperation
(
type
)
;
if
(
op
!=
null
)
{
Observer
fn
=
new
Observer
(
)
{
public
void
notifyFinished
(
Operation
op
)
{
OperationInfo
opInfo
=
operations
.
get
(
type
)
;
if
(
opInfo
!=
null
)
{
--
opInfo
.
amountLeft
;
}
}
public
void
notifyStarting
(
Operation
op
)
{
}
}
;
info
.
operation
=
new
ObserveableOp
(
op
,
fn
)
;
operations
.
put
(
type
,
info
)
;
}
}
if
(
opsLeft
>
0
)
{
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
numberOfFiles
=
Long
.
parseLong
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
replicationFactorPerFile
=
Short
.
parseShort
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
baseDir
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
readFileAfterOpen
=
Boolean
.
parseBoolean
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
displayUsage
(
)
;
isHelpMessage
=
true
;
checkArgs
(
i
+
1
,
args
.
length
)
;
numberOfFiles
=
Long
.
parseLong
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
replicationFactorPerFile
=
Short
.
parseShort
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
baseDir
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
readFileAfterOpen
=
Boolean
.
parseBoolean
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
displayUsage
(
)
;
isHelpMessage
=
true
;
}
}
LOG
.
info
(
)
;
numberOfFiles
=
Long
.
parseLong
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
replicationFactorPerFile
=
Short
.
parseShort
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
baseDir
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
readFileAfterOpen
=
Boolean
.
parseBoolean
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
displayUsage
(
)
;
isHelpMessage
=
true
;
}
}
LOG
.
info
(
)
;
LOG
.
info
(
+
operation
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
replicationFactorPerFile
=
Short
.
parseShort
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
baseDir
=
args
[
++
i
]
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
checkArgs
(
i
+
1
,
args
.
length
)
;
readFileAfterOpen
=
Boolean
.
parseBoolean
(
args
[
++
i
]
)
;
}
else
if
(
args
[
i
]
.
equals
(
)
)
{
displayUsage
(
)
;
isHelpMessage
=
true
;
}
}
LOG
.
info
(
)
;
LOG
.
info
(
+
operation
)
;
LOG
.
info
(
+
sdf
.
format
(
new
Date
(
startTime
)
)
)
;
Path
tempDir
=
new
Path
(
dir
,
)
;
fs
.
delete
(
dir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
dir
)
;
fs
.
mkdirs
(
tempDir
)
;
LongWritable
tkey
=
new
LongWritable
(
)
;
Text
tval
=
new
Text
(
)
;
SequenceFile
.
Writer
writer
=
SequenceFile
.
createWriter
(
fs
,
job
,
file
,
LongWritable
.
class
,
Text
.
class
,
compressionType
,
new
DefaultCodec
(
)
)
;
try
{
for
(
int
i
=
0
;
i
<
RECORDS
;
++
i
)
{
tkey
.
set
(
1234
)
;
tval
.
set
(
)
;
writer
.
append
(
tkey
,
tval
)
;
}
}
finally
{
writer
.
close
(
)
;
}
long
fileLength
=
fs
.
getFileStatus
(
file
)
.
getLen
(
)
;
private
static
void
createBigMapInputFile
(
Configuration
conf
,
FileSystem
fs
,
Path
dir
,
long
fileSizeInMB
)
throws
IOException
{
if
(
fs
.
exists
(
dir
)
)
{
FileStatus
[
]
list
=
fs
.
listStatus
(
dir
)
;
if
(
list
.
length
>
0
)
{
throw
new
IOException
(
+
dir
+
)
;
}
}
Path
file
=
new
Path
(
dir
,
)
;
SequenceFile
.
Writer
writer
=
SequenceFile
.
createWriter
(
fs
,
conf
,
file
,
BytesWritable
.
class
,
BytesWritable
.
class
,
CompressionType
.
NONE
)
;
long
numBytesToWrite
=
fileSizeInMB
*
1024
*
1024
;
int
minKeySize
=
conf
.
getInt
(
MIN_KEY
,
10
)
;
;
int
keySizeRange
=
conf
.
getInt
(
MAX_KEY
,
1000
)
-
minKeySize
;
int
minValueSize
=
conf
.
getInt
(
MIN_VALUE
,
0
)
;
int
valueSizeRange
=
conf
.
getInt
(
MAX_VALUE
,
20000
)
-
minValueSize
;
BytesWritable
randomKey
=
new
BytesWritable
(
)
;
BytesWritable
randomValue
=
new
BytesWritable
(
)
;
public
void
generateTextFile
(
FileSystem
fs
,
Path
inputFile
,
long
numLines
,
Order
sortOrder
)
throws
IOException
{
private
ArrayList
<
Long
>
runJobInSequence
(
JobConf
masterJobConf
,
int
numRuns
)
throws
IOException
{
Random
rand
=
new
Random
(
)
;
ArrayList
<
Long
>
execTimes
=
new
ArrayList
<
Long
>
(
)
;
for
(
int
i
=
0
;
i
<
numRuns
;
i
++
)
{
JobConf
jobConf
=
new
JobConf
(
masterJobConf
)
;
jobConf
.
setJar
(
masterJobConf
.
getJar
(
)
)
;
FileOutputFormat
.
setOutputPath
(
jobConf
,
new
Path
(
OUTPUT_DIR
,
+
rand
.
nextInt
(
)
)
)
;
private
void
checkJobExitStatus
(
int
status
,
String
jobName
)
{
if
(
status
!=
0
)
{
private
void
runTest
(
final
JobClient
jc
,
final
Configuration
conf
,
final
String
jobClass
,
final
String
[
]
args
,
KillTaskThread
killTaskThread
,
KillTrackerThread
killTrackerThread
)
throws
Exception
{
Thread
t
=
new
Thread
(
)
{
public
void
run
(
)
{
try
{
Class
<
?
>
jobClassObj
=
conf
.
getClassByName
(
jobClass
)
;
int
status
=
ToolRunner
.
run
(
conf
,
(
Tool
)
(
jobClassObj
.
newInstance
(
)
)
,
args
)
;
checkJobExitStatus
(
status
,
jobClass
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
jobClass
+
)
;
System
.
exit
(
-
1
)
;
}
}
}
;
t
.
setDaemon
(
true
)
;
t
.
start
(
)
;
JobStatus
[
]
jobs
;
while
(
(
jobs
=
jc
.
jobsToComplete
(
)
)
.
length
==
0
)
{
int
status
=
ToolRunner
.
run
(
conf
,
(
Tool
)
(
jobClassObj
.
newInstance
(
)
)
,
args
)
;
checkJobExitStatus
(
status
,
jobClass
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
jobClass
+
)
;
System
.
exit
(
-
1
)
;
}
}
}
;
t
.
setDaemon
(
true
)
;
t
.
start
(
)
;
JobStatus
[
]
jobs
;
while
(
(
jobs
=
jc
.
jobsToComplete
(
)
)
.
length
==
0
)
{
LOG
.
info
(
+
jobClass
+
)
;
Thread
.
sleep
(
1000
)
;
}
JobID
jobId
=
jobs
[
jobs
.
length
-
1
]
.
getJobID
(
)
;
RunningJob
rJob
=
jc
.
getJob
(
jobId
)
;
if
(
rJob
.
isComplete
(
)
)
{
catch
(
Exception
e
)
{
LOG
.
error
(
+
jobClass
+
)
;
System
.
exit
(
-
1
)
;
}
}
}
;
t
.
setDaemon
(
true
)
;
t
.
start
(
)
;
JobStatus
[
]
jobs
;
while
(
(
jobs
=
jc
.
jobsToComplete
(
)
)
.
length
==
0
)
{
LOG
.
info
(
+
jobClass
+
)
;
Thread
.
sleep
(
1000
)
;
}
JobID
jobId
=
jobs
[
jobs
.
length
-
1
]
.
getJobID
(
)
;
RunningJob
rJob
=
jc
.
getJob
(
jobId
)
;
if
(
rJob
.
isComplete
(
)
)
{
LOG
.
error
(
+
+
rJob
.
getJobID
(
)
+
)
;
System
.
exit
(
-
1
)
;
int
mapRecs
=
input
.
size
(
)
-
mapperBadRecords
.
size
(
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
MAP_INPUT_RECORDS
)
.
getCounter
(
)
,
mapRecs
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
MAP_OUTPUT_RECORDS
)
.
getCounter
(
)
,
mapRecs
)
;
int
redRecs
=
mapRecs
-
redBadRecords
.
size
(
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
REDUCE_SKIPPED_RECORDS
)
.
getCounter
(
)
,
redBadRecords
.
size
(
)
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
REDUCE_SKIPPED_GROUPS
)
.
getCounter
(
)
,
redBadRecords
.
size
(
)
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
REDUCE_INPUT_GROUPS
)
.
getCounter
(
)
,
redRecs
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
REDUCE_INPUT_RECORDS
)
.
getCounter
(
)
,
redRecs
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
REDUCE_OUTPUT_RECORDS
)
.
getCounter
(
)
,
redRecs
)
;
Path
skipDir
=
SkipBadRecords
.
getSkipOutputPath
(
conf
)
;
assertNotNull
(
skipDir
)
;
Path
[
]
skips
=
FileUtil
.
stat2Paths
(
getFileSystem
(
)
.
listStatus
(
skipDir
)
)
;
List
<
String
>
mapSkipped
=
new
ArrayList
<
String
>
(
)
;
List
<
String
>
redSkipped
=
new
ArrayList
<
String
>
(
)
;
for
(
Path
skipPath
:
skips
)
{
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
REDUCE_INPUT_RECORDS
)
.
getCounter
(
)
,
redRecs
)
;
assertEquals
(
counters
.
findCounter
(
TaskCounter
.
REDUCE_OUTPUT_RECORDS
)
.
getCounter
(
)
,
redRecs
)
;
Path
skipDir
=
SkipBadRecords
.
getSkipOutputPath
(
conf
)
;
assertNotNull
(
skipDir
)
;
Path
[
]
skips
=
FileUtil
.
stat2Paths
(
getFileSystem
(
)
.
listStatus
(
skipDir
)
)
;
List
<
String
>
mapSkipped
=
new
ArrayList
<
String
>
(
)
;
List
<
String
>
redSkipped
=
new
ArrayList
<
String
>
(
)
;
for
(
Path
skipPath
:
skips
)
{
LOG
.
info
(
+
skipPath
)
;
SequenceFile
.
Reader
reader
=
new
SequenceFile
.
Reader
(
getFileSystem
(
)
,
skipPath
,
conf
)
;
Object
key
=
ReflectionUtils
.
newInstance
(
reader
.
getKeyClass
(
)
,
conf
)
;
Object
value
=
ReflectionUtils
.
newInstance
(
reader
.
getValueClass
(
)
,
conf
)
;
key
=
reader
.
next
(
key
)
;
while
(
key
!=
null
)
{
value
=
reader
.
getCurrentValue
(
value
)
;
Object
value
=
ReflectionUtils
.
newInstance
(
reader
.
getValueClass
(
)
,
conf
)
;
key
=
reader
.
next
(
key
)
;
while
(
key
!=
null
)
{
value
=
reader
.
getCurrentValue
(
value
)
;
LOG
.
debug
(
+
key
+
+
value
.
toString
(
)
)
;
if
(
skipPath
.
getName
(
)
.
contains
(
)
)
{
redSkipped
.
add
(
value
.
toString
(
)
)
;
}
else
{
mapSkipped
.
add
(
value
.
toString
(
)
)
;
}
key
=
reader
.
next
(
key
)
;
}
reader
.
close
(
)
;
}
assertTrue
(
mapSkipped
.
containsAll
(
mapperBadRecords
)
)
;
assertTrue
(
redSkipped
.
containsAll
(
redBadRecords
)
)
;
Path
[
]
outputFiles
=
FileUtil
.
stat2Paths
(
getFileSystem
(
)
.
listStatus
(
getOutputDir
(
)
,
new
Utils
.
OutputFileUtils
.
OutputFilesFilter
(
)
)
)
;
List
<
String
>
mapperOutput
=
getProcessed
(
input
,
mapperBadRecords
)
;
while
(
key
!=
null
)
{
value
=
reader
.
getCurrentValue
(
value
)
;
LOG
.
debug
(
+
key
+
+
value
.
toString
(
)
)
;
if
(
skipPath
.
getName
(
)
.
contains
(
)
)
{
redSkipped
.
add
(
value
.
toString
(
)
)
;
}
else
{
mapSkipped
.
add
(
value
.
toString
(
)
)
;
}
key
=
reader
.
next
(
key
)
;
}
reader
.
close
(
)
;
}
assertTrue
(
mapSkipped
.
containsAll
(
mapperBadRecords
)
)
;
assertTrue
(
redSkipped
.
containsAll
(
redBadRecords
)
)
;
Path
[
]
outputFiles
=
FileUtil
.
stat2Paths
(
getFileSystem
(
)
.
listStatus
(
getOutputDir
(
)
,
new
Utils
.
OutputFileUtils
.
OutputFilesFilter
(
)
)
)
;
List
<
String
>
mapperOutput
=
getProcessed
(
input
,
mapperBadRecords
)
;
LOG
.
debug
(
+
mapperOutput
.
size
(
)
)
;
List
<
String
>
reducerOutput
=
getProcessed
(
mapperOutput
,
redBadRecords
)
;
private
void
validateCounters
(
org
.
apache
.
hadoop
.
mapreduce
.
Counters
counters
)
{
Iterator
<
org
.
apache
.
hadoop
.
mapreduce
.
CounterGroup
>
it
=
counters
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
org
.
apache
.
hadoop
.
mapreduce
.
CounterGroup
group
=
it
.
next
(
)
;
@
Test
(
timeout
=
10000
)
public
void
testFormat
(
)
throws
Exception
{
JobConf
job
=
new
JobConf
(
conf
)
;
Reporter
reporter
=
Reporter
.
NULL
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
Reporter
reporter
=
Reporter
.
NULL
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
LOG
.
info
(
+
seed
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
InputFormat
<
IntWritable
,
BytesWritable
>
format
=
new
CombineSequenceFileInputFormat
<
IntWritable
,
BytesWritable
>
(
)
;
IntWritable
key
=
new
IntWritable
(
)
;
BytesWritable
value
=
new
BytesWritable
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
(
SequenceFile
.
SYNC_INTERVAL
/
20
)
)
+
1
;
long
seed
=
random
.
nextLong
(
)
;
LOG
.
info
(
+
seed
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
InputFormat
<
IntWritable
,
BytesWritable
>
format
=
new
CombineSequenceFileInputFormat
<
IntWritable
,
BytesWritable
>
(
)
;
IntWritable
key
=
new
IntWritable
(
)
;
BytesWritable
value
=
new
BytesWritable
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
(
SequenceFile
.
SYNC_INTERVAL
/
20
)
)
+
1
;
LOG
.
info
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
numSplits
)
;
@
Test
(
timeout
=
10000
)
public
void
testFormat
(
)
throws
Exception
{
JobConf
job
=
new
JobConf
(
defaultConf
)
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
JobConf
job
=
new
JobConf
(
defaultConf
)
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
LOG
.
info
(
+
seed
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
CombineTextInputFormat
format
=
new
CombineTextInputFormat
(
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
20
)
+
1
;
long
seed
=
random
.
nextLong
(
)
;
LOG
.
info
(
+
seed
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
CombineTextInputFormat
format
=
new
CombineTextInputFormat
(
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
20
)
+
1
;
LOG
.
info
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
numSplits
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
CombineTextInputFormat
format
=
new
CombineTextInputFormat
(
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
20
)
+
1
;
LOG
.
info
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
numSplits
)
;
LOG
.
info
(
+
splits
.
length
)
;
assertEquals
(
,
1
,
splits
.
length
)
;
InputSplit
split
=
splits
[
0
]
;
assertEquals
(
,
CombineFileSplit
.
class
,
split
.
getClass
(
)
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
20
)
+
1
;
LOG
.
info
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
numSplits
)
;
LOG
.
info
(
+
splits
.
length
)
;
assertEquals
(
,
1
,
splits
.
length
)
;
InputSplit
split
=
splits
[
0
]
;
assertEquals
(
,
CombineFileSplit
.
class
,
split
.
getClass
(
)
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
LOG
.
debug
(
+
split
)
;
RecordReader
<
LongWritable
,
Text
>
reader
=
format
.
getRecordReader
(
split
,
job
,
voidReporter
)
;
try
{
int
count
=
0
;
while
(
reader
.
next
(
key
,
value
)
)
{
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
@
Test
(
timeout
=
5000
)
public
void
testNoRecordLength
(
)
throws
IOException
{
localFs
.
delete
(
workDir
,
true
)
;
Path
file
=
new
Path
(
workDir
,
new
String
(
)
)
;
createFile
(
file
,
null
,
10
,
10
)
;
JobConf
job
=
new
JobConf
(
defaultConf
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
FixedLengthInputFormat
format
=
new
FixedLengthInputFormat
(
)
;
format
.
configure
(
job
)
;
InputSplit
splits
[
]
=
format
.
getSplits
(
job
,
1
)
;
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
RecordReader
<
LongWritable
,
BytesWritable
>
reader
=
format
.
getRecordReader
(
split
,
job
,
voidReporter
)
;
}
catch
(
IOException
ioe
)
{
exceptionThrown
=
true
;
localFs
.
delete
(
workDir
,
true
)
;
Path
file
=
new
Path
(
workDir
,
new
String
(
)
)
;
createFile
(
file
,
null
,
10
,
10
)
;
JobConf
job
=
new
JobConf
(
defaultConf
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
FixedLengthInputFormat
format
=
new
FixedLengthInputFormat
(
)
;
format
.
setRecordLength
(
job
,
0
)
;
format
.
configure
(
job
)
;
InputSplit
splits
[
]
=
format
.
getSplits
(
job
,
1
)
;
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
RecordReader
<
LongWritable
,
BytesWritable
>
reader
=
format
.
getRecordReader
(
split
,
job
,
voidReporter
)
;
}
catch
(
IOException
ioe
)
{
exceptionThrown
=
true
;
localFs
.
delete
(
workDir
,
true
)
;
Path
file
=
new
Path
(
workDir
,
new
String
(
)
)
;
createFile
(
file
,
null
,
10
,
10
)
;
JobConf
job
=
new
JobConf
(
defaultConf
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
FixedLengthInputFormat
format
=
new
FixedLengthInputFormat
(
)
;
format
.
setRecordLength
(
job
,
-
10
)
;
format
.
configure
(
job
)
;
InputSplit
splits
[
]
=
format
.
getSplits
(
job
,
1
)
;
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
RecordReader
<
LongWritable
,
BytesWritable
>
reader
=
format
.
getRecordReader
(
split
,
job
,
voidReporter
)
;
}
catch
(
IOException
ioe
)
{
exceptionThrown
=
true
;
Path
file
=
new
Path
(
workDir
,
fileName
.
toString
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
LOG
.
info
(
+
seed
)
;
Random
random
=
new
Random
(
seed
)
;
int
MAX_TESTS
=
20
;
LongWritable
key
=
new
LongWritable
(
)
;
BytesWritable
value
=
new
BytesWritable
(
)
;
for
(
int
i
=
0
;
i
<
MAX_TESTS
;
i
++
)
{
LOG
.
info
(
)
;
int
totalRecords
=
random
.
nextInt
(
999
)
+
1
;
if
(
i
==
8
)
{
totalRecords
=
0
;
}
int
recordLength
=
random
.
nextInt
(
1024
*
100
)
+
1
;
if
(
i
==
10
)
{
recordLength
=
1
;
int
fileSize
=
(
totalRecords
*
recordLength
)
;
LOG
.
info
(
+
totalRecords
+
+
recordLength
)
;
JobConf
job
=
new
JobConf
(
defaultConf
)
;
if
(
codec
!=
null
)
{
ReflectionUtils
.
setConf
(
codec
,
job
)
;
}
ArrayList
<
String
>
recordList
=
createFile
(
file
,
codec
,
recordLength
,
totalRecords
)
;
assertTrue
(
localFs
.
exists
(
file
)
)
;
FixedLengthInputFormat
.
setRecordLength
(
job
,
recordLength
)
;
int
numSplits
=
1
;
if
(
i
>
0
)
{
if
(
i
==
(
MAX_TESTS
-
1
)
)
{
numSplits
=
(
int
)
(
fileSize
/
Math
.
max
(
1
,
Math
.
floor
(
recordLength
/
2
)
)
)
;
}
else
{
if
(
MAX_TESTS
%
i
==
0
)
{
numSplits
=
fileSize
/
(
fileSize
-
random
.
nextInt
(
fileSize
)
)
;
JobConf
job
=
new
JobConf
(
defaultConf
)
;
format
.
setRecordLength
(
job
,
5
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
if
(
codec
!=
null
)
{
ReflectionUtils
.
setConf
(
codec
,
job
)
;
}
format
.
configure
(
job
)
;
writeFile
(
localFs
,
new
Path
(
workDir
,
fileName
.
toString
(
)
)
,
codec
,
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
100
)
;
if
(
codec
!=
null
)
{
assertEquals
(
,
1
,
splits
.
length
)
;
}
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
List
<
String
>
results
=
readSplit
(
format
,
split
,
job
)
;
}
catch
(
IOException
ioe
)
{
private
void
testSuccessfulJob
(
String
filename
,
Class
<
?
extends
OutputCommitter
>
committer
,
String
[
]
exclude
)
throws
IOException
{
JobConf
jc
=
mr
.
createJobConf
(
)
;
Path
outDir
=
getNewOutputDir
(
)
;
configureJob
(
jc
,
,
1
,
0
,
outDir
)
;
jc
.
setOutputCommitter
(
committer
)
;
JobClient
jobClient
=
new
JobClient
(
jc
)
;
RunningJob
job
=
jobClient
.
submitJob
(
jc
)
;
JobID
id
=
job
.
getID
(
)
;
job
.
waitForCompletion
(
)
;
@
Test
public
void
testFormat
(
)
throws
Exception
{
JobConf
job
=
new
JobConf
(
)
;
Path
file
=
new
Path
(
workDir
,
)
;
Reporter
reporter
=
Reporter
.
NULL
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
@
Test
public
void
testFormat
(
)
throws
Exception
{
JobConf
job
=
new
JobConf
(
)
;
Path
file
=
new
Path
(
workDir
,
)
;
Reporter
reporter
=
Reporter
.
NULL
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
LOG
.
info
(
+
seed
)
;
Random
random
=
new
Random
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
LOG
.
debug
(
+
length
)
;
Writer
writer
=
new
OutputStreamWriter
(
localFs
.
create
(
file
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
*
2
)
)
;
writer
.
write
(
)
;
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
KeyValueTextInputFormat
format
=
new
KeyValueTextInputFormat
(
)
;
format
.
configure
(
job
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
Writer
writer
=
new
OutputStreamWriter
(
localFs
.
create
(
file
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
*
2
)
)
;
writer
.
write
(
)
;
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
KeyValueTextInputFormat
format
=
new
KeyValueTextInputFormat
(
)
;
format
.
configure
(
job
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
LOG
.
debug
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
numSplits
)
;
writer
.
write
(
Integer
.
toString
(
i
*
2
)
)
;
writer
.
write
(
)
;
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
KeyValueTextInputFormat
format
=
new
KeyValueTextInputFormat
(
)
;
format
.
configure
(
job
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
LOG
.
debug
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
numSplits
)
;
LOG
.
debug
(
+
splits
.
length
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
length
;
j
++
)
{
assertEquals
(
,
KeyValueLineRecordReader
.
class
,
readerClass
)
;
Text
key
=
reader
.
createKey
(
)
;
Class
keyClass
=
key
.
getClass
(
)
;
Text
value
=
reader
.
createValue
(
)
;
Class
valueClass
=
value
.
getClass
(
)
;
assertEquals
(
,
Text
.
class
,
keyClass
)
;
assertEquals
(
,
Text
.
class
,
valueClass
)
;
try
{
int
count
=
0
;
while
(
reader
.
next
(
key
,
value
)
)
{
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
LOG
.
debug
(
+
v
)
;
if
(
bits
.
get
(
v
)
)
{
LOG
.
warn
(
+
v
+
+
j
+
+
reader
.
getPos
(
)
)
;
}
assertFalse
(
,
bits
.
get
(
v
)
)
;
private
void
verifyEntity
(
File
entityFile
,
String
eventId
,
boolean
chkMetrics
,
boolean
chkCfg
,
Set
<
String
>
cfgsToVerify
,
boolean
checkIdPrefix
)
throws
IOException
{
BufferedReader
reader
=
null
;
String
strLine
;
try
{
reader
=
new
BufferedReader
(
new
FileReader
(
entityFile
)
)
;
long
idPrefix
=
-
1
;
while
(
(
strLine
=
reader
.
readLine
(
)
)
!=
null
)
{
if
(
strLine
.
trim
(
)
.
length
(
)
>
0
)
{
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEntity
entity
=
FileSystemTimelineReaderImpl
.
getTimelineRecordFromJSON
(
strLine
.
trim
(
)
,
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEntity
.
class
)
;
@
BeforeClass
public
static
void
setup
(
)
throws
IOException
{
dfs
=
new
MiniDFSCluster
.
Builder
(
conf
)
.
build
(
)
;
fileSys
=
dfs
.
getFileSystem
(
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
private
Path
initFiles
(
FileSystem
fs
,
int
numFiles
,
int
numBytes
)
throws
IOException
{
Path
dir
=
new
Path
(
System
.
getProperty
(
,
)
+
)
;
Path
multiFileDir
=
new
Path
(
dir
,
)
;
fs
.
delete
(
multiFileDir
,
true
)
;
fs
.
mkdirs
(
multiFileDir
)
;
private
Path
initFiles
(
FileSystem
fs
,
int
numFiles
,
int
numBytes
)
throws
IOException
{
Path
dir
=
new
Path
(
System
.
getProperty
(
,
)
+
)
;
Path
multiFileDir
=
new
Path
(
dir
,
)
;
fs
.
delete
(
multiFileDir
,
true
)
;
fs
.
mkdirs
(
multiFileDir
)
;
LOG
.
info
(
+
numFiles
+
+
multiFileDir
)
;
for
(
int
i
=
0
;
i
<
numFiles
;
i
++
)
{
Path
path
=
new
Path
(
multiFileDir
,
+
i
)
;
FSDataOutputStream
out
=
fs
.
create
(
path
)
;
if
(
numBytes
==
-
1
)
{
numBytes
=
rand
.
nextInt
(
MAX_BYTES
)
;
}
for
(
int
j
=
0
;
j
<
numBytes
;
j
++
)
{
out
.
write
(
rand
.
nextInt
(
)
)
;
}
out
.
close
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Test
public
void
testFormat
(
)
throws
IOException
{
LOG
.
info
(
)
;
LOG
.
info
(
+
MAX_SPLIT_COUNT
)
;
LOG
.
info
(
+
SPLIT_COUNT_INCR
)
;
LOG
.
info
(
+
MAX_BYTES
)
;
LOG
.
info
(
+
MAX_NUM_FILES
)
;
LOG
.
info
(
+
NUM_FILES_INCR
)
;
MultiFileInputFormat
<
Text
,
Text
>
format
=
new
DummyMultiFileInputFormat
(
)
;
FileSystem
fs
=
FileSystem
.
getLocal
(
job
)
;
for
(
int
numFiles
=
1
;
numFiles
<
MAX_NUM_FILES
;
numFiles
+=
(
NUM_FILES_INCR
/
2
)
+
rand
.
nextInt
(
NUM_FILES_INCR
/
2
)
)
{
Path
dir
=
initFiles
(
fs
,
numFiles
,
-
1
)
;
BitSet
bits
=
new
BitSet
(
numFiles
)
;
for
(
int
i
=
1
;
i
<
MAX_SPLIT_COUNT
;
i
+=
rand
.
nextInt
(
SPLIT_COUNT_INCR
)
+
1
)
{
@
Test
public
void
testRegexFilter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
RegexFilter
.
class
)
;
SequenceFileInputFilter
.
RegexFilter
.
setPattern
(
job
,
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
1
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
@
Test
public
void
testPercentFilter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
PercentFilter
.
class
)
;
SequenceFileInputFilter
.
PercentFilter
.
setFrequency
(
job
,
1000
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
@
Test
public
void
testPercentFilter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
PercentFilter
.
class
)
;
SequenceFileInputFilter
.
PercentFilter
.
setFrequency
(
job
,
1000
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
LOG
.
info
(
+
length
)
;
createSequenceFile
(
length
)
;
int
count
=
countRecords
(
1
)
;
@
Test
public
void
testMD5Filter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
MD5Filter
.
class
)
;
SequenceFileInputFilter
.
MD5Filter
.
setFrequency
(
job
,
1000
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
assertEquals
(
9
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
SortedRanges
.
Range
(
3
,
5
)
)
;
assertEquals
(
9
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
SortedRanges
.
Range
(
7
,
1
)
)
;
assertEquals
(
9
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
Range
(
1
,
12
)
)
;
assertEquals
(
12
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
Range
(
7
,
9
)
)
;
assertEquals
(
15
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
Range
(
31
,
10
)
)
;
sr
.
add
(
new
Range
(
51
,
10
)
)
;
sr
.
add
(
new
Range
(
66
,
10
)
)
;
assertEquals
(
45
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
Range
(
21
,
50
)
)
;
assertEquals
(
70
,
sr
.
getIndicesCount
(
)
)
;
assertEquals
(
19
,
sr
.
getIndicesCount
(
)
)
;
sr
.
remove
(
new
SortedRanges
.
Range
(
15
,
8
)
)
;
assertEquals
(
13
,
sr
.
getIndicesCount
(
)
)
;
sr
.
remove
(
new
SortedRanges
.
Range
(
6
,
5
)
)
;
assertEquals
(
8
,
sr
.
getIndicesCount
(
)
)
;
sr
.
remove
(
new
SortedRanges
.
Range
(
8
,
4
)
)
;
assertEquals
(
7
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
Range
(
18
,
5
)
)
;
assertEquals
(
12
,
sr
.
getIndicesCount
(
)
)
;
sr
.
add
(
new
Range
(
25
,
1
)
)
;
assertEquals
(
13
,
sr
.
getIndicesCount
(
)
)
;
sr
.
remove
(
new
SortedRanges
.
Range
(
7
,
24
)
)
;
assertEquals
(
4
,
sr
.
getIndicesCount
(
)
)
;
sr
.
remove
(
new
SortedRanges
.
Range
(
5
,
1
)
)
;
assertEquals
(
3
,
sr
.
getIndicesCount
(
)
)
;
conf
.
setOutputKeyClass
(
LongWritable
.
class
)
;
conf
.
setOutputValueClass
(
Text
.
class
)
;
conf
.
setMapperClass
(
IdentityMapper
.
class
)
;
conf
.
setReducerClass
(
IdentityReducer
.
class
)
;
FileInputFormat
.
setInputPaths
(
conf
,
inDir
)
;
FileOutputFormat
.
setOutputPath
(
conf
,
outDir
)
;
conf
.
setNumMapTasks
(
numMaps
)
;
conf
.
setNumReduceTasks
(
numReduces
)
;
RunningJob
runningJob
=
JobClient
.
runJob
(
conf
)
;
try
{
assertTrue
(
runningJob
.
isComplete
(
)
)
;
assertTrue
(
runningJob
.
isSuccessful
(
)
)
;
assertTrue
(
,
fs
.
exists
(
new
Path
(
+
OUTPUT_FILENAME
)
)
)
;
}
catch
(
NullPointerException
npe
)
{
fail
(
)
;
@
Test
(
timeout
=
500000
)
public
void
testFormat
(
)
throws
Exception
{
JobConf
job
=
new
JobConf
(
defaultConf
)
;
Path
file
=
new
Path
(
workDir
,
)
;
Reporter
reporter
=
Reporter
.
NULL
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
@
Test
(
timeout
=
500000
)
public
void
testFormat
(
)
throws
Exception
{
JobConf
job
=
new
JobConf
(
defaultConf
)
;
Path
file
=
new
Path
(
workDir
,
)
;
Reporter
reporter
=
Reporter
.
NULL
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
LOG
.
info
(
+
seed
)
;
Random
random
=
new
Random
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
LOG
.
debug
(
+
length
)
;
Writer
writer
=
new
OutputStreamWriter
(
localFs
.
create
(
file
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
TextInputFormat
format
=
new
TextInputFormat
(
)
;
format
.
configure
(
job
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
Writer
writer
=
new
OutputStreamWriter
(
localFs
.
create
(
file
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
TextInputFormat
format
=
new
TextInputFormat
(
)
;
format
.
configure
(
job
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
LOG
.
debug
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
job
,
numSplits
)
;
if
(
length
==
0
)
{
assertEquals
(
,
1
,
splits
.
length
)
;
assertEquals
(
,
0
,
splits
[
0
]
.
getLength
(
)
)
;
}
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
length
;
j
++
)
{
LOG
.
debug
(
+
j
+
+
splits
[
j
]
)
;
RecordReader
<
LongWritable
,
Text
>
reader
=
format
.
getRecordReader
(
splits
[
j
]
,
job
,
reporter
)
;
try
{
int
count
=
0
;
while
(
reader
.
next
(
key
,
value
)
)
{
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
LOG
.
debug
(
+
v
)
;
if
(
bits
.
get
(
v
)
)
{
LOG
.
warn
(
+
v
+
+
j
+
+
reader
.
getPos
(
)
)
;
}
assertFalse
(
,
bits
.
get
(
v
)
)
;
}
Path
file
=
new
Path
(
workDir
,
+
codec
.
getDefaultExtension
(
)
)
;
FileSystem
localFs
=
FileSystem
.
getLocal
(
conf
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
conf
,
workDir
)
;
int
length
=
250000
;
LOG
.
info
(
+
length
)
;
Writer
writer
=
new
OutputStreamWriter
(
codec
.
createOutputStream
(
localFs
.
create
(
file
)
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
for
(
long
splitpos
=
203418
;
splitpos
<
203430
;
++
splitpos
)
{
TextInputFormat
format
=
new
TextInputFormat
(
)
;
conf
.
setLong
(
,
splitpos
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
conf
,
2
)
;
LOG
.
info
(
+
splits
.
length
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
length
;
j
++
)
{
LOG
.
debug
(
+
j
+
+
splits
[
j
]
)
;
RecordReader
<
LongWritable
,
Text
>
reader
=
format
.
getRecordReader
(
splits
[
j
]
,
conf
,
Reporter
.
NULL
)
;
try
{
int
counter
=
0
;
while
(
reader
.
next
(
key
,
value
)
)
{
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
LOG
.
debug
(
+
v
)
;
if
(
bits
.
get
(
v
)
)
{
Text
value
=
new
Text
(
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
conf
,
2
)
;
LOG
.
info
(
+
splits
.
length
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
length
;
j
++
)
{
LOG
.
debug
(
+
j
+
+
splits
[
j
]
)
;
RecordReader
<
LongWritable
,
Text
>
reader
=
format
.
getRecordReader
(
splits
[
j
]
,
conf
,
Reporter
.
NULL
)
;
try
{
int
counter
=
0
;
while
(
reader
.
next
(
key
,
value
)
)
{
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
LOG
.
debug
(
+
v
)
;
if
(
bits
.
get
(
v
)
)
{
LOG
.
warn
(
+
v
+
+
j
+
+
reader
.
getPos
(
)
)
;
}
assertFalse
(
,
bits
.
get
(
v
)
)
;
private
void
verifyPartitions
(
int
length
,
int
numSplits
,
Path
file
,
CompressionCodec
codec
,
JobConf
conf
)
throws
IOException
{
private
void
verifyPartitions
(
int
length
,
int
numSplits
,
Path
file
,
CompressionCodec
codec
,
JobConf
conf
)
throws
IOException
{
LOG
.
info
(
+
length
)
;
Writer
writer
=
new
OutputStreamWriter
(
codec
.
createOutputStream
(
localFs
.
create
(
file
)
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
TextInputFormat
format
=
new
TextInputFormat
(
)
;
format
.
configure
(
conf
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
LOG
.
info
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
conf
,
numSplits
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
TextInputFormat
format
=
new
TextInputFormat
(
)
;
format
.
configure
(
conf
)
;
LongWritable
key
=
new
LongWritable
(
)
;
Text
value
=
new
Text
(
)
;
LOG
.
info
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
conf
,
numSplits
)
;
LOG
.
info
(
+
splits
.
length
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
length
;
j
++
)
{
LOG
.
info
(
+
numSplits
)
;
InputSplit
[
]
splits
=
format
.
getSplits
(
conf
,
numSplits
)
;
LOG
.
info
(
+
splits
.
length
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
length
;
j
++
)
{
LOG
.
debug
(
+
j
+
+
splits
[
j
]
)
;
RecordReader
<
LongWritable
,
Text
>
reader
=
format
.
getRecordReader
(
splits
[
j
]
,
conf
,
Reporter
.
NULL
)
;
try
{
int
counter
=
0
;
while
(
reader
.
next
(
key
,
value
)
)
{
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
LOG
.
debug
(
+
v
)
;
if
(
bits
.
get
(
v
)
)
{
LOG
.
warn
(
+
v
+
+
j
+
+
reader
.
getPos
(
)
)
;
}
assertFalse
(
,
bits
.
get
(
v
)
)
;
LOG
.
info
(
+
splits
.
length
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
length
;
j
++
)
{
LOG
.
debug
(
+
j
+
+
splits
[
j
]
)
;
RecordReader
<
LongWritable
,
Text
>
reader
=
format
.
getRecordReader
(
splits
[
j
]
,
conf
,
Reporter
.
NULL
)
;
try
{
int
counter
=
0
;
while
(
reader
.
next
(
key
,
value
)
)
{
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
LOG
.
debug
(
+
v
)
;
if
(
bits
.
get
(
v
)
)
{
LOG
.
warn
(
+
v
+
+
j
+
+
reader
.
getPos
(
)
)
;
}
assertFalse
(
,
bits
.
get
(
v
)
)
;
bits
.
set
(
v
)
;
counter
++
;
@
Test
(
timeout
=
20000
)
public
void
testJobSubmissionFailure
(
)
throws
Exception
{
when
(
resourceMgrDelegate
.
submitApplication
(
any
(
ApplicationSubmissionContext
.
class
)
)
)
.
thenReturn
(
appId
)
;
ApplicationReport
report
=
mock
(
ApplicationReport
.
class
)
;
when
(
report
.
getApplicationId
(
)
)
.
thenReturn
(
appId
)
;
when
(
report
.
getDiagnostics
(
)
)
.
thenReturn
(
failString
)
;
when
(
report
.
getYarnApplicationState
(
)
)
.
thenReturn
(
YarnApplicationState
.
FAILED
)
;
when
(
resourceMgrDelegate
.
getApplicationReport
(
appId
)
)
.
thenReturn
(
report
)
;
Credentials
credentials
=
new
Credentials
(
)
;
File
jobxml
=
new
File
(
testWorkDir
,
)
;
OutputStream
out
=
new
FileOutputStream
(
jobxml
)
;
conf
.
writeXml
(
out
)
;
out
.
close
(
)
;
try
{
yarnRunner
.
submitJob
(
jobId
,
testWorkDir
.
getAbsolutePath
(
)
.
toString
(
)
,
credentials
)
;
}
catch
(
IOException
io
)
{
job
.
setOutputFormat
(
SequenceFileOutputFormat
.
class
)
;
job
.
setMapperClass
(
Map
.
class
)
;
job
.
setReducerClass
(
IdentityReducer
.
class
)
;
job
.
setOutputKeyClass
(
BytesWritable
.
class
)
;
job
.
setOutputValueClass
(
BytesWritable
.
class
)
;
JobClient
client
=
new
JobClient
(
job
)
;
ClusterStatus
cluster
=
client
.
getClusterStatus
(
)
;
long
totalDataSize
=
dataSizePerMap
*
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
;
job
.
set
(
,
String
.
valueOf
(
dataSizePerMap
*
1024
*
1024
)
)
;
job
.
setNumReduceTasks
(
0
)
;
job
.
setNumMapTasks
(
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
)
;
FileOutputFormat
.
setOutputPath
(
job
,
INPUT_DIR
)
;
FileSystem
fs
=
FileSystem
.
get
(
job
)
;
fs
.
delete
(
BASE_DIR
,
true
)
;
LOG
.
info
(
)
;
job
.
setMapperClass
(
Map
.
class
)
;
job
.
setReducerClass
(
IdentityReducer
.
class
)
;
job
.
setOutputKeyClass
(
BytesWritable
.
class
)
;
job
.
setOutputValueClass
(
BytesWritable
.
class
)
;
JobClient
client
=
new
JobClient
(
job
)
;
ClusterStatus
cluster
=
client
.
getClusterStatus
(
)
;
long
totalDataSize
=
dataSizePerMap
*
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
;
job
.
set
(
,
String
.
valueOf
(
dataSizePerMap
*
1024
*
1024
)
)
;
job
.
setNumReduceTasks
(
0
)
;
job
.
setNumMapTasks
(
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
)
;
FileOutputFormat
.
setOutputPath
(
job
,
INPUT_DIR
)
;
FileSystem
fs
=
FileSystem
.
get
(
job
)
;
fs
.
delete
(
BASE_DIR
,
true
)
;
LOG
.
info
(
)
;
LOG
.
info
(
+
totalDataSize
+
)
;
job
.
setOutputKeyClass
(
BytesWritable
.
class
)
;
job
.
setOutputValueClass
(
BytesWritable
.
class
)
;
JobClient
client
=
new
JobClient
(
job
)
;
ClusterStatus
cluster
=
client
.
getClusterStatus
(
)
;
long
totalDataSize
=
dataSizePerMap
*
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
;
job
.
set
(
,
String
.
valueOf
(
dataSizePerMap
*
1024
*
1024
)
)
;
job
.
setNumReduceTasks
(
0
)
;
job
.
setNumMapTasks
(
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
)
;
FileOutputFormat
.
setOutputPath
(
job
,
INPUT_DIR
)
;
FileSystem
fs
=
FileSystem
.
get
(
job
)
;
fs
.
delete
(
BASE_DIR
,
true
)
;
LOG
.
info
(
)
;
LOG
.
info
(
+
totalDataSize
+
)
;
LOG
.
info
(
+
dataSizePerMap
+
)
;
LOG
.
info
(
+
numSpillsPerMap
)
;
job
.
setOutputValueClass
(
BytesWritable
.
class
)
;
JobClient
client
=
new
JobClient
(
job
)
;
ClusterStatus
cluster
=
client
.
getClusterStatus
(
)
;
long
totalDataSize
=
dataSizePerMap
*
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
;
job
.
set
(
,
String
.
valueOf
(
dataSizePerMap
*
1024
*
1024
)
)
;
job
.
setNumReduceTasks
(
0
)
;
job
.
setNumMapTasks
(
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
)
;
FileOutputFormat
.
setOutputPath
(
job
,
INPUT_DIR
)
;
FileSystem
fs
=
FileSystem
.
get
(
job
)
;
fs
.
delete
(
BASE_DIR
,
true
)
;
LOG
.
info
(
)
;
LOG
.
info
(
+
totalDataSize
+
)
;
LOG
.
info
(
+
dataSizePerMap
+
)
;
LOG
.
info
(
+
numSpillsPerMap
)
;
LOG
.
info
(
+
numMapsPerHost
)
;
ClusterStatus
cluster
=
client
.
getClusterStatus
(
)
;
job
.
setNumMapTasks
(
numMapsPerHost
*
cluster
.
getTaskTrackers
(
)
)
;
job
.
setNumReduceTasks
(
1
)
;
int
ioSortMb
=
(
int
)
Math
.
ceil
(
FACTOR
*
dataSizePerMap
)
;
job
.
set
(
JobContext
.
IO_SORT_MB
,
String
.
valueOf
(
ioSortMb
)
)
;
fs
=
FileSystem
.
get
(
job
)
;
LOG
.
info
(
)
;
long
startTime
=
System
.
currentTimeMillis
(
)
;
JobClient
.
runJob
(
job
)
;
long
endTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
String
.
valueOf
(
endTime
-
startTime
)
+
)
;
fs
.
delete
(
OUTPUT_DIR
,
true
)
;
JobConf
spilledJob
=
new
JobConf
(
job
,
ThreadedMapBenchmark
.
class
)
;
ioSortMb
=
(
int
)
Math
.
ceil
(
FACTOR
*
Math
.
ceil
(
(
double
)
dataSizePerMap
/
numSpillsPerMap
)
)
;
spilledJob
.
set
(
JobContext
.
IO_SORT_MB
,
String
.
valueOf
(
ioSortMb
)
)
;
public
JobInfo
parseHistoryFile
(
Path
path
)
throws
IOException
{
public
Configuration
parseConfiguration
(
Path
path
)
throws
IOException
{
LOG
.
info
(
context
.
getTaskAttemptID
(
)
.
getTaskID
(
)
+
)
;
}
else
{
LOG
.
info
(
context
.
getTaskAttemptID
(
)
.
getTaskID
(
)
+
+
jobs
.
size
(
)
+
)
;
}
for
(
JobFiles
job
:
jobs
)
{
String
jobIdStr
=
job
.
getJobId
(
)
;
LOG
.
info
(
+
jobIdStr
+
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
JobID
.
forName
(
jobIdStr
)
)
;
ApplicationId
appId
=
jobId
.
getAppId
(
)
;
try
{
Path
historyFilePath
=
job
.
getJobHistoryFilePath
(
)
;
Path
confFilePath
=
job
.
getJobConfFilePath
(
)
;
if
(
(
historyFilePath
==
null
)
||
(
confFilePath
==
null
)
)
{
continue
;
}
JobInfo
jobInfo
=
parser
.
parseHistoryFile
(
historyFilePath
)
;
Configuration
jobConf
=
parser
.
parseConfiguration
(
confFilePath
)
;
LOG
.
info
(
context
.
getTaskAttemptID
(
)
.
getTaskID
(
)
+
+
jobs
.
size
(
)
+
)
;
}
for
(
JobFiles
job
:
jobs
)
{
String
jobIdStr
=
job
.
getJobId
(
)
;
LOG
.
info
(
+
jobIdStr
+
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
JobID
.
forName
(
jobIdStr
)
)
;
ApplicationId
appId
=
jobId
.
getAppId
(
)
;
try
{
Path
historyFilePath
=
job
.
getJobHistoryFilePath
(
)
;
Path
confFilePath
=
job
.
getJobConfFilePath
(
)
;
if
(
(
historyFilePath
==
null
)
||
(
confFilePath
==
null
)
)
{
continue
;
}
JobInfo
jobInfo
=
parser
.
parseHistoryFile
(
historyFilePath
)
;
Configuration
jobConf
=
parser
.
parseConfiguration
(
confFilePath
)
;
LOG
.
info
(
+
jobIdStr
)
;
long
totalTime
=
0
;
long
totalTime
=
0
;
Set
<
TimelineEntity
>
entitySet
=
converter
.
createTimelineEntities
(
jobInfo
,
jobConf
)
;
LOG
.
info
(
+
jobIdStr
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
long
startWrite
=
System
.
nanoTime
(
)
;
try
{
switch
(
replayMode
)
{
case
JobHistoryFileReplayHelper
.
WRITE_ALL_AT_ONCE
:
writeAllEntities
(
tlc
,
entitySet
,
ugi
)
;
break
;
case
JobHistoryFileReplayHelper
.
WRITE_PER_ENTITY
:
writePerEntity
(
tlc
,
entitySet
,
ugi
)
;
break
;
default
:
break
;
}
}
catch
(
Exception
e
)
{
context
.
getCounter
(
PerfCounters
.
TIMELINE_SERVICE_WRITE_FAILURES
)
.
increment
(
1
)
;
LOG
.
error
(
,
e
)
;
private
void
writePerEntity
(
TimelineClient
tlc
,
Set
<
TimelineEntity
>
entitySet
,
UserGroupInformation
ugi
)
throws
IOException
,
YarnException
{
for
(
TimelineEntity
entity
:
entitySet
)
{
tlc
.
putEntities
(
entity
)
;
}
else
{
LOG
.
info
(
context
.
getTaskAttemptID
(
)
.
getTaskID
(
)
+
+
jobs
.
size
(
)
+
)
;
}
for
(
JobFiles
job
:
jobs
)
{
String
jobIdStr
=
job
.
getJobId
(
)
;
if
(
job
.
getJobConfFilePath
(
)
==
null
||
job
.
getJobHistoryFilePath
(
)
==
null
)
{
LOG
.
info
(
jobIdStr
+
+
)
;
continue
;
}
LOG
.
info
(
+
jobIdStr
+
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
JobID
.
forName
(
jobIdStr
)
)
;
ApplicationId
appId
=
jobId
.
getAppId
(
)
;
AppLevelTimelineCollector
collector
=
new
AppLevelTimelineCollector
(
appId
)
;
manager
.
putIfAbsent
(
appId
,
collector
)
;
try
{
JobInfo
jobInfo
=
parser
.
parseHistoryFile
(
job
.
getJobHistoryFilePath
(
)
)
;
Configuration
jobConf
=
parser
.
parseConfiguration
(
job
.
getJobConfFilePath
(
)
)
;
LOG
.
info
(
jobIdStr
+
+
)
;
continue
;
}
LOG
.
info
(
+
jobIdStr
+
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
JobID
.
forName
(
jobIdStr
)
)
;
ApplicationId
appId
=
jobId
.
getAppId
(
)
;
AppLevelTimelineCollector
collector
=
new
AppLevelTimelineCollector
(
appId
)
;
manager
.
putIfAbsent
(
appId
,
collector
)
;
try
{
JobInfo
jobInfo
=
parser
.
parseHistoryFile
(
job
.
getJobHistoryFilePath
(
)
)
;
Configuration
jobConf
=
parser
.
parseConfiguration
(
job
.
getJobConfFilePath
(
)
)
;
LOG
.
info
(
+
+
jobIdStr
)
;
TimelineCollectorContext
tlContext
=
collector
.
getTimelineEntityContext
(
)
;
tlContext
.
setFlowName
(
jobInfo
.
getJobname
(
)
)
;
tlContext
.
setFlowRunId
(
jobInfo
.
getSubmitTime
(
)
)
;
tlContext
.
setUserId
(
jobInfo
.
getUsername
(
)
)
;
long
totalTime
=
0
;
List
<
TimelineEntity
>
entitySet
=
converter
.
createTimelineEntities
(
jobInfo
,
jobConf
)
;
LOG
.
info
(
+
jobIdStr
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
long
startWrite
=
System
.
nanoTime
(
)
;
try
{
switch
(
replayMode
)
{
case
JobHistoryFileReplayHelper
.
WRITE_ALL_AT_ONCE
:
writeAllEntities
(
collector
,
entitySet
,
ugi
)
;
break
;
case
JobHistoryFileReplayHelper
.
WRITE_PER_ENTITY
:
writePerEntity
(
collector
,
entitySet
,
ugi
)
;
break
;
default
:
break
;
}
}
catch
(
Exception
e
)
{
context
.
getCounter
(
PerfCounters
.
TIMELINE_SERVICE_WRITE_FAILURES
)
.
increment
(
1
)
;
LOG
.
error
(
,
e
)
;
private
void
writePerEntity
(
AppLevelTimelineCollector
collector
,
List
<
TimelineEntity
>
entitySet
,
UserGroupInformation
ugi
)
throws
IOException
{
for
(
TimelineEntity
entity
:
entitySet
)
{
TimelineEntities
entities
=
new
TimelineEntities
(
)
;
entities
.
addEntity
(
entity
)
;
collector
.
putEntities
(
entities
,
ugi
)
;
public
static
String
readOutput
(
Path
outDir
,
Configuration
conf
)
throws
IOException
{
FileSystem
fs
=
outDir
.
getFileSystem
(
conf
)
;
StringBuffer
result
=
new
StringBuffer
(
)
;
Path
[
]
fileList
=
FileUtil
.
stat2Paths
(
fs
.
listStatus
(
outDir
,
new
Utils
.
OutputFileUtils
.
OutputFilesFilter
(
)
)
)
;
for
(
Path
outputFile
:
fileList
)
{
}
String
entId
=
taskAttemptId
+
+
Integer
.
toString
(
i
)
;
final
TimelineEntity
entity
=
new
TimelineEntity
(
)
;
entity
.
setEntityId
(
entId
)
;
entity
.
setEntityType
(
)
;
entity
.
addOtherInfo
(
,
payLoad
)
;
TimelineEvent
event
=
new
TimelineEvent
(
)
;
event
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
event
.
setEventType
(
)
;
entity
.
addEvent
(
event
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
long
startWrite
=
System
.
nanoTime
(
)
;
try
{
tlc
.
putEntities
(
entity
)
;
}
catch
(
Exception
e
)
{
context
.
getCounter
(
PerfCounters
.
TIMELINE_SERVICE_WRITE_FAILURES
)
.
increment
(
1
)
;
event
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
event
.
addInfo
(
,
)
;
entity
.
addEvent
(
event
)
;
TimelineMetric
metric
=
new
TimelineMetric
(
)
;
metric
.
setId
(
)
;
metric
.
addValue
(
System
.
currentTimeMillis
(
)
,
123456789L
)
;
entity
.
addMetric
(
metric
)
;
entity
.
addConfig
(
,
)
;
TimelineEntities
entities
=
new
TimelineEntities
(
)
;
entities
.
addEntity
(
entity
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
long
startWrite
=
System
.
nanoTime
(
)
;
try
{
collector
.
putEntities
(
entities
,
ugi
)
;
}
catch
(
Exception
e
)
{
final
Thread
toInterrupt
=
Thread
.
currentThread
(
)
;
Thread
interrupter
=
new
Thread
(
)
{
public
void
run
(
)
{
try
{
Thread
.
sleep
(
120
*
1000
)
;
toInterrupt
.
interrupt
(
)
;
}
catch
(
InterruptedException
ie
)
{
}
}
}
;
LOG
.
info
(
)
;
job
.
submit
(
)
;
LOG
.
info
(
)
;
interrupter
.
start
(
)
;
LOG
.
info
(
)
;
try
{
job
.
waitForCompletion
(
true
)
;
Path
outputDir
=
getOutputPath
(
)
;
Configuration
conf
=
new
Configuration
(
)
;
FileSystem
fs
=
FileSystem
.
getLocal
(
conf
)
;
FileStatus
[
]
stats
=
fs
.
listStatus
(
outputDir
)
;
int
valueSum
=
0
;
for
(
FileStatus
f
:
stats
)
{
FSDataInputStream
istream
=
fs
.
open
(
f
.
getPath
(
)
)
;
BufferedReader
r
=
new
BufferedReader
(
new
InputStreamReader
(
istream
)
)
;
String
line
=
null
;
while
(
(
line
=
r
.
readLine
(
)
)
!=
null
)
{
valueSum
+=
Integer
.
valueOf
(
line
.
trim
(
)
)
;
}
r
.
close
(
)
;
}
int
maxVal
=
NUMBER_FILE_VAL
-
1
;
int
expectedPerMapper
=
maxVal
*
(
maxVal
+
1
)
/
2
;
int
expectedSum
=
expectedPerMapper
*
numMaps
;
public
static
int
runTool
(
Configuration
conf
,
Tool
tool
,
String
[
]
args
,
OutputStream
out
)
throws
Exception
{
private
void
testListBlackList
(
Configuration
conf
)
throws
Exception
{
CLI
jc
=
createJobClient
(
)
;
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
int
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
,
}
,
out
)
;
assertEquals
(
,
-
1
,
exitCode
)
;
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
}
,
out
)
;
assertEquals
(
,
0
,
exitCode
)
;
String
line
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
out
.
toByteArray
(
)
)
)
)
;
int
counter
=
0
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
private
void
testListAttemptIds
(
String
jobId
,
Configuration
conf
)
throws
Exception
{
CLI
jc
=
createJobClient
(
)
;
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
int
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
}
,
out
)
;
assertEquals
(
,
-
1
,
exitCode
)
;
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
,
jobId
,
,
}
,
out
)
;
assertEquals
(
,
0
,
exitCode
)
;
String
line
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
out
.
toByteArray
(
)
)
)
)
;
int
counter
=
0
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
private
void
testListTrackers
(
Configuration
conf
)
throws
Exception
{
CLI
jc
=
createJobClient
(
)
;
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
int
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
,
}
,
out
)
;
assertEquals
(
,
-
1
,
exitCode
)
;
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
}
,
out
)
;
assertEquals
(
,
0
,
exitCode
)
;
String
line
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
out
.
toByteArray
(
)
)
)
)
;
int
counter
=
0
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
private
void
testJobEvents
(
String
jobId
,
Configuration
conf
)
throws
Exception
{
CLI
jc
=
createJobClient
(
)
;
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
int
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
}
,
out
)
;
assertEquals
(
,
-
1
,
exitCode
)
;
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
,
jobId
,
,
}
,
out
)
;
assertEquals
(
,
0
,
exitCode
)
;
String
line
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
out
.
toByteArray
(
)
)
)
)
;
int
counter
=
0
;
String
attemptId
=
(
+
jobId
.
substring
(
3
)
)
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
private
void
testJobStatus
(
String
jobId
,
Configuration
conf
)
throws
Exception
{
CLI
jc
=
createJobClient
(
)
;
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
int
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
}
,
out
)
;
assertEquals
(
,
-
1
,
exitCode
)
;
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
,
jobId
}
,
out
)
;
assertEquals
(
,
0
,
exitCode
)
;
String
line
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
out
.
toByteArray
(
)
)
)
)
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
protected
void
testAllJobList
(
String
jobId
,
Configuration
conf
)
throws
Exception
{
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
int
exitCode
=
runTool
(
conf
,
createJobClient
(
)
,
new
String
[
]
{
,
}
,
out
)
;
assertEquals
(
,
-
1
,
exitCode
)
;
exitCode
=
runTool
(
conf
,
createJobClient
(
)
,
new
String
[
]
{
,
}
,
out
)
;
assertEquals
(
,
0
,
exitCode
)
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
out
.
toByteArray
(
)
)
)
)
;
String
line
;
int
counter
=
0
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
protected
void
testSubmittedJobList
(
Configuration
conf
)
throws
Exception
{
Job
job
=
runJobInBackGround
(
conf
)
;
ByteArrayOutputStream
out
=
new
ByteArrayOutputStream
(
)
;
String
line
;
int
counter
=
0
;
int
exitCode
=
runTool
(
conf
,
createJobClient
(
)
,
new
String
[
]
{
}
,
out
)
;
assertEquals
(
,
0
,
exitCode
)
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
out
.
toByteArray
(
)
)
)
)
;
counter
=
0
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
protected
void
verifyJobPriority
(
String
jobId
,
String
priority
,
Configuration
conf
,
CLI
jc
)
throws
Exception
{
PipedInputStream
pis
=
new
PipedInputStream
(
)
;
PipedOutputStream
pos
=
new
PipedOutputStream
(
pis
)
;
int
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
,
}
,
pos
)
;
assertEquals
(
,
0
,
exitCode
)
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
pis
)
)
;
String
line
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
protected
void
verifyJobName
(
String
jobId
,
String
name
,
Configuration
conf
,
CLI
jc
)
throws
Exception
{
PipedInputStream
pis
=
new
PipedInputStream
(
)
;
PipedOutputStream
pos
=
new
PipedOutputStream
(
pis
)
;
int
exitCode
=
runTool
(
conf
,
jc
,
new
String
[
]
{
,
}
,
pos
)
;
assertEquals
(
,
0
,
exitCode
)
;
BufferedReader
br
=
new
BufferedReader
(
new
InputStreamReader
(
pis
)
)
;
String
line
=
null
;
while
(
(
line
=
br
.
readLine
(
)
)
!=
null
)
{
private
static
void
runTest
(
String
name
,
Job
job
)
throws
Exception
{
job
.
setNumReduceTasks
(
1
)
;
job
.
getConfiguration
(
)
.
set
(
MRConfig
.
FRAMEWORK_NAME
,
MRConfig
.
LOCAL_FRAMEWORK_NAME
)
;
job
.
getConfiguration
(
)
.
setInt
(
MRJobConfig
.
IO_SORT_FACTOR
,
1000
)
;
job
.
getConfiguration
(
)
.
set
(
,
)
;
job
.
getConfiguration
(
)
.
setInt
(
,
1
)
;
job
.
setInputFormatClass
(
FakeIF
.
class
)
;
job
.
setOutputFormatClass
(
NullOutputFormat
.
class
)
;
job
.
setMapperClass
(
Mapper
.
class
)
;
job
.
setReducerClass
(
SpillReducer
.
class
)
;
job
.
setMapOutputKeyClass
(
KeyWritable
.
class
)
;
job
.
setMapOutputValueClass
(
ValWritable
.
class
)
;
job
.
setSortComparatorClass
(
VariableComparator
.
class
)
;
@
Test
public
void
testRandom
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
Job
.
COMPLETION_POLL_INTERVAL_KEY
,
100
)
;
Job
job
=
Job
.
getInstance
(
conf
)
;
conf
=
job
.
getConfiguration
(
)
;
conf
.
setInt
(
MRJobConfig
.
IO_SORT_MB
,
1
)
;
conf
.
setClass
(
,
RandomFactory
.
class
,
RecordFactory
.
class
)
;
final
Random
r
=
new
Random
(
)
;
final
long
seed
=
r
.
nextLong
(
)
;
@
Test
public
void
testRandomCompress
(
)
throws
Exception
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
Job
.
COMPLETION_POLL_INTERVAL_KEY
,
100
)
;
Job
job
=
Job
.
getInstance
(
conf
)
;
conf
=
job
.
getConfiguration
(
)
;
conf
.
setInt
(
MRJobConfig
.
IO_SORT_MB
,
1
)
;
conf
.
setBoolean
(
MRJobConfig
.
MAP_OUTPUT_COMPRESS
,
true
)
;
conf
.
setClass
(
,
RandomFactory
.
class
,
RecordFactory
.
class
)
;
final
Random
r
=
new
Random
(
)
;
final
long
seed
=
r
.
nextLong
(
)
;
private
static
int
test0
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
int
errors
=
0
;
IntWritable
i
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
private
static
int
test0
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
int
errors
=
0
;
IntWritable
i
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
LOG
.
info
(
+
key
.
toString
(
)
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
expectedValues
.
add
(
i
)
;
int
errors
=
0
;
IntWritable
i
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
LOG
.
info
(
+
key
.
toString
(
)
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
expectedValues
.
add
(
i
)
;
LOG
.
info
(
key
+
+
i
)
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
int
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
LOG
.
info
(
+
key
.
toString
(
)
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
expectedValues
.
add
(
i
)
;
LOG
.
info
(
key
+
+
i
)
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
int
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
i
!=
expectedValues
.
get
(
count
)
)
{
private
static
int
test1
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
IntWritable
i
;
int
errors
=
0
;
int
count
=
0
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
ArrayList
<
IntWritable
>
expectedValues1
=
new
ArrayList
<
IntWritable
>
(
)
;
private
static
int
test1
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
IntWritable
i
;
int
errors
=
0
;
int
count
=
0
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
ArrayList
<
IntWritable
>
expectedValues1
=
new
ArrayList
<
IntWritable
>
(
)
;
LOG
.
info
(
+
key
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
+
key
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
expectedValues
.
add
(
i
)
;
if
(
count
==
2
)
{
break
;
}
count
++
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
expectedValues
.
add
(
i
)
;
if
(
count
==
2
)
{
break
;
}
count
++
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues
.
size
(
)
)
{
if
(
i
!=
expectedValues
.
get
(
count
)
)
{
errors
++
;
}
if
(
count
==
3
)
{
values
.
mark
(
)
;
LOG
.
info
(
+
key
+
+
i
)
;
}
if
(
count
>=
3
)
{
expectedValues1
.
add
(
i
)
;
}
if
(
count
==
5
)
{
break
;
}
count
++
;
}
if
(
count
<
expectedValues
.
size
(
)
)
{
LOG
.
info
(
(
)
)
;
errors
++
;
return
errors
;
}
values
.
reset
(
)
;
count
=
0
;
LOG
.
info
(
)
;
}
if
(
count
>=
3
)
{
expectedValues1
.
add
(
i
)
;
}
if
(
count
==
5
)
{
break
;
}
count
++
;
}
if
(
count
<
expectedValues
.
size
(
)
)
{
LOG
.
info
(
(
)
)
;
errors
++
;
return
errors
;
}
values
.
reset
(
)
;
count
=
0
;
LOG
.
info
(
)
;
expectedValues
.
clear
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
expectedValues
.
clear
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues1
.
size
(
)
)
{
if
(
i
!=
expectedValues1
.
get
(
count
)
)
{
errors
++
;
LOG
.
info
(
+
expectedValues1
.
get
(
count
)
+
+
i
)
;
return
errors
;
}
}
if
(
count
==
25
)
{
values
.
mark
(
)
;
LOG
.
info
(
+
key
+
+
i
)
;
}
if
(
count
>=
25
)
{
expectedValues
.
add
(
i
)
;
}
count
++
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues1
.
size
(
)
)
{
if
(
i
!=
expectedValues1
.
get
(
count
)
)
{
errors
++
;
LOG
.
info
(
+
expectedValues1
.
get
(
count
)
+
+
i
)
;
return
errors
;
}
}
if
(
count
==
25
)
{
values
.
mark
(
)
;
LOG
.
info
(
+
key
+
+
i
)
;
}
if
(
count
>=
25
)
{
expectedValues
.
add
(
i
)
;
}
count
++
;
}
if
(
count
<
expectedValues1
.
size
(
)
)
{
LOG
.
info
(
(
)
)
;
errors
++
;
private
static
int
test2
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
IntWritable
i
;
int
errors
=
0
;
int
count
=
0
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
ArrayList
<
IntWritable
>
expectedValues1
=
new
ArrayList
<
IntWritable
>
(
)
;
private
static
int
test2
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
IntWritable
i
;
int
errors
=
0
;
int
count
=
0
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
ArrayList
<
IntWritable
>
expectedValues1
=
new
ArrayList
<
IntWritable
>
(
)
;
LOG
.
info
(
+
key
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
+
key
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
expectedValues
.
add
(
i
)
;
if
(
count
==
8
)
{
break
;
}
count
++
;
}
values
.
reset
(
)
;
count
=
0
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
expectedValues
.
add
(
i
)
;
if
(
count
==
8
)
{
break
;
}
count
++
;
}
values
.
reset
(
)
;
count
=
0
;
LOG
.
info
(
)
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues
.
size
(
)
)
{
if
(
i
!=
expectedValues
.
get
(
count
)
)
{
errors
++
;
if
(
count
<
expectedValues
.
size
(
)
)
{
if
(
i
!=
expectedValues
.
get
(
count
)
)
{
errors
++
;
LOG
.
info
(
+
expectedValues
.
get
(
count
)
+
+
i
)
;
return
errors
;
}
}
if
(
count
==
3
)
{
values
.
mark
(
)
;
LOG
.
info
(
+
key
+
+
i
)
;
}
if
(
count
>=
3
)
{
expectedValues1
.
add
(
i
)
;
}
count
++
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
expectedValues
.
clear
(
)
;
count
=
0
;
return
errors
;
}
}
if
(
count
==
3
)
{
values
.
mark
(
)
;
LOG
.
info
(
+
key
+
+
i
)
;
}
if
(
count
>=
3
)
{
expectedValues1
.
add
(
i
)
;
}
count
++
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
expectedValues
.
clear
(
)
;
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues1
.
size
(
)
)
{
LOG
.
info
(
)
;
expectedValues
.
clear
(
)
;
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues1
.
size
(
)
)
{
if
(
i
!=
expectedValues1
.
get
(
count
)
)
{
errors
++
;
LOG
.
info
(
+
expectedValues1
.
get
(
count
)
+
+
i
)
;
return
errors
;
}
}
if
(
count
==
20
)
{
values
.
mark
(
)
;
LOG
.
info
(
+
key
+
+
i
)
;
}
if
(
count
>=
20
)
{
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues1
.
size
(
)
)
{
if
(
i
!=
expectedValues1
.
get
(
count
)
)
{
errors
++
;
LOG
.
info
(
+
expectedValues1
.
get
(
count
)
+
+
i
)
;
return
errors
;
}
}
if
(
count
==
20
)
{
values
.
mark
(
)
;
LOG
.
info
(
+
key
+
+
i
)
;
}
if
(
count
>=
20
)
{
expectedValues
.
add
(
i
)
;
}
count
++
;
}
values
.
reset
(
)
;
private
static
int
test3
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
int
errors
=
0
;
IntWritable
i
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
private
static
int
test3
(
IntWritable
key
,
MarkableIterator
<
IntWritable
>
values
)
throws
IOException
{
int
errors
=
0
;
IntWritable
i
;
ArrayList
<
IntWritable
>
expectedValues
=
new
ArrayList
<
IntWritable
>
(
)
;
LOG
.
info
(
+
key
)
;
values
.
mark
(
)
;
LOG
.
info
(
)
;
int
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
;
}
if
(
count
==
8
)
{
LOG
.
info
(
+
key
+
+
i
)
;
values
.
mark
(
)
;
}
if
(
count
>=
8
)
{
expectedValues
.
add
(
i
)
;
}
count
++
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
if
(
!
values
.
hasNext
(
)
)
{
errors
++
;
LOG
.
info
(
)
;
return
errors
;
}
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
}
if
(
count
>=
8
)
{
expectedValues
.
add
(
i
)
;
}
count
++
;
}
values
.
reset
(
)
;
LOG
.
info
(
)
;
if
(
!
values
.
hasNext
(
)
)
{
errors
++
;
LOG
.
info
(
)
;
return
errors
;
}
count
=
0
;
while
(
values
.
hasNext
(
)
)
{
i
=
values
.
next
(
)
;
LOG
.
info
(
key
+
+
i
)
;
if
(
count
<
expectedValues
.
size
(
)
)
{
if
(
i
!=
expectedValues
.
get
(
count
)
)
{
private
void
validateOutput
(
)
throws
IOException
{
Path
[
]
outputFiles
=
FileUtil
.
stat2Paths
(
localFs
.
listStatus
(
new
Path
(
TEST_ROOT_DIR
+
)
,
new
Utils
.
OutputFileUtils
.
OutputFilesFilter
(
)
)
)
;
if
(
outputFiles
.
length
>
0
)
{
InputStream
is
=
localFs
.
open
(
outputFiles
[
0
]
)
;
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
is
)
)
;
String
line
=
reader
.
readLine
(
)
;
while
(
line
!=
null
)
{
StringTokenizer
tokeniz
=
new
StringTokenizer
(
line
,
)
;
String
key
=
tokeniz
.
nextToken
(
)
;
String
value
=
tokeniz
.
nextToken
(
)
;
job
.
setEntityType
(
JOB
)
;
job
.
setEntityId
(
jobInfo
.
getJobId
(
)
.
toString
(
)
)
;
job
.
setStartTime
(
jobInfo
.
getSubmitTime
(
)
)
;
job
.
addPrimaryFilter
(
,
jobInfo
.
getJobname
(
)
)
;
job
.
addPrimaryFilter
(
,
jobInfo
.
getUsername
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getJobQueueName
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getSubmitTime
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getLaunchTime
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getFinishTime
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getJobStatus
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getPriority
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getTotalMaps
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getTotalReduces
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getUberized
(
)
)
;
job
.
addOtherInfo
(
,
jobInfo
.
getErrorInfo
(
)
)
;
private
Set
<
TimelineEntity
>
createTaskAndTaskAttemptEntities
(
JobInfo
jobInfo
)
{
Set
<
TimelineEntity
>
entities
=
new
HashSet
<
>
(
)
;
Map
<
TaskID
,
TaskInfo
>
taskInfoMap
=
jobInfo
.
getAllTasks
(
)
;
private
TimelineEntity
createTaskEntity
(
TaskInfo
taskInfo
)
{
TimelineEntity
task
=
new
TimelineEntity
(
)
;
task
.
setEntityType
(
TASK
)
;
task
.
setEntityId
(
taskInfo
.
getTaskId
(
)
.
toString
(
)
)
;
task
.
setStartTime
(
taskInfo
.
getStartTime
(
)
)
;
task
.
addOtherInfo
(
,
taskInfo
.
getStartTime
(
)
)
;
task
.
addOtherInfo
(
,
taskInfo
.
getFinishTime
(
)
)
;
task
.
addOtherInfo
(
,
taskInfo
.
getTaskType
(
)
)
;
task
.
addOtherInfo
(
,
taskInfo
.
getTaskStatus
(
)
)
;
task
.
addOtherInfo
(
,
taskInfo
.
getError
(
)
)
;
private
Set
<
TimelineEntity
>
createTaskAttemptEntities
(
TaskInfo
taskInfo
)
{
Set
<
TimelineEntity
>
taskAttempts
=
new
HashSet
<
TimelineEntity
>
(
)
;
Map
<
TaskAttemptID
,
TaskAttemptInfo
>
taskAttemptInfoMap
=
taskInfo
.
getAllTaskAttempts
(
)
;
private
TimelineEntity
createTaskAttemptEntity
(
TaskAttemptInfo
taskAttemptInfo
)
{
TimelineEntity
taskAttempt
=
new
TimelineEntity
(
)
;
taskAttempt
.
setEntityType
(
TASK_ATTEMPT
)
;
taskAttempt
.
setEntityId
(
taskAttemptInfo
.
getAttemptId
(
)
.
toString
(
)
)
;
taskAttempt
.
setStartTime
(
taskAttemptInfo
.
getStartTime
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getStartTime
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getFinishTime
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getMapFinishTime
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getShuffleFinishTime
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getSortFinishTime
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getTaskStatus
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getState
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getError
(
)
)
;
taskAttempt
.
addOtherInfo
(
,
taskAttemptInfo
.
getContainerId
(
)
.
toString
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getUsername
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getJobQueueName
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getSubmitTime
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getLaunchTime
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getFinishTime
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getJobStatus
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getPriority
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getTotalMaps
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getTotalReduces
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getUberized
(
)
)
;
job
.
addInfo
(
,
jobInfo
.
getErrorInfo
(
)
)
;
Counters
totalCounters
=
jobInfo
.
getTotalCounters
(
)
;
if
(
totalCounters
!=
null
)
{
addMetrics
(
job
,
totalCounters
)
;
}
addConfiguration
(
job
,
conf
)
;
private
List
<
TimelineEntity
>
createTaskAndTaskAttemptEntities
(
JobInfo
jobInfo
)
{
List
<
TimelineEntity
>
entities
=
new
ArrayList
<
>
(
)
;
Map
<
TaskID
,
TaskInfo
>
taskInfoMap
=
jobInfo
.
getAllTasks
(
)
;
private
Set
<
TimelineEntity
>
createTaskAttemptEntities
(
TaskInfo
taskInfo
)
{
Set
<
TimelineEntity
>
taskAttempts
=
new
HashSet
<
TimelineEntity
>
(
)
;
Map
<
TaskAttemptID
,
TaskAttemptInfo
>
taskAttemptInfoMap
=
taskInfo
.
getAllTaskAttempts
(
)
;
@
Test
(
timeout
=
10000
)
public
void
testFormat
(
)
throws
IOException
,
InterruptedException
{
Job
job
=
Job
.
getInstance
(
conf
)
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
,
job
)
;
TaskAttemptContext
context
=
MapReduceTestUtil
.
createDummyMapTaskAttemptContext
(
job
.
getConfiguration
(
)
)
;
InputFormat
<
IntWritable
,
BytesWritable
>
format
=
new
CombineSequenceFileInputFormat
<
IntWritable
,
BytesWritable
>
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
(
SequenceFile
.
SYNC_INTERVAL
/
20
)
)
+
1
;
Job
job
=
Job
.
getInstance
(
conf
)
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
,
job
)
;
TaskAttemptContext
context
=
MapReduceTestUtil
.
createDummyMapTaskAttemptContext
(
job
.
getConfiguration
(
)
)
;
InputFormat
<
IntWritable
,
BytesWritable
>
format
=
new
CombineSequenceFileInputFormat
<
IntWritable
,
BytesWritable
>
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
(
SequenceFile
.
SYNC_INTERVAL
/
20
)
)
+
1
;
LOG
.
info
(
+
numSplits
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
LOG
.
info
(
+
splits
.
size
(
)
)
;
assertEquals
(
,
1
,
splits
.
size
(
)
)
;
InputSplit
split
=
splits
.
get
(
0
)
;
assertEquals
(
,
CombineFileSplit
.
class
,
split
.
getClass
(
)
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
RecordReader
<
IntWritable
,
BytesWritable
>
reader
=
format
.
createRecordReader
(
split
,
context
)
;
MapContext
<
IntWritable
,
BytesWritable
,
IntWritable
,
BytesWritable
>
mcontext
=
new
MapContextImpl
<
IntWritable
,
BytesWritable
,
IntWritable
,
BytesWritable
>
(
job
.
getConfiguration
(
)
,
context
.
getTaskAttemptID
(
)
,
reader
,
null
,
null
,
MapReduceTestUtil
.
createDummyReporter
(
)
,
split
)
;
reader
.
initialize
(
split
,
mcontext
)
;
assertEquals
(
,
CombineFileRecordReader
.
class
,
reader
.
getClass
(
)
)
;
try
{
while
(
reader
.
nextKeyValue
(
)
)
{
IntWritable
key
=
reader
.
getCurrentKey
(
)
;
BytesWritable
value
=
reader
.
getCurrentValue
(
)
;
assertNotNull
(
,
value
)
;
final
int
k
=
key
.
get
(
)
;
@
Test
(
timeout
=
10000
)
public
void
testFormat
(
)
throws
Exception
{
Job
job
=
Job
.
getInstance
(
new
Configuration
(
defaultConf
)
)
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
@
Test
(
timeout
=
10000
)
public
void
testFormat
(
)
throws
Exception
{
Job
job
=
Job
.
getInstance
(
new
Configuration
(
defaultConf
)
)
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
LOG
.
info
(
+
seed
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
CombineTextInputFormat
format
=
new
CombineTextInputFormat
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
20
)
+
1
;
Job
job
=
Job
.
getInstance
(
new
Configuration
(
defaultConf
)
)
;
Random
random
=
new
Random
(
)
;
long
seed
=
random
.
nextLong
(
)
;
LOG
.
info
(
+
seed
)
;
random
.
setSeed
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
CombineTextInputFormat
format
=
new
CombineTextInputFormat
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
20
)
+
1
;
LOG
.
info
(
+
numSplits
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
length
=
10000
;
final
int
numFiles
=
10
;
createFiles
(
length
,
numFiles
,
random
)
;
CombineTextInputFormat
format
=
new
CombineTextInputFormat
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
length
/
20
)
+
1
;
LOG
.
info
(
+
numSplits
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
LOG
.
info
(
+
splits
.
size
(
)
)
;
assertEquals
(
,
1
,
splits
.
size
(
)
)
;
InputSplit
split
=
splits
.
get
(
0
)
;
assertEquals
(
,
CombineFileSplit
.
class
,
split
.
getClass
(
)
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
assertEquals
(
,
CombineFileSplit
.
class
,
split
.
getClass
(
)
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
LOG
.
debug
(
+
split
)
;
TaskAttemptContext
context
=
MapReduceTestUtil
.
createDummyMapTaskAttemptContext
(
job
.
getConfiguration
(
)
)
;
RecordReader
<
LongWritable
,
Text
>
reader
=
format
.
createRecordReader
(
split
,
context
)
;
assertEquals
(
,
CombineFileRecordReader
.
class
,
reader
.
getClass
(
)
)
;
MapContext
<
LongWritable
,
Text
,
LongWritable
,
Text
>
mcontext
=
new
MapContextImpl
<
LongWritable
,
Text
,
LongWritable
,
Text
>
(
job
.
getConfiguration
(
)
,
context
.
getTaskAttemptID
(
)
,
reader
,
null
,
null
,
MapReduceTestUtil
.
createDummyReporter
(
)
,
split
)
;
reader
.
initialize
(
split
,
mcontext
)
;
try
{
int
count
=
0
;
while
(
reader
.
nextKeyValue
(
)
)
{
LongWritable
key
=
reader
.
getCurrentKey
(
)
;
assertNotNull
(
,
key
)
;
Text
value
=
reader
.
getCurrentValue
(
)
;
final
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
Path
file
=
new
Path
(
workDir
,
new
String
(
)
)
;
createFile
(
file
,
null
,
10
,
10
)
;
Job
job
=
Job
.
getInstance
(
defaultConf
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
FixedLengthInputFormat
format
=
new
FixedLengthInputFormat
(
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
TaskAttemptContext
context
=
MapReduceTestUtil
.
createDummyMapTaskAttemptContext
(
job
.
getConfiguration
(
)
)
;
RecordReader
<
LongWritable
,
BytesWritable
>
reader
=
format
.
createRecordReader
(
split
,
context
)
;
MapContext
<
LongWritable
,
BytesWritable
,
LongWritable
,
BytesWritable
>
mcontext
=
new
MapContextImpl
<
LongWritable
,
BytesWritable
,
LongWritable
,
BytesWritable
>
(
job
.
getConfiguration
(
)
,
context
.
getTaskAttemptID
(
)
,
reader
,
null
,
null
,
MapReduceTestUtil
.
createDummyReporter
(
)
,
split
)
;
reader
.
initialize
(
split
,
mcontext
)
;
}
catch
(
IOException
ioe
)
{
exceptionThrown
=
true
;
createFile
(
file
,
null
,
10
,
10
)
;
Job
job
=
Job
.
getInstance
(
defaultConf
)
;
FixedLengthInputFormat
format
=
new
FixedLengthInputFormat
(
)
;
format
.
setRecordLength
(
job
.
getConfiguration
(
)
,
0
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
TaskAttemptContext
context
=
MapReduceTestUtil
.
createDummyMapTaskAttemptContext
(
job
.
getConfiguration
(
)
)
;
RecordReader
<
LongWritable
,
BytesWritable
>
reader
=
format
.
createRecordReader
(
split
,
context
)
;
MapContext
<
LongWritable
,
BytesWritable
,
LongWritable
,
BytesWritable
>
mcontext
=
new
MapContextImpl
<
LongWritable
,
BytesWritable
,
LongWritable
,
BytesWritable
>
(
job
.
getConfiguration
(
)
,
context
.
getTaskAttemptID
(
)
,
reader
,
null
,
null
,
MapReduceTestUtil
.
createDummyReporter
(
)
,
split
)
;
reader
.
initialize
(
split
,
mcontext
)
;
}
catch
(
IOException
ioe
)
{
exceptionThrown
=
true
;
createFile
(
file
,
null
,
10
,
10
)
;
Job
job
=
Job
.
getInstance
(
defaultConf
)
;
FixedLengthInputFormat
format
=
new
FixedLengthInputFormat
(
)
;
format
.
setRecordLength
(
job
.
getConfiguration
(
)
,
-
10
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
TaskAttemptContext
context
=
MapReduceTestUtil
.
createDummyMapTaskAttemptContext
(
job
.
getConfiguration
(
)
)
;
RecordReader
<
LongWritable
,
BytesWritable
>
reader
=
format
.
createRecordReader
(
split
,
context
)
;
MapContext
<
LongWritable
,
BytesWritable
,
LongWritable
,
BytesWritable
>
mcontext
=
new
MapContextImpl
<
LongWritable
,
BytesWritable
,
LongWritable
,
BytesWritable
>
(
job
.
getConfiguration
(
)
,
context
.
getTaskAttemptID
(
)
,
reader
,
null
,
null
,
MapReduceTestUtil
.
createDummyReporter
(
)
,
split
)
;
reader
.
initialize
(
split
,
mcontext
)
;
}
catch
(
IOException
ioe
)
{
exceptionThrown
=
true
;
Path
file
=
new
Path
(
workDir
,
fileName
.
toString
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
LOG
.
info
(
+
seed
)
;
Random
random
=
new
Random
(
seed
)
;
int
MAX_TESTS
=
20
;
LongWritable
key
;
BytesWritable
value
;
for
(
int
i
=
0
;
i
<
MAX_TESTS
;
i
++
)
{
LOG
.
info
(
)
;
int
totalRecords
=
random
.
nextInt
(
999
)
+
1
;
if
(
i
==
8
)
{
totalRecords
=
0
;
}
int
recordLength
=
random
.
nextInt
(
1024
*
100
)
+
1
;
if
(
i
==
10
)
{
recordLength
=
1
;
int
fileSize
=
(
totalRecords
*
recordLength
)
;
LOG
.
info
(
+
totalRecords
+
+
recordLength
)
;
Job
job
=
Job
.
getInstance
(
defaultConf
)
;
if
(
codec
!=
null
)
{
ReflectionUtils
.
setConf
(
codec
,
job
.
getConfiguration
(
)
)
;
}
ArrayList
<
String
>
recordList
=
createFile
(
file
,
codec
,
recordLength
,
totalRecords
)
;
assertTrue
(
localFs
.
exists
(
file
)
)
;
FixedLengthInputFormat
.
setRecordLength
(
job
.
getConfiguration
(
)
,
recordLength
)
;
int
numSplits
=
1
;
if
(
i
>
0
)
{
if
(
i
==
(
MAX_TESTS
-
1
)
)
{
numSplits
=
(
int
)
(
fileSize
/
Math
.
floor
(
recordLength
/
2
)
)
;
}
else
{
if
(
MAX_TESTS
%
i
==
0
)
{
numSplits
=
fileSize
/
(
fileSize
-
random
.
nextInt
(
fileSize
)
)
;
fileName
.
append
(
)
;
ReflectionUtils
.
setConf
(
codec
,
job
.
getConfiguration
(
)
)
;
}
writeFile
(
localFs
,
new
Path
(
workDir
,
fileName
.
toString
(
)
)
,
codec
,
)
;
FixedLengthInputFormat
format
=
new
FixedLengthInputFormat
(
)
;
format
.
setRecordLength
(
job
.
getConfiguration
(
)
,
5
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
if
(
codec
!=
null
)
{
assertEquals
(
,
1
,
splits
.
size
(
)
)
;
}
boolean
exceptionThrown
=
false
;
for
(
InputSplit
split
:
splits
)
{
try
{
List
<
String
>
results
=
readSplit
(
format
,
split
,
job
)
;
}
catch
(
IOException
ioe
)
{
exceptionThrown
=
true
;
@
Test
public
void
testFormat
(
)
throws
Exception
{
Job
job
=
Job
.
getInstance
(
new
Configuration
(
defaultConf
)
)
;
Path
file
=
new
Path
(
workDir
,
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
@
Test
public
void
testFormat
(
)
throws
Exception
{
Job
job
=
Job
.
getInstance
(
new
Configuration
(
defaultConf
)
)
;
Path
file
=
new
Path
(
workDir
,
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
LOG
.
info
(
+
seed
)
;
Random
random
=
new
Random
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
MAX_LENGTH
=
10000
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
final
int
MAX_LENGTH
=
10000
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
LOG
.
debug
(
+
length
)
;
Writer
writer
=
new
OutputStreamWriter
(
localFs
.
create
(
file
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
*
2
)
)
;
writer
.
write
(
)
;
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
KeyValueTextInputFormat
format
=
new
KeyValueTextInputFormat
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
LOG
.
debug
(
+
length
)
;
Writer
writer
=
new
OutputStreamWriter
(
localFs
.
create
(
file
)
)
;
try
{
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
*
2
)
)
;
writer
.
write
(
)
;
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
KeyValueTextInputFormat
format
=
new
KeyValueTextInputFormat
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
LOG
.
debug
(
+
numSplits
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
for
(
int
i
=
0
;
i
<
length
;
i
++
)
{
writer
.
write
(
Integer
.
toString
(
i
*
2
)
)
;
writer
.
write
(
)
;
writer
.
write
(
Integer
.
toString
(
i
)
)
;
writer
.
write
(
)
;
}
}
finally
{
writer
.
close
(
)
;
}
KeyValueTextInputFormat
format
=
new
KeyValueTextInputFormat
(
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
int
numSplits
=
random
.
nextInt
(
MAX_LENGTH
/
20
)
+
1
;
LOG
.
debug
(
+
numSplits
)
;
List
<
InputSplit
>
splits
=
format
.
getSplits
(
job
)
;
LOG
.
debug
(
+
splits
.
size
(
)
)
;
BitSet
bits
=
new
BitSet
(
length
)
;
for
(
int
j
=
0
;
j
<
splits
.
size
(
)
;
j
++
)
{
try
{
int
count
=
0
;
while
(
reader
.
nextKeyValue
(
)
)
{
key
=
reader
.
getCurrentKey
(
)
;
clazz
=
key
.
getClass
(
)
;
assertEquals
(
,
Text
.
class
,
clazz
)
;
value
=
reader
.
getCurrentValue
(
)
;
clazz
=
value
.
getClass
(
)
;
assertEquals
(
,
Text
.
class
,
clazz
)
;
final
int
k
=
Integer
.
parseInt
(
key
.
toString
(
)
)
;
final
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
assertEquals
(
,
0
,
k
%
2
)
;
assertEquals
(
,
k
/
2
,
v
)
;
LOG
.
debug
(
+
v
)
;
assertFalse
(
,
bits
.
get
(
v
)
)
;
final
Configuration
conf
=
job
.
getConfiguration
(
)
;
CompressionCodec
codec
=
null
;
try
{
codec
=
(
CompressionCodec
)
ReflectionUtils
.
newInstance
(
conf
.
getClassByName
(
)
,
conf
)
;
}
catch
(
ClassNotFoundException
cnfe
)
{
throw
new
IOException
(
)
;
}
Path
file
=
new
Path
(
workDir
,
+
codec
.
getDefaultExtension
(
)
)
;
int
seed
=
new
Random
(
)
.
nextInt
(
)
;
LOG
.
info
(
+
seed
)
;
Random
random
=
new
Random
(
seed
)
;
localFs
.
delete
(
workDir
,
true
)
;
FileInputFormat
.
setInputPaths
(
job
,
workDir
)
;
final
int
MAX_LENGTH
=
500000
;
FileInputFormat
.
setMaxInputSplitSize
(
job
,
MAX_LENGTH
/
20
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
4
)
+
1
)
{
RecordReader
<
Text
,
Text
>
reader
=
format
.
createRecordReader
(
splits
.
get
(
j
)
,
context
)
;
Class
<
?
>
clazz
=
reader
.
getClass
(
)
;
MapContext
<
Text
,
Text
,
Text
,
Text
>
mcontext
=
new
MapContextImpl
<
Text
,
Text
,
Text
,
Text
>
(
job
.
getConfiguration
(
)
,
context
.
getTaskAttemptID
(
)
,
reader
,
null
,
null
,
MapReduceTestUtil
.
createDummyReporter
(
)
,
splits
.
get
(
j
)
)
;
reader
.
initialize
(
splits
.
get
(
j
)
,
mcontext
)
;
Text
key
=
null
;
Text
value
=
null
;
try
{
int
count
=
0
;
while
(
reader
.
nextKeyValue
(
)
)
{
key
=
reader
.
getCurrentKey
(
)
;
value
=
reader
.
getCurrentValue
(
)
;
final
int
k
=
Integer
.
parseInt
(
key
.
toString
(
)
)
;
final
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
assertEquals
(
,
0
,
k
%
2
)
;
assertEquals
(
,
k
/
2
,
v
)
;
MapContext
<
Text
,
Text
,
Text
,
Text
>
mcontext
=
new
MapContextImpl
<
Text
,
Text
,
Text
,
Text
>
(
job
.
getConfiguration
(
)
,
context
.
getTaskAttemptID
(
)
,
reader
,
null
,
null
,
MapReduceTestUtil
.
createDummyReporter
(
)
,
splits
.
get
(
j
)
)
;
reader
.
initialize
(
splits
.
get
(
j
)
,
mcontext
)
;
Text
key
=
null
;
Text
value
=
null
;
try
{
int
count
=
0
;
while
(
reader
.
nextKeyValue
(
)
)
{
key
=
reader
.
getCurrentKey
(
)
;
value
=
reader
.
getCurrentValue
(
)
;
final
int
k
=
Integer
.
parseInt
(
key
.
toString
(
)
)
;
final
int
v
=
Integer
.
parseInt
(
value
.
toString
(
)
)
;
assertEquals
(
,
0
,
k
%
2
)
;
assertEquals
(
,
k
/
2
,
v
)
;
LOG
.
debug
(
+
k
+
+
v
)
;
assertFalse
(
k
+
+
v
+
,
bits
.
get
(
v
)
)
;
@
Test
public
void
testRegexFilter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
RegexFilter
.
class
)
;
SequenceFileInputFilter
.
RegexFilter
.
setPattern
(
job
.
getConfiguration
(
)
,
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
1
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
@
Test
public
void
testPercentFilter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
PercentFilter
.
class
)
;
SequenceFileInputFilter
.
PercentFilter
.
setFrequency
(
job
.
getConfiguration
(
)
,
1000
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
@
Test
public
void
testPercentFilter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
PercentFilter
.
class
)
;
SequenceFileInputFilter
.
PercentFilter
.
setFrequency
(
job
.
getConfiguration
(
)
,
1000
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
LOG
.
info
(
+
length
)
;
createSequenceFile
(
length
)
;
int
count
=
countRecords
(
1
)
;
@
Test
public
void
testMD5Filter
(
)
throws
Exception
{
LOG
.
info
(
)
;
SequenceFileInputFilter
.
setFilterClass
(
job
,
SequenceFileInputFilter
.
MD5Filter
.
class
)
;
SequenceFileInputFilter
.
MD5Filter
.
setFrequency
(
job
.
getConfiguration
(
)
,
1000
)
;
fs
.
delete
(
inDir
,
true
)
;
for
(
int
length
=
0
;
length
<
MAX_LENGTH
;
length
+=
random
.
nextInt
(
MAX_LENGTH
/
10
)
+
1
)
{
private
void
testKeySpecs
(
String
input
,
String
expectedOutput
,
KeyFieldHelper
helper
,
int
s1
,
int
e1
)
{
private
void
testKeySpecs
(
String
input
,
String
expectedOutput
,
KeyFieldHelper
helper
,
int
s1
,
int
e1
)
{
LOG
.
info
(
+
input
)
;
String
keySpecs
=
helper
.
keySpecs
(
)
.
get
(
0
)
.
toString
(
)
;
private
void
testKeySpecs
(
String
input
,
String
expectedOutput
,
KeyFieldHelper
helper
,
int
s1
,
int
e1
)
{
LOG
.
info
(
+
input
)
;
String
keySpecs
=
helper
.
keySpecs
(
)
.
get
(
0
)
.
toString
(
)
;
LOG
.
info
(
+
keySpecs
)
;
byte
[
]
inputBytes
=
input
.
getBytes
(
)
;
if
(
e1
==
-
1
)
{
e1
=
inputBytes
.
length
;
}
LOG
.
info
(
+
e1
)
;
int
[
]
indices
=
helper
.
getWordLengths
(
inputBytes
,
s1
,
e1
)
;
int
start
=
helper
.
getStartOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
start
)
;
if
(
expectedOutput
==
null
)
{
assertEquals
(
,
-
1
,
start
)
;
return
;
}
int
end
=
helper
.
getEndOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
keySpecs
)
;
byte
[
]
inputBytes
=
input
.
getBytes
(
)
;
if
(
e1
==
-
1
)
{
e1
=
inputBytes
.
length
;
}
LOG
.
info
(
+
e1
)
;
int
[
]
indices
=
helper
.
getWordLengths
(
inputBytes
,
s1
,
e1
)
;
int
start
=
helper
.
getStartOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
start
)
;
if
(
expectedOutput
==
null
)
{
assertEquals
(
,
-
1
,
start
)
;
return
;
}
int
end
=
helper
.
getEndOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
end
)
;
end
=
(
end
>=
inputBytes
.
length
)
?
inputBytes
.
length
-
1
:
end
;
int
length
=
end
+
1
-
start
;
}
LOG
.
info
(
+
e1
)
;
int
[
]
indices
=
helper
.
getWordLengths
(
inputBytes
,
s1
,
e1
)
;
int
start
=
helper
.
getStartOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
start
)
;
if
(
expectedOutput
==
null
)
{
assertEquals
(
,
-
1
,
start
)
;
return
;
}
int
end
=
helper
.
getEndOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
end
)
;
end
=
(
end
>=
inputBytes
.
length
)
?
inputBytes
.
length
-
1
:
end
;
int
length
=
end
+
1
-
start
;
LOG
.
info
(
+
length
)
;
byte
[
]
outputBytes
=
new
byte
[
length
]
;
System
.
arraycopy
(
inputBytes
,
start
,
outputBytes
,
0
,
length
)
;
String
output
=
new
String
(
outputBytes
)
;
LOG
.
info
(
+
e1
)
;
int
[
]
indices
=
helper
.
getWordLengths
(
inputBytes
,
s1
,
e1
)
;
int
start
=
helper
.
getStartOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
start
)
;
if
(
expectedOutput
==
null
)
{
assertEquals
(
,
-
1
,
start
)
;
return
;
}
int
end
=
helper
.
getEndOffset
(
inputBytes
,
s1
,
e1
,
indices
,
helper
.
keySpecs
(
)
.
get
(
0
)
)
;
LOG
.
info
(
+
end
)
;
end
=
(
end
>=
inputBytes
.
length
)
?
inputBytes
.
length
-
1
:
end
;
int
length
=
end
+
1
-
start
;
LOG
.
info
(
+
length
)
;
byte
[
]
outputBytes
=
new
byte
[
length
]
;
System
.
arraycopy
(
inputBytes
,
start
,
outputBytes
,
0
,
length
)
;
String
output
=
new
String
(
outputBytes
)
;
long
tokenFetchTime
;
try
{
jobHistoryServer
=
new
JobHistoryServer
(
)
{
protected
void
doSecureLogin
(
Configuration
conf
)
throws
IOException
{
}
@
Override
protected
JHSDelegationTokenSecretManager
createJHSSecretManager
(
Configuration
conf
,
HistoryServerStateStoreService
store
)
{
return
new
JHSDelegationTokenSecretManager
(
initialInterval
,
maxLifetime
,
renewInterval
,
3600000
,
store
)
;
}
}
;
jobHistoryServer
.
init
(
conf
)
;
jobHistoryServer
.
start
(
)
;
final
MRClientProtocol
hsService
=
jobHistoryServer
.
getClientService
(
)
.
getClientHandler
(
)
;
UserGroupInformation
loggedInUser
=
UserGroupInformation
.
createRemoteUser
(
)
;
Assert
.
assertEquals
(
,
loggedInUser
.
getShortUserName
(
)
)
;
loggedInUser
.
setAuthenticationMethod
(
AuthenticationMethod
.
KERBEROS
)
;
Token
token
=
getDelegationToken
(
loggedInUser
,
hsService
,
loggedInUser
.
getShortUserName
(
)
)
;
tokenFetchTime
=
System
.
currentTimeMillis
(
)
;
clientUsingDT
.
getJobReport
(
jobReportRequest
)
;
}
catch
(
IOException
e
)
{
Assert
.
assertEquals
(
,
e
.
getMessage
(
)
)
;
}
while
(
System
.
currentTimeMillis
(
)
<
tokenFetchTime
+
initialInterval
/
2
)
{
Thread
.
sleep
(
500l
)
;
}
long
nextExpTime
=
renewDelegationToken
(
loggedInUser
,
hsService
,
token
)
;
long
renewalTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
renewalTime
+
+
nextExpTime
)
;
while
(
System
.
currentTimeMillis
(
)
>
tokenFetchTime
+
initialInterval
&&
System
.
currentTimeMillis
(
)
<
nextExpTime
)
{
Thread
.
sleep
(
500l
)
;
}
Thread
.
sleep
(
50l
)
;
try
{
clientUsingDT
.
getJobReport
(
jobReportRequest
)
;
}
catch
(
IOException
e
)
{
Assert
.
assertEquals
(
,
e
.
getMessage
(
)
)
;
LOG
.
info
(
+
renewalTime
+
+
nextExpTime
)
;
while
(
System
.
currentTimeMillis
(
)
>
tokenFetchTime
+
initialInterval
&&
System
.
currentTimeMillis
(
)
<
nextExpTime
)
{
Thread
.
sleep
(
500l
)
;
}
Thread
.
sleep
(
50l
)
;
try
{
clientUsingDT
.
getJobReport
(
jobReportRequest
)
;
}
catch
(
IOException
e
)
{
Assert
.
assertEquals
(
,
e
.
getMessage
(
)
)
;
}
while
(
System
.
currentTimeMillis
(
)
<
renewalTime
+
renewInterval
)
{
Thread
.
sleep
(
500l
)
;
}
Thread
.
sleep
(
50l
)
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
+
)
;
try
{
clientUsingDT
.
getJobReport
(
jobReportRequest
)
;
fail
(
)
;
Thread
.
sleep
(
500l
)
;
}
Thread
.
sleep
(
50l
)
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
+
)
;
try
{
clientUsingDT
.
getJobReport
(
jobReportRequest
)
;
fail
(
)
;
}
catch
(
IOException
e
)
{
assertTrue
(
e
.
getCause
(
)
.
getMessage
(
)
.
contains
(
)
)
;
}
if
(
clientUsingDT
!=
null
)
{
clientUsingDT
=
null
;
}
token
=
getDelegationToken
(
loggedInUser
,
hsService
,
loggedInUser
.
getShortUserName
(
)
)
;
tokenFetchTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
tokenFetchTime
)
;
clientUsingDT
=
getMRClientProtocol
(
token
,
jobHistoryServer
.
getClientService
(
)
.
getBindAddress
(
)
,
,
conf
)
;
try
{
@
Test
public
void
testJobTokenRpc
(
)
throws
Exception
{
TaskUmbilicalProtocol
mockTT
=
mock
(
TaskUmbilicalProtocol
.
class
)
;
doReturn
(
TaskUmbilicalProtocol
.
versionID
)
.
when
(
mockTT
)
.
getProtocolVersion
(
anyString
(
)
,
anyLong
(
)
)
;
doReturn
(
ProtocolSignature
.
getProtocolSignature
(
mockTT
,
TaskUmbilicalProtocol
.
class
.
getName
(
)
,
TaskUmbilicalProtocol
.
versionID
,
0
)
)
.
when
(
mockTT
)
.
getProtocolSignature
(
anyString
(
)
,
anyLong
(
)
,
anyInt
(
)
)
;
JobTokenSecretManager
sm
=
new
JobTokenSecretManager
(
)
;
final
Server
server
=
new
RPC
.
Builder
(
conf
)
.
setProtocol
(
TaskUmbilicalProtocol
.
class
)
.
setInstance
(
mockTT
)
.
setBindAddress
(
ADDRESS
)
.
setPort
(
0
)
.
setNumHandlers
(
5
)
.
setVerbose
(
true
)
.
setSecretManager
(
sm
)
.
build
(
)
;
server
.
start
(
)
;
final
UserGroupInformation
current
=
UserGroupInformation
.
getCurrentUser
(
)
;
final
InetSocketAddress
addr
=
NetUtils
.
getConnectAddress
(
server
)
;
String
jobId
=
current
.
getUserName
(
)
;
JobTokenIdentifier
tokenId
=
new
JobTokenIdentifier
(
new
Text
(
jobId
)
)
;
Token
<
JobTokenIdentifier
>
token
=
new
Token
<
JobTokenIdentifier
>
(
tokenId
,
sm
)
;
sm
.
addTokenForJob
(
jobId
,
token
)
;
SecurityUtil
.
setTokenService
(
token
,
addr
)
;
private
String
relativeToWorking
(
String
pathname
)
{
String
cwd
=
System
.
getProperty
(
,
)
;
pathname
=
(
new
Path
(
pathname
)
)
.
toUri
(
)
.
getPath
(
)
;
cwd
=
(
new
Path
(
cwd
)
)
.
toUri
(
)
.
getPath
(
)
;
String
[
]
cwdParts
=
cwd
.
split
(
Path
.
SEPARATOR
)
;
String
[
]
pathParts
=
pathname
.
split
(
Path
.
SEPARATOR
)
;
if
(
cwd
.
equals
(
pathname
)
)
{
if
(
cwdParts
[
i
]
.
equals
(
pathParts
[
i
]
)
)
{
common
++
;
}
else
{
break
;
}
}
StringBuilder
sb
=
new
StringBuilder
(
)
;
int
parentDirsRequired
=
cwdParts
.
length
-
common
;
for
(
int
i
=
0
;
i
<
parentDirsRequired
;
i
++
)
{
sb
.
append
(
)
;
sb
.
append
(
Path
.
SEPARATOR
)
;
}
for
(
int
i
=
common
;
i
<
pathParts
.
length
;
i
++
)
{
sb
.
append
(
pathParts
[
i
]
)
;
sb
.
append
(
Path
.
SEPARATOR
)
;
}
String
s
=
sb
.
toString
(
)
;
if
(
s
.
endsWith
(
Path
.
SEPARATOR
)
)
{
s
=
s
.
substring
(
0
,
s
.
length
(
)
-
1
)
;
}
conf
.
set
(
MRConfig
.
FRAMEWORK_NAME
,
MRConfig
.
YARN_FRAMEWORK_NAME
)
;
String
stagingDir
=
conf
.
get
(
MRJobConfig
.
MR_AM_STAGING_DIR
)
;
if
(
stagingDir
==
null
||
stagingDir
.
equals
(
MRJobConfig
.
DEFAULT_MR_AM_STAGING_DIR
)
)
{
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
new
File
(
getTestWorkDir
(
)
,
)
.
getAbsolutePath
(
)
)
;
}
if
(
!
conf
.
getBoolean
(
MRConfig
.
MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING
,
MRConfig
.
DEFAULT_MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING
)
)
{
conf
.
setBoolean
(
YarnConfiguration
.
NM_PMEM_CHECK_ENABLED
,
false
)
;
conf
.
setBoolean
(
YarnConfiguration
.
NM_VMEM_CHECK_ENABLED
,
false
)
;
}
conf
.
set
(
CommonConfigurationKeys
.
FS_PERMISSIONS_UMASK_KEY
,
)
;
try
{
Path
stagingPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
conf
.
get
(
MRJobConfig
.
MR_AM_STAGING_DIR
)
)
)
;
if
(
Path
.
WINDOWS
)
{
if
(
LocalFileSystem
.
class
.
isInstance
(
stagingPath
.
getFileSystem
(
conf
)
)
)
{
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
new
File
(
conf
.
get
(
MRJobConfig
.
MR_AM_STAGING_DIR
)
)
.
getAbsolutePath
(
)
)
;
}
}
FileContext
fc
=
FileContext
.
getFileContext
(
stagingPath
.
toUri
(
)
,
conf
)
;
if
(
fc
.
util
(
)
.
exists
(
stagingPath
)
)
{
if
(
stagingDir
==
null
||
stagingDir
.
equals
(
MRJobConfig
.
DEFAULT_MR_AM_STAGING_DIR
)
)
{
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
new
File
(
getTestWorkDir
(
)
,
)
.
getAbsolutePath
(
)
)
;
}
if
(
!
conf
.
getBoolean
(
MRConfig
.
MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING
,
MRConfig
.
DEFAULT_MAPREDUCE_MINICLUSTER_CONTROL_RESOURCE_MONITORING
)
)
{
conf
.
setBoolean
(
YarnConfiguration
.
NM_PMEM_CHECK_ENABLED
,
false
)
;
conf
.
setBoolean
(
YarnConfiguration
.
NM_VMEM_CHECK_ENABLED
,
false
)
;
}
conf
.
set
(
CommonConfigurationKeys
.
FS_PERMISSIONS_UMASK_KEY
,
)
;
try
{
Path
stagingPath
=
FileContext
.
getFileContext
(
conf
)
.
makeQualified
(
new
Path
(
conf
.
get
(
MRJobConfig
.
MR_AM_STAGING_DIR
)
)
)
;
if
(
Path
.
WINDOWS
)
{
if
(
LocalFileSystem
.
class
.
isInstance
(
stagingPath
.
getFileSystem
(
conf
)
)
)
{
conf
.
set
(
MRJobConfig
.
MR_AM_STAGING_DIR
,
new
File
(
conf
.
get
(
MRJobConfig
.
MR_AM_STAGING_DIR
)
)
.
getAbsolutePath
(
)
)
;
}
}
FileContext
fc
=
FileContext
.
getFileContext
(
stagingPath
.
toUri
(
)
,
conf
)
;
if
(
fc
.
util
(
)
.
exists
(
stagingPath
)
)
{
LOG
.
info
(
stagingPath
+
)
;
fc
.
delete
(
stagingPath
,
true
)
;
@
Before
public
void
setup
(
)
throws
Exception
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
public
void
testJobWithNonNormalizedCapabilities
(
)
throws
Exception
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
After
public
void
tearDown
(
)
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
BeforeClass
public
static
void
setup
(
)
throws
IOException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
private
void
testSleepJobInternal
(
Configuration
sleepConf
,
boolean
useRemoteJar
,
boolean
jobSubmissionShouldSucceed
,
ResourceViolation
violation
)
throws
Exception
{
@
Test
(
timeout
=
3000000
)
public
void
testJobWithChangePriority
(
)
throws
Exception
{
Configuration
sleepConf
=
new
Configuration
(
mrCluster
.
getConfig
(
)
)
;
Assume
.
assumeFalse
(
sleepConf
.
get
(
YarnConfiguration
.
RM_SCHEDULER
)
.
equals
(
FairScheduler
.
class
.
getCanonicalName
(
)
)
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
(
timeout
=
300000
)
public
void
testJobWithWorkflowPriority
(
)
throws
Exception
{
Configuration
sleepConf
=
new
Configuration
(
mrCluster
.
getConfig
(
)
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
private
void
testJobClassloader
(
boolean
useCustomClasses
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
private
void
testJobClassloader
(
boolean
useCustomClasses
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
LOG
.
info
(
+
+
useCustomClasses
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
(
timeout
=
60000
)
public
void
testRandomWriter
(
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
LOG
.
info
(
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
(
timeout
=
60000
)
public
void
testFailingMapper
(
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
LOG
.
info
(
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
(
timeout
=
120000
)
public
void
testContainerRollingLog
(
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
private
void
testDistributedCache
(
String
jobJarPath
,
boolean
withWildcard
)
throws
Exception
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
(
timeout
=
120000
)
public
void
testThreadDumpOnTaskTimeout
(
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
public
void
testSharedCache
(
)
throws
Exception
{
Path
localJobJarPath
=
makeJobJarWithLib
(
TEST_ROOT_DIR
.
toUri
(
)
.
toString
(
)
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Before
public
void
setup
(
)
throws
InterruptedException
,
IOException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
After
public
void
tearDown
(
)
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
(
timeout
=
90000
)
public
void
testJobHistoryData
(
)
throws
IOException
,
InterruptedException
,
AvroRemoteException
,
ClassNotFoundException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
job
.
waitForCompletion
(
true
)
;
Counters
counterMR
=
job
.
getCounters
(
)
;
JobId
jobId
=
TypeConverter
.
toYarn
(
job
.
getJobID
(
)
)
;
ApplicationId
appID
=
jobId
.
getAppId
(
)
;
int
pollElapsed
=
0
;
while
(
true
)
{
Thread
.
sleep
(
1000
)
;
pollElapsed
+=
1000
;
if
(
TERMINAL_RM_APP_STATES
.
contains
(
mrCluster
.
getResourceManager
(
)
.
getRMContext
(
)
.
getRMApps
(
)
.
get
(
appID
)
.
getState
(
)
)
)
{
break
;
}
if
(
pollElapsed
>=
60000
)
{
LOG
.
warn
(
)
;
break
;
}
}
Assert
.
assertEquals
(
RMAppState
.
FINISHED
,
mrCluster
.
getResourceManager
(
)
.
getRMContext
(
)
.
getRMApps
(
)
.
get
(
appID
)
.
getState
(
)
)
;
Counters
counterHS
=
job
.
getCounters
(
)
;
@
BeforeClass
public
static
void
setup
(
)
throws
InterruptedException
,
IOException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
AfterClass
public
static
void
tearDown
(
)
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
private
void
testProfilerInternal
(
boolean
useDefault
)
throws
Exception
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
BeforeClass
public
static
void
setup
(
)
throws
IOException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
public
void
testJobSucceed
(
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
LOG
.
info
(
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
public
void
testJobFail
(
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
LOG
.
info
(
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
BeforeClass
public
static
void
setup
(
)
throws
IOException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
public
void
testRMNMInfo
(
)
throws
Exception
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Before
public
void
setup
(
)
throws
IOException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
public
void
testExecNonSpeculative
(
)
throws
Exception
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
BeforeClass
public
static
void
setup
(
)
throws
IOException
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Test
public
void
testSpeculativeExecution
(
)
throws
Exception
{
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
@
Override
@
Test
public
void
testFailingMapper
(
)
throws
IOException
,
InterruptedException
,
ClassNotFoundException
{
LOG
.
info
(
)
;
if
(
!
(
new
File
(
MiniMRYarnCluster
.
APPJAR
)
)
.
exists
(
)
)
{
public
static
INativeHandler
create
(
String
nativeHandlerName
,
Configuration
conf
,
DataChannel
channel
)
throws
IOException
{
final
int
bufferSize
=
conf
.
getInt
(
Constants
.
NATIVE_PROCESSOR_BUFFER_KB
,
1024
)
*
1024
;
@
SuppressWarnings
(
)
@
Override
public
void
init
(
Context
context
)
throws
IOException
,
ClassNotFoundException
{
this
.
context
=
context
;
this
.
job
=
context
.
getJobConf
(
)
;
Platforms
.
init
(
job
)
;
if
(
job
.
getNumReduceTasks
(
)
==
0
)
{
String
message
=
;
@
SuppressWarnings
(
)
@
Override
public
void
init
(
Context
context
)
throws
IOException
,
ClassNotFoundException
{
this
.
context
=
context
;
this
.
job
=
context
.
getJobConf
(
)
;
Platforms
.
init
(
job
)
;
if
(
job
.
getNumReduceTasks
(
)
==
0
)
{
String
message
=
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
Class
<
?
>
comparatorClass
=
job
.
getClass
(
MRJobConfig
.
KEY_COMPARATOR
,
null
,
RawComparator
.
class
)
;
if
(
comparatorClass
!=
null
&&
!
Platforms
.
define
(
comparatorClass
)
)
{
String
message
=
+
job
.
get
(
MRJobConfig
.
KEY_COMPARATOR
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
if
(
!
QuickSort
.
class
.
getName
(
)
.
equals
(
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
)
)
{
String
message
=
+
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
;
if
(
job
.
getNumReduceTasks
(
)
==
0
)
{
String
message
=
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
Class
<
?
>
comparatorClass
=
job
.
getClass
(
MRJobConfig
.
KEY_COMPARATOR
,
null
,
RawComparator
.
class
)
;
if
(
comparatorClass
!=
null
&&
!
Platforms
.
define
(
comparatorClass
)
)
{
String
message
=
+
job
.
get
(
MRJobConfig
.
KEY_COMPARATOR
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
if
(
!
QuickSort
.
class
.
getName
(
)
.
equals
(
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
)
)
{
String
message
=
+
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
if
(
job
.
getBoolean
(
MRConfig
.
SHUFFLE_SSL_ENABLED_KEY
,
false
)
==
true
)
{
String
message
=
;
String
message
=
+
job
.
get
(
MRJobConfig
.
KEY_COMPARATOR
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
if
(
!
QuickSort
.
class
.
getName
(
)
.
equals
(
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
)
)
{
String
message
=
+
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
if
(
job
.
getBoolean
(
MRConfig
.
SHUFFLE_SSL_ENABLED_KEY
,
false
)
==
true
)
{
String
message
=
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
final
Class
<
?
>
keyCls
=
job
.
getMapOutputKeyClass
(
)
;
try
{
@
SuppressWarnings
(
)
final
INativeSerializer
serializer
=
NativeSerialization
.
getInstance
(
)
.
getSerializer
(
keyCls
)
;
if
(
null
==
serializer
)
{
if
(
!
QuickSort
.
class
.
getName
(
)
.
equals
(
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
)
)
{
String
message
=
+
job
.
get
(
Constants
.
MAP_SORT_CLASS
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
if
(
job
.
getBoolean
(
MRConfig
.
SHUFFLE_SSL_ENABLED_KEY
,
false
)
==
true
)
{
String
message
=
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
final
Class
<
?
>
keyCls
=
job
.
getMapOutputKeyClass
(
)
;
try
{
@
SuppressWarnings
(
)
final
INativeSerializer
serializer
=
NativeSerialization
.
getInstance
(
)
.
getSerializer
(
keyCls
)
;
if
(
null
==
serializer
)
{
String
message
=
+
keyCls
.
getName
(
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
if
(
job
.
getBoolean
(
MRConfig
.
SHUFFLE_SSL_ENABLED_KEY
,
false
)
==
true
)
{
String
message
=
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
final
Class
<
?
>
keyCls
=
job
.
getMapOutputKeyClass
(
)
;
try
{
@
SuppressWarnings
(
)
final
INativeSerializer
serializer
=
NativeSerialization
.
getInstance
(
)
.
getSerializer
(
keyCls
)
;
if
(
null
==
serializer
)
{
String
message
=
+
keyCls
.
getName
(
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
else
if
(
!
Platforms
.
support
(
keyCls
.
getName
(
)
,
serializer
,
job
)
)
{
String
message
=
+
+
keyCls
.
getName
(
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
@
SuppressWarnings
(
)
final
INativeSerializer
serializer
=
NativeSerialization
.
getInstance
(
)
.
getSerializer
(
keyCls
)
;
if
(
null
==
serializer
)
{
String
message
=
+
keyCls
.
getName
(
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
else
if
(
!
Platforms
.
support
(
keyCls
.
getName
(
)
,
serializer
,
job
)
)
{
String
message
=
+
+
keyCls
.
getName
(
)
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
}
catch
(
final
IOException
e
)
{
String
message
=
+
keyCls
.
getName
(
)
;
LOG
.
error
(
message
)
;
throw
new
IOException
(
message
)
;
}
final
boolean
ret
=
NativeRuntime
.
isNativeLibraryLoaded
(
)
;
if
(
ret
)
{
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
}
catch
(
final
IOException
e
)
{
String
message
=
+
keyCls
.
getName
(
)
;
LOG
.
error
(
message
)
;
throw
new
IOException
(
message
)
;
}
final
boolean
ret
=
NativeRuntime
.
isNativeLibraryLoaded
(
)
;
if
(
ret
)
{
if
(
job
.
getBoolean
(
MRJobConfig
.
MAP_OUTPUT_COMPRESS
,
false
)
)
{
String
codec
=
job
.
get
(
MRJobConfig
.
MAP_OUTPUT_COMPRESS_CODEC
)
;
if
(
!
NativeRuntime
.
supportsCompressionCodec
(
codec
.
getBytes
(
Charsets
.
UTF_8
)
)
)
{
String
message
=
+
codec
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
}
NativeRuntime
.
configure
(
job
)
;
if
(
job
.
getBoolean
(
MRJobConfig
.
MAP_OUTPUT_COMPRESS
,
false
)
)
{
String
codec
=
job
.
get
(
MRJobConfig
.
MAP_OUTPUT_COMPRESS_CODEC
)
;
if
(
!
NativeRuntime
.
supportsCompressionCodec
(
codec
.
getBytes
(
Charsets
.
UTF_8
)
)
)
{
String
message
=
+
codec
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
}
NativeRuntime
.
configure
(
job
)
;
final
long
updateInterval
=
job
.
getLong
(
Constants
.
NATIVE_STATUS_UPDATE_INTERVAL
,
Constants
.
NATIVE_STATUS_UPDATE_INTERVAL_DEFVAL
)
;
updater
=
new
StatusReportChecker
(
context
.
getReporter
(
)
,
updateInterval
)
;
updater
.
start
(
)
;
}
else
{
String
message
=
+
;
LOG
.
error
(
message
)
;
throw
new
InvalidJobConfException
(
message
)
;
}
this
.
handler
=
null
;
public
static
boolean
support
(
String
keyClassName
,
INativeSerializer
<
?
>
serializer
,
JobConf
job
)
{
synchronized
(
platforms
)
{
for
(
Platform
platform
:
platforms
)
{
if
(
platform
.
support
(
keyClassName
,
serializer
,
job
)
)
{
public
static
boolean
define
(
Class
<
?
>
keyComparator
)
{
synchronized
(
platforms
)
{
for
(
Platform
platform
:
platforms
)
{
if
(
platform
.
define
(
keyComparator
)
)
{
public
static
NativeTaskOutput
createNativeTaskOutput
(
Configuration
conf
,
String
id
)
{
Class
<
?
>
clazz
=
conf
.
getClass
(
OutputUtil
.
NATIVE_TASK_OUTPUT_MANAGER
,
NativeTaskOutputFiles
.
class
)
;
@
Test
public
void
testLargeValueCombiner
(
)
throws
Exception
{
final
Configuration
normalConf
=
ScenarioConfiguration
.
getNormalConfiguration
(
)
;
final
Configuration
nativeConf
=
ScenarioConfiguration
.
getNativeConfiguration
(
)
;
normalConf
.
addResource
(
TestConstants
.
COMBINER_CONF_PATH
)
;
nativeConf
.
addResource
(
TestConstants
.
COMBINER_CONF_PATH
)
;
final
int
deafult_KVSize_Maximum
=
1
<<
22
;
final
int
KVSize_Maximum
=
normalConf
.
getInt
(
TestConstants
.
NATIVETASK_KVSIZE_MAX_LARGEKV_TEST
,
deafult_KVSize_Maximum
)
;
final
String
inputPath
=
TestConstants
.
NATIVETASK_COMBINER_TEST_INPUTDIR
+
;
final
String
nativeOutputPath
=
TestConstants
.
NATIVETASK_COMBINER_TEST_NATIVE_OUTPUTDIR
+
;
final
String
hadoopOutputPath
=
TestConstants
.
NATIVETASK_COMBINER_TEST_NORMAL_OUTPUTDIR
+
;
final
FileSystem
fs
=
FileSystem
.
get
(
normalConf
)
;
for
(
int
i
=
65536
;
i
<=
KVSize_Maximum
;
i
*=
4
)
{
int
max
=
i
;
int
min
=
Math
.
max
(
i
/
4
,
max
-
10
)
;
@
Parameters
(
name
=
)
public
static
Iterable
<
Class
<
?
>
[
]
>
data
(
)
throws
Exception
{
final
String
valueClassesStr
=
nativekvtestconf
.
get
(
TestConstants
.
NATIVETASK_KVTEST_VALUECLASSES
)
;
@
Parameters
(
name
=
)
public
static
Iterable
<
Class
<
?
>
[
]
>
data
(
)
throws
Exception
{
final
String
valueClassesStr
=
nativekvtestconf
.
get
(
TestConstants
.
NATIVETASK_KVTEST_VALUECLASSES
)
;
LOG
.
info
(
+
valueClassesStr
)
;
List
<
Class
<
?
>>
valueClasses
=
parseClassNames
(
valueClassesStr
)
;
final
String
keyClassesStr
=
nativekvtestconf
.
get
(
TestConstants
.
NATIVETASK_KVTEST_KEYCLASSES
)
;
public
void
createSequenceTestFile
(
String
filepath
,
int
base
,
byte
start
)
throws
Exception
{
ServerBootstrap
bootstrap
=
new
ServerBootstrap
(
selector
)
;
timer
=
new
HashedWheelTimer
(
)
;
try
{
pipelineFact
=
new
HttpPipelineFactory
(
conf
,
timer
)
;
}
catch
(
Exception
ex
)
{
throw
new
RuntimeException
(
ex
)
;
}
bootstrap
.
setOption
(
,
conf
.
getInt
(
SHUFFLE_LISTEN_QUEUE_SIZE
,
DEFAULT_SHUFFLE_LISTEN_QUEUE_SIZE
)
)
;
bootstrap
.
setOption
(
,
true
)
;
bootstrap
.
setPipelineFactory
(
pipelineFact
)
;
port
=
conf
.
getInt
(
SHUFFLE_PORT_CONFIG_KEY
,
DEFAULT_SHUFFLE_PORT
)
;
Channel
ch
=
bootstrap
.
bind
(
new
InetSocketAddress
(
port
)
)
;
accepted
.
add
(
ch
)
;
port
=
(
(
InetSocketAddress
)
ch
.
getLocalAddress
(
)
)
.
getPort
(
)
;
conf
.
set
(
SHUFFLE_PORT_CONFIG_KEY
,
Integer
.
toString
(
port
)
)
;
pipelineFact
.
SHUFFLE
.
setPort
(
port
)
;
private
void
startStore
(
Path
recoveryRoot
)
throws
IOException
{
Options
options
=
new
Options
(
)
;
options
.
createIfMissing
(
false
)
;
Path
dbPath
=
new
Path
(
recoveryRoot
,
STATE_DB_NAME
)
;
private
void
checkVersion
(
)
throws
IOException
{
Version
loadedVersion
=
loadVersion
(
)
;
private
void
addJobToken
(
JobID
jobId
,
String
user
,
Token
<
JobTokenIdentifier
>
jobToken
)
{
userRsrc
.
put
(
jobId
.
toString
(
)
,
user
)
;
secretManager
.
addTokenForJob
(
jobId
.
toString
(
)
,
jobToken
)
;
@
Test
(
timeout
=
100000
)
public
void
testMapFileAccess
(
)
throws
IOException
{
assumeTrue
(
NativeIO
.
isAvailable
(
)
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
ShuffleHandler
.
SHUFFLE_PORT_CONFIG_KEY
,
0
)
;
conf
.
setInt
(
ShuffleHandler
.
MAX_SHUFFLE_CONNECTIONS
,
3
)
;
conf
.
set
(
CommonConfigurationKeysPublic
.
HADOOP_SECURITY_AUTHENTICATION
,
)
;
UserGroupInformation
.
setConfiguration
(
conf
)
;
conf
.
set
(
YarnConfiguration
.
NM_LOCAL_DIRS
,
ABS_LOG_DIR
.
getAbsolutePath
(
)
)
;
ApplicationId
appId
=
ApplicationId
.
newInstance
(
12345
,
1
)
;
@
VisibleForTesting
void
collectPackages
(
)
throws
UploaderException
{
parseLists
(
)
;
String
[
]
list
=
StringUtils
.
split
(
input
,
File
.
pathSeparatorChar
)
;
for
(
String
item
:
list
)
{
@
VisibleForTesting
void
collectPackages
(
)
throws
UploaderException
{
parseLists
(
)
;
String
[
]
list
=
StringUtils
.
split
(
input
,
File
.
pathSeparatorChar
)
;
for
(
String
item
:
list
)
{
LOG
.
info
(
+
item
)
;
String
expanded
=
expandEnvironmentVariables
(
item
,
System
.
getenv
(
)
)
;
parseLists
(
)
;
String
[
]
list
=
StringUtils
.
split
(
input
,
File
.
pathSeparatorChar
)
;
for
(
String
item
:
list
)
{
LOG
.
info
(
+
item
)
;
String
expanded
=
expandEnvironmentVariables
(
item
,
System
.
getenv
(
)
)
;
LOG
.
info
(
+
expanded
)
;
if
(
expanded
.
endsWith
(
)
)
{
File
path
=
new
File
(
expanded
.
substring
(
0
,
expanded
.
length
(
)
-
1
)
)
;
if
(
path
.
isDirectory
(
)
)
{
File
[
]
files
=
path
.
listFiles
(
)
;
if
(
files
!=
null
)
{
for
(
File
jar
:
files
)
{
if
(
!
jar
.
isDirectory
(
)
)
{
addJar
(
jar
)
;
}
else
{
@
VisibleForTesting
void
beginUpload
(
)
throws
IOException
,
UploaderException
{
if
(
targetStream
==
null
)
{
int
lastIndex
=
target
.
indexOf
(
'#'
)
;
targetPath
=
new
Path
(
target
.
substring
(
0
,
lastIndex
==
-
1
?
target
.
length
(
)
:
lastIndex
)
)
;
alias
=
lastIndex
!=
-
1
?
target
.
substring
(
lastIndex
+
1
)
:
targetPath
.
getName
(
)
;
@
VisibleForTesting
void
beginUpload
(
)
throws
IOException
,
UploaderException
{
if
(
targetStream
==
null
)
{
int
lastIndex
=
target
.
indexOf
(
'#'
)
;
targetPath
=
new
Path
(
target
.
substring
(
0
,
lastIndex
==
-
1
?
target
.
length
(
)
:
lastIndex
)
)
;
alias
=
lastIndex
!=
-
1
?
target
.
substring
(
lastIndex
+
1
)
:
targetPath
.
getName
(
)
;
LOG
.
info
(
+
targetPath
)
;
FileSystem
fileSystem
=
targetPath
.
getFileSystem
(
conf
)
;
targetStream
=
null
;
if
(
fileSystem
instanceof
DistributedFileSystem
)
{
@
VisibleForTesting
void
beginUpload
(
)
throws
IOException
,
UploaderException
{
if
(
targetStream
==
null
)
{
int
lastIndex
=
target
.
indexOf
(
'#'
)
;
targetPath
=
new
Path
(
target
.
substring
(
0
,
lastIndex
==
-
1
?
target
.
length
(
)
:
lastIndex
)
)
;
alias
=
lastIndex
!=
-
1
?
target
.
substring
(
lastIndex
+
1
)
:
targetPath
.
getName
(
)
;
LOG
.
info
(
+
targetPath
)
;
FileSystem
fileSystem
=
targetPath
.
getFileSystem
(
conf
)
;
targetStream
=
null
;
if
(
fileSystem
instanceof
DistributedFileSystem
)
{
LOG
.
info
(
+
initialReplication
+
+
targetPath
)
;
FileSystem
fileSystem
=
targetPath
.
getFileSystem
(
conf
)
;
targetStream
=
null
;
if
(
fileSystem
instanceof
DistributedFileSystem
)
{
LOG
.
info
(
+
initialReplication
+
+
targetPath
)
;
LOG
.
info
(
+
targetPath
)
;
DistributedFileSystem
dfs
=
(
DistributedFileSystem
)
fileSystem
;
DistributedFileSystem
.
HdfsDataOutputStreamBuilder
builder
=
dfs
.
createFile
(
targetPath
)
.
overwrite
(
true
)
.
ecPolicyName
(
SystemErasureCodingPolicies
.
getReplicationPolicy
(
)
.
getName
(
)
)
;
if
(
initialReplication
>
0
)
{
builder
.
replication
(
initialReplication
)
;
}
targetStream
=
builder
.
build
(
)
;
}
else
{
LOG
.
warn
(
+
initialReplication
+
+
targetPath
+
+
fileSystem
.
getClass
(
)
.
getName
(
)
)
;
}
if
(
targetStream
==
null
)
{
targetStream
=
fileSystem
.
create
(
targetPath
,
true
)
;
}
if
(
!
FRAMEWORK_PERMISSION
.
equals
(
FRAMEWORK_PERMISSION
.
applyUMask
(
FsPermission
.
getUMask
(
conf
)
)
)
)
{
private
void
endUpload
(
)
throws
IOException
,
InterruptedException
{
FileSystem
fileSystem
=
targetPath
.
getFileSystem
(
conf
)
;
if
(
fileSystem
instanceof
DistributedFileSystem
)
{
fileSystem
.
setReplication
(
targetPath
,
finalReplication
)
;
FileSystem
fileSystem
=
targetPath
.
getFileSystem
(
conf
)
;
if
(
fileSystem
instanceof
DistributedFileSystem
)
{
fileSystem
.
setReplication
(
targetPath
,
finalReplication
)
;
LOG
.
info
(
+
finalReplication
+
+
targetPath
)
;
if
(
timeout
==
0
)
{
LOG
.
info
(
)
;
}
else
{
long
startTime
=
System
.
currentTimeMillis
(
)
;
long
endTime
=
startTime
;
long
currentReplication
=
0
;
while
(
endTime
-
startTime
<
timeout
*
1000
&&
currentReplication
<
acceptableReplication
)
{
Thread
.
sleep
(
1000
)
;
endTime
=
System
.
currentTimeMillis
(
)
;
currentReplication
=
getSmallestReplicatedBlockCount
(
)
;
}
if
(
endTime
-
startTime
>=
timeout
*
1000
)
{
fileSystem
.
setReplication
(
targetPath
,
finalReplication
)
;
LOG
.
info
(
+
finalReplication
+
+
targetPath
)
;
if
(
timeout
==
0
)
{
LOG
.
info
(
)
;
}
else
{
long
startTime
=
System
.
currentTimeMillis
(
)
;
long
endTime
=
startTime
;
long
currentReplication
=
0
;
while
(
endTime
-
startTime
<
timeout
*
1000
&&
currentReplication
<
acceptableReplication
)
{
Thread
.
sleep
(
1000
)
;
endTime
=
System
.
currentTimeMillis
(
)
;
currentReplication
=
getSmallestReplicatedBlockCount
(
)
;
}
if
(
endTime
-
startTime
>=
timeout
*
1000
)
{
LOG
.
error
(
String
.
format
(
+
,
timeout
,
acceptableReplication
,
currentReplication
)
)
;
}
}
}
else
{
@
VisibleForTesting
void
buildPackage
(
)
throws
IOException
,
UploaderException
,
InterruptedException
{
beginUpload
(
)
;
LOG
.
info
(
)
;
try
(
TarArchiveOutputStream
out
=
new
TarArchiveOutputStream
(
targetStream
)
)
{
for
(
String
fullPath
:
filteredInputFiles
)
{
private
void
parseLists
(
)
throws
UploaderException
{
Map
<
String
,
String
>
env
=
System
.
getenv
(
)
;
for
(
Map
.
Entry
<
String
,
String
>
item
:
env
.
entrySet
(
)
)
{
private
void
addJar
(
File
jar
)
throws
UploaderException
{
boolean
found
=
false
;
if
(
!
jar
.
getName
(
)
.
endsWith
(
)
)
{
for
(
Pattern
pattern
:
whitelistedFiles
)
{
Matcher
matcher
=
pattern
.
matcher
(
jar
.
getAbsolutePath
(
)
)
;
if
(
matcher
.
matches
(
)
)
{
LOG
.
info
(
+
jar
.
getAbsolutePath
(
)
)
;
found
=
true
;
break
;
}
}
boolean
excluded
=
false
;
for
(
Pattern
pattern
:
blacklistedFiles
)
{
Matcher
matcher
=
pattern
.
matcher
(
jar
.
getAbsolutePath
(
)
)
;
if
(
matcher
.
matches
(
)
)
{
LOG
.
info
(
+
jar
.
getAbsolutePath
(
)
)
;
excluded
=
true
;
break
;
}
}
if
(
ignoreSymlink
&&
!
excluded
)
{
excluded
=
checkSymlink
(
jar
)
;
found
=
true
;
break
;
}
}
boolean
excluded
=
false
;
for
(
Pattern
pattern
:
blacklistedFiles
)
{
Matcher
matcher
=
pattern
.
matcher
(
jar
.
getAbsolutePath
(
)
)
;
if
(
matcher
.
matches
(
)
)
{
LOG
.
info
(
+
jar
.
getAbsolutePath
(
)
)
;
excluded
=
true
;
break
;
}
}
if
(
ignoreSymlink
&&
!
excluded
)
{
excluded
=
checkSymlink
(
jar
)
;
}
if
(
found
&&
!
excluded
)
{
LOG
.
info
(
+
jar
.
getAbsolutePath
(
)
)
;
if
(
!
filteredInputFiles
.
add
(
jar
.
getAbsolutePath
(
)
)
)
{
throw
new
UploaderException
(
+
jar
.
getAbsolutePath
(
)
)
;
}
}
boolean
excluded
=
false
;
for
(
Pattern
pattern
:
blacklistedFiles
)
{
Matcher
matcher
=
pattern
.
matcher
(
jar
.
getAbsolutePath
(
)
)
;
if
(
matcher
.
matches
(
)
)
{
LOG
.
info
(
+
jar
.
getAbsolutePath
(
)
)
;
excluded
=
true
;
break
;
}
}
if
(
ignoreSymlink
&&
!
excluded
)
{
excluded
=
checkSymlink
(
jar
)
;
}
if
(
found
&&
!
excluded
)
{
LOG
.
info
(
+
jar
.
getAbsolutePath
(
)
)
;
if
(
!
filteredInputFiles
.
add
(
jar
.
getAbsolutePath
(
)
)
)
{
throw
new
UploaderException
(
+
jar
.
getAbsolutePath
(
)
)
;
}
}
if
(
!
found
)
{
LOG
.
info
(
+
jar
.
getAbsolutePath
(
)
+
+
)
;
@
VisibleForTesting
boolean
checkSymlink
(
File
jar
)
{
if
(
Files
.
isSymbolicLink
(
jar
.
toPath
(
)
)
)
{
try
{
java
.
nio
.
file
.
Path
link
=
Files
.
readSymbolicLink
(
jar
.
toPath
(
)
)
;
java
.
nio
.
file
.
Path
jarPath
=
Paths
.
get
(
jar
.
getAbsolutePath
(
)
)
;
String
linkString
=
link
.
toString
(
)
;
java
.
nio
.
file
.
Path
jarParent
=
jarPath
.
getParent
(
)
;
java
.
nio
.
file
.
Path
linkPath
=
jarParent
==
null
?
null
:
jarParent
.
resolve
(
linkString
)
;
java
.
nio
.
file
.
Path
linkPathParent
=
linkPath
==
null
?
null
:
linkPath
.
getParent
(
)
;
java
.
nio
.
file
.
Path
normalizedLinkPath
=
linkPathParent
==
null
?
null
:
linkPathParent
.
normalize
(
)
;
if
(
normalizedLinkPath
!=
null
&&
jarParent
.
normalize
(
)
.
equals
(
normalizedLinkPath
)
)
{
initialReplication
=
Short
.
parseShort
(
parser
.
getCommandLine
(
)
.
getOptionValue
(
,
)
)
;
finalReplication
=
Short
.
parseShort
(
parser
.
getCommandLine
(
)
.
getOptionValue
(
,
)
)
;
acceptableReplication
=
Short
.
parseShort
(
parser
.
getCommandLine
(
)
.
getOptionValue
(
,
)
)
;
timeout
=
Integer
.
parseInt
(
parser
.
getCommandLine
(
)
.
getOptionValue
(
,
)
)
;
if
(
parser
.
getCommandLine
(
)
.
hasOption
(
)
)
{
ignoreSymlink
=
true
;
}
String
fs
=
parser
.
getCommandLine
(
)
.
getOptionValue
(
,
null
)
;
String
path
=
parser
.
getCommandLine
(
)
.
getOptionValue
(
,
)
;
boolean
isFullPath
=
path
.
startsWith
(
)
||
path
.
startsWith
(
)
;
if
(
fs
==
null
)
{
fs
=
conf
.
getTrimmed
(
FS_DEFAULT_NAME_KEY
)
;
if
(
fs
==
null
&&
!
isFullPath
)
{
LOG
.
error
(
)
;
printHelp
(
opts
)
;
return
false
;
private
boolean
verify
(
)
throws
SQLException
{
String
countAccessQuery
=
;
String
sumPageviewQuery
=
;
Statement
st
=
null
;
ResultSet
rs
=
null
;
try
{
st
=
connection
.
createStatement
(
)
;
rs
=
st
.
executeQuery
(
countAccessQuery
)
;
rs
.
next
(
)
;
long
totalPageview
=
rs
.
getLong
(
1
)
;
rs
=
st
.
executeQuery
(
sumPageviewQuery
)
;
rs
.
next
(
)
;
long
sumPageview
=
rs
.
getLong
(
1
)
;
private
boolean
verify
(
)
throws
SQLException
{
String
countAccessQuery
=
;
String
sumPageviewQuery
=
;
Statement
st
=
null
;
ResultSet
rs
=
null
;
try
{
st
=
connection
.
createStatement
(
)
;
rs
=
st
.
executeQuery
(
countAccessQuery
)
;
rs
.
next
(
)
;
long
totalPageview
=
rs
.
getLong
(
1
)
;
rs
=
st
.
executeQuery
(
sumPageviewQuery
)
;
rs
.
next
(
)
;
long
sumPageview
=
rs
.
getLong
(
1
)
;
LOG
.
info
(
+
totalPageview
)
;
void
pickBestSplits
(
Host
host
)
{
int
tasksToPick
=
Math
.
min
(
slotsPerHost
,
(
int
)
Math
.
ceil
(
(
double
)
remainingSplits
/
hosts
.
size
(
)
)
)
;
Split
[
]
best
=
new
Split
[
tasksToPick
]
;
for
(
Split
cur
:
host
.
splits
)
{
private
boolean
rejectRootDirectoryDelete
(
boolean
isEmptyDir
,
boolean
recursive
)
throws
IOException
{
private
void
createFakeDirectoryIfNecessary
(
Path
f
)
throws
IOException
{
String
key
=
pathToKey
(
f
)
;
if
(
StringUtils
.
isNotEmpty
(
key
)
&&
!
exists
(
f
)
)
{
@
Override
public
FileStatus
[
]
listStatus
(
Path
path
)
throws
IOException
{
String
key
=
pathToKey
(
path
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
FileStatus
[
]
listStatus
(
Path
path
)
throws
IOException
{
String
key
=
pathToKey
(
path
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
path
)
;
}
final
List
<
FileStatus
>
result
=
new
ArrayList
<
FileStatus
>
(
)
;
final
FileStatus
fileStatus
=
getFileStatus
(
path
)
;
if
(
fileStatus
.
isDirectory
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
key
)
;
}
ObjectListing
objects
=
store
.
listObjects
(
key
,
maxKeys
,
null
,
false
)
;
while
(
true
)
{
for
(
OSSObjectSummary
objectSummary
:
objects
.
getObjectSummaries
(
)
)
{
String
objKey
=
objectSummary
.
getKey
(
)
;
if
(
objKey
.
equals
(
key
+
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
final
List
<
FileStatus
>
result
=
new
ArrayList
<
FileStatus
>
(
)
;
final
FileStatus
fileStatus
=
getFileStatus
(
path
)
;
if
(
fileStatus
.
isDirectory
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
key
)
;
}
ObjectListing
objects
=
store
.
listObjects
(
key
,
maxKeys
,
null
,
false
)
;
while
(
true
)
{
for
(
OSSObjectSummary
objectSummary
:
objects
.
getObjectSummaries
(
)
)
{
String
objKey
=
objectSummary
.
getKey
(
)
;
if
(
objKey
.
equals
(
key
+
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
objKey
)
;
}
continue
;
}
else
{
Path
keyPath
=
keyToPath
(
objectSummary
.
getKey
(
)
)
.
makeQualified
(
uri
,
workingDir
)
;
}
ObjectListing
objects
=
store
.
listObjects
(
key
,
maxKeys
,
null
,
false
)
;
while
(
true
)
{
for
(
OSSObjectSummary
objectSummary
:
objects
.
getObjectSummaries
(
)
)
{
String
objKey
=
objectSummary
.
getKey
(
)
;
if
(
objKey
.
equals
(
key
+
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
objKey
)
;
}
continue
;
}
else
{
Path
keyPath
=
keyToPath
(
objectSummary
.
getKey
(
)
)
.
makeQualified
(
uri
,
workingDir
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
keyPath
)
;
}
result
.
add
(
new
OSSFileStatus
(
objectSummary
.
getSize
(
)
,
false
,
1
,
getDefaultBlockSize
(
keyPath
)
,
objectSummary
.
getLastModified
(
)
.
getTime
(
)
,
keyPath
,
username
)
)
;
}
}
for
(
String
prefix
:
objects
.
getCommonPrefixes
(
)
)
{
if
(
prefix
.
equals
(
key
+
)
)
{
if
(
objKey
.
equals
(
key
+
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
objKey
)
;
}
continue
;
}
else
{
Path
keyPath
=
keyToPath
(
objectSummary
.
getKey
(
)
)
.
makeQualified
(
uri
,
workingDir
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
keyPath
)
;
}
result
.
add
(
new
OSSFileStatus
(
objectSummary
.
getSize
(
)
,
false
,
1
,
getDefaultBlockSize
(
keyPath
)
,
objectSummary
.
getLastModified
(
)
.
getTime
(
)
,
keyPath
,
username
)
)
;
}
}
for
(
String
prefix
:
objects
.
getCommonPrefixes
(
)
)
{
if
(
prefix
.
equals
(
key
+
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
prefix
)
;
}
continue
;
}
else
{
private
RemoteIterator
<
LocatedFileStatus
>
innerList
(
final
Path
f
,
final
FileStatus
status
,
final
PathFilter
filter
,
final
FileStatusAcceptor
acceptor
,
final
boolean
recursive
)
throws
IOException
{
Path
qualifiedPath
=
f
.
makeQualified
(
uri
,
workingDir
)
;
String
key
=
pathToKey
(
qualifiedPath
)
;
if
(
status
.
isFile
(
)
)
{
if
(
StringUtils
.
isNotEmpty
(
proxyHost
)
)
{
clientConf
.
setProxyHost
(
proxyHost
)
;
if
(
proxyPort
>=
0
)
{
clientConf
.
setProxyPort
(
proxyPort
)
;
}
else
{
if
(
secureConnections
)
{
LOG
.
warn
(
)
;
clientConf
.
setProxyPort
(
443
)
;
}
else
{
LOG
.
warn
(
)
;
clientConf
.
setProxyPort
(
80
)
;
}
}
String
proxyUsername
=
conf
.
getTrimmed
(
PROXY_USERNAME_KEY
)
;
String
proxyPassword
=
conf
.
getTrimmed
(
PROXY_PASSWORD_KEY
)
;
if
(
(
proxyUsername
==
null
)
!=
(
proxyPassword
==
null
)
)
{
String
msg
=
+
PROXY_USERNAME_KEY
+
+
PROXY_PASSWORD_KEY
+
;
clientConf
.
setProxyPort
(
443
)
;
}
else
{
LOG
.
warn
(
)
;
clientConf
.
setProxyPort
(
80
)
;
}
}
String
proxyUsername
=
conf
.
getTrimmed
(
PROXY_USERNAME_KEY
)
;
String
proxyPassword
=
conf
.
getTrimmed
(
PROXY_PASSWORD_KEY
)
;
if
(
(
proxyUsername
==
null
)
!=
(
proxyPassword
==
null
)
)
{
String
msg
=
+
PROXY_USERNAME_KEY
+
+
PROXY_PASSWORD_KEY
+
;
LOG
.
error
(
msg
)
;
throw
new
IllegalArgumentException
(
msg
)
;
}
clientConf
.
setProxyUsername
(
proxyUsername
)
;
clientConf
.
setProxyPassword
(
proxyPassword
)
;
clientConf
.
setProxyDomain
(
conf
.
getTrimmed
(
PROXY_DOMAIN_KEY
)
)
;
clientConf
.
setProxyWorkstation
(
conf
.
getTrimmed
(
PROXY_WORKSTATION_KEY
)
)
;
}
else
if
(
proxyPort
>=
0
)
{
private
boolean
singleCopy
(
String
srcKey
,
String
dstKey
)
{
CopyObjectResult
copyResult
=
ossClient
.
copyObject
(
bucketName
,
srcKey
,
bucketName
,
dstKey
)
;
statistics
.
incrementWriteOps
(
1
)
;
int
tries
=
3
;
while
(
tries
>
0
)
{
try
{
instream
=
new
FileInputStream
(
file
)
;
UploadPartRequest
uploadRequest
=
new
UploadPartRequest
(
)
;
uploadRequest
.
setBucketName
(
bucketName
)
;
uploadRequest
.
setKey
(
key
)
;
uploadRequest
.
setUploadId
(
uploadId
)
;
uploadRequest
.
setInputStream
(
instream
)
;
uploadRequest
.
setPartSize
(
file
.
length
(
)
)
;
uploadRequest
.
setPartNumber
(
idx
)
;
UploadPartResult
uploadResult
=
ossClient
.
uploadPart
(
uploadRequest
)
;
statistics
.
incrementWriteOps
(
1
)
;
return
uploadResult
.
getPartETag
(
)
;
}
catch
(
Exception
e
)
{
static
int
intOption
(
Configuration
conf
,
String
key
,
int
defVal
,
int
min
)
{
int
v
=
conf
.
getInt
(
key
,
defVal
)
;
Preconditions
.
checkArgument
(
v
>=
min
,
String
.
format
(
,
key
,
v
,
min
)
)
;
static
long
longOption
(
Configuration
conf
,
String
key
,
long
defVal
,
long
min
)
{
long
v
=
conf
.
getLong
(
key
,
defVal
)
;
Preconditions
.
checkArgument
(
v
>=
min
,
String
.
format
(
,
key
,
v
,
min
)
)
;
@
Test
public
void
testSeekFile
(
)
throws
Exception
{
Path
smallSeekFile
=
setPath
(
)
;
long
size
=
5
*
1024
*
1024
;
ContractTestUtils
.
generateTestFile
(
this
.
fs
,
smallSeekFile
,
size
,
256
,
255
)
;
LOG
.
info
(
)
;
FSDataInputStream
instream
=
this
.
fs
.
open
(
smallSeekFile
)
;
int
seekTimes
=
5
;
LOG
.
info
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
size
/
(
seekTimes
-
i
)
-
1
;
@
Test
public
void
testSeekFile
(
)
throws
Exception
{
Path
smallSeekFile
=
setPath
(
)
;
long
size
=
5
*
1024
*
1024
;
ContractTestUtils
.
generateTestFile
(
this
.
fs
,
smallSeekFile
,
size
,
256
,
255
)
;
LOG
.
info
(
)
;
FSDataInputStream
instream
=
this
.
fs
.
open
(
smallSeekFile
)
;
int
seekTimes
=
5
;
LOG
.
info
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
size
/
(
seekTimes
-
i
)
-
1
;
LOG
.
info
(
+
pos
)
;
instream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
instream
.
getPos
(
)
,
instream
.
getPos
(
)
==
pos
)
;
ContractTestUtils
.
generateTestFile
(
this
.
fs
,
smallSeekFile
,
size
,
256
,
255
)
;
LOG
.
info
(
)
;
FSDataInputStream
instream
=
this
.
fs
.
open
(
smallSeekFile
)
;
int
seekTimes
=
5
;
LOG
.
info
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
size
/
(
seekTimes
-
i
)
-
1
;
LOG
.
info
(
+
pos
)
;
instream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
instream
.
getPos
(
)
,
instream
.
getPos
(
)
==
pos
)
;
LOG
.
info
(
+
instream
.
getPos
(
)
)
;
}
LOG
.
info
(
)
;
Random
rand
=
new
Random
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
Math
.
abs
(
rand
.
nextLong
(
)
)
%
size
;
int
seekTimes
=
5
;
LOG
.
info
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
size
/
(
seekTimes
-
i
)
-
1
;
LOG
.
info
(
+
pos
)
;
instream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
instream
.
getPos
(
)
,
instream
.
getPos
(
)
==
pos
)
;
LOG
.
info
(
+
instream
.
getPos
(
)
)
;
}
LOG
.
info
(
)
;
Random
rand
=
new
Random
(
)
;
for
(
int
i
=
0
;
i
<
seekTimes
;
i
++
)
{
long
pos
=
Math
.
abs
(
rand
.
nextLong
(
)
)
%
size
;
LOG
.
info
(
+
pos
)
;
instream
.
seek
(
pos
)
;
assertTrue
(
+
pos
+
+
instream
.
getPos
(
)
,
instream
.
getPos
(
)
==
pos
)
;
@
Test
public
void
testReadFile
(
)
throws
Exception
{
final
int
bufLen
=
256
;
final
int
sizeFlag
=
5
;
String
filename
=
+
sizeFlag
+
;
Path
readTestFile
=
setPath
(
+
filename
)
;
long
size
=
sizeFlag
*
1024
*
1024
;
ContractTestUtils
.
generateTestFile
(
this
.
fs
,
readTestFile
,
size
,
256
,
255
)
;
@
Override
public
void
testListEmptyRootDirectory
(
)
throws
IOException
{
for
(
int
attempt
=
1
,
maxAttempts
=
10
;
attempt
<=
maxAttempts
;
++
attempt
)
{
try
{
super
.
testListEmptyRootDirectory
(
)
;
break
;
}
catch
(
AssertionError
|
FileNotFoundException
e
)
{
if
(
attempt
<
maxAttempts
)
{
LOG
.
info
(
+
,
attempt
,
maxAttempts
)
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e2
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
fail
(
)
;
break
;
}
}
else
{
List
<
LogAggregationFileController
>
fileControllers
=
factory
.
getConfiguredLogAggregationFileControllerList
(
)
;
if
(
fileControllers
==
null
||
fileControllers
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
if
(
verbose
)
{
LOG
.
info
(
+
YarnConfiguration
.
LOG_AGGREGATION_FILE_FORMATS
)
;
}
return
0
;
}
try
{
fs
=
FileSystem
.
get
(
conf
)
;
int
previousTotal
=
0
;
for
(
LogAggregationFileController
fileController
:
fileControllers
)
{
Path
remoteRootLogDir
=
fileController
.
getRemoteRootLogDir
(
)
;
String
suffix
=
fileController
.
getRemoteRootLogDirSuffix
(
)
;
Path
workingDir
=
new
Path
(
remoteRootLogDir
,
)
;
if
(
verbose
)
{
LOG
.
info
(
+
fileController
.
getClass
(
)
.
getName
(
)
)
;
if
(
fileControllers
==
null
||
fileControllers
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
if
(
verbose
)
{
LOG
.
info
(
+
YarnConfiguration
.
LOG_AGGREGATION_FILE_FORMATS
)
;
}
return
0
;
}
try
{
fs
=
FileSystem
.
get
(
conf
)
;
int
previousTotal
=
0
;
for
(
LogAggregationFileController
fileController
:
fileControllers
)
{
Path
remoteRootLogDir
=
fileController
.
getRemoteRootLogDir
(
)
;
String
suffix
=
fileController
.
getRemoteRootLogDirSuffix
(
)
;
Path
workingDir
=
new
Path
(
remoteRootLogDir
,
)
;
if
(
verbose
)
{
LOG
.
info
(
+
fileController
.
getClass
(
)
.
getName
(
)
)
;
LOG
.
info
(
+
remoteRootLogDir
)
;
LOG
.
info
(
)
;
if
(
verbose
)
{
LOG
.
info
(
+
YarnConfiguration
.
LOG_AGGREGATION_FILE_FORMATS
)
;
}
return
0
;
}
try
{
fs
=
FileSystem
.
get
(
conf
)
;
int
previousTotal
=
0
;
for
(
LogAggregationFileController
fileController
:
fileControllers
)
{
Path
remoteRootLogDir
=
fileController
.
getRemoteRootLogDir
(
)
;
String
suffix
=
fileController
.
getRemoteRootLogDirSuffix
(
)
;
Path
workingDir
=
new
Path
(
remoteRootLogDir
,
)
;
if
(
verbose
)
{
LOG
.
info
(
+
fileController
.
getClass
(
)
.
getName
(
)
)
;
LOG
.
info
(
+
remoteRootLogDir
)
;
LOG
.
info
(
+
suffix
)
;
Path
workingDir
=
new
Path
(
remoteRootLogDir
,
)
;
if
(
verbose
)
{
LOG
.
info
(
+
fileController
.
getClass
(
)
.
getName
(
)
)
;
LOG
.
info
(
+
remoteRootLogDir
)
;
LOG
.
info
(
+
suffix
)
;
LOG
.
info
(
+
workingDir
)
;
}
checkFilesAndSeedApps
(
fs
,
remoteRootLogDir
,
suffix
,
workingDir
)
;
filterAppsByAggregatedStatus
(
)
;
if
(
eligibleApplications
.
size
(
)
>
previousTotal
)
{
workingDirs
.
add
(
workingDir
)
;
previousTotal
=
eligibleApplications
.
size
(
)
;
}
}
checkMaxEligible
(
)
;
if
(
workingDirs
.
isEmpty
(
)
||
eligibleApplications
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
return
0
;
}
checkFilesAndSeedApps
(
fs
,
remoteRootLogDir
,
suffix
,
workingDir
)
;
filterAppsByAggregatedStatus
(
)
;
if
(
eligibleApplications
.
size
(
)
>
previousTotal
)
{
workingDirs
.
add
(
workingDir
)
;
previousTotal
=
eligibleApplications
.
size
(
)
;
}
}
checkMaxEligible
(
)
;
if
(
workingDirs
.
isEmpty
(
)
||
eligibleApplications
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
return
0
;
}
for
(
Path
workingDir
:
workingDirs
)
{
if
(
!
prepareWorkingDir
(
fs
,
workingDir
)
)
{
LOG
.
error
(
+
workingDir
.
toString
(
)
)
;
return
1
;
}
}
StringBuilder
sb
=
new
StringBuilder
(
)
;
for
(
AppInfo
app
:
eligibleApplications
)
{
opts
.
addOption
(
maxTotalLogsSizeOpt
)
;
opts
.
addOption
(
memoryOpt
)
;
opts
.
addOption
(
verboseOpt
)
;
opts
.
addOption
(
forceOpt
)
;
opts
.
addOption
(
noProxyOpt
)
;
try
{
CommandLineParser
parser
=
new
GnuParser
(
)
;
CommandLine
commandLine
=
parser
.
parse
(
opts
,
args
)
;
if
(
commandLine
.
hasOption
(
HELP_OPTION
)
)
{
HelpFormatter
formatter
=
new
HelpFormatter
(
)
;
formatter
.
printHelp
(
,
opts
)
;
System
.
exit
(
0
)
;
}
if
(
commandLine
.
hasOption
(
MAX_ELIGIBLE_APPS_OPTION
)
)
{
maxEligible
=
Integer
.
parseInt
(
commandLine
.
getOptionValue
(
MAX_ELIGIBLE_APPS_OPTION
)
)
;
if
(
maxEligible
==
0
)
{
@
VisibleForTesting
void
filterAppsByAggregatedStatus
(
)
throws
IOException
,
YarnException
{
YarnClient
client
=
YarnClient
.
createYarnClient
(
)
;
try
{
client
.
init
(
getConf
(
)
)
;
client
.
start
(
)
;
for
(
Iterator
<
AppInfo
>
it
=
eligibleApplications
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
AppInfo
app
=
it
.
next
(
)
;
try
{
ApplicationReport
report
=
client
.
getApplicationReport
(
ApplicationId
.
fromString
(
app
.
getAppId
(
)
)
)
;
LogAggregationStatus
aggStatus
=
report
.
getLogAggregationStatus
(
)
;
if
(
aggStatus
.
equals
(
LogAggregationStatus
.
RUNNING
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
RUNNING_WITH_FAILURE
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
NOT_START
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
DISABLED
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
FAILED
)
)
{
if
(
verbose
)
{
YarnClient
client
=
YarnClient
.
createYarnClient
(
)
;
try
{
client
.
init
(
getConf
(
)
)
;
client
.
start
(
)
;
for
(
Iterator
<
AppInfo
>
it
=
eligibleApplications
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
AppInfo
app
=
it
.
next
(
)
;
try
{
ApplicationReport
report
=
client
.
getApplicationReport
(
ApplicationId
.
fromString
(
app
.
getAppId
(
)
)
)
;
LogAggregationStatus
aggStatus
=
report
.
getLogAggregationStatus
(
)
;
if
(
aggStatus
.
equals
(
LogAggregationStatus
.
RUNNING
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
RUNNING_WITH_FAILURE
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
NOT_START
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
DISABLED
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
FAILED
)
)
{
if
(
verbose
)
{
LOG
.
info
(
+
app
.
getAppId
(
)
+
+
aggStatus
)
;
}
it
.
remove
(
)
;
}
else
{
if
(
verbose
)
{
for
(
Iterator
<
AppInfo
>
it
=
eligibleApplications
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
AppInfo
app
=
it
.
next
(
)
;
try
{
ApplicationReport
report
=
client
.
getApplicationReport
(
ApplicationId
.
fromString
(
app
.
getAppId
(
)
)
)
;
LogAggregationStatus
aggStatus
=
report
.
getLogAggregationStatus
(
)
;
if
(
aggStatus
.
equals
(
LogAggregationStatus
.
RUNNING
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
RUNNING_WITH_FAILURE
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
NOT_START
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
DISABLED
)
||
aggStatus
.
equals
(
LogAggregationStatus
.
FAILED
)
)
{
if
(
verbose
)
{
LOG
.
info
(
+
app
.
getAppId
(
)
+
+
aggStatus
)
;
}
it
.
remove
(
)
;
}
else
{
if
(
verbose
)
{
LOG
.
info
(
app
.
getAppId
(
)
+
+
aggStatus
)
;
}
app
.
setFinishTime
(
report
.
getFinishTime
(
)
)
;
}
}
catch
(
ApplicationNotFoundException
e
)
{
if
(
verbose
)
{
@
VisibleForTesting
void
checkFilesAndSeedApps
(
FileSystem
fs
,
Path
remoteRootLogDir
,
String
suffix
,
Path
workingDir
)
throws
IOException
{
for
(
RemoteIterator
<
FileStatus
>
userIt
=
fs
.
listStatusIterator
(
remoteRootLogDir
)
;
userIt
.
hasNext
(
)
;
)
{
Path
userLogPath
=
userIt
.
next
(
)
.
getPath
(
)
;
try
{
for
(
RemoteIterator
<
FileStatus
>
appIt
=
fs
.
listStatusIterator
(
new
Path
(
userLogPath
,
suffix
)
)
;
appIt
.
hasNext
(
)
;
)
{
Path
appLogPath
=
appIt
.
next
(
)
.
getPath
(
)
;
try
{
FileStatus
[
]
files
=
fs
.
listStatus
(
appLogPath
)
;
if
(
files
.
length
>=
minNumLogFiles
)
{
boolean
eligible
=
true
;
long
totalFileSize
=
0L
;
for
(
FileStatus
file
:
files
)
{
if
(
file
.
getPath
(
)
.
getName
(
)
.
equals
(
appLogPath
.
getName
(
)
+
)
)
{
eligible
=
false
;
if
(
verbose
)
{
try
{
FileStatus
[
]
files
=
fs
.
listStatus
(
appLogPath
)
;
if
(
files
.
length
>=
minNumLogFiles
)
{
boolean
eligible
=
true
;
long
totalFileSize
=
0L
;
for
(
FileStatus
file
:
files
)
{
if
(
file
.
getPath
(
)
.
getName
(
)
.
equals
(
appLogPath
.
getName
(
)
+
)
)
{
eligible
=
false
;
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
)
;
}
break
;
}
totalFileSize
+=
file
.
getLen
(
)
;
if
(
totalFileSize
>
maxTotalLogsSize
)
{
eligible
=
false
;
if
(
verbose
)
{
long
totalFileSize
=
0L
;
for
(
FileStatus
file
:
files
)
{
if
(
file
.
getPath
(
)
.
getName
(
)
.
equals
(
appLogPath
.
getName
(
)
+
)
)
{
eligible
=
false
;
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
)
;
}
break
;
}
totalFileSize
+=
file
.
getLen
(
)
;
if
(
totalFileSize
>
maxTotalLogsSize
)
{
eligible
=
false
;
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
+
totalFileSize
+
+
maxTotalLogsSize
+
)
;
}
break
;
}
}
if
(
eligible
)
{
if
(
verbose
)
{
if
(
totalFileSize
>
maxTotalLogsSize
)
{
eligible
=
false
;
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
+
totalFileSize
+
+
maxTotalLogsSize
+
)
;
}
break
;
}
}
if
(
eligible
)
{
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
userLogPath
.
getName
(
)
)
;
}
AppInfo
context
=
new
AppInfo
(
)
;
context
.
setAppId
(
appLogPath
.
getName
(
)
)
;
context
.
setUser
(
userLogPath
.
getName
(
)
)
;
context
.
setSuffix
(
suffix
)
;
context
.
setRemoteRootLogDir
(
remoteRootLogDir
)
;
context
.
setWorkingDir
(
workingDir
)
;
eligibleApplications
.
add
(
context
)
;
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
+
totalFileSize
+
+
maxTotalLogsSize
+
)
;
}
break
;
}
}
if
(
eligible
)
{
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
userLogPath
.
getName
(
)
)
;
}
AppInfo
context
=
new
AppInfo
(
)
;
context
.
setAppId
(
appLogPath
.
getName
(
)
)
;
context
.
setUser
(
userLogPath
.
getName
(
)
)
;
context
.
setSuffix
(
suffix
)
;
context
.
setRemoteRootLogDir
(
remoteRootLogDir
)
;
context
.
setWorkingDir
(
workingDir
)
;
eligibleApplications
.
add
(
context
)
;
}
}
else
{
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
+
files
.
length
+
+
minNumLogFiles
+
)
;
}
}
if
(
eligible
)
{
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
userLogPath
.
getName
(
)
)
;
}
AppInfo
context
=
new
AppInfo
(
)
;
context
.
setAppId
(
appLogPath
.
getName
(
)
)
;
context
.
setUser
(
userLogPath
.
getName
(
)
)
;
context
.
setSuffix
(
suffix
)
;
context
.
setRemoteRootLogDir
(
remoteRootLogDir
)
;
context
.
setWorkingDir
(
workingDir
)
;
eligibleApplications
.
add
(
context
)
;
}
}
else
{
if
(
verbose
)
{
LOG
.
info
(
+
appLogPath
.
getName
(
)
+
+
+
files
.
length
+
+
minNumLogFiles
+
)
;
}
}
}
catch
(
IOException
ioe
)
{
if
(
verbose
)
{
@
VisibleForTesting
void
checkMaxEligible
(
)
{
if
(
maxEligible
>
0
&&
eligibleApplications
.
size
(
)
>
maxEligible
)
{
if
(
verbose
)
{
@
VisibleForTesting
void
generateScript
(
File
localScript
)
throws
IOException
{
if
(
verbose
)
{
private
boolean
runDistributedShell
(
File
localScript
)
throws
Exception
{
String
[
]
dsArgs
=
{
,
,
,
ApplicationMaster
.
class
.
getProtectionDomain
(
)
.
getCodeSource
(
)
.
getLocation
(
)
.
getPath
(
)
,
,
Integer
.
toString
(
eligibleApplications
.
size
(
)
)
,
,
Long
.
toString
(
memory
)
,
,
localScript
.
getAbsolutePath
(
)
}
;
if
(
verbose
)
{
@
Override
public
int
run
(
String
[
]
args
)
throws
Exception
{
handleOpts
(
args
)
;
Integer
exitCode
=
1
;
UserGroupInformation
loginUser
=
UserGroupInformation
.
getLoginUser
(
)
;
if
(
!
proxy
||
loginUser
.
getShortUserName
(
)
.
equals
(
user
)
)
{
checkNotEmpty
(
)
;
if
(
reuseLastProvider
&&
lastProvider
!=
null
)
{
return
lastProvider
.
getCredentials
(
)
;
}
AmazonClientException
lastException
=
null
;
for
(
AWSCredentialsProvider
provider
:
providers
)
{
try
{
AWSCredentials
credentials
=
provider
.
getCredentials
(
)
;
Preconditions
.
checkNotNull
(
credentials
,
,
provider
)
;
if
(
(
credentials
.
getAWSAccessKeyId
(
)
!=
null
&&
credentials
.
getAWSSecretKey
(
)
!=
null
)
||
(
credentials
instanceof
AnonymousAWSCredentials
)
)
{
lastProvider
=
provider
;
LOG
.
debug
(
,
provider
)
;
return
credentials
;
}
}
catch
(
NoAwsCredentialsException
e
)
{
if
(
lastException
==
null
)
{
lastException
=
e
;
return
lastProvider
.
getCredentials
(
)
;
}
AmazonClientException
lastException
=
null
;
for
(
AWSCredentialsProvider
provider
:
providers
)
{
try
{
AWSCredentials
credentials
=
provider
.
getCredentials
(
)
;
Preconditions
.
checkNotNull
(
credentials
,
,
provider
)
;
if
(
(
credentials
.
getAWSAccessKeyId
(
)
!=
null
&&
credentials
.
getAWSSecretKey
(
)
!=
null
)
||
(
credentials
instanceof
AnonymousAWSCredentials
)
)
{
lastProvider
=
provider
;
LOG
.
debug
(
,
provider
)
;
return
credentials
;
}
}
catch
(
NoAwsCredentialsException
e
)
{
if
(
lastException
==
null
)
{
lastException
=
e
;
}
LOG
.
debug
(
,
provider
,
e
.
toString
(
)
)
;
}
catch
(
AmazonClientException
e
)
{
public
boolean
shouldDelay
(
String
key
)
{
float
p
=
getDelayKeyProbability
(
)
;
boolean
delay
=
key
.
contains
(
getDelayKeySubstring
(
)
)
;
delay
=
delay
&&
trueWithProbability
(
p
)
;
@
Override
public
void
deleteObject
(
DeleteObjectRequest
deleteObjectRequest
)
throws
AmazonClientException
,
AmazonServiceException
{
String
key
=
deleteObjectRequest
.
getKey
(
)
;
@
Override
public
PutObjectResult
putObject
(
PutObjectRequest
putObjectRequest
)
throws
AmazonClientException
,
AmazonServiceException
{
private
ObjectListing
innerlistObjects
(
ListObjectsRequest
listObjectsRequest
)
throws
AmazonClientException
,
AmazonServiceException
{
private
ListObjectsV2Result
innerListObjectsV2
(
ListObjectsV2Request
request
)
{
private
void
addSummaryIfNotPresent
(
List
<
S3ObjectSummary
>
list
,
S3ObjectSummary
item
)
{
String
key
=
item
.
getKey
(
)
;
if
(
list
.
stream
(
)
.
noneMatch
(
(
member
)
->
member
.
getKey
(
)
.
equals
(
key
)
)
)
{
private
void
addPrefixIfNotPresent
(
List
<
String
>
prefixes
,
String
ancestor
,
String
child
)
{
Path
prefixCandidate
=
new
Path
(
child
)
.
getParent
(
)
;
Path
ancestorPath
=
new
Path
(
ancestor
)
;
Preconditions
.
checkArgument
(
child
.
startsWith
(
ancestor
)
,
+
,
child
,
ancestor
)
;
while
(
!
prefixCandidate
.
isRoot
(
)
)
{
Path
nextParent
=
prefixCandidate
.
getParent
(
)
;
if
(
nextParent
.
equals
(
ancestorPath
)
)
{
String
prefix
=
prefixCandidate
.
toString
(
)
;
if
(
!
prefixes
.
contains
(
prefix
)
)
{
private
boolean
isKeyDelayed
(
Long
enqueueTime
,
String
key
)
{
if
(
enqueueTime
==
null
)
{
private
void
enqueueDelayedPut
(
String
key
)
{
@
Override
public
S3Object
getObject
(
GetObjectRequest
var1
)
throws
SdkClientException
,
AmazonServiceException
{
maybeFail
(
,
404
)
;
S3Object
o
=
super
.
getObject
(
var1
)
;
@
Override
public
S3Object
getObject
(
String
bucketName
,
String
key
)
throws
SdkClientException
,
AmazonServiceException
{
S3Object
o
=
super
.
getObject
(
bucketName
,
key
)
;
@
Retries
.
RetryRaw
public
<
T
>
T
retryUntranslated
(
String
text
,
boolean
idempotent
,
Retried
retrying
,
Operation
<
T
>
operation
)
throws
IOException
{
Preconditions
.
checkArgument
(
retrying
!=
null
,
)
;
int
retryCount
=
0
;
Exception
caught
;
RetryPolicy
.
RetryAction
retryAction
;
boolean
shouldRetry
;
do
{
try
{
if
(
retryCount
>
0
)
{
public
RemoteIterator
<
S3ALocatedFileStatus
>
getListFilesAssumingDir
(
Path
path
,
boolean
recursive
,
Listing
.
FileStatusAcceptor
acceptor
,
boolean
collectTombstones
,
boolean
forceNonAuthoritativeMS
)
throws
IOException
{
String
key
=
maybeAddTrailingSlash
(
pathToKey
(
path
)
)
;
String
delimiter
=
recursive
?
null
:
;
if
(
recursive
)
{
public
long
uploadCompleted
(
)
{
long
delta
=
upload
.
getProgress
(
)
.
getBytesTransferred
(
)
-
lastBytesTransferred
;
if
(
delta
>
0
)
{
private
synchronized
S3ADataBlocks
.
DataBlock
createBlockIfNeeded
(
)
throws
IOException
{
if
(
activeBlock
==
null
)
{
blockCount
++
;
if
(
blockCount
>=
Constants
.
MAX_MULTIPART_COUNT
)
{
if
(
multiPartUpload
==
null
)
{
if
(
hasBlock
)
{
bytes
=
putObject
(
)
;
bytesSubmitted
=
bytes
;
}
}
else
{
if
(
hasBlock
&&
(
block
.
hasData
(
)
||
multiPartUpload
.
getPartsSubmitted
(
)
==
0
)
)
{
uploadCurrentBlock
(
)
;
}
final
List
<
PartETag
>
partETags
=
multiPartUpload
.
waitForAllPartUploads
(
)
;
bytes
=
bytesSubmitted
;
if
(
putTracker
.
aboutToComplete
(
multiPartUpload
.
getUploadId
(
)
,
partETags
,
bytes
)
)
{
multiPartUpload
.
complete
(
partETags
)
;
}
else
{
LOG
.
info
(
,
key
)
;
}
}
if
(
!
putTracker
.
outputImmediatelyVisible
(
)
)
{
statistics
.
commitUploaded
(
bytes
)
;
if
(
hasBlock
&&
(
block
.
hasData
(
)
||
multiPartUpload
.
getPartsSubmitted
(
)
==
0
)
)
{
uploadCurrentBlock
(
)
;
}
final
List
<
PartETag
>
partETags
=
multiPartUpload
.
waitForAllPartUploads
(
)
;
bytes
=
bytesSubmitted
;
if
(
putTracker
.
aboutToComplete
(
multiPartUpload
.
getUploadId
(
)
,
partETags
,
bytes
)
)
{
multiPartUpload
.
complete
(
partETags
)
;
}
else
{
LOG
.
info
(
,
key
)
;
}
}
if
(
!
putTracker
.
outputImmediatelyVisible
(
)
)
{
statistics
.
commitUploaded
(
bytes
)
;
}
LOG
.
debug
(
,
key
,
writeOperationHelper
)
;
}
catch
(
IOException
ioe
)
{
if
(
multiPartUpload
!=
null
)
{
multiPartUpload
.
abort
(
)
;
}
writeOperationHelper
.
writeFailed
(
ioe
)
;
public
void
initialize
(
URI
name
,
Configuration
originalConf
)
throws
IOException
{
bucket
=
name
.
getHost
(
)
;
try
{
initCannedAcls
(
conf
)
;
doBucketProbing
(
)
;
inputPolicy
=
S3AInputPolicy
.
getPolicy
(
conf
.
getTrimmed
(
INPUT_FADVISE
,
INPUT_FADV_NORMAL
)
)
;
LOG
.
debug
(
,
inputPolicy
)
;
changeDetectionPolicy
=
ChangeDetectionPolicy
.
getPolicy
(
conf
)
;
LOG
.
debug
(
,
changeDetectionPolicy
)
;
boolean
magicCommitterEnabled
=
conf
.
getBoolean
(
CommitConstants
.
MAGIC_COMMITTER_ENABLED
,
CommitConstants
.
DEFAULT_MAGIC_COMMITTER_ENABLED
)
;
LOG
.
debug
(
,
magicCommitterEnabled
?
:
)
;
committerIntegration
=
new
MagicCommitIntegration
(
this
,
magicCommitterEnabled
)
;
selectBinding
=
new
SelectBinding
(
writeHelper
)
;
boolean
blockUploadEnabled
=
conf
.
getBoolean
(
FAST_UPLOAD
,
true
)
;
if
(
!
blockUploadEnabled
)
{
LOG
.
warn
(
)
;
}
blockOutputBuffer
=
conf
.
getTrimmed
(
FAST_UPLOAD_BUFFER
,
DEFAULT_FAST_UPLOAD_BUFFER
)
;
partSize
=
ensureOutputParameterInRange
(
MULTIPART_SIZE
,
partSize
)
;
LOG
.
debug
(
,
magicCommitterEnabled
?
:
)
;
committerIntegration
=
new
MagicCommitIntegration
(
this
,
magicCommitterEnabled
)
;
selectBinding
=
new
SelectBinding
(
writeHelper
)
;
boolean
blockUploadEnabled
=
conf
.
getBoolean
(
FAST_UPLOAD
,
true
)
;
if
(
!
blockUploadEnabled
)
{
LOG
.
warn
(
)
;
}
blockOutputBuffer
=
conf
.
getTrimmed
(
FAST_UPLOAD_BUFFER
,
DEFAULT_FAST_UPLOAD_BUFFER
)
;
partSize
=
ensureOutputParameterInRange
(
MULTIPART_SIZE
,
partSize
)
;
blockFactory
=
S3ADataBlocks
.
createFactory
(
this
,
blockOutputBuffer
)
;
blockOutputActiveBlocks
=
intOption
(
conf
,
FAST_UPLOAD_ACTIVE_BLOCKS
,
DEFAULT_FAST_UPLOAD_ACTIVE_BLOCKS
,
1
)
;
LOG
.
debug
(
+
,
blockOutputBuffer
,
partSize
,
blockOutputActiveBlocks
)
;
long
authDirTtl
=
conf
.
getTimeDuration
(
METADATASTORE_METADATA_TTL
,
DEFAULT_METADATASTORE_METADATA_TTL
,
TimeUnit
.
MILLISECONDS
)
;
ttlTimeProvider
=
new
S3Guard
.
TtlTimeProvider
(
authDirTtl
)
;
setMetadataStore
(
S3Guard
.
getMetadataStore
(
this
,
ttlTimeProvider
)
)
;
allowAuthoritativeMetadataStore
=
conf
.
getBoolean
(
METADATASTORE_AUTHORITATIVE
,
DEFAULT_METADATASTORE_AUTHORITATIVE
)
;
@
Retries
.
RetryTranslated
public
void
abortOutstandingMultipartUploads
(
long
seconds
)
throws
IOException
{
Preconditions
.
checkArgument
(
seconds
>=
0
)
;
Date
purgeBefore
=
new
Date
(
new
Date
(
)
.
getTime
(
)
-
seconds
*
1000
)
;
protected
void
setAmazonS3Client
(
AmazonS3
client
)
{
Preconditions
.
checkNotNull
(
client
,
)
;
@
InterfaceStability
.
Unstable
public
void
setInputPolicy
(
S3AInputPolicy
inputPolicy
)
{
Objects
.
requireNonNull
(
inputPolicy
,
)
;
@
Override
public
Path
makeQualified
(
final
Path
path
)
{
Path
q
=
super
.
makeQualified
(
path
)
;
if
(
!
q
.
isRoot
(
)
)
{
String
urlString
=
q
.
toUri
(
)
.
toString
(
)
;
if
(
urlString
.
endsWith
(
Path
.
SEPARATOR
)
)
{
@
Retries
.
RetryTranslated
public
boolean
rename
(
Path
src
,
Path
dst
)
throws
IOException
{
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
false
,
,
src
,
dst
)
)
{
long
bytesCopied
=
innerRename
(
src
,
dst
)
;
}
S3AFileStatus
srcStatus
=
innerGetFileStatus
(
src
,
true
,
StatusProbeEnum
.
ALL
)
;
if
(
srcKey
.
equals
(
dstKey
)
)
{
LOG
.
debug
(
,
dst
)
;
throw
new
RenameFailedException
(
src
,
dst
,
)
.
withExitCode
(
srcStatus
.
isFile
(
)
)
;
}
S3AFileStatus
dstStatus
=
null
;
try
{
dstStatus
=
innerGetFileStatus
(
dst
,
true
,
StatusProbeEnum
.
ALL
)
;
if
(
srcStatus
.
isDirectory
(
)
)
{
if
(
dstStatus
.
isFile
(
)
)
{
throw
new
RenameFailedException
(
src
,
dst
,
)
.
withExitCode
(
srcStatus
.
isFile
(
)
)
;
}
else
if
(
dstStatus
.
isEmptyDirectory
(
)
!=
Tristate
.
TRUE
)
{
throw
new
RenameFailedException
(
src
,
dst
,
)
.
withExitCode
(
false
)
;
}
}
else
{
if
(
dstStatus
.
isFile
(
)
)
{
throw
new
RenameFailedException
(
src
,
dst
,
)
.
withExitCode
(
false
)
;
@
Retries
.
RetryMixed
private
long
innerRename
(
Path
source
,
Path
dest
)
throws
RenameFailedException
,
FileNotFoundException
,
IOException
,
AmazonClientException
{
Path
src
=
qualify
(
source
)
;
Path
dst
=
qualify
(
dest
)
;
@
Retries
.
RetryTranslated
public
Map
<
String
,
Object
>
getObjectHeaders
(
Path
path
)
throws
IOException
{
@
Retries
.
RetryRaw
protected
ObjectMetadata
getObjectMetadata
(
String
key
,
ChangeTracker
changeTracker
,
Invoker
changeInvoker
,
String
operation
)
throws
IOException
{
GetObjectMetadataRequest
request
=
new
GetObjectMetadataRequest
(
bucket
,
key
)
;
generateSSECustomerKey
(
)
.
ifPresent
(
request
::
setSSECustomerKey
)
;
ObjectMetadata
meta
=
changeInvoker
.
retryUntranslated
(
+
key
,
true
,
(
)
->
{
incrementStatistic
(
OBJECT_METADATA_REQUESTS
)
;
@
Retries
.
RetryRaw
protected
S3ListResult
listObjects
(
S3ListRequest
request
)
throws
IOException
{
incrementReadOperations
(
)
;
incrementStatistic
(
OBJECT_LIST_REQUESTS
)
;
@
Retries
.
OnceRaw
public
UploadInfo
putObject
(
PutObjectRequest
putObjectRequest
)
{
long
len
=
getPutRequestLength
(
putObjectRequest
)
;
@
VisibleForTesting
@
Retries
.
OnceRaw
(
)
PutObjectResult
putObjectDirect
(
PutObjectRequest
putObjectRequest
)
throws
AmazonClientException
,
MetadataPersistenceException
{
long
len
=
getPutRequestLength
(
putObjectRequest
)
;
public
void
incrementPutStartStatistics
(
long
bytes
)
{
public
void
incrementPutCompletedStatistics
(
boolean
success
,
long
bytes
)
{
@
Retries
.
RetryRaw
private
DeleteObjectsResult
removeKeysS3
(
List
<
DeleteObjectsRequest
.
KeyVersion
>
keysToDelete
,
boolean
deleteFakeDir
,
boolean
quiet
)
throws
MultiObjectDeleteException
,
AmazonClientException
,
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Retries
.
RetryRaw
private
DeleteObjectsResult
removeKeysS3
(
List
<
DeleteObjectsRequest
.
KeyVersion
>
keysToDelete
,
boolean
deleteFakeDir
,
boolean
quiet
)
throws
MultiObjectDeleteException
,
AmazonClientException
,
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
keysToDelete
.
size
(
)
)
;
for
(
DeleteObjectsRequest
.
KeyVersion
key
:
keysToDelete
)
{
@
Retries
.
RetryTranslated
private
void
createFakeDirectoryIfNecessary
(
Path
f
)
throws
IOException
,
AmazonClientException
{
String
key
=
pathToKey
(
f
)
;
if
(
!
key
.
isEmpty
(
)
&&
!
s3Exists
(
f
,
StatusProbeEnum
.
DIRECTORIES
)
)
{
private
S3AFileStatus
[
]
innerListStatus
(
Path
f
)
throws
FileNotFoundException
,
IOException
,
AmazonClientException
{
Path
path
=
qualify
(
f
)
;
String
key
=
pathToKey
(
path
)
;
private
S3AFileStatus
[
]
innerListStatus
(
Path
f
)
throws
FileNotFoundException
,
IOException
,
AmazonClientException
{
Path
path
=
qualify
(
f
)
;
String
key
=
pathToKey
(
path
)
;
LOG
.
debug
(
,
path
)
;
entryPoint
(
INVOCATION_LIST_STATUS
)
;
List
<
S3AFileStatus
>
result
;
final
S3AFileStatus
fileStatus
=
innerGetFileStatus
(
path
,
false
,
StatusProbeEnum
.
ALL
)
;
if
(
fileStatus
.
isDirectory
(
)
)
{
if
(
!
key
.
isEmpty
(
)
)
{
key
=
key
+
'/'
;
}
boolean
allowAuthoritative
=
allowAuthoritative
(
f
)
;
DirListingMetadata
dirMeta
=
S3Guard
.
listChildrenWithTtl
(
metadataStore
,
path
,
ttlTimeProvider
,
allowAuthoritative
)
;
if
(
allowAuthoritative
&&
dirMeta
!=
null
&&
dirMeta
.
isAuthoritative
(
)
)
{
return
S3Guard
.
dirMetaToStatuses
(
dirMeta
)
;
}
S3ListRequest
request
=
createListObjectsRequest
(
key
,
)
;
if
(
fileStatus
.
isDirectory
(
)
)
{
if
(
!
key
.
isEmpty
(
)
)
{
key
=
key
+
'/'
;
}
boolean
allowAuthoritative
=
allowAuthoritative
(
f
)
;
DirListingMetadata
dirMeta
=
S3Guard
.
listChildrenWithTtl
(
metadataStore
,
path
,
ttlTimeProvider
,
allowAuthoritative
)
;
if
(
allowAuthoritative
&&
dirMeta
!=
null
&&
dirMeta
.
isAuthoritative
(
)
)
{
return
S3Guard
.
dirMetaToStatuses
(
dirMeta
)
;
}
S3ListRequest
request
=
createListObjectsRequest
(
key
,
)
;
LOG
.
debug
(
,
key
)
;
Listing
.
FileStatusListingIterator
files
=
listing
.
createFileStatusListingIterator
(
path
,
request
,
ACCEPT_ALL
,
new
Listing
.
AcceptAllButSelfAndS3nDirs
(
path
)
)
;
result
=
new
ArrayList
<
>
(
files
.
getBatchSize
(
)
)
;
while
(
files
.
hasNext
(
)
)
{
result
.
add
(
files
.
next
(
)
)
;
}
return
S3Guard
.
dirListingUnion
(
metadataStore
,
path
,
result
,
dirMeta
,
allowAuthoritative
,
ttlTimeProvider
)
;
}
else
{
private
boolean
innerMkdirs
(
Path
p
,
FsPermission
permission
)
throws
IOException
,
FileAlreadyExistsException
,
AmazonClientException
{
Path
f
=
qualify
(
p
)
;
@
VisibleForTesting
@
Retries
.
RetryTranslated
S3AFileStatus
innerGetFileStatus
(
final
Path
f
,
final
boolean
needEmptyDirectoryFlag
,
final
Set
<
StatusProbeEnum
>
probes
)
throws
IOException
{
final
Path
path
=
qualify
(
f
)
;
String
key
=
pathToKey
(
path
)
;
@
VisibleForTesting
@
Retries
.
RetryTranslated
S3AFileStatus
innerGetFileStatus
(
final
Path
f
,
final
boolean
needEmptyDirectoryFlag
,
final
Set
<
StatusProbeEnum
>
probes
)
throws
IOException
{
final
Path
path
=
qualify
(
f
)
;
String
key
=
pathToKey
(
path
)
;
LOG
.
debug
(
,
path
,
key
,
needEmptyDirectoryFlag
)
;
boolean
allowAuthoritative
=
allowAuthoritative
(
path
)
;
PathMetadata
pm
=
null
;
if
(
hasMetadataStore
(
)
)
{
pm
=
S3Guard
.
getWithTtl
(
metadataStore
,
path
,
ttlTimeProvider
,
needEmptyDirectoryFlag
,
allowAuthoritative
)
;
}
Set
<
Path
>
tombstones
=
Collections
.
emptySet
(
)
;
if
(
pm
!=
null
)
{
S3AFileStatus
msStatus
=
pm
.
getFileStatus
(
)
;
if
(
pm
.
isDeleted
(
)
)
{
OffsetDateTime
deletedAt
=
OffsetDateTime
.
ofInstant
(
Instant
.
ofEpochMilli
(
msStatus
.
getModificationTime
(
)
)
,
ZoneOffset
.
UTC
)
;
throw
new
FileNotFoundException
(
+
path
+
+
+
deletedAt
)
;
}
if
(
!
msStatus
.
isDirectory
(
)
&&
!
allowAuthoritative
&&
probes
.
contains
(
StatusProbeEnum
.
Head
)
)
{
boolean
allowAuthoritative
=
allowAuthoritative
(
path
)
;
PathMetadata
pm
=
null
;
if
(
hasMetadataStore
(
)
)
{
pm
=
S3Guard
.
getWithTtl
(
metadataStore
,
path
,
ttlTimeProvider
,
needEmptyDirectoryFlag
,
allowAuthoritative
)
;
}
Set
<
Path
>
tombstones
=
Collections
.
emptySet
(
)
;
if
(
pm
!=
null
)
{
S3AFileStatus
msStatus
=
pm
.
getFileStatus
(
)
;
if
(
pm
.
isDeleted
(
)
)
{
OffsetDateTime
deletedAt
=
OffsetDateTime
.
ofInstant
(
Instant
.
ofEpochMilli
(
msStatus
.
getModificationTime
(
)
)
,
ZoneOffset
.
UTC
)
;
throw
new
FileNotFoundException
(
+
path
+
+
+
deletedAt
)
;
}
if
(
!
msStatus
.
isDirectory
(
)
&&
!
allowAuthoritative
&&
probes
.
contains
(
StatusProbeEnum
.
Head
)
)
{
LOG
.
debug
(
,
path
)
;
long
validTime
=
ttlTimeProvider
.
getNow
(
)
-
ttlTimeProvider
.
getMetadataTtl
(
)
;
final
long
msModTime
=
msStatus
.
getModificationTime
(
)
;
if
(
msModTime
<
validTime
)
{
Set
<
Path
>
tombstones
=
Collections
.
emptySet
(
)
;
if
(
pm
!=
null
)
{
S3AFileStatus
msStatus
=
pm
.
getFileStatus
(
)
;
if
(
pm
.
isDeleted
(
)
)
{
OffsetDateTime
deletedAt
=
OffsetDateTime
.
ofInstant
(
Instant
.
ofEpochMilli
(
msStatus
.
getModificationTime
(
)
)
,
ZoneOffset
.
UTC
)
;
throw
new
FileNotFoundException
(
+
path
+
+
+
deletedAt
)
;
}
if
(
!
msStatus
.
isDirectory
(
)
&&
!
allowAuthoritative
&&
probes
.
contains
(
StatusProbeEnum
.
Head
)
)
{
LOG
.
debug
(
,
path
)
;
long
validTime
=
ttlTimeProvider
.
getNow
(
)
-
ttlTimeProvider
.
getMetadataTtl
(
)
;
final
long
msModTime
=
msStatus
.
getModificationTime
(
)
;
if
(
msModTime
<
validTime
)
{
LOG
.
debug
(
,
path
)
;
try
{
S3AFileStatus
s3AFileStatus
=
s3GetFileStatus
(
path
,
key
,
probes
,
tombstones
,
needEmptyDirectoryFlag
)
;
final
long
s3ModTime
=
s3AFileStatus
.
getModificationTime
(
)
;
final
long
s3ModTime
=
s3AFileStatus
.
getModificationTime
(
)
;
if
(
s3ModTime
>
msModTime
)
{
LOG
.
debug
(
+
,
path
,
s3ModTime
,
msModTime
)
;
S3Guard
.
putAndReturn
(
metadataStore
,
s3AFileStatus
,
ttlTimeProvider
)
;
}
else
{
S3Guard
.
refreshEntry
(
metadataStore
,
pm
,
s3AFileStatus
,
ttlTimeProvider
)
;
}
return
s3AFileStatus
;
}
catch
(
FileNotFoundException
fne
)
{
LOG
.
warn
(
+
,
path
)
;
}
}
}
if
(
needEmptyDirectoryFlag
&&
msStatus
.
isDirectory
(
)
)
{
if
(
pm
.
isEmptyDirectory
(
)
!=
Tristate
.
UNKNOWN
)
{
return
msStatus
;
}
else
{
DirListingMetadata
children
=
S3Guard
.
listChildrenWithTtl
(
metadataStore
,
path
,
ttlTimeProvider
,
allowAuthoritative
)
;
if
(
children
!=
null
)
{
@
VisibleForTesting
@
Retries
.
RetryTranslated
S3AFileStatus
s3GetFileStatus
(
final
Path
path
,
final
String
key
,
final
Set
<
StatusProbeEnum
>
probes
,
@
Nullable
final
Set
<
Path
>
tombstones
,
final
boolean
needEmptyDirectoryFlag
)
throws
IOException
{
@
VisibleForTesting
@
Retries
.
RetryTranslated
S3AFileStatus
s3GetFileStatus
(
final
Path
path
,
final
String
key
,
final
Set
<
StatusProbeEnum
>
probes
,
@
Nullable
final
Set
<
Path
>
tombstones
,
final
boolean
needEmptyDirectoryFlag
)
throws
IOException
{
LOG
.
debug
(
,
path
)
;
Preconditions
.
checkArgument
(
!
needEmptyDirectoryFlag
||
probes
.
contains
(
StatusProbeEnum
.
List
)
,
+
,
path
)
;
if
(
!
key
.
isEmpty
(
)
&&
!
key
.
endsWith
(
)
&&
probes
.
contains
(
StatusProbeEnum
.
Head
)
)
{
try
{
ObjectMetadata
meta
=
getObjectMetadata
(
key
)
;
}
else
{
listSize
=
Math
.
min
(
2
+
tombstones
.
size
(
)
,
Math
.
max
(
2
,
maxKeys
)
)
;
}
S3ListRequest
request
=
createListObjectsRequest
(
dirKey
,
,
listSize
)
;
S3ListResult
listResult
=
listObjects
(
request
)
;
if
(
listResult
.
hasPrefixesOrObjects
(
contextAccessors
,
tombstones
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
listResult
.
logAtDebug
(
LOG
)
;
}
if
(
needEmptyDirectoryFlag
&&
listResult
.
representsEmptyDirectory
(
contextAccessors
,
dirKey
,
tombstones
)
)
{
return
new
S3AFileStatus
(
Tristate
.
TRUE
,
path
,
username
)
;
}
return
new
S3AFileStatus
(
Tristate
.
FALSE
,
path
,
username
)
;
}
else
if
(
key
.
isEmpty
(
)
)
{
LOG
.
debug
(
)
;
return
new
S3AFileStatus
(
Tristate
.
TRUE
,
path
,
username
)
;
}
}
catch
(
AmazonServiceException
e
)
{
@
Override
public
void
copyFromLocalFile
(
boolean
delSrc
,
boolean
overwrite
,
Path
src
,
Path
dst
)
throws
IOException
{
entryPoint
(
INVOCATION_COPY_FROM_LOCAL_FILE
)
;
@
Retries
.
RetryTranslated
private
void
innerCopyFromLocalFile
(
boolean
delSrc
,
boolean
overwrite
,
Path
src
,
Path
dst
)
throws
IOException
,
FileAlreadyExistsException
,
AmazonClientException
{
entryPoint
(
INVOCATION_COPY_FROM_LOCAL_FILE
)
;
@
Retries
.
RetryTranslated
private
CopyResult
copyFile
(
String
srcKey
,
String
dstKey
,
long
size
,
S3ObjectAttributes
srcAttributes
,
S3AReadOpContext
readContext
)
throws
IOException
,
InterruptedIOException
{
@
Retries
.
RetryTranslated
private
CopyResult
copyFile
(
String
srcKey
,
String
dstKey
,
long
size
,
S3ObjectAttributes
srcAttributes
,
S3AReadOpContext
readContext
)
throws
IOException
,
InterruptedIOException
{
LOG
.
debug
(
,
srcKey
,
dstKey
)
;
ProgressListener
progressListener
=
progressEvent
->
{
switch
(
progressEvent
.
getEventType
(
)
)
{
case
TRANSFER_PART_COMPLETED_EVENT
:
incrementWriteOperations
(
)
;
break
;
default
:
break
;
}
}
;
ChangeTracker
changeTracker
=
new
ChangeTracker
(
keyToQualifiedPath
(
srcKey
)
.
toString
(
)
,
changeDetectionPolicy
,
readContext
.
instrumentation
.
newInputStreamStatistics
(
)
.
getVersionMismatchCounter
(
)
,
srcAttributes
)
;
String
action
=
+
srcKey
+
+
dstKey
+
;
Invoker
readInvoker
=
readContext
.
getReadInvoker
(
)
;
ObjectMetadata
srcom
;
try
{
srcom
=
once
(
action
,
srcKey
,
(
)
->
getObjectMetadata
(
srcKey
,
changeTracker
,
readInvoker
,
)
)
;
}
catch
(
FileNotFoundException
e
)
{
private
void
setOptionalCopyObjectRequestParameters
(
ObjectMetadata
srcom
,
CopyObjectRequest
copyObjectRequest
)
{
String
sourceKMSId
=
srcom
.
getSSEAwsKmsKeyId
(
)
;
if
(
isNotEmpty
(
sourceKMSId
)
)
{
@
Retries
.
OnceRaw
InitiateMultipartUploadResult
initiateMultipartUpload
(
InitiateMultipartUploadRequest
request
)
throws
IOException
{
@
InterfaceAudience
.
Private
@
Retries
.
RetryTranslated
(
+
)
void
finishedWrite
(
String
key
,
long
length
,
String
eTag
,
String
versionId
,
@
Nullable
final
BulkOperationState
operationState
)
throws
MetadataPersistenceException
{
if
(
activeState
==
null
)
{
stateToClose
=
S3Guard
.
initiateBulkWrite
(
metadataStore
,
isDir
?
BulkOperationState
.
OperationType
.
Mkdir
:
BulkOperationState
.
OperationType
.
Put
,
keyToPath
(
key
)
)
;
activeState
=
stateToClose
;
}
S3Guard
.
addAncestors
(
metadataStore
,
p
,
ttlTimeProvider
,
activeState
)
;
S3AFileStatus
status
=
createUploadFileStatus
(
p
,
isDir
,
length
,
getDefaultBlockSize
(
p
)
,
username
,
eTag
,
versionId
)
;
boolean
authoritative
=
false
;
if
(
isDir
)
{
status
.
setIsEmptyDirectory
(
Tristate
.
TRUE
)
;
authoritative
=
allowAuthoritative
(
p
)
;
}
if
(
!
authoritative
)
{
S3Guard
.
putAndReturn
(
metadataStore
,
status
,
ttlTimeProvider
,
activeState
)
;
}
else
{
S3Guard
.
putAuthDirectoryMarker
(
metadataStore
,
status
,
ttlTimeProvider
,
activeState
)
;
}
}
waitForCompletionIgnoringExceptions
(
deletion
)
;
}
catch
(
IOException
e
)
{
@
Override
@
Retries
.
RetryTranslated
public
EtagChecksum
getFileChecksum
(
Path
f
,
final
long
length
)
throws
IOException
{
Preconditions
.
checkArgument
(
length
>=
0
)
;
entryPoint
(
INVOCATION_GET_FILE_CHECKSUM
)
;
if
(
getConf
(
)
.
getBoolean
(
ETAG_CHECKSUM_ENABLED
,
ETAG_CHECKSUM_ENABLED_DEFAULT
)
)
{
Path
path
=
qualify
(
f
)
;
@
Retries
.
RetryTranslated
private
RemoteIterator
<
S3ALocatedFileStatus
>
innerListFiles
(
final
Path
f
,
final
boolean
recursive
,
final
Listing
.
FileStatusAcceptor
acceptor
,
final
S3AFileStatus
status
,
final
boolean
collectTombstones
,
final
boolean
forceNonAuthoritativeMS
)
throws
IOException
{
entryPoint
(
INVOCATION_LIST_FILES
)
;
Path
path
=
qualify
(
f
)
;
@
Retries
.
RetryTranslated
private
RemoteIterator
<
S3ALocatedFileStatus
>
innerListFiles
(
final
Path
f
,
final
boolean
recursive
,
final
Listing
.
FileStatusAcceptor
acceptor
,
final
S3AFileStatus
status
,
final
boolean
collectTombstones
,
final
boolean
forceNonAuthoritativeMS
)
throws
IOException
{
entryPoint
(
INVOCATION_LIST_FILES
)
;
Path
path
=
qualify
(
f
)
;
LOG
.
debug
(
,
path
,
recursive
)
;
try
{
if
(
status
!=
null
&&
status
.
isFile
(
)
)
{
@
Override
@
Retries
.
OnceTranslated
(
)
public
RemoteIterator
<
LocatedFileStatus
>
listLocatedStatus
(
final
Path
f
,
final
PathFilter
filter
)
throws
FileNotFoundException
,
IOException
{
entryPoint
(
INVOCATION_LIST_LOCATED_STATUS
)
;
Path
path
=
qualify
(
f
)
;
@
Retries
.
OnceRaw
void
abortMultipartUpload
(
String
destKey
,
String
uploadId
)
{
@
Retries
.
OnceRaw
void
abortMultipartUpload
(
MultipartUpload
upload
)
{
String
destKey
;
String
uploadId
;
destKey
=
upload
.
getKey
(
)
;
uploadId
=
upload
.
getUploadId
(
)
;
if
(
LOG
.
isInfoEnabled
(
)
)
{
DateFormat
df
=
new
SimpleDateFormat
(
)
;
public
AWSCredentialProviderList
shareCredentials
(
final
String
purpose
)
{
@
Override
@
Retries
.
RetryTranslated
public
CompletableFuture
<
FSDataInputStream
>
openFileWithOptions
(
final
Path
rawPath
,
final
OpenFileParameters
parameters
)
throws
IOException
{
final
Path
path
=
qualify
(
rawPath
)
;
Configuration
options
=
parameters
.
getOptions
(
)
;
Set
<
String
>
mandatoryKeys
=
parameters
.
getMandatoryKeys
(
)
;
String
sql
=
options
.
get
(
SelectConstants
.
SELECT_SQL
,
null
)
;
boolean
isSelect
=
sql
!=
null
;
if
(
isSelect
)
{
rejectUnknownMandatoryKeys
(
mandatoryKeys
,
InternalSelectConstants
.
SELECT_OPTIONS
,
+
path
+
)
;
}
else
{
rejectUnknownMandatoryKeys
(
mandatoryKeys
,
InternalConstants
.
STANDARD_OPENFILE_KEYS
,
+
path
+
)
;
}
FileStatus
providedStatus
=
parameters
.
getStatus
(
)
;
S3AFileStatus
fileStatus
;
if
(
providedStatus
!=
null
)
{
Preconditions
.
checkArgument
(
path
.
equals
(
providedStatus
.
getPath
(
)
)
,
,
path
,
providedStatus
)
;
if
(
providedStatus
instanceof
S3AFileStatus
)
{
Set
<
String
>
mandatoryKeys
=
parameters
.
getMandatoryKeys
(
)
;
String
sql
=
options
.
get
(
SelectConstants
.
SELECT_SQL
,
null
)
;
boolean
isSelect
=
sql
!=
null
;
if
(
isSelect
)
{
rejectUnknownMandatoryKeys
(
mandatoryKeys
,
InternalSelectConstants
.
SELECT_OPTIONS
,
+
path
+
)
;
}
else
{
rejectUnknownMandatoryKeys
(
mandatoryKeys
,
InternalConstants
.
STANDARD_OPENFILE_KEYS
,
+
path
+
)
;
}
FileStatus
providedStatus
=
parameters
.
getStatus
(
)
;
S3AFileStatus
fileStatus
;
if
(
providedStatus
!=
null
)
{
Preconditions
.
checkArgument
(
path
.
equals
(
providedStatus
.
getPath
(
)
)
,
,
path
,
providedStatus
)
;
if
(
providedStatus
instanceof
S3AFileStatus
)
{
LOG
.
debug
(
+
,
providedStatus
)
;
fileStatus
=
(
S3AFileStatus
)
providedStatus
;
}
else
if
(
providedStatus
instanceof
S3ALocatedFileStatus
)
{
if
(
isSelect
)
{
rejectUnknownMandatoryKeys
(
mandatoryKeys
,
InternalSelectConstants
.
SELECT_OPTIONS
,
+
path
+
)
;
}
else
{
rejectUnknownMandatoryKeys
(
mandatoryKeys
,
InternalConstants
.
STANDARD_OPENFILE_KEYS
,
+
path
+
)
;
}
FileStatus
providedStatus
=
parameters
.
getStatus
(
)
;
S3AFileStatus
fileStatus
;
if
(
providedStatus
!=
null
)
{
Preconditions
.
checkArgument
(
path
.
equals
(
providedStatus
.
getPath
(
)
)
,
,
path
,
providedStatus
)
;
if
(
providedStatus
instanceof
S3AFileStatus
)
{
LOG
.
debug
(
+
,
providedStatus
)
;
fileStatus
=
(
S3AFileStatus
)
providedStatus
;
}
else
if
(
providedStatus
instanceof
S3ALocatedFileStatus
)
{
LOG
.
debug
(
+
,
providedStatus
)
;
fileStatus
=
(
(
S3ALocatedFileStatus
)
providedStatus
)
.
toS3AFileStatus
(
)
;
}
else
{
}
long
diff
=
targetPos
-
pos
;
if
(
diff
>
0
)
{
int
available
=
wrappedStream
.
available
(
)
;
long
forwardSeekRange
=
Math
.
max
(
readahead
,
available
)
;
long
remainingInCurrentRequest
=
remainingInCurrentRequest
(
)
;
long
forwardSeekLimit
=
Math
.
min
(
remainingInCurrentRequest
,
forwardSeekRange
)
;
boolean
skipForward
=
remainingInCurrentRequest
>
0
&&
diff
<
forwardSeekLimit
;
if
(
skipForward
)
{
LOG
.
debug
(
,
uri
,
diff
)
;
streamStatistics
.
seekForwards
(
diff
)
;
long
skipped
=
wrappedStream
.
skip
(
diff
)
;
if
(
skipped
>
0
)
{
pos
+=
skipped
;
incrementBytesRead
(
diff
)
;
}
if
(
pos
==
targetPos
)
{
@
Retries
.
OnceTranslated
private
void
onReadFailure
(
IOException
ioe
,
int
length
,
boolean
forceAbort
)
throws
IOException
{
@
Retries
.
OnceRaw
private
void
closeStream
(
String
reason
,
long
length
,
boolean
forceAbort
)
{
if
(
isObjectStreamOpen
(
)
)
{
long
remaining
=
remainingInCurrentRequest
(
)
;
public
MutableGaugeLong
lookupGauge
(
String
name
)
{
MutableMetric
metric
=
lookupMetric
(
name
)
;
if
(
metric
==
null
)
{
public
MutableQuantiles
lookupQuantiles
(
String
name
)
{
MutableMetric
metric
=
lookupMetric
(
name
)
;
if
(
metric
==
null
)
{
public
long
incrementCounter
(
Statistic
op
,
long
count
)
{
long
updated
=
opsCount
.
get
(
op
)
.
addAndGet
(
count
)
;
public
static
AWSCredentialProviderList
createAWSCredentialProviderSet
(
@
Nullable
URI
binding
,
Configuration
conf
)
throws
IOException
{
S3xLoginHelper
.
rejectSecretsInURIs
(
binding
)
;
AWSCredentialProviderList
credentials
=
buildAWSProviderList
(
binding
,
conf
,
AWS_CREDENTIALS_PROVIDER
,
STANDARD_AWS_PROVIDERS
,
new
HashSet
<
>
(
)
)
;
public
static
int
intOption
(
Configuration
conf
,
String
key
,
int
defVal
,
int
min
)
{
int
v
=
conf
.
getInt
(
key
,
defVal
)
;
Preconditions
.
checkArgument
(
v
>=
min
,
String
.
format
(
,
key
,
v
,
min
)
)
;
public
static
long
longOption
(
Configuration
conf
,
String
key
,
long
defVal
,
long
min
)
{
long
v
=
conf
.
getLong
(
key
,
defVal
)
;
Preconditions
.
checkArgument
(
v
>=
min
,
String
.
format
(
,
key
,
v
,
min
)
)
;
public
static
long
longBytesOption
(
Configuration
conf
,
String
key
,
long
defVal
,
long
min
)
{
long
v
=
conf
.
getLongBytes
(
key
,
defVal
)
;
Preconditions
.
checkArgument
(
v
>=
min
,
String
.
format
(
,
key
,
v
,
min
)
)
;
public
static
Configuration
propagateBucketOptions
(
Configuration
source
,
String
bucket
)
{
Preconditions
.
checkArgument
(
StringUtils
.
isNotEmpty
(
bucket
)
,
)
;
final
String
bucketPrefix
=
FS_S3A_BUCKET_PREFIX
+
bucket
+
'.'
;
Preconditions
.
checkArgument
(
StringUtils
.
isNotEmpty
(
bucket
)
,
)
;
final
String
bucketPrefix
=
FS_S3A_BUCKET_PREFIX
+
bucket
+
'.'
;
LOG
.
debug
(
,
bucketPrefix
)
;
final
Configuration
dest
=
new
Configuration
(
source
)
;
for
(
Map
.
Entry
<
String
,
String
>
entry
:
source
)
{
final
String
key
=
entry
.
getKey
(
)
;
final
String
value
=
entry
.
getValue
(
)
;
if
(
!
key
.
startsWith
(
bucketPrefix
)
||
bucketPrefix
.
equals
(
key
)
)
{
continue
;
}
final
String
stripped
=
key
.
substring
(
bucketPrefix
.
length
(
)
)
;
if
(
stripped
.
startsWith
(
)
||
.
equals
(
stripped
)
)
{
LOG
.
debug
(
,
key
)
;
}
else
{
String
origin
=
+
StringUtils
.
join
(
source
.
getPropertySources
(
key
)
,
)
+
;
final
String
generic
=
FS_S3A_PREFIX
+
stripped
;
initProxySupport
(
conf
,
bucket
,
awsConf
)
;
initUserAgent
(
conf
,
awsConf
)
;
if
(
StringUtils
.
isNotEmpty
(
awsServiceIdentifier
)
)
{
String
configKey
=
null
;
switch
(
awsServiceIdentifier
)
{
case
AWS_SERVICE_IDENTIFIER_S3
:
configKey
=
SIGNING_ALGORITHM_S3
;
break
;
case
AWS_SERVICE_IDENTIFIER_DDB
:
configKey
=
SIGNING_ALGORITHM_DDB
;
break
;
case
AWS_SERVICE_IDENTIFIER_STS
:
configKey
=
SIGNING_ALGORITHM_STS
;
break
;
default
:
}
if
(
configKey
!=
null
)
{
String
signerOverride
=
conf
.
getTrimmed
(
configKey
,
)
;
if
(
!
signerOverride
.
isEmpty
(
)
)
{
public
static
void
initConnectionSettings
(
Configuration
conf
,
ClientConfiguration
awsConf
)
throws
IOException
{
awsConf
.
setMaxConnections
(
intOption
(
conf
,
MAXIMUM_CONNECTIONS
,
DEFAULT_MAXIMUM_CONNECTIONS
,
1
)
)
;
initProtocolSettings
(
conf
,
awsConf
)
;
awsConf
.
setMaxErrorRetry
(
intOption
(
conf
,
MAX_ERROR_RETRIES
,
DEFAULT_MAX_ERROR_RETRIES
,
0
)
)
;
awsConf
.
setConnectionTimeout
(
intOption
(
conf
,
ESTABLISH_TIMEOUT
,
DEFAULT_ESTABLISH_TIMEOUT
,
0
)
)
;
awsConf
.
setSocketTimeout
(
intOption
(
conf
,
SOCKET_TIMEOUT
,
DEFAULT_SOCKET_TIMEOUT
,
0
)
)
;
int
sockSendBuffer
=
intOption
(
conf
,
SOCKET_SEND_BUFFER
,
DEFAULT_SOCKET_SEND_BUFFER
,
2048
)
;
int
sockRecvBuffer
=
intOption
(
conf
,
SOCKET_RECV_BUFFER
,
DEFAULT_SOCKET_RECV_BUFFER
,
2048
)
;
long
requestTimeoutMillis
=
conf
.
getTimeDuration
(
REQUEST_TIMEOUT
,
DEFAULT_REQUEST_TIMEOUT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
if
(
requestTimeoutMillis
>
Integer
.
MAX_VALUE
)
{
awsConf
.
setMaxConnections
(
intOption
(
conf
,
MAXIMUM_CONNECTIONS
,
DEFAULT_MAXIMUM_CONNECTIONS
,
1
)
)
;
initProtocolSettings
(
conf
,
awsConf
)
;
awsConf
.
setMaxErrorRetry
(
intOption
(
conf
,
MAX_ERROR_RETRIES
,
DEFAULT_MAX_ERROR_RETRIES
,
0
)
)
;
awsConf
.
setConnectionTimeout
(
intOption
(
conf
,
ESTABLISH_TIMEOUT
,
DEFAULT_ESTABLISH_TIMEOUT
,
0
)
)
;
awsConf
.
setSocketTimeout
(
intOption
(
conf
,
SOCKET_TIMEOUT
,
DEFAULT_SOCKET_TIMEOUT
,
0
)
)
;
int
sockSendBuffer
=
intOption
(
conf
,
SOCKET_SEND_BUFFER
,
DEFAULT_SOCKET_SEND_BUFFER
,
2048
)
;
int
sockRecvBuffer
=
intOption
(
conf
,
SOCKET_RECV_BUFFER
,
DEFAULT_SOCKET_RECV_BUFFER
,
2048
)
;
long
requestTimeoutMillis
=
conf
.
getTimeDuration
(
REQUEST_TIMEOUT
,
DEFAULT_REQUEST_TIMEOUT
,
TimeUnit
.
SECONDS
,
TimeUnit
.
MILLISECONDS
)
;
if
(
requestTimeoutMillis
>
Integer
.
MAX_VALUE
)
{
LOG
.
debug
(
,
requestTimeoutMillis
,
Integer
.
MAX_VALUE
)
;
requestTimeoutMillis
=
Integer
.
MAX_VALUE
;
}
awsConf
.
setRequestTimeout
(
(
int
)
requestTimeoutMillis
)
;
awsConf
.
setSocketBufferSizeHints
(
sockSendBuffer
,
sockRecvBuffer
)
;
String
signerOverride
=
conf
.
getTrimmed
(
SIGNING_ALGORITHM
,
)
;
if
(
!
signerOverride
.
isEmpty
(
)
)
{
if
(
!
proxyHost
.
isEmpty
(
)
)
{
awsConf
.
setProxyHost
(
proxyHost
)
;
if
(
proxyPort
>=
0
)
{
awsConf
.
setProxyPort
(
proxyPort
)
;
}
else
{
if
(
conf
.
getBoolean
(
SECURE_CONNECTIONS
,
DEFAULT_SECURE_CONNECTIONS
)
)
{
LOG
.
warn
(
)
;
awsConf
.
setProxyPort
(
443
)
;
}
else
{
LOG
.
warn
(
)
;
awsConf
.
setProxyPort
(
80
)
;
}
}
final
String
proxyUsername
=
lookupPassword
(
bucket
,
conf
,
PROXY_USERNAME
,
null
,
null
)
;
final
String
proxyPassword
=
lookupPassword
(
bucket
,
conf
,
PROXY_PASSWORD
,
null
,
null
)
;
if
(
(
proxyUsername
==
null
)
!=
(
proxyPassword
==
null
)
)
{
String
msg
=
+
PROXY_USERNAME
+
+
PROXY_PASSWORD
+
;
LOG
.
warn
(
)
;
awsConf
.
setProxyPort
(
443
)
;
}
else
{
LOG
.
warn
(
)
;
awsConf
.
setProxyPort
(
80
)
;
}
}
final
String
proxyUsername
=
lookupPassword
(
bucket
,
conf
,
PROXY_USERNAME
,
null
,
null
)
;
final
String
proxyPassword
=
lookupPassword
(
bucket
,
conf
,
PROXY_PASSWORD
,
null
,
null
)
;
if
(
(
proxyUsername
==
null
)
!=
(
proxyPassword
==
null
)
)
{
String
msg
=
+
PROXY_USERNAME
+
+
PROXY_PASSWORD
+
;
LOG
.
error
(
msg
)
;
throw
new
IllegalArgumentException
(
msg
)
;
}
awsConf
.
setProxyUsername
(
proxyUsername
)
;
awsConf
.
setProxyPassword
(
proxyPassword
)
;
awsConf
.
setProxyDomain
(
conf
.
getTrimmed
(
PROXY_DOMAIN
)
)
;
awsConf
.
setProxyWorkstation
(
conf
.
getTrimmed
(
PROXY_WORKSTATION
)
)
;
else
{
LOG
.
warn
(
)
;
awsConf
.
setProxyPort
(
80
)
;
}
}
final
String
proxyUsername
=
lookupPassword
(
bucket
,
conf
,
PROXY_USERNAME
,
null
,
null
)
;
final
String
proxyPassword
=
lookupPassword
(
bucket
,
conf
,
PROXY_PASSWORD
,
null
,
null
)
;
if
(
(
proxyUsername
==
null
)
!=
(
proxyPassword
==
null
)
)
{
String
msg
=
+
PROXY_USERNAME
+
+
PROXY_PASSWORD
+
;
LOG
.
error
(
msg
)
;
throw
new
IllegalArgumentException
(
msg
)
;
}
awsConf
.
setProxyUsername
(
proxyUsername
)
;
awsConf
.
setProxyPassword
(
proxyPassword
)
;
awsConf
.
setProxyDomain
(
conf
.
getTrimmed
(
PROXY_DOMAIN
)
)
;
awsConf
.
setProxyWorkstation
(
conf
.
getTrimmed
(
PROXY_WORKSTATION
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
awsConf
.
getProxyHost
(
)
,
awsConf
.
getProxyPort
(
)
,
String
.
valueOf
(
awsConf
.
getProxyUsername
(
)
)
,
awsConf
.
getProxyPassword
(
)
,
awsConf
.
getProxyDomain
(
)
,
awsConf
.
getProxyWorkstation
(
)
)
;
static
void
patchSecurityCredentialProviders
(
Configuration
conf
)
{
Collection
<
String
>
customCredentials
=
conf
.
getStringCollection
(
S3A_SECURITY_CREDENTIAL_PROVIDER_PATH
)
;
Collection
<
String
>
hadoopCredentials
=
conf
.
getStringCollection
(
CREDENTIAL_PROVIDER_PATH
)
;
if
(
!
customCredentials
.
isEmpty
(
)
)
{
List
<
String
>
all
=
Lists
.
newArrayList
(
customCredentials
)
;
all
.
addAll
(
hadoopCredentials
)
;
String
joined
=
StringUtils
.
join
(
all
,
','
)
;
public
static
void
clearBucketOption
(
Configuration
conf
,
String
bucket
,
String
genericKey
)
{
final
String
baseKey
=
genericKey
.
startsWith
(
FS_S3A_PREFIX
)
?
genericKey
.
substring
(
FS_S3A_PREFIX
.
length
(
)
)
:
genericKey
;
String
k
=
FS_S3A_BUCKET_PREFIX
+
bucket
+
'.'
+
baseKey
;
@
Override
protected
Map
<
Class
<
?
extends
Exception
>
,
RetryPolicy
>
createExceptionMap
(
)
{
Map
<
Class
<
?
extends
Exception
>
,
RetryPolicy
>
b
=
super
.
createExceptionMap
(
)
;
Configuration
conf
=
getConfiguration
(
)
;
int
limit
=
conf
.
getInt
(
S3GUARD_CONSISTENCY_RETRY_LIMIT
,
S3GUARD_CONSISTENCY_RETRY_LIMIT_DEFAULT
)
;
long
interval
=
conf
.
getTimeDuration
(
S3GUARD_CONSISTENCY_RETRY_INTERVAL
,
S3GUARD_CONSISTENCY_RETRY_INTERVAL_DEFAULT
,
TimeUnit
.
MILLISECONDS
)
;
RetryPolicy
retryPolicy
=
retryUpToMaximumCountWithProportionalSleep
(
limit
,
interval
,
TimeUnit
.
MILLISECONDS
)
;
public
void
logAtDebug
(
Logger
log
)
{
Collection
<
String
>
prefixes
=
getCommonPrefixes
(
)
;
Collection
<
S3ObjectSummary
>
summaries
=
getObjectSummaries
(
)
;
public
void
logAtDebug
(
Logger
log
)
{
Collection
<
String
>
prefixes
=
getCommonPrefixes
(
)
;
Collection
<
S3ObjectSummary
>
summaries
=
getObjectSummaries
(
)
;
log
.
debug
(
,
prefixes
.
size
(
)
,
summaries
.
size
(
)
)
;
for
(
S3ObjectSummary
summary
:
summaries
)
{
public
void
writeFailed
(
Exception
ex
)
{
@
Retries
.
RetryTranslated
public
String
initiateMultiPartUpload
(
String
destKey
)
throws
IOException
{
@
Retries
.
RetryTranslated
public
CompleteMultipartUploadResult
completeMPUwithRetries
(
String
destKey
,
String
uploadId
,
List
<
PartETag
>
partETags
,
long
length
,
AtomicInteger
errorCount
)
throws
IOException
{
checkNotNull
(
uploadId
)
;
checkNotNull
(
partETags
)
;
@
Retries
.
RetryTranslated
public
int
abortMultipartUploadsUnderPath
(
String
prefix
)
throws
IOException
{
@
Retries
.
RetryTranslated
public
int
abortMultipartUploadsUnderPath
(
String
prefix
)
throws
IOException
{
LOG
.
debug
(
,
prefix
)
;
int
count
=
0
;
List
<
MultipartUpload
>
multipartUploads
=
owner
.
listMultipartUploads
(
prefix
)
;
public
UploadPartRequest
newUploadPartRequest
(
String
destKey
,
String
uploadId
,
int
partNumber
,
int
size
,
InputStream
uploadStream
,
File
sourceFile
,
Long
offset
)
throws
PathIOException
{
checkNotNull
(
uploadId
)
;
checkArgument
(
(
uploadStream
!=
null
)
^
(
sourceFile
!=
null
)
,
)
;
checkArgument
(
size
>=
0
,
,
size
)
;
checkArgument
(
partNumber
>
0
,
,
DEFAULT_UPLOAD_PART_COUNT_LIMIT
,
partNumber
)
;
@
Retries
.
RetryTranslated
public
CompleteMultipartUploadResult
commitUpload
(
String
destKey
,
String
uploadId
,
List
<
PartETag
>
partETags
,
long
length
,
@
Nullable
BulkOperationState
operationState
)
throws
IOException
{
checkNotNull
(
uploadId
)
;
checkNotNull
(
partETags
)
;
@
Retries
.
RetryTranslated
public
SelectObjectContentResult
select
(
final
Path
source
,
final
SelectObjectContentRequest
request
,
final
String
action
)
throws
IOException
{
String
bucketName
=
request
.
getBucketName
(
)
;
Preconditions
.
checkArgument
(
bucket
.
equals
(
bucketName
)
,
,
bucketName
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Retries
.
RetryTranslated
public
SelectObjectContentResult
select
(
final
Path
source
,
final
SelectObjectContentRequest
request
,
final
String
action
)
throws
IOException
{
String
bucketName
=
request
.
getBucketName
(
)
;
Preconditions
.
checkArgument
(
bucket
.
equals
(
bucketName
)
,
,
bucketName
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
source
,
request
.
getExpression
(
)
)
;
public
void
operationRetried
(
String
text
,
Exception
ex
,
int
retries
,
boolean
idempotent
)
{
if
(
retries
==
0
)
{
public
static
AWSSecurityTokenServiceClientBuilder
builder
(
final
AWSCredentialsProvider
credentials
,
final
ClientConfiguration
awsConf
,
final
String
stsEndpoint
,
final
String
stsRegion
)
{
final
AWSSecurityTokenServiceClientBuilder
builder
=
AWSSecurityTokenServiceClientBuilder
.
standard
(
)
;
Preconditions
.
checkArgument
(
credentials
!=
null
,
)
;
builder
.
withClientConfiguration
(
awsConf
)
;
builder
.
withCredentials
(
credentials
)
;
boolean
destIsStandardEndpoint
=
STS_STANDARD
.
equals
(
stsEndpoint
)
;
if
(
isNotEmpty
(
stsEndpoint
)
&&
!
destIsStandardEndpoint
)
{
Preconditions
.
checkArgument
(
isNotEmpty
(
stsRegion
)
,
,
stsEndpoint
)
;
for
(
String
customSigner
:
customSigners
)
{
String
[
]
parts
=
customSigner
.
split
(
)
;
if
(
!
(
parts
.
length
==
1
||
parts
.
length
==
2
||
parts
.
length
==
3
)
)
{
String
message
=
+
+
+
customSigner
+
;
LOG
.
error
(
message
)
;
throw
new
IllegalArgumentException
(
message
)
;
}
if
(
parts
.
length
==
1
)
{
}
else
{
maybeRegisterSigner
(
parts
[
0
]
,
parts
[
1
]
,
ownerConf
)
;
if
(
parts
.
length
==
3
)
{
Class
<
?
extends
AwsSignerInitializer
>
clazz
=
null
;
try
{
clazz
=
(
Class
<
?
extends
AwsSignerInitializer
>
)
ownerConf
.
getClassByName
(
parts
[
2
]
)
;
}
catch
(
ClassNotFoundException
e
)
{
throw
new
RuntimeException
(
String
.
format
(
+
,
parts
[
2
]
,
parts
[
0
]
)
,
e
)
;
public
Token
<
AbstractS3ATokenIdentifier
>
createDelegationToken
(
final
Optional
<
RoleModel
.
Policy
>
policy
,
final
EncryptionSecrets
encryptionSecrets
,
final
Text
renewer
)
throws
IOException
{
requireServiceStarted
(
)
;
final
AbstractS3ATokenIdentifier
tokenIdentifier
=
createTokenIdentifier
(
policy
,
encryptionSecrets
,
renewer
)
;
if
(
tokenIdentifier
!=
null
)
{
Token
<
AbstractS3ATokenIdentifier
>
token
=
new
Token
<
>
(
tokenIdentifier
,
secretManager
)
;
token
.
setKind
(
getKind
(
)
)
;
@
Override
protected
void
serviceInit
(
final
Configuration
conf
)
throws
Exception
{
super
.
serviceInit
(
conf
)
;
checkState
(
hasDelegationTokenBinding
(
conf
)
,
E_DELEGATION_TOKENS_DISABLED
)
;
Class
<
?
extends
AbstractDelegationTokenBinding
>
binding
=
conf
.
getClass
(
DelegationConstants
.
DELEGATION_TOKEN_BINDING
,
SessionTokenBinding
.
class
,
AbstractDelegationTokenBinding
.
class
)
;
tokenBinding
=
binding
.
newInstance
(
)
;
tokenBinding
.
bindToFileSystem
(
getCanonicalUri
(
)
,
getStoreContext
(
)
,
getPolicyProvider
(
)
)
;
tokenBinding
.
init
(
conf
)
;
tokenBindingName
=
tokenBinding
.
getKind
(
)
.
toString
(
)
;
@
Override
protected
void
serviceStart
(
)
throws
Exception
{
super
.
serviceStart
(
)
;
tokenBinding
.
start
(
)
;
bindToAnyDelegationToken
(
)
;
@
VisibleForTesting
public
void
bindToDelegationToken
(
final
Token
<
AbstractS3ATokenIdentifier
>
token
)
throws
IOException
{
checkState
(
!
credentialProviders
.
isPresent
(
)
,
E_ALREADY_DEPLOYED
)
;
boundDT
=
Optional
.
of
(
token
)
;
AbstractS3ATokenIdentifier
dti
=
extractIdentifier
(
token
)
;
private
void
noteTokenCreated
(
final
Token
<
AbstractS3ATokenIdentifier
>
token
)
{
@
VisibleForTesting
public
static
Token
<
AbstractS3ATokenIdentifier
>
lookupToken
(
final
Credentials
credentials
,
final
Text
service
,
final
Text
kind
)
throws
DelegationTokenIOException
{
@
VisibleForTesting
public
static
Token
<
AbstractS3ATokenIdentifier
>
lookupToken
(
final
Credentials
credentials
,
final
Text
service
,
final
Text
kind
)
throws
DelegationTokenIOException
{
LOG
.
debug
(
,
service
)
;
Token
<
?
>
token
=
credentials
.
getToken
(
service
)
;
if
(
token
!=
null
)
{
Text
tokenKind
=
token
.
getKind
(
)
;
protected
void
setWorkPath
(
Path
workPath
)
{
@
Override
public
void
abortJob
(
JobContext
context
,
JobStatus
.
State
state
)
throws
IOException
{
protected
void
maybeIgnore
(
boolean
suppress
,
String
action
,
IOException
ex
)
throws
IOException
{
if
(
suppress
)
{
private
synchronized
ExecutorService
buildThreadPool
(
JobContext
context
,
int
numThreads
)
{
Preconditions
.
checkArgument
(
numThreads
>
0
,
)
;
if
(
threadPool
==
null
)
{
private
MaybeIOE
commit
(
final
SinglePendingCommit
commit
,
final
String
origin
,
final
BulkOperationState
operationState
)
{
private
MaybeIOE
commit
(
final
SinglePendingCommit
commit
,
final
String
origin
,
final
BulkOperationState
operationState
)
{
LOG
.
debug
(
,
commit
)
;
MaybeIOE
outcome
;
String
destKey
=
;
try
{
commit
.
validate
(
)
;
destKey
=
commit
.
getDestinationKey
(
)
;
long
l
=
innerCommit
(
commit
,
operationState
)
;
private
void
abortSingleCommit
(
SinglePendingCommit
commit
)
throws
IOException
{
String
destKey
=
commit
.
getDestinationKey
(
)
;
String
origin
=
commit
.
getFilename
(
)
!=
null
?
(
+
commit
.
getFilename
(
)
)
:
;
String
uploadId
=
commit
.
getUploadId
(
)
;
public
MaybeIOE
abortAllSinglePendingCommits
(
Path
pendingDir
,
boolean
recursive
)
throws
IOException
{
Preconditions
.
checkArgument
(
pendingDir
!=
null
,
)
;
RemoteIterator
<
LocatedFileStatus
>
pendingFiles
;
try
{
pendingFiles
=
ls
(
pendingDir
,
recursive
)
;
}
catch
(
FileNotFoundException
fnfe
)
{
LOG
.
info
(
,
pendingDir
)
;
return
MaybeIOE
.
NONE
;
}
MaybeIOE
outcome
=
MaybeIOE
.
NONE
;
if
(
!
pendingFiles
.
hasNext
(
)
)
{
LOG
.
debug
(
,
pendingDir
)
;
}
while
(
pendingFiles
.
hasNext
(
)
)
{
Path
pendingFile
=
pendingFiles
.
next
(
)
.
getPath
(
)
;
if
(
pendingFile
.
getName
(
)
.
endsWith
(
CommitConstants
.
PENDING_SUFFIX
)
)
{
try
{
abortSingleCommit
(
SinglePendingCommit
.
load
(
fs
,
pendingFile
)
)
;
}
catch
(
FileNotFoundException
e
)
{
public
void
revertCommit
(
SinglePendingCommit
commit
,
BulkOperationState
operationState
)
throws
IOException
{
public
SinglePendingCommit
uploadFileToPendingCommit
(
File
localFile
,
Path
destPath
,
String
partition
,
long
uploadPartSize
,
Progressable
progress
)
throws
IOException
{
long
length
=
localFile
.
length
(
)
;
SinglePendingCommit
commitData
=
new
SinglePendingCommit
(
)
;
commitData
.
setDestinationKey
(
destKey
)
;
commitData
.
setBucket
(
fs
.
getBucket
(
)
)
;
commitData
.
touch
(
System
.
currentTimeMillis
(
)
)
;
commitData
.
setUploadId
(
uploadId
)
;
commitData
.
setUri
(
destURI
)
;
commitData
.
setText
(
partition
!=
null
?
+
partition
:
)
;
commitData
.
setLength
(
length
)
;
long
offset
=
0
;
long
numParts
=
(
length
/
uploadPartSize
+
(
(
length
%
uploadPartSize
)
>
0
?
1
:
0
)
)
;
if
(
numParts
==
0
)
{
numParts
=
1
;
}
if
(
numParts
>
InternalConstants
.
DEFAULT_UPLOAD_PART_COUNT_LIMIT
)
{
throw
new
PathIOException
(
destPath
.
toString
(
)
,
String
.
format
(
+
,
numParts
,
length
)
)
;
for
(
int
partNumber
=
1
;
partNumber
<=
numParts
;
partNumber
+=
1
)
{
long
size
=
Math
.
min
(
length
-
offset
,
uploadPartSize
)
;
UploadPartRequest
part
;
part
=
writeOperations
.
newUploadPartRequest
(
destKey
,
uploadId
,
partNumber
,
(
int
)
size
,
null
,
localFile
,
offset
)
;
part
.
setLastPart
(
partNumber
==
numParts
)
;
UploadPartResult
partResult
=
writeOperations
.
uploadPart
(
part
)
;
offset
+=
uploadPartSize
;
parts
.
add
(
partResult
.
getPartETag
(
)
)
;
}
commitData
.
bindCommitData
(
parts
)
;
statistics
.
commitUploaded
(
length
)
;
progress
.
progress
(
)
;
threw
=
false
;
return
commitData
;
}
finally
{
if
(
threw
&&
uploadId
!=
null
)
{
public
static
void
verifyIsMagicCommitFS
(
S3AFileSystem
fs
)
throws
PathCommitException
{
if
(
!
fs
.
isMagicCommitEnabled
(
)
)
{
String
fsUri
=
fs
.
getUri
(
)
.
toString
(
)
;
public
PutTracker
createTracker
(
Path
path
,
String
key
)
{
final
List
<
String
>
elements
=
splitPathToElements
(
path
)
;
PutTracker
tracker
;
if
(
isMagicFile
(
elements
)
)
{
if
(
isMagicCommitPath
(
elements
)
)
{
final
String
destKey
=
keyOfFinalDestination
(
elements
,
key
)
;
String
pendingsetPath
=
key
+
CommitConstants
.
PENDING_SUFFIX
;
owner
.
getInstrumentation
(
)
.
incrementCounter
(
Statistic
.
COMMITTER_MAGIC_FILES_CREATED
,
1
)
;
tracker
=
new
MagicCommitTracker
(
path
,
owner
.
getBucket
(
)
,
key
,
destKey
,
pendingsetPath
,
owner
.
getWriteOperationHelper
(
)
)
;
@
Override
public
PathOutputCommitter
createTaskCommitter
(
S3AFileSystem
fileSystem
,
Path
outputPath
,
TaskAttemptContext
context
)
throws
IOException
{
AbstractS3ACommitterFactory
factory
=
chooseCommitterFactory
(
fileSystem
,
outputPath
,
context
.
getConfiguration
(
)
)
;
if
(
factory
!=
null
)
{
PathOutputCommitter
committer
=
factory
.
createTaskCommitter
(
fileSystem
,
outputPath
,
context
)
;
private
AbstractS3ACommitterFactory
chooseCommitterFactory
(
S3AFileSystem
fileSystem
,
Path
outputPath
,
Configuration
taskConf
)
throws
PathCommitException
{
AbstractS3ACommitterFactory
factory
;
Configuration
fsConf
=
fileSystem
.
getConf
(
)
;
String
name
=
fsConf
.
getTrimmed
(
FS_S3A_COMMITTER_NAME
,
COMMITTER_NAME_FILE
)
;
name
=
taskConf
.
getTrimmed
(
FS_S3A_COMMITTER_NAME
,
name
)
;
private
static
void
waitFor
(
Collection
<
Future
<
?
>>
futures
)
{
int
size
=
futures
.
size
(
)
;
private
static
void
waitFor
(
Collection
<
Future
<
?
>>
futures
)
{
int
size
=
futures
.
size
(
)
;
LOG
.
debug
(
,
size
)
;
int
oldNumFinished
=
0
;
while
(
true
)
{
int
numFinished
=
(
int
)
futures
.
stream
(
)
.
filter
(
Future
::
isDone
)
.
count
(
)
;
if
(
oldNumFinished
!=
numFinished
)
{
public
static
PendingSet
load
(
FileSystem
fs
,
Path
path
)
throws
IOException
{
public
static
SuccessData
load
(
FileSystem
fs
,
Path
path
)
throws
IOException
{
@
Override
public
boolean
aboutToComplete
(
String
uploadId
,
List
<
PartETag
>
parts
,
long
bytesWritten
)
throws
IOException
{
Preconditions
.
checkArgument
(
StringUtils
.
isNotEmpty
(
uploadId
)
,
+
uploadId
)
;
Preconditions
.
checkArgument
(
parts
!=
null
,
)
;
Preconditions
.
checkArgument
(
!
parts
.
isEmpty
(
)
,
)
;
SinglePendingCommit
commitData
=
new
SinglePendingCommit
(
)
;
commitData
.
touch
(
System
.
currentTimeMillis
(
)
)
;
commitData
.
setDestinationKey
(
getDestKey
(
)
)
;
commitData
.
setBucket
(
bucket
)
;
commitData
.
setUri
(
path
.
toUri
(
)
.
toString
(
)
)
;
commitData
.
setUploadId
(
uploadId
)
;
commitData
.
setText
(
)
;
commitData
.
setLength
(
bytesWritten
)
;
commitData
.
bindCommitData
(
parts
)
;
byte
[
]
bytes
=
commitData
.
toBytes
(
)
;
@
Override
public
boolean
aboutToComplete
(
String
uploadId
,
List
<
PartETag
>
parts
,
long
bytesWritten
)
throws
IOException
{
Preconditions
.
checkArgument
(
StringUtils
.
isNotEmpty
(
uploadId
)
,
+
uploadId
)
;
Preconditions
.
checkArgument
(
parts
!=
null
,
)
;
Preconditions
.
checkArgument
(
!
parts
.
isEmpty
(
)
,
)
;
SinglePendingCommit
commitData
=
new
SinglePendingCommit
(
)
;
commitData
.
touch
(
System
.
currentTimeMillis
(
)
)
;
commitData
.
setDestinationKey
(
getDestKey
(
)
)
;
commitData
.
setBucket
(
bucket
)
;
commitData
.
setUri
(
path
.
toUri
(
)
.
toString
(
)
)
;
commitData
.
setUploadId
(
uploadId
)
;
commitData
.
setText
(
)
;
commitData
.
setLength
(
bytesWritten
)
;
commitData
.
bindCommitData
(
parts
)
;
byte
[
]
bytes
=
commitData
.
toBytes
(
)
;
LOG
.
info
(
+
,
path
.
toUri
(
)
,
parts
.
size
(
)
,
pendingPartKey
,
bytesWritten
)
;
@
Override
public
void
commitTask
(
TaskAttemptContext
context
)
throws
IOException
{
try
(
DurationInfo
d
=
new
DurationInfo
(
LOG
,
,
context
.
getTaskAttemptID
(
)
)
)
{
PendingSet
commits
=
innerCommitTask
(
context
)
;
Pair
<
PendingSet
,
List
<
Pair
<
LocatedFileStatus
,
IOException
>>>
loaded
=
actions
.
loadSinglePendingCommits
(
taskAttemptPath
,
true
)
;
PendingSet
pendingSet
=
loaded
.
getKey
(
)
;
List
<
Pair
<
LocatedFileStatus
,
IOException
>>
failures
=
loaded
.
getValue
(
)
;
if
(
!
failures
.
isEmpty
(
)
)
{
LOG
.
error
(
)
;
abortPendingUploads
(
context
,
pendingSet
.
getCommits
(
)
,
true
)
;
throw
failures
.
get
(
0
)
.
getValue
(
)
;
}
String
jobId
=
String
.
valueOf
(
context
.
getJobID
(
)
)
;
String
taskId
=
String
.
valueOf
(
context
.
getTaskAttemptID
(
)
)
;
for
(
SinglePendingCommit
commit
:
pendingSet
.
getCommits
(
)
)
{
commit
.
setJobId
(
jobId
)
;
commit
.
setTaskId
(
taskId
)
;
}
Path
jobAttemptPath
=
getJobAttemptPath
(
context
)
;
TaskAttemptID
taskAttemptID
=
context
.
getTaskAttemptID
(
)
;
Path
taskOutcomePath
=
new
Path
(
jobAttemptPath
,
taskAttemptID
.
getTaskID
(
)
.
toString
(
)
+
CommitConstants
.
PENDINGSET_SUFFIX
)
;
@
Override
public
void
setupJob
(
JobContext
context
)
throws
IOException
{
Path
outputPath
=
getOutputPath
(
)
;
FileSystem
fs
=
getDestFS
(
)
;
ConflictResolution
conflictResolution
=
getConflictResolutionMode
(
context
,
fs
.
getConf
(
)
)
;
@
Override
public
void
preCommitJob
(
final
JobContext
context
,
final
ActiveCommit
pending
)
throws
IOException
{
super
.
preCommitJob
(
context
,
pending
)
;
Path
outputPath
=
getOutputPath
(
)
;
FileSystem
fs
=
getDestFS
(
)
;
Configuration
fsConf
=
fs
.
getConf
(
)
;
switch
(
getConflictResolutionMode
(
context
,
fsConf
)
)
{
case
FAIL
:
break
;
case
APPEND
:
break
;
case
REPLACE
:
if
(
fs
.
delete
(
outputPath
,
true
)
)
{
Map
<
Path
,
String
>
partitions
=
new
ConcurrentHashMap
<
>
(
)
;
FileSystem
sourceFS
=
pending
.
getSourceFS
(
)
;
Tasks
.
Submitter
submitter
=
buildSubmitter
(
context
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
)
)
{
Tasks
.
foreach
(
pending
.
getSourceFiles
(
)
)
.
stopOnFailure
(
)
.
suppressExceptions
(
false
)
.
executeWith
(
submitter
)
.
run
(
path
->
{
PendingSet
pendingSet
=
PendingSet
.
load
(
sourceFS
,
path
)
;
Path
lastParent
=
null
;
for
(
SinglePendingCommit
commit
:
pendingSet
.
getCommits
(
)
)
{
Path
parent
=
commit
.
destinationPath
(
)
.
getParent
(
)
;
if
(
parent
!=
null
&&
!
parent
.
equals
(
lastParent
)
)
{
partitions
.
put
(
parent
,
)
;
lastParent
=
parent
;
}
}
}
)
;
}
FileSystem
fs
=
getDestFS
(
)
;
Tasks
.
foreach
(
partitions
.
keySet
(
)
)
.
stopOnFailure
(
)
.
suppressExceptions
(
false
)
.
executeWith
(
submitter
)
.
run
(
partitionPath
->
{
protected
List
<
LocatedFileStatus
>
getTaskOutput
(
TaskAttemptContext
context
)
throws
IOException
{
Path
attemptPath
=
getTaskAttemptPath
(
context
)
;
Preconditions
.
checkNotNull
(
attemptPath
,
,
this
)
;
@
Override
public
void
setupJob
(
JobContext
context
)
throws
IOException
{
@
Override
public
void
cleanupStagingDirs
(
)
{
Path
workPath
=
getWorkPath
(
)
;
if
(
workPath
!=
null
)
{
@
Override
public
boolean
needsTaskCommit
(
TaskAttemptContext
context
)
throws
IOException
{
try
(
DurationInfo
d
=
new
DurationInfo
(
LOG
,
,
getRole
(
)
,
context
.
getTaskAttemptID
(
)
)
)
{
Path
attemptPath
=
getTaskAttemptPath
(
context
)
;
FileSystem
fs
=
getTaskAttemptFilesystem
(
context
)
;
FileStatus
[
]
stats
=
fs
.
listStatus
(
attemptPath
)
;
@
Override
public
void
commitTask
(
TaskAttemptContext
context
)
throws
IOException
{
try
(
DurationInfo
d
=
new
DurationInfo
(
LOG
,
,
getRole
(
)
,
context
.
getTaskAttemptID
(
)
)
)
{
int
count
=
commitTaskInternal
(
context
,
getTaskOutput
(
context
)
)
;
protected
int
commitTaskInternal
(
final
TaskAttemptContext
context
,
List
<
?
extends
FileStatus
>
taskOutput
)
throws
IOException
{
LOG
.
debug
(
,
getRole
(
)
)
;
Configuration
conf
=
context
.
getConfiguration
(
)
;
final
Path
attemptPath
=
getTaskAttemptPath
(
context
)
;
FileSystem
attemptFS
=
getTaskAttemptFilesystem
(
context
)
;
protected
int
commitTaskInternal
(
final
TaskAttemptContext
context
,
List
<
?
extends
FileStatus
>
taskOutput
)
throws
IOException
{
LOG
.
debug
(
,
getRole
(
)
)
;
Configuration
conf
=
context
.
getConfiguration
(
)
;
final
Path
attemptPath
=
getTaskAttemptPath
(
context
)
;
FileSystem
attemptFS
=
getTaskAttemptFilesystem
(
context
)
;
LOG
.
debug
(
,
getRole
(
)
,
attemptPath
)
;
Path
commitsAttemptPath
=
wrappedCommitter
.
getTaskAttemptPath
(
context
)
;
FileSystem
commitsFS
=
commitsAttemptPath
.
getFileSystem
(
conf
)
;
int
commitCount
=
taskOutput
.
size
(
)
;
final
Queue
<
SinglePendingCommit
>
commits
=
new
ConcurrentLinkedQueue
<
>
(
)
;
protected
int
commitTaskInternal
(
final
TaskAttemptContext
context
,
List
<
?
extends
FileStatus
>
taskOutput
)
throws
IOException
{
LOG
.
debug
(
,
getRole
(
)
)
;
Configuration
conf
=
context
.
getConfiguration
(
)
;
final
Path
attemptPath
=
getTaskAttemptPath
(
context
)
;
FileSystem
attemptFS
=
getTaskAttemptFilesystem
(
context
)
;
LOG
.
debug
(
,
getRole
(
)
,
attemptPath
)
;
Path
commitsAttemptPath
=
wrappedCommitter
.
getTaskAttemptPath
(
context
)
;
FileSystem
commitsFS
=
commitsAttemptPath
.
getFileSystem
(
conf
)
;
int
commitCount
=
taskOutput
.
size
(
)
;
final
Queue
<
SinglePendingCommit
>
commits
=
new
ConcurrentLinkedQueue
<
>
(
)
;
LOG
.
info
(
,
getRole
(
)
,
attemptPath
)
;
if
(
taskOutput
.
isEmpty
(
)
)
{
LOG
.
warn
(
,
getRole
(
)
)
;
}
else
{
boolean
threw
=
true
;
context
.
progress
(
)
;
PendingSet
pendingCommits
=
new
PendingSet
(
commitCount
)
;
try
{
Tasks
.
foreach
(
taskOutput
)
.
stopOnFailure
(
)
.
suppressExceptions
(
false
)
.
executeWith
(
buildSubmitter
(
context
)
)
.
run
(
stat
->
{
Path
path
=
stat
.
getPath
(
)
;
File
localFile
=
new
File
(
path
.
toUri
(
)
.
getPath
(
)
)
;
String
relative
=
Paths
.
getRelativePath
(
attemptPath
,
path
)
;
String
partition
=
Paths
.
getPartition
(
relative
)
;
String
key
=
getFinalKey
(
relative
,
context
)
;
Path
destPath
=
getDestS3AFS
(
)
.
keyToQualifiedPath
(
key
)
;
SinglePendingCommit
commit
=
getCommitOperations
(
)
.
uploadFileToPendingCommit
(
localFile
,
destPath
,
partition
,
uploadPartSize
,
context
)
;
PendingSet
pendingCommits
=
new
PendingSet
(
commitCount
)
;
try
{
Tasks
.
foreach
(
taskOutput
)
.
stopOnFailure
(
)
.
suppressExceptions
(
false
)
.
executeWith
(
buildSubmitter
(
context
)
)
.
run
(
stat
->
{
Path
path
=
stat
.
getPath
(
)
;
File
localFile
=
new
File
(
path
.
toUri
(
)
.
getPath
(
)
)
;
String
relative
=
Paths
.
getRelativePath
(
attemptPath
,
path
)
;
String
partition
=
Paths
.
getPartition
(
relative
)
;
String
key
=
getFinalKey
(
relative
,
context
)
;
Path
destPath
=
getDestS3AFS
(
)
.
keyToQualifiedPath
(
key
)
;
SinglePendingCommit
commit
=
getCommitOperations
(
)
.
uploadFileToPendingCommit
(
localFile
,
destPath
,
partition
,
uploadPartSize
,
context
)
;
LOG
.
debug
(
,
getRole
(
)
,
commit
)
;
commits
.
add
(
commit
)
;
}
)
;
for
(
SinglePendingCommit
commit
:
commits
)
{
pendingCommits
.
add
(
commit
)
;
File
localFile
=
new
File
(
path
.
toUri
(
)
.
getPath
(
)
)
;
String
relative
=
Paths
.
getRelativePath
(
attemptPath
,
path
)
;
String
partition
=
Paths
.
getPartition
(
relative
)
;
String
key
=
getFinalKey
(
relative
,
context
)
;
Path
destPath
=
getDestS3AFS
(
)
.
keyToQualifiedPath
(
key
)
;
SinglePendingCommit
commit
=
getCommitOperations
(
)
.
uploadFileToPendingCommit
(
localFile
,
destPath
,
partition
,
uploadPartSize
,
context
)
;
LOG
.
debug
(
,
getRole
(
)
,
commit
)
;
commits
.
add
(
commit
)
;
}
)
;
for
(
SinglePendingCommit
commit
:
commits
)
{
pendingCommits
.
add
(
commit
)
;
}
LOG
.
debug
(
,
pendingCommits
.
size
(
)
,
commitsAttemptPath
)
;
pendingCommits
.
save
(
commitsFS
,
commitsAttemptPath
,
false
)
;
threw
=
false
;
}
finally
{
protected
PathExistsException
failDestinationExists
(
final
Path
path
,
final
String
description
)
{
protected
PathExistsException
failDestinationExists
(
final
Path
path
,
final
String
description
)
{
LOG
.
error
(
+
,
description
,
getJobContext
(
)
.
getJobID
(
)
,
path
)
;
try
{
int
limit
=
10
;
RemoteIterator
<
LocatedFileStatus
>
lf
=
getDestFS
(
)
.
listFiles
(
path
,
true
)
;
LOG
.
info
(
)
;
while
(
limit
>
0
&&
lf
.
hasNext
(
)
)
{
limit
--
;
LocatedFileStatus
status
=
lf
.
next
(
)
;
public
void
bulkDeleteRetried
(
DeleteObjectsRequest
deleteRequest
,
Exception
ex
)
{
private
void
onDeleteThrottled
(
final
DeleteObjectsRequest
deleteRequest
)
{
final
List
<
DeleteObjectsRequest
.
KeyVersion
>
keys
=
deleteRequest
.
getKeys
(
)
;
final
int
size
=
keys
.
size
(
)
;
incrementStatistic
(
STORE_IO_THROTTLED
,
size
)
;
instrumentation
.
addValueToQuantiles
(
STORE_IO_THROTTLE_RATE
,
size
)
;
public
void
processResponse
(
final
CopyResult
copyResult
)
throws
PathIOException
{
String
newRevisionId
=
policy
.
getRevisionId
(
copyResult
)
;
@
Retries
.
RetryTranslated
public
Boolean
execute
(
)
throws
IOException
{
executeOnlyOnce
(
)
;
StoreContext
context
=
getStoreContext
(
)
;
Path
path
=
status
.
getPath
(
)
;
@
Retries
.
RetryTranslated
public
Boolean
execute
(
)
throws
IOException
{
executeOnlyOnce
(
)
;
StoreContext
context
=
getStoreContext
(
)
;
Path
path
=
status
.
getPath
(
)
;
LOG
.
debug
(
,
path
,
recursive
)
;
LOG
.
debug
(
,
status
.
isFile
(
)
?
:
(
status
.
isEmptyDirectory
(
)
==
Tristate
.
TRUE
?
:
)
)
;
String
key
=
context
.
pathToKey
(
path
)
;
if
(
status
.
isDirectory
(
)
)
{
Path
path
=
status
.
getPath
(
)
;
LOG
.
debug
(
,
path
,
recursive
)
;
LOG
.
debug
(
,
status
.
isFile
(
)
?
:
(
status
.
isEmptyDirectory
(
)
==
Tristate
.
TRUE
?
:
)
)
;
String
key
=
context
.
pathToKey
(
path
)
;
if
(
status
.
isDirectory
(
)
)
{
LOG
.
debug
(
,
path
)
;
checkArgument
(
status
.
isEmptyDirectory
(
)
!=
Tristate
.
UNKNOWN
,
)
;
if
(
!
key
.
endsWith
(
)
)
{
key
=
key
+
;
}
if
(
.
equals
(
key
)
)
{
LOG
.
error
(
+
,
status
.
getPath
(
)
,
recursive
)
;
return
false
;
}
if
(
!
recursive
&&
status
.
isEmptyDirectory
(
)
==
Tristate
.
FALSE
)
{
throw
new
PathIsNotEmptyDirectoryException
(
path
.
toString
(
)
)
;
}
if
(
status
.
isEmptyDirectory
(
)
==
Tristate
.
TRUE
)
{
LOG
.
debug
(
,
path
)
;
checkArgument
(
status
.
isEmptyDirectory
(
)
!=
Tristate
.
UNKNOWN
,
)
;
if
(
!
key
.
endsWith
(
)
)
{
key
=
key
+
;
}
if
(
.
equals
(
key
)
)
{
LOG
.
error
(
+
,
status
.
getPath
(
)
,
recursive
)
;
return
false
;
}
if
(
!
recursive
&&
status
.
isEmptyDirectory
(
)
==
Tristate
.
FALSE
)
{
throw
new
PathIsNotEmptyDirectoryException
(
path
.
toString
(
)
)
;
}
if
(
status
.
isEmptyDirectory
(
)
==
Tristate
.
TRUE
)
{
LOG
.
debug
(
,
path
)
;
deleteObjectAtPath
(
path
,
key
,
false
)
;
}
else
{
deleteDirectoryTree
(
path
,
key
)
;
}
}
else
{
@
Retries
.
RetryTranslated
protected
void
deleteDirectoryTree
(
final
Path
path
,
final
String
dirKey
)
throws
IOException
{
operationState
=
S3Guard
.
initiateBulkWrite
(
metadataStore
,
BulkOperationState
.
OperationType
.
Delete
,
path
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
false
,
,
dirKey
)
)
{
resetDeleteList
(
)
;
deleteFuture
=
null
;
LOG
.
debug
(
,
dirKey
)
;
final
RemoteIterator
<
S3ALocatedFileStatus
>
locatedFiles
=
callbacks
.
listFilesAndEmptyDirectories
(
path
,
status
,
false
,
true
)
;
while
(
locatedFiles
.
hasNext
(
)
)
{
S3AFileStatus
child
=
locatedFiles
.
next
(
)
.
toS3AFileStatus
(
)
;
queueForDeletion
(
child
)
;
}
LOG
.
debug
(
)
;
submitNextBatch
(
)
;
maybeAwaitCompletion
(
deleteFuture
)
;
if
(
callbacks
.
allowAuthoritative
(
path
)
)
{
LOG
.
debug
(
+
)
;
final
RemoteIterator
<
S3AFileStatus
>
objects
=
callbacks
.
listObjects
(
path
,
dirKey
)
;
while
(
objects
.
hasNext
(
)
)
{
extraFilesDeleted
++
;
queueForDeletion
(
deletionKey
(
objects
.
next
(
)
)
,
null
)
;
}
if
(
extraFilesDeleted
>
0
)
{
maybeAwaitCompletion
(
deleteFuture
)
;
if
(
callbacks
.
allowAuthoritative
(
path
)
)
{
LOG
.
debug
(
+
)
;
final
RemoteIterator
<
S3AFileStatus
>
objects
=
callbacks
.
listObjects
(
path
,
dirKey
)
;
while
(
objects
.
hasNext
(
)
)
{
extraFilesDeleted
++
;
queueForDeletion
(
deletionKey
(
objects
.
next
(
)
)
,
null
)
;
}
if
(
extraFilesDeleted
>
0
)
{
LOG
.
debug
(
,
extraFilesDeleted
)
;
submitNextBatch
(
)
;
maybeAwaitCompletion
(
deleteFuture
)
;
}
}
try
(
DurationInfo
ignored2
=
new
DurationInfo
(
LOG
,
false
,
)
)
{
metadataStore
.
deleteSubtree
(
path
,
operationState
)
;
}
}
finally
{
IOUtils
.
cleanupWithLogger
(
LOG
,
operationState
)
;
private
void
queueForDeletion
(
final
String
key
,
@
Nullable
final
Path
deletePath
)
throws
IOException
{
@
Retries
.
RetryTranslated
private
void
deleteObjectAtPath
(
final
Path
path
,
final
String
key
,
final
boolean
isFile
)
throws
IOException
{
@
Retries
.
RetryTranslated
private
void
asyncDeleteAction
(
final
BulkOperationState
state
,
final
List
<
DeleteObjectsRequest
.
KeyVersion
>
keyList
,
final
List
<
Path
>
pathList
,
final
boolean
auditDeletedKeys
)
throws
IOException
{
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
false
,
)
)
{
DeleteObjectsResult
result
=
null
;
List
<
Path
>
undeletedObjects
=
new
ArrayList
<
>
(
)
;
if
(
!
keyList
.
isEmpty
(
)
)
{
result
=
Invoker
.
once
(
,
status
.
getPath
(
)
.
toString
(
)
,
(
)
->
callbacks
.
removeKeys
(
keyList
,
false
,
undeletedObjects
,
state
,
!
auditDeletedKeys
)
)
;
}
if
(
!
pathList
.
isEmpty
(
)
)
{
metadataStore
.
deletePaths
(
pathList
,
state
)
;
}
if
(
auditDeletedKeys
&&
result
!=
null
)
{
List
<
DeleteObjectsResult
.
DeletedObject
>
deletedObjects
=
result
.
getDeletedObjects
(
)
;
if
(
deletedObjects
.
size
(
)
!=
keyList
.
size
(
)
)
{
LOG
.
warn
(
+
+
,
keyList
.
size
(
)
,
deletedObjects
.
size
(
)
)
;
for
(
DeleteObjectsResult
.
DeletedObject
del
:
deletedObjects
)
{
keyList
.
removeIf
(
kv
->
kv
.
getKey
(
)
.
equals
(
del
.
getKey
(
)
)
)
;
}
for
(
DeleteObjectsRequest
.
KeyVersion
kv
:
keyList
)
{
public
List
<
Path
>
removeAllowedMarkers
(
DirectoryPolicy
policy
)
{
List
<
Path
>
removed
=
new
ArrayList
<
>
(
)
;
Iterator
<
Map
.
Entry
<
Path
,
Marker
>>
entries
=
surplusMarkers
.
entrySet
(
)
.
iterator
(
)
;
while
(
entries
.
hasNext
(
)
)
{
Map
.
Entry
<
Path
,
Marker
>
entry
=
entries
.
next
(
)
;
Path
path
=
entry
.
getKey
(
)
;
if
(
policy
.
keepDirectoryMarkers
(
path
)
)
{
entries
.
remove
(
)
;
public
Pair
<
List
<
Path
>
,
List
<
Path
>>
splitUndeletedKeys
(
final
MultiObjectDeleteException
deleteException
,
final
Collection
<
DeleteObjectsRequest
.
KeyVersion
>
keysToDelete
)
{
@
Retries
.
OnceTranslated
private
void
completeActiveCopies
(
String
reason
)
throws
IOException
{
private
void
queueToDelete
(
Path
path
,
String
key
,
String
version
)
{
private
OperationDuration
copyEmptyDirectoryMarkers
(
final
String
srcKey
,
final
String
dstKey
,
final
DirMarkerTracker
dirMarkerTracker
)
throws
IOException
{
private
OperationDuration
copyEmptyDirectoryMarkers
(
final
String
srcKey
,
final
String
dstKey
,
final
DirMarkerTracker
dirMarkerTracker
)
throws
IOException
{
LOG
.
debug
(
,
dirMarkerTracker
)
;
final
StoreContext
storeContext
=
getStoreContext
(
)
;
Map
<
Path
,
DirMarkerTracker
.
Marker
>
leafMarkers
=
dirMarkerTracker
.
getLeafMarkers
(
)
;
Map
<
Path
,
DirMarkerTracker
.
Marker
>
surplus
=
dirMarkerTracker
.
getSurplusMarkers
(
)
;
DurationInfo
duration
=
new
DurationInfo
(
LOG
,
false
,
,
leafMarkers
.
size
(
)
,
surplus
.
size
(
)
)
;
for
(
DirMarkerTracker
.
Marker
entry
:
leafMarkers
.
values
(
)
)
{
Path
source
=
entry
.
getPath
(
)
;
String
key
=
entry
.
getKey
(
)
;
String
newDestKey
=
dstKey
+
key
.
substring
(
srcKey
.
length
(
)
)
;
Path
childDestPath
=
storeContext
.
keyToPath
(
newDestKey
)
;
@
Retries
.
RetryTranslated
private
void
removeSourceObjects
(
final
List
<
DeleteObjectsRequest
.
KeyVersion
>
keys
,
final
List
<
Path
>
paths
)
throws
IOException
{
List
<
Path
>
undeletedObjects
=
new
ArrayList
<
>
(
)
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Retries
.
RetryTranslated
private
void
removeSourceObjects
(
final
List
<
DeleteObjectsRequest
.
KeyVersion
>
keys
,
final
List
<
Path
>
paths
)
throws
IOException
{
List
<
Path
>
undeletedObjects
=
new
ArrayList
<
>
(
)
;
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
keys
.
size
(
)
)
;
for
(
DeleteObjectsRequest
.
KeyVersion
key
:
keys
)
{
int
nonauth
=
0
;
final
Queue
<
DDBPathMetadata
>
queue
=
new
ArrayDeque
<
>
(
)
;
final
boolean
isRoot
=
path
.
isRoot
(
)
;
final
DDBPathMetadata
baseData
=
metastore
.
get
(
path
)
;
if
(
baseData
==
null
)
{
throw
new
ExitUtil
.
ExitException
(
LauncherExitCodes
.
EXIT_NOT_FOUND
,
+
path
)
;
}
if
(
isRoot
||
isDirectory
(
baseData
)
)
{
queue
.
add
(
baseData
)
;
}
else
{
LOG
.
info
(
)
;
return
Pair
.
of
(
0
,
0
)
;
}
while
(
!
queue
.
isEmpty
(
)
)
{
dirs
++
;
final
DDBPathMetadata
dir
=
queue
.
poll
(
)
;
final
Path
p
=
dir
.
getFileStatus
(
)
.
getPath
(
)
;
if
(
isRoot
||
isDirectory
(
baseData
)
)
{
queue
.
add
(
baseData
)
;
}
else
{
LOG
.
info
(
)
;
return
Pair
.
of
(
0
,
0
)
;
}
while
(
!
queue
.
isEmpty
(
)
)
{
dirs
++
;
final
DDBPathMetadata
dir
=
queue
.
poll
(
)
;
final
Path
p
=
dir
.
getFileStatus
(
)
.
getPath
(
)
;
LOG
.
debug
(
,
dir
.
prettyPrint
(
)
)
;
if
(
!
p
.
isRoot
(
)
)
{
if
(
!
dir
.
isAuthoritativeDir
(
)
)
{
LOG
.
warn
(
,
p
)
;
nonauth
++
;
verifyAuthDir
(
dir
,
requireAuth
)
;
}
else
{
LOG
.
info
(
)
;
return
Pair
.
of
(
0
,
0
)
;
}
while
(
!
queue
.
isEmpty
(
)
)
{
dirs
++
;
final
DDBPathMetadata
dir
=
queue
.
poll
(
)
;
final
Path
p
=
dir
.
getFileStatus
(
)
.
getPath
(
)
;
LOG
.
debug
(
,
dir
.
prettyPrint
(
)
)
;
if
(
!
p
.
isRoot
(
)
)
{
if
(
!
dir
.
isAuthoritativeDir
(
)
)
{
LOG
.
warn
(
,
p
)
;
nonauth
++
;
verifyAuthDir
(
dir
,
requireAuth
)
;
}
else
{
LOG
.
info
(
,
p
)
;
nonauth
++
;
verifyAuthDir
(
dir
,
requireAuth
)
;
}
else
{
LOG
.
info
(
,
p
)
;
}
}
else
{
LOG
.
info
(
,
p
)
;
}
if
(
recursive
)
{
final
DirListingMetadata
entry
=
metastore
.
listChildren
(
p
)
;
if
(
entry
!=
null
)
{
final
Collection
<
PathMetadata
>
listing
=
entry
.
getListing
(
)
;
int
files
=
0
,
subdirs
=
0
;
for
(
PathMetadata
e
:
listing
)
{
if
(
isDirectory
(
e
)
)
{
queue
.
add
(
(
DDBPathMetadata
)
e
)
;
subdirs
++
;
protected
long
dumpRawS3ObjectStore
(
final
CsvFile
csv
)
throws
IOException
{
S3AFileSystem
fs
=
getFilesystem
(
)
;
Path
rootPath
=
fs
.
qualify
(
new
Path
(
)
)
;
Listing
listing
=
fs
.
getListing
(
)
;
S3ListRequest
request
=
listing
.
createListObjectsRequest
(
,
null
)
;
long
count
=
0
;
RemoteIterator
<
S3AFileStatus
>
st
=
listing
.
createFileStatusListingIterator
(
rootPath
,
request
,
ACCEPT_ALL
,
new
Listing
.
AcceptAllButSelfAndS3nDirs
(
rootPath
)
)
;
while
(
st
.
hasNext
(
)
)
{
count
++
;
S3AFileStatus
next
=
st
.
next
(
)
;
private
void
dumpEntry
(
CsvFile
csv
,
DDBPathMetadata
md
)
{
public
static
DumpS3GuardDynamoTable
dumpStore
(
@
Nullable
final
S3AFileSystem
fs
,
@
Nullable
DynamoDBMetadataStore
store
,
@
Nullable
Configuration
conf
,
final
File
destFile
,
@
Nullable
URI
uri
)
throws
ExitUtil
.
ExitException
{
ServiceLauncher
<
Service
>
serviceLauncher
=
new
ServiceLauncher
<
>
(
NAME
)
;
if
(
conf
==
null
)
{
conf
=
checkNotNull
(
fs
,
)
.
getConf
(
)
;
}
if
(
store
==
null
)
{
store
=
(
DynamoDBMetadataStore
)
checkNotNull
(
fs
,
)
.
getMetadataStore
(
)
;
}
DumpS3GuardDynamoTable
dump
=
new
DumpS3GuardDynamoTable
(
fs
,
store
,
destFile
,
uri
)
;
ExitUtil
.
ExitException
ex
=
serviceLauncher
.
launchService
(
conf
,
dump
,
Collections
.
emptyList
(
)
,
false
,
true
)
;
if
(
ex
!=
null
&&
ex
.
getExitCode
(
)
!=
0
)
{
throw
ex
;
}
LOG
.
info
(
)
;
Pair
<
Long
,
Long
>
r
=
dump
.
getScanEntryResult
(
)
;
LOG
.
info
(
,
r
)
;
LOG
.
info
(
,
r
.
getLeft
(
)
+
r
.
getRight
(
)
,
r
.
getLeft
(
)
,
r
.
getRight
(
)
)
;
LOG
.
info
(
,
dump
.
getRawObjectStoreCount
(
)
)
;
ServiceLauncher
<
Service
>
serviceLauncher
=
new
ServiceLauncher
<
>
(
NAME
)
;
if
(
conf
==
null
)
{
conf
=
checkNotNull
(
fs
,
)
.
getConf
(
)
;
}
if
(
store
==
null
)
{
store
=
(
DynamoDBMetadataStore
)
checkNotNull
(
fs
,
)
.
getMetadataStore
(
)
;
}
DumpS3GuardDynamoTable
dump
=
new
DumpS3GuardDynamoTable
(
fs
,
store
,
destFile
,
uri
)
;
ExitUtil
.
ExitException
ex
=
serviceLauncher
.
launchService
(
conf
,
dump
,
Collections
.
emptyList
(
)
,
false
,
true
)
;
if
(
ex
!=
null
&&
ex
.
getExitCode
(
)
!=
0
)
{
throw
ex
;
}
LOG
.
info
(
)
;
Pair
<
Long
,
Long
>
r
=
dump
.
getScanEntryResult
(
)
;
LOG
.
info
(
,
r
)
;
LOG
.
info
(
,
r
.
getLeft
(
)
+
r
.
getRight
(
)
,
r
.
getLeft
(
)
,
r
.
getRight
(
)
)
;
LOG
.
info
(
,
dump
.
getRawObjectStoreCount
(
)
)
;
LOG
.
info
(
,
dump
.
getTreewalkCount
(
)
)
;
}
if
(
store
==
null
)
{
store
=
(
DynamoDBMetadataStore
)
checkNotNull
(
fs
,
)
.
getMetadataStore
(
)
;
}
DumpS3GuardDynamoTable
dump
=
new
DumpS3GuardDynamoTable
(
fs
,
store
,
destFile
,
uri
)
;
ExitUtil
.
ExitException
ex
=
serviceLauncher
.
launchService
(
conf
,
dump
,
Collections
.
emptyList
(
)
,
false
,
true
)
;
if
(
ex
!=
null
&&
ex
.
getExitCode
(
)
!=
0
)
{
throw
ex
;
}
LOG
.
info
(
)
;
Pair
<
Long
,
Long
>
r
=
dump
.
getScanEntryResult
(
)
;
LOG
.
info
(
,
r
)
;
LOG
.
info
(
,
r
.
getLeft
(
)
+
r
.
getRight
(
)
,
r
.
getLeft
(
)
,
r
.
getRight
(
)
)
;
LOG
.
info
(
,
dump
.
getRawObjectStoreCount
(
)
)
;
LOG
.
info
(
,
dump
.
getTreewalkCount
(
)
)
;
LOG
.
info
(
,
dump
.
getListStatusCount
(
)
)
;
r
=
dump
.
getSecondScanResult
(
)
;
if
(
r
!=
null
)
{
private
DynamoDB
createDynamoDB
(
final
Configuration
conf
,
final
String
s3Region
,
final
String
bucket
,
final
AWSCredentialsProvider
credentials
)
throws
IOException
{
if
(
amazonDynamoDB
==
null
)
{
Preconditions
.
checkNotNull
(
conf
)
;
final
Class
<
?
extends
DynamoDBClientFactory
>
cls
=
conf
.
getClass
(
S3GUARD_DDB_CLIENT_FACTORY_IMPL
,
S3GUARD_DDB_CLIENT_FACTORY_IMPL_DEFAULT
,
DynamoDBClientFactory
.
class
)
;
@
Override
@
Retries
.
OnceRaw
public
void
initialize
(
FileSystem
fs
,
ITtlTimeProvider
ttlTp
)
throws
IOException
{
Preconditions
.
checkNotNull
(
fs
,
)
;
Preconditions
.
checkArgument
(
fs
instanceof
S3AFileSystem
,
,
fs
)
;
bindToOwnerFilesystem
(
(
S3AFileSystem
)
fs
)
;
final
String
bucket
=
owner
.
getBucket
(
)
;
String
confRegion
=
conf
.
getTrimmed
(
S3GUARD_DDB_REGION_KEY
)
;
if
(
!
StringUtils
.
isEmpty
(
confRegion
)
)
{
region
=
confRegion
;
@
Override
@
Retries
.
OnceRaw
public
void
initialize
(
FileSystem
fs
,
ITtlTimeProvider
ttlTp
)
throws
IOException
{
Preconditions
.
checkNotNull
(
fs
,
)
;
Preconditions
.
checkArgument
(
fs
instanceof
S3AFileSystem
,
,
fs
)
;
bindToOwnerFilesystem
(
(
S3AFileSystem
)
fs
)
;
final
String
bucket
=
owner
.
getBucket
(
)
;
String
confRegion
=
conf
.
getTrimmed
(
S3GUARD_DDB_REGION_KEY
)
;
if
(
!
StringUtils
.
isEmpty
(
confRegion
)
)
{
region
=
confRegion
;
LOG
.
debug
(
,
region
)
;
}
else
{
try
{
region
=
owner
.
getBucketLocation
(
)
;
}
catch
(
AccessDeniedException
e
)
{
URI
uri
=
owner
.
getUri
(
)
;
String
message
=
+
RolePolicies
.
S3_GET_BUCKET_LOCATION
+
+
uri
;
@
Override
@
Retries
.
RetryTranslated
public
void
forgetMetadata
(
Path
path
)
throws
IOException
{
@
Retries
.
RetryTranslated
private
void
innerDelete
(
final
Path
path
,
final
boolean
tombstone
,
final
AncestorState
ancestorState
)
throws
IOException
{
checkPath
(
path
)
;
@
Override
@
Retries
.
RetryTranslated
public
void
deleteSubtree
(
Path
path
,
final
BulkOperationState
operationState
)
throws
IOException
{
checkPath
(
path
)
;
@
Override
@
Retries
.
RetryTranslated
public
void
deleteSubtree
(
Path
path
,
final
BulkOperationState
operationState
)
throws
IOException
{
checkPath
(
path
)
;
LOG
.
debug
(
,
tableName
,
region
,
path
)
;
final
PathMetadata
meta
=
get
(
path
)
;
if
(
meta
==
null
)
{
@
Override
@
Retries
.
RetryTranslated
public
DDBPathMetadata
get
(
Path
path
,
boolean
wantEmptyDirectoryFlag
)
throws
IOException
{
checkPath
(
path
)
;
@
Override
@
Retries
.
RetryTranslated
public
DDBPathMetadata
get
(
Path
path
,
boolean
wantEmptyDirectoryFlag
)
throws
IOException
{
checkPath
(
path
)
;
LOG
.
debug
(
,
tableName
,
region
,
path
,
wantEmptyDirectoryFlag
)
;
DDBPathMetadata
result
=
innerGet
(
path
,
wantEmptyDirectoryFlag
)
;
final
DDBPathMetadata
meta
;
if
(
path
.
isRoot
(
)
)
{
meta
=
new
DDBPathMetadata
(
makeDirStatus
(
username
,
path
)
)
;
}
else
{
final
Item
item
=
getConsistentItem
(
path
)
;
meta
=
itemToPathMetadata
(
item
,
username
)
;
LOG
.
debug
(
,
tableName
,
region
,
path
,
meta
)
;
}
if
(
wantEmptyDirectoryFlag
&&
meta
!=
null
&&
!
meta
.
isDeleted
(
)
)
{
final
FileStatus
status
=
meta
.
getFileStatus
(
)
;
if
(
status
.
isDirectory
(
)
)
{
final
QuerySpec
spec
=
new
QuerySpec
(
)
.
withHashKey
(
pathToParentKeyAttribute
(
path
)
)
.
withConsistentRead
(
true
)
.
withFilterExpression
(
IS_DELETED
+
)
.
withValueMap
(
DELETE_TRACKING_VALUE_MAP
)
;
boolean
hasChildren
=
readOp
.
retry
(
,
path
.
toString
(
)
,
true
,
(
)
->
{
final
IteratorSupport
<
Item
,
QueryOutcome
>
it
=
table
.
query
(
spec
)
.
iterator
(
)
;
if
(
it
.
hasNext
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
meta
=
new
DDBPathMetadata
(
makeDirStatus
(
username
,
path
)
)
;
}
else
{
final
Item
item
=
getConsistentItem
(
path
)
;
meta
=
itemToPathMetadata
(
item
,
username
)
;
LOG
.
debug
(
,
tableName
,
region
,
path
,
meta
)
;
}
if
(
wantEmptyDirectoryFlag
&&
meta
!=
null
&&
!
meta
.
isDeleted
(
)
)
{
final
FileStatus
status
=
meta
.
getFileStatus
(
)
;
if
(
status
.
isDirectory
(
)
)
{
final
QuerySpec
spec
=
new
QuerySpec
(
)
.
withHashKey
(
pathToParentKeyAttribute
(
path
)
)
.
withConsistentRead
(
true
)
.
withFilterExpression
(
IS_DELETED
+
)
.
withValueMap
(
DELETE_TRACKING_VALUE_MAP
)
;
boolean
hasChildren
=
readOp
.
retry
(
,
path
.
toString
(
)
,
true
,
(
)
->
{
final
IteratorSupport
<
Item
,
QueryOutcome
>
it
=
table
.
query
(
spec
)
.
iterator
(
)
;
if
(
it
.
hasNext
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
status
.
getPath
(
)
)
;
while
(
it
.
hasNext
(
)
)
{
@
Override
@
Retries
.
RetryTranslated
public
DirListingMetadata
listChildren
(
final
Path
path
)
throws
IOException
{
checkPath
(
path
)
;
private
Collection
<
DDBPathMetadata
>
completeAncestry
(
final
Collection
<
DDBPathMetadata
>
pathsToCreate
,
final
AncestorState
ancestorState
)
throws
IOException
{
Map
<
Path
,
Pair
<
EntryOrigin
,
DDBPathMetadata
>>
ancestry
=
new
HashMap
<
>
(
)
;
private
Collection
<
DDBPathMetadata
>
completeAncestry
(
final
Collection
<
DDBPathMetadata
>
pathsToCreate
,
final
AncestorState
ancestorState
)
throws
IOException
{
Map
<
Path
,
Pair
<
EntryOrigin
,
DDBPathMetadata
>>
ancestry
=
new
HashMap
<
>
(
)
;
LOG
.
debug
(
,
pathsToCreate
.
size
(
)
)
;
List
<
DDBPathMetadata
>
sortedPaths
=
new
ArrayList
<
>
(
pathsToCreate
)
;
sortedPaths
.
sort
(
PathOrderComparators
.
TOPMOST_PM_FIRST
)
;
for
(
DDBPathMetadata
entry
:
sortedPaths
)
{
Preconditions
.
checkArgument
(
entry
!=
null
)
;
Path
path
=
entry
.
getFileStatus
(
)
.
getPath
(
)
;
Path
path
=
entry
.
getFileStatus
(
)
.
getPath
(
)
;
LOG
.
debug
(
,
path
)
;
if
(
path
.
isRoot
(
)
)
{
break
;
}
DDBPathMetadata
oldEntry
=
ancestorState
.
put
(
path
,
entry
)
;
boolean
addAncestors
=
true
;
if
(
oldEntry
!=
null
)
{
boolean
oldWasDir
=
oldEntry
.
getFileStatus
(
)
.
isDirectory
(
)
;
boolean
newIsDir
=
entry
.
getFileStatus
(
)
.
isDirectory
(
)
;
if
(
(
oldWasDir
&&
!
newIsDir
)
||
(
!
oldWasDir
&&
newIsDir
)
)
{
LOG
.
warn
(
,
oldEntry
)
;
LOG
.
warn
(
,
entry
)
;
ancestorState
.
put
(
path
,
oldEntry
)
;
throw
new
PathIOException
(
path
.
toString
(
)
,
String
.
format
(
,
E_INCONSISTENT_UPDATE
,
oldEntry
,
entry
)
)
;
}
else
{
LOG
.
warn
(
,
oldEntry
)
;
LOG
.
warn
(
,
entry
)
;
ancestorState
.
put
(
path
,
oldEntry
)
;
throw
new
PathIOException
(
path
.
toString
(
)
,
String
.
format
(
,
E_INCONSISTENT_UPDATE
,
oldEntry
,
entry
)
)
;
}
else
{
LOG
.
debug
(
,
path
,
entry
)
;
addAncestors
=
false
;
}
}
ancestry
.
put
(
path
,
Pair
.
of
(
EntryOrigin
.
Requested
,
entry
)
)
;
Path
parent
=
path
.
getParent
(
)
;
while
(
addAncestors
&&
!
parent
.
isRoot
(
)
&&
!
ancestry
.
containsKey
(
parent
)
)
{
if
(
!
ancestorState
.
findEntry
(
parent
,
true
)
)
{
DDBPathMetadata
md
;
Pair
<
EntryOrigin
,
DDBPathMetadata
>
newEntry
;
final
Item
item
=
getConsistentItem
(
parent
)
;
if
(
item
!=
null
&&
!
itemToPathMetadata
(
item
,
username
)
.
isDeleted
(
)
)
{
}
else
{
LOG
.
debug
(
,
path
,
entry
)
;
addAncestors
=
false
;
}
}
ancestry
.
put
(
path
,
Pair
.
of
(
EntryOrigin
.
Requested
,
entry
)
)
;
Path
parent
=
path
.
getParent
(
)
;
while
(
addAncestors
&&
!
parent
.
isRoot
(
)
&&
!
ancestry
.
containsKey
(
parent
)
)
{
if
(
!
ancestorState
.
findEntry
(
parent
,
true
)
)
{
DDBPathMetadata
md
;
Pair
<
EntryOrigin
,
DDBPathMetadata
>
newEntry
;
final
Item
item
=
getConsistentItem
(
parent
)
;
if
(
item
!=
null
&&
!
itemToPathMetadata
(
item
,
username
)
.
isDeleted
(
)
)
{
md
=
itemToPathMetadata
(
item
,
username
)
;
LOG
.
debug
(
,
md
)
;
newEntry
=
Pair
.
of
(
EntryOrigin
.
Retrieved
,
md
)
;
addAncestors
=
false
;
@
Override
@
Retries
.
RetryTranslated
public
void
put
(
final
PathMetadata
meta
,
@
Nullable
final
BulkOperationState
operationState
)
throws
IOException
{
@
Override
@
Retries
.
RetryTranslated
public
void
put
(
final
DirListingMetadata
meta
,
final
List
<
Path
>
unchangedEntries
,
@
Nullable
final
BulkOperationState
operationState
)
throws
IOException
{
@
Override
@
Retries
.
RetryTranslated
public
long
prune
(
PruneMode
pruneMode
,
long
cutoff
,
String
keyPrefix
)
throws
IOException
{
long
delay
=
conf
.
getTimeDuration
(
S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_KEY
,
S3GUARD_DDB_BACKGROUND_SLEEP_MSEC_DEFAULT
,
TimeUnit
.
MILLISECONDS
)
;
Set
<
Path
>
parentPathSet
=
new
HashSet
<
>
(
)
;
Set
<
Path
>
clearedParentPathSet
=
new
HashSet
<
>
(
)
;
FunctionsRaisingIOE
.
CallableRaisingIOE
<
Void
>
deleteBatchOperation
=
(
)
->
{
deletionBatch
.
sort
(
PathOrderComparators
.
TOPMOST_PATH_LAST
)
;
processBatchWriteRequest
(
state
,
pathToKey
(
deletionBatch
)
,
null
)
;
removeAuthoritativeDirFlag
(
parentPathSet
,
state
)
;
clearedParentPathSet
.
addAll
(
parentPathSet
)
;
parentPathSet
.
clear
(
)
;
return
null
;
}
;
for
(
Item
item
:
items
)
{
DDBPathMetadata
md
=
PathMetadataDynamoDBTranslation
.
itemToPathMetadata
(
item
,
username
)
;
Path
path
=
md
.
getFileStatus
(
)
.
getPath
(
)
;
boolean
tombstone
=
md
.
isDeleted
(
)
;
if
(
!
tombstone
&&
parentPath
!=
null
&&
!
parentPath
.
isRoot
(
)
&&
!
clearedParentPathSet
.
contains
(
parentPath
)
)
{
parentPathSet
.
add
(
parentPath
)
;
}
itemCount
++
;
if
(
deletionBatch
.
size
(
)
==
S3GUARD_DDB_BATCH_WRITE_REQUEST_LIMIT
)
{
deleteBatchOperation
.
apply
(
)
;
deletionBatch
.
clear
(
)
;
if
(
delay
>
0
)
{
Thread
.
sleep
(
delay
)
;
}
}
}
if
(
!
deletionBatch
.
isEmpty
(
)
)
{
deleteBatchOperation
.
apply
(
)
;
}
}
catch
(
InterruptedException
e
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
throw
new
InterruptedIOException
(
)
;
}
catch
(
AmazonDynamoDBException
e
)
{
throw
translateDynamoDBException
(
keyPrefix
,
+
keyPrefix
+
,
e
)
;
private
void
removeAuthoritativeDirFlag
(
final
Set
<
Path
>
pathSet
,
final
AncestorState
state
)
throws
IOException
{
AtomicReference
<
IOException
>
rIOException
=
new
AtomicReference
<
>
(
)
;
Set
<
DDBPathMetadata
>
metas
=
pathSet
.
stream
(
)
.
map
(
path
->
{
try
{
if
(
path
.
isRoot
(
)
)
{
LOG
.
debug
(
)
;
return
null
;
}
if
(
state
!=
null
&&
state
.
get
(
path
)
!=
null
)
{
LOG
.
debug
(
)
;
return
null
;
}
DDBPathMetadata
ddbPathMetadata
=
get
(
path
)
;
if
(
ddbPathMetadata
==
null
)
{
LOG
.
debug
(
,
path
)
;
return
null
;
}
if
(
ddbPathMetadata
.
isDeleted
(
)
)
{
try
{
if
(
path
.
isRoot
(
)
)
{
LOG
.
debug
(
)
;
return
null
;
}
if
(
state
!=
null
&&
state
.
get
(
path
)
!=
null
)
{
LOG
.
debug
(
)
;
return
null
;
}
DDBPathMetadata
ddbPathMetadata
=
get
(
path
)
;
if
(
ddbPathMetadata
==
null
)
{
LOG
.
debug
(
,
path
)
;
return
null
;
}
if
(
ddbPathMetadata
.
isDeleted
(
)
)
{
LOG
.
debug
(
,
path
)
;
return
null
;
}
if
(
!
ddbPathMetadata
.
getFileStatus
(
)
.
isDirectory
(
)
)
{
}
DDBPathMetadata
ddbPathMetadata
=
get
(
path
)
;
if
(
ddbPathMetadata
==
null
)
{
LOG
.
debug
(
,
path
)
;
return
null
;
}
if
(
ddbPathMetadata
.
isDeleted
(
)
)
{
LOG
.
debug
(
,
path
)
;
return
null
;
}
if
(
!
ddbPathMetadata
.
getFileStatus
(
)
.
isDirectory
(
)
)
{
LOG
.
debug
(
,
path
)
;
return
null
;
}
LOG
.
debug
(
,
ddbPathMetadata
)
;
ddbPathMetadata
.
setAuthoritativeDir
(
false
)
;
ddbPathMetadata
.
setLastUpdated
(
ttlTimeProvider
.
getNow
(
)
)
;
return
ddbPathMetadata
;
}
catch
(
IOException
e
)
{
if
(
!
ddbPathMetadata
.
getFileStatus
(
)
.
isDirectory
(
)
)
{
LOG
.
debug
(
,
path
)
;
return
null
;
}
LOG
.
debug
(
,
ddbPathMetadata
)
;
ddbPathMetadata
.
setAuthoritativeDir
(
false
)
;
ddbPathMetadata
.
setLastUpdated
(
ttlTimeProvider
.
getNow
(
)
)
;
return
ddbPathMetadata
;
}
catch
(
IOException
e
)
{
String
msg
=
String
.
format
(
+
,
path
)
;
LOG
.
error
(
msg
,
e
)
;
rIOException
.
set
(
e
)
;
return
null
;
}
}
)
.
filter
(
Objects
::
nonNull
)
.
collect
(
Collectors
.
toSet
(
)
)
;
try
{
LOG
.
debug
(
,
metas
)
;
@
Retries
.
OnceRaw
private
PutItemOutcome
putItem
(
Item
item
)
{
@
Override
@
Retries
.
OnceRaw
public
void
updateParameters
(
Map
<
String
,
String
>
parameters
)
throws
IOException
{
Preconditions
.
checkNotNull
(
table
,
)
;
TableDescription
desc
=
getTableDescription
(
true
)
;
ProvisionedThroughputDescription
current
=
desc
.
getProvisionedThroughput
(
)
;
long
currentRead
=
current
.
getReadCapacityUnits
(
)
;
long
newRead
=
getLongParam
(
parameters
,
S3GUARD_DDB_TABLE_CAPACITY_READ_KEY
,
currentRead
)
;
long
currentWrite
=
current
.
getWriteCapacityUnits
(
)
;
long
newWrite
=
getLongParam
(
parameters
,
S3GUARD_DDB_TABLE_CAPACITY_WRITE_KEY
,
currentWrite
)
;
if
(
currentRead
==
0
||
currentWrite
==
0
)
{
throw
new
IOException
(
E_ON_DEMAND_NO_SET_CAPACITY
)
;
}
if
(
newRead
!=
currentRead
||
newWrite
!=
currentWrite
)
{
LOG
.
info
(
,
currentRead
,
currentWrite
)
;
LOG
.
info
(
,
newRead
,
newWrite
)
;
tableHandler
.
provisionTableBlocking
(
newRead
,
newWrite
)
;
}
else
{
void
retryEvent
(
String
text
,
IOException
ex
,
int
attempts
,
boolean
idempotent
)
{
if
(
S3AUtils
.
isThrottleException
(
ex
)
)
{
instrumentation
.
throttled
(
)
;
int
eventCount
=
throttleEventCount
.
addAndGet
(
1
)
;
if
(
attempts
==
1
&&
eventCount
<
THROTTLE_EVENT_LOG_LIMIT
)
{
LOG
.
warn
(
+
,
text
,
ex
.
toString
(
)
)
;
}
Preconditions
.
checkArgument
(
operationState
instanceof
AncestorState
,
,
operationState
)
;
final
AncestorState
state
=
(
AncestorState
)
operationState
;
final
String
simpleDestKey
=
pathToParentKey
(
dest
)
;
final
String
destPathKey
=
simpleDestKey
+
;
final
String
opId
=
AncestorState
.
stateAsString
(
state
)
;
LOG
.
debug
(
,
opId
,
destPathKey
)
;
final
List
<
DDBPathMetadata
>
dirsToUpdate
=
new
ArrayList
<
>
(
)
;
synchronized
(
state
)
{
for
(
Map
.
Entry
<
Path
,
DDBPathMetadata
>
entry
:
state
.
getAncestry
(
)
.
entrySet
(
)
)
{
final
Path
path
=
entry
.
getKey
(
)
;
final
DDBPathMetadata
md
=
entry
.
getValue
(
)
;
final
String
key
=
pathToParentKey
(
path
)
;
if
(
md
.
getFileStatus
(
)
.
isDirectory
(
)
&&
(
key
.
equals
(
simpleDestKey
)
||
key
.
startsWith
(
destPathKey
)
)
)
{
md
.
setAuthoritativeDir
(
true
)
;
md
.
setLastUpdated
(
ttlTimeProvider
.
getNow
(
)
)
;
private
static
void
logPut
(
@
Nullable
AncestorState
state
,
Item
[
]
items
)
{
if
(
OPERATIONS_LOG
.
isDebugEnabled
(
)
)
{
String
stateStr
=
AncestorState
.
stateAsString
(
state
)
;
for
(
Item
item
:
items
)
{
boolean
tombstone
=
!
itemExists
(
item
)
;
boolean
isDir
=
getBoolAttribute
(
item
,
IS_DIR
,
false
)
;
boolean
auth
=
getBoolAttribute
(
item
,
IS_AUTHORITATIVE
,
false
)
;
private
static
void
logDelete
(
@
Nullable
AncestorState
state
,
PrimaryKey
[
]
keysDeleted
)
{
if
(
OPERATIONS_LOG
.
isDebugEnabled
(
)
)
{
String
stateStr
=
AncestorState
.
stateAsString
(
state
)
;
for
(
PrimaryKey
key
:
keysDeleted
)
{
@
VisibleForTesting
@
Retries
.
RetryTranslated
Table
initTable
(
)
throws
IOException
{
table
=
dynamoDB
.
getTable
(
tableName
)
;
try
{
try
{
@
VisibleForTesting
@
Retries
.
RetryTranslated
Table
initTable
(
)
throws
IOException
{
table
=
dynamoDB
.
getTable
(
tableName
)
;
try
{
try
{
LOG
.
debug
(
,
tableName
)
;
TableDescription
description
=
table
.
describe
(
)
;
LOG
.
debug
(
,
description
)
;
tableArn
=
description
.
getTableArn
(
)
;
final
String
status
=
description
.
getTableStatus
(
)
;
switch
(
status
)
{
case
:
LOG
.
debug
(
+
+
,
tableName
,
region
)
;
waitForTableActive
(
table
)
;
break
;
case
:
throw
new
FileNotFoundException
(
+
+
tableName
+
+
+
region
)
;
case
:
LOG
.
debug
(
)
;
break
;
case
:
break
;
default
:
throw
new
IOException
(
+
status
+
+
tableName
+
+
region
)
;
}
verifyVersionCompatibility
(
)
;
final
Item
versionMarker
=
getVersionMarkerItem
(
)
;
Long
created
=
extractCreationTimeFromMarker
(
versionMarker
)
;
@
VisibleForTesting
protected
void
verifyVersionCompatibility
(
)
throws
IOException
{
final
Item
versionMarkerItem
=
getVersionMarkerItem
(
)
;
Item
versionMarkerFromTag
=
null
;
boolean
canReadDdbTags
=
true
;
try
{
versionMarkerFromTag
=
getVersionMarkerFromTags
(
table
,
amazonDynamoDB
)
;
}
catch
(
AccessDeniedException
e
)
{
LOG
.
debug
(
)
;
canReadDdbTags
=
false
;
}
LOG
.
debug
(
,
versionMarkerItem
,
versionMarkerFromTag
)
;
if
(
versionMarkerItem
==
null
&&
versionMarkerFromTag
==
null
)
{
if
(
!
isEmptyTable
(
tableName
,
amazonDynamoDB
)
)
{
LOG
.
error
(
)
;
throw
new
IOException
(
E_NO_VERSION_MARKER_AND_NOT_EMPTY
+
+
tableName
)
;
}
if
(
canReadDdbTags
)
{
try
{
versionMarkerFromTag
=
getVersionMarkerFromTags
(
table
,
amazonDynamoDB
)
;
}
catch
(
AccessDeniedException
e
)
{
LOG
.
debug
(
)
;
canReadDdbTags
=
false
;
}
LOG
.
debug
(
,
versionMarkerItem
,
versionMarkerFromTag
)
;
if
(
versionMarkerItem
==
null
&&
versionMarkerFromTag
==
null
)
{
if
(
!
isEmptyTable
(
tableName
,
amazonDynamoDB
)
)
{
LOG
.
error
(
)
;
throw
new
IOException
(
E_NO_VERSION_MARKER_AND_NOT_EMPTY
+
+
tableName
)
;
}
if
(
canReadDdbTags
)
{
LOG
.
info
(
+
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
tagTableWithVersionMarker
(
)
;
}
if
(
!
canReadDdbTags
)
{
canReadDdbTags
=
false
;
}
LOG
.
debug
(
,
versionMarkerItem
,
versionMarkerFromTag
)
;
if
(
versionMarkerItem
==
null
&&
versionMarkerFromTag
==
null
)
{
if
(
!
isEmptyTable
(
tableName
,
amazonDynamoDB
)
)
{
LOG
.
error
(
)
;
throw
new
IOException
(
E_NO_VERSION_MARKER_AND_NOT_EMPTY
+
+
tableName
)
;
}
if
(
canReadDdbTags
)
{
LOG
.
info
(
+
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
tagTableWithVersionMarker
(
)
;
}
if
(
!
canReadDdbTags
)
{
LOG
.
info
(
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
}
}
if
(
versionMarkerItem
==
null
&&
versionMarkerFromTag
!=
null
)
{
final
int
tagVersionMarker
=
extractVersionFromMarker
(
versionMarkerFromTag
)
;
LOG
.
error
(
)
;
throw
new
IOException
(
E_NO_VERSION_MARKER_AND_NOT_EMPTY
+
+
tableName
)
;
}
if
(
canReadDdbTags
)
{
LOG
.
info
(
+
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
tagTableWithVersionMarker
(
)
;
}
if
(
!
canReadDdbTags
)
{
LOG
.
info
(
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
}
}
if
(
versionMarkerItem
==
null
&&
versionMarkerFromTag
!=
null
)
{
final
int
tagVersionMarker
=
extractVersionFromMarker
(
versionMarkerFromTag
)
;
throwExceptionOnVersionMismatch
(
tagVersionMarker
,
tableName
,
E_INCOMPATIBLE_TAG_VERSION
)
;
LOG
.
info
(
+
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
}
if
(
versionMarkerItem
!=
null
&&
versionMarkerFromTag
==
null
&&
canReadDdbTags
)
{
}
if
(
!
canReadDdbTags
)
{
LOG
.
info
(
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
}
}
if
(
versionMarkerItem
==
null
&&
versionMarkerFromTag
!=
null
)
{
final
int
tagVersionMarker
=
extractVersionFromMarker
(
versionMarkerFromTag
)
;
throwExceptionOnVersionMismatch
(
tagVersionMarker
,
tableName
,
E_INCOMPATIBLE_TAG_VERSION
)
;
LOG
.
info
(
+
+
,
tableName
)
;
putVersionMarkerItemToTable
(
)
;
}
if
(
versionMarkerItem
!=
null
&&
versionMarkerFromTag
==
null
&&
canReadDdbTags
)
{
final
int
itemVersionMarker
=
extractVersionFromMarker
(
versionMarkerItem
)
;
throwExceptionOnVersionMismatch
(
itemVersionMarker
,
tableName
,
E_INCOMPATIBLE_ITEM_VERSION
)
;
LOG
.
info
(
+
+
,
tableName
)
;
tagTableWithVersionMarker
(
)
;
}
if
(
versionMarkerItem
!=
null
&&
versionMarkerFromTag
!=
null
)
{
final
int
tagVersionMarker
=
extractVersionFromMarker
(
versionMarkerFromTag
)
;
@
Retries
.
OnceRaw
private
PutItemOutcome
putItem
(
Item
item
)
{
private
long
importDir
(
)
throws
IOException
{
Preconditions
.
checkArgument
(
status
.
isDirectory
(
)
)
;
long
totalCountOfEntriesWritten
=
0
;
final
Path
basePath
=
status
.
getPath
(
)
;
final
MetadataStore
ms
=
getStore
(
)
;
long
countOfFilesWritten
=
0
;
long
countOfDirsWritten
=
0
;
RemoteIterator
<
S3ALocatedFileStatus
>
it
=
getFilesystem
(
)
.
listFilesAndEmptyDirectoriesForceNonAuth
(
basePath
,
true
)
;
while
(
it
.
hasNext
(
)
)
{
S3ALocatedFileStatus
located
=
it
.
next
(
)
;
S3AFileStatus
child
;
final
Path
path
=
located
.
getPath
(
)
;
final
boolean
isDirectory
=
located
.
isDirectory
(
)
;
if
(
isDirectory
)
{
child
=
DynamoDBMetadataStore
.
makeDirStatus
(
path
,
located
.
getOwner
(
)
)
;
dirCache
.
add
(
path
)
;
countOfDirsWritten
++
;
}
else
{
child
=
located
.
toS3AFileStatus
(
)
;
}
int
parentsWritten
=
putParentsIfNotPresent
(
child
,
operationState
)
;
if
(
!
isDirectory
)
{
final
PathMetadata
existingEntry
=
S3Guard
.
getWithTtl
(
ms
,
path
,
null
,
false
,
true
)
;
if
(
existingEntry
!=
null
)
{
final
S3AFileStatus
existingStatus
=
existingEntry
.
getFileStatus
(
)
;
if
(
existingStatus
.
isFile
(
)
)
{
final
String
existingEtag
=
existingStatus
.
getETag
(
)
;
final
String
childEtag
=
child
.
getETag
(
)
;
if
(
child
.
getModificationTime
(
)
!=
existingStatus
.
getModificationTime
(
)
||
existingStatus
.
getLen
(
)
!=
child
.
getLen
(
)
||
existingEtag
==
null
||
!
existingEtag
.
equals
(
childEtag
)
)
{
if
(
childEtag
.
equals
(
existingEtag
)
)
{
child
.
setVersionId
(
existingStatus
.
getVersionId
(
)
)
;
}
}
else
{
child
=
null
;
}
}
}
if
(
child
!=
null
)
{
countOfFilesWritten
++
;
}
}
if
(
child
!=
null
)
{
if
(
existingEntry
!=
null
)
{
final
S3AFileStatus
existingStatus
=
existingEntry
.
getFileStatus
(
)
;
if
(
existingStatus
.
isFile
(
)
)
{
final
String
existingEtag
=
existingStatus
.
getETag
(
)
;
final
String
childEtag
=
child
.
getETag
(
)
;
if
(
child
.
getModificationTime
(
)
!=
existingStatus
.
getModificationTime
(
)
||
existingStatus
.
getLen
(
)
!=
child
.
getLen
(
)
||
existingEtag
==
null
||
!
existingEtag
.
equals
(
childEtag
)
)
{
if
(
childEtag
.
equals
(
existingEtag
)
)
{
child
.
setVersionId
(
existingStatus
.
getVersionId
(
)
)
;
}
}
else
{
child
=
null
;
}
}
}
if
(
child
!=
null
)
{
countOfFilesWritten
++
;
}
}
if
(
child
!=
null
)
{
String
t
=
isDirectory
?
:
;
if
(
verbose
)
{
final
String
childEtag
=
child
.
getETag
(
)
;
if
(
child
.
getModificationTime
(
)
!=
existingStatus
.
getModificationTime
(
)
||
existingStatus
.
getLen
(
)
!=
child
.
getLen
(
)
||
existingEtag
==
null
||
!
existingEtag
.
equals
(
childEtag
)
)
{
if
(
childEtag
.
equals
(
existingEtag
)
)
{
child
.
setVersionId
(
existingStatus
.
getVersionId
(
)
)
;
}
}
else
{
child
=
null
;
}
}
}
if
(
child
!=
null
)
{
countOfFilesWritten
++
;
}
}
if
(
child
!=
null
)
{
String
t
=
isDirectory
?
:
;
if
(
verbose
)
{
LOG
.
info
(
,
t
,
path
)
;
}
else
{
LOG
.
debug
(
,
t
,
path
)
;
}
S3Guard
.
putWithTtl
(
ms
,
new
PathMetadata
(
child
)
,
getFilesystem
(
)
.
getTtlTimeProvider
(
)
,
operationState
)
;
@
Override
public
synchronized
DirListingMetadata
listChildren
(
Path
p
)
throws
IOException
{
Path
path
=
standardize
(
p
)
;
DirListingMetadata
listing
=
getDirListingMeta
(
path
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
move
(
@
Nullable
Collection
<
Path
>
pathsToDelete
,
@
Nullable
Collection
<
PathMetadata
>
pathsToCreate
,
@
Nullable
final
BulkOperationState
operationState
)
throws
IOException
{
@
Override
public
void
put
(
PathMetadata
meta
,
final
BulkOperationState
operationState
)
throws
IOException
{
Preconditions
.
checkNotNull
(
meta
)
;
S3AFileStatus
status
=
meta
.
getFileStatus
(
)
;
Path
path
=
standardize
(
status
.
getPath
(
)
)
;
synchronized
(
this
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
synchronized
void
put
(
DirListingMetadata
meta
,
final
List
<
Path
>
unchangedEntries
,
final
BulkOperationState
operationState
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
path
)
;
if
(
entry
.
hasPathMeta
(
)
)
{
if
(
tombstone
)
{
PathMetadata
pmd
=
PathMetadata
.
tombstone
(
path
,
ttlTimeProvider
.
getNow
(
)
)
;
entry
.
setPathMetadata
(
pmd
)
;
}
else
{
entry
.
setPathMetadata
(
null
)
;
}
}
if
(
entry
.
hasDirMeta
(
)
)
{
LOG
.
debug
(
,
path
)
;
entry
.
setDirListingMetadata
(
null
)
;
}
if
(
!
entry
.
hasDirMeta
(
)
&&
!
entry
.
hasPathMeta
(
)
)
{
localCache
.
invalidate
(
entry
)
;
}
Path
parent
=
path
.
getParent
(
)
;
if
(
parent
!=
null
)
{
DirListingMetadata
dir
=
getDirListingMeta
(
parent
)
;
@
Override
public
void
fileCopied
(
final
Path
sourcePath
,
final
S3ObjectAttributes
sourceAttributes
,
final
S3ObjectAttributes
destAttributes
,
final
Path
destPath
,
final
long
blockSize
,
final
boolean
addAncestors
)
throws
IOException
{
final
List
<
PathMetadata
>
entriesToAdd
=
new
ArrayList
<
>
(
1
)
;
@
Override
public
void
fileCopied
(
final
Path
sourcePath
,
final
S3ObjectAttributes
sourceAttributes
,
final
S3ObjectAttributes
destAttributes
,
final
Path
destPath
,
final
long
blockSize
,
final
boolean
addAncestors
)
throws
IOException
{
final
List
<
PathMetadata
>
entriesToAdd
=
new
ArrayList
<
>
(
1
)
;
LOG
.
debug
(
,
sourcePath
)
;
MetadataStore
store
=
getMetadataStore
(
)
;
synchronized
(
this
)
{
checkArgument
(
!
pathsToDelete
.
contains
(
sourcePath
)
,
,
destPath
)
;
S3Guard
.
addMoveFile
(
store
,
pathsToDelete
,
entriesToAdd
,
sourcePath
,
destPath
,
sourceAttributes
.
getLen
(
)
,
blockSize
,
getOwner
(
)
,
destAttributes
.
getETag
(
)
,
destAttributes
.
getVersionId
(
)
)
;
@
Override
public
int
execute
(
)
throws
ServiceLaunchException
,
IOException
{
URI
uri
=
getUri
(
)
;
String
host
=
uri
.
getHost
(
)
;
String
prefix
=
+
host
+
;
DynamoDBMetadataStore
ddbms
=
getStore
(
)
;
S3GuardTableAccess
tableAccess
=
new
S3GuardTableAccess
(
ddbms
)
;
ExpressionSpecBuilder
builder
=
new
ExpressionSpecBuilder
(
)
;
builder
.
withKeyCondition
(
ExpressionSpecBuilder
.
S
(
PARENT
)
.
beginsWith
(
prefix
)
)
;
LOG
.
info
(
,
prefix
,
ddbms
)
;
Iterable
<
DDBPathMetadata
>
entries
=
ddbms
.
wrapWithRetries
(
tableAccess
.
scanMetadata
(
builder
)
)
;
List
<
Path
>
list
=
new
ArrayList
<
>
(
)
;
entries
.
iterator
(
)
.
forEachRemaining
(
e
->
{
if
(
!
(
e
instanceof
S3GuardTableAccess
.
VersionMarker
)
)
{
Path
p
=
e
.
getFileStatus
(
)
.
getPath
(
)
;
String
type
=
e
.
getFileStatus
(
)
.
isFile
(
)
?
:
;
boolean
tombstone
=
e
.
isDeleted
(
)
;
if
(
tombstone
)
{
type
=
+
type
;
}
LOG
.
info
(
,
type
,
p
)
;
list
.
add
(
p
)
;
}
}
)
;
int
count
=
list
.
size
(
)
;
filesFound
=
count
;
type
=
+
type
;
}
LOG
.
info
(
,
type
,
p
)
;
list
.
add
(
p
)
;
}
}
)
;
int
count
=
list
.
size
(
)
;
filesFound
=
count
;
LOG
.
info
(
,
count
,
(
count
==
0
?
:
)
)
;
if
(
count
>
0
)
{
if
(
force
)
{
DurationInfo
duration
=
new
DurationInfo
(
LOG
,
,
count
,
ddbms
.
toString
(
)
)
;
for
(
Path
path
:
list
)
{
ddbms
.
getInvoker
(
)
.
retry
(
,
prefix
,
true
,
(
)
->
tableAccess
.
delete
(
path
)
)
;
}
duration
.
close
(
)
;
long
durationMillis
=
duration
.
value
(
)
;
long
timePerEntry
=
durationMillis
/
count
;
public
IOException
renameFailed
(
Exception
ex
)
{
@
Retries
.
OnceTranslated
public
static
MetadataStore
getMetadataStore
(
FileSystem
fs
,
ITtlTimeProvider
ttlTimeProvider
)
throws
IOException
{
Preconditions
.
checkNotNull
(
fs
)
;
Configuration
conf
=
fs
.
getConf
(
)
;
Preconditions
.
checkNotNull
(
conf
)
;
MetadataStore
msInstance
;
try
{
Class
<
?
extends
MetadataStore
>
msClass
=
getMetadataStoreClass
(
conf
)
;
msInstance
=
ReflectionUtils
.
newInstance
(
msClass
,
conf
)
;
@
Retries
.
OnceTranslated
public
static
MetadataStore
getMetadataStore
(
FileSystem
fs
,
ITtlTimeProvider
ttlTimeProvider
)
throws
IOException
{
Preconditions
.
checkNotNull
(
fs
)
;
Configuration
conf
=
fs
.
getConf
(
)
;
Preconditions
.
checkNotNull
(
conf
)
;
MetadataStore
msInstance
;
try
{
Class
<
?
extends
MetadataStore
>
msClass
=
getMetadataStoreClass
(
conf
)
;
msInstance
=
ReflectionUtils
.
newInstance
(
msClass
,
conf
)
;
LOG
.
debug
(
,
msClass
.
getSimpleName
(
)
,
fs
.
getScheme
(
)
)
;
msInstance
.
initialize
(
fs
,
ttlTimeProvider
)
;
return
msInstance
;
}
catch
(
FileNotFoundException
e
)
{
throw
e
;
}
catch
(
RuntimeException
|
IOException
e
)
{
String
message
=
+
conf
.
get
(
S3_METADATA_STORE_IMPL
)
+
+
S3_METADATA_STORE_IMPL
+
+
e
;
Set
<
Path
>
deleted
=
dirMeta
.
listTombstones
(
)
;
final
Map
<
Path
,
PathMetadata
>
dirMetaMap
=
dirMeta
.
getListing
(
)
.
stream
(
)
.
collect
(
Collectors
.
toMap
(
pm
->
pm
.
getFileStatus
(
)
.
getPath
(
)
,
pm
->
pm
)
)
;
for
(
S3AFileStatus
s
:
backingStatuses
)
{
final
Path
statusPath
=
s
.
getPath
(
)
;
if
(
deleted
.
contains
(
statusPath
)
)
{
continue
;
}
PathMetadata
pathMetadata
=
dirMetaMap
.
get
(
statusPath
)
;
boolean
shouldUpdate
;
if
(
pathMetadata
!=
null
)
{
shouldUpdate
=
s
.
getModificationTime
(
)
>
(
pathMetadata
.
getFileStatus
(
)
)
.
getModificationTime
(
)
;
pathMetadata
=
new
PathMetadata
(
s
)
;
}
else
{
pathMetadata
=
new
PathMetadata
(
s
)
;
shouldUpdate
=
DIR_MERGE_UPDATES_ALL_RECORDS_NONAUTH
;
}
if
(
shouldUpdate
)
{
if
(
deleted
.
contains
(
statusPath
)
)
{
continue
;
}
PathMetadata
pathMetadata
=
dirMetaMap
.
get
(
statusPath
)
;
boolean
shouldUpdate
;
if
(
pathMetadata
!=
null
)
{
shouldUpdate
=
s
.
getModificationTime
(
)
>
(
pathMetadata
.
getFileStatus
(
)
)
.
getModificationTime
(
)
;
pathMetadata
=
new
PathMetadata
(
s
)
;
}
else
{
pathMetadata
=
new
PathMetadata
(
s
)
;
shouldUpdate
=
DIR_MERGE_UPDATES_ALL_RECORDS_NONAUTH
;
}
if
(
shouldUpdate
)
{
LOG
.
debug
(
,
s
)
;
entriesToAdd
.
add
(
pathMetadata
)
;
}
dirMeta
.
put
(
pathMetadata
)
;
}
if
(
!
entriesToAdd
.
isEmpty
(
)
)
{
final
PathMetadata
pathMetadata
=
ms
.
get
(
path
,
needEmptyDirectoryFlag
)
;
if
(
timeProvider
==
null
)
{
LOG
.
debug
(
)
;
return
pathMetadata
;
}
if
(
allowAuthoritative
)
{
LOG
.
debug
(
)
;
return
pathMetadata
;
}
long
ttl
=
timeProvider
.
getMetadataTtl
(
)
;
if
(
pathMetadata
!=
null
)
{
if
(
pathMetadata
.
getLastUpdated
(
)
==
0
)
{
LOG
.
debug
(
+
,
path
)
;
return
pathMetadata
;
}
if
(
!
pathMetadata
.
isExpired
(
ttl
,
timeProvider
.
getNow
(
)
)
)
{
return
pathMetadata
;
}
else
{
final
Queue
<
S3AFileStatus
>
queue
=
new
ArrayDeque
<
>
(
)
;
queue
.
add
(
root
)
;
while
(
!
queue
.
isEmpty
(
)
)
{
final
S3AFileStatus
currentDir
=
queue
.
poll
(
)
;
final
Path
currentDirPath
=
currentDir
.
getPath
(
)
;
try
{
List
<
FileStatus
>
s3DirListing
=
Arrays
.
asList
(
rawFS
.
listStatus
(
currentDirPath
)
)
;
compareAuthoritativeDirectoryFlag
(
comparePairs
,
currentDirPath
,
s3DirListing
)
;
s3DirListing
.
stream
(
)
.
filter
(
pm
->
pm
.
isDirectory
(
)
)
.
map
(
S3AFileStatus
.
class
::
cast
)
.
forEach
(
pm
->
queue
.
add
(
pm
)
)
;
final
List
<
S3AFileStatus
>
children
=
s3DirListing
.
stream
(
)
.
filter
(
status
->
!
status
.
isDirectory
(
)
)
.
map
(
S3AFileStatus
.
class
::
cast
)
.
collect
(
toList
(
)
)
;
final
List
<
ComparePair
>
compareResult
=
compareS3DirContentToMs
(
currentDir
,
children
)
;
comparePairs
.
addAll
(
compareResult
)
;
scannedItems
++
;
scannedItems
+=
children
.
size
(
)
;
}
catch
(
FileNotFoundException
e
)
{
List
<
FileStatus
>
s3DirListing
=
Arrays
.
asList
(
rawFS
.
listStatus
(
currentDirPath
)
)
;
compareAuthoritativeDirectoryFlag
(
comparePairs
,
currentDirPath
,
s3DirListing
)
;
s3DirListing
.
stream
(
)
.
filter
(
pm
->
pm
.
isDirectory
(
)
)
.
map
(
S3AFileStatus
.
class
::
cast
)
.
forEach
(
pm
->
queue
.
add
(
pm
)
)
;
final
List
<
S3AFileStatus
>
children
=
s3DirListing
.
stream
(
)
.
filter
(
status
->
!
status
.
isDirectory
(
)
)
.
map
(
S3AFileStatus
.
class
::
cast
)
.
collect
(
toList
(
)
)
;
final
List
<
ComparePair
>
compareResult
=
compareS3DirContentToMs
(
currentDir
,
children
)
;
comparePairs
.
addAll
(
compareResult
)
;
scannedItems
++
;
scannedItems
+=
children
.
size
(
)
;
}
catch
(
FileNotFoundException
e
)
{
LOG
.
error
(
+
currentDirPath
,
e
)
;
}
}
stopwatch
.
stop
(
)
;
S3GuardFsckViolationHandler
handler
=
new
S3GuardFsckViolationHandler
(
rawFS
,
metadataStore
)
;
for
(
ComparePair
comparePair
:
comparePairs
)
{
handler
.
logError
(
comparePair
)
;
}
LOG
.
info
(
,
stopwatch
.
now
(
TimeUnit
.
SECONDS
)
)
;
protected
ComparePair
compareFileStatusToPathMetadata
(
S3AFileStatus
s3FileStatus
,
PathMetadata
msPathMetadata
)
throws
IOException
{
final
Path
path
=
s3FileStatus
.
getPath
(
)
;
if
(
msPathMetadata
!=
null
)
{
public
List
<
ComparePair
>
checkDdbInternalConsistency
(
Path
basePath
)
throws
IOException
{
Preconditions
.
checkArgument
(
basePath
.
isAbsolute
(
)
,
)
;
List
<
ComparePair
>
comparePairs
=
new
ArrayList
<
>
(
)
;
String
rootStr
=
basePath
.
toString
(
)
;
if
(
baseMeta
==
null
)
{
throw
new
FileNotFoundException
(
+
+
)
;
}
}
else
{
baseMeta
=
new
DDBPathMetadata
(
new
S3AFileStatus
(
Tristate
.
UNKNOWN
,
basePath
,
username
)
)
;
}
DDBTreeNode
root
=
new
DDBTreeNode
(
baseMeta
)
;
ddbTree
.
addNode
(
root
)
;
ddbTree
.
setRoot
(
root
)
;
ExpressionSpecBuilder
builder
=
new
ExpressionSpecBuilder
(
)
;
builder
.
withCondition
(
ExpressionSpecBuilder
.
S
(
)
.
beginsWith
(
pathToParentKey
(
basePath
)
)
)
;
final
IteratorSupport
<
Item
,
ScanOutcome
>
resultIterator
=
table
.
scan
(
builder
.
buildForScan
(
)
)
.
iterator
(
)
;
resultIterator
.
forEachRemaining
(
item
->
{
final
DDBPathMetadata
pmd
=
itemToPathMetadata
(
item
,
username
)
;
DDBTreeNode
ddbTreeNode
=
new
DDBTreeNode
(
pmd
)
;
ddbTree
.
addNode
(
ddbTreeNode
)
;
}
)
;
public
void
logError
(
S3GuardFsck
.
ComparePair
comparePair
)
throws
IOException
{
if
(
!
comparePair
.
containsViolation
(
)
)
{
public
void
doFix
(
S3GuardFsck
.
ComparePair
comparePair
)
throws
IOException
{
if
(
!
comparePair
.
containsViolation
(
)
)
{
protected
void
handleComparePair
(
S3GuardFsck
.
ComparePair
comparePair
,
StringBuilder
sB
,
HandleMode
handleMode
)
throws
IOException
{
for
(
S3GuardFsck
.
Violation
violation
:
comparePair
.
getViolations
(
)
)
{
try
{
ViolationHandler
handler
=
violation
.
getHandler
(
)
.
getDeclaredConstructor
(
S3GuardFsck
.
ComparePair
.
class
)
.
newInstance
(
comparePair
)
;
switch
(
handleMode
)
{
case
FIX
:
final
String
errorStr
=
handler
.
getError
(
)
;
sB
.
append
(
errorStr
)
;
break
;
case
LOG
:
final
String
fixStr
=
handler
.
fixViolation
(
rawFs
,
metadataStore
)
;
sB
.
append
(
fixStr
)
;
break
;
default
:
throw
new
UnsupportedOperationException
(
+
handleMode
)
;
}
}
catch
(
NoSuchMethodException
e
)
{
LOG
.
error
(
,
violation
.
getHandler
(
)
)
;
}
catch
(
IllegalAccessException
|
InstantiationException
|
InvocationTargetException
e
)
{
protected
void
initS3AFileSystem
(
String
path
)
throws
IOException
{
protected
void
initS3AFileSystem
(
String
path
)
throws
IOException
{
LOG
.
debug
(
,
path
)
;
URI
uri
=
toUri
(
path
)
;
Configuration
conf
=
new
Configuration
(
getConf
(
)
)
;
String
nullStore
=
NullMetadataStore
.
class
.
getName
(
)
;
conf
.
set
(
S3_METADATA_STORE_IMPL
,
nullStore
)
;
String
bucket
=
uri
.
getHost
(
)
;
S3AUtils
.
setBucketOption
(
conf
,
bucket
,
S3_METADATA_STORE_IMPL
,
S3GUARD_METASTORE_NULL
)
;
String
updatedBucketOption
=
S3AUtils
.
getBucketOption
(
conf
,
bucket
,
S3_METADATA_STORE_IMPL
)
;
exit
(
ret
,
)
;
}
catch
(
CommandFormat
.
UnknownOptionException
e
)
{
errorln
(
e
.
getMessage
(
)
)
;
printHelp
(
)
;
exit
(
E_USAGE
,
e
.
getMessage
(
)
)
;
}
catch
(
ExitUtil
.
ExitException
e
)
{
LOG
.
debug
(
,
e
)
;
exit
(
e
.
getExitCode
(
)
,
e
.
toString
(
)
)
;
}
catch
(
FileNotFoundException
e
)
{
errorln
(
e
.
toString
(
)
)
;
LOG
.
debug
(
,
e
)
;
exit
(
EXIT_NOT_FOUND
,
e
.
toString
(
)
)
;
}
catch
(
Throwable
e
)
{
if
(
e
instanceof
ExitCodeProvider
)
{
final
ExitCodeProvider
ec
=
(
ExitCodeProvider
)
e
;
stream
=
FutureIOSupport
.
awaitFuture
(
builder
.
build
(
)
)
;
}
catch
(
FileNotFoundException
e
)
{
throw
storeNotFound
(
e
)
;
}
try
{
if
(
toConsole
)
{
bytesRead
=
0
;
@
SuppressWarnings
(
)
Scanner
scanner
=
new
Scanner
(
new
BufferedReader
(
new
InputStreamReader
(
stream
,
StandardCharsets
.
UTF_8
)
)
)
;
scanner
.
useDelimiter
(
)
;
while
(
scanner
.
hasNextLine
(
)
)
{
linesRead
++
;
String
l
=
scanner
.
nextLine
(
)
;
bytesRead
+=
l
.
length
(
)
+
1
;
println
(
out
,
,
l
)
;
}
}
else
{
FileSystem
destFS
=
destPath
.
getFileSystem
(
getConf
(
)
)
;
describe
(
+
maxAttempts
+
)
;
for
(
int
attempt
=
1
;
attempt
<=
maxAttempts
;
++
attempt
)
{
try
{
super
.
testListEmptyRootDirectory
(
)
;
break
;
}
catch
(
AssertionError
|
FileNotFoundException
e
)
{
if
(
attempt
<
maxAttempts
)
{
LOG
.
info
(
+
+
,
attempt
,
maxAttempts
,
e
)
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e2
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
fail
(
)
;
break
;
}
}
else
{
@
Test
public
void
testSeekToReadaheadAndRead
(
)
throws
Throwable
{
describe
(
+
)
;
Path
path
=
path
(
)
;
FileSystem
fs
=
getFileSystem
(
)
;
writeTestDataset
(
path
)
;
try
(
FSDataInputStream
in
=
fs
.
open
(
path
)
)
{
readAtEndAndReturn
(
in
)
;
final
byte
[
]
temp
=
new
byte
[
5
]
;
int
offset
=
READAHEAD
-
1
;
in
.
seek
(
offset
)
;
int
l
=
in
.
read
(
temp
)
;
assertTrue
(
,
l
>
0
)
;
@
Test
public
void
testSeekToReadaheadExactlyAndRead
(
)
throws
Throwable
{
describe
(
+
)
;
Path
path
=
path
(
)
;
FileSystem
fs
=
getFileSystem
(
)
;
writeTestDataset
(
path
)
;
try
(
FSDataInputStream
in
=
fs
.
open
(
path
)
)
{
readAtEndAndReturn
(
in
)
;
final
byte
[
]
temp
=
new
byte
[
5
]
;
int
offset
=
READAHEAD
;
in
.
seek
(
offset
)
;
int
l
=
in
.
read
(
temp
)
;
@
Test
public
void
testSeekToReadaheadExactlyAndReadByte
(
)
throws
Throwable
{
describe
(
+
)
;
Path
path
=
path
(
)
;
FileSystem
fs
=
getFileSystem
(
)
;
writeTestDataset
(
path
)
;
try
(
FSDataInputStream
in
=
fs
.
open
(
path
)
)
{
readAtEndAndReturn
(
in
)
;
final
byte
[
]
temp
=
new
byte
[
1
]
;
int
offset
=
READAHEAD
;
in
.
seek
(
offset
)
;
temp
[
0
]
=
in
.
readByte
(
)
;
assertDatasetEquals
(
READAHEAD
,
,
temp
,
1
)
;
protected
void
describe
(
String
text
,
Object
...
args
)
{
@
Test
@
SuppressWarnings
(
)
public
void
testBlockSize
(
)
throws
Exception
{
FileSystem
fs
=
getFileSystem
(
)
;
long
defaultBlockSize
=
fs
.
getDefaultBlockSize
(
)
;
assertEquals
(
,
S3AFileSystem
.
DEFAULT_BLOCKSIZE
,
defaultBlockSize
)
;
long
newBlockSize
=
defaultBlockSize
*
2
;
fs
.
getConf
(
)
.
setLong
(
Constants
.
FS_S3A_BLOCK_SIZE
,
newBlockSize
)
;
Path
dir
=
path
(
)
;
Path
file
=
new
Path
(
dir
,
)
;
createFile
(
fs
,
file
,
true
,
dataset
(
1024
,
'a'
,
'z'
-
'a'
)
)
;
FileStatus
fileStatus
=
fs
.
getFileStatus
(
file
)
;
assertEquals
(
+
fileStatus
,
newBlockSize
,
fileStatus
.
getBlockSize
(
)
)
;
boolean
found
=
false
;
FileStatus
[
]
listing
=
fs
.
listStatus
(
dir
)
;
for
(
FileStatus
stat
:
listing
)
{
@
Test
public
void
testEncryptionAlgorithmSetToDES
(
)
throws
Throwable
{
assumeEnabled
(
)
;
intercept
(
IOException
.
class
,
,
(
)
->
{
Configuration
conf
=
super
.
createConfiguration
(
)
;
conf
.
set
(
Constants
.
SERVER_SIDE_ENCRYPTION_ALGORITHM
,
)
;
S3AContract
contract
=
(
S3AContract
)
createContract
(
conf
)
;
contract
.
init
(
)
;
FileSystem
fileSystem
=
contract
.
getTestFileSystem
(
)
;
assertNotNull
(
,
fileSystem
)
;
URI
fsURI
=
fileSystem
.
getUri
(
)
;
@
Test
public
void
testEncryptionAlgorithmSSECWithNoEncryptionKey
(
)
throws
Throwable
{
assumeEnabled
(
)
;
intercept
(
IllegalArgumentException
.
class
,
+
Constants
.
SERVER_SIDE_ENCRYPTION_KEY
+
,
(
)
->
{
Configuration
conf
=
super
.
createConfiguration
(
)
;
conf
.
set
(
Constants
.
SERVER_SIDE_ENCRYPTION_ALGORITHM
,
S3AEncryptionMethods
.
SSE_C
.
getMethod
(
)
)
;
conf
.
set
(
Constants
.
SERVER_SIDE_ENCRYPTION_KEY
,
null
)
;
S3AContract
contract
=
(
S3AContract
)
createContract
(
conf
)
;
contract
.
init
(
)
;
FileSystem
fileSystem
=
contract
.
getTestFileSystem
(
)
;
assertNotNull
(
,
fileSystem
)
;
URI
fsURI
=
fileSystem
.
getUri
(
)
;
@
Test
public
void
testEncryptionAlgorithmSSECWithBlankEncryptionKey
(
)
throws
Throwable
{
intercept
(
IOException
.
class
,
S3AUtils
.
SSE_C_NO_KEY_ERROR
,
(
)
->
{
Configuration
conf
=
super
.
createConfiguration
(
)
;
conf
.
set
(
Constants
.
SERVER_SIDE_ENCRYPTION_ALGORITHM
,
S3AEncryptionMethods
.
SSE_C
.
getMethod
(
)
)
;
conf
.
set
(
Constants
.
SERVER_SIDE_ENCRYPTION_KEY
,
)
;
S3AContract
contract
=
(
S3AContract
)
createContract
(
conf
)
;
contract
.
init
(
)
;
FileSystem
fileSystem
=
contract
.
getTestFileSystem
(
)
;
assertNotNull
(
,
fileSystem
)
;
URI
fsURI
=
fileSystem
.
getUri
(
)
;
@
Test
public
void
testEncryptionAlgorithmSSES3WithEncryptionKey
(
)
throws
Throwable
{
assumeEnabled
(
)
;
intercept
(
IOException
.
class
,
S3AUtils
.
SSE_S3_WITH_KEY_ERROR
,
(
)
->
{
Configuration
conf
=
super
.
createConfiguration
(
)
;
conf
.
set
(
Constants
.
SERVER_SIDE_ENCRYPTION_ALGORITHM
,
S3AEncryptionMethods
.
SSE_S3
.
getMethod
(
)
)
;
conf
.
set
(
Constants
.
SERVER_SIDE_ENCRYPTION_KEY
,
)
;
S3AContract
contract
=
(
S3AContract
)
createContract
(
conf
)
;
contract
.
init
(
)
;
FileSystem
fileSystem
=
contract
.
getTestFileSystem
(
)
;
assertNotNull
(
,
fileSystem
)
;
URI
fsURI
=
fileSystem
.
getUri
(
)
;
tmpFile
.
delete
(
)
;
try
{
URI
localFileURI
=
tmpFile
.
toURI
(
)
;
FileSystem
localFS
=
FileSystem
.
get
(
localFileURI
,
getFileSystem
(
)
.
getConf
(
)
)
;
Path
localPath
=
new
Path
(
localFileURI
)
;
int
len
=
10
*
1024
;
byte
[
]
data
=
dataset
(
len
,
'A'
,
'Z'
)
;
writeDataset
(
localFS
,
localPath
,
data
,
len
,
1024
,
true
)
;
S3AFileSystem
s3a
=
getFileSystem
(
)
;
Path
remotePath
=
methodPath
(
)
;
verifyMetrics
(
(
)
->
{
s3a
.
copyFromLocalFile
(
false
,
true
,
localPath
,
remotePath
)
;
return
;
}
,
with
(
INVOCATION_COPY_FROM_LOCAL_FILE
,
1
)
,
with
(
OBJECT_PUT_REQUESTS
,
1
)
,
with
(
OBJECT_PUT_BYTES
,
len
)
)
;
verifyFileContents
(
s3a
,
remotePath
,
data
)
;
@
Test
public
void
testEmptyFileChecksums
(
)
throws
Throwable
{
assumeNoDefaultEncryption
(
)
;
final
S3AFileSystem
fs
=
getFileSystem
(
)
;
Path
file1
=
touchFile
(
)
;
EtagChecksum
checksum1
=
fs
.
getFileChecksum
(
file1
,
0
)
;
private
void
assertUploadsPresent
(
MultipartUtils
.
UploadIterator
list
,
Set
<
MultipartTestUtils
.
IdKey
>
ourUploads
)
throws
IOException
{
Set
<
MultipartTestUtils
.
IdKey
>
uploads
=
new
HashSet
<
>
(
ourUploads
)
;
while
(
list
.
hasNext
(
)
)
{
MultipartTestUtils
.
IdKey
listing
=
toIdKey
(
list
.
next
(
)
)
;
if
(
uploads
.
contains
(
listing
)
)
{
private
Path
writeEventuallyConsistentData
(
final
AmazonS3
s3ClientSpy
,
final
Path
testpath
,
final
byte
[
]
dataset
,
final
int
getObjectInconsistencyCount
,
final
int
getMetadataInconsistencyCount
,
final
int
copyInconsistencyCount
)
throws
IOException
{
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
,
1024
,
true
)
;
S3AFileStatus
originalStatus
=
(
S3AFileStatus
)
fs
.
getFileStatus
(
testpath
)
;
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
/
2
,
1024
,
true
)
;
private
Path
writeEventuallyConsistentData
(
final
AmazonS3
s3ClientSpy
,
final
Path
testpath
,
final
byte
[
]
dataset
,
final
int
getObjectInconsistencyCount
,
final
int
getMetadataInconsistencyCount
,
final
int
copyInconsistencyCount
)
throws
IOException
{
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
,
1024
,
true
)
;
S3AFileStatus
originalStatus
=
(
S3AFileStatus
)
fs
.
getFileStatus
(
testpath
)
;
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
/
2
,
1024
,
true
)
;
LOG
.
debug
(
,
testpath
,
originalStatus
.
getVersionId
(
)
,
originalStatus
.
getETag
(
)
)
;
S3AFileStatus
newStatus
=
(
S3AFileStatus
)
fs
.
getFileStatus
(
testpath
)
;
private
Path
writeEventuallyConsistentData
(
final
AmazonS3
s3ClientSpy
,
final
Path
testpath
,
final
byte
[
]
dataset
,
final
int
getObjectInconsistencyCount
,
final
int
getMetadataInconsistencyCount
,
final
int
copyInconsistencyCount
)
throws
IOException
{
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
,
1024
,
true
)
;
S3AFileStatus
originalStatus
=
(
S3AFileStatus
)
fs
.
getFileStatus
(
testpath
)
;
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
/
2
,
1024
,
true
)
;
LOG
.
debug
(
,
testpath
,
originalStatus
.
getVersionId
(
)
,
originalStatus
.
getETag
(
)
)
;
S3AFileStatus
newStatus
=
(
S3AFileStatus
)
fs
.
getFileStatus
(
testpath
)
;
LOG
.
debug
(
,
testpath
,
newStatus
.
getVersionId
(
)
,
newStatus
.
getETag
(
)
)
;
private
Path
writeEventuallyConsistentData
(
final
AmazonS3
s3ClientSpy
,
final
Path
testpath
,
final
byte
[
]
dataset
,
final
int
getObjectInconsistencyCount
,
final
int
getMetadataInconsistencyCount
,
final
int
copyInconsistencyCount
)
throws
IOException
{
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
,
1024
,
true
)
;
S3AFileStatus
originalStatus
=
(
S3AFileStatus
)
fs
.
getFileStatus
(
testpath
)
;
writeDataset
(
fs
,
testpath
,
dataset
,
dataset
.
length
/
2
,
1024
,
true
)
;
LOG
.
debug
(
,
testpath
,
originalStatus
.
getVersionId
(
)
,
originalStatus
.
getETag
(
)
)
;
S3AFileStatus
newStatus
=
(
S3AFileStatus
)
fs
.
getFileStatus
(
testpath
)
;
LOG
.
debug
(
,
testpath
,
newStatus
.
getVersionId
(
)
,
newStatus
.
getETag
(
)
)
;
LOG
.
debug
(
,
testpath
,
getMetadataInconsistencyCount
,
getObjectInconsistencyCount
)
;
stubTemporaryUnavailable
(
s3ClientSpy
,
getObjectInconsistencyCount
,
testpath
,
newStatus
)
;
stubTemporaryWrongVersion
(
s3ClientSpy
,
getObjectInconsistencyCount
,
testpath
,
originalStatus
)
;
if
(
versionCheckingIsOnServer
(
)
)
{
private
void
stubTemporaryUnavailable
(
AmazonS3
s3ClientSpy
,
int
inconsistentCallCount
,
Path
testpath
,
S3AFileStatus
newStatus
)
{
Answer
<
S3Object
>
temporarilyUnavailableAnswer
=
new
Answer
<
S3Object
>
(
)
{
private
int
callCount
=
0
;
@
Override
public
S3Object
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
callCount
++
;
if
(
callCount
<=
inconsistentCallCount
)
{
private
void
stubTemporaryWrongVersion
(
AmazonS3
s3ClientSpy
,
int
inconsistentCallCount
,
Path
testpath
,
S3AFileStatus
originalStatus
)
{
Answer
<
S3Object
>
temporarilyWrongVersionAnswer
=
new
Answer
<
S3Object
>
(
)
{
private
int
callCount
=
0
;
@
Override
public
S3Object
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
callCount
++
;
S3Object
s3Object
=
(
S3Object
)
invocation
.
callRealMethod
(
)
;
if
(
callCount
<=
inconsistentCallCount
)
{
private
void
stubTemporaryCopyInconsistency
(
AmazonS3
s3ClientSpy
,
Path
testpath
,
S3AFileStatus
newStatus
,
int
copyInconsistentCallCount
)
{
Answer
<
CopyObjectResult
>
temporarilyPreconditionsNotMetAnswer
=
new
Answer
<
CopyObjectResult
>
(
)
{
private
int
callCount
=
0
;
@
Override
public
CopyObjectResult
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
callCount
++
;
if
(
callCount
<=
copyInconsistentCallCount
)
{
String
message
=
+
callCount
+
+
copyInconsistentCallCount
;
private
void
stubTemporaryMetadataInconsistency
(
AmazonS3
s3ClientSpy
,
Path
testpath
,
S3AFileStatus
originalStatus
,
S3AFileStatus
newStatus
,
int
metadataInconsistentCallCount
)
{
Answer
<
ObjectMetadata
>
temporarilyOldMetadataAnswer
=
new
Answer
<
ObjectMetadata
>
(
)
{
private
int
callCount
=
0
;
@
Override
public
ObjectMetadata
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
ObjectMetadata
objectMetadata
=
(
ObjectMetadata
)
invocation
.
callRealMethod
(
)
;
callCount
++
;
if
(
callCount
<=
metadataInconsistentCallCount
)
{
private
void
stubTemporaryNotFound
(
AmazonS3
s3ClientSpy
,
int
inconsistentCallCount
,
Path
testpath
)
{
Answer
<
ObjectMetadata
>
notFound
=
new
Answer
<
ObjectMetadata
>
(
)
{
private
int
callCount
=
0
;
@
Override
public
ObjectMetadata
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
callCount
++
;
if
(
callCount
<=
inconsistentCallCount
)
{
Configuration
conf2
=
new
Configuration
(
conf
)
;
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
AWS_CREDENTIALS_PROVIDER
)
;
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
ACCESS_KEY
)
;
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
SECRET_KEY
)
;
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
SESSION_TOKEN
)
;
MarshalledCredentials
mc
=
fromSTSCredentials
(
sessionCreds
)
;
updateConfigWithSessionCreds
(
conf2
,
mc
)
;
conf2
.
set
(
AWS_CREDENTIALS_PROVIDER
,
TEMPORARY_AWS_CREDENTIALS
)
;
try
(
S3AFileSystem
fs
=
S3ATestUtils
.
createTestFileSystem
(
conf2
)
)
{
createAndVerifyFile
(
fs
,
path
(
)
,
TEST_FILE_SIZE
)
;
}
conf2
.
set
(
SESSION_TOKEN
,
+
sessionCreds
.
getSessionToken
(
)
)
;
try
(
S3AFileSystem
fs
=
S3ATestUtils
.
createTestFileSystem
(
conf2
)
)
{
createAndVerifyFile
(
fs
,
path
(
)
,
TEST_FILE_SIZE
)
;
fail
(
+
fs
.
getUri
(
)
+
+
fs
)
;
}
catch
(
AWSS3IOException
|
AWSBadRequestException
ex
)
{
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
AWS_CREDENTIALS_PROVIDER
)
;
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
ACCESS_KEY
)
;
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
SECRET_KEY
)
;
S3AUtils
.
clearBucketOption
(
conf2
,
bucket
,
SESSION_TOKEN
)
;
MarshalledCredentials
mc
=
fromSTSCredentials
(
sessionCreds
)
;
updateConfigWithSessionCreds
(
conf2
,
mc
)
;
conf2
.
set
(
AWS_CREDENTIALS_PROVIDER
,
TEMPORARY_AWS_CREDENTIALS
)
;
try
(
S3AFileSystem
fs
=
S3ATestUtils
.
createTestFileSystem
(
conf2
)
)
{
createAndVerifyFile
(
fs
,
path
(
)
,
TEST_FILE_SIZE
)
;
}
conf2
.
set
(
SESSION_TOKEN
,
+
sessionCreds
.
getSessionToken
(
)
)
;
try
(
S3AFileSystem
fs
=
S3ATestUtils
.
createTestFileSystem
(
conf2
)
)
{
createAndVerifyFile
(
fs
,
path
(
)
,
TEST_FILE_SIZE
)
;
fail
(
+
fs
.
getUri
(
)
+
+
fs
)
;
}
catch
(
AWSS3IOException
|
AWSBadRequestException
ex
)
{
LOG
.
info
(
,
ex
.
toString
(
)
)
;
@
Test
public
void
testSessionCredentialsRegionBadEndpoint
(
)
throws
Throwable
{
describe
(
)
;
IllegalArgumentException
ex
=
expectedSessionRequestFailure
(
IllegalArgumentException
.
class
,
,
EU_IRELAND
,
)
;
@
Test
public
void
testConsistentListLocatedStatusAfterPut
(
)
throws
Exception
{
final
S3AFileSystem
fs
=
getFileSystem
(
)
;
String
rootDir
=
;
fs
.
mkdirs
(
path
(
rootDir
)
)
;
final
int
[
]
numOfPaths
=
{
0
,
1
,
5
}
;
for
(
int
normalPathNum
:
numOfPaths
)
{
for
(
int
delayedPathNum
:
new
int
[
]
{
0
,
2
}
)
{
for
(
;
index
<
normalFileNum
;
index
++
)
{
fileNames
.
add
(
+
index
)
;
}
for
(
;
index
<
normalFileNum
+
delayedFileNum
;
index
++
)
{
fileNames
.
add
(
+
index
+
+
DEFAULT_DELAY_KEY_SUBSTRING
)
;
}
int
filesAndEmptyDirectories
=
0
;
for
(
Path
dir
:
testDirs
)
{
for
(
String
fileName
:
fileNames
)
{
writeTextFile
(
fs
,
new
Path
(
dir
,
fileName
)
,
+
fileName
,
false
)
;
filesAndEmptyDirectories
++
;
}
}
final
RemoteIterator
<
LocatedFileStatus
>
statusIterator
=
fs
.
listFiles
(
baseTestDir
,
recursive
)
;
final
Collection
<
Path
>
listedFiles
=
new
HashSet
<
>
(
)
;
for
(
;
statusIterator
.
hasNext
(
)
;
)
{
final
FileStatus
status
=
statusIterator
.
next
(
)
;
assertTrue
(
+
status
+
,
status
.
isFile
(
)
)
;
listedFiles
.
add
(
status
.
getPath
(
)
)
;
@
Test
public
void
testInconsistentS3ClientDeletes
(
)
throws
Throwable
{
describe
(
+
)
;
assumeV2ListAPI
(
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
Path
root
=
path
(
+
DEFAULT_DELAY_KEY_SUBSTRING
)
;
for
(
int
i
=
0
;
i
<
3
;
i
++
)
{
fs
.
mkdirs
(
new
Path
(
root
,
+
i
)
)
;
touch
(
fs
,
new
Path
(
root
,
+
i
)
)
;
for
(
int
j
=
0
;
j
<
3
;
j
++
)
{
touch
(
fs
,
new
Path
(
new
Path
(
root
,
+
i
)
,
+
i
+
+
j
)
)
;
}
}
clearInconsistency
(
fs
)
;
String
key
=
fs
.
pathToKey
(
root
)
+
;
LOG
.
info
(
)
;
ListObjectsV2Result
preDeleteDelimited
=
listObjectsV2
(
fs
,
key
,
)
;
ListObjectsV2Result
preDeleteUndelimited
=
listObjectsV2
(
fs
,
key
,
null
)
;
private
<
T
>
void
assertListSizeEqual
(
String
message
,
List
<
T
>
expected
,
List
<
T
>
actual
)
{
String
leftContents
=
expected
.
stream
(
)
.
map
(
n
->
n
.
toString
(
)
)
.
collect
(
Collectors
.
joining
(
)
)
;
String
rightContents
=
actual
.
stream
(
)
.
map
(
n
->
n
.
toString
(
)
)
.
collect
(
Collectors
.
joining
(
)
)
;
String
summary
=
+
leftContents
+
+
+
rightContents
+
;
if
(
expected
.
size
(
)
!=
actual
.
size
(
)
)
{
@
Test
public
void
testTombstoneExpiryGuardedDeleteRawCreate
(
)
throws
Exception
{
boolean
allowAuthoritative
=
authoritative
;
Path
testFilePath
=
path
(
+
UUID
.
randomUUID
(
)
+
)
;
final
AtomicLong
now
=
new
AtomicLong
(
1
)
;
final
AtomicLong
metadataTtl
=
new
AtomicLong
(
1
)
;
ITtlTimeProvider
testTimeProvider
=
new
ITtlTimeProvider
(
)
{
@
Override
public
long
getNow
(
)
{
return
now
.
get
(
)
;
}
@
Override
public
long
getMetadataTtl
(
)
{
return
metadataTtl
.
get
(
)
;
}
}
;
guardedFs
.
setTtlTimeProvider
(
testTimeProvider
)
;
createAndAwaitFs
(
guardedFs
,
testFilePath
,
originalText
)
;
deleteGuardedTombstoned
(
guardedFs
,
testFilePath
,
now
)
;
createAndAwaitFs
(
rawFS
,
testFilePath
,
newText
)
;
checkListingDoesNotContainPath
(
guardedFs
,
testFilePath
)
;
long
willExpire
=
now
.
get
(
)
+
metadataTtl
.
get
(
)
+
1L
;
now
.
set
(
willExpire
)
;
@
Override
public
long
getMetadataTtl
(
)
{
return
metadataTtl
.
get
(
)
;
}
}
;
guardedFs
.
setTtlTimeProvider
(
testTimeProvider
)
;
createAndAwaitFs
(
guardedFs
,
testFilePath
,
originalText
)
;
deleteGuardedTombstoned
(
guardedFs
,
testFilePath
,
now
)
;
createAndAwaitFs
(
rawFS
,
testFilePath
,
newText
)
;
checkListingDoesNotContainPath
(
guardedFs
,
testFilePath
)
;
long
willExpire
=
now
.
get
(
)
+
metadataTtl
.
get
(
)
+
1L
;
now
.
set
(
willExpire
)
;
LOG
.
info
(
,
willExpire
,
testTimeProvider
.
getNow
(
)
,
testTimeProvider
.
getMetadataTtl
(
)
)
;
if
(
authoritative
)
{
intercept
(
FileNotFoundException
.
class
,
testFilePath
.
toString
(
)
,
,
(
)
->
readBytesToString
(
guardedFs
,
testFilePath
,
newText
.
length
(
)
)
)
;
}
else
{
String
newRead
=
readBytesToString
(
guardedFs
,
testFilePath
,
newText
.
length
(
)
)
;
private
void
outOfBandDeletes
(
final
Path
testFilePath
,
final
boolean
allowAuthoritative
)
throws
Exception
{
try
{
String
text
=
;
writeTextFile
(
guardedFs
,
testFilePath
,
text
,
true
)
;
awaitFileStatus
(
rawFS
,
testFilePath
)
;
deleteFile
(
rawFS
,
testFilePath
)
;
FileStatus
status
=
guardedFs
.
getFileStatus
(
testFilePath
)
;
private
void
overwriteFile
(
String
firstText
,
String
secondText
)
throws
Exception
{
boolean
allowAuthoritative
=
authoritative
;
Path
testFilePath
=
path
(
+
UUID
.
randomUUID
(
)
)
;
private
void
overwriteFileInListing
(
String
firstText
,
String
secondText
)
throws
Exception
{
boolean
allowAuthoritative
=
authoritative
;
@
Test
public
void
testListingDelete
(
)
throws
Exception
{
boolean
allowAuthoritative
=
authoritative
;
String
testFile
=
testDir
+
+
rUUID
;
Path
testDirPath
=
path
(
testDir
)
;
Path
testFilePath
=
guardedFs
.
qualify
(
path
(
testFile
)
)
;
String
text
=
;
try
{
writeTextFile
(
guardedFs
,
testFilePath
,
text
,
true
)
;
awaitFileStatus
(
rawFS
,
testFilePath
)
;
final
FileStatus
[
]
origList
=
guardedFs
.
listStatus
(
testDirPath
)
;
assertEquals
(
+
,
1
,
origList
.
length
)
;
final
DirListingMetadata
dirListingMetadata
=
realMs
.
listChildren
(
guardedFs
.
qualify
(
testDirPath
)
)
;
assertListingAuthority
(
allowAuthoritative
,
dirListingMetadata
)
;
deleteFile
(
rawFS
,
testFilePath
)
;
interceptFuture
(
FileNotFoundException
.
class
,
,
rawFS
.
openFile
(
testFilePath
)
.
build
(
)
)
;
intercept
(
FileNotFoundException
.
class
,
(
)
->
rawFS
.
open
(
testFilePath
)
.
close
(
)
)
;
S3AFileStatus
status
=
(
S3AFileStatus
)
guardedFs
.
getFileStatus
(
testFilePath
)
;
@
Test
public
void
testDeleteIgnoresTombstones
(
)
throws
Throwable
{
describe
(
)
;
Path
dir
=
path
(
)
;
Path
testFilePath
=
new
Path
(
dir
,
)
;
createAndAwaitFs
(
guardedFs
,
testFilePath
,
)
;
@
Test
public
void
testDeleteIgnoresTombstones
(
)
throws
Throwable
{
describe
(
)
;
Path
dir
=
path
(
)
;
Path
testFilePath
=
new
Path
(
dir
,
)
;
createAndAwaitFs
(
guardedFs
,
testFilePath
,
)
;
LOG
.
info
(
,
dir
)
;
guardedFs
.
delete
(
dir
,
true
)
;
awaitDeletedFileDisappearance
(
guardedFs
,
testFilePath
)
;
createAndAwaitFs
(
rawFS
,
testFilePath
,
)
;
awaitListingContainsChild
(
rawFS
,
dir
,
testFilePath
)
;
Path
sibling
=
new
Path
(
dir
,
)
;
guardedFs
.
mkdirs
(
sibling
)
;
@
Test
public
void
testDeleteIgnoresTombstones
(
)
throws
Throwable
{
describe
(
)
;
Path
dir
=
path
(
)
;
Path
testFilePath
=
new
Path
(
dir
,
)
;
createAndAwaitFs
(
guardedFs
,
testFilePath
,
)
;
LOG
.
info
(
,
dir
)
;
guardedFs
.
delete
(
dir
,
true
)
;
awaitDeletedFileDisappearance
(
guardedFs
,
testFilePath
)
;
createAndAwaitFs
(
rawFS
,
testFilePath
,
)
;
awaitListingContainsChild
(
rawFS
,
dir
,
testFilePath
)
;
Path
sibling
=
new
Path
(
dir
,
)
;
guardedFs
.
mkdirs
(
sibling
)
;
LOG
.
info
(
,
dir
)
;
guardedFs
.
delete
(
dir
,
true
)
;
private
void
awaitListingContainsChild
(
S3AFileSystem
fs
,
final
Path
dir
,
final
Path
testFilePath
)
throws
Exception
{
@
Override
public
void
setAmazonS3Client
(
AmazonS3
client
)
{
static
void
cleanupParts
(
S3AFileSystem
fs
,
Set
<
IdKey
>
keySet
)
{
boolean
anyFailure
=
false
;
for
(
IdKey
ik
:
keySet
)
{
try
{
public
static
IdKey
createPartUpload
(
S3AFileSystem
fs
,
String
key
,
int
len
,
int
partNo
)
throws
IOException
{
WriteOperationHelper
writeHelper
=
fs
.
getWriteOperationHelper
(
)
;
byte
[
]
data
=
dataset
(
len
,
'a'
,
'z'
)
;
InputStream
in
=
new
ByteArrayInputStream
(
data
)
;
String
uploadId
=
writeHelper
.
initiateMultiPartUpload
(
key
)
;
UploadPartRequest
req
=
writeHelper
.
newUploadPartRequest
(
key
,
uploadId
,
partNo
,
len
,
in
,
null
,
0L
)
;
PartETag
partEtag
=
fs
.
uploadPart
(
req
)
.
getPartETag
(
)
;
public
static
void
clearAnyUploads
(
S3AFileSystem
fs
,
Path
path
)
{
try
{
String
key
=
fs
.
pathToKey
(
path
)
;
MultipartUtils
.
UploadIterator
uploads
=
fs
.
listUploads
(
key
)
;
while
(
uploads
.
hasNext
(
)
)
{
MultipartUpload
upload
=
uploads
.
next
(
)
;
fs
.
getWriteOperationHelper
(
)
.
abortMultipartUpload
(
upload
.
getKey
(
)
,
upload
.
getUploadId
(
)
,
LOG_EVENT
)
;
public
static
void
print
(
Logger
log
,
S3ATestUtils
.
MetricDiff
...
metrics
)
{
for
(
S3ATestUtils
.
MetricDiff
metric
:
metrics
)
{
@
Test
public
void
testCreateCredentialProvider
(
)
throws
IOException
{
describe
(
)
;
Configuration
conf
=
createValidRoleConf
(
)
;
try
(
AssumedRoleCredentialProvider
provider
=
new
AssumedRoleCredentialProvider
(
uri
,
conf
)
)
{
@
Test
public
void
testCreateCredentialProviderNoURI
(
)
throws
IOException
{
describe
(
)
;
Configuration
conf
=
createValidRoleConf
(
)
;
try
(
AssumedRoleCredentialProvider
provider
=
new
AssumedRoleCredentialProvider
(
null
,
conf
)
)
{
@
Test
public
void
testAssumeRoleCreateFS
(
)
throws
IOException
{
describe
(
)
;
String
roleARN
=
getAssumedRoleARN
(
)
;
Configuration
conf
=
createAssumedRoleConfig
(
roleARN
)
;
Path
path
=
new
Path
(
getFileSystem
(
)
.
getUri
(
)
)
;
SinglePendingCommit
pending
=
fullOperations
.
uploadFileToPendingCommit
(
src
,
dest
,
,
uploadPartSize
,
progress
)
;
pending
.
save
(
fs
,
new
Path
(
readOnlyDir
,
name
+
CommitConstants
.
PENDING_SUFFIX
)
,
true
)
;
assertTrue
(
src
.
delete
(
)
)
;
}
)
)
;
progress
.
assertCount
(
,
range
)
;
try
{
Pair
<
PendingSet
,
List
<
Pair
<
LocatedFileStatus
,
IOException
>>>
pendingCommits
=
operations
.
loadSinglePendingCommits
(
readOnlyDir
,
true
)
;
List
<
SinglePendingCommit
>
commits
=
pendingCommits
.
getLeft
(
)
.
getCommits
(
)
;
assertEquals
(
range
,
commits
.
size
(
)
)
;
try
(
CommitOperations
.
CommitContext
commitContext
=
operations
.
initiateCommitOperation
(
uploadDest
)
)
{
commits
.
parallelStream
(
)
.
forEach
(
(
c
)
->
{
CommitOperations
.
MaybeIOE
maybeIOE
=
commitContext
.
commit
(
c
,
)
;
Path
path
=
c
.
destinationPath
(
)
;
assertCommitAccessDenied
(
path
,
maybeIOE
)
;
}
)
;
@
Override
public
void
setup
(
)
throws
Exception
{
super
.
setup
(
)
;
regionName
=
determineRegion
(
getFileSystem
(
)
.
getBucket
(
)
)
;
protected
FileStatus
[
]
globFS
(
final
S3AFileSystem
fs
,
final
Path
path
,
final
PathFilter
filter
,
boolean
expectAuthFailure
,
final
int
expectedCount
)
throws
IOException
{
protected
FileStatus
[
]
globFS
(
final
S3AFileSystem
fs
,
final
Path
path
,
final
PathFilter
filter
,
boolean
expectAuthFailure
,
final
int
expectedCount
)
throws
IOException
{
LOG
.
info
(
,
path
)
;
S3ATestUtils
.
MetricDiff
getMetric
=
new
S3ATestUtils
.
MetricDiff
(
fs
,
Statistic
.
OBJECT_METADATA_REQUESTS
)
;
S3ATestUtils
.
MetricDiff
listMetric
=
new
S3ATestUtils
.
MetricDiff
(
fs
,
Statistic
.
OBJECT_LIST_REQUESTS
)
;
FileStatus
[
]
st
;
try
{
st
=
filter
==
null
?
fs
.
globStatus
(
path
)
:
fs
.
globStatus
(
path
,
filter
)
;
LOG
.
info
(
,
path
)
;
S3ATestUtils
.
MetricDiff
getMetric
=
new
S3ATestUtils
.
MetricDiff
(
fs
,
Statistic
.
OBJECT_METADATA_REQUESTS
)
;
S3ATestUtils
.
MetricDiff
listMetric
=
new
S3ATestUtils
.
MetricDiff
(
fs
,
Statistic
.
OBJECT_LIST_REQUESTS
)
;
FileStatus
[
]
st
;
try
{
st
=
filter
==
null
?
fs
.
globStatus
(
path
)
:
fs
.
globStatus
(
path
,
filter
)
;
LOG
.
info
(
,
getMetric
,
listMetric
)
;
if
(
expectAuthFailure
)
{
String
resultStr
;
if
(
st
==
null
)
{
resultStr
=
;
}
else
{
resultStr
=
StringUtils
.
join
(
st
,
)
;
}
fail
(
String
.
format
(
+
,
path
,
resultStr
)
)
;
}
}
catch
(
AccessDeniedException
e
)
{
public
static
Configuration
bindRolePolicy
(
final
Configuration
conf
,
final
Policy
policy
)
throws
JsonProcessingException
{
String
p
=
MODEL
.
toJson
(
policy
)
;
public
static
AbstractS3ATokenIdentifier
lookupToken
(
Credentials
submittedCredentials
,
URI
uri
,
Text
kind
)
throws
IOException
{
final
Token
<
AbstractS3ATokenIdentifier
>
token
=
requireNonNull
(
lookupS3ADelegationToken
(
submittedCredentials
,
uri
)
,
+
uri
)
;
assertEquals
(
+
token
,
kind
,
token
.
getKind
(
)
)
;
AbstractS3ATokenIdentifier
tid
=
token
.
decodeIdentifier
(
)
;
protected
void
enableDelegationTokens
(
Configuration
conf
,
String
binding
)
{
removeBaseAndBucketOverrides
(
conf
,
DELEGATION_TOKEN_BINDING
)
;
@
Test
public
void
testCreate10Tokens
(
)
throws
Throwable
{
File
file
=
fetchTokens
(
10
)
;
String
csv
=
FileUtils
.
readFileToString
(
file
,
)
;
catch
(
IOException
e
)
{
ex
=
e
;
}
timer
.
end
(
)
;
return
new
Outcome
(
id
,
startTime
,
timer
,
ex
)
;
}
)
;
}
NanoTimerStats
stats
=
new
NanoTimerStats
(
)
;
NanoTimerStats
success
=
new
NanoTimerStats
(
)
;
NanoTimerStats
throttled
=
new
NanoTimerStats
(
)
;
List
<
Outcome
>
throttledEvents
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
tokens
;
i
++
)
{
Outcome
outcome
=
completionService
.
take
(
)
.
get
(
)
;
ContractTestUtils
.
NanoTimer
timer
=
outcome
.
timer
;
Exception
ex
=
outcome
.
exception
;
outcome
.
writeln
(
csvout
)
;
stats
.
add
(
timer
)
;
NanoTimerStats
stats
=
new
NanoTimerStats
(
)
;
NanoTimerStats
success
=
new
NanoTimerStats
(
)
;
NanoTimerStats
throttled
=
new
NanoTimerStats
(
)
;
List
<
Outcome
>
throttledEvents
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
tokens
;
i
++
)
{
Outcome
outcome
=
completionService
.
take
(
)
.
get
(
)
;
ContractTestUtils
.
NanoTimer
timer
=
outcome
.
timer
;
Exception
ex
=
outcome
.
exception
;
outcome
.
writeln
(
csvout
)
;
stats
.
add
(
timer
)
;
if
(
ex
!=
null
)
{
LOG
.
info
(
,
i
,
ex
)
;
throttled
.
add
(
timer
)
;
throttledEvents
.
add
(
outcome
)
;
}
else
{
ContractTestUtils
.
NanoTimer
timer
=
outcome
.
timer
;
Exception
ex
=
outcome
.
exception
;
outcome
.
writeln
(
csvout
)
;
stats
.
add
(
timer
)
;
if
(
ex
!=
null
)
{
LOG
.
info
(
,
i
,
ex
)
;
throttled
.
add
(
timer
)
;
throttledEvents
.
add
(
outcome
)
;
}
else
{
success
.
add
(
timer
)
;
}
}
csvout
.
close
(
)
;
jobTimer
.
end
(
)
;
LOG
.
info
(
+
csvFile
)
;
LOG
.
info
(
,
tokens
,
throttled
.
getCount
(
)
,
stats
,
throttled
,
success
)
;
double
duration
=
jobTimer
.
duration
(
)
;
@
Test
public
void
testCommonCrawlLookup
(
)
throws
Throwable
{
FileSystem
resourceFS
=
EXTRA_JOB_RESOURCE_PATH
.
getFileSystem
(
getConfiguration
(
)
)
;
FileStatus
status
=
resourceFS
.
getFileStatus
(
EXTRA_JOB_RESOURCE_PATH
)
;
job
.
setOutputKeyClass
(
Text
.
class
)
;
job
.
setOutputValueClass
(
IntWritable
.
class
)
;
FileInputFormat
.
addInputPath
(
job
,
input
)
;
FileOutputFormat
.
setOutputPath
(
job
,
output
)
;
job
.
setMaxMapAttempts
(
1
)
;
job
.
setMaxReduceAttempts
(
1
)
;
URI
partitionUri
=
new
URI
(
EXTRA_JOB_RESOURCE_PATH
.
toString
(
)
+
)
;
job
.
addCacheFile
(
partitionUri
)
;
describe
(
,
output
)
;
job
.
submit
(
)
;
final
JobStatus
status
=
job
.
getStatus
(
)
;
assertEquals
(
,
MockJob
.
NAME
,
status
.
getSchedulingInfo
(
)
)
;
assertEquals
(
,
JobStatus
.
State
.
RUNNING
,
status
.
getState
(
)
)
;
final
Credentials
submittedCredentials
=
requireNonNull
(
job
.
getSubmittedCredentials
(
)
,
)
;
final
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
=
submittedCredentials
.
getAllTokens
(
)
;
FileInputFormat
.
addInputPath
(
job
,
input
)
;
FileOutputFormat
.
setOutputPath
(
job
,
output
)
;
job
.
setMaxMapAttempts
(
1
)
;
job
.
setMaxReduceAttempts
(
1
)
;
URI
partitionUri
=
new
URI
(
EXTRA_JOB_RESOURCE_PATH
.
toString
(
)
+
)
;
job
.
addCacheFile
(
partitionUri
)
;
describe
(
,
output
)
;
job
.
submit
(
)
;
final
JobStatus
status
=
job
.
getStatus
(
)
;
assertEquals
(
,
MockJob
.
NAME
,
status
.
getSchedulingInfo
(
)
)
;
assertEquals
(
,
JobStatus
.
State
.
RUNNING
,
status
.
getState
(
)
)
;
final
Credentials
submittedCredentials
=
requireNonNull
(
job
.
getSubmittedCredentials
(
)
,
)
;
final
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
=
submittedCredentials
.
getAllTokens
(
)
;
LOG
.
info
(
,
tokens
.
size
(
)
)
;
for
(
Token
<
?
extends
TokenIdentifier
>
token
:
tokens
)
{
@
Test
public
void
testCreateRoleModel
(
)
throws
Throwable
{
describe
(
)
;
EnumSet
<
AWSPolicyProvider
.
AccessLevel
>
access
=
EnumSet
.
of
(
AWSPolicyProvider
.
AccessLevel
.
READ
,
AWSPolicyProvider
.
AccessLevel
.
WRITE
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
List
<
RoleModel
.
Statement
>
rules
=
fs
.
listAWSPolicyRules
(
access
)
;
assertTrue
(
,
!
rules
.
isEmpty
(
)
)
;
String
ruleset
=
new
RoleModel
(
)
.
toJson
(
new
RoleModel
.
Policy
(
rules
)
)
;
@
Test
public
void
testAddTokensFromFileSystem
(
)
throws
Throwable
{
describe
(
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
Credentials
cred
=
new
Credentials
(
)
;
Token
<
?
>
[
]
tokens
=
fs
.
addDelegationTokens
(
YARN_RM
,
cred
)
;
assertEquals
(
,
1
,
tokens
.
length
)
;
Token
<
?
>
token
=
requireNonNull
(
tokens
[
0
]
,
)
;
@
Test
public
void
testCanRetrieveTokenFromCurrentUserCreds
(
)
throws
Throwable
{
describe
(
+
)
;
delegationTokens
.
start
(
)
;
Credentials
cred
=
createDelegationTokens
(
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
ugi
.
addCredentials
(
cred
)
;
Token
<
?
>
[
]
tokens
=
cred
.
getAllTokens
(
)
.
toArray
(
new
Token
<
?
>
[
0
]
)
;
Token
<
?
>
token0
=
tokens
[
0
]
;
Text
service
=
token0
.
getService
(
)
;
final
Text
tokenKind
=
getTokenKind
(
)
;
AbstractS3ATokenIdentifier
origTokenId
=
requireNonNull
(
lookupToken
(
creds
,
uri
,
tokenKind
)
,
)
;
final
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
currentUser
.
addCredentials
(
creds
)
;
requireNonNull
(
lookupToken
(
currentUser
.
getCredentials
(
)
,
uri
,
tokenKind
)
,
)
;
Configuration
conf
=
new
Configuration
(
getConfiguration
(
)
)
;
String
bucket
=
fs
.
getBucket
(
)
;
disableFilesystemCaching
(
conf
)
;
unsetHadoopCredentialProviders
(
conf
)
;
removeBaseAndBucketOverrides
(
bucket
,
conf
,
ACCESS_KEY
,
SECRET_KEY
,
SESSION_TOKEN
,
SERVER_SIDE_ENCRYPTION_ALGORITHM
,
SERVER_SIDE_ENCRYPTION_KEY
,
DELEGATION_TOKEN_ROLE_ARN
,
DELEGATION_TOKEN_ENDPOINT
)
;
conf
.
set
(
DELEGATION_TOKEN_ENDPOINT
,
)
;
bindProviderList
(
bucket
,
conf
,
CountInvocationsProvider
.
NAME
)
;
long
originalCount
=
CountInvocationsProvider
.
getInvocationCount
(
)
;
Path
testPath
=
path
(
)
;
try
(
S3AFileSystem
delegatedFS
=
newS3AInstance
(
uri
,
conf
)
)
{
@
Test
public
void
testHDFSFetchDTCommand
(
)
throws
Throwable
{
describe
(
)
;
ExitUtil
.
disableSystemExit
(
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
Configuration
conf
=
fs
.
getConf
(
)
;
URI
fsUri
=
fs
.
getUri
(
)
;
String
fsurl
=
fsUri
.
toString
(
)
;
File
tokenfile
=
createTempTokenFile
(
)
;
String
tokenFilePath
=
tokenfile
.
getAbsolutePath
(
)
;
doAs
(
bobUser
,
(
)
->
DelegationTokenFetcher
.
main
(
conf
,
args
(
,
fsurl
,
tokenFilePath
)
)
)
;
assertTrue
(
+
tokenfile
,
tokenfile
.
exists
(
)
)
;
String
s
=
DelegationTokenFetcher
.
printTokensToString
(
conf
,
new
Path
(
tokenfile
.
toURI
(
)
)
,
false
)
;
protected
String
dtutil
(
int
expected
,
String
...
args
)
throws
Exception
{
final
ByteArrayOutputStream
dtUtilContent
=
new
ByteArrayOutputStream
(
)
;
DtUtilShell
dt
=
new
DtUtilShell
(
)
;
dt
.
setOut
(
new
PrintStream
(
dtUtilContent
)
)
;
dtUtilContent
.
reset
(
)
;
int
r
=
doAs
(
aliceUser
,
(
)
->
ToolRunner
.
run
(
getConfiguration
(
)
,
dt
,
args
)
)
;
String
s
=
dtUtilContent
.
toString
(
)
;
@
SuppressWarnings
(
)
protected
AbstractS3ATokenIdentifier
verifyCredentialPropagation
(
final
S3AFileSystem
fs
,
final
MarshalledCredentials
session
,
final
Configuration
conf
)
throws
Exception
{
describe
(
)
;
unsetHadoopCredentialProviders
(
conf
)
;
conf
.
set
(
DELEGATION_TOKEN_CREDENTIALS_PROVIDER
,
TemporaryAWSCredentialsProvider
.
NAME
)
;
session
.
setSecretsInConfiguration
(
conf
)
;
try
(
S3ADelegationTokens
delegationTokens2
=
new
S3ADelegationTokens
(
)
)
{
delegationTokens2
.
bindToFileSystem
(
fs
.
getCanonicalUri
(
)
,
fs
.
createStoreContext
(
)
,
fs
.
createDelegationOperations
(
)
)
;
delegationTokens2
.
init
(
conf
)
;
delegationTokens2
.
start
(
)
;
final
Token
<
AbstractS3ATokenIdentifier
>
newDT
=
delegationTokens2
.
getBoundOrNewDT
(
new
EncryptionSecrets
(
)
,
null
)
;
delegationTokens2
.
resetTokenBindingToDT
(
newDT
)
;
final
AbstractS3ATokenIdentifier
boundId
=
delegationTokens2
.
getDecodedIdentifier
(
)
.
get
(
)
;
public
static
SuccessData
validateSuccessFile
(
final
Path
outputPath
,
final
String
committerName
,
final
S3AFileSystem
fs
,
final
String
origin
,
final
int
minimumFileCount
)
throws
IOException
{
SuccessData
successData
=
loadSuccessFile
(
fs
,
outputPath
,
origin
)
;
String
commitDetails
=
successData
.
toString
(
)
;
public
static
SuccessData
validateSuccessFile
(
final
Path
outputPath
,
final
String
committerName
,
final
S3AFileSystem
fs
,
final
String
origin
,
final
int
minimumFileCount
)
throws
IOException
{
SuccessData
successData
=
loadSuccessFile
(
fs
,
outputPath
,
origin
)
;
String
commitDetails
=
successData
.
toString
(
)
;
LOG
.
info
(
+
committerName
+
,
commitDetails
)
;
public
static
SuccessData
validateSuccessFile
(
final
Path
outputPath
,
final
String
committerName
,
final
S3AFileSystem
fs
,
final
String
origin
,
final
int
minimumFileCount
)
throws
IOException
{
SuccessData
successData
=
loadSuccessFile
(
fs
,
outputPath
,
origin
)
;
String
commitDetails
=
successData
.
toString
(
)
;
LOG
.
info
(
+
committerName
+
,
commitDetails
)
;
LOG
.
info
(
,
successData
.
dumpMetrics
(
,
,
)
)
;
public
static
SuccessData
loadSuccessFile
(
final
S3AFileSystem
fs
,
final
Path
outputPath
,
final
String
origin
)
throws
IOException
{
ContractTestUtils
.
assertPathExists
(
fs
,
+
outputPath
+
+
origin
+
,
outputPath
)
;
Path
success
=
new
Path
(
outputPath
,
_SUCCESS
)
;
FileStatus
status
=
ContractTestUtils
.
verifyPathExists
(
fs
,
+
success
+
+
origin
+
,
success
)
;
assertTrue
(
+
origin
+
+
status
,
status
.
isFile
(
)
)
;
assertTrue
(
+
success
+
+
origin
+
,
status
.
getLen
(
)
>
0
)
;
protected
Configuration
patchConfigurationForCommitter
(
final
Configuration
jobConf
)
{
jobConf
.
setBoolean
(
FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES
,
isUniqueFilenames
(
)
)
;
bindCommitter
(
jobConf
,
CommitConstants
.
S3A_COMMITTER_FACTORY
,
committerName
(
)
)
;
jobConf
.
setBoolean
(
KEY_SCALE_TESTS_ENABLED
,
isScaleTest
(
)
)
;
String
staging
=
stagingFilesDir
.
getRoot
(
)
.
getAbsolutePath
(
)
;
private
void
verifyCommitExists
(
SinglePendingCommit
commit
)
throws
FileNotFoundException
,
ValidationFailure
,
IOException
{
commit
.
validate
(
)
;
Path
path
=
getFileSystem
(
)
.
keyToQualifiedPath
(
commit
.
getDestinationKey
(
)
)
;
FileStatus
status
=
getFileSystem
(
)
.
getFileStatus
(
path
)
;
private
Path
validatePendingCommitData
(
String
filename
,
Path
magicFile
)
throws
IOException
{
S3AFileSystem
fs
=
getFileSystem
(
)
;
Path
pendingDataPath
=
new
Path
(
magicFile
.
getParent
(
)
,
filename
+
PENDING_SUFFIX
)
;
FileStatus
fileStatus
=
verifyPathExists
(
fs
,
,
pendingDataPath
)
;
assertTrue
(
+
fileStatus
,
fileStatus
.
getLen
(
)
>
0
)
;
String
data
=
read
(
fs
,
pendingDataPath
)
;
Path
destFile3
=
new
Path
(
subdir
,
)
;
List
<
Path
>
destinations
=
Lists
.
newArrayList
(
destFile1
,
destFile2
,
destFile3
)
;
List
<
SinglePendingCommit
>
commits
=
new
ArrayList
<
>
(
3
)
;
for
(
Path
destination
:
destinations
)
{
SinglePendingCommit
commit1
=
actions
.
uploadFileToPendingCommit
(
localFile
,
destination
,
null
,
DEFAULT_MULTIPART_SIZE
,
progress
)
;
commits
.
add
(
commit1
)
;
}
resetFailures
(
)
;
assertPathDoesNotExist
(
,
destDir
)
;
assertPathDoesNotExist
(
,
subdir
)
;
LOG
.
info
(
)
;
try
(
CommitOperations
.
CommitContext
commitContext
=
actions
.
initiateCommitOperation
(
destDir
)
)
{
MetricDiff
writes
=
new
MetricDiff
(
fs
,
Statistic
.
S3GUARD_METADATASTORE_RECORD_WRITES
)
;
LOG
.
info
(
)
;
commitContext
.
commitOrFail
(
commits
.
get
(
0
)
)
;
final
String
firstCommitContextString
=
commitContext
.
toString
(
)
;
public
static
AmazonS3
newMockS3Client
(
final
ClientResults
results
,
final
ClientErrors
errors
)
{
AmazonS3Client
mockClient
=
mock
(
AmazonS3Client
.
class
)
;
final
Object
lock
=
new
Object
(
)
;
when
(
mockClient
.
initiateMultipartUpload
(
any
(
InitiateMultipartUploadRequest
.
class
)
)
)
.
thenAnswer
(
invocation
->
{
when
(
mockClient
.
initiateMultipartUpload
(
any
(
InitiateMultipartUploadRequest
.
class
)
)
)
.
thenAnswer
(
invocation
->
{
LOG
.
debug
(
,
mockClient
)
;
synchronized
(
lock
)
{
if
(
results
.
requests
.
size
(
)
==
errors
.
failOnInit
)
{
if
(
errors
.
recover
)
{
errors
.
failOnInit
(
-
1
)
;
}
throw
new
AmazonClientException
(
+
results
.
requests
.
size
(
)
)
;
}
String
uploadId
=
UUID
.
randomUUID
(
)
.
toString
(
)
;
InitiateMultipartUploadRequest
req
=
getArgumentAt
(
invocation
,
0
,
InitiateMultipartUploadRequest
.
class
)
;
results
.
requests
.
put
(
uploadId
,
req
)
;
results
.
activeUploads
.
put
(
uploadId
,
req
.
getKey
(
)
)
;
results
.
uploads
.
add
(
uploadId
)
;
return
newResult
(
results
.
requests
.
get
(
uploadId
)
,
uploadId
)
;
}
}
)
;
when
(
mockClient
.
uploadPart
(
any
(
UploadPartRequest
.
class
)
)
)
.
thenAnswer
(
invocation
->
{
when
(
mockClient
.
uploadPart
(
any
(
UploadPartRequest
.
class
)
)
)
.
thenAnswer
(
invocation
->
{
LOG
.
debug
(
,
mockClient
)
;
synchronized
(
lock
)
{
if
(
results
.
parts
.
size
(
)
==
errors
.
failOnUpload
)
{
if
(
errors
.
recover
)
{
errors
.
failOnUpload
(
-
1
)
;
}
LOG
.
info
(
)
;
throw
new
AmazonClientException
(
+
results
.
parts
.
size
(
)
)
;
}
UploadPartRequest
req
=
getArgumentAt
(
invocation
,
0
,
UploadPartRequest
.
class
)
;
results
.
parts
.
add
(
req
)
;
String
etag
=
UUID
.
randomUUID
(
)
.
toString
(
)
;
List
<
String
>
etags
=
results
.
tagsByUpload
.
get
(
req
.
getUploadId
(
)
)
;
if
(
etags
==
null
)
{
etags
=
Lists
.
newArrayList
(
)
;
results
.
tagsByUpload
.
put
(
req
.
getUploadId
(
)
,
etags
)
;
List
<
String
>
etags
=
results
.
tagsByUpload
.
get
(
req
.
getUploadId
(
)
)
;
if
(
etags
==
null
)
{
etags
=
Lists
.
newArrayList
(
)
;
results
.
tagsByUpload
.
put
(
req
.
getUploadId
(
)
,
etags
)
;
}
etags
.
add
(
etag
)
;
return
newResult
(
req
,
etag
)
;
}
}
)
;
when
(
mockClient
.
completeMultipartUpload
(
any
(
CompleteMultipartUploadRequest
.
class
)
)
)
.
thenAnswer
(
invocation
->
{
LOG
.
debug
(
,
mockClient
)
;
synchronized
(
lock
)
{
if
(
results
.
commits
.
size
(
)
==
errors
.
failOnCommit
)
{
if
(
errors
.
recover
)
{
errors
.
failOnCommit
(
-
1
)
;
}
throw
new
AmazonClientException
(
+
results
.
commits
.
size
(
)
)
;
}
CompleteMultipartUploadRequest
req
=
getArgumentAt
(
invocation
,
0
,
CompleteMultipartUploadRequest
.
class
)
;
if
(
results
.
commits
.
size
(
)
==
errors
.
failOnCommit
)
{
if
(
errors
.
recover
)
{
errors
.
failOnCommit
(
-
1
)
;
}
throw
new
AmazonClientException
(
+
results
.
commits
.
size
(
)
)
;
}
CompleteMultipartUploadRequest
req
=
getArgumentAt
(
invocation
,
0
,
CompleteMultipartUploadRequest
.
class
)
;
String
uploadId
=
req
.
getUploadId
(
)
;
removeUpload
(
results
,
uploadId
)
;
results
.
commits
.
add
(
req
)
;
return
newResult
(
req
)
;
}
}
)
;
doAnswer
(
invocation
->
{
LOG
.
debug
(
,
mockClient
)
;
synchronized
(
lock
)
{
if
(
results
.
aborts
.
size
(
)
==
errors
.
failOnAbort
)
{
if
(
errors
.
recover
)
{
CompleteMultipartUploadRequest
req
=
getArgumentAt
(
invocation
,
0
,
CompleteMultipartUploadRequest
.
class
)
;
String
uploadId
=
req
.
getUploadId
(
)
;
removeUpload
(
results
,
uploadId
)
;
results
.
commits
.
add
(
req
)
;
return
newResult
(
req
)
;
}
}
)
;
doAnswer
(
invocation
->
{
LOG
.
debug
(
,
mockClient
)
;
synchronized
(
lock
)
{
if
(
results
.
aborts
.
size
(
)
==
errors
.
failOnAbort
)
{
if
(
errors
.
recover
)
{
errors
.
failOnAbort
(
-
1
)
;
}
throw
new
AmazonClientException
(
+
results
.
aborts
.
size
(
)
)
;
}
AbortMultipartUploadRequest
req
=
getArgumentAt
(
invocation
,
0
,
AbortMultipartUploadRequest
.
class
)
;
String
id
=
req
.
getUploadId
(
)
;
protected
void
describe
(
String
text
,
Object
...
args
)
{
@
Test
public
void
testValidateDefaultConflictMode
(
)
throws
Throwable
{
Configuration
baseConf
=
new
Configuration
(
true
)
;
String
[
]
sources
=
baseConf
.
getPropertySources
(
FS_S3A_COMMITTER_STAGING_CONFLICT_MODE
)
;
String
sourceStr
=
Arrays
.
stream
(
sources
)
.
collect
(
Collectors
.
joining
(
)
)
;
completedStage
(
,
d
)
;
}
)
;
final
StringBuilder
results
=
new
StringBuilder
(
)
;
results
.
append
(
)
;
Consumer
<
String
>
stage
=
(
s
)
->
{
DurationInfo
duration
=
completedStages
.
get
(
s
)
;
results
.
append
(
String
.
format
(
,
s
,
duration
==
null
?
:
duration
)
)
;
}
;
stage
.
accept
(
)
;
stage
.
accept
(
)
;
stage
.
accept
(
)
;
stage
.
accept
(
)
;
String
text
=
results
.
toString
(
)
;
File
resultsFile
=
File
.
createTempFile
(
,
)
;
FileUtils
.
write
(
resultsFile
,
text
,
StandardCharsets
.
UTF_8
)
;
protected
void
dumpOutputTree
(
Path
path
)
throws
Exception
{
@
Test
public
void
testRenameDirFailsInDelete
(
)
throws
Throwable
{
describe
(
,
multiDelete
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
roleFS
.
mkdirs
(
writableDir
)
;
List
<
Path
>
createdFiles
=
createFiles
(
fs
,
readOnlyDir
,
dirDepth
,
fileCount
,
dirCount
)
;
int
expectedFileCount
=
createdFiles
.
size
(
)
;
assertFileCount
(
,
roleFS
,
readOnlyDir
,
expectedFileCount
)
;
private
AccessDeniedException
expectRenameForbidden
(
Path
src
,
Path
dest
)
throws
Exception
{
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
true
,
,
src
,
dest
)
)
{
return
forbidden
(
+
src
+
+
dest
,
,
(
)
->
{
boolean
result
=
roleFS
.
rename
(
src
,
dest
)
;
private
AccessDeniedException
expectRenameForbidden
(
Path
src
,
Path
dest
)
throws
Exception
{
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
true
,
,
src
,
dest
)
)
{
return
forbidden
(
+
src
+
+
dest
,
,
(
)
->
{
boolean
result
=
roleFS
.
rename
(
src
,
dest
)
;
LOG
.
error
(
,
result
)
;
private
AccessDeniedException
expectRenameForbidden
(
Path
src
,
Path
dest
)
throws
Exception
{
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
true
,
,
src
,
dest
)
)
{
return
forbidden
(
+
src
+
+
dest
,
,
(
)
->
{
boolean
result
=
roleFS
.
rename
(
src
,
dest
)
;
LOG
.
error
(
,
result
)
;
LOG
.
error
(
,
ContractTestUtils
.
ls
(
getFileSystem
(
)
,
src
.
getParent
(
)
)
)
;
private
<
T
>
List
<
T
>
dump
(
List
<
T
>
l
)
{
int
c
=
1
;
for
(
T
t
:
l
)
{
public
<
T
>
T
exec
(
Callable
<
T
>
eval
,
ExpectedProbe
...
expectedA
)
throws
Exception
{
List
<
ExpectedProbe
>
expected
=
Arrays
.
asList
(
expectedA
)
;
resetMetricDiffs
(
)
;
assumeProbesEnabled
(
expected
)
;
T
r
=
eval
.
call
(
)
;
String
text
=
+
(
r
!=
null
?
r
.
toString
(
)
:
)
;
public
<
T
>
T
exec
(
Callable
<
T
>
eval
,
ExpectedProbe
...
expectedA
)
throws
Exception
{
List
<
ExpectedProbe
>
expected
=
Arrays
.
asList
(
expectedA
)
;
resetMetricDiffs
(
)
;
assumeProbesEnabled
(
expected
)
;
T
r
=
eval
.
call
(
)
;
String
text
=
+
(
r
!=
null
?
r
.
toString
(
)
:
)
;
LOG
.
info
(
,
text
)
;
LOG
.
info
(
,
this
)
;
private
void
doTestBatchWrite
(
int
numDelete
,
int
numPut
,
DynamoDBMetadataStore
ms
)
throws
IOException
{
Path
path
=
new
Path
(
+
numDelete
+
'_'
+
numPut
)
;
final
Path
root
=
fileSystem
.
makeQualified
(
path
)
;
final
Path
oldDir
=
new
Path
(
root
,
)
;
final
Path
newDir
=
new
Path
(
root
,
)
;
newMetas
.
add
(
new
PathMetadata
(
basicFileStatus
(
new
Path
(
newDir
,
+
i
)
,
i
,
false
)
)
)
;
}
Collection
<
Path
>
pathsToDelete
=
null
;
if
(
oldMetas
!=
null
)
{
ms
.
put
(
new
DirListingMetadata
(
oldDir
,
oldMetas
,
false
)
,
UNCHANGED_ENTRIES
,
putState
)
;
assertEquals
(
,
0
,
ms
.
listChildren
(
newDir
)
.
withoutTombstones
(
)
.
numEntries
(
)
)
;
Assertions
.
assertThat
(
ms
.
listChildren
(
oldDir
)
.
getListing
(
)
)
.
describedAs
(
)
.
containsExactlyInAnyOrderElementsOf
(
oldMetas
)
;
assertTrue
(
CollectionUtils
.
isEqualCollection
(
oldMetas
,
ms
.
listChildren
(
oldDir
)
.
getListing
(
)
)
)
;
pathsToDelete
=
new
ArrayList
<
>
(
oldMetas
.
size
(
)
)
;
for
(
PathMetadata
meta
:
oldMetas
)
{
pathsToDelete
.
add
(
meta
.
getFileStatus
(
)
.
getPath
(
)
)
;
}
}
AncestorState
state
=
checkNotNull
(
ms
.
initiateBulkWrite
(
BulkOperationState
.
OperationType
.
Put
,
newDir
)
,
)
;
assertEquals
(
,
newDir
,
state
.
getDest
(
)
)
;
ThrottleTracker
throttleTracker
=
new
ThrottleTracker
(
ms
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
true
,
)
)
{
ms
.
move
(
pathsToDelete
,
newMetas
,
state
)
;
String
subdir
=
base
+
;
Path
subDirPath
=
strToPath
(
subdir
)
;
createNewDirs
(
base
,
subdir
)
;
String
subFile
=
subdir
+
;
Path
subFilePath
=
strToPath
(
subFile
)
;
putListStatusFiles
(
subdir
,
true
,
subFile
)
;
final
DDBPathMetadata
subDirMetadataOrig
=
ms
.
get
(
subDirPath
)
;
Assertions
.
assertThat
(
subDirMetadataOrig
.
isAuthoritativeDir
(
)
)
.
describedAs
(
,
subDirMetadataOrig
)
.
isTrue
(
)
;
long
now
=
getTime
(
)
;
long
oldTime
=
now
-
MINUTE
;
putFile
(
subdir
,
oldTime
,
null
)
;
getFile
(
subdir
)
;
Path
basePath
=
strToPath
(
base
)
;
DirListingMetadata
listing
=
ms
.
listChildren
(
basePath
)
;
String
childText
=
listing
.
prettyPrint
(
)
;
@
Test
public
void
testDumpTable
(
)
throws
Throwable
{
describe
(
)
;
String
target
=
System
.
getProperty
(
,
)
;
File
buildDir
=
new
File
(
target
)
.
getAbsoluteFile
(
)
;
String
name
=
;
File
destFile
=
new
File
(
buildDir
,
name
)
;
DumpS3GuardDynamoTable
.
dumpStore
(
null
,
ddbmsStatic
,
getFileSystem
(
)
.
getConf
(
)
,
destFile
,
fsUri
)
;
File
storeFile
=
new
File
(
buildDir
,
name
+
DumpS3GuardDynamoTable
.
SCAN_CSV
)
;
try
(
BufferedReader
in
=
new
BufferedReader
(
new
InputStreamReader
(
new
FileInputStream
(
storeFile
)
,
Charset
.
forName
(
)
)
)
)
{
for
(
String
line
:
org
.
apache
.
commons
.
io
.
IOUtils
.
readLines
(
in
)
)
{
@
Test
public
void
test_900_instrumentation
(
)
throws
Throwable
{
describe
(
)
;
Assume
.
assumeTrue
(
,
expectThrottling
(
)
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
String
fsSummary
=
fs
.
toString
(
)
;
S3AStorageStatistics
statistics
=
fs
.
getStorageStatistics
(
)
;
for
(
StorageStatistics
.
LongStatistic
statistic
:
statistics
)
{
@
Test
public
void
test_999_delete_all_entries
(
)
throws
Throwable
{
describe
(
)
;
S3GuardTableAccess
tableAccess
=
new
S3GuardTableAccess
(
ddbms
)
;
ExpressionSpecBuilder
builder
=
new
ExpressionSpecBuilder
(
)
;
final
String
path
=
;
builder
.
withCondition
(
ExpressionSpecBuilder
.
S
(
PARENT
)
.
beginsWith
(
path
)
)
;
Iterable
<
DDBPathMetadata
>
entries
=
ddbms
.
wrapWithRetries
(
tableAccess
.
scanMetadata
(
builder
)
)
;
List
<
Path
>
list
=
new
ArrayList
<
>
(
)
;
try
{
entries
.
iterator
(
)
.
forEachRemaining
(
e
->
{
Path
p
=
e
.
getFileStatus
(
)
.
getPath
(
)
;
final
ExecutorService
executorService
=
Executors
.
newFixedThreadPool
(
THREADS
)
;
final
List
<
Callable
<
ExecutionOutcome
>>
tasks
=
new
ArrayList
<
>
(
THREADS
)
;
final
AtomicInteger
throttleExceptions
=
new
AtomicInteger
(
0
)
;
for
(
int
i
=
0
;
i
<
THREADS
;
i
++
)
{
tasks
.
add
(
(
)
->
{
final
ExecutionOutcome
outcome
=
new
ExecutionOutcome
(
)
;
final
ContractTestUtils
.
NanoTimer
t
=
new
ContractTestUtils
.
NanoTimer
(
)
;
for
(
int
j
=
0
;
j
<
operationsPerThread
;
j
++
)
{
if
(
tracker
.
isThrottlingDetected
(
)
||
throttleExceptions
.
get
(
)
>
0
)
{
outcome
.
skipped
=
true
;
return
outcome
;
}
try
{
action
.
call
(
)
;
outcome
.
completed
++
;
}
catch
(
AWSServiceThrottledException
e
)
{
final
List
<
Callable
<
ExecutionOutcome
>>
tasks
=
new
ArrayList
<
>
(
THREADS
)
;
final
AtomicInteger
throttleExceptions
=
new
AtomicInteger
(
0
)
;
for
(
int
i
=
0
;
i
<
THREADS
;
i
++
)
{
tasks
.
add
(
(
)
->
{
final
ExecutionOutcome
outcome
=
new
ExecutionOutcome
(
)
;
final
ContractTestUtils
.
NanoTimer
t
=
new
ContractTestUtils
.
NanoTimer
(
)
;
for
(
int
j
=
0
;
j
<
operationsPerThread
;
j
++
)
{
if
(
tracker
.
isThrottlingDetected
(
)
||
throttleExceptions
.
get
(
)
>
0
)
{
outcome
.
skipped
=
true
;
return
outcome
;
}
try
{
action
.
call
(
)
;
outcome
.
completed
++
;
}
catch
(
AWSServiceThrottledException
e
)
{
LOG
.
info
(
+
e
,
j
,
e
)
;
final
ContractTestUtils
.
NanoTimer
t
=
new
ContractTestUtils
.
NanoTimer
(
)
;
for
(
int
j
=
0
;
j
<
operationsPerThread
;
j
++
)
{
if
(
tracker
.
isThrottlingDetected
(
)
||
throttleExceptions
.
get
(
)
>
0
)
{
outcome
.
skipped
=
true
;
return
outcome
;
}
try
{
action
.
call
(
)
;
outcome
.
completed
++
;
}
catch
(
AWSServiceThrottledException
e
)
{
LOG
.
info
(
+
e
,
j
,
e
)
;
LOG
.
debug
(
e
.
toString
(
)
,
e
)
;
throttleExceptions
.
incrementAndGet
(
)
;
outcome
.
throttleExceptions
.
add
(
e
)
;
outcome
.
throttled
++
;
}
catch
(
Exception
e
)
{
}
catch
(
AWSServiceThrottledException
e
)
{
LOG
.
info
(
+
e
,
j
,
e
)
;
LOG
.
debug
(
e
.
toString
(
)
,
e
)
;
throttleExceptions
.
incrementAndGet
(
)
;
outcome
.
throttleExceptions
.
add
(
e
)
;
outcome
.
throttled
++
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
operation
,
e
)
;
outcome
.
exceptions
.
add
(
e
)
;
break
;
}
tracker
.
probe
(
)
;
}
LOG
.
info
(
,
operation
,
t
.
elapsedTimeMs
(
)
,
outcome
,
tracker
)
;
return
outcome
;
}
)
;
}
final
List
<
Future
<
ExecutionOutcome
>>
futures
=
executorService
.
invokeAll
(
tasks
,
getTestTimeoutMillis
(
)
,
TimeUnit
.
MILLISECONDS
)
;
return
new
Thread
(
r
,
+
count
.
getAndIncrement
(
)
)
;
}
}
)
;
(
(
ThreadPoolExecutor
)
executor
)
.
prestartAllCoreThreads
(
)
;
Future
<
Exception
>
[
]
futures
=
new
Future
[
concurrentOps
]
;
for
(
int
f
=
0
;
f
<
concurrentOps
;
f
++
)
{
final
int
index
=
f
;
futures
[
f
]
=
executor
.
submit
(
new
Callable
<
Exception
>
(
)
{
@
Override
public
Exception
call
(
)
throws
Exception
{
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
Exception
result
=
null
;
try
(
DynamoDBMetadataStore
store
=
new
DynamoDBMetadataStore
(
)
)
{
store
.
initialize
(
conf
,
new
S3Guard
.
TtlTimeProvider
(
conf
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
e
.
getClass
(
)
+
+
e
.
getMessage
(
)
)
;
result
=
e
;
@
Test
public
void
testCLIFsckWithParam
(
)
throws
Exception
{
LOG
.
info
(
+
)
;
final
int
result
=
run
(
S3GuardTool
.
Fsck
.
NAME
,
,
+
getFileSystem
(
)
.
getBucket
(
)
)
;
@
Test
public
void
testCLIFsckDDbInternalParam
(
)
throws
Exception
{
describe
(
+
)
;
final
int
result
=
run
(
S3GuardTool
.
Fsck
.
NAME
,
+
Fsck
.
DDB_MS_CONSISTENCY_FLAG
,
+
getFileSystem
(
)
.
getBucket
(
)
)
;
@
Test
public
void
testCLIFsckDDbFixOnlyFails
(
)
throws
Exception
{
describe
(
+
)
;
final
int
result
=
run
(
S3GuardTool
.
Fsck
.
NAME
,
+
Fsck
.
FIX_FLAG
,
+
getFileSystem
(
)
.
getBucket
(
)
)
;
@
Test
public
void
testCLIFsckDDbFixAndInternalSucceed
(
)
throws
Exception
{
describe
(
+
)
;
final
int
result
=
run
(
S3GuardTool
.
Fsck
.
NAME
,
+
Fsck
.
FIX_FLAG
,
+
Fsck
.
DDB_MS_CONSISTENCY_FLAG
,
+
getFileSystem
(
)
.
getBucket
(
)
)
;
@
Test
public
void
testStoreInfo
(
)
throws
Throwable
{
S3GuardTool
.
BucketInfo
cmd
=
toClose
(
new
S3GuardTool
.
BucketInfo
(
getFileSystem
(
)
.
getConf
(
)
)
)
;
cmd
.
setStore
(
getMetadataStore
(
)
)
;
try
{
String
output
=
exec
(
cmd
,
cmd
.
getName
(
)
,
+
BucketInfo
.
GUARDED_FLAG
,
getFileSystem
(
)
.
getUri
(
)
.
toString
(
)
)
;
@
Test
public
void
testSetCapacity
(
)
throws
Throwable
{
S3GuardTool
cmd
=
toClose
(
new
S3GuardTool
.
SetCapacity
(
getFileSystem
(
)
.
getConf
(
)
)
)
;
cmd
.
setStore
(
getMetadataStore
(
)
)
;
try
{
String
output
=
exec
(
cmd
,
cmd
.
getName
(
)
,
+
READ_FLAG
,
,
+
WRITE_FLAG
,
,
getFileSystem
(
)
.
getUri
(
)
.
toString
(
)
)
;
S3GuardTool
.
Uploads
cmd
=
new
S3GuardTool
.
Uploads
(
fs
.
getConf
(
)
)
;
ByteArrayOutputStream
buf
=
new
ByteArrayOutputStream
(
)
;
allOptions
.
add
(
cmd
.
getName
(
)
)
;
allOptions
.
addAll
(
Arrays
.
asList
(
options
)
)
;
if
(
ageSeconds
>
0
)
{
allOptions
.
add
(
+
Uploads
.
SECONDS_FLAG
)
;
allOptions
.
add
(
String
.
valueOf
(
ageSeconds
)
)
;
}
allOptions
.
add
(
path
.
toString
(
)
)
;
exec
(
0
,
,
cmd
,
buf
,
allOptions
.
toArray
(
new
String
[
0
]
)
)
;
try
(
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
new
ByteArrayInputStream
(
buf
.
toByteArray
(
)
)
)
)
)
{
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
String
[
]
fields
=
line
.
split
(
)
;
if
(
fields
.
length
==
4
&&
fields
[
0
]
.
equals
(
Uploads
.
TOTAL
)
)
{
int
parsedUploads
=
Integer
.
parseInt
(
fields
[
1
]
)
;
protected
void
describe
(
String
text
,
Object
...
args
)
{
describe
(
)
;
int
width
=
getConf
(
)
.
getInt
(
KEY_DIRECTORY_COUNT
,
DEFAULT_DIRECTORY_COUNT
)
;
int
depth
=
width
;
long
operations
=
getConf
(
)
.
getLong
(
KEY_OPERATION_COUNT
,
DEFAULT_OPERATION_COUNT
)
;
List
<
PathMetadata
>
origMetas
=
new
ArrayList
<
>
(
)
;
createDirTree
(
BUCKET_ROOT
,
depth
,
width
,
origMetas
)
;
List
<
Path
>
origPaths
=
metasToPaths
(
origMetas
)
;
List
<
PathMetadata
>
movedMetas
=
moveMetas
(
origMetas
,
BUCKET_ROOT
,
new
Path
(
BUCKET_ROOT
,
)
)
;
List
<
Path
>
movedPaths
=
metasToPaths
(
movedMetas
)
;
long
count
=
1
;
try
(
MetadataStore
ms
=
createMetadataStore
(
)
)
{
try
{
count
=
populateMetadataStore
(
origMetas
,
ms
)
;
describe
(
)
;
NanoTimer
moveTimer
=
new
NanoTimer
(
)
;
private
static
void
printTiming
(
Logger
log
,
String
op
,
NanoTimer
timer
,
long
count
)
{
double
msec
=
(
double
)
timer
.
duration
(
)
/
1000
;
double
msecPerOp
=
msec
/
count
;
long
blocks
=
filesize
/
uploadBlockSize
;
long
blocksPerMB
=
_1MB
/
uploadBlockSize
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
StorageStatistics
storageStatistics
=
fs
.
getStorageStatistics
(
)
;
String
putRequests
=
Statistic
.
OBJECT_PUT_REQUESTS
.
getSymbol
(
)
;
String
putBytes
=
Statistic
.
OBJECT_PUT_BYTES
.
getSymbol
(
)
;
Statistic
putRequestsActive
=
Statistic
.
OBJECT_PUT_REQUESTS_ACTIVE
;
Statistic
putBytesPending
=
Statistic
.
OBJECT_PUT_BYTES_PENDING
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
S3AInstrumentation
.
OutputStreamStatistics
streamStatistics
;
long
blocksPer10MB
=
blocksPerMB
*
10
;
ProgressCallback
progress
=
new
ProgressCallback
(
timer
)
;
try
(
FSDataOutputStream
out
=
fs
.
create
(
fileToCreate
,
true
,
uploadBlockSize
,
progress
)
)
{
try
{
streamStatistics
=
getOutputStreamStatistics
(
out
)
;
S3AInstrumentation
.
OutputStreamStatistics
streamStatistics
;
long
blocksPer10MB
=
blocksPerMB
*
10
;
ProgressCallback
progress
=
new
ProgressCallback
(
timer
)
;
try
(
FSDataOutputStream
out
=
fs
.
create
(
fileToCreate
,
true
,
uploadBlockSize
,
progress
)
)
{
try
{
streamStatistics
=
getOutputStreamStatistics
(
out
)
;
}
catch
(
ClassCastException
e
)
{
LOG
.
info
(
,
out
.
getWrappedStream
(
)
)
;
streamStatistics
=
null
;
}
for
(
long
block
=
1
;
block
<=
blocks
;
block
++
)
{
out
.
write
(
data
)
;
long
written
=
block
*
uploadBlockSize
;
if
(
block
%
blocksPer10MB
==
0
||
written
==
filesize
)
{
long
percentage
=
written
*
100
/
filesize
;
double
elapsedTime
=
timer
.
elapsedTime
(
)
/
1.0e9
;
ProgressCallback
progress
=
new
ProgressCallback
(
timer
)
;
try
(
FSDataOutputStream
out
=
fs
.
create
(
fileToCreate
,
true
,
uploadBlockSize
,
progress
)
)
{
try
{
streamStatistics
=
getOutputStreamStatistics
(
out
)
;
}
catch
(
ClassCastException
e
)
{
LOG
.
info
(
,
out
.
getWrappedStream
(
)
)
;
streamStatistics
=
null
;
}
for
(
long
block
=
1
;
block
<=
blocks
;
block
++
)
{
out
.
write
(
data
)
;
long
written
=
block
*
uploadBlockSize
;
if
(
block
%
blocksPer10MB
==
0
||
written
==
filesize
)
{
long
percentage
=
written
*
100
/
filesize
;
double
elapsedTime
=
timer
.
elapsedTime
(
)
/
1.0e9
;
double
writtenMB
=
1.0
*
written
/
_1MB
;
LOG
.
info
(
String
.
format
(
+
+
,
percentage
,
writtenMB
,
filesizeMB
,
storageStatistics
.
getLong
(
putBytes
)
,
gaugeValue
(
putBytesPending
)
,
storageStatistics
.
getLong
(
putRequests
)
,
gaugeValue
(
putRequestsActive
)
,
elapsedTime
,
writtenMB
/
elapsedTime
)
)
;
ProgressCallback
progress
=
new
ProgressCallback
(
timer
)
;
try
(
FSDataOutputStream
out
=
fs
.
create
(
fileToCreate
,
true
,
uploadBlockSize
,
progress
)
)
{
try
{
streamStatistics
=
getOutputStreamStatistics
(
out
)
;
}
catch
(
ClassCastException
e
)
{
LOG
.
info
(
,
out
.
getWrappedStream
(
)
)
;
streamStatistics
=
null
;
}
for
(
long
block
=
1
;
block
<=
blocks
;
block
++
)
{
out
.
write
(
data
)
;
long
written
=
block
*
uploadBlockSize
;
if
(
block
%
blocksPer10MB
==
0
||
written
==
filesize
)
{
long
percentage
=
written
*
100
/
filesize
;
double
elapsedTime
=
timer
.
elapsedTime
(
)
/
1.0e9
;
double
writtenMB
=
1.0
*
written
/
_1MB
;
LOG
.
info
(
String
.
format
(
+
+
,
percentage
,
writtenMB
,
filesizeMB
,
storageStatistics
.
getLong
(
putBytes
)
,
gaugeValue
(
putBytesPending
)
,
storageStatistics
.
getLong
(
putRequests
)
,
gaugeValue
(
putRequestsActive
)
,
elapsedTime
,
writtenMB
/
elapsedTime
)
)
;
streamStatistics
=
null
;
}
for
(
long
block
=
1
;
block
<=
blocks
;
block
++
)
{
out
.
write
(
data
)
;
long
written
=
block
*
uploadBlockSize
;
if
(
block
%
blocksPer10MB
==
0
||
written
==
filesize
)
{
long
percentage
=
written
*
100
/
filesize
;
double
elapsedTime
=
timer
.
elapsedTime
(
)
/
1.0e9
;
double
writtenMB
=
1.0
*
written
/
_1MB
;
LOG
.
info
(
String
.
format
(
+
+
,
percentage
,
writtenMB
,
filesizeMB
,
storageStatistics
.
getLong
(
putBytes
)
,
gaugeValue
(
putBytesPending
)
,
storageStatistics
.
getLong
(
putRequests
)
,
gaugeValue
(
putRequestsActive
)
,
elapsedTime
,
writtenMB
/
elapsedTime
)
)
;
}
}
LOG
.
info
(
,
out
)
;
LOG
.
info
(
,
streamStatistics
)
;
ContractTestUtils
.
NanoTimer
closeTimer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
out
.
close
(
)
;
closeTimer
.
end
(
)
;
}
timer
.
end
(
,
filesizeMB
,
uploadBlockSize
)
;
long
written
=
block
*
uploadBlockSize
;
if
(
block
%
blocksPer10MB
==
0
||
written
==
filesize
)
{
long
percentage
=
written
*
100
/
filesize
;
double
elapsedTime
=
timer
.
elapsedTime
(
)
/
1.0e9
;
double
writtenMB
=
1.0
*
written
/
_1MB
;
LOG
.
info
(
String
.
format
(
+
+
,
percentage
,
writtenMB
,
filesizeMB
,
storageStatistics
.
getLong
(
putBytes
)
,
gaugeValue
(
putBytesPending
)
,
storageStatistics
.
getLong
(
putRequests
)
,
gaugeValue
(
putRequestsActive
)
,
elapsedTime
,
writtenMB
/
elapsedTime
)
)
;
}
}
LOG
.
info
(
,
out
)
;
LOG
.
info
(
,
streamStatistics
)
;
ContractTestUtils
.
NanoTimer
closeTimer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
out
.
close
(
)
;
closeTimer
.
end
(
)
;
}
timer
.
end
(
,
filesizeMB
,
uploadBlockSize
)
;
logFSState
(
)
;
bandwidth
(
timer
,
filesize
)
;
LOG
.
info
(
,
streamStatistics
)
;
@
Test
public
void
test_040_PositionedReadHugeFile
(
)
throws
Throwable
{
assumeHugeFileExists
(
)
;
final
String
encryption
=
getConf
(
)
.
getTrimmed
(
SERVER_SIDE_ENCRYPTION_ALGORITHM
)
;
boolean
encrypted
=
encryption
!=
null
;
if
(
encrypted
)
{
long
eof
=
size
-
1
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
ContractTestUtils
.
NanoTimer
readAtByte0
,
readAtByte0Again
,
readAtEOF
;
try
(
FSDataInputStream
in
=
fs
.
open
(
hugefile
,
uploadBlockSize
)
)
{
readAtByte0
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0
.
end
(
)
;
ops
++
;
readAtEOF
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
eof
-
bufferSize
,
buffer
)
;
readAtEOF
.
end
(
)
;
ops
++
;
readAtByte0Again
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0Again
.
end
(
)
;
readAtByte0
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0
.
end
(
)
;
ops
++
;
readAtEOF
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
eof
-
bufferSize
,
buffer
)
;
readAtEOF
.
end
(
)
;
ops
++
;
readAtByte0Again
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0Again
.
end
(
)
;
ops
++
;
LOG
.
info
(
,
in
)
;
}
long
mb
=
Math
.
max
(
size
/
_1MB
,
1
)
;
logFSState
(
)
;
@
Test
public
void
test_050_readHugeFile
(
)
throws
Throwable
{
assumeHugeFileExists
(
)
;
describe
(
,
hugefile
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
FileStatus
status
=
fs
.
getFileStatus
(
hugefile
)
;
long
size
=
status
.
getLen
(
)
;
long
blocks
=
size
/
uploadBlockSize
;
byte
[
]
data
=
new
byte
[
uploadBlockSize
]
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
try
(
FSDataInputStream
in
=
fs
.
open
(
hugefile
,
uploadBlockSize
)
)
{
for
(
long
block
=
0
;
block
<
blocks
;
block
++
)
{
in
.
readFully
(
data
)
;
}
LOG
.
info
(
,
in
)
;
}
long
mb
=
Math
.
max
(
size
/
_1MB
,
1
)
;
timer
.
end
(
,
mb
)
;
@
Test
public
void
test_100_renameHugeFile
(
)
throws
Throwable
{
assumeHugeFileExists
(
)
;
describe
(
,
hugefile
,
hugefileRenamed
)
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
FileStatus
status
=
fs
.
getFileStatus
(
hugefile
)
;
long
size
=
status
.
getLen
(
)
;
fs
.
delete
(
hugefileRenamed
,
false
)
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
fs
.
rename
(
hugefile
,
hugefileRenamed
)
;
long
mb
=
Math
.
max
(
size
/
_1MB
,
1
)
;
timer
.
end
(
,
mb
)
;
FileStatus
status
=
fs
.
getFileStatus
(
hugefile
)
;
long
size
=
status
.
getLen
(
)
;
fs
.
delete
(
hugefileRenamed
,
false
)
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
fs
.
rename
(
hugefile
,
hugefileRenamed
)
;
long
mb
=
Math
.
max
(
size
/
_1MB
,
1
)
;
timer
.
end
(
,
mb
)
;
LOG
.
info
(
,
toHuman
(
timer
.
nanosPerOperation
(
mb
)
)
)
;
bandwidth
(
timer
,
size
)
;
logFSState
(
)
;
FileStatus
destFileStatus
=
fs
.
getFileStatus
(
hugefileRenamed
)
;
assertEquals
(
size
,
destFileStatus
.
getLen
(
)
)
;
ContractTestUtils
.
NanoTimer
timer2
=
new
ContractTestUtils
.
NanoTimer
(
)
;
fs
.
rename
(
hugefileRenamed
,
hugefile
)
;
timer2
.
end
(
)
;
@
Test
public
void
test_900_dumpStats
(
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
getFileSystem
(
)
.
getStorageStatistics
(
)
.
forEach
(
kv
->
sb
.
append
(
kv
.
toString
(
)
)
.
append
(
)
)
;
@
Test
public
void
test_020_DeleteThrottling
(
)
throws
Throwable
{
describe
(
)
;
final
File
results
=
deleteFiles
(
requests
,
pageSize
)
;
File
csvFile
=
new
File
(
dataDir
,
String
.
format
(
,
requestCount
,
entries
,
throttle
)
)
;
describe
(
,
requestCount
,
entries
,
csvFile
)
;
Path
basePath
=
path
(
)
;
final
S3AFileSystem
fs
=
getFileSystem
(
)
;
final
String
base
=
fs
.
pathToKey
(
basePath
)
;
final
List
<
DeleteObjectsRequest
.
KeyVersion
>
fileList
=
buildDeleteRequest
(
base
,
entries
)
;
final
FileWriter
out
=
new
FileWriter
(
csvFile
)
;
Csvout
csvout
=
new
Csvout
(
out
,
,
)
;
Outcome
.
writeSchema
(
csvout
)
;
final
ContractTestUtils
.
NanoTimer
jobTimer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
for
(
int
i
=
0
;
i
<
requestCount
;
i
++
)
{
final
int
id
=
i
;
completionService
.
submit
(
(
)
->
{
final
long
startTime
=
System
.
currentTimeMillis
(
)
;
Thread
.
currentThread
(
)
.
setName
(
+
id
)
;
catch
(
IOException
e
)
{
ex
=
e
;
}
timer
.
end
(
+
id
)
;
return
new
Outcome
(
id
,
startTime
,
timer
,
ex
)
;
}
)
;
}
NanoTimerStats
stats
=
new
NanoTimerStats
(
)
;
NanoTimerStats
success
=
new
NanoTimerStats
(
)
;
NanoTimerStats
throttled
=
new
NanoTimerStats
(
)
;
List
<
Outcome
>
throttledEvents
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
requestCount
;
i
++
)
{
Outcome
outcome
=
completionService
.
take
(
)
.
get
(
)
;
ContractTestUtils
.
NanoTimer
timer
=
outcome
.
timer
;
Exception
ex
=
outcome
.
exception
;
outcome
.
writeln
(
csvout
)
;
stats
.
add
(
timer
)
;
NanoTimerStats
stats
=
new
NanoTimerStats
(
)
;
NanoTimerStats
success
=
new
NanoTimerStats
(
)
;
NanoTimerStats
throttled
=
new
NanoTimerStats
(
)
;
List
<
Outcome
>
throttledEvents
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
requestCount
;
i
++
)
{
Outcome
outcome
=
completionService
.
take
(
)
.
get
(
)
;
ContractTestUtils
.
NanoTimer
timer
=
outcome
.
timer
;
Exception
ex
=
outcome
.
exception
;
outcome
.
writeln
(
csvout
)
;
stats
.
add
(
timer
)
;
if
(
ex
!=
null
)
{
LOG
.
info
(
,
i
,
ex
)
;
throttled
.
add
(
timer
)
;
throttledEvents
.
add
(
outcome
)
;
}
else
{
ContractTestUtils
.
NanoTimer
timer
=
outcome
.
timer
;
Exception
ex
=
outcome
.
exception
;
outcome
.
writeln
(
csvout
)
;
stats
.
add
(
timer
)
;
if
(
ex
!=
null
)
{
LOG
.
info
(
,
i
,
ex
)
;
throttled
.
add
(
timer
)
;
throttledEvents
.
add
(
outcome
)
;
}
else
{
success
.
add
(
timer
)
;
}
}
csvout
.
close
(
)
;
jobTimer
.
end
(
)
;
LOG
.
info
(
+
csvFile
)
;
LOG
.
info
(
,
requestCount
,
throttled
.
getCount
(
)
,
stats
,
throttled
,
success
)
;
double
duration
=
jobTimer
.
duration
(
)
;
}
}
}
LOG
.
info
(
)
;
ExecutorService
executor
=
Executors
.
newFixedThreadPool
(
concurrentRenames
,
new
ThreadFactory
(
)
{
private
AtomicInteger
count
=
new
AtomicInteger
(
0
)
;
public
Thread
newThread
(
Runnable
r
)
{
return
new
Thread
(
r
,
+
count
.
getAndIncrement
(
)
)
;
}
}
)
;
try
{
(
(
ThreadPoolExecutor
)
executor
)
.
prestartAllCoreThreads
(
)
;
Future
<
Boolean
>
[
]
futures
=
new
Future
[
concurrentRenames
]
;
for
(
int
i
=
0
;
i
<
concurrentRenames
;
i
++
)
{
final
int
index
=
i
;
futures
[
i
]
=
executor
.
submit
(
new
Callable
<
Boolean
>
(
)
{
@
Override
public
Boolean
call
(
)
throws
Exception
{
NanoTimer
timer
=
new
NanoTimer
(
)
;
boolean
result
=
fs
.
rename
(
source
[
index
]
,
target
[
index
]
)
;
final
Path
finalParentDir
=
new
Path
(
scaleTestDir
,
)
;
final
Path
finalDir
=
new
Path
(
finalParentDir
,
)
;
final
S3AFileSystem
fs
=
getFileSystem
(
)
;
rm
(
fs
,
scaleTestDir
,
true
,
false
)
;
fs
.
mkdirs
(
srcDir
)
;
fs
.
mkdirs
(
finalParentDir
)
;
createFiles
(
fs
,
srcDir
,
1
,
count
,
0
)
;
FileStatus
[
]
statuses
=
fs
.
listStatus
(
srcDir
)
;
int
nSrcFiles
=
statuses
.
length
;
long
sourceSize
=
Arrays
.
stream
(
statuses
)
.
mapToLong
(
FileStatus
::
getLen
)
.
sum
(
)
;
assertEquals
(
,
count
,
nSrcFiles
)
;
ContractTestUtils
.
NanoTimer
renameTimer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
,
srcDir
,
finalDir
)
)
{
rename
(
srcDir
,
finalDir
)
;
}
renameTimer
.
end
(
)
;
renameTimer
.
end
(
)
;
LOG
.
info
(
,
renameTimer
.
bandwidthDescription
(
sourceSize
)
)
;
LOG
.
info
(
String
.
format
(
,
(
renameTimer
.
nanosPerOperation
(
count
)
*
1.0f
)
/
1.0e6
)
)
;
Assertions
.
assertThat
(
lsR
(
fs
,
srcParentDir
,
true
)
)
.
describedAs
(
,
srcParentDir
)
.
isEqualTo
(
0
)
;
assertPathDoesNotExist
(
,
new
Path
(
srcDir
,
filenameOfIndex
(
0
)
)
)
;
assertPathDoesNotExist
(
,
new
Path
(
srcDir
,
filenameOfIndex
(
count
/
2
)
)
)
;
assertPathDoesNotExist
(
,
new
Path
(
srcDir
,
filenameOfIndex
(
count
-
1
)
)
)
;
Assertions
.
assertThat
(
lsR
(
fs
,
finalDir
,
true
)
)
.
describedAs
(
,
finalDir
)
.
isEqualTo
(
count
)
;
Assertions
.
assertThat
(
fs
.
listStatus
(
finalDir
)
)
.
describedAs
(
,
finalDir
)
.
hasSize
(
count
)
;
assertPathExists
(
,
new
Path
(
finalDir
,
filenameOfIndex
(
0
)
)
)
;
assertPathExists
(
,
new
Path
(
finalDir
,
filenameOfIndex
(
count
/
2
)
)
)
;
assertPathExists
(
,
new
Path
(
finalDir
,
filenameOfIndex
(
count
-
1
)
)
)
;
ContractTestUtils
.
NanoTimer
deleteTimer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
,
finalDir
)
)
{
assertDeleted
(
finalDir
,
true
)
;
int
scale
=
getConf
(
)
.
getInt
(
KEY_DIRECTORY_COUNT
,
DEFAULT_DIRECTORY_COUNT
)
;
int
width
=
scale
;
int
depth
=
scale
;
int
files
=
scale
;
MetricDiff
metadataRequests
=
new
MetricDiff
(
fs
,
OBJECT_METADATA_REQUESTS
)
;
MetricDiff
listRequests
=
new
MetricDiff
(
fs
,
OBJECT_LIST_REQUESTS
)
;
MetricDiff
listContinueRequests
=
new
MetricDiff
(
fs
,
OBJECT_CONTINUE_LIST_REQUESTS
)
;
MetricDiff
listStatusCalls
=
new
MetricDiff
(
fs
,
INVOCATION_LIST_FILES
)
;
MetricDiff
getFileStatusCalls
=
new
MetricDiff
(
fs
,
INVOCATION_GET_FILE_STATUS
)
;
NanoTimer
createTimer
=
new
NanoTimer
(
)
;
TreeScanResults
created
=
createSubdirs
(
fs
,
listDir
,
depth
,
width
,
files
,
0
)
;
int
emptyDepth
=
1
*
scale
;
int
emptyWidth
=
3
*
scale
;
created
.
add
(
createSubdirs
(
fs
,
listDir
,
emptyDepth
,
emptyWidth
,
0
,
0
,
,
,
)
)
;
createTimer
.
end
(
,
created
)
;
protected
void
logTimePerIOP
(
String
operation
,
NanoTimer
timer
,
long
count
)
{
int
remaining
=
blockSize
;
long
blockId
=
i
+
1
;
NanoTimer
blockTimer
=
new
NanoTimer
(
)
;
int
reads
=
0
;
while
(
remaining
>
0
)
{
NanoTimer
readTimer
=
new
NanoTimer
(
)
;
int
bytesRead
=
in
.
read
(
block
,
offset
,
remaining
)
;
reads
++
;
if
(
bytesRead
==
1
)
{
break
;
}
remaining
-=
bytesRead
;
offset
+=
bytesRead
;
count
+=
bytesRead
;
readTimer
.
end
(
)
;
if
(
bytesRead
!=
0
)
{
NanoTimer
readTimer
=
new
NanoTimer
(
)
;
int
bytesRead
=
in
.
read
(
block
,
offset
,
remaining
)
;
reads
++
;
if
(
bytesRead
==
1
)
{
break
;
}
remaining
-=
bytesRead
;
offset
+=
bytesRead
;
count
+=
bytesRead
;
readTimer
.
end
(
)
;
if
(
bytesRead
!=
0
)
{
LOG
.
debug
(
+
+
,
reads
,
bytesRead
,
blockSize
-
remaining
,
remaining
,
readTimer
.
duration
(
)
,
readTimer
.
nanosPerOperation
(
bytesRead
)
,
readTimer
.
bandwidthDescription
(
bytesRead
)
)
;
}
else
{
LOG
.
warn
(
,
reads
)
;
}
}
blockTimer
.
end
(
,
blockId
,
reads
)
;
String
bw
=
blockTimer
.
bandwidthDescription
(
blockSize
)
;
offset
+=
bytesRead
;
count
+=
bytesRead
;
readTimer
.
end
(
)
;
if
(
bytesRead
!=
0
)
{
LOG
.
debug
(
+
+
,
reads
,
bytesRead
,
blockSize
-
remaining
,
remaining
,
readTimer
.
duration
(
)
,
readTimer
.
nanosPerOperation
(
bytesRead
)
,
readTimer
.
bandwidthDescription
(
bytesRead
)
)
;
}
else
{
LOG
.
warn
(
,
reads
)
;
}
}
blockTimer
.
end
(
,
blockId
,
reads
)
;
String
bw
=
blockTimer
.
bandwidthDescription
(
blockSize
)
;
LOG
.
info
(
,
blockId
,
bw
)
;
if
(
bandwidth
(
blockTimer
,
blockSize
)
<
minimumBandwidth
)
{
LOG
.
warn
(
,
bw
,
blockId
)
;
Assert
.
assertTrue
(
+
bw
+
+
resetCount
+
,
resetCount
<=
maxResetCount
)
;
resetCount
++
;
getS3AInputStream
(
in
)
.
resetConnection
(
)
;
protected
void
executeSeekReadSequence
(
long
blockSize
,
long
readahead
,
S3AInputPolicy
policy
)
throws
IOException
{
in
=
openTestFile
(
policy
,
readahead
)
;
long
len
=
testDataStatus
.
getLen
(
)
;
NanoTimer
timer
=
new
NanoTimer
(
)
;
long
blockCount
=
len
/
blockSize
;
int
readOps
=
0
;
while
(
bytesRead
<
halfReadahead
)
{
int
length
=
buffer
.
length
-
offset
;
int
read
=
in
.
read
(
currentPos
,
buffer
,
offset
,
length
)
;
bytesRead
+=
read
;
offset
+=
read
;
readOps
++
;
assertEquals
(
+
readOps
+
+
bytesRead
+
+
currentPos
+
+
fs
+
+
in
,
1
,
streamStatistics
.
openOperations
)
;
for
(
int
i
=
currentPos
;
i
<
currentPos
+
read
;
i
++
)
{
assertEquals
(
+
i
,
sourceData
[
i
]
,
buffer
[
i
]
)
;
}
currentPos
+=
read
;
}
assertStreamOpenedExactlyOnce
(
)
;
assertEquals
(
readahead
,
currentPos
)
;
readTimer
.
end
(
,
bytesRead
,
readOps
)
;
bandwidth
(
readTimer
,
bytesRead
)
;
while
(
bytesRead
<
halfReadahead
)
{
int
length
=
buffer
.
length
-
offset
;
int
read
=
in
.
read
(
currentPos
,
buffer
,
offset
,
length
)
;
bytesRead
+=
read
;
offset
+=
read
;
readOps
++
;
assertEquals
(
+
readOps
+
+
bytesRead
+
+
currentPos
+
+
fs
+
+
in
,
1
,
streamStatistics
.
openOperations
)
;
for
(
int
i
=
currentPos
;
i
<
currentPos
+
read
;
i
++
)
{
assertEquals
(
+
i
,
sourceData
[
i
]
,
buffer
[
i
]
)
;
}
currentPos
+=
read
;
}
assertStreamOpenedExactlyOnce
(
)
;
assertEquals
(
readahead
,
currentPos
)
;
readTimer
.
end
(
,
bytesRead
,
readOps
)
;
bandwidth
(
readTimer
,
bytesRead
)
;
LOG
.
info
(
,
toHuman
(
readTimer
.
nanosPerOperation
(
bytesRead
)
)
)
;
assertEquals
(
,
sourceData
[
currentPos
]
,
(
int
)
buffer
[
currentPos
]
)
;
currentPos
++
;
describe
(
,
in
)
;
long
readCount
=
0
;
NanoTimer
timer
=
new
NanoTimer
(
)
;
LOG
.
info
(
)
;
in
.
seek
(
currentPos
)
;
LOG
.
info
(
)
;
while
(
currentPos
<
datasetLen
)
{
int
r
=
in
.
read
(
)
;
assertTrue
(
+
currentPos
+
+
in
,
r
>=
0
)
;
buffer
[
currentPos
]
=
(
byte
)
r
;
assertEquals
(
+
in
,
sourceData
[
currentPos
]
,
r
)
;
currentPos
++
;
readCount
++
;
protected
List
<
String
>
parseToLines
(
final
FSDataInputStream
selection
,
int
maxLines
)
throws
IOException
{
List
<
String
>
result
=
new
ArrayList
<
>
(
)
;
String
stats
;
try
(
Scanner
scanner
=
new
Scanner
(
new
BufferedReader
(
new
InputStreamReader
(
selection
)
)
)
)
{
scanner
.
useDelimiter
(
CSV_INPUT_RECORD_DELIMITER_DEFAULT
)
;
while
(
maxLines
>
0
)
{
try
{
String
l
=
scanner
.
nextLine
(
)
;
protected
List
<
String
>
verifySelectionCount
(
final
int
min
,
final
int
max
,
final
String
expression
,
final
List
<
String
>
selection
)
{
int
size
=
selection
.
size
(
)
;
if
(
size
<
min
||
(
max
>
-
1
&&
size
>
max
)
)
{
String
listing
=
prepareToPrint
(
selection
)
;
protected
static
<
T
extends
Exception
>
T
logIntercepted
(
T
ex
)
{
@
SuppressWarnings
(
)
@
Test
public
void
testReadWholeFileClassicAPI
(
)
throws
Throwable
{
describe
(
)
;
int
lines
;
try
(
BufferedReader
reader
=
new
BufferedReader
(
new
InputStreamReader
(
getFileSystem
(
)
.
open
(
csvPath
)
)
)
)
{
lines
=
0
;
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
lines
++
;
S3AFileSystem
fs
=
getFileSystem
(
)
;
int
len
=
(
int
)
fs
.
getFileStatus
(
path
)
.
getLen
(
)
;
byte
[
]
fullData
=
new
byte
[
len
]
;
int
actualLen
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
,
path
)
;
FSDataInputStream
sourceStream
=
select
(
fs
,
path
,
selectConf
,
SELECT_EVERYTHING
)
)
{
actualLen
=
IOUtils
.
read
(
sourceStream
,
fullData
)
;
}
int
seekRange
=
20
;
try
(
FSDataInputStream
seekStream
=
select
(
fs
,
path
,
selectConf
,
SELECT_EVERYTHING
)
)
{
SelectInputStream
sis
=
(
SelectInputStream
)
seekStream
.
getWrappedStream
(
)
;
S3AInstrumentation
.
InputStreamStatistics
streamStats
=
sis
.
getS3AStreamStatistics
(
)
;
seekStream
.
seek
(
0
)
;
assertEquals
(
,
fullData
[
0
]
,
seekStream
.
read
(
)
)
;
seekStream
.
seek
(
1
)
;
seekStream
.
seek
(
1
)
;
PathIOException
ex
=
intercept
(
PathIOException
.
class
,
SelectInputStream
.
SEEK_UNSUPPORTED
,
(
)
->
seekStream
.
seek
(
0
)
)
;
intercept
(
PathIOException
.
class
,
SelectInputStream
.
SEEK_UNSUPPORTED
,
(
)
->
seekStream
.
readFully
(
0
,
buffer
)
)
;
assertPosition
(
seekStream
,
pos
+
1
)
;
intercept
(
PathIOException
.
class
,
SelectInputStream
.
SEEK_UNSUPPORTED
,
(
)
->
seekStream
.
readFully
(
pos
,
buffer
)
)
;
seekStream
.
setReadahead
(
null
)
;
assertEquals
(
,
Constants
.
DEFAULT_READAHEAD_RANGE
,
sis
.
getReadahead
(
)
)
;
long
target
=
seekStream
.
getPos
(
)
+
seekRange
;
seek
(
seekStream
,
target
)
;
assertPosition
(
seekStream
,
target
)
;
assertEquals
(
,
fullData
[
(
int
)
seekStream
.
getPos
(
)
]
,
seekStream
.
read
(
)
)
;
assertEquals
(
+
streamStats
,
seekRange
,
streamStats
.
bytesSkippedOnSeek
)
;
intercept
(
IllegalArgumentException
.
class
,
S3AInputStream
.
E_NEGATIVE_READAHEAD_VALUE
,
(
)
->
seekStream
.
setReadahead
(
-
1L
)
)
;
int
read
=
seekStream
.
read
(
seekStream
.
getPos
(
)
+
2
,
buffer
,
0
,
1
)
;
assertEquals
(
1
,
read
)
;
logIntercepted
(
expectSeekEOF
(
seekStream
,
actualLen
*
2
)
)
;
assertPosition
(
seekStream
,
actualLen
)
;
assertEquals
(
,
Constants
.
DEFAULT_READAHEAD_RANGE
,
sis
.
getReadahead
(
)
)
;
long
target
=
seekStream
.
getPos
(
)
+
seekRange
;
seek
(
seekStream
,
target
)
;
assertPosition
(
seekStream
,
target
)
;
assertEquals
(
,
fullData
[
(
int
)
seekStream
.
getPos
(
)
]
,
seekStream
.
read
(
)
)
;
assertEquals
(
+
streamStats
,
seekRange
,
streamStats
.
bytesSkippedOnSeek
)
;
intercept
(
IllegalArgumentException
.
class
,
S3AInputStream
.
E_NEGATIVE_READAHEAD_VALUE
,
(
)
->
seekStream
.
setReadahead
(
-
1L
)
)
;
int
read
=
seekStream
.
read
(
seekStream
.
getPos
(
)
+
2
,
buffer
,
0
,
1
)
;
assertEquals
(
1
,
read
)
;
logIntercepted
(
expectSeekEOF
(
seekStream
,
actualLen
*
2
)
)
;
assertPosition
(
seekStream
,
actualLen
)
;
assertEquals
(
-
1
,
seekStream
.
read
(
)
)
;
LOG
.
info
(
,
streamStats
)
;
assertFalse
(
+
seekStream
,
seekStream
.
seekToNewSource
(
0
)
)
;
seekStream
.
setReadahead
(
0L
)
;
@
Test
public
void
testSelectFileExample
(
)
throws
Throwable
{
describe
(
)
;
int
len
=
(
int
)
getFileSystem
(
)
.
getFileStatus
(
csvPath
)
.
getLen
(
)
;
FutureDataInputStreamBuilder
builder
=
getFileSystem
(
)
.
openFile
(
csvPath
)
.
must
(
,
SELECT_ODD_ENTRIES
)
.
must
(
,
)
.
must
(
,
)
.
must
(
,
)
.
must
(
,
)
;
CompletableFuture
<
FSDataInputStream
>
future
=
builder
.
build
(
)
;
try
(
FSDataInputStream
select
=
future
.
get
(
)
)
{
byte
[
]
bytes
=
new
byte
[
len
]
;
int
actual
=
select
.
read
(
bytes
)
;
@
Test
public
void
testSelectDatestampsConverted
(
)
throws
Throwable
{
describe
(
)
;
JobConf
conf
=
createJobConf
(
)
;
inputMust
(
conf
,
CSV_INPUT_HEADER
,
CSV_HEADER_OPT_USE
)
;
inputMust
(
conf
,
CSV_OUTPUT_QUOTE_FIELDS
,
CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED
)
;
String
sql
=
SELECT_TO_DATE
;
List
<
String
>
records
=
expectRecordsRead
(
ALL_ROWS_COUNT
,
conf
,
sql
)
;
@
Test
public
void
testCommentsSkipped
(
)
throws
Throwable
{
describe
(
)
;
selectConf
.
set
(
CSV_INPUT_HEADER
,
CSV_HEADER_OPT_USE
)
;
List
<
String
>
lines
=
verifySelectionCount
(
ALL_ROWS_COUNT_WITH_HEADER
,
,
parseToLines
(
select
(
getFileSystem
(
)
,
brokenCSV
,
selectConf
,
)
)
)
;
@
Test
public
void
testEmptyColumnsRegenerated
(
)
throws
Throwable
{
describe
(
+
)
;
selectConf
.
set
(
CSV_INPUT_HEADER
,
CSV_HEADER_OPT_USE
)
;
List
<
String
>
lines
=
verifySelectionCount
(
ALL_ROWS_COUNT_WITH_HEADER
,
,
parseToLines
(
select
(
getFileSystem
(
)
,
brokenCSV
,
selectConf
,
)
)
)
;
@
Test
public
void
testLandsatToFile
(
)
throws
Throwable
{
describe
(
)
;
int
lineCount
=
LINE_COUNT
;
S3AFileSystem
landsatFS
=
(
S3AFileSystem
)
getLandsatGZ
(
)
.
getFileSystem
(
getConfiguration
(
)
)
;
S3ATestUtils
.
MetricDiff
selectCount
=
new
S3ATestUtils
.
MetricDiff
(
landsatFS
,
Statistic
.
OBJECT_SELECT_REQUESTS
)
;
run
(
selectConf
,
selectTool
,
D
,
v
(
CSV_OUTPUT_QUOTE_CHARACTER
,
)
,
D
,
v
(
CSV_OUTPUT_QUOTE_FIELDS
,
CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED
)
,
,
o
(
OPT_HEADER
)
,
CSV_HEADER_OPT_USE
,
o
(
OPT_COMPRESSION
)
,
COMPRESSION_OPT_GZIP
,
o
(
OPT_LIMIT
)
,
Integer
.
toString
(
lineCount
)
,
o
(
OPT_OUTPUT
)
,
localFile
.
toString
(
)
,
landsatSrc
,
SELECT_SUNNY_ROWS_NO_LIMIT
)
;
List
<
String
>
lines
=
IOUtils
.
readLines
(
new
FileInputStream
(
localFile
)
,
Charset
.
defaultCharset
(
)
)
;
@
Test
public
void
testSelectCloudcoverIgnoreHeader
(
)
throws
Throwable
{
describe
(
)
;
selectConf
.
set
(
CSV_INPUT_HEADER
,
CSV_HEADER_OPT_IGNORE
)
;
String
sql
=
+
+
+
+
LIMITED
;
List
<
String
>
list
=
selectLandsatFile
(
selectConf
,
sql
)
;
@
Test
public
void
testSelectCloudcoverUseHeader
(
)
throws
Throwable
{
describe
(
+
)
;
S3ATestUtils
.
MetricDiff
selectCount
=
new
S3ATestUtils
.
MetricDiff
(
getLandsatFS
(
)
,
Statistic
.
OBJECT_SELECT_REQUESTS
)
;
List
<
String
>
list
=
selectLandsatFile
(
selectConf
,
SELECT_ENTITY_ID_ALL_CLOUDS
)
;
@
Test
public
void
testFileContextIntegration
(
)
throws
Throwable
{
describe
(
)
;
FileContext
fc
=
S3ATestUtils
.
createTestFileContext
(
getConfiguration
(
)
)
;
List
<
String
>
list
=
parseToLines
(
select
(
fc
,
getLandsatGZ
(
)
,
selectConf
,
SELECT_ENTITY_ID_ALL_CLOUDS
)
,
SELECT_LIMIT
*
2
)
;
intercept
(
PathIOException
.
class
,
SelectInputStream
.
SEEK_UNSUPPORTED
,
(
)
->
seekStream
.
readFully
(
0
,
buffer
)
)
;
long
target
=
seekStream
.
getPos
(
)
+
seekRange
;
seek
(
seekStream
,
target
)
;
assertEquals
(
+
seekStream
,
target
,
seekStream
.
getPos
(
)
)
;
assertEquals
(
,
dataset
[
(
int
)
seekStream
.
getPos
(
)
]
,
seekStream
.
read
(
)
)
;
assertEquals
(
+
streamStats
,
seekRange
,
streamStats
.
bytesSkippedOnSeek
)
;
long
offset
;
long
increment
=
64
*
_1KB
;
for
(
offset
=
32
*
_1KB
;
offset
<
actualLen
;
offset
+=
increment
)
{
seek
(
seekStream
,
offset
)
;
assertEquals
(
+
seekStream
,
offset
,
seekStream
.
getPos
(
)
)
;
assertEquals
(
,
dataset
[
(
int
)
seekStream
.
getPos
(
)
]
,
seekStream
.
read
(
)
)
;
}
for
(
;
offset
<
len
;
offset
+=
_1MB
)
{
seek
(
seekStream
,
offset
)
;
assertEquals
(
+
seekStream
,
offset
,
seekStream
.
getPos
(
)
)
;
jobConf
.
set
(
FS_S3A_COMMITTER_NAME
,
StagingCommitter
.
NAME
)
;
jobConf
.
setBoolean
(
FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES
,
false
)
;
final
String
query
=
ITestS3SelectLandsat
.
SELECT_PROCESSING_LEVEL_NO_LIMIT
;
inputMust
(
jobConf
,
SELECT_SQL
,
query
)
;
inputMust
(
jobConf
,
SELECT_INPUT_COMPRESSION
,
COMPRESSION_OPT_GZIP
)
;
inputMust
(
jobConf
,
SELECT_INPUT_FORMAT
,
SELECT_FORMAT_CSV
)
;
inputMust
(
jobConf
,
CSV_INPUT_HEADER
,
CSV_HEADER_OPT_USE
)
;
inputMust
(
jobConf
,
SELECT_OUTPUT_FORMAT
,
SELECT_FORMAT_CSV
)
;
inputMust
(
jobConf
,
CSV_OUTPUT_QUOTE_FIELDS
,
CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED
)
;
enablePassthroughCodec
(
jobConf
,
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
+
query
)
)
{
int
exitCode
=
job
.
waitForCompletion
(
true
)
?
0
:
1
;
assertEquals
(
,
0
,
exitCode
)
;
}
Path
successPath
=
new
Path
(
output
,
)
;
SuccessData
success
=
SuccessData
.
load
(
fs
,
successPath
)
;
jobConf
.
setBoolean
(
FS_S3A_COMMITTER_STAGING_UNIQUE_FILENAMES
,
false
)
;
final
String
query
=
ITestS3SelectLandsat
.
SELECT_PROCESSING_LEVEL_NO_LIMIT
;
inputMust
(
jobConf
,
SELECT_SQL
,
query
)
;
inputMust
(
jobConf
,
SELECT_INPUT_COMPRESSION
,
COMPRESSION_OPT_GZIP
)
;
inputMust
(
jobConf
,
SELECT_INPUT_FORMAT
,
SELECT_FORMAT_CSV
)
;
inputMust
(
jobConf
,
CSV_INPUT_HEADER
,
CSV_HEADER_OPT_USE
)
;
inputMust
(
jobConf
,
SELECT_OUTPUT_FORMAT
,
SELECT_FORMAT_CSV
)
;
inputMust
(
jobConf
,
CSV_OUTPUT_QUOTE_FIELDS
,
CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED
)
;
enablePassthroughCodec
(
jobConf
,
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
+
query
)
)
{
int
exitCode
=
job
.
waitForCompletion
(
true
)
?
0
:
1
;
assertEquals
(
,
0
,
exitCode
)
;
}
Path
successPath
=
new
Path
(
output
,
)
;
SuccessData
success
=
SuccessData
.
load
(
fs
,
successPath
)
;
LOG
.
info
(
,
success
)
;
inputMust
(
jobConf
,
CSV_OUTPUT_QUOTE_FIELDS
,
CSV_OUTPUT_QUOTE_FIELDS_AS_NEEEDED
)
;
enablePassthroughCodec
(
jobConf
,
)
;
try
(
DurationInfo
ignored
=
new
DurationInfo
(
LOG
,
+
query
)
)
{
int
exitCode
=
job
.
waitForCompletion
(
true
)
?
0
:
1
;
assertEquals
(
,
0
,
exitCode
)
;
}
Path
successPath
=
new
Path
(
output
,
)
;
SuccessData
success
=
SuccessData
.
load
(
fs
,
successPath
)
;
LOG
.
info
(
,
success
)
;
LOG
.
info
(
,
query
)
;
final
AtomicLong
parts
=
new
AtomicLong
(
0
)
;
S3AUtils
.
applyLocatedFiles
(
fs
.
listFiles
(
output
,
false
)
,
(
status
)
->
{
Path
path
=
status
.
getPath
(
)
;
if
(
path
.
getName
(
)
.
startsWith
(
)
)
{
parts
.
incrementAndGet
(
)
;
String
result
=
readStringFromFile
(
path
)
;
protected
List
<
String
>
readOutput
(
final
File
outputFile
)
throws
IOException
{
try
(
FileReader
reader
=
new
FileReader
(
outputFile
)
)
{
final
List
<
String
>
lines
=
org
.
apache
.
commons
.
io
.
IOUtils
.
readLines
(
reader
)
;
job
.
setJarByClass
(
WordCount
.
class
)
;
job
.
setMapperClass
(
WordCount
.
TokenizerMapper
.
class
)
;
job
.
setCombinerClass
(
WordCount
.
IntSumReducer
.
class
)
;
job
.
setReducerClass
(
WordCount
.
IntSumReducer
.
class
)
;
job
.
setOutputKeyClass
(
Text
.
class
)
;
job
.
setOutputValueClass
(
IntWritable
.
class
)
;
FileInputFormat
.
addInputPath
(
job
,
input
)
;
FileOutputFormat
.
setOutputPath
(
job
,
output
)
;
int
exitCode
=
(
job
.
waitForCompletion
(
true
)
?
0
:
1
)
;
assertEquals
(
,
0
,
exitCode
)
;
Path
success
=
new
Path
(
output
,
_SUCCESS
)
;
FileStatus
status
=
fs
.
getFileStatus
(
success
)
;
assertTrue
(
+
success
,
status
.
getLen
(
)
>
0
)
;
SuccessData
successData
=
SuccessData
.
load
(
fs
,
success
)
;
String
commitDetails
=
successData
.
toString
(
)
;
public
void
init
(
)
throws
IOException
,
InterruptedException
{
when
(
mockClient
.
submitJob
(
any
(
JobID
.
class
)
,
any
(
String
.
class
)
,
any
(
Credentials
.
class
)
)
)
.
thenAnswer
(
invocation
->
{
final
Object
[
]
args
=
invocation
.
getArguments
(
)
;
String
name
=
(
String
)
args
[
1
]
;
public
static
Configuration
propagateAccountOptions
(
Configuration
source
,
String
accountName
)
{
Preconditions
.
checkArgument
(
StringUtils
.
isNotEmpty
(
accountName
)
,
)
;
final
String
accountPrefix
=
AZURE_AD_ACCOUNT_PREFIX
+
accountName
+
'.'
;
boolean
operationStatus
=
false
;
boolean
threadsEnabled
=
false
;
int
threadCount
=
this
.
threadCount
;
ThreadPoolExecutor
ioThreadPool
=
null
;
long
start
=
Time
.
monotonicNow
(
)
;
threadCount
=
Math
.
min
(
contents
.
length
,
threadCount
)
;
if
(
threadCount
>
1
)
{
try
{
ioThreadPool
=
getThreadPool
(
threadCount
)
;
threadsEnabled
=
true
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
,
threadCount
,
operation
,
key
,
config
)
;
}
}
else
{
LOG
.
warn
(
,
operation
,
threadCount
)
;
}
if
(
threadsEnabled
)
{
ioThreadPool
=
getThreadPool
(
threadCount
)
;
threadsEnabled
=
true
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
,
threadCount
,
operation
,
key
,
config
)
;
}
}
else
{
LOG
.
warn
(
,
operation
,
threadCount
)
;
}
if
(
threadsEnabled
)
{
LOG
.
debug
(
,
operation
,
threadCount
)
;
boolean
started
=
false
;
AzureFileSystemThreadRunnable
runnable
=
new
AzureFileSystemThreadRunnable
(
contents
,
threadOperation
,
operation
)
;
for
(
int
i
=
0
;
i
<
threadCount
&&
runnable
.
lastException
==
null
&&
runnable
.
operationStatus
;
i
++
)
{
try
{
ioThreadPool
.
execute
(
runnable
)
;
started
=
true
;
}
catch
(
RejectedExecutionException
ex
)
{
else
{
LOG
.
warn
(
,
operation
,
threadCount
)
;
}
if
(
threadsEnabled
)
{
LOG
.
debug
(
,
operation
,
threadCount
)
;
boolean
started
=
false
;
AzureFileSystemThreadRunnable
runnable
=
new
AzureFileSystemThreadRunnable
(
contents
,
threadOperation
,
operation
)
;
for
(
int
i
=
0
;
i
<
threadCount
&&
runnable
.
lastException
==
null
&&
runnable
.
operationStatus
;
i
++
)
{
try
{
ioThreadPool
.
execute
(
runnable
)
;
started
=
true
;
}
catch
(
RejectedExecutionException
ex
)
{
LOG
.
error
(
+
+
,
operation
,
key
,
config
)
;
}
}
ioThreadPool
.
shutdown
(
)
;
try
{
ioThreadPool
.
awaitTermination
(
Long
.
MAX_VALUE
,
TimeUnit
.
DAYS
)
;
AzureFileSystemThreadRunnable
runnable
=
new
AzureFileSystemThreadRunnable
(
contents
,
threadOperation
,
operation
)
;
for
(
int
i
=
0
;
i
<
threadCount
&&
runnable
.
lastException
==
null
&&
runnable
.
operationStatus
;
i
++
)
{
try
{
ioThreadPool
.
execute
(
runnable
)
;
started
=
true
;
}
catch
(
RejectedExecutionException
ex
)
{
LOG
.
error
(
+
+
,
operation
,
key
,
config
)
;
}
}
ioThreadPool
.
shutdown
(
)
;
try
{
ioThreadPool
.
awaitTermination
(
Long
.
MAX_VALUE
,
TimeUnit
.
DAYS
)
;
}
catch
(
InterruptedException
intrEx
)
{
ioThreadPool
.
shutdownNow
(
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
LOG
.
error
(
,
operation
,
key
)
;
}
int
threadsNotUsed
=
threadCount
-
runnable
.
threadsUsed
.
get
(
)
;
started
=
true
;
}
catch
(
RejectedExecutionException
ex
)
{
LOG
.
error
(
+
+
,
operation
,
key
,
config
)
;
}
}
ioThreadPool
.
shutdown
(
)
;
try
{
ioThreadPool
.
awaitTermination
(
Long
.
MAX_VALUE
,
TimeUnit
.
DAYS
)
;
}
catch
(
InterruptedException
intrEx
)
{
ioThreadPool
.
shutdownNow
(
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
LOG
.
error
(
,
operation
,
key
)
;
}
int
threadsNotUsed
=
threadCount
-
runnable
.
threadsUsed
.
get
(
)
;
if
(
threadsNotUsed
>
0
)
{
LOG
.
warn
(
,
threadsNotUsed
,
operation
,
key
)
;
}
if
(
!
started
)
{
threadsEnabled
=
false
;
ioThreadPool
.
shutdown
(
)
;
try
{
ioThreadPool
.
awaitTermination
(
Long
.
MAX_VALUE
,
TimeUnit
.
DAYS
)
;
}
catch
(
InterruptedException
intrEx
)
{
ioThreadPool
.
shutdownNow
(
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
LOG
.
error
(
,
operation
,
key
)
;
}
int
threadsNotUsed
=
threadCount
-
runnable
.
threadsUsed
.
get
(
)
;
if
(
threadsNotUsed
>
0
)
{
LOG
.
warn
(
,
threadsNotUsed
,
operation
,
key
)
;
}
if
(
!
started
)
{
threadsEnabled
=
false
;
LOG
.
info
(
,
operation
,
key
,
operation
)
;
}
else
{
IOException
lastException
=
runnable
.
lastException
;
LOG
.
error
(
,
operation
,
key
)
;
}
int
threadsNotUsed
=
threadCount
-
runnable
.
threadsUsed
.
get
(
)
;
if
(
threadsNotUsed
>
0
)
{
LOG
.
warn
(
,
threadsNotUsed
,
operation
,
key
)
;
}
if
(
!
started
)
{
threadsEnabled
=
false
;
LOG
.
info
(
,
operation
,
key
,
operation
)
;
}
else
{
IOException
lastException
=
runnable
.
lastException
;
if
(
lastException
==
null
&&
runnable
.
operationStatus
&&
runnable
.
filesProcessed
.
get
(
)
<
contents
.
length
)
{
LOG
.
error
(
,
operation
)
;
lastException
=
new
IOException
(
operation
+
)
;
}
if
(
lastException
!=
null
)
{
throw
lastException
;
}
operationStatus
=
runnable
.
operationStatus
;
this
.
storageInteractionLayer
=
new
StorageInterfaceImpl
(
)
;
}
else
{
this
.
storageInteractionLayer
=
new
SecureStorageInterfaceImpl
(
useLocalSasKeyMode
,
conf
)
;
}
}
configureAzureStorageSession
(
)
;
createAzureStorageSession
(
)
;
pageBlobDirs
=
getDirectorySet
(
KEY_PAGE_BLOB_DIRECTORIES
)
;
LOG
.
debug
(
,
setToString
(
pageBlobDirs
)
)
;
userAgentId
=
conf
.
get
(
USER_AGENT_ID_KEY
,
USER_AGENT_ID_DEFAULT
)
;
blockBlobWithCompationDirs
=
getDirectorySet
(
KEY_BLOCK_BLOB_WITH_COMPACTION_DIRECTORIES
)
;
LOG
.
debug
(
,
setToString
(
blockBlobWithCompationDirs
)
)
;
atomicRenameDirs
=
getDirectorySet
(
KEY_ATOMIC_RENAME_DIRECTORIES
)
;
String
hbaseRoot
;
try
{
hbaseRoot
=
verifyAndConvertToStandardFormat
(
sessionConfiguration
.
get
(
,
)
)
;
if
(
hbaseRoot
!=
null
)
{
}
int
cpuCores
=
2
*
Runtime
.
getRuntime
(
)
.
availableProcessors
(
)
;
concurrentWrites
=
sessionConfiguration
.
getInt
(
KEY_CONCURRENT_CONNECTION_VALUE_OUT
,
Math
.
min
(
cpuCores
,
DEFAULT_CONCURRENT_WRITES
)
)
;
minBackoff
=
sessionConfiguration
.
getInt
(
KEY_MIN_BACKOFF_INTERVAL
,
DEFAULT_MIN_BACKOFF_INTERVAL
)
;
maxBackoff
=
sessionConfiguration
.
getInt
(
KEY_MAX_BACKOFF_INTERVAL
,
DEFAULT_MAX_BACKOFF_INTERVAL
)
;
deltaBackoff
=
sessionConfiguration
.
getInt
(
KEY_BACKOFF_INTERVAL
,
DEFAULT_BACKOFF_INTERVAL
)
;
maxRetries
=
sessionConfiguration
.
getInt
(
KEY_MAX_IO_RETRIES
,
DEFAULT_MAX_RETRY_ATTEMPTS
)
;
storageInteractionLayer
.
setRetryPolicyFactory
(
new
RetryExponentialRetry
(
minBackoff
,
deltaBackoff
,
maxBackoff
,
maxRetries
)
)
;
selfThrottlingEnabled
=
sessionConfiguration
.
getBoolean
(
KEY_SELF_THROTTLE_ENABLE
,
DEFAULT_SELF_THROTTLE_ENABLE
)
;
selfThrottlingReadFactor
=
sessionConfiguration
.
getFloat
(
KEY_SELF_THROTTLE_READ_FACTOR
,
DEFAULT_SELF_THROTTLE_READ_FACTOR
)
;
selfThrottlingWriteFactor
=
sessionConfiguration
.
getFloat
(
KEY_SELF_THROTTLE_WRITE_FACTOR
,
DEFAULT_SELF_THROTTLE_WRITE_FACTOR
)
;
if
(
!
selfThrottlingEnabled
)
{
autoThrottlingEnabled
=
sessionConfiguration
.
getBoolean
(
KEY_AUTO_THROTTLE_ENABLE
,
DEFAULT_AUTO_THROTTLE_ENABLE
)
;
if
(
autoThrottlingEnabled
)
{
ClientThrottlingIntercept
.
initializeSingleton
(
)
;
}
}
else
{
}
instrumentation
.
setAccountName
(
accountName
)
;
String
containerName
=
getContainerFromAuthority
(
sessionUri
)
;
instrumentation
.
setContainerName
(
containerName
)
;
if
(
isStorageEmulatorAccount
(
accountName
)
)
{
connectUsingCredentials
(
accountName
,
null
,
containerName
)
;
return
;
}
if
(
useSecureMode
)
{
connectToAzureStorageInSecureMode
(
accountName
,
containerName
,
sessionUri
)
;
return
;
}
String
propertyValue
=
sessionConfiguration
.
get
(
KEY_ACCOUNT_SAS_PREFIX
+
containerName
+
+
accountName
)
;
if
(
propertyValue
!=
null
)
{
connectUsingSASCredentials
(
accountName
,
containerName
,
propertyValue
)
;
return
;
}
propertyValue
=
getAccountKeyFromConfiguration
(
accountName
,
sessionConfiguration
)
;
if
(
StringUtils
.
isNotEmpty
(
propertyValue
)
)
{
final
String
errMsg
=
String
.
format
(
,
sessionUri
)
;
throw
new
AssertionError
(
errMsg
)
;
}
LOG
.
debug
(
,
key
)
;
try
{
if
(
checkContainer
(
ContainerAccessType
.
PureRead
)
==
ContainerState
.
DoesntExist
)
{
return
null
;
}
if
(
key
.
equals
(
)
)
{
return
new
FileMetadata
(
key
,
0
,
defaultPermissionNoBlobMetadata
(
)
,
BlobMaterialization
.
Implicit
,
hadoopBlockSize
)
;
}
CloudBlobWrapper
blob
=
getBlobReference
(
key
)
;
if
(
null
!=
blob
&&
blob
.
exists
(
getInstrumentedContext
(
)
)
)
{
LOG
.
debug
(
,
key
)
;
try
{
blob
.
downloadAttributes
(
getInstrumentedContext
(
)
)
;
BlobProperties
properties
=
blob
.
getProperties
(
)
;
if
(
retrieveFolderAttribute
(
blob
)
)
{
LOG
.
debug
(
,
key
)
;
try
{
if
(
checkContainer
(
ContainerAccessType
.
PureRead
)
==
ContainerState
.
DoesntExist
)
{
return
null
;
}
if
(
key
.
equals
(
)
)
{
return
new
FileMetadata
(
key
,
0
,
defaultPermissionNoBlobMetadata
(
)
,
BlobMaterialization
.
Implicit
,
hadoopBlockSize
)
;
}
CloudBlobWrapper
blob
=
getBlobReference
(
key
)
;
if
(
null
!=
blob
&&
blob
.
exists
(
getInstrumentedContext
(
)
)
)
{
LOG
.
debug
(
,
key
)
;
try
{
blob
.
downloadAttributes
(
getInstrumentedContext
(
)
)
;
BlobProperties
properties
=
blob
.
getProperties
(
)
;
if
(
retrieveFolderAttribute
(
blob
)
)
{
LOG
.
debug
(
,
key
)
;
return
new
FileMetadata
(
key
,
properties
.
getLastModified
(
)
.
getTime
(
)
,
getPermissionStatus
(
blob
)
,
BlobMaterialization
.
Explicit
,
hadoopBlockSize
)
;
Throwable
t
=
e
.
getCause
(
)
;
if
(
t
instanceof
StorageException
)
{
StorageException
se
=
(
StorageException
)
t
;
if
(
.
equals
(
se
.
getErrorCode
(
)
)
)
{
SelfRenewingLease
lease
=
null
;
try
{
lease
=
acquireLease
(
key
)
;
return
delete
(
key
,
lease
)
;
}
catch
(
AzureException
e3
)
{
LOG
.
warn
(
+
key
+
+
e3
.
getMessage
(
)
)
;
throw
e3
;
}
finally
{
try
{
if
(
lease
!=
null
)
{
lease
.
free
(
)
;
@
Override
public
void
rename
(
String
srcKey
,
String
dstKey
,
boolean
acquireLease
,
SelfRenewingLease
existingLease
,
boolean
overwriteDestination
)
throws
IOException
{
@
Override
public
SelfRenewingLease
acquireLease
(
String
key
)
throws
AzureException
{
if
(
closed
)
{
return
;
}
flush
(
)
;
ioThreadPool
.
shutdown
(
)
;
try
{
if
(
!
ioThreadPool
.
awaitTermination
(
CLOSE_UPLOAD_DELAY
,
TimeUnit
.
MINUTES
)
)
{
LOG
.
error
(
+
+
,
key
)
;
NativeAzureFileSystemHelper
.
logAllLiveStackTraces
(
)
;
throw
new
AzureException
(
)
;
}
}
catch
(
InterruptedException
ex
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
}
if
(
firstError
.
get
(
)
==
null
&&
blobExist
)
{
try
{
lease
.
free
(
)
;
}
catch
(
StorageException
ex
)
{
private
void
writeBlockRequestInternal
(
String
blockId
,
ByteBuffer
dataPayload
,
boolean
bufferPoolBuffer
)
{
IOException
lastLocalException
=
null
;
int
uploadRetryAttempts
=
0
;
while
(
uploadRetryAttempts
<
MAX_BLOCK_UPLOAD_RETRIES
)
{
try
{
long
startTime
=
System
.
nanoTime
(
)
;
blob
.
uploadBlock
(
blockId
,
accessCondition
,
new
ByteArrayInputStream
(
dataPayload
.
array
(
)
)
,
dataPayload
.
position
(
)
,
new
BlobRequestOptions
(
)
,
opContext
)
;
private
void
writeBlockListRequestInternal
(
)
{
IOException
lastLocalException
=
null
;
int
uploadRetryAttempts
=
0
;
while
(
uploadRetryAttempts
<
MAX_BLOCK_UPLOAD_RETRIES
)
{
try
{
long
startTime
=
System
.
nanoTime
(
)
;
blob
.
commitBlockList
(
blockEntries
,
accessCondition
,
new
BlobRequestOptions
(
)
,
opContext
)
;
public
void
put
(
K
key
,
V
value
)
{
if
(
isEnabled
)
{
double
reductionFactor
=
(
consecutiveNoErrorCount
*
analysisPeriodMs
>=
RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS
)
?
RAPID_SLEEP_DECREASE_FACTOR
:
SLEEP_DECREASE_FACTOR
;
newSleepDuration
=
sleepDuration
*
reductionFactor
;
}
else
if
(
errorPercentage
<
MAX_EQUILIBRIUM_ERROR_PERCENTAGE
)
{
newSleepDuration
=
sleepDuration
;
}
else
{
consecutiveNoErrorCount
=
0
;
double
additionalDelayNeeded
=
5
*
analysisPeriodMs
;
if
(
bytesSuccessful
>
0
)
{
additionalDelayNeeded
=
(
bytesSuccessful
+
bytesFailed
)
*
periodMs
/
bytesSuccessful
-
periodMs
;
}
newSleepDuration
=
additionalDelayNeeded
/
(
operationsFailed
+
operationsSuccessful
)
;
final
double
maxSleepDuration
=
analysisPeriodMs
;
final
double
minSleepDuration
=
sleepDuration
*
SLEEP_INCREASE_FACTOR
;
newSleepDuration
=
Math
.
max
(
newSleepDuration
,
minSleepDuration
)
+
1
;
newSleepDuration
=
Math
.
min
(
newSleepDuration
,
maxSleepDuration
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
URI
getContainerSASUri
(
String
accountName
,
String
container
)
throws
SASKeyGenerationException
{
private
CloudStorageAccount
getSASKeyBasedStorageAccountInstance
(
String
accountName
)
throws
SASKeyGenerationException
{
super
.
initialize
(
uri
,
conf
)
;
if
(
store
==
null
)
{
store
=
createDefaultStore
(
conf
)
;
}
instrumentation
=
new
AzureFileSystemInstrumentation
(
conf
)
;
if
(
!
conf
.
getBoolean
(
SKIP_AZURE_METRICS_PROPERTY_NAME
,
false
)
)
{
AzureFileSystemMetricsSystem
.
fileSystemStarted
(
)
;
metricsSourceName
=
newMetricsSourceName
(
)
;
String
sourceDesc
=
;
AzureFileSystemMetricsSystem
.
registerSource
(
metricsSourceName
,
sourceDesc
,
instrumentation
)
;
}
store
.
initialize
(
uri
,
conf
,
instrumentation
)
;
setConf
(
conf
)
;
this
.
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
this
.
uri
=
URI
.
create
(
uri
.
getScheme
(
)
+
+
uri
.
getAuthority
(
)
)
;
this
.
workingDir
=
new
Path
(
,
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
)
.
makeQualified
(
getUri
(
)
,
getWorkingDirectory
(
)
)
;
this
.
appendSupportEnabled
=
conf
.
getBoolean
(
APPEND_SUPPORT_ENABLE_PROPERTY_NAME
,
false
)
;
private
FSDataOutputStream
create
(
Path
f
,
FsPermission
permission
,
boolean
overwrite
,
boolean
createParent
,
int
bufferSize
,
short
replication
,
long
blockSize
,
Progressable
progress
,
SelfRenewingLease
parentFolderLease
)
throws
FileAlreadyExistsException
,
IOException
{
private
boolean
deleteWithAuthEnabled
(
Path
f
,
boolean
recursive
,
boolean
skipParentFolderLastModifiedTimeUpdate
)
throws
IOException
{
return
false
;
}
FileMetadata
parentMetadata
=
null
;
String
parentKey
=
null
;
if
(
parentPath
!=
null
)
{
parentKey
=
pathToKey
(
parentPath
)
;
try
{
parentMetadata
=
store
.
retrieveMetadata
(
parentKey
)
;
}
catch
(
IOException
e
)
{
Throwable
innerException
=
checkForAzureStorageException
(
e
)
;
if
(
innerException
instanceof
StorageException
)
{
if
(
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
}
}
throw
e
;
}
if
(
parentMetadata
==
null
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
if
(
parentMetadata
==
null
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
}
if
(
!
parentMetadata
.
isDirectory
(
)
)
{
throw
new
AzureException
(
+
f
+
+
parentPath
+
)
;
}
}
if
(
!
metaFile
.
isDirectory
(
)
)
{
if
(
parentPath
!=
null
&&
parentPath
.
getParent
(
)
!=
null
)
{
if
(
parentMetadata
.
getBlobMaterialization
(
)
==
BlobMaterialization
.
Implicit
)
{
LOG
.
debug
(
+
+
,
f
,
parentKey
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
else
{
if
(
!
skipParentFolderLastModifiedTimeUpdate
)
{
updateParentFolderLastModifiedTime
(
key
)
;
}
}
}
if
(
isStickyBitCheckViolated
(
metaFile
,
parentMetadata
)
)
{
throw
new
WasbAuthorizationException
(
String
.
format
(
+
,
parentPath
,
f
)
)
;
}
try
{
if
(
!
parentMetadata
.
isDirectory
(
)
)
{
throw
new
AzureException
(
+
f
+
+
parentPath
+
)
;
}
}
if
(
!
metaFile
.
isDirectory
(
)
)
{
if
(
parentPath
!=
null
&&
parentPath
.
getParent
(
)
!=
null
)
{
if
(
parentMetadata
.
getBlobMaterialization
(
)
==
BlobMaterialization
.
Implicit
)
{
LOG
.
debug
(
+
+
,
f
,
parentKey
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
else
{
if
(
!
skipParentFolderLastModifiedTimeUpdate
)
{
updateParentFolderLastModifiedTime
(
key
)
;
}
}
}
if
(
isStickyBitCheckViolated
(
metaFile
,
parentMetadata
)
)
{
throw
new
WasbAuthorizationException
(
String
.
format
(
+
,
parentPath
,
f
)
)
;
}
try
{
if
(
store
.
delete
(
key
)
)
{
instrumentation
.
fileDeleted
(
)
;
return
false
;
}
throw
e
;
}
}
else
{
LOG
.
debug
(
,
f
)
;
if
(
parentPath
!=
null
&&
parentPath
.
getParent
(
)
!=
null
)
{
if
(
parentMetadata
.
getBlobMaterialization
(
)
==
BlobMaterialization
.
Implicit
)
{
LOG
.
debug
(
+
+
,
f
,
parentKey
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
}
if
(
!
metaFile
.
getKey
(
)
.
equals
(
)
&&
isStickyBitCheckViolated
(
metaFile
,
parentMetadata
)
)
{
throw
new
WasbAuthorizationException
(
String
.
format
(
+
,
parentPath
,
f
)
)
;
}
ArrayList
<
FileMetadata
>
fileMetadataList
=
new
ArrayList
<
>
(
)
;
boolean
isPartialDelete
=
false
;
long
start
=
Time
.
monotonicNow
(
)
;
try
{
isPartialDelete
=
getFolderContentsToDelete
(
metaFile
,
fileMetadataList
)
;
}
}
else
{
LOG
.
debug
(
,
f
)
;
if
(
parentPath
!=
null
&&
parentPath
.
getParent
(
)
!=
null
)
{
if
(
parentMetadata
.
getBlobMaterialization
(
)
==
BlobMaterialization
.
Implicit
)
{
LOG
.
debug
(
+
+
,
f
,
parentKey
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
}
if
(
!
metaFile
.
getKey
(
)
.
equals
(
)
&&
isStickyBitCheckViolated
(
metaFile
,
parentMetadata
)
)
{
throw
new
WasbAuthorizationException
(
String
.
format
(
+
,
parentPath
,
f
)
)
;
}
ArrayList
<
FileMetadata
>
fileMetadataList
=
new
ArrayList
<
>
(
)
;
boolean
isPartialDelete
=
false
;
long
start
=
Time
.
monotonicNow
(
)
;
try
{
isPartialDelete
=
getFolderContentsToDelete
(
metaFile
,
fileMetadataList
)
;
}
catch
(
IOException
e
)
{
Throwable
innerException
=
checkForAzureStorageException
(
e
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
}
if
(
!
metaFile
.
getKey
(
)
.
equals
(
)
&&
isStickyBitCheckViolated
(
metaFile
,
parentMetadata
)
)
{
throw
new
WasbAuthorizationException
(
String
.
format
(
+
,
parentPath
,
f
)
)
;
}
ArrayList
<
FileMetadata
>
fileMetadataList
=
new
ArrayList
<
>
(
)
;
boolean
isPartialDelete
=
false
;
long
start
=
Time
.
monotonicNow
(
)
;
try
{
isPartialDelete
=
getFolderContentsToDelete
(
metaFile
,
fileMetadataList
)
;
}
catch
(
IOException
e
)
{
Throwable
innerException
=
checkForAzureStorageException
(
e
)
;
if
(
innerException
instanceof
StorageException
&&
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
return
false
;
}
throw
e
;
}
long
end
=
Time
.
monotonicNow
(
)
;
LOG
.
debug
(
,
fileMetadataList
.
size
(
)
,
(
end
-
start
)
)
;
private
boolean
deleteWithoutAuth
(
Path
f
,
boolean
recursive
,
boolean
skipParentFolderLastModifiedTimeUpdate
)
throws
IOException
{
return
false
;
}
if
(
!
metaFile
.
isDirectory
(
)
)
{
if
(
parentPath
.
getParent
(
)
!=
null
)
{
String
parentKey
=
pathToKey
(
parentPath
)
;
FileMetadata
parentMetadata
=
null
;
try
{
parentMetadata
=
store
.
retrieveMetadata
(
parentKey
)
;
}
catch
(
IOException
e
)
{
Throwable
innerException
=
checkForAzureStorageException
(
e
)
;
if
(
innerException
instanceof
StorageException
)
{
if
(
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
}
}
throw
e
;
}
if
(
parentMetadata
==
null
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
throw
e
;
}
if
(
parentMetadata
==
null
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
}
if
(
!
parentMetadata
.
isDirectory
(
)
)
{
throw
new
AzureException
(
+
f
+
+
parentPath
+
)
;
}
if
(
parentMetadata
.
getBlobMaterialization
(
)
==
BlobMaterialization
.
Implicit
)
{
LOG
.
debug
(
+
+
,
f
,
parentKey
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
else
{
if
(
!
skipParentFolderLastModifiedTimeUpdate
)
{
updateParentFolderLastModifiedTime
(
key
)
;
}
}
}
try
{
if
(
store
.
delete
(
key
)
)
{
instrumentation
.
fileDeleted
(
)
;
}
else
{
}
}
}
try
{
if
(
store
.
delete
(
key
)
)
{
instrumentation
.
fileDeleted
(
)
;
}
else
{
return
false
;
}
}
catch
(
IOException
e
)
{
Throwable
innerException
=
checkForAzureStorageException
(
e
)
;
if
(
innerException
instanceof
StorageException
&&
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
return
false
;
}
throw
e
;
}
}
else
{
LOG
.
debug
(
,
f
)
;
if
(
parentPath
.
getParent
(
)
!=
null
)
{
String
parentKey
=
pathToKey
(
parentPath
)
;
FileMetadata
parentMetadata
=
null
;
catch
(
IOException
e
)
{
Throwable
innerException
=
checkForAzureStorageException
(
e
)
;
if
(
innerException
instanceof
StorageException
)
{
if
(
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
}
}
throw
e
;
}
if
(
parentMetadata
==
null
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
}
if
(
parentMetadata
.
getBlobMaterialization
(
)
==
BlobMaterialization
.
Implicit
)
{
LOG
.
debug
(
+
+
,
f
,
parentKey
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
}
long
start
=
Time
.
monotonicNow
(
)
;
final
FileMetadata
[
]
contents
;
try
{
contents
=
store
.
list
(
key
,
AZURE_LIST_ALL
,
AZURE_UNBOUNDED_DEPTH
)
;
throw
e
;
}
if
(
parentMetadata
==
null
)
{
throw
new
IOException
(
+
f
+
+
parentPath
+
)
;
}
if
(
parentMetadata
.
getBlobMaterialization
(
)
==
BlobMaterialization
.
Implicit
)
{
LOG
.
debug
(
+
+
,
f
,
parentKey
)
;
store
.
storeEmptyFolder
(
parentKey
,
createPermissionStatus
(
FsPermission
.
getDefault
(
)
)
)
;
}
}
long
start
=
Time
.
monotonicNow
(
)
;
final
FileMetadata
[
]
contents
;
try
{
contents
=
store
.
list
(
key
,
AZURE_LIST_ALL
,
AZURE_UNBOUNDED_DEPTH
)
;
}
catch
(
IOException
e
)
{
Throwable
innerException
=
checkForAzureStorageException
(
e
)
;
if
(
innerException
instanceof
StorageException
&&
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
return
false
;
}
throw
e
;
while
(
!
foldersToProcess
.
empty
(
)
)
{
FileMetadata
currentFolder
=
foldersToProcess
.
pop
(
)
;
Path
currentPath
=
makeAbsolute
(
currentFolder
.
getPath
(
)
)
;
boolean
canDeleteChildren
=
true
;
try
{
performAuthCheck
(
currentPath
,
WasbAuthorizationOperations
.
WRITE
,
,
pathToDelete
)
;
}
catch
(
WasbAuthorizationException
we
)
{
LOG
.
debug
(
,
currentPath
)
;
canDeleteChildren
=
false
;
}
if
(
canDeleteChildren
)
{
FileMetadata
[
]
fileMetadataList
=
store
.
list
(
currentFolder
.
getKey
(
)
,
AZURE_LIST_ALL
,
maxListingDepth
)
;
for
(
FileMetadata
childItem
:
fileMetadataList
)
{
if
(
isStickyBitCheckViolated
(
childItem
,
currentFolder
,
false
)
)
{
canDeleteChildren
=
false
;
Path
filePath
=
makeAbsolute
(
childItem
.
getPath
(
)
)
;
}
catch
(
WasbAuthorizationException
we
)
{
LOG
.
debug
(
,
currentPath
)
;
canDeleteChildren
=
false
;
}
if
(
canDeleteChildren
)
{
FileMetadata
[
]
fileMetadataList
=
store
.
list
(
currentFolder
.
getKey
(
)
,
AZURE_LIST_ALL
,
maxListingDepth
)
;
for
(
FileMetadata
childItem
:
fileMetadataList
)
{
if
(
isStickyBitCheckViolated
(
childItem
,
currentFolder
,
false
)
)
{
canDeleteChildren
=
false
;
Path
filePath
=
makeAbsolute
(
childItem
.
getPath
(
)
)
;
LOG
.
error
(
+
,
filePath
)
;
}
else
{
if
(
childItem
.
isDirectory
(
)
)
{
foldersToProcess
.
push
(
childItem
)
;
}
folderContentsMap
.
put
(
childItem
.
getKey
(
)
,
childItem
)
;
}
}
}
else
{
if
(
canDeleteChildren
)
{
FileMetadata
[
]
fileMetadataList
=
store
.
list
(
currentFolder
.
getKey
(
)
,
AZURE_LIST_ALL
,
maxListingDepth
)
;
for
(
FileMetadata
childItem
:
fileMetadataList
)
{
if
(
isStickyBitCheckViolated
(
childItem
,
currentFolder
,
false
)
)
{
canDeleteChildren
=
false
;
Path
filePath
=
makeAbsolute
(
childItem
.
getPath
(
)
)
;
LOG
.
error
(
+
,
filePath
)
;
}
else
{
if
(
childItem
.
isDirectory
(
)
)
{
foldersToProcess
.
push
(
childItem
)
;
}
folderContentsMap
.
put
(
childItem
.
getKey
(
)
,
childItem
)
;
}
}
}
else
{
LOG
.
error
(
+
,
currentPath
)
;
}
if
(
!
canDeleteChildren
)
{
String
pathToRemove
=
currentFolder
.
getKey
(
)
;
@
Override
public
FileStatus
getFileStatus
(
Path
f
)
throws
FileNotFoundException
,
IOException
{
private
FileStatus
getFileStatusInternal
(
Path
f
)
throws
FileNotFoundException
,
IOException
{
Path
absolutePath
=
makeAbsolute
(
f
)
;
String
key
=
pathToKey
(
absolutePath
)
;
if
(
key
.
length
(
)
==
0
)
{
return
new
FileStatus
(
0
,
true
,
1
,
store
.
getHadoopBlockSize
(
)
,
0
,
0
,
FsPermission
.
getDefault
(
)
,
,
,
absolutePath
.
makeQualified
(
getUri
(
)
,
getWorkingDirectory
(
)
)
)
;
}
FileMetadata
meta
=
null
;
try
{
meta
=
store
.
retrieveMetadata
(
key
)
;
}
catch
(
Exception
ex
)
{
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
&&
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
throw
new
FileNotFoundException
(
String
.
format
(
,
key
)
)
;
}
throw
ex
;
}
if
(
meta
!=
null
)
{
if
(
meta
.
isDirectory
(
)
)
{
@
Override
public
FileStatus
[
]
listStatus
(
Path
f
)
throws
FileNotFoundException
,
IOException
{
boolean
renamed
=
conditionalRedoFolderRenames
(
listing
)
;
if
(
renamed
)
{
listing
=
listWithErrorHandling
(
key
,
AZURE_LIST_ALL
,
1
)
;
}
FileMetadata
[
]
result
=
null
;
if
(
key
.
equals
(
)
)
{
ArrayList
<
FileMetadata
>
status
=
new
ArrayList
<
>
(
listing
.
length
)
;
for
(
FileMetadata
fileMetadata
:
listing
)
{
if
(
fileMetadata
.
isDirectory
(
)
)
{
if
(
fileMetadata
.
getKey
(
)
.
equals
(
AZURE_TEMP_FOLDER
)
)
{
continue
;
}
status
.
add
(
updateFileStatusPath
(
fileMetadata
,
fileMetadata
.
getPath
(
)
)
)
;
}
else
{
status
.
add
(
updateFileStatusPath
(
fileMetadata
,
fileMetadata
.
getPath
(
)
)
)
;
}
}
result
=
status
.
toArray
(
new
FileMetadata
[
0
]
)
;
}
else
{
private
Path
getAncestor
(
Path
f
)
throws
IOException
{
for
(
Path
current
=
f
,
parent
=
current
.
getParent
(
)
;
parent
!=
null
;
current
=
parent
,
parent
=
current
.
getParent
(
)
)
{
String
currentKey
=
pathToKey
(
current
)
;
FileMetadata
currentMetadata
=
store
.
retrieveMetadata
(
currentKey
)
;
if
(
currentMetadata
!=
null
&&
currentMetadata
.
isDirectory
(
)
)
{
Path
ancestor
=
currentMetadata
.
getPath
(
)
;
public
boolean
mkdirs
(
Path
f
,
FsPermission
permission
,
boolean
noUmask
)
throws
IOException
{
@
Override
public
FSDataInputStream
open
(
Path
f
,
int
bufferSize
)
throws
FileNotFoundException
,
IOException
{
@
Override
public
boolean
rename
(
Path
src
,
Path
dst
)
throws
FileNotFoundException
,
IOException
{
FolderRenamePending
renamePending
=
null
;
Path
absoluteSrcPath
=
makeAbsolute
(
src
)
;
Path
srcParentFolder
=
absoluteSrcPath
.
getParent
(
)
;
if
(
srcParentFolder
==
null
)
{
return
false
;
}
String
srcKey
=
pathToKey
(
absoluteSrcPath
)
;
if
(
srcKey
.
length
(
)
==
0
)
{
return
false
;
}
performAuthCheck
(
srcParentFolder
,
WasbAuthorizationOperations
.
WRITE
,
,
absoluteSrcPath
)
;
if
(
this
.
azureAuthorization
)
{
try
{
performStickyBitCheckForRenameOperation
(
absoluteSrcPath
,
srcParentFolder
)
;
}
catch
(
FileNotFoundException
ex
)
{
return
false
;
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
checkForAzureStorageException
(
ex
)
;
}
catch
(
FileNotFoundException
ex
)
{
return
false
;
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
&&
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
+
,
srcKey
)
;
return
false
;
}
throw
ex
;
}
}
Path
absoluteDstPath
=
makeAbsolute
(
dst
)
;
Path
dstParentFolder
=
absoluteDstPath
.
getParent
(
)
;
String
dstKey
=
pathToKey
(
absoluteDstPath
)
;
FileMetadata
dstMetadata
=
null
;
try
{
dstMetadata
=
store
.
retrieveMetadata
(
dstKey
)
;
}
catch
(
IOException
ex
)
{
if
(
innerException
instanceof
StorageException
&&
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
+
,
srcKey
)
;
return
false
;
}
throw
ex
;
}
}
Path
absoluteDstPath
=
makeAbsolute
(
dst
)
;
Path
dstParentFolder
=
absoluteDstPath
.
getParent
(
)
;
String
dstKey
=
pathToKey
(
absoluteDstPath
)
;
FileMetadata
dstMetadata
=
null
;
try
{
dstMetadata
=
store
.
retrieveMetadata
(
dstKey
)
;
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
)
{
if
(
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
+
,
dstKey
)
;
return
false
;
}
throw
ex
;
}
}
Path
absoluteDstPath
=
makeAbsolute
(
dst
)
;
Path
dstParentFolder
=
absoluteDstPath
.
getParent
(
)
;
String
dstKey
=
pathToKey
(
absoluteDstPath
)
;
FileMetadata
dstMetadata
=
null
;
try
{
dstMetadata
=
store
.
retrieveMetadata
(
dstKey
)
;
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
)
{
if
(
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
+
,
dstKey
)
;
}
}
else
{
throw
ex
;
try
{
dstMetadata
=
store
.
retrieveMetadata
(
dstKey
)
;
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
)
{
if
(
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
+
,
dstKey
)
;
}
}
else
{
throw
ex
;
}
}
if
(
dstMetadata
!=
null
&&
dstMetadata
.
isDirectory
(
)
)
{
performAuthCheck
(
absoluteDstPath
,
WasbAuthorizationOperations
.
WRITE
,
,
absoluteDstPath
)
;
dstKey
=
pathToKey
(
makeAbsolute
(
new
Path
(
dst
,
src
.
getName
(
)
)
)
)
;
LOG
.
debug
(
+
,
dst
,
dstKey
)
;
}
else
if
(
dstMetadata
!=
null
)
{
LOG
.
debug
(
+
,
dst
)
;
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
)
{
if
(
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
+
,
dstKey
)
;
}
}
else
{
throw
ex
;
}
}
if
(
dstMetadata
!=
null
&&
dstMetadata
.
isDirectory
(
)
)
{
performAuthCheck
(
absoluteDstPath
,
WasbAuthorizationOperations
.
WRITE
,
,
absoluteDstPath
)
;
dstKey
=
pathToKey
(
makeAbsolute
(
new
Path
(
dst
,
src
.
getName
(
)
)
)
)
;
LOG
.
debug
(
+
,
dst
,
dstKey
)
;
}
else
if
(
dstMetadata
!=
null
)
{
LOG
.
debug
(
+
,
dst
)
;
return
false
;
}
else
{
FileMetadata
parentOfDestMetadata
=
null
;
LOG
.
debug
(
+
,
dstKey
)
;
}
}
else
{
throw
ex
;
}
}
if
(
dstMetadata
!=
null
&&
dstMetadata
.
isDirectory
(
)
)
{
performAuthCheck
(
absoluteDstPath
,
WasbAuthorizationOperations
.
WRITE
,
,
absoluteDstPath
)
;
dstKey
=
pathToKey
(
makeAbsolute
(
new
Path
(
dst
,
src
.
getName
(
)
)
)
)
;
LOG
.
debug
(
+
,
dst
,
dstKey
)
;
}
else
if
(
dstMetadata
!=
null
)
{
LOG
.
debug
(
+
,
dst
)
;
return
false
;
}
else
{
FileMetadata
parentOfDestMetadata
=
null
;
try
{
parentOfDestMetadata
=
store
.
retrieveMetadata
(
pathToKey
(
absoluteDstPath
.
getParent
(
)
)
)
;
}
catch
(
IOException
ex
)
{
else
if
(
!
parentOfDestMetadata
.
isDirectory
(
)
)
{
LOG
.
debug
(
+
,
dst
)
;
return
false
;
}
else
{
performAuthCheck
(
dstParentFolder
,
WasbAuthorizationOperations
.
WRITE
,
,
absoluteDstPath
)
;
}
}
FileMetadata
srcMetadata
=
null
;
try
{
srcMetadata
=
store
.
retrieveMetadata
(
srcKey
)
;
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
&&
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
,
src
)
;
return
false
;
}
throw
ex
;
}
if
(
srcMetadata
==
null
)
{
performAuthCheck
(
dstParentFolder
,
WasbAuthorizationOperations
.
WRITE
,
,
absoluteDstPath
)
;
}
}
FileMetadata
srcMetadata
=
null
;
try
{
srcMetadata
=
store
.
retrieveMetadata
(
srcKey
)
;
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
if
(
innerException
instanceof
StorageException
&&
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
)
{
LOG
.
debug
(
,
src
)
;
return
false
;
}
throw
ex
;
}
if
(
srcMetadata
==
null
)
{
LOG
.
debug
(
,
src
)
;
return
false
;
}
else
if
(
!
srcMetadata
.
isDirectory
(
)
)
{
LOG
.
debug
(
,
src
)
;
store
.
updateFolderLastModifiedTime
(
parentKey
,
lease
)
;
}
catch
(
AzureException
e
)
{
String
errorCode
=
;
try
{
StorageException
e2
=
(
StorageException
)
e
.
getCause
(
)
;
errorCode
=
e2
.
getErrorCode
(
)
;
}
catch
(
Exception
e3
)
{
}
if
(
errorCode
.
equals
(
)
)
{
throw
new
FileNotFoundException
(
+
parentKey
)
;
}
LOG
.
warn
(
,
parentKey
,
e
.
getMessage
(
)
)
;
throw
e
;
}
finally
{
try
{
if
(
lease
!=
null
)
{
lease
.
free
(
)
;
private
void
performStickyBitCheckForRenameOperation
(
Path
srcPath
,
Path
srcParentPath
)
throws
FileNotFoundException
,
WasbAuthorizationException
,
IOException
{
String
srcKey
=
pathToKey
(
srcPath
)
;
FileMetadata
srcMetadata
=
null
;
srcMetadata
=
store
.
retrieveMetadata
(
srcKey
)
;
if
(
srcMetadata
==
null
)
{
public
void
recoverFilesWithDanglingTempData
(
Path
root
,
Path
destination
)
throws
IOException
{
public
void
deleteFilesWithDanglingTempData
(
Path
root
)
throws
IOException
{
@
VisibleForTesting
public
String
getOwnerForPath
(
Path
absolutePath
)
throws
IOException
{
String
owner
=
;
FileMetadata
meta
=
null
;
String
key
=
pathToKey
(
absolutePath
)
;
try
{
meta
=
store
.
retrieveMetadata
(
key
)
;
if
(
meta
!=
null
)
{
owner
=
meta
.
getOwner
(
)
;
String
owner
=
;
FileMetadata
meta
=
null
;
String
key
=
pathToKey
(
absolutePath
)
;
try
{
meta
=
store
.
retrieveMetadata
(
key
)
;
if
(
meta
!=
null
)
{
owner
=
meta
.
getOwner
(
)
;
LOG
.
debug
(
,
owner
,
absolutePath
)
;
}
else
{
LOG
.
debug
(
,
absolutePath
)
;
}
}
catch
(
IOException
ex
)
{
Throwable
innerException
=
NativeAzureFileSystemHelper
.
checkForAzureStorageException
(
ex
)
;
boolean
isfileNotFoundException
=
innerException
instanceof
StorageException
&&
NativeAzureFileSystemHelper
.
isFileNotFoundException
(
(
StorageException
)
innerException
)
;
if
(
!
isfileNotFoundException
)
{
String
errorMsg
=
+
absolutePath
;
@
Override
public
void
createBlobClient
(
CloudStorageAccount
account
)
{
String
errorMsg
=
+
;
@
Override
public
void
createBlobClient
(
URI
baseUri
)
{
String
errorMsg
=
+
;
@
Override
public
void
createBlobClient
(
URI
baseUri
,
StorageCredentials
credentials
)
{
String
errorMsg
=
+
;
@
Override
public
StorageCredentials
getCredentials
(
)
{
String
errorMsg
=
+
;
private
Token
<
?
>
getDelegationToken
(
UserGroupInformation
userGroupInformation
)
throws
IOException
{
if
(
this
.
delegationToken
==
null
)
{
Token
<
?
>
token
=
null
;
for
(
Token
iterToken
:
userGroupInformation
.
getTokens
(
)
)
{
if
(
iterToken
.
getKind
(
)
.
equals
(
WasbDelegationTokenIdentifier
.
TOKEN_KIND
)
)
{
token
=
iterToken
;
sleepMultiple
=
(
1
/
writeFactor
)
-
1
;
}
else
{
operationIsRead
=
true
;
sleepMultiple
=
(
1
/
readFactor
)
-
1
;
}
long
sleepDuration
=
(
long
)
(
sleepMultiple
*
lastLatency
)
;
if
(
sleepDuration
<
0
)
{
sleepDuration
=
0
;
}
if
(
sleepDuration
>
0
)
{
try
{
Thread
.
sleep
(
sleepDuration
)
;
}
catch
(
InterruptedException
ie
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
}
sendEvent
.
getRequestResult
(
)
.
setStartDate
(
new
Date
(
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
boolean
isFirstRequest
=
(
lastLatency
==
0
)
;
try
{
final
RetryPolicy
.
RetryAction
a
=
(
retryPolicy
!=
null
)
?
retryPolicy
.
shouldRetry
(
ioe
,
retry
,
0
,
true
)
:
RetryPolicy
.
RetryAction
.
FAIL
;
boolean
isRetry
=
a
.
action
==
RetryPolicy
.
RetryAction
.
RetryDecision
.
RETRY
;
boolean
isFailoverAndRetry
=
a
.
action
==
RetryPolicy
.
RetryAction
.
RetryDecision
.
FAILOVER_AND_RETRY
;
if
(
isRetry
||
isFailoverAndRetry
)
{
LOG
.
debug
(
+
+
,
url
,
retry
,
retryPolicy
,
a
.
delayMillis
)
;
Thread
.
sleep
(
a
.
delayMillis
)
;
return
;
}
}
catch
(
InterruptedIOException
e
)
{
LOG
.
warn
(
e
.
getMessage
(
)
,
e
)
;
Thread
.
currentThread
(
)
.
interrupt
(
)
;
return
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
ioe
)
;
throw
new
WasbRemoteCallException
(
e
.
getMessage
(
)
,
e
)
;
@
Override
public
void
initialize
(
URI
uri
,
Configuration
configuration
)
throws
IOException
{
uri
=
ensureAuthority
(
uri
,
configuration
)
;
super
.
initialize
(
uri
,
configuration
)
;
setConf
(
configuration
)
;
abfsCounters
=
new
AbfsCountersImpl
(
uri
)
;
this
.
abfsStore
=
new
AzureBlobFileSystemStore
(
uri
,
this
.
isSecureScheme
(
)
,
configuration
,
abfsCounters
)
;
LOG
.
trace
(
)
;
final
AbfsConfiguration
abfsConfiguration
=
abfsStore
.
getAbfsConfiguration
(
)
;
this
.
setWorkingDirectory
(
this
.
getHomeDirectory
(
)
)
;
if
(
abfsConfiguration
.
getCreateRemoteFileSystemDuringInitialization
(
)
)
{
if
(
this
.
tryGetFileStatus
(
new
Path
(
AbfsHttpConstants
.
ROOT_PATH
)
)
==
null
)
{
try
{
this
.
createFileSystem
(
)
;
}
catch
(
AzureBlobFileSystemException
ex
)
{
checkException
(
null
,
ex
,
AzureServiceErrorCode
.
FILE_SYSTEM_ALREADY_EXISTS
)
;
}
}
}
LOG
.
trace
(
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
this
.
delegationTokenEnabled
=
abfsConfiguration
.
isDelegationTokenManagerEnabled
(
)
;
if
(
this
.
delegationTokenEnabled
)
{
final
AbfsConfiguration
abfsConfiguration
=
abfsStore
.
getAbfsConfiguration
(
)
;
this
.
setWorkingDirectory
(
this
.
getHomeDirectory
(
)
)
;
if
(
abfsConfiguration
.
getCreateRemoteFileSystemDuringInitialization
(
)
)
{
if
(
this
.
tryGetFileStatus
(
new
Path
(
AbfsHttpConstants
.
ROOT_PATH
)
)
==
null
)
{
try
{
this
.
createFileSystem
(
)
;
}
catch
(
AzureBlobFileSystemException
ex
)
{
checkException
(
null
,
ex
,
AzureServiceErrorCode
.
FILE_SYSTEM_ALREADY_EXISTS
)
;
}
}
}
LOG
.
trace
(
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
this
.
delegationTokenEnabled
=
abfsConfiguration
.
isDelegationTokenManagerEnabled
(
)
;
if
(
this
.
delegationTokenEnabled
)
{
LOG
.
debug
(
,
uri
)
;
this
.
delegationTokenManager
=
abfsConfiguration
.
getDelegationTokenManager
(
)
;
delegationTokenManager
.
bind
(
getUri
(
)
,
configuration
)
;
if
(
abfsConfiguration
.
getCreateRemoteFileSystemDuringInitialization
(
)
)
{
if
(
this
.
tryGetFileStatus
(
new
Path
(
AbfsHttpConstants
.
ROOT_PATH
)
)
==
null
)
{
try
{
this
.
createFileSystem
(
)
;
}
catch
(
AzureBlobFileSystemException
ex
)
{
checkException
(
null
,
ex
,
AzureServiceErrorCode
.
FILE_SYSTEM_ALREADY_EXISTS
)
;
}
}
}
LOG
.
trace
(
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
this
.
delegationTokenEnabled
=
abfsConfiguration
.
isDelegationTokenManagerEnabled
(
)
;
if
(
this
.
delegationTokenEnabled
)
{
LOG
.
debug
(
,
uri
)
;
this
.
delegationTokenManager
=
abfsConfiguration
.
getDelegationTokenManager
(
)
;
delegationTokenManager
.
bind
(
getUri
(
)
,
configuration
)
;
LOG
.
debug
(
,
delegationTokenManager
)
;
}
}
AbfsClientThrottlingIntercept
.
initializeSingleton
(
abfsConfiguration
.
isAutoThrottlingEnabled
(
)
)
;
@
Override
public
FSDataInputStream
open
(
final
Path
path
,
final
int
bufferSize
)
throws
IOException
{
@
Override
public
FSDataOutputStream
create
(
final
Path
f
,
final
FsPermission
permission
,
final
boolean
overwrite
,
final
int
bufferSize
,
final
short
replication
,
final
long
blockSize
,
final
Progressable
progress
)
throws
IOException
{
@
Override
public
FSDataOutputStream
append
(
final
Path
f
,
final
int
bufferSize
,
final
Progressable
progress
)
throws
IOException
{
public
boolean
rename
(
final
Path
src
,
final
Path
dst
)
throws
IOException
{
@
Override
public
boolean
delete
(
final
Path
f
,
final
boolean
recursive
)
throws
IOException
{
@
Override
public
FileStatus
[
]
listStatus
(
final
Path
f
)
throws
IOException
{
@
Override
public
boolean
mkdirs
(
final
Path
f
,
final
FsPermission
permission
)
throws
IOException
{
@
Override
public
FileStatus
getFileStatus
(
final
Path
f
)
throws
IOException
{
@
Override
public
void
setOwner
(
final
Path
path
,
final
String
owner
,
final
String
group
)
throws
IOException
{
@
Override
public
void
setXAttr
(
final
Path
path
,
final
String
name
,
final
byte
[
]
value
,
final
EnumSet
<
XAttrSetFlag
>
flag
)
throws
IOException
{
@
Override
public
byte
[
]
getXAttr
(
final
Path
path
,
final
String
name
)
throws
IOException
{
@
Override
public
void
setPermission
(
final
Path
path
,
final
FsPermission
permission
)
throws
IOException
{
@
Override
public
void
modifyAclEntries
(
final
Path
path
,
final
List
<
AclEntry
>
aclSpec
)
throws
IOException
{
@
Override
public
void
removeAclEntries
(
final
Path
path
,
final
List
<
AclEntry
>
aclSpec
)
throws
IOException
{
@
Override
public
void
removeDefaultAcl
(
final
Path
path
)
throws
IOException
{
@
Override
public
void
removeAcl
(
final
Path
path
)
throws
IOException
{
@
Override
public
void
setAcl
(
final
Path
path
,
final
List
<
AclEntry
>
aclSpec
)
throws
IOException
{
@
Override
public
AclStatus
getAclStatus
(
final
Path
path
)
throws
IOException
{
@
Override
public
void
access
(
final
Path
path
,
final
FsAction
mode
)
throws
IOException
{
public
Hashtable
<
String
,
String
>
getPathStatus
(
final
Path
path
)
throws
AzureBlobFileSystemException
{
try
(
AbfsPerfInfo
perfInfo
=
startTracking
(
,
)
)
{
public
void
setPathProperties
(
final
Path
path
,
final
Hashtable
<
String
,
String
>
properties
)
throws
AzureBlobFileSystemException
{
try
(
AbfsPerfInfo
perfInfo
=
startTracking
(
,
)
)
{
public
OutputStream
createFile
(
final
Path
path
,
final
FileSystem
.
Statistics
statistics
,
final
boolean
overwrite
,
final
FsPermission
permission
,
final
FsPermission
umask
)
throws
AzureBlobFileSystemException
{
try
(
AbfsPerfInfo
perfInfo
=
startTracking
(
,
)
)
{
boolean
isNamespaceEnabled
=
getIsNamespaceEnabled
(
)
;
public
void
createDirectory
(
final
Path
path
,
final
FsPermission
permission
,
final
FsPermission
umask
)
throws
AzureBlobFileSystemException
{
try
(
AbfsPerfInfo
perfInfo
=
startTracking
(
,
)
)
{
boolean
isNamespaceEnabled
=
getIsNamespaceEnabled
(
)
;
public
AbfsInputStream
openFileForRead
(
final
Path
path
,
final
FileSystem
.
Statistics
statistics
)
throws
AzureBlobFileSystemException
{
try
(
AbfsPerfInfo
perfInfo
=
startTracking
(
,
)
)
{
public
OutputStream
openFileForWrite
(
final
Path
path
,
final
FileSystem
.
Statistics
statistics
,
final
boolean
overwrite
)
throws
AzureBlobFileSystemException
{
try
(
AbfsPerfInfo
perfInfo
=
startTracking
(
,
)
)
{
public
void
delete
(
final
Path
path
,
final
boolean
recursive
)
throws
AzureBlobFileSystemException
{
final
Instant
startAggregate
=
abfsPerfTracker
.
getLatencyInstant
(
)
;
long
countAggregate
=
0
;
boolean
shouldContinue
=
true
;
public
FileStatus
getFileStatus
(
final
Path
path
)
throws
IOException
{
try
(
AbfsPerfInfo
perfInfo
=
startTracking
(
,
)
)
{
boolean
isNamespaceEnabled
=
getIsNamespaceEnabled
(
)
;
@
InterfaceStability
.
Unstable
public
FileStatus
[
]
listStatus
(
final
Path
path
,
final
String
startFrom
)
throws
IOException
{
final
Instant
startAggregate
=
abfsPerfTracker
.
getLatencyInstant
(
)
;
long
countAggregate
=
0
;
boolean
shouldContinue
=
true
;
public
void
access
(
final
Path
path
,
final
FsAction
mode
)
throws
AzureBlobFileSystemException
{
public
static
AzureADToken
getTokenFromMsi
(
final
String
authEndpoint
,
final
String
tenantGuid
,
final
String
clientId
,
String
authority
,
boolean
bypassCache
)
throws
IOException
{
QueryParams
qp
=
new
QueryParams
(
)
;
qp
.
add
(
,
)
;
qp
.
add
(
,
RESOURCE_NAME
)
;
if
(
tenantGuid
!=
null
&&
tenantGuid
.
length
(
)
>
0
)
{
authority
=
authority
+
tenantGuid
;
httperror
=
0
;
ex
=
null
;
try
{
token
=
getTokenSingleCall
(
authEndpoint
,
body
,
headers
,
httpMethod
,
isMsi
)
;
}
catch
(
HttpException
e
)
{
httperror
=
e
.
httpErrorCode
;
ex
=
e
;
}
catch
(
IOException
e
)
{
httperror
=
-
1
;
isRecoverableFailure
=
isRecoverableFailure
(
e
)
;
ex
=
new
HttpException
(
httperror
,
,
String
.
format
(
,
e
.
getClass
(
)
.
getTypeName
(
)
,
e
.
getMessage
(
)
)
,
authEndpoint
,
,
)
;
}
succeeded
=
(
(
httperror
==
0
)
&&
(
ex
==
null
)
)
;
shouldRetry
=
!
succeeded
&&
isRecoverableFailure
&&
tokenFetchRetryPolicy
.
shouldRetry
(
retryCount
,
httperror
)
;
retryCount
++
;
if
(
shouldRetry
)
{
jp
.
nextToken
(
)
;
while
(
jp
.
hasCurrentToken
(
)
)
{
if
(
jp
.
getCurrentToken
(
)
==
JsonToken
.
FIELD_NAME
)
{
fieldName
=
jp
.
getCurrentName
(
)
;
jp
.
nextToken
(
)
;
fieldValue
=
jp
.
getText
(
)
;
if
(
fieldName
.
equals
(
)
)
{
token
.
setAccessToken
(
fieldValue
)
;
}
if
(
fieldName
.
equals
(
)
)
{
expiryPeriodInSecs
=
Integer
.
parseInt
(
fieldValue
)
;
}
if
(
fieldName
.
equals
(
)
)
{
expiresOnInSecs
=
Long
.
parseLong
(
fieldValue
)
;
}
}
jp
.
nextToken
(
)
;
}
jp
.
close
(
)
;
if
(
expiresOnInSecs
>
0
)
{
expiryPeriodInSecs
=
Integer
.
parseInt
(
fieldValue
)
;
}
if
(
fieldName
.
equals
(
)
)
{
expiresOnInSecs
=
Long
.
parseLong
(
fieldValue
)
;
}
}
jp
.
nextToken
(
)
;
}
jp
.
close
(
)
;
if
(
expiresOnInSecs
>
0
)
{
LOG
.
debug
(
,
expiresOnInSecs
)
;
token
.
setExpiry
(
new
Date
(
expiresOnInSecs
*
1000
)
)
;
}
else
{
if
(
isMsi
)
{
throw
new
UnsupportedOperationException
(
)
;
}
LOG
.
debug
(
,
expiryPeriodInSecs
)
;
long
expiry
=
System
.
currentTimeMillis
(
)
;
expiry
=
expiry
+
expiryPeriodInSecs
*
1000L
;
token
.
setExpiry
(
new
Date
(
expiry
)
)
;
public
Token
<
DelegationTokenIdentifier
>
getDelegationToken
(
String
renewer
)
throws
IOException
{
double
reductionFactor
=
(
consecutiveNoErrorCount
*
analysisPeriodMs
>=
RAPID_SLEEP_DECREASE_TRANSITION_PERIOD_MS
)
?
RAPID_SLEEP_DECREASE_FACTOR
:
SLEEP_DECREASE_FACTOR
;
newSleepDuration
=
sleepDuration
*
reductionFactor
;
}
else
if
(
errorPercentage
<
MAX_EQUILIBRIUM_ERROR_PERCENTAGE
)
{
newSleepDuration
=
sleepDuration
;
}
else
{
consecutiveNoErrorCount
=
0
;
double
additionalDelayNeeded
=
5
*
analysisPeriodMs
;
if
(
bytesSuccessful
>
0
)
{
additionalDelayNeeded
=
(
bytesSuccessful
+
bytesFailed
)
*
periodMs
/
bytesSuccessful
-
periodMs
;
}
newSleepDuration
=
additionalDelayNeeded
/
(
operationsFailed
+
operationsSuccessful
)
;
final
double
maxSleepDuration
=
analysisPeriodMs
;
final
double
minSleepDuration
=
sleepDuration
*
SLEEP_INCREASE_FACTOR
;
newSleepDuration
=
Math
.
max
(
newSleepDuration
,
minSleepDuration
)
+
1
;
newSleepDuration
=
Math
.
min
(
newSleepDuration
,
maxSleepDuration
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
synchronized
int
read
(
final
byte
[
]
b
,
final
int
off
,
final
int
len
)
throws
IOException
{
if
(
b
!=
null
)
{
if
(
b
==
null
)
{
throw
new
IllegalArgumentException
(
)
;
}
if
(
offset
>=
b
.
length
)
{
throw
new
IllegalArgumentException
(
)
;
}
if
(
length
<
0
)
{
throw
new
IllegalArgumentException
(
)
;
}
if
(
length
>
(
b
.
length
-
offset
)
)
{
throw
new
IllegalArgumentException
(
)
;
}
final
AbfsRestOperation
op
;
AbfsPerfTracker
tracker
=
client
.
getAbfsPerfTracker
(
)
;
try
(
AbfsPerfInfo
perfInfo
=
new
AbfsPerfInfo
(
tracker
,
,
)
)
{
LOG
.
trace
(
,
path
,
position
,
offset
,
length
)
;
op
=
client
.
read
(
path
,
position
,
b
,
offset
,
length
,
tolerateOobAppends
?
:
eTag
,
cachedSasToken
.
get
(
)
)
;
cachedSasToken
.
update
(
op
.
getSasToken
(
)
)
;
if
(
streamStatistics
!=
null
)
{
@
Override
public
synchronized
void
seek
(
long
n
)
throws
IOException
{
LOG
.
debug
(
,
n
)
;
if
(
closed
)
{
throw
new
IOException
(
FSExceptionMessages
.
STREAM_IS_CLOSED
)
;
}
if
(
n
<
0
)
{
throw
new
EOFException
(
FSExceptionMessages
.
NEGATIVE_SEEK
)
;
}
if
(
n
>
contentLength
)
{
throw
new
EOFException
(
FSExceptionMessages
.
CANNOT_SEEK_PAST_EOF
)
;
}
if
(
streamStatistics
!=
null
)
{
streamStatistics
.
seek
(
n
,
fCursor
)
;
}
if
(
n
>=
fCursor
-
limit
&&
n
<=
fCursor
)
{
bCursor
=
(
int
)
(
n
-
(
fCursor
-
limit
)
)
;
if
(
streamStatistics
!=
null
)
{
streamStatistics
.
seekInBuffer
(
)
;
}
return
;
}
fCursor
=
n
;
public
static
void
dumpHeadersToDebugLog
(
final
String
origin
,
final
Map
<
String
,
List
<
String
>>
headers
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
offerToQueue
(
Instant
trackerStart
,
String
latencyDetails
)
{
queue
.
offer
(
latencyDetails
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
Instant
trackerStop
=
Instant
.
now
(
)
;
long
elapsed
=
Duration
.
between
(
trackerStart
,
trackerStop
)
.
toMillis
(
)
;
break
;
}
AbfsIoUtils
.
dumpHeadersToDebugLog
(
,
httpOperation
.
getConnection
(
)
.
getRequestProperties
(
)
)
;
AbfsClientThrottlingIntercept
.
sendingRequest
(
operationType
,
abfsCounters
)
;
if
(
hasRequestBody
)
{
httpOperation
.
sendRequest
(
buffer
,
bufferOffset
,
bufferLength
)
;
incrementCounter
(
AbfsStatistic
.
SEND_REQUESTS
,
1
)
;
incrementCounter
(
AbfsStatistic
.
BYTES_SENT
,
bufferLength
)
;
}
httpOperation
.
processResponse
(
buffer
,
bufferOffset
,
bufferLength
)
;
incrementCounter
(
AbfsStatistic
.
GET_RESPONSES
,
1
)
;
incrementCounter
(
AbfsStatistic
.
BYTES_RECEIVED
,
httpOperation
.
getBytesReceived
(
)
)
;
}
catch
(
IOException
ex
)
{
if
(
ex
instanceof
UnknownHostException
)
{
LOG
.
warn
(
String
.
format
(
,
httpOperation
.
getUrl
(
)
.
getHost
(
)
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
httpOperation
!=
null
)
{
AbfsIoUtils
.
dumpHeadersToDebugLog
(
,
httpOperation
.
getConnection
(
)
.
getRequestProperties
(
)
)
;
AbfsClientThrottlingIntercept
.
sendingRequest
(
operationType
,
abfsCounters
)
;
if
(
hasRequestBody
)
{
httpOperation
.
sendRequest
(
buffer
,
bufferOffset
,
bufferLength
)
;
incrementCounter
(
AbfsStatistic
.
SEND_REQUESTS
,
1
)
;
incrementCounter
(
AbfsStatistic
.
BYTES_SENT
,
bufferLength
)
;
}
httpOperation
.
processResponse
(
buffer
,
bufferOffset
,
bufferLength
)
;
incrementCounter
(
AbfsStatistic
.
GET_RESPONSES
,
1
)
;
incrementCounter
(
AbfsStatistic
.
BYTES_RECEIVED
,
httpOperation
.
getBytesReceived
(
)
)
;
}
catch
(
IOException
ex
)
{
if
(
ex
instanceof
UnknownHostException
)
{
LOG
.
warn
(
String
.
format
(
,
httpOperation
.
getUrl
(
)
.
getHost
(
)
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
httpOperation
!=
null
)
{
LOG
.
debug
(
+
httpOperation
.
toString
(
)
,
ex
)
;
incrementCounter
(
AbfsStatistic
.
BYTES_RECEIVED
,
httpOperation
.
getBytesReceived
(
)
)
;
}
catch
(
IOException
ex
)
{
if
(
ex
instanceof
UnknownHostException
)
{
LOG
.
warn
(
String
.
format
(
,
httpOperation
.
getUrl
(
)
.
getHost
(
)
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
httpOperation
!=
null
)
{
LOG
.
debug
(
+
httpOperation
.
toString
(
)
,
ex
)
;
}
else
{
LOG
.
debug
(
+
method
+
+
url
,
ex
)
;
}
}
if
(
!
client
.
getRetryPolicy
(
)
.
shouldRetry
(
retryCount
,
-
1
)
)
{
throw
new
InvalidAbfsRestOperationException
(
ex
)
;
}
if
(
ex
instanceof
HttpException
)
{
throw
new
AbfsRestOperationException
(
(
HttpException
)
ex
)
;
}
return
false
;
}
finally
{
public
void
signRequest
(
HttpURLConnection
connection
,
final
long
contentLength
)
throws
UnsupportedEncodingException
{
String
gmtTime
=
getGMTTime
(
)
;
connection
.
setRequestProperty
(
HttpHeaderConfigurations
.
X_MS_DATE
,
gmtTime
)
;
final
String
stringToSign
=
canonicalize
(
connection
,
accountName
,
contentLength
)
;
final
String
computedBase64Signature
=
computeHmac256
(
stringToSign
)
;
String
signature
=
String
.
format
(
,
,
accountName
,
computedBase64Signature
)
;
connection
.
setRequestProperty
(
HttpHeaderConfigurations
.
AUTHORIZATION
,
signature
)
;
int
signedExpiryLen
=
3
;
int
start
=
token
.
indexOf
(
signedExpiry
)
;
if
(
start
==
-
1
)
{
return
OffsetDateTime
.
MIN
;
}
start
+=
signedExpiryLen
;
int
end
=
token
.
indexOf
(
,
start
)
;
String
seValue
=
(
end
==
-
1
)
?
token
.
substring
(
start
)
:
token
.
substring
(
start
,
end
)
;
try
{
seValue
=
URLDecoder
.
decode
(
seValue
,
)
;
}
catch
(
UnsupportedEncodingException
ex
)
{
LOG
.
error
(
,
seValue
,
ex
)
;
return
OffsetDateTime
.
MIN
;
}
OffsetDateTime
seDate
=
OffsetDateTime
.
MIN
;
try
{
seDate
=
OffsetDateTime
.
parse
(
seValue
,
DateTimeFormatter
.
ISO_DATE_TIME
)
;
LOG
.
error
(
,
seValue
,
ex
)
;
return
OffsetDateTime
.
MIN
;
}
OffsetDateTime
seDate
=
OffsetDateTime
.
MIN
;
try
{
seDate
=
OffsetDateTime
.
parse
(
seValue
,
DateTimeFormatter
.
ISO_DATE_TIME
)
;
}
catch
(
DateTimeParseException
ex
)
{
LOG
.
error
(
,
seValue
,
ex
)
;
}
String
signedKeyExpiry
=
;
int
signedKeyExpiryLen
=
4
;
start
=
token
.
indexOf
(
signedKeyExpiry
)
;
if
(
start
==
-
1
)
{
return
seDate
;
}
start
+=
signedKeyExpiryLen
;
end
=
token
.
indexOf
(
,
start
)
;
String
skeValue
=
(
end
==
-
1
)
?
token
.
substring
(
start
)
:
token
.
substring
(
start
,
end
)
;
}
catch
(
DateTimeParseException
ex
)
{
LOG
.
error
(
,
seValue
,
ex
)
;
}
String
signedKeyExpiry
=
;
int
signedKeyExpiryLen
=
4
;
start
=
token
.
indexOf
(
signedKeyExpiry
)
;
if
(
start
==
-
1
)
{
return
seDate
;
}
start
+=
signedKeyExpiryLen
;
end
=
token
.
indexOf
(
,
start
)
;
String
skeValue
=
(
end
==
-
1
)
?
token
.
substring
(
start
)
:
token
.
substring
(
start
,
end
)
;
try
{
skeValue
=
URLDecoder
.
decode
(
skeValue
,
)
;
}
catch
(
UnsupportedEncodingException
ex
)
{
LOG
.
error
(
,
skeValue
,
ex
)
;
return
OffsetDateTime
.
MIN
;
private
static
void
loadMap
(
HashMap
<
String
,
String
>
cache
,
String
fileLocation
,
int
noOfFields
,
int
keyIndex
)
throws
IOException
{
LOG
.
debug
(
,
fileLocation
)
;
int
errorRecord
=
0
;
File
file
=
new
File
(
fileLocation
)
;
LineIterator
it
=
null
;
try
{
it
=
FileUtils
.
lineIterator
(
file
,
)
;
while
(
it
.
hasNext
(
)
)
{
String
line
=
it
.
nextLine
(
)
;
if
(
!
Strings
.
isNullOrEmpty
(
line
.
trim
(
)
)
&&
!
line
.
startsWith
(
HASH
)
)
{
if
(
line
.
split
(
COLON
)
.
length
!=
noOfFields
)
{
errorRecord
+=
1
;
continue
;
}
cache
.
put
(
line
.
split
(
COLON
)
[
keyIndex
]
,
line
)
;
}
}
LOG
.
debug
(
,
fileLocation
,
cache
.
size
(
)
,
errorRecord
)
;
}
catch
(
ArrayIndexOutOfBoundsException
e
)
{
protected
void
describe
(
String
text
,
Object
...
args
)
{
private
void
createTestFileAndSetLength
(
)
throws
IOException
{
FileSystem
fs
=
accountUsingInputStreamV1
.
getFileSystem
(
)
;
if
(
fs
.
exists
(
TEST_FILE_PATH
)
)
{
testFileStatus
=
fs
.
getFileStatus
(
TEST_FILE_PATH
)
;
testFileLength
=
testFileStatus
.
getLen
(
)
;
@
Test
public
void
test_0315_SequentialReadPerformance
(
)
throws
IOException
{
assumeHugeFileExists
(
)
;
final
int
maxAttempts
=
10
;
final
double
maxAcceptableRatio
=
1.01
;
double
v1ElapsedMs
=
0
,
v2ElapsedMs
=
0
;
double
ratio
=
Double
.
MAX_VALUE
;
for
(
int
i
=
0
;
i
<
maxAttempts
&&
ratio
>=
maxAcceptableRatio
;
i
++
)
{
v1ElapsedMs
=
sequentialRead
(
1
,
accountUsingInputStreamV1
.
getFileSystem
(
)
,
false
)
;
v2ElapsedMs
=
sequentialRead
(
2
,
accountUsingInputStreamV2
.
getFileSystem
(
)
,
false
)
;
ratio
=
v2ElapsedMs
/
v1ElapsedMs
;
@
Test
public
void
test_0316_SequentialReadAfterReverseSeekPerformanceV2
(
)
throws
IOException
{
assumeHugeFileExists
(
)
;
final
int
maxAttempts
=
10
;
final
double
maxAcceptableRatio
=
1.01
;
double
beforeSeekElapsedMs
=
0
,
afterSeekElapsedMs
=
0
;
double
ratio
=
Double
.
MAX_VALUE
;
for
(
int
i
=
0
;
i
<
maxAttempts
&&
ratio
>=
maxAcceptableRatio
;
i
++
)
{
beforeSeekElapsedMs
=
sequentialRead
(
2
,
accountUsingInputStreamV2
.
getFileSystem
(
)
,
false
)
;
afterSeekElapsedMs
=
sequentialRead
(
2
,
accountUsingInputStreamV2
.
getFileSystem
(
)
,
true
)
;
ratio
=
afterSeekElapsedMs
/
beforeSeekElapsedMs
;
private
long
sequentialRead
(
int
version
,
FileSystem
fs
,
boolean
afterReverseSeek
)
throws
IOException
{
byte
[
]
buffer
=
new
byte
[
16
*
KILOBYTE
]
;
long
totalBytesRead
=
0
;
long
bytesRead
=
0
;
try
(
FSDataInputStream
inputStream
=
fs
.
open
(
TEST_FILE_PATH
)
)
{
if
(
afterReverseSeek
)
{
while
(
bytesRead
>
0
&&
totalBytesRead
<
4
*
MEGABYTE
)
{
bytesRead
=
inputStream
.
read
(
buffer
)
;
totalBytesRead
+=
bytesRead
;
}
totalBytesRead
=
0
;
inputStream
.
seek
(
0
)
;
}
NanoTimer
timer
=
new
NanoTimer
(
)
;
while
(
(
bytesRead
=
inputStream
.
read
(
buffer
)
)
>
0
)
{
totalBytesRead
+=
bytesRead
;
}
long
elapsedTimeMs
=
timer
.
elapsedTimeMs
(
)
;
@
Test
public
void
test_0317_RandomReadPerformance
(
)
throws
IOException
{
assumeHugeFileExists
(
)
;
final
int
maxAttempts
=
10
;
final
double
maxAcceptableRatio
=
0.10
;
double
v1ElapsedMs
=
0
,
v2ElapsedMs
=
0
;
double
ratio
=
Double
.
MAX_VALUE
;
for
(
int
i
=
0
;
i
<
maxAttempts
&&
ratio
>=
maxAcceptableRatio
;
i
++
)
{
v1ElapsedMs
=
randomRead
(
1
,
accountUsingInputStreamV1
.
getFileSystem
(
)
)
;
v2ElapsedMs
=
randomRead
(
2
,
accountUsingInputStreamV2
.
getFileSystem
(
)
)
;
ratio
=
v2ElapsedMs
/
v1ElapsedMs
;
assumeHugeFileExists
(
)
;
final
int
minBytesToRead
=
2
*
MEGABYTE
;
Random
random
=
new
Random
(
)
;
byte
[
]
buffer
=
new
byte
[
8
*
KILOBYTE
]
;
long
totalBytesRead
=
0
;
long
bytesRead
=
0
;
try
(
FSDataInputStream
inputStream
=
fs
.
open
(
TEST_FILE_PATH
)
)
{
NanoTimer
timer
=
new
NanoTimer
(
)
;
do
{
bytesRead
=
inputStream
.
read
(
buffer
)
;
totalBytesRead
+=
bytesRead
;
inputStream
.
seek
(
random
.
nextInt
(
(
int
)
(
testFileLength
-
buffer
.
length
)
)
)
;
}
while
(
bytesRead
>
0
&&
totalBytesRead
<
minBytesToRead
)
;
long
elapsedTimeMs
=
timer
.
elapsedTimeMs
(
)
;
inputStream
.
close
(
)
;
protected
void
assertInLog
(
String
content
,
String
term
)
{
assertTrue
(
,
!
content
.
isEmpty
(
)
)
;
if
(
!
content
.
contains
(
term
)
)
{
String
message
=
+
term
+
;
protected
void
assertNotInLog
(
String
content
,
String
term
)
{
assertTrue
(
,
!
content
.
isEmpty
(
)
)
;
if
(
content
.
contains
(
term
)
)
{
String
message
=
term
+
;
@
Override
public
void
setUp
(
)
throws
Exception
{
super
.
setUp
(
)
;
Configuration
conf
=
getConfiguration
(
)
;
threads
=
AzureTestUtils
.
getTestPropertyInt
(
conf
,
,
NUMBER_OF_THREADS
)
;
filesPerThread
=
AzureTestUtils
.
getTestPropertyInt
(
conf
,
,
NUMBER_OF_FILES_PER_THREAD
)
;
expectedFileCount
=
threads
*
filesPerThread
;
final
String
basePath
=
(
fs
.
getWorkingDirectory
(
)
.
toUri
(
)
.
getPath
(
)
+
+
TEST_DIR_PATH
+
)
.
substring
(
1
)
;
ArrayList
<
Callable
<
Integer
>>
tasks
=
new
ArrayList
<
>
(
threads
)
;
fs
.
mkdirs
(
TEST_DIR_PATH
)
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
for
(
int
i
=
0
;
i
<
threads
;
i
++
)
{
tasks
.
add
(
new
Callable
<
Integer
>
(
)
{
public
Integer
call
(
)
{
int
written
=
0
;
for
(
int
j
=
0
;
j
<
filesPerThread
;
j
++
)
{
String
blobName
=
basePath
+
UUID
.
randomUUID
(
)
.
toString
(
)
;
try
{
CloudBlockBlob
blob
=
container
.
getBlockBlobReference
(
blobName
)
;
blob
.
uploadText
(
)
;
written
++
;
}
catch
(
Exception
e
)
{
int
written
=
0
;
for
(
int
j
=
0
;
j
<
filesPerThread
;
j
++
)
{
String
blobName
=
basePath
+
UUID
.
randomUUID
(
)
.
toString
(
)
;
try
{
CloudBlockBlob
blob
=
container
.
getBlockBlobReference
(
blobName
)
;
blob
.
uploadText
(
)
;
written
++
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
blobName
,
e
)
;
break
;
}
}
LOG
.
info
(
,
written
)
;
return
written
;
}
}
)
;
}
List
<
Future
<
Integer
>>
futures
=
executorService
.
invokeAll
(
tasks
,
getTestTimeoutMillis
(
)
,
TimeUnit
.
MILLISECONDS
)
;
long
elapsedMs
=
timer
.
elapsedTimeMs
(
)
;
@
Test
public
void
test_0200_ListStatusPerformance
(
)
throws
Exception
{
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
FileStatus
[
]
fileList
=
fs
.
listStatus
(
TEST_DIR_PATH
)
;
long
elapsedMs
=
timer
.
elapsedTimeMs
(
)
;
@
Test
public
void
test_0200_ListStatusPerformance
(
)
throws
Exception
{
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
FileStatus
[
]
fileList
=
fs
.
listStatus
(
TEST_DIR_PATH
)
;
long
elapsedMs
=
timer
.
elapsedTimeMs
(
)
;
LOG
.
info
(
String
.
format
(
,
fileList
.
length
,
elapsedMs
)
)
;
Map
<
Path
,
FileStatus
>
foundInList
=
new
HashMap
<
>
(
expectedFileCount
)
;
for
(
FileStatus
fileStatus
:
fileList
)
{
foundInList
.
put
(
fileStatus
.
getPath
(
)
,
fileStatus
)
;
Map
<
Path
,
FileStatus
>
foundInList
=
new
HashMap
<
>
(
expectedFileCount
)
;
for
(
FileStatus
fileStatus
:
fileList
)
{
foundInList
.
put
(
fileStatus
.
getPath
(
)
,
fileStatus
)
;
LOG
.
info
(
,
fileStatus
.
getPath
(
)
,
fileStatus
.
isDirectory
(
)
?
:
)
;
}
assertEquals
(
,
expectedFileCount
,
fileList
.
length
)
;
ContractTestUtils
.
NanoTimer
initialStatusCallTimer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
RemoteIterator
<
LocatedFileStatus
>
listing
=
fs
.
listFiles
(
TEST_DIR_PATH
,
true
)
;
long
initialListTime
=
initialStatusCallTimer
.
elapsedTimeMs
(
)
;
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
while
(
listing
.
hasNext
(
)
)
{
FileStatus
fileStatus
=
listing
.
next
(
)
;
Path
path
=
fileStatus
.
getPath
(
)
;
FileStatus
removed
=
foundInList
.
remove
(
path
)
;
assertNotNull
(
+
path
+
,
removed
)
;
}
elapsedMs
=
timer
.
elapsedTimeMs
(
)
;
@
Test
public
void
test_0300_BulkDeletePerformance
(
)
throws
Exception
{
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
fs
.
delete
(
TEST_DIR_PATH
,
true
)
;
long
elapsedMs
=
timer
.
elapsedTimeMs
(
)
;
final
AtomicInteger
successfulRenameCount
=
new
AtomicInteger
(
0
)
;
final
AtomicReference
<
IOException
>
unexpectedError
=
new
AtomicReference
<
IOException
>
(
)
;
final
Path
dest
=
path
(
)
;
List
<
Thread
>
threads
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
10
;
i
++
)
{
final
int
threadNumber
=
i
;
Path
src
=
path
(
+
threadNumber
)
;
threads
.
add
(
new
Thread
(
(
)
->
{
try
{
latch
.
await
(
Long
.
MAX_VALUE
,
TimeUnit
.
SECONDS
)
;
}
catch
(
InterruptedException
e
)
{
}
try
{
try
(
OutputStream
output
=
fs
.
create
(
src
)
)
{
output
.
write
(
(
+
threadNumber
)
.
getBytes
(
)
)
;
}
if
(
fs
.
rename
(
src
,
dest
)
)
{
@
Test
public
void
testDeleteThrowsExceptionWithLeaseExistsErrorMessage
(
)
throws
Exception
{
LOG
.
info
(
)
;
Path
path
=
methodPath
(
)
;
fs
.
create
(
path
)
;
assertPathExists
(
,
path
)
;
NativeAzureFileSystem
nfs
=
fs
;
final
String
fullKey
=
nfs
.
pathToKey
(
nfs
.
makeAbsolute
(
path
)
)
;
final
AzureNativeFileSystemStore
store
=
nfs
.
getStore
(
)
;
final
CountDownLatch
leaseAttemptComplete
=
new
CountDownLatch
(
1
)
;
final
CountDownLatch
beginningDeleteAttempt
=
new
CountDownLatch
(
1
)
;
Thread
t
=
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
SelfRenewingLease
lease
=
null
;
try
{
lease
=
store
.
acquireLease
(
fullKey
)
;
private
void
createTestFileAndSetLength
(
)
throws
IOException
{
if
(
fs
.
exists
(
TEST_FILE_PATH
)
)
{
testFileStatus
=
fs
.
getFileStatus
(
TEST_FILE_PATH
)
;
testFileLength
=
testFileStatus
.
getLen
(
)
;
@
Test
public
void
testEnumContainers
(
)
throws
Throwable
{
describe
(
)
;
int
count
=
0
;
CloudStorageAccount
storageAccount
=
getTestAccount
(
)
.
getRealAccount
(
)
;
CloudBlobClient
blobClient
=
storageAccount
.
createCloudBlobClient
(
)
;
Iterable
<
CloudBlobContainer
>
containers
=
blobClient
.
listContainers
(
CONTAINER_PREFIX
)
;
for
(
CloudBlobContainer
container
:
containers
)
{
count
++
;
@
Test
public
void
testDeleteContainers
(
)
throws
Throwable
{
describe
(
)
;
int
count
=
0
;
CloudStorageAccount
storageAccount
=
getTestAccount
(
)
.
getRealAccount
(
)
;
CloudBlobClient
blobClient
=
storageAccount
.
createCloudBlobClient
(
)
;
Iterable
<
CloudBlobContainer
>
containers
=
blobClient
.
listContainers
(
CONTAINER_PREFIX
)
;
for
(
CloudBlobContainer
container
:
containers
)
{
protected
void
logTimePerIOP
(
String
operation
,
ContractTestUtils
.
NanoTimer
timer
,
long
count
)
{
private
void
logFSState
(
)
{
StorageStatistics
statistics
=
getFileSystem
(
)
.
getStorageStatistics
(
)
;
Iterator
<
StorageStatistics
.
LongStatistic
>
longStatistics
=
statistics
.
getLongStatistics
(
)
;
while
(
longStatistics
.
hasNext
(
)
)
{
StorageStatistics
.
LongStatistic
next
=
longStatistics
.
next
(
)
;
byte
[
]
data
=
SOURCE_DATA
;
long
blocks
=
filesize
/
UPLOAD_BLOCKSIZE
;
long
blocksPerMB
=
S_1M
/
UPLOAD_BLOCKSIZE
;
NativeAzureFileSystem
fs
=
getFileSystem
(
)
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
long
blocksPer10MB
=
blocksPerMB
*
10
;
fs
.
mkdirs
(
hugefile
.
getParent
(
)
)
;
try
(
FSDataOutputStream
out
=
fs
.
create
(
hugefile
,
true
,
UPLOAD_BLOCKSIZE
,
null
)
)
{
for
(
long
block
=
1
;
block
<=
blocks
;
block
++
)
{
out
.
write
(
data
)
;
long
written
=
block
*
UPLOAD_BLOCKSIZE
;
if
(
block
%
blocksPer10MB
==
0
||
written
==
filesize
)
{
long
percentage
=
written
*
100
/
filesize
;
double
elapsedTime
=
timer
.
elapsedTime
(
)
/
NANOSEC
;
double
writtenMB
=
1.0
*
written
/
S_1M
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
ContractTestUtils
.
NanoTimer
readAtByte0
,
readAtByte0Again
,
readAtEOF
;
try
(
FSDataInputStream
in
=
openDataFile
(
)
)
{
readAtByte0
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0
.
end
(
)
;
ops
++
;
readAtEOF
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
eof
-
bufferSize
,
buffer
)
;
readAtEOF
.
end
(
)
;
ops
++
;
readAtByte0Again
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0Again
.
end
(
)
;
ops
++
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0
.
end
(
)
;
ops
++
;
readAtEOF
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
eof
-
bufferSize
,
buffer
)
;
readAtEOF
.
end
(
)
;
ops
++
;
readAtByte0Again
=
new
ContractTestUtils
.
NanoTimer
(
)
;
in
.
readFully
(
0
,
buffer
)
;
readAtByte0Again
.
end
(
)
;
ops
++
;
LOG
.
info
(
,
in
)
;
}
long
mb
=
Math
.
max
(
filesize
/
S_1M
,
1
)
;
logFSState
(
)
;
timer
.
end
(
,
mb
)
;
@
Test
public
void
test_050_readHugeFile
(
)
throws
Throwable
{
assumeHugeFileExists
(
)
;
describe
(
,
hugefile
)
;
NativeAzureFileSystem
fs
=
getFileSystem
(
)
;
FileStatus
status
=
fs
.
getFileStatus
(
hugefile
)
;
long
filesize
=
status
.
getLen
(
)
;
long
blocks
=
filesize
/
UPLOAD_BLOCKSIZE
;
byte
[
]
data
=
new
byte
[
UPLOAD_BLOCKSIZE
]
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
try
(
FSDataInputStream
in
=
openDataFile
(
)
)
{
for
(
long
block
=
0
;
block
<
blocks
;
block
++
)
{
in
.
readFully
(
data
)
;
}
LOG
.
info
(
,
in
)
;
}
long
mb
=
Math
.
max
(
filesize
/
S_1M
,
1
)
;
timer
.
end
(
,
mb
)
;
int
remaining
=
blockSize
;
long
blockId
=
i
+
1
;
NanoTimer
blockTimer
=
new
NanoTimer
(
)
;
int
reads
=
0
;
while
(
remaining
>
0
)
{
NanoTimer
readTimer
=
new
NanoTimer
(
)
;
int
bytesRead
=
in
.
read
(
block
,
offset
,
remaining
)
;
reads
++
;
if
(
bytesRead
==
1
)
{
break
;
}
remaining
-=
bytesRead
;
offset
+=
bytesRead
;
count
+=
bytesRead
;
readTimer
.
end
(
)
;
if
(
bytesRead
!=
0
)
{
NanoTimer
readTimer
=
new
NanoTimer
(
)
;
int
bytesRead
=
in
.
read
(
block
,
offset
,
remaining
)
;
reads
++
;
if
(
bytesRead
==
1
)
{
break
;
}
remaining
-=
bytesRead
;
offset
+=
bytesRead
;
count
+=
bytesRead
;
readTimer
.
end
(
)
;
if
(
bytesRead
!=
0
)
{
LOG
.
debug
(
+
+
,
reads
,
bytesRead
,
blockSize
-
remaining
,
remaining
,
readTimer
.
duration
(
)
,
readTimer
.
nanosPerOperation
(
bytesRead
)
,
readTimer
.
bandwidthDescription
(
bytesRead
)
)
;
}
else
{
LOG
.
warn
(
,
reads
)
;
}
}
blockTimer
.
end
(
,
blockId
,
reads
)
;
String
bw
=
blockTimer
.
bandwidthDescription
(
blockSize
)
;
count
+=
bytesRead
;
readTimer
.
end
(
)
;
if
(
bytesRead
!=
0
)
{
LOG
.
debug
(
+
+
,
reads
,
bytesRead
,
blockSize
-
remaining
,
remaining
,
readTimer
.
duration
(
)
,
readTimer
.
nanosPerOperation
(
bytesRead
)
,
readTimer
.
bandwidthDescription
(
bytesRead
)
)
;
}
else
{
LOG
.
warn
(
,
reads
)
;
}
}
blockTimer
.
end
(
,
blockId
,
reads
)
;
String
bw
=
blockTimer
.
bandwidthDescription
(
blockSize
)
;
LOG
.
info
(
,
blockId
,
bw
)
;
if
(
bandwidthInBytes
(
blockTimer
,
blockSize
)
<
minimumBandwidth
)
{
LOG
.
warn
(
,
bw
,
blockId
)
;
Assert
.
assertTrue
(
+
bw
+
+
resetCount
+
,
resetCount
<=
maxResetCount
)
;
resetCount
++
;
}
}
}
finally
{
IOUtils
.
closeStream
(
in
)
;
@
Test
public
void
test_100_renameHugeFile
(
)
throws
Throwable
{
assumeHugeFileExists
(
)
;
describe
(
,
hugefile
,
hugefileRenamed
)
;
NativeAzureFileSystem
fs
=
getFileSystem
(
)
;
FileStatus
status
=
fs
.
getFileStatus
(
hugefile
)
;
long
filesize
=
status
.
getLen
(
)
;
fs
.
delete
(
hugefileRenamed
,
false
)
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
fs
.
rename
(
hugefile
,
hugefileRenamed
)
;
long
mb
=
Math
.
max
(
filesize
/
S_1M
,
1
)
;
timer
.
end
(
,
mb
)
;
FileStatus
status
=
fs
.
getFileStatus
(
hugefile
)
;
long
filesize
=
status
.
getLen
(
)
;
fs
.
delete
(
hugefileRenamed
,
false
)
;
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
fs
.
rename
(
hugefile
,
hugefileRenamed
)
;
long
mb
=
Math
.
max
(
filesize
/
S_1M
,
1
)
;
timer
.
end
(
,
mb
)
;
LOG
.
info
(
,
toHuman
(
timer
.
nanosPerOperation
(
mb
)
)
)
;
bandwidth
(
timer
,
filesize
)
;
logFSState
(
)
;
FileStatus
destFileStatus
=
fs
.
getFileStatus
(
hugefileRenamed
)
;
assertEquals
(
filesize
,
destFileStatus
.
getLen
(
)
)
;
ContractTestUtils
.
NanoTimer
timer2
=
new
ContractTestUtils
.
NanoTimer
(
)
;
fs
.
rename
(
hugefileRenamed
,
hugefile
)
;
timer2
.
end
(
)
;
final
int
FILE_SIZE
=
1000
;
getBandwidthGaugeUpdater
(
)
.
suppressAutoUpdate
(
)
;
Date
start
=
new
Date
(
)
;
OutputStream
outputStream
=
getFileSystem
(
)
.
create
(
filePath
)
;
outputStream
.
write
(
nonZeroByteArray
(
FILE_SIZE
)
)
;
outputStream
.
close
(
)
;
long
uploadDurationMs
=
new
Date
(
)
.
getTime
(
)
-
start
.
getTime
(
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
2
,
15
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
true
)
;
long
bytesWritten
=
AzureMetricsTestUtil
.
getCurrentBytesWritten
(
getInstrumentation
(
)
)
;
assertTrue
(
+
bytesWritten
+
+
FILE_SIZE
+
,
bytesWritten
>
(
FILE_SIZE
/
2
)
&&
bytesWritten
<
(
FILE_SIZE
*
2
)
)
;
long
totalBytesWritten
=
AzureMetricsTestUtil
.
getCurrentTotalBytesWritten
(
getInstrumentation
(
)
)
;
assertTrue
(
+
totalBytesWritten
+
+
FILE_SIZE
+
,
totalBytesWritten
>=
FILE_SIZE
&&
totalBytesWritten
<
(
FILE_SIZE
*
2
)
)
;
long
uploadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_UPLOAD_RATE
)
;
outputStream
.
write
(
nonZeroByteArray
(
FILE_SIZE
)
)
;
outputStream
.
close
(
)
;
long
uploadDurationMs
=
new
Date
(
)
.
getTime
(
)
-
start
.
getTime
(
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
2
,
15
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
true
)
;
long
bytesWritten
=
AzureMetricsTestUtil
.
getCurrentBytesWritten
(
getInstrumentation
(
)
)
;
assertTrue
(
+
bytesWritten
+
+
FILE_SIZE
+
,
bytesWritten
>
(
FILE_SIZE
/
2
)
&&
bytesWritten
<
(
FILE_SIZE
*
2
)
)
;
long
totalBytesWritten
=
AzureMetricsTestUtil
.
getCurrentTotalBytesWritten
(
getInstrumentation
(
)
)
;
assertTrue
(
+
totalBytesWritten
+
+
FILE_SIZE
+
,
totalBytesWritten
>=
FILE_SIZE
&&
totalBytesWritten
<
(
FILE_SIZE
*
2
)
)
;
long
uploadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_UPLOAD_RATE
)
;
LOG
.
info
(
+
uploadRate
+
)
;
long
expectedRate
=
(
FILE_SIZE
*
1000L
)
/
uploadDurationMs
;
assertTrue
(
+
uploadRate
+
+
expectedRate
+
+
+
,
uploadRate
>=
expectedRate
)
;
long
uploadLatency
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_UPLOAD_LATENCY
)
;
InputStream
inputStream
=
getFileSystem
(
)
.
open
(
filePath
)
;
int
count
=
0
;
while
(
inputStream
.
read
(
)
>=
0
)
{
count
++
;
}
inputStream
.
close
(
)
;
long
downloadDurationMs
=
new
Date
(
)
.
getTime
(
)
-
start
.
getTime
(
)
;
assertEquals
(
FILE_SIZE
,
count
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
1
,
10
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
false
)
;
long
totalBytesRead
=
AzureMetricsTestUtil
.
getCurrentTotalBytesRead
(
getInstrumentation
(
)
)
;
assertEquals
(
FILE_SIZE
,
totalBytesRead
)
;
long
bytesRead
=
AzureMetricsTestUtil
.
getCurrentBytesRead
(
getInstrumentation
(
)
)
;
assertTrue
(
+
bytesRead
+
+
FILE_SIZE
+
,
bytesRead
>
(
FILE_SIZE
/
2
)
&&
bytesRead
<
(
FILE_SIZE
*
2
)
)
;
long
downloadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_DOWNLOAD_RATE
)
;
}
inputStream
.
close
(
)
;
long
downloadDurationMs
=
new
Date
(
)
.
getTime
(
)
-
start
.
getTime
(
)
;
assertEquals
(
FILE_SIZE
,
count
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
1
,
10
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
false
)
;
long
totalBytesRead
=
AzureMetricsTestUtil
.
getCurrentTotalBytesRead
(
getInstrumentation
(
)
)
;
assertEquals
(
FILE_SIZE
,
totalBytesRead
)
;
long
bytesRead
=
AzureMetricsTestUtil
.
getCurrentBytesRead
(
getInstrumentation
(
)
)
;
assertTrue
(
+
bytesRead
+
+
FILE_SIZE
+
,
bytesRead
>
(
FILE_SIZE
/
2
)
&&
bytesRead
<
(
FILE_SIZE
*
2
)
)
;
long
downloadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_DOWNLOAD_RATE
)
;
LOG
.
info
(
+
downloadRate
+
)
;
expectedRate
=
(
FILE_SIZE
*
1000L
)
/
downloadDurationMs
;
assertTrue
(
+
downloadRate
+
+
expectedRate
+
+
+
,
downloadRate
>=
expectedRate
)
;
long
downloadLatency
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_DOWNLOAD_LATENCY
)
;
@
Test
public
void
testMetricsOnBigFileCreateRead
(
)
throws
Exception
{
long
base
=
getBaseWebResponses
(
)
;
assertEquals
(
0
,
AzureMetricsTestUtil
.
getCurrentBytesWritten
(
getInstrumentation
(
)
)
)
;
Path
filePath
=
new
Path
(
)
;
final
int
FILE_SIZE
=
100
*
1024
*
1024
;
getBandwidthGaugeUpdater
(
)
.
suppressAutoUpdate
(
)
;
OutputStream
outputStream
=
getFileSystem
(
)
.
create
(
filePath
)
;
outputStream
.
write
(
new
byte
[
FILE_SIZE
]
)
;
outputStream
.
close
(
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
20
,
50
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
true
)
;
long
totalBytesWritten
=
AzureMetricsTestUtil
.
getCurrentTotalBytesWritten
(
getInstrumentation
(
)
)
;
assertTrue
(
+
totalBytesWritten
+
+
FILE_SIZE
+
,
totalBytesWritten
>=
FILE_SIZE
&&
totalBytesWritten
<
(
FILE_SIZE
*
2
)
)
;
long
uploadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_UPLOAD_RATE
)
;
assertEquals
(
0
,
AzureMetricsTestUtil
.
getCurrentBytesWritten
(
getInstrumentation
(
)
)
)
;
Path
filePath
=
new
Path
(
)
;
final
int
FILE_SIZE
=
100
*
1024
*
1024
;
getBandwidthGaugeUpdater
(
)
.
suppressAutoUpdate
(
)
;
OutputStream
outputStream
=
getFileSystem
(
)
.
create
(
filePath
)
;
outputStream
.
write
(
new
byte
[
FILE_SIZE
]
)
;
outputStream
.
close
(
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
20
,
50
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
true
)
;
long
totalBytesWritten
=
AzureMetricsTestUtil
.
getCurrentTotalBytesWritten
(
getInstrumentation
(
)
)
;
assertTrue
(
+
totalBytesWritten
+
+
FILE_SIZE
+
,
totalBytesWritten
>=
FILE_SIZE
&&
totalBytesWritten
<
(
FILE_SIZE
*
2
)
)
;
long
uploadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_UPLOAD_RATE
)
;
LOG
.
info
(
+
uploadRate
+
)
;
long
uploadLatency
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_UPLOAD_LATENCY
)
;
long
uploadLatency
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_UPLOAD_LATENCY
)
;
LOG
.
info
(
+
uploadLatency
)
;
assertTrue
(
+
uploadLatency
+
,
uploadLatency
>
0
)
;
InputStream
inputStream
=
getFileSystem
(
)
.
open
(
filePath
)
;
int
count
=
0
;
while
(
inputStream
.
read
(
)
>=
0
)
{
count
++
;
}
inputStream
.
close
(
)
;
assertEquals
(
FILE_SIZE
,
count
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
20
,
40
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
false
)
;
long
totalBytesRead
=
AzureMetricsTestUtil
.
getCurrentTotalBytesRead
(
getInstrumentation
(
)
)
;
assertEquals
(
FILE_SIZE
,
totalBytesRead
)
;
long
downloadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_DOWNLOAD_RATE
)
;
assertTrue
(
+
uploadLatency
+
,
uploadLatency
>
0
)
;
InputStream
inputStream
=
getFileSystem
(
)
.
open
(
filePath
)
;
int
count
=
0
;
while
(
inputStream
.
read
(
)
>=
0
)
{
count
++
;
}
inputStream
.
close
(
)
;
assertEquals
(
FILE_SIZE
,
count
)
;
logOpResponseCount
(
,
base
)
;
base
=
assertWebResponsesInRange
(
base
,
20
,
40
)
;
getBandwidthGaugeUpdater
(
)
.
triggerUpdate
(
false
)
;
long
totalBytesRead
=
AzureMetricsTestUtil
.
getCurrentTotalBytesRead
(
getInstrumentation
(
)
)
;
assertEquals
(
FILE_SIZE
,
totalBytesRead
)
;
long
downloadRate
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_DOWNLOAD_RATE
)
;
LOG
.
info
(
+
downloadRate
+
)
;
long
downloadLatency
=
AzureMetricsTestUtil
.
getLongGaugeValue
(
getInstrumentation
(
)
,
WASB_DOWNLOAD_LATENCY
)
;
private
void
logOpResponseCount
(
String
opName
,
long
base
)
{
protected
void
describe
(
String
text
,
Object
...
args
)
{
@
Test
public
void
testSeekStatistics
(
)
throws
IOException
{
describe
(
+
)
;
AzureBlobFileSystem
fs
=
getFileSystem
(
)
;
AzureBlobFileSystemStore
abfss
=
fs
.
getAbfsStore
(
)
;
Path
seekStatPath
=
path
(
getMethodName
(
)
)
;
AbfsOutputStream
out
=
null
;
AbfsInputStream
in
=
null
;
try
{
out
=
createAbfsOutputStreamWithFlushEnabled
(
fs
,
seekStatPath
)
;
out
.
write
(
defBuffer
)
;
out
.
hflush
(
)
;
in
=
abfss
.
openFileForRead
(
seekStatPath
,
fs
.
getFsStatistics
(
)
)
;
int
result
=
in
.
read
(
defBuffer
,
0
,
ONE_MB
)
;
AzureBlobFileSystemStore
abfss
=
fs
.
getAbfsStore
(
)
;
Path
seekStatPath
=
path
(
getMethodName
(
)
)
;
AbfsOutputStream
out
=
null
;
AbfsInputStream
in
=
null
;
try
{
out
=
createAbfsOutputStreamWithFlushEnabled
(
fs
,
seekStatPath
)
;
out
.
write
(
defBuffer
)
;
out
.
hflush
(
)
;
in
=
abfss
.
openFileForRead
(
seekStatPath
,
fs
.
getFsStatistics
(
)
)
;
int
result
=
in
.
read
(
defBuffer
,
0
,
ONE_MB
)
;
LOG
.
info
(
,
result
)
;
for
(
int
i
=
0
;
i
<
OPERATIONS
;
i
++
)
{
in
.
seek
(
0
)
;
in
.
seek
(
ONE_MB
)
;
}
AbfsInputStreamStatisticsImpl
stats
=
(
AbfsInputStreamStatisticsImpl
)
in
.
getStreamStatistics
(
)
;
in
=
abfss
.
openFileForRead
(
seekStatPath
,
fs
.
getFsStatistics
(
)
)
;
int
result
=
in
.
read
(
defBuffer
,
0
,
ONE_MB
)
;
LOG
.
info
(
,
result
)
;
for
(
int
i
=
0
;
i
<
OPERATIONS
;
i
++
)
{
in
.
seek
(
0
)
;
in
.
seek
(
ONE_MB
)
;
}
AbfsInputStreamStatisticsImpl
stats
=
(
AbfsInputStreamStatisticsImpl
)
in
.
getStreamStatistics
(
)
;
LOG
.
info
(
,
stats
.
toString
(
)
)
;
assertEquals
(
,
2
*
OPERATIONS
,
stats
.
getSeekOperations
(
)
)
;
assertEquals
(
,
OPERATIONS
,
stats
.
getBackwardSeekOperations
(
)
)
;
assertEquals
(
,
OPERATIONS
,
stats
.
getForwardSeekOperations
(
)
)
;
assertEquals
(
,
-
1
*
OPERATIONS
*
ONE_MB
,
stats
.
getBytesBackwardsOnSeek
(
)
)
;
assertEquals
(
,
0
,
stats
.
getBytesSkippedOnSeek
(
)
)
;
assertEquals
(
,
2
*
OPERATIONS
,
stats
.
getSeekInBuffer
(
)
)
;
in
.
close
(
)
;
@
Test
public
void
testReadStatistics
(
)
throws
IOException
{
describe
(
+
)
;
AzureBlobFileSystem
fs
=
getFileSystem
(
)
;
AzureBlobFileSystemStore
abfss
=
fs
.
getAbfsStore
(
)
;
Path
readStatPath
=
path
(
getMethodName
(
)
)
;
AbfsOutputStream
out
=
null
;
AbfsInputStream
in
=
null
;
try
{
out
=
createAbfsOutputStreamWithFlushEnabled
(
fs
,
readStatPath
)
;
out
.
write
(
defBuffer
)
;
out
.
hflush
(
)
;
in
=
abfss
.
openFileForRead
(
readStatPath
,
fs
.
getFsStatistics
(
)
)
;
for
(
int
i
=
0
;
i
<
OPERATIONS
;
i
++
)
{
in
.
read
(
)
;
}
AbfsInputStreamStatisticsImpl
stats
=
(
AbfsInputStreamStatisticsImpl
)
in
.
getStreamStatistics
(
)
;
AbfsOutputStream
out
=
null
;
AbfsInputStream
in
=
null
;
try
{
out
=
createAbfsOutputStreamWithFlushEnabled
(
fs
,
readStatPath
)
;
out
.
write
(
defBuffer
)
;
out
.
hflush
(
)
;
in
=
abfss
.
openFileForRead
(
readStatPath
,
fs
.
getFsStatistics
(
)
)
;
for
(
int
i
=
0
;
i
<
OPERATIONS
;
i
++
)
{
in
.
read
(
)
;
}
AbfsInputStreamStatisticsImpl
stats
=
(
AbfsInputStreamStatisticsImpl
)
in
.
getStreamStatistics
(
)
;
LOG
.
info
(
,
stats
.
toString
(
)
)
;
assertEquals
(
,
OPERATIONS
,
stats
.
getBytesRead
(
)
)
;
assertEquals
(
,
OPERATIONS
,
stats
.
getReadOperations
(
)
)
;
assertEquals
(
,
1
,
stats
.
getRemoteReadOperations
(
)
)
;
in
.
close
(
)
;
describe
(
)
;
AzureBlobFileSystem
fs
=
getFileSystem
(
)
;
Path
nullStatFilePath
=
path
(
getMethodName
(
)
)
;
byte
[
]
oneKbBuff
=
new
byte
[
ONE_KB
]
;
AbfsInputStreamContext
abfsInputStreamContext
=
new
AbfsInputStreamContext
(
getConfiguration
(
)
.
getSasTokenRenewPeriodForStreamsInSeconds
(
)
)
.
withReadBufferSize
(
getConfiguration
(
)
.
getReadBufferSize
(
)
)
.
withReadAheadQueueDepth
(
getConfiguration
(
)
.
getReadAheadQueueDepth
(
)
)
.
withStreamStatistics
(
null
)
.
build
(
)
;
AbfsOutputStream
out
=
null
;
AbfsInputStream
in
=
null
;
try
{
out
=
createAbfsOutputStreamWithFlushEnabled
(
fs
,
nullStatFilePath
)
;
out
.
write
(
oneKbBuff
)
;
out
.
hflush
(
)
;
AbfsRestOperation
abfsRestOperation
=
fs
.
getAbfsClient
(
)
.
getPathStatus
(
nullStatFilePath
.
toUri
(
)
.
getPath
(
)
,
false
)
;
in
=
new
AbfsInputStream
(
fs
.
getAbfsClient
(
)
,
null
,
nullStatFilePath
.
toUri
(
)
.
getPath
(
)
,
ONE_KB
,
abfsInputStreamContext
,
abfsRestOperation
.
getResult
(
)
.
getResponseHeader
(
)
)
;
assertNotEquals
(
+
,
-
1
,
in
.
read
(
)
)
;
in
.
seek
(
ONE_KB
)
;
AzureBlobFileSystem
fs
=
getFileSystem
(
)
;
Path
getResponsePath
=
path
(
getMethodName
(
)
)
;
Map
<
String
,
Long
>
metricMap
;
String
testResponseString
=
;
long
getResponses
,
bytesReceived
;
FSDataOutputStream
out
=
null
;
FSDataInputStream
in
=
null
;
try
{
out
=
fs
.
create
(
getResponsePath
)
;
out
.
write
(
testResponseString
.
getBytes
(
)
)
;
out
.
hflush
(
)
;
metricMap
=
fs
.
getInstrumentationMap
(
)
;
long
getResponsesBeforeTest
=
metricMap
.
get
(
CONNECTIONS_MADE
.
getStatName
(
)
)
;
in
=
fs
.
open
(
getResponsePath
)
;
int
result
=
in
.
read
(
)
;
String
testReadWriteOps
=
;
statistics
.
reset
(
)
;
assertReadWriteOps
(
,
0
,
statistics
.
getWriteOps
(
)
)
;
assertReadWriteOps
(
,
0
,
statistics
.
getReadOps
(
)
)
;
FSDataOutputStream
outForOneOperation
=
null
;
FSDataInputStream
inForOneOperation
=
null
;
try
{
outForOneOperation
=
fs
.
create
(
smallOperationsFile
)
;
statistics
.
reset
(
)
;
outForOneOperation
.
write
(
testReadWriteOps
.
getBytes
(
)
)
;
assertReadWriteOps
(
,
1
,
statistics
.
getWriteOps
(
)
)
;
outForOneOperation
.
hflush
(
)
;
inForOneOperation
=
fs
.
open
(
smallOperationsFile
)
;
statistics
.
reset
(
)
;
int
result
=
inForOneOperation
.
read
(
testReadWriteOps
.
getBytes
(
)
,
0
,
testReadWriteOps
.
getBytes
(
)
.
length
)
;
@
Test
@
Ignore
(
)
public
void
testRandomReadPerformance
(
)
throws
Exception
{
Assume
.
assumeFalse
(
,
this
.
getFileSystem
(
)
.
getIsNamespaceEnabled
(
)
)
;
createTestFile
(
)
;
assumeHugeFileExists
(
)
;
final
AzureBlobFileSystem
abFs
=
this
.
getFileSystem
(
)
;
final
NativeAzureFileSystem
wasbFs
=
this
.
getWasbFileSystem
(
)
;
final
int
maxAttempts
=
10
;
final
double
maxAcceptableRatio
=
1.025
;
double
v1ElapsedMs
=
0
,
v2ElapsedMs
=
0
;
double
ratio
=
Double
.
MAX_VALUE
;
for
(
int
i
=
0
;
i
<
maxAttempts
&&
ratio
>=
maxAcceptableRatio
;
i
++
)
{
v1ElapsedMs
=
randomRead
(
1
,
wasbFs
)
;
v2ElapsedMs
=
randomRead
(
2
,
abFs
)
;
ratio
=
v2ElapsedMs
/
v1ElapsedMs
;
private
long
sequentialRead
(
String
version
,
FileSystem
fs
,
boolean
afterReverseSeek
)
throws
IOException
{
byte
[
]
buffer
=
new
byte
[
SEQUENTIAL_READ_BUFFER_SIZE
]
;
long
totalBytesRead
=
0
;
long
bytesRead
=
0
;
try
(
FSDataInputStream
inputStream
=
fs
.
open
(
TEST_FILE_PATH
)
)
{
if
(
afterReverseSeek
)
{
while
(
bytesRead
>
0
&&
totalBytesRead
<
4
*
MEGABYTE
)
{
bytesRead
=
inputStream
.
read
(
buffer
)
;
totalBytesRead
+=
bytesRead
;
}
totalBytesRead
=
0
;
inputStream
.
seek
(
0
)
;
}
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
while
(
(
bytesRead
=
inputStream
.
read
(
buffer
)
)
>
0
)
{
totalBytesRead
+=
bytesRead
;
}
long
elapsedTimeMs
=
timer
.
elapsedTimeMs
(
)
;
assumeHugeFileExists
(
)
;
final
long
minBytesToRead
=
2
*
MEGABYTE
;
Random
random
=
new
Random
(
)
;
byte
[
]
buffer
=
new
byte
[
8
*
KILOBYTE
]
;
long
totalBytesRead
=
0
;
long
bytesRead
=
0
;
try
(
FSDataInputStream
inputStream
=
fs
.
open
(
TEST_FILE_PATH
)
)
{
ContractTestUtils
.
NanoTimer
timer
=
new
ContractTestUtils
.
NanoTimer
(
)
;
do
{
bytesRead
=
inputStream
.
read
(
buffer
)
;
totalBytesRead
+=
bytesRead
;
inputStream
.
seek
(
random
.
nextInt
(
(
int
)
(
TEST_FILE_SIZE
-
buffer
.
length
)
)
)
;
}
while
(
bytesRead
>
0
&&
totalBytesRead
<
minBytesToRead
)
;
long
elapsedTimeMs
=
timer
.
elapsedTimeMs
(
)
;
inputStream
.
close
(
)
;
@
Override
public
void
initialize
(
final
Configuration
configuration
)
throws
IOException
{
initialized
=
true
;
owner
=
UserGroupInformation
.
getCurrentUser
(
)
;
protected
void
innerBind
(
final
URI
uri
,
final
Configuration
conf
)
throws
IOException
{
Preconditions
.
checkState
(
initialized
,
)
;
Preconditions
.
checkState
(
fsURI
==
null
,
)
;
fsURI
=
uri
;
canonicalServiceName
=
uri
.
toString
(
)
;
public
StubAbfsTokenIdentifier
verifyCredentialsContainsToken
(
final
Credentials
credentials
,
final
String
serviceName
,
final
String
tokenService
)
throws
IOException
{
Token
<
?
extends
TokenIdentifier
>
token
=
credentials
.
getToken
(
new
Text
(
serviceName
)
)
;
assertEquals
(
+
token
,
StubAbfsTokenIdentifier
.
TOKEN_KIND
,
token
.
getKind
(
)
)
;
assertEquals
(
+
token
,
tokenService
,
token
.
getService
(
)
.
toString
(
)
)
;
StubAbfsTokenIdentifier
abfsId
=
(
StubAbfsTokenIdentifier
)
token
.
decodeIdentifier
(
)
;
protected
String
dtutil
(
final
int
expected
,
final
Configuration
conf
,
final
String
...
args
)
throws
Exception
{
final
ByteArrayOutputStream
dtUtilContent
=
new
ByteArrayOutputStream
(
)
;
DtUtilShell
dt
=
new
DtUtilShell
(
)
;
dt
.
setOut
(
new
PrintStream
(
dtUtilContent
)
)
;
dtUtilContent
.
reset
(
)
;
int
r
=
doAs
(
aliceUser
,
(
)
->
ToolRunner
.
run
(
conf
,
dt
,
args
)
)
;
String
s
=
dtUtilContent
.
toString
(
)
;
@
Test
public
void
verifyGettingLatencyRecordsIsCheapWhenDisabled
(
)
throws
Exception
{
final
double
maxLatencyWhenDisabledMs
=
1000
;
final
double
minLatencyWhenDisabledMs
=
0
;
final
long
numTasks
=
1000
;
long
aggregateLatency
=
0
;
AbfsPerfTracker
abfsPerfTracker
=
new
AbfsPerfTracker
(
accountName
,
filesystemName
,
false
)
;
List
<
Callable
<
Long
>>
tasks
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
numTasks
;
i
++
)
{
tasks
.
add
(
(
)
->
{
Instant
startGet
=
Instant
.
now
(
)
;
abfsPerfTracker
.
getClientLatency
(
)
;
long
latencyGet
=
Duration
.
between
(
startGet
,
Instant
.
now
(
)
)
.
toMillis
(
)
;
@
Test
public
void
verifyGettingLatencyRecordsIsCheapWhenEnabled
(
)
throws
Exception
{
final
double
maxLatencyWhenDisabledMs
=
5000
;
final
double
minLatencyWhenDisabledMs
=
0
;
final
long
numTasks
=
1000
;
long
aggregateLatency
=
0
;
AbfsPerfTracker
abfsPerfTracker
=
new
AbfsPerfTracker
(
accountName
,
filesystemName
,
true
)
;
List
<
Callable
<
Long
>>
tasks
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
numTasks
;
i
++
)
{
tasks
.
add
(
(
)
->
{
Instant
startRecord
=
Instant
.
now
(
)
;
abfsPerfTracker
.
getClientLatency
(
)
;
long
latencyRecord
=
Duration
.
between
(
startRecord
,
Instant
.
now
(
)
)
.
toMillis
(
)
;
public
void
checkContainers
(
)
throws
Throwable
{
Assume
.
assumeTrue
(
this
.
getAuthType
(
)
==
AuthType
.
SharedKey
)
;
int
count
=
0
;
CloudStorageAccount
storageAccount
=
AzureBlobStorageTestAccount
.
createTestAccount
(
)
;
CloudBlobClient
blobClient
=
storageAccount
.
createCloudBlobClient
(
)
;
Iterable
<
CloudBlobContainer
>
containers
=
blobClient
.
listContainers
(
TEST_CONTAINER_PREFIX
)
;
for
(
CloudBlobContainer
container
:
containers
)
{
count
++
;
public
void
deleteContainers
(
)
throws
Throwable
{
Assume
.
assumeTrue
(
this
.
getAuthType
(
)
==
AuthType
.
SharedKey
)
;
int
count
=
0
;
CloudStorageAccount
storageAccount
=
AzureBlobStorageTestAccount
.
createTestAccount
(
)
;
CloudBlobClient
blobClient
=
storageAccount
.
createCloudBlobClient
(
)
;
Iterable
<
CloudBlobContainer
>
containers
=
blobClient
.
listContainers
(
TEST_CONTAINER_PREFIX
)
;
for
(
CloudBlobContainer
container
:
containers
)
{
sb
.
append
(
path
)
;
}
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
sv
)
;
sb
.
append
(
)
;
sb
.
append
(
sr
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
String
stringToSign
=
sb
.
toString
(
)
;
while
(
arg1
.
hasNext
(
)
)
{
this
.
numOfValues
+=
1
;
if
(
this
.
numOfValues
%
100
==
0
)
{
reporter
.
setStatus
(
+
key
.
toString
(
)
+
+
this
.
numOfValues
)
;
}
if
(
this
.
numOfValues
>
this
.
maxNumOfValuesPerGroup
)
{
continue
;
}
aRecord
=
(
(
TaggedMapOutput
)
arg1
.
next
(
)
)
.
clone
(
job
)
;
Text
tag
=
aRecord
.
getTag
(
)
;
ResetableIterator
data
=
retv
.
get
(
tag
)
;
if
(
data
==
null
)
{
data
=
createResetableIterator
(
)
;
retv
.
put
(
tag
,
data
)
;
}
data
.
add
(
aRecord
)
;
}
if
(
this
.
numOfValues
>
this
.
largestNumOfValues
)
{
this
.
largestNumOfValues
=
numOfValues
;
OptionsParser
.
usage
(
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
try
{
context
=
new
DistCpContext
(
OptionsParser
.
parse
(
argv
)
)
;
checkSplitLargeFile
(
)
;
setTargetPathExists
(
)
;
LOG
.
info
(
+
context
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
,
e
)
;
System
.
err
.
println
(
+
e
.
getMessage
(
)
)
;
OptionsParser
.
usage
(
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
try
{
execute
(
)
;
}
catch
(
InvalidInputException
e
)
{
try
{
context
=
new
DistCpContext
(
OptionsParser
.
parse
(
argv
)
)
;
checkSplitLargeFile
(
)
;
setTargetPathExists
(
)
;
LOG
.
info
(
+
context
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
,
e
)
;
System
.
err
.
println
(
+
e
.
getMessage
(
)
)
;
OptionsParser
.
usage
(
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
try
{
execute
(
)
;
}
catch
(
InvalidInputException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
setTargetPathExists
(
)
;
LOG
.
info
(
+
context
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
,
e
)
;
System
.
err
.
println
(
+
e
.
getMessage
(
)
)
;
OptionsParser
.
usage
(
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
try
{
execute
(
)
;
}
catch
(
InvalidInputException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
catch
(
DuplicateFileException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
DUPLICATE_INPUT
;
catch
(
Throwable
e
)
{
LOG
.
error
(
,
e
)
;
System
.
err
.
println
(
+
e
.
getMessage
(
)
)
;
OptionsParser
.
usage
(
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
try
{
execute
(
)
;
}
catch
(
InvalidInputException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
catch
(
DuplicateFileException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
DUPLICATE_INPUT
;
}
catch
(
AclsNotSupportedException
e
)
{
LOG
.
error
(
,
e
)
;
OptionsParser
.
usage
(
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
try
{
execute
(
)
;
}
catch
(
InvalidInputException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
INVALID_ARGUMENT
;
}
catch
(
DuplicateFileException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
DUPLICATE_INPUT
;
}
catch
(
AclsNotSupportedException
e
)
{
LOG
.
error
(
,
e
)
;
return
DistCpConstants
.
ACLS_NOT_SUPPORTED
;
}
catch
(
XAttrsNotSupportedException
e
)
{
LOG
.
error
(
,
e
)
;
}
if
(
context
.
shouldAtomicCommit
(
)
)
{
Path
workDir
=
context
.
getAtomicWorkPath
(
)
;
if
(
workDir
==
null
)
{
workDir
=
targetPath
.
getParent
(
)
;
}
workDir
=
new
Path
(
workDir
,
WIP_PREFIX
+
targetPath
.
getName
(
)
+
rand
.
nextInt
(
)
)
;
FileSystem
workFS
=
workDir
.
getFileSystem
(
configuration
)
;
if
(
!
FileUtil
.
compareFs
(
targetFS
,
workFS
)
)
{
throw
new
IllegalArgumentException
(
+
workDir
+
+
targetPath
+
)
;
}
CopyOutputFormat
.
setWorkingDirectory
(
job
,
workDir
)
;
}
else
{
CopyOutputFormat
.
setWorkingDirectory
(
job
,
targetPath
)
;
}
CopyOutputFormat
.
setCommitDirectory
(
job
,
targetPath
)
;
Path
logPath
=
context
.
getLogPath
(
)
;
if
(
logPath
==
null
)
{
logPath
=
new
Path
(
metaFolder
,
)
;
@
Override
public
boolean
shouldCopy
(
Path
path
)
{
for
(
Pattern
filter
:
filters
)
{
if
(
filter
.
matcher
(
path
.
toString
(
)
)
.
matches
(
)
)
{
@
VisibleForTesting
protected
void
doBuildListingWithSnapshotDiff
(
SequenceFile
.
Writer
fileListWriter
,
DistCpContext
context
)
throws
IOException
{
ArrayList
<
DiffInfo
>
diffList
=
distCpSync
.
prepareDiffListForCopyListing
(
)
;
Path
sourceRoot
=
context
.
getSourcePaths
(
)
.
get
(
0
)
;
FileSystem
sourceFS
=
sourceRoot
.
getFileSystem
(
getConf
(
)
)
;
try
{
List
<
FileStatusInfo
>
fileStatuses
=
Lists
.
newArrayList
(
)
;
for
(
DiffInfo
diff
:
diffList
)
{
diff
.
setTarget
(
new
Path
(
context
.
getSourcePaths
(
)
.
get
(
0
)
,
diff
.
getTarget
(
)
)
)
;
if
(
diff
.
getType
(
)
==
SnapshotDiffReport
.
DiffType
.
MODIFY
)
{
addToFileListing
(
fileListWriter
,
sourceRoot
,
diff
.
getTarget
(
)
,
context
)
;
}
else
if
(
diff
.
getType
(
)
==
SnapshotDiffReport
.
DiffType
.
CREATE
)
{
addToFileListing
(
fileListWriter
,
sourceRoot
,
diff
.
getTarget
(
)
,
context
)
;
FileStatus
sourceStatus
=
sourceFS
.
getFileStatus
(
diff
.
getTarget
(
)
)
;
if
(
sourceStatus
.
isDirectory
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
FileSystem
sourceFS
=
path
.
getFileSystem
(
getConf
(
)
)
;
final
boolean
preserveAcls
=
context
.
shouldPreserve
(
FileAttribute
.
ACL
)
;
final
boolean
preserveXAttrs
=
context
.
shouldPreserve
(
FileAttribute
.
XATTR
)
;
final
boolean
preserveRawXAttrs
=
context
.
shouldPreserveRawXattrs
(
)
;
path
=
makeQualified
(
path
)
;
FileStatus
rootStatus
=
sourceFS
.
getFileStatus
(
path
)
;
Path
sourcePathRoot
=
computeSourceRootPath
(
rootStatus
,
context
)
;
FileStatus
[
]
sourceFiles
=
sourceFS
.
listStatus
(
path
)
;
boolean
explore
=
(
sourceFiles
!=
null
&&
sourceFiles
.
length
>
0
)
;
if
(
!
explore
||
rootStatus
.
isDirectory
(
)
)
{
LinkedList
<
CopyListingFileStatus
>
rootCopyListingStatus
=
DistCpUtils
.
toCopyListingFileStatus
(
sourceFS
,
rootStatus
,
preserveAcls
,
preserveXAttrs
,
preserveRawXAttrs
,
context
.
getBlocksPerChunk
(
)
)
;
writeToFileListingRoot
(
fileListWriter
,
rootCopyListingStatus
,
sourcePathRoot
,
context
)
;
}
if
(
explore
)
{
ArrayList
<
FileStatus
>
sourceDirs
=
new
ArrayList
<
FileStatus
>
(
)
;
for
(
FileStatus
sourceStatus
:
sourceFiles
)
{
if
(
!
explore
||
rootStatus
.
isDirectory
(
)
)
{
LinkedList
<
CopyListingFileStatus
>
rootCopyListingStatus
=
DistCpUtils
.
toCopyListingFileStatus
(
sourceFS
,
rootStatus
,
preserveAcls
,
preserveXAttrs
,
preserveRawXAttrs
,
context
.
getBlocksPerChunk
(
)
)
;
writeToFileListingRoot
(
fileListWriter
,
rootCopyListingStatus
,
sourcePathRoot
,
context
)
;
}
if
(
explore
)
{
ArrayList
<
FileStatus
>
sourceDirs
=
new
ArrayList
<
FileStatus
>
(
)
;
for
(
FileStatus
sourceStatus
:
sourceFiles
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
sourceStatus
.
getPath
(
)
+
)
;
}
LinkedList
<
CopyListingFileStatus
>
sourceCopyListingStatus
=
DistCpUtils
.
toCopyListingFileStatus
(
sourceFS
,
sourceStatus
,
preserveAcls
&&
sourceStatus
.
isDirectory
(
)
,
preserveXAttrs
&&
sourceStatus
.
isDirectory
(
)
,
preserveRawXAttrs
&&
sourceStatus
.
isDirectory
(
)
,
context
.
getBlocksPerChunk
(
)
)
;
for
(
CopyListingFileStatus
fs
:
sourceCopyListingStatus
)
{
if
(
randomizeFileListing
)
{
addToFileListing
(
statusList
,
new
FileStatusInfo
(
fs
,
sourcePathRoot
)
,
fileListWriter
)
;
}
else
{
writeToFileListing
(
fileListWriter
,
fs
,
sourcePathRoot
)
;
}
}
if
(
sourceStatus
.
isDirectory
(
)
)
{
private
void
writeToFileListingRoot
(
SequenceFile
.
Writer
fileListWriter
,
LinkedList
<
CopyListingFileStatus
>
fileStatus
,
Path
sourcePathRoot
,
DistCpContext
context
)
throws
IOException
{
boolean
syncOrOverwrite
=
context
.
shouldSyncFolder
(
)
||
context
.
shouldOverwrite
(
)
;
for
(
CopyListingFileStatus
fs
:
fileStatus
)
{
if
(
fs
.
getPath
(
)
.
equals
(
sourcePathRoot
)
&&
fs
.
isDirectory
(
)
&&
syncOrOverwrite
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
writeToFileListing
(
SequenceFile
.
Writer
fileListWriter
,
CopyListingFileStatus
fileStatus
,
Path
sourcePathRoot
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
cleanup
(
Configuration
conf
)
{
Path
metaFolder
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_META_FOLDER
)
)
;
try
{
FileSystem
fs
=
metaFolder
.
getFileSystem
(
conf
)
;
return
;
}
Path
sourceListing
=
new
Path
(
spath
)
;
SequenceFile
.
Reader
sourceReader
=
new
SequenceFile
.
Reader
(
conf
,
SequenceFile
.
Reader
.
file
(
sourceListing
)
)
;
Path
targetRoot
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_WORK_PATH
)
)
;
try
{
CopyListingFileStatus
srcFileStatus
=
new
CopyListingFileStatus
(
)
;
Text
srcRelPath
=
new
Text
(
)
;
CopyListingFileStatus
lastFileStatus
=
null
;
LinkedList
<
Path
>
allChunkPaths
=
new
LinkedList
<
Path
>
(
)
;
while
(
sourceReader
.
next
(
srcRelPath
,
srcFileStatus
)
)
{
if
(
srcFileStatus
.
isDirectory
(
)
)
{
continue
;
}
Path
targetFile
=
new
Path
(
targetRoot
.
toString
(
)
+
+
srcRelPath
)
;
Path
targetFileChunkPath
=
DistCpUtils
.
getSplitChunkPath
(
targetFile
,
srcFileStatus
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
preserveFileAttributesForDirectories
(
Configuration
conf
)
throws
IOException
{
String
attrSymbols
=
conf
.
get
(
DistCpConstants
.
CONF_LABEL_PRESERVE_STATUS
)
;
final
boolean
syncOrOverwrite
=
syncFolder
||
overwrite
;
private
void
trackMissing
(
Configuration
conf
)
throws
IOException
{
Path
trackDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TRACK_MISSING
)
)
;
Path
sourceListing
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_LISTING_FILE_PATH
)
)
;
private
void
trackMissing
(
Configuration
conf
)
throws
IOException
{
Path
trackDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TRACK_MISSING
)
)
;
Path
sourceListing
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_LISTING_FILE_PATH
)
)
;
LOG
.
info
(
,
trackDir
)
;
Path
sourceSortedListing
=
new
Path
(
trackDir
,
DistCpConstants
.
SOURCE_SORTED_FILE
)
;
private
void
trackMissing
(
Configuration
conf
)
throws
IOException
{
Path
trackDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TRACK_MISSING
)
)
;
Path
sourceListing
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_LISTING_FILE_PATH
)
)
;
LOG
.
info
(
,
trackDir
)
;
Path
sourceSortedListing
=
new
Path
(
trackDir
,
DistCpConstants
.
SOURCE_SORTED_FILE
)
;
LOG
.
info
(
,
sourceSortedListing
)
;
DistCpUtils
.
sortListing
(
conf
,
sourceListing
,
sourceSortedListing
)
;
Path
targetListing
=
new
Path
(
trackDir
,
TARGET_LISTING_FILE
)
;
Path
sortedTargetListing
=
new
Path
(
trackDir
,
TARGET_SORTED_FILE
)
;
listTargetFiles
(
conf
,
targetListing
,
sortedTargetListing
)
;
private
void
deleteMissing
(
Configuration
conf
)
throws
IOException
{
LOG
.
info
(
+
)
;
long
listingStart
=
System
.
currentTimeMillis
(
)
;
Path
sourceListing
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_LISTING_FILE_PATH
)
)
;
FileSystem
clusterFS
=
sourceListing
.
getFileSystem
(
conf
)
;
Path
sortedSourceListing
=
DistCpUtils
.
sortListing
(
conf
,
sourceListing
)
;
long
sourceListingCompleted
=
System
.
currentTimeMillis
(
)
;
private
void
deleteMissing
(
Configuration
conf
)
throws
IOException
{
LOG
.
info
(
+
)
;
long
listingStart
=
System
.
currentTimeMillis
(
)
;
Path
sourceListing
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_LISTING_FILE_PATH
)
)
;
FileSystem
clusterFS
=
sourceListing
.
getFileSystem
(
conf
)
;
Path
sortedSourceListing
=
DistCpUtils
.
sortListing
(
conf
,
sourceListing
)
;
long
sourceListingCompleted
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
,
formatDuration
(
sourceListingCompleted
-
listingStart
)
)
;
Path
targetListing
=
new
Path
(
sourceListing
.
getParent
(
)
,
)
;
Path
sortedTargetListing
=
new
Path
(
targetListing
.
toString
(
)
+
)
;
Path
targetFinalPath
=
listTargetFiles
(
conf
,
targetListing
,
sortedTargetListing
)
;
long
totalLen
=
clusterFS
.
getFileStatus
(
sortedTargetListing
)
.
getLen
(
)
;
SequenceFile
.
Reader
sourceReader
=
new
SequenceFile
.
Reader
(
conf
,
SequenceFile
.
Reader
.
file
(
sortedSourceListing
)
)
;
SequenceFile
.
Reader
targetReader
=
new
SequenceFile
.
Reader
(
conf
,
SequenceFile
.
Reader
.
file
(
sortedTargetListing
)
)
;
long
deletionStart
=
System
.
currentTimeMillis
(
)
;
long
skippedDeletes
=
0
;
long
deletedDirectories
=
0
;
final
DeletedDirTracker
tracker
=
new
DeletedDirTracker
(
1000
)
;
try
{
CopyListingFileStatus
srcFileStatus
=
new
CopyListingFileStatus
(
)
;
Text
srcRelPath
=
new
Text
(
)
;
CopyListingFileStatus
trgtFileStatus
=
new
CopyListingFileStatus
(
)
;
Text
trgtRelPath
=
new
Text
(
)
;
final
FileSystem
targetFS
=
targetFinalPath
.
getFileSystem
(
conf
)
;
boolean
showProgress
;
boolean
srcAvailable
=
sourceReader
.
next
(
srcRelPath
,
srcFileStatus
)
;
while
(
targetReader
.
next
(
trgtRelPath
,
trgtFileStatus
)
)
{
while
(
srcAvailable
&&
trgtRelPath
.
compareTo
(
srcRelPath
)
>
0
)
{
srcAvailable
=
sourceReader
.
next
(
srcRelPath
,
srcFileStatus
)
;
}
Path
targetEntry
=
trgtFileStatus
.
getPath
(
)
;
CopyListingFileStatus
trgtFileStatus
=
new
CopyListingFileStatus
(
)
;
Text
trgtRelPath
=
new
Text
(
)
;
final
FileSystem
targetFS
=
targetFinalPath
.
getFileSystem
(
conf
)
;
boolean
showProgress
;
boolean
srcAvailable
=
sourceReader
.
next
(
srcRelPath
,
srcFileStatus
)
;
while
(
targetReader
.
next
(
trgtRelPath
,
trgtFileStatus
)
)
{
while
(
srcAvailable
&&
trgtRelPath
.
compareTo
(
srcRelPath
)
>
0
)
{
srcAvailable
=
sourceReader
.
next
(
srcRelPath
,
srcFileStatus
)
;
}
Path
targetEntry
=
trgtFileStatus
.
getPath
(
)
;
LOG
.
debug
(
,
srcFileStatus
.
getPath
(
)
,
targetEntry
)
;
if
(
srcAvailable
&&
trgtRelPath
.
equals
(
srcRelPath
)
)
continue
;
if
(
tracker
.
shouldDelete
(
trgtFileStatus
)
)
{
showProgress
=
true
;
try
{
if
(
targetFS
.
delete
(
targetEntry
,
true
)
)
{
srcAvailable
=
sourceReader
.
next
(
srcRelPath
,
srcFileStatus
)
;
}
Path
targetEntry
=
trgtFileStatus
.
getPath
(
)
;
LOG
.
debug
(
,
srcFileStatus
.
getPath
(
)
,
targetEntry
)
;
if
(
srcAvailable
&&
trgtRelPath
.
equals
(
srcRelPath
)
)
continue
;
if
(
tracker
.
shouldDelete
(
trgtFileStatus
)
)
{
showProgress
=
true
;
try
{
if
(
targetFS
.
delete
(
targetEntry
,
true
)
)
{
LOG
.
info
(
+
targetEntry
+
)
;
deletedEntries
++
;
if
(
trgtFileStatus
.
isDirectory
(
)
)
{
deletedDirectories
++
;
}
else
{
filesDeleted
++
;
}
}
else
{
showProgress
=
true
;
try
{
if
(
targetFS
.
delete
(
targetEntry
,
true
)
)
{
LOG
.
info
(
+
targetEntry
+
)
;
deletedEntries
++
;
if
(
trgtFileStatus
.
isDirectory
(
)
)
{
deletedDirectories
++
;
}
else
{
filesDeleted
++
;
}
}
else
{
LOG
.
info
(
,
targetEntry
,
trgtFileStatus
)
;
missingDeletes
++
;
}
}
catch
(
IOException
e
)
{
if
(
!
ignoreFailures
)
{
throw
e
;
try
{
if
(
targetFS
.
delete
(
targetEntry
,
true
)
)
{
LOG
.
info
(
+
targetEntry
+
)
;
deletedEntries
++
;
if
(
trgtFileStatus
.
isDirectory
(
)
)
{
deletedDirectories
++
;
}
else
{
filesDeleted
++
;
}
}
else
{
LOG
.
info
(
,
targetEntry
,
trgtFileStatus
)
;
missingDeletes
++
;
}
}
catch
(
IOException
e
)
{
if
(
!
ignoreFailures
)
{
throw
e
;
}
else
{
deletedEntries
++
;
if
(
trgtFileStatus
.
isDirectory
(
)
)
{
deletedDirectories
++
;
}
else
{
filesDeleted
++
;
}
}
else
{
LOG
.
info
(
,
targetEntry
,
trgtFileStatus
)
;
missingDeletes
++
;
}
}
catch
(
IOException
e
)
{
if
(
!
ignoreFailures
)
{
throw
e
;
}
else
{
LOG
.
info
(
,
targetEntry
,
e
.
toString
(
)
)
;
LOG
.
debug
(
,
targetEntry
,
e
)
;
failedDeletes
++
;
}
catch
(
IOException
e
)
{
if
(
!
ignoreFailures
)
{
throw
e
;
}
else
{
LOG
.
info
(
,
targetEntry
,
e
.
toString
(
)
)
;
LOG
.
debug
(
,
targetEntry
,
e
)
;
failedDeletes
++
;
}
}
}
else
{
LOG
.
debug
(
,
targetEntry
)
;
skippedDeletes
++
;
showProgress
=
false
;
}
if
(
showProgress
)
{
taskAttemptContext
.
progress
(
)
;
taskAttemptContext
.
setStatus
(
+
targetReader
.
getPosition
(
)
*
100
/
totalLen
+
)
;
}
}
LOG
.
info
(
,
targetFS
)
;
private
Path
listTargetFiles
(
final
Configuration
conf
,
final
Path
targetListing
,
final
Path
sortedTargetListing
)
throws
IOException
{
CopyListing
target
=
new
GlobbedCopyListing
(
new
Configuration
(
conf
)
,
null
)
;
Path
targetFinalPath
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_FINAL_PATH
)
)
;
List
<
Path
>
targets
=
new
ArrayList
<
>
(
1
)
;
targets
.
add
(
targetFinalPath
)
;
Path
resultNonePath
=
Path
.
getPathWithoutSchemeAndAuthority
(
targetFinalPath
)
.
toString
(
)
.
startsWith
(
DistCpConstants
.
HDFS_RESERVED_RAW_DIRECTORY_NAME
)
?
DistCpConstants
.
RAW_NONE_PATH
:
DistCpConstants
.
NONE_PATH
;
int
threads
=
conf
.
getInt
(
DistCpConstants
.
CONF_LABEL_LISTSTATUS_THREADS
,
DistCpConstants
.
DEFAULT_LISTSTATUS_THREADS
)
;
private
void
commitData
(
Configuration
conf
)
throws
IOException
{
Path
workDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_WORK_PATH
)
)
;
Path
finalDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_FINAL_PATH
)
)
;
FileSystem
targetFS
=
workDir
.
getFileSystem
(
conf
)
;
private
void
commitData
(
Configuration
conf
)
throws
IOException
{
Path
workDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_WORK_PATH
)
)
;
Path
finalDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_FINAL_PATH
)
)
;
FileSystem
targetFS
=
workDir
.
getFileSystem
(
conf
)
;
LOG
.
info
(
+
workDir
+
+
finalDir
)
;
if
(
targetFS
.
exists
(
finalDir
)
&&
targetFS
.
exists
(
workDir
)
)
{
Path
workDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_WORK_PATH
)
)
;
Path
finalDir
=
new
Path
(
conf
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_FINAL_PATH
)
)
;
FileSystem
targetFS
=
workDir
.
getFileSystem
(
conf
)
;
LOG
.
info
(
+
workDir
+
+
finalDir
)
;
if
(
targetFS
.
exists
(
finalDir
)
&&
targetFS
.
exists
(
workDir
)
)
{
LOG
.
error
(
+
finalDir
)
;
throw
new
IOException
(
+
+
finalDir
+
+
workDir
+
)
;
}
boolean
result
=
targetFS
.
rename
(
workDir
,
finalDir
)
;
if
(
!
result
)
{
LOG
.
warn
(
)
;
result
=
targetFS
.
exists
(
finalDir
)
&&
!
targetFS
.
exists
(
workDir
)
;
}
if
(
result
)
{
LOG
.
info
(
+
finalDir
)
;
taskAttemptContext
.
setStatus
(
+
finalDir
)
;
}
else
{
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
targetFile
+
+
allChunkPaths
.
size
(
)
)
;
}
FileSystem
dstfs
=
targetFile
.
getFileSystem
(
conf
)
;
FileSystem
srcfs
=
sourceFile
.
getFileSystem
(
conf
)
;
Path
firstChunkFile
=
allChunkPaths
.
removeFirst
(
)
;
Path
[
]
restChunkFiles
=
new
Path
[
allChunkPaths
.
size
(
)
]
;
allChunkPaths
.
toArray
(
restChunkFiles
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
dstfs
.
getFileStatus
(
firstChunkFile
)
)
;
int
i
=
0
;
for
(
Path
f
:
restChunkFiles
)
{
LOG
.
debug
(
+
i
+
+
dstfs
.
getFileStatus
(
f
)
)
;
++
i
;
}
}
dstfs
.
concat
(
firstChunkFile
,
restChunkFiles
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
map
(
Text
relPath
,
CopyListingFileStatus
sourceFileStatus
,
Context
context
)
throws
IOException
,
InterruptedException
{
Path
sourcePath
=
sourceFileStatus
.
getPath
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
LOG
.
debug
(
+
sourcePath
+
+
relPath
)
;
Path
target
=
new
Path
(
targetWorkPath
.
makeQualified
(
targetFS
.
getUri
(
)
,
targetFS
.
getWorkingDirectory
(
)
)
+
relPath
.
toString
(
)
)
;
EnumSet
<
DistCpOptions
.
FileAttribute
>
fileAttributes
=
getFileAttributeSettings
(
context
)
;
final
boolean
preserveRawXattrs
=
context
.
getConfiguration
(
)
.
getBoolean
(
DistCpConstants
.
CONF_LABEL_PRESERVE_RAWXATTRS
,
false
)
;
final
String
description
=
+
sourcePath
+
+
target
;
context
.
setStatus
(
description
)
;
}
catch
(
FileNotFoundException
e
)
{
throw
new
IOException
(
new
RetriableFileCopyCommand
.
CopyReadException
(
e
)
)
;
}
FileStatus
targetStatus
=
null
;
try
{
targetStatus
=
targetFS
.
getFileStatus
(
target
)
;
}
catch
(
FileNotFoundException
ignore
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
LOG
.
debug
(
+
target
,
ignore
)
;
}
if
(
targetStatus
!=
null
&&
(
targetStatus
.
isDirectory
(
)
!=
sourceCurrStatus
.
isDirectory
(
)
)
)
{
throw
new
IOException
(
+
target
+
+
getFileType
(
targetStatus
)
+
+
getFileType
(
sourceCurrStatus
)
)
;
}
if
(
sourceCurrStatus
.
isDirectory
(
)
)
{
createTargetDirsWithRetry
(
description
,
target
,
context
,
sourceStatus
)
;
return
;
}
FileAction
action
=
checkUpdate
(
sourceFS
,
sourceCurrStatus
,
target
,
targetStatus
)
;
Path
tmpTarget
=
target
;
if
(
action
==
FileAction
.
SKIP
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
LOG
.
debug
(
+
target
,
ignore
)
;
}
if
(
targetStatus
!=
null
&&
(
targetStatus
.
isDirectory
(
)
!=
sourceCurrStatus
.
isDirectory
(
)
)
)
{
throw
new
IOException
(
+
target
+
+
getFileType
(
targetStatus
)
+
+
getFileType
(
sourceCurrStatus
)
)
;
}
if
(
sourceCurrStatus
.
isDirectory
(
)
)
{
createTargetDirsWithRetry
(
description
,
target
,
context
,
sourceStatus
)
;
return
;
}
FileAction
action
=
checkUpdate
(
sourceFS
,
sourceCurrStatus
,
target
,
targetStatus
)
;
Path
tmpTarget
=
target
;
if
(
action
==
FileAction
.
SKIP
)
{
LOG
.
info
(
+
sourceCurrStatus
.
getPath
(
)
+
+
target
)
;
updateSkipCounters
(
context
,
sourceCurrStatus
)
;
context
.
write
(
null
,
new
Text
(
+
sourceCurrStatus
.
getPath
(
)
)
)
;
if
(
verboseLog
)
{
context
.
write
(
null
,
new
Text
(
+
sourceFileStatus
.
getPath
(
)
+
+
sourceFileStatus
.
getLen
(
)
+
+
+
target
+
+
(
targetStatus
==
null
?
0
:
targetStatus
.
getLen
(
)
)
)
)
;
}
}
else
{
private
void
handleFailures
(
IOException
exception
,
CopyListingFileStatus
sourceFileStatus
,
Path
target
,
Context
context
)
throws
IOException
,
InterruptedException
{
private
long
doCopy
(
CopyListingFileStatus
source
,
Path
target
,
Mapper
.
Context
context
,
EnumSet
<
FileAttribute
>
fileAttributes
,
FileStatus
sourceStatus
)
throws
IOException
{
private
long
doCopy
(
CopyListingFileStatus
source
,
Path
target
,
Mapper
.
Context
context
,
EnumSet
<
FileAttribute
>
fileAttributes
,
FileStatus
sourceStatus
)
throws
IOException
{
LOG
.
info
(
,
source
.
getPath
(
)
,
target
)
;
final
boolean
toAppend
=
action
==
FileAction
.
APPEND
;
final
boolean
useTempTarget
=
!
toAppend
&&
!
directWrite
;
Path
targetPath
=
useTempTarget
?
getTempFile
(
target
,
context
)
:
target
;
final
boolean
toAppend
=
action
==
FileAction
.
APPEND
;
final
boolean
useTempTarget
=
!
toAppend
&&
!
directWrite
;
Path
targetPath
=
useTempTarget
?
getTempFile
(
target
,
context
)
:
target
;
LOG
.
info
(
,
useTempTarget
?
:
,
targetPath
)
;
final
Configuration
configuration
=
context
.
getConfiguration
(
)
;
FileSystem
targetFS
=
target
.
getFileSystem
(
configuration
)
;
try
{
final
Path
sourcePath
=
source
.
getPath
(
)
;
final
FileSystem
sourceFS
=
sourcePath
.
getFileSystem
(
configuration
)
;
final
FileChecksum
sourceChecksum
=
fileAttributes
.
contains
(
FileAttribute
.
CHECKSUMTYPE
)
?
sourceFS
.
getFileChecksum
(
sourcePath
)
:
null
;
long
offset
=
(
action
==
FileAction
.
APPEND
)
?
targetFS
.
getFileStatus
(
target
)
.
getLen
(
)
:
source
.
getChunkOffset
(
)
;
long
bytesRead
=
copyToFile
(
targetPath
,
targetFS
,
source
,
offset
,
context
,
fileAttributes
,
sourceChecksum
,
sourceStatus
)
;
if
(
!
source
.
isSplit
(
)
)
{
DistCpUtils
.
compareFileLengthsAndChecksums
(
source
.
getLen
(
)
,
sourceFS
,
sourcePath
,
sourceChecksum
,
targetFS
,
targetPath
,
skipCrc
,
source
.
getLen
(
)
)
;
}
if
(
useTempTarget
)
{
private
Path
getTempFile
(
Path
target
,
Mapper
.
Context
context
)
{
Path
targetWorkPath
=
new
Path
(
context
.
getConfiguration
(
)
.
get
(
DistCpConstants
.
CONF_LABEL_TARGET_WORK_PATH
)
)
;
Path
root
=
target
.
equals
(
targetWorkPath
)
?
targetWorkPath
.
getParent
(
)
:
targetWorkPath
;
Path
tempFile
=
new
Path
(
root
,
+
context
.
getTaskAttemptID
(
)
.
toString
(
)
+
+
String
.
valueOf
(
System
.
currentTimeMillis
(
)
)
)
;
private
List
<
InputSplit
>
getSplits
(
Configuration
configuration
,
int
numSplits
,
long
totalSizeBytes
)
throws
IOException
{
List
<
InputSplit
>
splits
=
new
ArrayList
<
InputSplit
>
(
numSplits
)
;
long
nBytesPerSplit
=
(
long
)
Math
.
ceil
(
totalSizeBytes
*
1.0
/
numSplits
)
;
CopyListingFileStatus
srcFileStatus
=
new
CopyListingFileStatus
(
)
;
Text
srcRelPath
=
new
Text
(
)
;
long
currentSplitSize
=
0
;
long
lastSplitStart
=
0
;
long
lastPosition
=
0
;
final
Path
listingFilePath
=
getListingFilePath
(
configuration
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
CopyListingFileStatus
srcFileStatus
=
new
CopyListingFileStatus
(
)
;
Text
srcRelPath
=
new
Text
(
)
;
long
currentSplitSize
=
0
;
long
lastSplitStart
=
0
;
long
lastPosition
=
0
;
final
Path
listingFilePath
=
getListingFilePath
(
configuration
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
nBytesPerSplit
+
+
numSplits
+
+
totalSizeBytes
)
;
}
SequenceFile
.
Reader
reader
=
null
;
try
{
reader
=
getListingFileReader
(
configuration
)
;
while
(
reader
.
next
(
srcRelPath
,
srcFileStatus
)
)
{
if
(
currentSplitSize
+
srcFileStatus
.
getChunkLength
(
)
>
nBytesPerSplit
&&
lastPosition
!=
0
)
{
FileSplit
split
=
new
FileSplit
(
listingFilePath
,
lastSplitStart
,
lastPosition
-
lastSplitStart
,
null
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
SequenceFile
.
Reader
reader
=
null
;
try
{
reader
=
getListingFileReader
(
configuration
)
;
while
(
reader
.
next
(
srcRelPath
,
srcFileStatus
)
)
{
if
(
currentSplitSize
+
srcFileStatus
.
getChunkLength
(
)
>
nBytesPerSplit
&&
lastPosition
!=
0
)
{
FileSplit
split
=
new
FileSplit
(
listingFilePath
,
lastSplitStart
,
lastPosition
-
lastSplitStart
,
null
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
split
+
+
currentSplitSize
)
;
}
splits
.
add
(
split
)
;
lastSplitStart
=
lastPosition
;
currentSplitSize
=
0
;
}
currentSplitSize
+=
srcFileStatus
.
getChunkLength
(
)
;
lastPosition
=
reader
.
getPosition
(
)
;
}
if
(
lastPosition
>
lastSplitStart
)
{
FileSplit
split
=
new
FileSplit
(
listingFilePath
,
lastSplitStart
,
lastPosition
-
lastSplitStart
,
null
)
;
public
void
release
(
)
throws
IOException
{
close
(
)
;
if
(
!
chunkContext
.
getFs
(
)
.
delete
(
chunkFilePath
,
false
)
)
{
public
DynamicInputChunk
acquire
(
TaskAttemptContext
taskAttemptContext
)
throws
IOException
,
InterruptedException
{
String
taskId
=
taskAttemptContext
.
getTaskAttemptID
(
)
.
getTaskID
(
)
.
toString
(
)
;
Path
acquiredFilePath
=
new
Path
(
getChunkRootPath
(
)
,
taskId
)
;
if
(
fs
.
exists
(
acquiredFilePath
)
)
{
@
Override
public
List
<
InputSplit
>
getSplits
(
JobContext
jobContext
)
throws
IOException
,
InterruptedException
{
public
static
LinkedList
<
CopyListingFileStatus
>
toCopyListingFileStatus
(
FileSystem
fileSystem
,
FileStatus
fileStatus
,
boolean
preserveAcls
,
boolean
preserveXAttrs
,
boolean
preserveRawXAttrs
,
int
blocksPerChunk
)
throws
IOException
{
LinkedList
<
CopyListingFileStatus
>
copyListingFileStatus
=
new
LinkedList
<
CopyListingFileStatus
>
(
)
;
final
CopyListingFileStatus
clfs
=
toCopyListingFileStatusHelper
(
fileSystem
,
fileStatus
,
preserveAcls
,
preserveXAttrs
,
preserveRawXAttrs
,
0
,
fileStatus
.
getLen
(
)
)
;
final
long
blockSize
=
fileStatus
.
getBlockSize
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
curPos
=
0
;
if
(
numBlocks
<=
blocksPerChunk
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
clfs
)
;
}
copyListingFileStatus
.
add
(
clfs
)
;
}
else
{
int
i
=
0
;
while
(
i
<
numBlocks
)
{
long
curLength
=
0
;
for
(
int
j
=
0
;
j
<
blocksPerChunk
&&
i
<
numBlocks
;
++
j
,
++
i
)
{
curLength
+=
blockLocations
[
i
]
.
getLength
(
)
;
}
if
(
curLength
>
0
)
{
CopyListingFileStatus
clfs1
=
new
CopyListingFileStatus
(
clfs
)
;
clfs1
.
setChunkOffset
(
curPos
)
;
clfs1
.
setChunkLength
(
curLength
)
;
copyListingFileStatus
.
add
(
clfs
)
;
}
else
{
int
i
=
0
;
while
(
i
<
numBlocks
)
{
long
curLength
=
0
;
for
(
int
j
=
0
;
j
<
blocksPerChunk
&&
i
<
numBlocks
;
++
j
,
++
i
)
{
curLength
+=
blockLocations
[
i
]
.
getLength
(
)
;
}
if
(
curLength
>
0
)
{
CopyListingFileStatus
clfs1
=
new
CopyListingFileStatus
(
clfs
)
;
clfs1
.
setChunkOffset
(
curPos
)
;
clfs1
.
setChunkLength
(
curLength
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
clfs1
)
;
}
copyListingFileStatus
.
add
(
clfs1
)
;
curPos
+=
curLength
;
try
{
validatePaths
(
new
DistCpContext
(
options
)
)
;
Assert
.
fail
(
)
;
}
catch
(
InvalidInputException
ignore
)
{
}
TestDistCpUtils
.
delete
(
fs
,
)
;
srcPaths
.
clear
(
)
;
srcPaths
.
add
(
new
Path
(
)
)
;
fs
.
mkdirs
(
new
Path
(
)
)
;
fs
.
create
(
target
)
.
close
(
)
;
try
{
validatePaths
(
new
DistCpContext
(
options
)
)
;
Assert
.
fail
(
)
;
}
catch
(
InvalidInputException
ignore
)
{
}
TestDistCpUtils
.
delete
(
fs
,
)
;
}
catch
(
IOException
e
)
{
fs
=
FileSystem
.
get
(
getConf
(
)
)
;
List
<
Path
>
srcPaths
=
new
ArrayList
<
Path
>
(
)
;
srcPaths
.
add
(
new
Path
(
)
)
;
TestDistCpUtils
.
createFile
(
fs
,
)
;
TestDistCpUtils
.
createFile
(
fs
,
)
;
Path
target
=
new
Path
(
)
;
Path
listingFile
=
new
Path
(
)
;
final
DistCpOptions
options
=
new
DistCpOptions
.
Builder
(
srcPaths
,
target
)
.
build
(
)
;
final
DistCpContext
context
=
new
DistCpContext
(
options
)
;
CopyListing
listing
=
CopyListing
.
getCopyListing
(
getConf
(
)
,
CREDENTIALS
,
context
)
;
try
{
listing
.
buildListing
(
listingFile
,
context
)
;
Assert
.
fail
(
)
;
}
catch
(
DuplicateFileException
ignore
)
{
}
}
catch
(
IOException
e
)
{
final
DistCpOptions
options
=
new
DistCpOptions
.
Builder
(
srcPaths
,
target
)
.
withSyncFolder
(
true
)
.
build
(
)
;
CopyListing
listing
=
new
SimpleCopyListing
(
getConf
(
)
,
CREDENTIALS
)
;
try
{
listing
.
buildListing
(
listingFile
,
new
DistCpContext
(
options
)
)
;
Assert
.
fail
(
)
;
}
catch
(
DuplicateFileException
ignore
)
{
}
assertThat
(
listing
.
getBytesToCopy
(
)
)
.
isEqualTo
(
10
)
;
assertThat
(
listing
.
getNumberOfPaths
(
)
)
.
isEqualTo
(
3
)
;
TestDistCpUtils
.
delete
(
fs
,
)
;
try
{
listing
.
buildListing
(
listingFile
,
new
DistCpContext
(
options
)
)
;
Assert
.
fail
(
)
;
}
catch
(
InvalidInputException
ignore
)
{
}
TestDistCpUtils
.
delete
(
fs
,
)
;
}
catch
(
IOException
e
)
{
private
void
validateFinalListing
(
Path
pathToListFile
,
List
<
Path
>
srcFiles
)
throws
IOException
{
FileSystem
fs
=
pathToListFile
.
getFileSystem
(
config
)
;
try
(
SequenceFile
.
Reader
reader
=
new
SequenceFile
.
Reader
(
config
,
SequenceFile
.
Reader
.
file
(
pathToListFile
)
)
)
{
CopyListingFileStatus
currentVal
=
new
CopyListingFileStatus
(
)
;
Text
currentKey
=
new
Text
(
)
;
int
idx
=
0
;
while
(
reader
.
next
(
currentKey
)
)
{
reader
.
getCurrentValue
(
currentVal
)
;
Assert
.
assertEquals
(
+
srcFiles
.
size
(
)
+
+
idx
,
fs
.
makeQualified
(
srcFiles
.
get
(
idx
)
)
,
currentVal
.
getPath
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
TestDistCpUtils
.
createFile
(
fs
,
decoyFile
.
toString
(
)
)
;
TestDistCpUtils
.
createFile
(
fs
,
targetFile
.
toString
(
)
)
;
List
<
Path
>
srcPaths
=
new
ArrayList
<
Path
>
(
)
;
srcPaths
.
add
(
sourceFile
)
;
DistCpOptions
options
=
new
DistCpOptions
.
Builder
(
srcPaths
,
targetFile
)
.
build
(
)
;
CopyListing
listing
=
new
SimpleCopyListing
(
getConf
(
)
,
CREDENTIALS
)
;
final
Path
listFile
=
new
Path
(
testRoot
,
)
;
listing
.
buildListing
(
listFile
,
new
DistCpContext
(
options
)
)
;
reader
=
new
SequenceFile
.
Reader
(
getConf
(
)
,
SequenceFile
.
Reader
.
file
(
listFile
)
)
;
CopyListingFileStatus
fileStatus
=
new
CopyListingFileStatus
(
)
;
Text
relativePath
=
new
Text
(
)
;
Assert
.
assertTrue
(
reader
.
next
(
relativePath
,
fileStatus
)
)
;
Assert
.
assertTrue
(
relativePath
.
toString
(
)
.
equals
(
)
)
;
}
catch
(
Exception
e
)
{
Assert
.
fail
(
)
;
private
void
compareFiles
(
FileSystem
fs
,
FileStatus
srcStat
,
FileStatus
dstStat
)
throws
Exception
{
FSDataInputStream
srcIn
=
fs
.
open
(
srcStat
.
getPath
(
)
)
;
FSDataInputStream
dstIn
=
fs
.
open
(
dstStat
.
getPath
(
)
)
;
try
{
byte
[
]
readSrc
=
new
byte
[
(
int
)
HdfsClientConfigKeys
.
DFS_BLOCK_SIZE_DEFAULT
]
;
byte
[
]
readDst
=
new
byte
[
(
int
)
HdfsClientConfigKeys
.
DFS_BLOCK_SIZE_DEFAULT
]
;
int
srcBytesRead
=
0
,
tgtBytesRead
=
0
;
int
srcIdx
=
0
,
tgtIdx
=
0
;
long
totalComparedBytes
=
0
;
while
(
true
)
{
if
(
srcBytesRead
==
0
)
{
srcBytesRead
=
srcIn
.
read
(
readSrc
)
;
srcIdx
=
0
;
}
if
(
tgtBytesRead
==
0
)
{
tgtBytesRead
=
dstIn
.
read
(
readDst
)
;
tgtIdx
=
0
;
}
if
(
tgtBytesRead
==
0
)
{
tgtBytesRead
=
dstIn
.
read
(
readDst
)
;
tgtIdx
=
0
;
}
if
(
srcBytesRead
==
0
||
tgtBytesRead
==
0
)
{
LOG
.
info
(
+
totalComparedBytes
+
)
;
if
(
srcBytesRead
!=
tgtBytesRead
)
{
Assert
.
fail
(
+
totalComparedBytes
+
+
srcStat
+
+
dstStat
)
;
}
if
(
totalComparedBytes
!=
srcStat
.
getLen
(
)
)
{
Assert
.
fail
(
+
totalComparedBytes
+
+
srcStat
+
+
dstStat
)
;
}
else
{
break
;
}
}
for
(
;
srcIdx
<
srcBytesRead
&&
tgtIdx
<
tgtBytesRead
;
++
srcIdx
,
++
tgtIdx
)
{
if
(
readSrc
[
srcIdx
]
!=
readDst
[
tgtIdx
]
)
{
Assert
.
fail
(
+
totalComparedBytes
+
+
srcStat
+
+
dstStat
)
;
}
++
totalComparedBytes
;
if
(
tgtBytesRead
==
0
)
{
tgtBytesRead
=
dstIn
.
read
(
readDst
)
;
tgtIdx
=
0
;
}
if
(
srcBytesRead
==
0
||
tgtBytesRead
==
0
)
{
LOG
.
info
(
+
totalComparedBytes
+
)
;
if
(
srcBytesRead
!=
tgtBytesRead
)
{
Assert
.
fail
(
+
totalComparedBytes
+
+
srcStat
+
+
dstStat
)
;
}
if
(
totalComparedBytes
!=
srcStat
.
getLen
(
)
)
{
Assert
.
fail
(
+
totalComparedBytes
+
+
srcStat
+
+
dstStat
)
;
}
else
{
break
;
}
}
for
(
;
srcIdx
<
srcBytesRead
&&
tgtIdx
<
tgtBytesRead
;
++
srcIdx
,
++
tgtIdx
)
{
if
(
readSrc
[
srcIdx
]
!=
readDst
[
tgtIdx
]
)
{
Assert
.
fail
(
+
totalComparedBytes
+
+
srcStat
+
+
dstStat
)
;
}
++
totalComparedBytes
;
private
void
copyAndVerify
(
final
DistributedFileSystem
fs
,
final
FileEntry
[
]
srcFiles
,
final
FileStatus
[
]
srcStats
,
final
String
testDst
,
final
String
[
]
args
)
throws
Exception
{
final
String
testRoot
=
;
FsShell
shell
=
new
FsShell
(
fs
.
getConf
(
)
)
;
LOG
.
info
(
)
;
private
void
copyAndVerify
(
final
DistributedFileSystem
fs
,
final
FileEntry
[
]
srcFiles
,
final
FileStatus
[
]
srcStats
,
final
String
testDst
,
final
String
[
]
args
)
throws
Exception
{
final
String
testRoot
=
;
FsShell
shell
=
new
FsShell
(
fs
.
getConf
(
)
)
;
LOG
.
info
(
)
;
LOG
.
info
(
execCmd
(
shell
,
,
testRoot
)
)
;
LOG
.
info
(
+
args
[
0
]
+
+
args
[
1
]
)
;
ToolRunner
.
run
(
conf
,
new
DistCp
(
)
,
args
)
;
LOG
.
info
(
)
;
byte
[
]
contents2
=
.
getBytes
(
)
;
Assert
.
assertEquals
(
contents1
.
length
,
contents2
.
length
)
;
try
{
addEntries
(
listFile
,
)
;
createWithContents
(
,
contents1
)
;
createWithContents
(
,
contents2
)
;
Path
target
=
new
Path
(
root
+
)
;
runTest
(
listFile
,
target
,
false
,
false
,
false
,
true
)
;
checkResult
(
target
,
1
,
)
;
FSDataInputStream
is
=
fs
.
open
(
new
Path
(
root
+
)
)
;
byte
[
]
dstContents
=
new
byte
[
contents1
.
length
]
;
is
.
readFully
(
dstContents
)
;
is
.
close
(
)
;
Assert
.
assertArrayEquals
(
contents1
,
dstContents
)
;
}
catch
(
IOException
e
)
{
protected
Job
distCpUpdateDeepDirectoryStructure
(
final
Path
destDir
)
throws
Exception
{
describe
(
)
;
Path
srcDir
=
inputDir
;
ContractTestUtils
.
assertIsFile
(
remoteFS
,
outputFileNew1
)
;
ContractTestUtils
.
assertPathExists
(
localFS
,
,
trackDir
)
;
Path
sortedSourceListing
=
new
Path
(
trackDir
,
DistCpConstants
.
SOURCE_SORTED_FILE
)
;
ContractTestUtils
.
assertIsFile
(
localFS
,
sortedSourceListing
)
;
Path
sortedTargetListing
=
new
Path
(
trackDir
,
DistCpConstants
.
TARGET_SORTED_FILE
)
;
ContractTestUtils
.
assertIsFile
(
localFS
,
sortedTargetListing
)
;
ContractTestUtils
.
assertPathsExist
(
remoteFS
,
,
outputFile2
,
outputFile3
,
outputFile4
,
outputSubDir4
)
;
Map
<
String
,
Path
>
sourceFiles
=
new
HashMap
<
>
(
10
)
;
Map
<
String
,
Path
>
targetFiles
=
new
HashMap
<
>
(
10
)
;
try
(
SequenceFile
.
Reader
sourceReader
=
new
SequenceFile
.
Reader
(
conf
,
SequenceFile
.
Reader
.
file
(
sortedSourceListing
)
)
;
SequenceFile
.
Reader
targetReader
=
new
SequenceFile
.
Reader
(
conf
,
SequenceFile
.
Reader
.
file
(
sortedTargetListing
)
)
)
{
CopyListingFileStatus
copyStatus
=
new
CopyListingFileStatus
(
)
;
Text
name
=
new
Text
(
)
;
while
(
sourceReader
.
next
(
name
,
copyStatus
)
)
{
String
key
=
name
.
toString
(
)
;
Path
path
=
copyStatus
.
getPath
(
)
;
ContractTestUtils
.
assertIsFile
(
localFS
,
sortedTargetListing
)
;
ContractTestUtils
.
assertPathsExist
(
remoteFS
,
,
outputFile2
,
outputFile3
,
outputFile4
,
outputSubDir4
)
;
Map
<
String
,
Path
>
sourceFiles
=
new
HashMap
<
>
(
10
)
;
Map
<
String
,
Path
>
targetFiles
=
new
HashMap
<
>
(
10
)
;
try
(
SequenceFile
.
Reader
sourceReader
=
new
SequenceFile
.
Reader
(
conf
,
SequenceFile
.
Reader
.
file
(
sortedSourceListing
)
)
;
SequenceFile
.
Reader
targetReader
=
new
SequenceFile
.
Reader
(
conf
,
SequenceFile
.
Reader
.
file
(
sortedTargetListing
)
)
)
{
CopyListingFileStatus
copyStatus
=
new
CopyListingFileStatus
(
)
;
Text
name
=
new
Text
(
)
;
while
(
sourceReader
.
next
(
name
,
copyStatus
)
)
{
String
key
=
name
.
toString
(
)
;
Path
path
=
copyStatus
.
getPath
(
)
;
LOG
.
info
(
,
key
,
path
)
;
sourceFiles
.
put
(
key
,
path
)
;
}
while
(
targetReader
.
next
(
name
,
copyStatus
)
)
{
String
key
=
name
.
toString
(
)
;
Path
path
=
copyStatus
.
getPath
(
)
;
public
void
lsR
(
final
String
description
,
final
FileSystem
fs
,
final
Path
dir
)
throws
IOException
{
RemoteIterator
<
LocatedFileStatus
>
files
=
fs
.
listFiles
(
dir
,
true
)
;
CopyListing
listing
=
new
GlobbedCopyListing
(
conf
,
CREDENTIALS
)
;
Path
listingFile
=
new
Path
(
+
String
.
valueOf
(
rand
.
nextLong
(
)
)
)
;
listing
.
buildListing
(
listingFile
,
context
)
;
conf
.
set
(
CONF_LABEL_TARGET_WORK_PATH
,
targetBase
)
;
conf
.
set
(
CONF_LABEL_TARGET_FINAL_PATH
,
targetBase
)
;
OutputCommitter
committer
=
new
CopyCommitter
(
null
,
taskAttemptContext
)
;
try
{
committer
.
commitJob
(
jobContext
)
;
if
(
!
skipCrc
)
{
Assert
.
fail
(
)
;
}
Path
sourcePath
=
new
Path
(
sourceBase
+
srcFilename
)
;
CopyListingFileStatus
sourceCurrStatus
=
new
CopyListingFileStatus
(
fs
.
getFileStatus
(
sourcePath
)
)
;
Assert
.
assertFalse
(
DistCpUtils
.
checksumsAreEqual
(
fs
,
new
Path
(
sourceBase
+
srcFilename
)
,
null
,
fs
,
new
Path
(
targetBase
+
srcFilename
)
,
sourceCurrStatus
.
getLen
(
)
)
)
;
}
catch
(
IOException
exception
)
{
if
(
skipCrc
)
{
protected
static
Configuration
getConfigurationForCluster
(
)
throws
IOException
{
Configuration
configuration
=
new
Configuration
(
)
;
System
.
setProperty
(
,
)
;
configuration
.
set
(
,
)
;
configuration
.
set
(
,
)
;
protected
static
Configuration
getConfigurationForCluster
(
)
throws
IOException
{
Configuration
configuration
=
new
Configuration
(
)
;
System
.
setProperty
(
,
)
;
configuration
.
set
(
,
)
;
configuration
.
set
(
,
)
;
LOG
.
debug
(
+
configuration
.
get
(
)
)
;
copyMapper
.
setup
(
context
)
;
final
Path
path
=
new
Path
(
SOURCE_PATH
+
)
;
int
manyBytes
=
100000000
;
appendFile
(
path
,
manyBytes
)
;
ScheduledExecutorService
scheduledExecutorService
=
Executors
.
newSingleThreadScheduledExecutor
(
)
;
Runnable
task
=
new
Runnable
(
)
{
public
void
run
(
)
{
try
{
int
maxAppendAttempts
=
20
;
int
appendCount
=
0
;
while
(
appendCount
<
maxAppendAttempts
)
{
appendFile
(
path
,
1000
)
;
Thread
.
sleep
(
200
)
;
appendCount
++
;
}
}
catch
(
IOException
|
InterruptedException
e
)
{
try
{
int
maxAppendAttempts
=
20
;
int
appendCount
=
0
;
while
(
appendCount
<
maxAppendAttempts
)
{
appendFile
(
path
,
1000
)
;
Thread
.
sleep
(
200
)
;
appendCount
++
;
}
}
catch
(
IOException
|
InterruptedException
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
+
e
.
getMessage
(
)
)
;
}
}
}
;
scheduledExecutorService
.
schedule
(
task
,
10
,
TimeUnit
.
MILLISECONDS
)
;
try
{
copyMapper
.
map
(
new
Text
(
DistCpUtils
.
getRelativePath
(
new
Path
(
SOURCE_PATH
)
,
path
)
)
,
new
CopyListingFileStatus
(
cluster
.
getFileSystem
(
)
.
getFileStatus
(
path
)
)
,
context
)
;
}
catch
(
Exception
ex
)
{
try
{
deleteState
(
)
;
createSourceData
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
CopyMapper
copyMapper
=
new
CopyMapper
(
)
;
StubContext
stubContext
=
new
StubContext
(
getConfiguration
(
)
,
null
,
0
)
;
Mapper
<
Text
,
CopyListingFileStatus
,
Text
,
Text
>
.
Context
context
=
stubContext
.
getContext
(
)
;
mkdirs
(
SOURCE_PATH
+
)
;
touchFile
(
TARGET_PATH
+
)
;
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
fs
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
}
catch
(
IOException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
}
catch
(
Exception
e
)
{
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
EnumSet
<
DistCpOptions
.
FileAttribute
>
preserveStatus
=
EnumSet
.
allOf
(
DistCpOptions
.
FileAttribute
.
class
)
;
preserveStatus
.
remove
(
DistCpOptions
.
FileAttribute
.
ACL
)
;
preserveStatus
.
remove
(
DistCpOptions
.
FileAttribute
.
XATTR
)
;
context
.
getConfiguration
(
)
.
set
(
DistCpConstants
.
CONF_LABEL_PRESERVE_STATUS
,
DistCpUtils
.
packAttributes
(
preserveStatus
)
)
;
touchFile
(
SOURCE_PATH
+
)
;
mkdirs
(
TARGET_PATH
)
;
cluster
.
getFileSystem
(
)
.
setPermission
(
new
Path
(
TARGET_PATH
)
,
new
FsPermission
(
(
short
)
511
)
)
;
final
FileSystem
tmpFS
=
tmpUser
.
doAs
(
new
PrivilegedAction
<
FileSystem
>
(
)
{
@
Override
public
FileSystem
run
(
)
{
try
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
+
e
.
getMessage
(
)
)
;
throw
new
RuntimeException
(
)
;
}
}
}
)
;
tmpUser
.
doAs
(
new
PrivilegedAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
{
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
tmpFS
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
Assert
.
fail
(
)
;
}
catch
(
AccessControlException
e
)
{
Assert
.
assertTrue
(
+
e
.
getMessage
(
)
,
true
)
;
}
catch
(
Exception
e
)
{
StubContext
stubContext
=
new
StubContext
(
getConfiguration
(
)
,
null
,
0
)
;
return
stubContext
.
getContext
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
touchFile
(
SOURCE_PATH
+
)
;
mkdirs
(
TARGET_PATH
)
;
cluster
.
getFileSystem
(
)
.
setPermission
(
new
Path
(
SOURCE_PATH
+
)
,
new
FsPermission
(
FsAction
.
READ
,
FsAction
.
READ
,
FsAction
.
READ
)
)
;
cluster
.
getFileSystem
(
)
.
setPermission
(
new
Path
(
TARGET_PATH
)
,
new
FsPermission
(
(
short
)
511
)
)
;
final
FileSystem
tmpFS
=
tmpUser
.
doAs
(
new
PrivilegedAction
<
FileSystem
>
(
)
{
@
Override
public
FileSystem
run
(
)
{
try
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
final
FileSystem
tmpFS
=
tmpUser
.
doAs
(
new
PrivilegedAction
<
FileSystem
>
(
)
{
@
Override
public
FileSystem
run
(
)
{
try
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
+
e
.
getMessage
(
)
)
;
throw
new
RuntimeException
(
)
;
}
}
}
)
;
tmpUser
.
doAs
(
new
PrivilegedAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
{
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
tmpFS
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
}
catch
(
Exception
e
)
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
+
e
.
getMessage
(
)
)
;
throw
new
RuntimeException
(
)
;
}
}
}
)
;
tmpUser
.
doAs
(
new
PrivilegedAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
{
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
tmpFS
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
assertThat
(
stubContext
.
getWriter
(
)
.
values
(
)
.
size
(
)
)
.
isEqualTo
(
1
)
;
Assert
.
assertTrue
(
stubContext
.
getWriter
(
)
.
values
(
)
.
get
(
0
)
.
toString
(
)
.
startsWith
(
)
)
;
Assert
.
assertTrue
(
stubContext
.
getWriter
(
)
.
values
(
)
.
get
(
0
)
.
toString
(
)
.
contains
(
SOURCE_PATH
+
)
)
;
}
catch
(
Exception
e
)
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
fail
(
+
e
.
getMessage
(
)
)
;
throw
new
RuntimeException
(
)
;
}
}
}
)
;
tmpUser
.
doAs
(
new
PrivilegedAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
{
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
tmpFS
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
Assert
.
fail
(
)
;
}
catch
(
AccessControlException
ignore
)
{
}
catch
(
Exception
e
)
{
if
(
e
.
getCause
(
)
==
null
||
e
.
getCause
(
)
.
getCause
(
)
==
null
||
!
(
e
.
getCause
(
)
.
getCause
(
)
instanceof
AccessControlException
)
)
{
try
{
deleteState
(
)
;
createSourceData
(
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
CopyMapper
copyMapper
=
new
CopyMapper
(
)
;
StubContext
stubContext
=
new
StubContext
(
getConfiguration
(
)
,
null
,
0
)
;
Mapper
<
Text
,
CopyListingFileStatus
,
Text
,
Text
>
.
Context
context
=
stubContext
.
getContext
(
)
;
touchFile
(
SOURCE_PATH
+
)
;
mkdirs
(
TARGET_PATH
+
)
;
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
fs
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
}
catch
(
IOException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
}
catch
(
Exception
e
)
{
return
stubContext
.
getContext
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
touchFile
(
SOURCE_PATH
+
)
;
mkdirs
(
TARGET_PATH
)
;
cluster
.
getFileSystem
(
)
.
setPermission
(
new
Path
(
SOURCE_PATH
+
)
,
new
FsPermission
(
FsAction
.
NONE
,
FsAction
.
NONE
,
FsAction
.
NONE
)
)
;
cluster
.
getFileSystem
(
)
.
setPermission
(
new
Path
(
TARGET_PATH
)
,
new
FsPermission
(
(
short
)
511
)
)
;
context
.
getConfiguration
(
)
.
setBoolean
(
DistCpOptionSwitch
.
IGNORE_FAILURES
.
getConfigLabel
(
)
,
ignoreFailures
)
;
final
FileSystem
tmpFS
=
tmpUser
.
doAs
(
new
PrivilegedAction
<
FileSystem
>
(
)
{
@
Override
public
FileSystem
run
(
)
{
try
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
mkdirs
(
TARGET_PATH
)
;
cluster
.
getFileSystem
(
)
.
setPermission
(
new
Path
(
SOURCE_PATH
+
)
,
new
FsPermission
(
FsAction
.
NONE
,
FsAction
.
NONE
,
FsAction
.
NONE
)
)
;
cluster
.
getFileSystem
(
)
.
setPermission
(
new
Path
(
TARGET_PATH
)
,
new
FsPermission
(
(
short
)
511
)
)
;
context
.
getConfiguration
(
)
.
setBoolean
(
DistCpOptionSwitch
.
IGNORE_FAILURES
.
getConfigLabel
(
)
,
ignoreFailures
)
;
final
FileSystem
tmpFS
=
tmpUser
.
doAs
(
new
PrivilegedAction
<
FileSystem
>
(
)
{
@
Override
public
FileSystem
run
(
)
{
try
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
tmpUser
.
doAs
(
new
PrivilegedAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
{
try
{
context
.
getConfiguration
(
)
.
setBoolean
(
DistCpOptionSwitch
.
IGNORE_FAILURES
.
getConfigLabel
(
)
,
ignoreFailures
)
;
final
FileSystem
tmpFS
=
tmpUser
.
doAs
(
new
PrivilegedAction
<
FileSystem
>
(
)
{
@
Override
public
FileSystem
run
(
)
{
try
{
return
FileSystem
.
get
(
cluster
.
getConfiguration
(
0
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
tmpUser
.
doAs
(
new
PrivilegedAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
{
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
tmpFS
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
Assert
.
assertTrue
(
+
,
ignoreFailures
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
RuntimeException
(
e
)
;
}
}
}
)
;
tmpUser
.
doAs
(
new
PrivilegedAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
{
try
{
copyMapper
.
setup
(
context
)
;
copyMapper
.
map
(
new
Text
(
)
,
new
CopyListingFileStatus
(
tmpFS
.
getFileStatus
(
new
Path
(
SOURCE_PATH
+
)
)
)
,
context
)
;
Assert
.
assertTrue
(
+
,
ignoreFailures
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
Assert
.
assertFalse
(
+
,
ignoreFailures
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
}
job
.
getConfiguration
(
)
.
set
(
DistCpConstants
.
CONF_LABEL_TARGET_WORK_PATH
,
)
;
CopyOutputFormat
.
setCommitDirectory
(
job
,
new
Path
(
)
)
;
try
{
JobContext
context
=
new
JobContextImpl
(
job
.
getConfiguration
(
)
,
jobID
)
;
outputFormat
.
checkOutputSpecs
(
context
)
;
Assert
.
fail
(
)
;
}
catch
(
IllegalStateException
ignore
)
{
}
CopyOutputFormat
.
setWorkingDirectory
(
job
,
new
Path
(
)
)
;
CopyOutputFormat
.
setCommitDirectory
(
job
,
new
Path
(
)
)
;
try
{
JobContext
context
=
new
JobContextImpl
(
job
.
getConfiguration
(
)
,
jobID
)
;
outputFormat
.
checkOutputSpecs
(
context
)
;
}
catch
(
IllegalStateException
ignore
)
{
Assert
.
fail
(
)
;
}
}
catch
(
IOException
e
)
{
protected
void
deletePaths
(
final
List
<
CopyListingFileStatus
>
statusList
,
final
AtomicInteger
deletedFiles
,
final
AtomicInteger
deletedDirs
)
{
for
(
CopyListingFileStatus
status
:
statusList
)
{
if
(
shouldDelete
(
status
)
)
{
AtomicInteger
r
=
status
.
isDirectory
(
)
?
deletedDirs
:
deletedFiles
;
r
.
incrementAndGet
(
)
;
private
static
Configuration
getConfigurationForCluster
(
)
{
Configuration
configuration
=
new
Configuration
(
)
;
System
.
setProperty
(
,
)
;
configuration
.
set
(
,
)
;
private
static
Configuration
getConfigurationForCluster
(
)
{
Configuration
configuration
=
new
Configuration
(
)
;
System
.
setProperty
(
,
)
;
configuration
.
set
(
,
)
;
LOG
.
debug
(
+
configuration
.
get
(
)
)
;
allTokens
=
ByteBuffer
.
wrap
(
dob
.
getData
(
)
,
0
,
dob
.
getLength
(
)
)
;
AMRMClientAsync
.
AbstractCallbackHandler
allocListener
=
new
RMCallbackHandler
(
)
;
amRMClient
=
AMRMClientAsync
.
createAMRMClientAsync
(
1000
,
allocListener
)
;
amRMClient
.
init
(
conf
)
;
amRMClient
.
start
(
)
;
containerListener
=
createNMCallbackHandler
(
)
;
nmClientAsync
=
new
NMClientAsyncImpl
(
containerListener
)
;
nmClientAsync
.
init
(
conf
)
;
nmClientAsync
.
start
(
)
;
String
appMasterHostname
=
NetUtils
.
getHostname
(
)
;
amRMClient
.
registerApplicationMaster
(
appMasterHostname
,
-
1
,
)
;
Supplier
<
Boolean
>
exitCritera
=
this
::
isComplete
;
Optional
<
Properties
>
namenodeProperties
=
Optional
.
empty
(
)
;
if
(
launchNameNode
)
{
ContainerRequest
nnContainerRequest
=
setupContainerAskForRM
(
amOptions
.
getNameNodeMemoryMB
(
)
,
amOptions
.
getNameNodeVirtualCores
(
)
,
0
,
amOptions
.
getNameNodeNodeLabelExpression
(
)
)
;
amRMClient
.
init
(
conf
)
;
amRMClient
.
start
(
)
;
containerListener
=
createNMCallbackHandler
(
)
;
nmClientAsync
=
new
NMClientAsyncImpl
(
containerListener
)
;
nmClientAsync
.
init
(
conf
)
;
nmClientAsync
.
start
(
)
;
String
appMasterHostname
=
NetUtils
.
getHostname
(
)
;
amRMClient
.
registerApplicationMaster
(
appMasterHostname
,
-
1
,
)
;
Supplier
<
Boolean
>
exitCritera
=
this
::
isComplete
;
Optional
<
Properties
>
namenodeProperties
=
Optional
.
empty
(
)
;
if
(
launchNameNode
)
{
ContainerRequest
nnContainerRequest
=
setupContainerAskForRM
(
amOptions
.
getNameNodeMemoryMB
(
)
,
amOptions
.
getNameNodeVirtualCores
(
)
,
0
,
amOptions
.
getNameNodeNodeLabelExpression
(
)
)
;
LOG
.
info
(
+
nnContainerRequest
.
toString
(
)
)
;
amRMClient
.
addContainerRequest
(
nnContainerRequest
)
;
Path
namenodeInfoPath
=
new
Path
(
remoteStoragePath
,
DynoConstants
.
NN_INFO_FILE_NAME
)
;
String
appMasterHostname
=
NetUtils
.
getHostname
(
)
;
amRMClient
.
registerApplicationMaster
(
appMasterHostname
,
-
1
,
)
;
Supplier
<
Boolean
>
exitCritera
=
this
::
isComplete
;
Optional
<
Properties
>
namenodeProperties
=
Optional
.
empty
(
)
;
if
(
launchNameNode
)
{
ContainerRequest
nnContainerRequest
=
setupContainerAskForRM
(
amOptions
.
getNameNodeMemoryMB
(
)
,
amOptions
.
getNameNodeVirtualCores
(
)
,
0
,
amOptions
.
getNameNodeNodeLabelExpression
(
)
)
;
LOG
.
info
(
+
nnContainerRequest
.
toString
(
)
)
;
amRMClient
.
addContainerRequest
(
nnContainerRequest
)
;
Path
namenodeInfoPath
=
new
Path
(
remoteStoragePath
,
DynoConstants
.
NN_INFO_FILE_NAME
)
;
LOG
.
info
(
+
namenodeInfoPath
)
;
namenodeProperties
=
DynoInfraUtils
.
waitForAndGetNameNodeProperties
(
exitCritera
,
conf
,
namenodeInfoPath
,
LOG
)
;
if
(
!
namenodeProperties
.
isPresent
(
)
)
{
cleanup
(
)
;
return
false
;
}
namenodeServiceRpcAddress
=
DynoInfraUtils
.
getNameNodeServiceRpcAddr
(
namenodeProperties
.
get
(
)
)
.
toString
(
)
;
if
(
launchNameNode
)
{
ContainerRequest
nnContainerRequest
=
setupContainerAskForRM
(
amOptions
.
getNameNodeMemoryMB
(
)
,
amOptions
.
getNameNodeVirtualCores
(
)
,
0
,
amOptions
.
getNameNodeNodeLabelExpression
(
)
)
;
LOG
.
info
(
+
nnContainerRequest
.
toString
(
)
)
;
amRMClient
.
addContainerRequest
(
nnContainerRequest
)
;
Path
namenodeInfoPath
=
new
Path
(
remoteStoragePath
,
DynoConstants
.
NN_INFO_FILE_NAME
)
;
LOG
.
info
(
+
namenodeInfoPath
)
;
namenodeProperties
=
DynoInfraUtils
.
waitForAndGetNameNodeProperties
(
exitCritera
,
conf
,
namenodeInfoPath
,
LOG
)
;
if
(
!
namenodeProperties
.
isPresent
(
)
)
{
cleanup
(
)
;
return
false
;
}
namenodeServiceRpcAddress
=
DynoInfraUtils
.
getNameNodeServiceRpcAddr
(
namenodeProperties
.
get
(
)
)
.
toString
(
)
;
LOG
.
info
(
+
namenodeProperties
.
get
(
)
)
;
LOG
.
info
(
+
DynoInfraUtils
.
getNameNodeHdfsUri
(
namenodeProperties
.
get
(
)
)
.
toString
(
)
)
;
DynoInfraUtils
.
waitForNameNodeStartup
(
namenodeProperties
.
get
(
)
,
exitCritera
,
LOG
)
;
}
else
{
cleanup
(
)
;
return
false
;
}
namenodeServiceRpcAddress
=
DynoInfraUtils
.
getNameNodeServiceRpcAddr
(
namenodeProperties
.
get
(
)
)
.
toString
(
)
;
LOG
.
info
(
+
namenodeProperties
.
get
(
)
)
;
LOG
.
info
(
+
DynoInfraUtils
.
getNameNodeHdfsUri
(
namenodeProperties
.
get
(
)
)
.
toString
(
)
)
;
DynoInfraUtils
.
waitForNameNodeStartup
(
namenodeProperties
.
get
(
)
,
exitCritera
,
LOG
)
;
}
else
{
LOG
.
info
(
+
namenodeServiceRpcAddress
)
;
}
blockListFiles
=
Collections
.
synchronizedList
(
getDataNodeBlockListingFiles
(
)
)
;
numTotalDataNodes
=
blockListFiles
.
size
(
)
;
if
(
numTotalDataNodes
==
0
)
{
LOG
.
error
(
)
;
markCompleted
(
)
;
return
false
;
}
numTotalDataNodeContainers
=
(
int
)
Math
.
ceil
(
(
(
double
)
numTotalDataNodes
)
/
Math
.
max
(
1
,
amOptions
.
getDataNodesPerCluster
(
)
)
)
;
LOG
.
info
(
+
namenodeProperties
.
get
(
)
)
;
LOG
.
info
(
+
DynoInfraUtils
.
getNameNodeHdfsUri
(
namenodeProperties
.
get
(
)
)
.
toString
(
)
)
;
DynoInfraUtils
.
waitForNameNodeStartup
(
namenodeProperties
.
get
(
)
,
exitCritera
,
LOG
)
;
}
else
{
LOG
.
info
(
+
namenodeServiceRpcAddress
)
;
}
blockListFiles
=
Collections
.
synchronizedList
(
getDataNodeBlockListingFiles
(
)
)
;
numTotalDataNodes
=
blockListFiles
.
size
(
)
;
if
(
numTotalDataNodes
==
0
)
{
LOG
.
error
(
)
;
markCompleted
(
)
;
return
false
;
}
numTotalDataNodeContainers
=
(
int
)
Math
.
ceil
(
(
(
double
)
numTotalDataNodes
)
/
Math
.
max
(
1
,
amOptions
.
getDataNodesPerCluster
(
)
)
)
;
LOG
.
info
(
,
numTotalDataNodeContainers
,
amOptions
.
getDataNodeMemoryMB
(
)
,
amOptions
.
getDataNodeVirtualCores
(
)
)
;
for
(
int
i
=
0
;
i
<
numTotalDataNodeContainers
;
++
i
)
{
ContainerRequest
datanodeAsk
=
setupContainerAskForRM
(
amOptions
.
getDataNodeMemoryMB
(
)
,
amOptions
.
getDataNodeVirtualCores
(
)
,
1
,
amOptions
.
getDataNodeNodeLabelExpression
(
)
)
;
private
List
<
LocalResource
>
getDataNodeBlockListingFiles
(
)
throws
IOException
{
Path
blockListDirPath
=
new
Path
(
System
.
getenv
(
)
.
get
(
DynoConstants
.
BLOCK_LIST_PATH_ENV
)
)
;
public
boolean
run
(
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
yarnClient
.
start
(
)
;
YarnClusterMetrics
clusterMetrics
=
yarnClient
.
getYarnClusterMetrics
(
)
;
public
boolean
run
(
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
yarnClient
.
start
(
)
;
YarnClusterMetrics
clusterMetrics
=
yarnClient
.
getYarnClusterMetrics
(
)
;
LOG
.
info
(
,
clusterMetrics
.
getNumNodeManagers
(
)
)
;
QueueInfo
queueInfo
=
yarnClient
.
getQueueInfo
(
this
.
amQueue
)
;
public
boolean
run
(
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
yarnClient
.
start
(
)
;
YarnClusterMetrics
clusterMetrics
=
yarnClient
.
getYarnClusterMetrics
(
)
;
LOG
.
info
(
,
clusterMetrics
.
getNumNodeManagers
(
)
)
;
QueueInfo
queueInfo
=
yarnClient
.
getQueueInfo
(
this
.
amQueue
)
;
LOG
.
info
(
+
+
,
queueInfo
.
getQueueName
(
)
,
queueInfo
.
getCurrentCapacity
(
)
,
queueInfo
.
getMaximumCapacity
(
)
,
queueInfo
.
getApplications
(
)
.
size
(
)
,
queueInfo
.
getChildQueues
(
)
.
size
(
)
)
;
YarnClientApplication
app
=
yarnClient
.
createApplication
(
)
;
GetNewApplicationResponse
appResponse
=
app
.
getNewApplicationResponse
(
)
;
long
maxMem
=
appResponse
.
getMaximumResourceCapability
(
)
.
getMemorySize
(
)
;
public
boolean
run
(
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
yarnClient
.
start
(
)
;
YarnClusterMetrics
clusterMetrics
=
yarnClient
.
getYarnClusterMetrics
(
)
;
LOG
.
info
(
,
clusterMetrics
.
getNumNodeManagers
(
)
)
;
QueueInfo
queueInfo
=
yarnClient
.
getQueueInfo
(
this
.
amQueue
)
;
LOG
.
info
(
+
+
,
queueInfo
.
getQueueName
(
)
,
queueInfo
.
getCurrentCapacity
(
)
,
queueInfo
.
getMaximumCapacity
(
)
,
queueInfo
.
getApplications
(
)
.
size
(
)
,
queueInfo
.
getChildQueues
(
)
.
size
(
)
)
;
YarnClientApplication
app
=
yarnClient
.
createApplication
(
)
;
GetNewApplicationResponse
appResponse
=
app
.
getNewApplicationResponse
(
)
;
long
maxMem
=
appResponse
.
getMaximumResourceCapability
(
)
.
getMemorySize
(
)
;
LOG
.
info
(
+
maxMem
)
;
int
maxVCores
=
appResponse
.
getMaximumResourceCapability
(
)
.
getVirtualCores
(
)
;
Resource
capability
=
Records
.
newRecord
(
Resource
.
class
)
;
capability
.
setMemorySize
(
amMemory
)
;
capability
.
setVirtualCores
(
amVCores
)
;
appContext
.
setResource
(
capability
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
ByteBuffer
fsTokens
;
if
(
tokenFileLocation
!=
null
)
{
fsTokens
=
ByteBuffer
.
wrap
(
Files
.
readAllBytes
(
Paths
.
get
(
tokenFileLocation
)
)
)
;
}
else
{
Credentials
credentials
=
new
Credentials
(
)
;
String
tokenRenewer
=
getConf
(
)
.
get
(
YarnConfiguration
.
RM_PRINCIPAL
)
;
if
(
tokenRenewer
==
null
||
tokenRenewer
.
length
(
)
==
0
)
{
throw
new
IOException
(
+
)
;
}
final
Token
<
?
>
[
]
tokens
=
fs
.
addDelegationTokens
(
tokenRenewer
,
credentials
)
;
if
(
tokens
!=
null
)
{
private
List
<
String
>
getAMCommand
(
)
{
List
<
String
>
vargs
=
new
ArrayList
<
>
(
)
;
vargs
.
add
(
Environment
.
JAVA_HOME
.
$
(
)
+
)
;
long
appMasterHeapSize
=
Math
.
round
(
amMemory
*
0.85
)
;
vargs
.
add
(
+
appMasterHeapSize
+
)
;
vargs
.
add
(
ApplicationMaster
.
class
.
getCanonicalName
(
)
)
;
amOptions
.
addToVargs
(
vargs
)
;
vargs
.
add
(
+
ApplicationConstants
.
LOG_DIR_EXPANSION_VAR
+
)
;
vargs
.
add
(
+
ApplicationConstants
.
LOG_DIR_EXPANSION_VAR
+
)
;
String
srcScheme
=
srcSchemes
.
iterator
(
)
.
next
(
)
;
String
srcPathString
=
+
Joiner
.
on
(
)
.
join
(
srcPaths
)
+
;
if
(
srcScheme
==
null
||
srcScheme
.
equals
(
FileSystem
.
getLocal
(
getConf
(
)
)
.
getScheme
(
)
)
||
srcScheme
.
equals
(
)
)
{
List
<
File
>
srcFiles
=
srcURIs
.
stream
(
)
.
map
(
URI
::
getSchemeSpecificPart
)
.
map
(
File
::
new
)
.
collect
(
Collectors
.
toList
(
)
)
;
Path
dstPathBase
=
getRemoteStoragePath
(
getConf
(
)
,
appId
)
;
boolean
shouldArchive
=
srcFiles
.
size
(
)
>
1
||
srcFiles
.
get
(
0
)
.
isDirectory
(
)
||
(
resource
.
getType
(
)
==
LocalResourceType
.
ARCHIVE
&&
Arrays
.
stream
(
ARCHIVE_FILE_TYPES
)
.
noneMatch
(
suffix
->
srcFiles
.
get
(
0
)
.
getName
(
)
.
endsWith
(
suffix
)
)
)
;
if
(
shouldArchive
)
{
if
(
.
equals
(
srcScheme
)
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
+
,
resource
.
getResourcePath
(
)
,
srcPathString
)
)
;
}
else
if
(
resource
.
getType
(
)
!=
LocalResourceType
.
ARCHIVE
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
,
resource
.
getType
(
)
,
srcPathString
)
)
;
}
dstPath
=
new
Path
(
dstPathBase
,
resource
.
getResourcePath
(
)
)
.
suffix
(
)
;
}
else
{
dstPath
=
new
Path
(
dstPathBase
,
srcFiles
.
get
(
0
)
.
getName
(
)
)
;
}
FileSystem
remoteFS
=
dstPath
.
getFileSystem
(
getConf
(
)
)
;
else
if
(
shouldArchive
)
{
List
<
File
>
filesToZip
;
if
(
srcFiles
.
size
(
)
==
1
&&
srcFiles
.
get
(
0
)
.
isDirectory
(
)
)
{
File
[
]
childFiles
=
srcFiles
.
get
(
0
)
.
listFiles
(
)
;
if
(
childFiles
==
null
||
childFiles
.
length
==
0
)
{
throw
new
IllegalArgumentException
(
)
;
}
filesToZip
=
Lists
.
newArrayList
(
childFiles
)
;
}
else
{
filesToZip
=
srcFiles
;
}
ZipOutputStream
zout
=
new
ZipOutputStream
(
outputStream
)
;
for
(
File
fileToZip
:
filesToZip
)
{
addFileToZipRecursively
(
fileToZip
.
getParentFile
(
)
,
fileToZip
,
zout
)
;
}
zout
.
close
(
)
;
}
else
{
try
(
InputStream
inputStream
=
new
FileInputStream
(
srcFiles
.
get
(
0
)
)
)
{
private
boolean
monitorInfraApplication
(
)
throws
YarnException
,
IOException
{
boolean
loggedApplicationInfo
=
false
;
boolean
success
=
false
;
Thread
namenodeMonitoringThread
=
new
Thread
(
(
)
->
{
Supplier
<
Boolean
>
exitCritera
=
(
)
->
Apps
.
isApplicationFinalState
(
infraAppState
)
;
Optional
<
Properties
>
namenodeProperties
=
Optional
.
empty
(
)
;
while
(
!
exitCritera
.
get
(
)
)
{
try
{
if
(
!
namenodeProperties
.
isPresent
(
)
)
{
namenodeProperties
=
DynoInfraUtils
.
waitForAndGetNameNodeProperties
(
exitCritera
,
getConf
(
)
,
getNameNodeInfoPath
(
)
,
LOG
)
;
if
(
namenodeProperties
.
isPresent
(
)
)
{
Properties
props
=
namenodeProperties
.
get
(
)
;
private
boolean
monitorInfraApplication
(
)
throws
YarnException
,
IOException
{
boolean
loggedApplicationInfo
=
false
;
boolean
success
=
false
;
Thread
namenodeMonitoringThread
=
new
Thread
(
(
)
->
{
Supplier
<
Boolean
>
exitCritera
=
(
)
->
Apps
.
isApplicationFinalState
(
infraAppState
)
;
Optional
<
Properties
>
namenodeProperties
=
Optional
.
empty
(
)
;
while
(
!
exitCritera
.
get
(
)
)
{
try
{
if
(
!
namenodeProperties
.
isPresent
(
)
)
{
namenodeProperties
=
DynoInfraUtils
.
waitForAndGetNameNodeProperties
(
exitCritera
,
getConf
(
)
,
getNameNodeInfoPath
(
)
,
LOG
)
;
if
(
namenodeProperties
.
isPresent
(
)
)
{
Properties
props
=
namenodeProperties
.
get
(
)
;
LOG
.
info
(
,
DynoInfraUtils
.
getNameNodeHdfsUri
(
props
)
)
;
private
boolean
monitorInfraApplication
(
)
throws
YarnException
,
IOException
{
boolean
loggedApplicationInfo
=
false
;
boolean
success
=
false
;
Thread
namenodeMonitoringThread
=
new
Thread
(
(
)
->
{
Supplier
<
Boolean
>
exitCritera
=
(
)
->
Apps
.
isApplicationFinalState
(
infraAppState
)
;
Optional
<
Properties
>
namenodeProperties
=
Optional
.
empty
(
)
;
while
(
!
exitCritera
.
get
(
)
)
{
try
{
if
(
!
namenodeProperties
.
isPresent
(
)
)
{
namenodeProperties
=
DynoInfraUtils
.
waitForAndGetNameNodeProperties
(
exitCritera
,
getConf
(
)
,
getNameNodeInfoPath
(
)
,
LOG
)
;
if
(
namenodeProperties
.
isPresent
(
)
)
{
Properties
props
=
namenodeProperties
.
get
(
)
;
LOG
.
info
(
,
DynoInfraUtils
.
getNameNodeHdfsUri
(
props
)
)
;
LOG
.
info
(
,
DynoInfraUtils
.
getNameNodeWebUri
(
props
)
)
;
while
(
!
exitCritera
.
get
(
)
)
{
try
{
if
(
!
namenodeProperties
.
isPresent
(
)
)
{
namenodeProperties
=
DynoInfraUtils
.
waitForAndGetNameNodeProperties
(
exitCritera
,
getConf
(
)
,
getNameNodeInfoPath
(
)
,
LOG
)
;
if
(
namenodeProperties
.
isPresent
(
)
)
{
Properties
props
=
namenodeProperties
.
get
(
)
;
LOG
.
info
(
,
DynoInfraUtils
.
getNameNodeHdfsUri
(
props
)
)
;
LOG
.
info
(
,
DynoInfraUtils
.
getNameNodeWebUri
(
props
)
)
;
LOG
.
info
(
,
DynoInfraUtils
.
getNameNodeTrackingUri
(
props
)
)
;
}
else
{
break
;
}
}
DynoInfraUtils
.
waitForNameNodeStartup
(
namenodeProperties
.
get
(
)
,
exitCritera
,
LOG
)
;
DynoInfraUtils
.
waitForNameNodeReadiness
(
namenodeProperties
.
get
(
)
,
numTotalDataNodes
,
false
,
exitCritera
,
getConf
(
)
,
LOG
)
;
break
;
}
catch
(
IOException
ioe
)
{
}
catch
(
IOException
ioe
)
{
LOG
.
error
(
,
ioe
)
;
}
catch
(
InterruptedException
ie
)
{
return
;
}
}
if
(
!
Apps
.
isApplicationFinalState
(
infraAppState
)
&&
launchWorkloadJob
)
{
launchAndMonitorWorkloadDriver
(
namenodeProperties
.
get
(
)
)
;
}
}
)
;
if
(
launchNameNode
)
{
namenodeMonitoringThread
.
start
(
)
;
}
while
(
true
)
{
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
debug
(
)
;
}
ApplicationReport
report
=
yarnClient
.
getApplicationReport
(
infraAppId
)
;
LOG
.
error
(
,
ioe
)
;
}
catch
(
InterruptedException
ie
)
{
return
;
}
}
if
(
!
Apps
.
isApplicationFinalState
(
infraAppState
)
&&
launchWorkloadJob
)
{
launchAndMonitorWorkloadDriver
(
namenodeProperties
.
get
(
)
)
;
}
}
)
;
if
(
launchNameNode
)
{
namenodeMonitoringThread
.
start
(
)
;
}
while
(
true
)
{
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
debug
(
)
;
}
ApplicationReport
report
=
yarnClient
.
getApplicationReport
(
infraAppId
)
;
if
(
report
.
getTrackingUrl
(
)
!=
null
&&
!
loggedApplicationInfo
)
{
}
while
(
true
)
{
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
debug
(
)
;
}
ApplicationReport
report
=
yarnClient
.
getApplicationReport
(
infraAppId
)
;
if
(
report
.
getTrackingUrl
(
)
!=
null
&&
!
loggedApplicationInfo
)
{
loggedApplicationInfo
=
true
;
LOG
.
info
(
+
report
.
getTrackingUrl
(
)
)
;
LOG
.
info
(
+
report
.
getApplicationId
(
)
)
;
}
LOG
.
debug
(
+
+
+
+
,
infraAppId
.
getId
(
)
,
report
.
getClientToAMToken
(
)
,
report
.
getDiagnostics
(
)
,
report
.
getHost
(
)
,
report
.
getQueue
(
)
,
report
.
getRpcPort
(
)
,
report
.
getStartTime
(
)
,
report
.
getYarnApplicationState
(
)
,
report
.
getFinalApplicationStatus
(
)
,
report
.
getTrackingUrl
(
)
,
report
.
getUser
(
)
)
;
infraAppState
=
report
.
getYarnApplicationState
(
)
;
if
(
infraAppState
==
YarnApplicationState
.
KILLED
)
{
if
(
!
launchWorkloadJob
)
{
success
=
true
;
workloadConf
.
setInt
(
AuditReplayMapper
.
NUM_THREADS_KEY
,
workloadThreadsPerMapper
)
;
workloadConf
.
setDouble
(
AuditReplayMapper
.
RATE_FACTOR_KEY
,
workloadRateFactor
)
;
for
(
Map
.
Entry
<
String
,
String
>
configPair
:
workloadExtraConfigs
.
entrySet
(
)
)
{
workloadConf
.
set
(
configPair
.
getKey
(
)
,
configPair
.
getValue
(
)
)
;
}
workloadJob
=
WorkloadDriver
.
getJobForSubmission
(
workloadConf
,
nameNodeURI
.
toString
(
)
,
workloadStartTime
,
AuditReplayMapper
.
class
)
;
workloadJob
.
submit
(
)
;
while
(
!
Apps
.
isApplicationFinalState
(
infraAppState
)
&&
!
isCompleted
(
workloadAppState
)
)
{
workloadJob
.
monitorAndPrintJob
(
)
;
Thread
.
sleep
(
5000
)
;
workloadAppState
=
workloadJob
.
getJobState
(
)
;
}
if
(
isCompleted
(
workloadAppState
)
)
{
LOG
.
info
(
)
;
}
else
{
LOG
.
warn
(
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
info
(
)
;
if
(
workloadJob
!=
null
)
{
try
{
workloadAppState
=
workloadJob
.
getJobState
(
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
,
ioe
)
;
}
catch
(
InterruptedException
ie
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
return
;
}
if
(
!
isCompleted
(
workloadAppState
)
)
{
try
{
LOG
.
info
(
,
workloadJob
.
getJobID
(
)
)
;
workloadJob
.
killJob
(
)
;
LOG
.
info
(
)
;
}
catch
(
IOException
ioe
)
{
workloadAppState
=
workloadJob
.
getJobState
(
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
+
,
ioe
)
;
}
catch
(
InterruptedException
ie
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
return
;
}
if
(
!
isCompleted
(
workloadAppState
)
)
{
try
{
LOG
.
info
(
,
workloadJob
.
getJobID
(
)
)
;
workloadJob
.
killJob
(
)
;
LOG
.
info
(
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
error
(
,
workloadJob
.
getJobID
(
)
,
ioe
)
;
}
}
}
if
(
infraAppId
!=
null
&&
!
Apps
.
isApplicationFinalState
(
infraAppState
)
)
{
try
{
}
catch
(
InterruptedException
ie
)
{
Thread
.
currentThread
(
)
.
interrupt
(
)
;
return
;
}
if
(
!
isCompleted
(
workloadAppState
)
)
{
try
{
LOG
.
info
(
,
workloadJob
.
getJobID
(
)
)
;
workloadJob
.
killJob
(
)
;
LOG
.
info
(
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
error
(
,
workloadJob
.
getJobID
(
)
,
ioe
)
;
}
}
}
if
(
infraAppId
!=
null
&&
!
Apps
.
isApplicationFinalState
(
infraAppState
)
)
{
try
{
LOG
.
info
(
+
infraAppId
)
;
forceKillApplication
(
infraAppId
)
;
LOG
.
info
(
)
;
public
static
File
fetchHadoopTarball
(
File
destinationDir
,
String
version
,
Configuration
conf
,
Logger
log
)
throws
IOException
{
public
static
File
fetchHadoopTarball
(
File
destinationDir
,
String
version
,
Configuration
conf
,
Logger
log
)
throws
IOException
{
log
.
info
(
+
version
)
;
File
destinationFile
=
new
File
(
destinationDir
,
String
.
format
(
HADOOP_TAR_FILENAME_FORMAT
,
version
)
)
;
if
(
destinationFile
.
exists
(
)
)
{
long
lastUnderRepBlocks
=
Long
.
MAX_VALUE
;
try
{
while
(
true
)
{
try
{
Thread
.
sleep
(
TimeUnit
.
MINUTES
.
toMillis
(
1
)
)
;
long
underRepBlocks
=
Long
.
parseLong
(
fetchNameNodeJMXValue
(
nameNodeProperties
,
FSNAMESYSTEM_JMX_QUERY
,
JMX_MISSING_BLOCKS
)
)
+
Long
.
parseLong
(
fetchNameNodeJMXValue
(
nameNodeProperties
,
FSNAMESYSTEM_STATE_JMX_QUERY
,
JMX_UNDER_REPLICATED_BLOCKS
)
)
;
long
blockDecrease
=
lastUnderRepBlocks
-
underRepBlocks
;
lastUnderRepBlocks
=
underRepBlocks
;
if
(
blockDecrease
<
0
||
blockDecrease
>
(
totalBlocks
*
0.001
)
)
{
continue
;
}
String
liveNodeListString
=
fetchNameNodeJMXValue
(
nameNodeProperties
,
NAMENODE_INFO_JMX_QUERY
,
JMX_LIVE_NODES_LIST
)
;
Set
<
String
>
datanodesToReport
=
parseStaleDataNodeList
(
liveNodeListString
,
blockThreshold
,
log
)
;
if
(
datanodesToReport
.
isEmpty
(
)
&&
doneWaiting
.
get
(
)
)
{
log
.
info
(
+
)
;
break
;
@
SuppressWarnings
(
)
private
static
void
waitForNameNodeJMXValue
(
String
valueName
,
String
jmxBeanQuery
,
String
jmxProperty
,
double
threshold
,
double
printThreshold
,
boolean
decreasing
,
Properties
nameNodeProperties
,
Supplier
<
Boolean
>
shouldExit
,
Logger
log
)
throws
InterruptedException
{
double
lastPrintedValue
=
decreasing
?
Double
.
MAX_VALUE
:
Double
.
MIN_VALUE
;
double
value
;
int
retryCount
=
0
;
long
startTime
=
Time
.
monotonicNow
(
)
;
while
(
!
shouldExit
.
get
(
)
)
{
try
{
value
=
Double
.
parseDouble
(
fetchNameNodeJMXValue
(
nameNodeProperties
,
jmxBeanQuery
,
jmxProperty
)
)
;
if
(
(
decreasing
&&
value
<=
threshold
)
||
(
!
decreasing
&&
value
>=
threshold
)
)
{
for
(
JsonToken
tok
=
parser
.
nextToken
(
)
;
tok
!=
null
;
tok
=
parser
.
nextToken
(
)
)
{
if
(
tok
==
JsonToken
.
START_OBJECT
)
{
objectDepth
++
;
}
else
if
(
tok
==
JsonToken
.
END_OBJECT
)
{
objectDepth
--
;
}
else
if
(
tok
==
JsonToken
.
FIELD_NAME
)
{
if
(
objectDepth
==
1
)
{
currentNodeAddr
=
parser
.
getCurrentName
(
)
;
}
else
if
(
objectDepth
==
2
)
{
if
(
parser
.
getCurrentName
(
)
.
equals
(
)
)
{
JsonToken
valueToken
=
parser
.
nextToken
(
)
;
if
(
valueToken
!=
JsonToken
.
VALUE_NUMBER_INT
||
currentNodeAddr
==
null
)
{
throw
new
IOException
(
String
.
format
(
+
,
valueToken
,
currentNodeAddr
,
liveNodeJsonString
)
)
;
}
int
numBlocks
=
parser
.
getIntValue
(
)
;
if
(
numBlocks
<
blockThreshold
)
{
}
else
if
(
tok
==
JsonToken
.
END_OBJECT
)
{
objectDepth
--
;
}
else
if
(
tok
==
JsonToken
.
FIELD_NAME
)
{
if
(
objectDepth
==
1
)
{
currentNodeAddr
=
parser
.
getCurrentName
(
)
;
}
else
if
(
objectDepth
==
2
)
{
if
(
parser
.
getCurrentName
(
)
.
equals
(
)
)
{
JsonToken
valueToken
=
parser
.
nextToken
(
)
;
if
(
valueToken
!=
JsonToken
.
VALUE_NUMBER_INT
||
currentNodeAddr
==
null
)
{
throw
new
IOException
(
String
.
format
(
+
,
valueToken
,
currentNodeAddr
,
liveNodeJsonString
)
)
;
}
int
numBlocks
=
parser
.
getIntValue
(
)
;
if
(
numBlocks
<
blockThreshold
)
{
log
.
debug
(
String
.
format
(
,
currentNodeAddr
,
numBlocks
)
)
;
dataNodesToReport
.
add
(
currentNodeAddr
)
;
}
else
{
public
static
Job
getJobForSubmission
(
Configuration
baseConf
,
String
nnURI
,
long
startTimestampMs
,
Class
<
?
extends
WorkloadMapper
<
?
,
?
,
?
,
?
>>
mapperClass
)
throws
IOException
,
InstantiationException
,
IllegalAccessException
{
Configuration
conf
=
new
Configuration
(
baseConf
)
;
conf
.
set
(
NN_URI
,
nnURI
)
;
conf
.
setBoolean
(
MRJobConfig
.
MAP_SPECULATIVE
,
false
)
;
String
startTimeString
=
new
SimpleDateFormat
(
)
.
format
(
new
Date
(
startTimestampMs
)
)
;
t
.
addToQueue
(
AuditReplayCommand
.
getPoisonPill
(
highestTimestamp
+
1
)
)
;
}
Optional
<
Exception
>
threadException
=
Optional
.
empty
(
)
;
for
(
AuditReplayThread
t
:
threads
)
{
t
.
join
(
)
;
t
.
drainCounters
(
context
)
;
t
.
drainCommandLatencies
(
context
)
;
if
(
t
.
getException
(
)
!=
null
)
{
threadException
=
Optional
.
of
(
t
.
getException
(
)
)
;
}
}
progressExecutor
.
shutdown
(
)
;
if
(
threadException
.
isPresent
(
)
)
{
throw
new
RuntimeException
(
,
threadException
.
get
(
)
)
;
}
LOG
.
info
(
+
(
System
.
currentTimeMillis
(
)
-
startTimestampMs
)
)
;
long
totalCommands
=
context
.
getCounter
(
REPLAYCOUNTERS
.
TOTALCOMMANDS
)
.
getValue
(
)
;
if
(
totalCommands
!=
0
)
{
double
percentageOfInvalidOps
=
context
.
getCounter
(
REPLAYCOUNTERS
.
TOTALINVALIDCOMMANDS
)
.
getValue
(
)
*
100.0
/
totalCommands
;
@
Override
public
void
run
(
)
{
long
currentEpoch
=
System
.
currentTimeMillis
(
)
;
long
delay
=
startTimestampMs
-
currentEpoch
;
try
{
if
(
delay
>
0
)
{
LOG
.
info
(
+
delay
+
)
;
Thread
.
sleep
(
delay
)
;
}
else
{
LOG
.
warn
(
+
(
-
1
*
delay
)
+
)
;
}
AuditReplayCommand
cmd
=
commandQueue
.
take
(
)
;
while
(
!
cmd
.
isPoison
(
)
)
{
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
TOTALCOMMANDS
)
.
increment
(
1
)
;
delay
=
cmd
.
getDelay
(
TimeUnit
.
MILLISECONDS
)
;
if
(
delay
<
-
5
)
{
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
LATECOMMANDS
)
.
increment
(
1
)
;
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
LATECOMMANDSTOTALTIME
)
.
increment
(
-
1
*
delay
)
;
}
if
(
!
replayLog
(
cmd
)
)
{
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
TOTALINVALIDCOMMANDS
)
.
increment
(
1
)
;
}
cmd
=
commandQueue
.
take
(
)
;
}
}
catch
(
InterruptedException
e
)
{
else
{
LOG
.
warn
(
+
(
-
1
*
delay
)
+
)
;
}
AuditReplayCommand
cmd
=
commandQueue
.
take
(
)
;
while
(
!
cmd
.
isPoison
(
)
)
{
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
TOTALCOMMANDS
)
.
increment
(
1
)
;
delay
=
cmd
.
getDelay
(
TimeUnit
.
MILLISECONDS
)
;
if
(
delay
<
-
5
)
{
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
LATECOMMANDS
)
.
increment
(
1
)
;
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
LATECOMMANDSTOTALTIME
)
.
increment
(
-
1
*
delay
)
;
}
if
(
!
replayLog
(
cmd
)
)
{
replayCountersMap
.
get
(
REPLAYCOUNTERS
.
TOTALINVALIDCOMMANDS
)
.
increment
(
1
)
;
}
cmd
=
commandQueue
.
take
(
)
;
}
}
catch
(
InterruptedException
e
)
{
LOG
.
error
(
,
e
)
;
}
catch
(
Exception
e
)
{
private
void
testAuditWorkloadWithOutput
(
String
auditOutputPath
)
throws
Exception
{
long
workloadStartTime
=
System
.
currentTimeMillis
(
)
+
10000
;
Job
workloadJob
=
WorkloadDriver
.
getJobForSubmission
(
conf
,
dfs
.
getUri
(
)
.
toString
(
)
,
workloadStartTime
,
AuditReplayMapper
.
class
)
;
boolean
success
=
workloadJob
.
waitForCompletion
(
true
)
;
assertTrue
(
,
success
)
;
Counters
counters
=
workloadJob
.
getCounters
(
)
;
assertEquals
(
6
,
counters
.
findCounter
(
AuditReplayMapper
.
REPLAYCOUNTERS
.
TOTALCOMMANDS
)
.
getValue
(
)
)
;
assertEquals
(
1
,
counters
.
findCounter
(
AuditReplayMapper
.
REPLAYCOUNTERS
.
TOTALINVALIDCOMMANDS
)
.
getValue
(
)
)
;
assertTrue
(
dfs
.
getFileStatus
(
new
Path
(
)
)
.
isFile
(
)
)
;
assertTrue
(
dfs
.
getFileStatus
(
new
Path
(
)
)
.
isDirectory
(
)
)
;
assertFalse
(
dfs
.
exists
(
new
Path
(
)
)
)
;
assertTrue
(
dfs
.
exists
(
new
Path
(
auditOutputPath
)
)
)
;
try
(
FSDataInputStream
auditOutputFile
=
dfs
.
open
(
new
Path
(
auditOutputPath
,
)
)
)
{
String
auditOutput
=
IOUtils
.
toString
(
auditOutputFile
,
StandardCharsets
.
UTF_8
)
;
isIgnoreFailures
=
true
;
}
else
if
(
.
equals
(
args
[
idx
]
)
)
{
if
(
++
idx
==
args
.
length
)
{
System
.
out
.
println
(
)
;
System
.
out
.
println
(
USAGE
)
;
return
-
1
;
}
logpath
=
new
Path
(
args
[
idx
]
)
;
}
else
if
(
'-'
==
args
[
idx
]
.
codePointAt
(
0
)
)
{
System
.
out
.
println
(
+
args
[
idx
]
)
;
System
.
out
.
println
(
USAGE
)
;
ToolRunner
.
printGenericCommandUsage
(
System
.
out
)
;
return
-
1
;
}
else
{
ops
.
add
(
new
FileOperation
(
args
[
idx
]
)
)
;
}
}
if
(
ops
.
isEmpty
(
)
)
{
isIgnoreFailures
=
true
;
}
else
if
(
.
equals
(
args
[
idx
]
)
)
{
if
(
++
idx
==
args
.
length
)
{
System
.
out
.
println
(
)
;
System
.
out
.
println
(
USAGE
)
;
return
-
1
;
}
logpath
=
new
Path
(
args
[
idx
]
)
;
}
else
if
(
'-'
==
args
[
idx
]
.
codePointAt
(
0
)
)
{
System
.
out
.
println
(
+
args
[
idx
]
)
;
System
.
out
.
println
(
USAGE
)
;
ToolRunner
.
printGenericCommandUsage
(
System
.
out
)
;
return
-
1
;
}
else
{
ops
.
add
(
new
FileOperation
(
args
[
idx
]
)
)
;
}
}
if
(
ops
.
isEmpty
(
)
)
{
private
boolean
setup
(
List
<
FileOperation
>
ops
,
Path
log
)
throws
IOException
{
final
String
randomId
=
getRandomId
(
)
;
JobClient
jClient
=
new
JobClient
(
jobconf
)
;
Path
stagingArea
;
try
{
stagingArea
=
JobSubmissionFiles
.
getStagingDir
(
jClient
.
getClusterHandle
(
)
,
jobconf
)
;
}
catch
(
InterruptedException
ie
)
{
throw
new
IOException
(
ie
)
;
}
Path
jobdir
=
new
Path
(
stagingArea
+
NAME
+
+
randomId
)
;
FsPermission
mapredSysPerms
=
new
FsPermission
(
JobSubmissionFiles
.
JOB_DIR_PERMISSION
)
;
FileSystem
.
mkdirs
(
jClient
.
getFs
(
)
,
jobdir
,
mapredSysPerms
)
;
LOG
.
info
(
JOB_DIR_LABEL
+
+
jobdir
)
;
if
(
log
==
null
)
{
log
=
new
Path
(
jobdir
,
)
;
}
FileOutputFormat
.
setOutputPath
(
jobconf
,
log
)
;
FileStatus
srcstat
=
fs
.
getFileStatus
(
op
.
src
)
;
if
(
srcstat
.
isDirectory
(
)
&&
op
.
isDifferent
(
srcstat
)
)
{
++
opCount
;
opWriter
.
append
(
new
Text
(
op
.
src
.
toString
(
)
)
,
op
)
;
}
Stack
<
Path
>
pathstack
=
new
Stack
<
Path
>
(
)
;
for
(
pathstack
.
push
(
op
.
src
)
;
!
pathstack
.
empty
(
)
;
)
{
for
(
FileStatus
stat
:
fs
.
listStatus
(
pathstack
.
pop
(
)
)
)
{
if
(
stat
.
isDirectory
(
)
)
{
pathstack
.
push
(
stat
.
getPath
(
)
)
;
}
if
(
op
.
isDifferent
(
stat
)
)
{
++
opCount
;
if
(
++
synCount
>
SYNC_FILE_MAX
)
{
opWriter
.
sync
(
)
;
synCount
=
0
;
}
Path
f
=
stat
.
getPath
(
)
;
void
diffDistCp
(
)
throws
IOException
,
RetryException
{
RunningJobStatus
job
=
getCurrentJob
(
)
;
if
(
job
!=
null
)
{
if
(
job
.
isComplete
(
)
)
{
jobId
=
null
;
if
(
job
.
isSuccessful
(
)
)
{
@
VisibleForTesting
void
updateStage
(
Stage
value
)
{
String
oldStage
=
stage
==
null
?
:
stage
.
name
(
)
;
String
newStage
=
value
==
null
?
:
value
.
name
(
)
;
if
(
useSnapshotDiff
)
{
command
.
add
(
)
;
command
.
add
(
LAST_SNAPSHOT_NAME
)
;
command
.
add
(
CURRENT_SNAPSHOT_NAME
)
;
}
command
.
add
(
)
;
command
.
add
(
mapNum
+
)
;
command
.
add
(
)
;
command
.
add
(
bandWidth
+
)
;
command
.
add
(
srcParam
)
;
command
.
add
(
dstParam
)
;
Configuration
config
=
new
Configuration
(
conf
)
;
DistCp
distCp
;
try
{
distCp
=
new
DistCp
(
config
,
OptionsParser
.
parse
(
command
.
toArray
(
new
String
[
]
{
}
)
)
)
;
Job
job
=
distCp
.
createAndSubmitJob
(
)
;
private
int
continueJob
(
)
throws
InterruptedException
{
BalanceProcedureScheduler
scheduler
=
new
BalanceProcedureScheduler
(
getConf
(
)
)
;
try
{
scheduler
.
init
(
true
)
;
while
(
true
)
{
Collection
<
BalanceJob
>
jobs
=
scheduler
.
getAllJobs
(
)
;
int
unfinished
=
0
;
for
(
BalanceJob
job
:
jobs
)
{
if
(
!
job
.
isJobDone
(
)
)
{
unfinished
++
;
}
LOG
.
info
(
job
.
toString
(
)
)
;
}
if
(
unfinished
==
0
)
{
break
;
}
Thread
.
sleep
(
TimeUnit
.
SECONDS
.
toMillis
(
10
)
)
;
}
}
catch
(
IOException
e
)
{
public
void
recoverJob
(
BalanceJob
job
)
throws
IOException
{
FSDataInputStream
in
=
null
;
try
{
Path
logPath
=
getLatestStateJobPath
(
job
)
;
FileSystem
fs
=
FileSystem
.
get
(
workUri
,
conf
)
;
in
=
fs
.
open
(
logPath
)
;
job
.
readFields
(
in
)
;
}
catch
(
FileNotFoundException
e
)
{
LOG
.
debug
(
,
workPath
)
;
fs
.
mkdirs
(
workPath
)
;
return
new
BalanceJob
[
0
]
;
}
BalanceJob
[
]
jobs
=
new
BalanceJob
[
statuses
.
length
]
;
StringBuilder
builder
=
new
StringBuilder
(
)
;
builder
.
append
(
)
;
for
(
int
i
=
0
;
i
<
statuses
.
length
;
i
++
)
{
if
(
statuses
[
i
]
.
isDirectory
(
)
)
{
jobs
[
i
]
=
new
BalanceJob
.
Builder
<
>
(
)
.
build
(
)
;
jobs
[
i
]
.
setId
(
statuses
[
i
]
.
getPath
(
)
.
getName
(
)
)
;
builder
.
append
(
jobs
[
i
]
)
;
if
(
i
<
statuses
.
length
-
1
)
{
builder
.
append
(
)
;
}
}
}
builder
.
append
(
)
;
void
delay
(
BalanceJob
job
,
long
delayInMilliseconds
)
{
delayQueue
.
add
(
new
DelayWrapper
(
job
,
delayInMilliseconds
)
)
;
@
Override
public
boolean
execute
(
)
throws
IOException
{
if
(
currentPhase
<
totalPhase
)
{
}
long
seed
=
r
.
nextLong
(
)
;
r
.
setSeed
(
seed
)
;
System
.
out
.
println
(
name
.
getMethodName
(
)
+
+
seed
)
;
conf
=
new
HdfsConfiguration
(
)
;
conf
.
set
(
SingleUGIResolver
.
USER
,
singleUser
)
;
conf
.
set
(
SingleUGIResolver
.
GROUP
,
singleGroup
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_PROVIDER_STORAGEUUID
,
DFSConfigKeys
.
DFS_PROVIDER_STORAGEUUID_DEFAULT
)
;
conf
.
setBoolean
(
DFSConfigKeys
.
DFS_NAMENODE_PROVIDED_ENABLED
,
true
)
;
conf
.
setClass
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_CLASS
,
TextFileRegionAliasMap
.
class
,
BlockAliasMap
.
class
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_TEXT_WRITE_DIR
,
nnDirPath
.
toString
(
)
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_TEXT_READ_FILE
,
new
Path
(
nnDirPath
,
fileNameFromBlockPoolID
(
bpid
)
)
.
toString
(
)
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER
,
)
;
conf
.
set
(
MiniDFSCluster
.
HDFS_MINIDFS_BASEDIR_PROVIDED
,
new
File
(
providedPath
.
toUri
(
)
)
.
toString
(
)
)
;
File
imageDir
=
new
File
(
providedPath
.
toUri
(
)
)
;
if
(
!
imageDir
.
exists
(
)
)
{
conf
.
setClass
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_CLASS
,
TextFileRegionAliasMap
.
class
,
BlockAliasMap
.
class
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_TEXT_WRITE_DIR
,
nnDirPath
.
toString
(
)
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_TEXT_READ_FILE
,
new
Path
(
nnDirPath
,
fileNameFromBlockPoolID
(
bpid
)
)
.
toString
(
)
)
;
conf
.
set
(
DFSConfigKeys
.
DFS_PROVIDED_ALIASMAP_TEXT_DELIMITER
,
)
;
conf
.
set
(
MiniDFSCluster
.
HDFS_MINIDFS_BASEDIR_PROVIDED
,
new
File
(
providedPath
.
toUri
(
)
)
.
toString
(
)
)
;
File
imageDir
=
new
File
(
providedPath
.
toUri
(
)
)
;
if
(
!
imageDir
.
exists
(
)
)
{
LOG
.
info
(
+
imageDir
)
;
imageDir
.
mkdirs
(
)
;
}
File
nnDir
=
new
File
(
nnDirPath
.
toUri
(
)
)
;
if
(
!
nnDir
.
exists
(
)
)
{
nnDir
.
mkdirs
(
)
;
}
for
(
int
i
=
0
;
i
<
numFiles
;
i
++
)
{
File
newFile
=
new
File
(
new
Path
(
providedPath
,
filePrefix
+
i
+
fileSuffix
)
.
toUri
(
)
)
;
if
(
!
newFile
.
exists
(
)
)
{
private
void
setAndUnsetReplication
(
String
filename
)
throws
Exception
{
Path
file
=
new
Path
(
filename
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
short
newReplication
=
4
;
private
void
setAndUnsetReplication
(
String
filename
)
throws
Exception
{
Path
file
=
new
Path
(
filename
)
;
FileSystem
fs
=
cluster
.
getFileSystem
(
)
;
short
newReplication
=
4
;
LOG
.
info
(
,
filename
,
newReplication
)
;
fs
.
setReplication
(
file
,
newReplication
)
;
DFSTestUtil
.
waitForReplication
(
(
DistributedFileSystem
)
fs
,
file
,
newReplication
,
10000
)
;
DFSClient
client
=
new
DFSClient
(
new
InetSocketAddress
(
,
cluster
.
getNameNodePort
(
)
)
,
cluster
.
getConfiguration
(
0
)
)
;
getAndCheckBlockLocations
(
client
,
filename
,
baseFileLen
,
1
,
newReplication
)
;
newReplication
=
1
;
private
void
verifyPathsWithHAFailoverIfNecessary
(
MiniDFSNNTopology
topology
,
String
providedNameservice
)
throws
Exception
{
List
<
Integer
>
nnIndexes
=
cluster
.
getNNIndexes
(
providedNameservice
)
;
if
(
topology
.
isHA
(
)
)
{
int
nn1
=
nnIndexes
.
get
(
0
)
;
int
nn2
=
nnIndexes
.
get
(
1
)
;
try
{
verifyFileSystemContents
(
nn1
)
;
fail
(
)
;
}
catch
(
RemoteException
e
)
{
LOG
.
info
(
+
e
)
;
}
cluster
.
transitionToActive
(
nn1
)
;
LOG
.
info
(
,
nn1
)
;
verifyFileSystemContents
(
nn1
)
;
cluster
.
transitionToStandby
(
nn1
)
;
cluster
.
transitionToActive
(
nn2
)
;
fail
(
)
;
}
catch
(
RemoteException
e
)
{
LOG
.
info
(
+
e
)
;
}
cluster
.
transitionToActive
(
nn1
)
;
LOG
.
info
(
,
nn1
)
;
verifyFileSystemContents
(
nn1
)
;
cluster
.
transitionToStandby
(
nn1
)
;
cluster
.
transitionToActive
(
nn2
)
;
LOG
.
info
(
,
nn2
)
;
verifyFileSystemContents
(
nn2
)
;
cluster
.
shutdownNameNodes
(
)
;
try
{
verifyFileSystemContents
(
nn2
)
;
fail
(
)
;
}
catch
(
NullPointerException
e
)
{
@
Test
public
void
testInMemoryAliasMapMultiTopologies
(
)
throws
Exception
{
MiniDFSNNTopology
[
]
topologies
=
new
MiniDFSNNTopology
[
]
{
MiniDFSNNTopology
.
simpleHATopology
(
)
,
MiniDFSNNTopology
.
simpleFederatedTopology
(
3
)
,
MiniDFSNNTopology
.
simpleHAFederatedTopology
(
3
)
}
;
for
(
MiniDFSNNTopology
topology
:
topologies
)
{
@
Test
public
void
testProvidedWithHierarchicalTopology
(
)
throws
Exception
{
conf
.
setClass
(
ImageWriter
.
Options
.
UGI_CLASS
,
FsUGIResolver
.
class
,
UGIResolver
.
class
)
;
String
packageName
=
;
String
[
]
policies
=
new
String
[
]
{
,
,
,
}
;
createImage
(
new
FSTreeWalk
(
providedPath
,
conf
)
,
nnDirPath
,
FixedBlockResolver
.
class
)
;
String
[
]
racks
=
{
,
,
,
,
,
,
,
}
;
for
(
String
policy
:
policies
)
{
static
void
setupDataGeneratorConfig
(
Configuration
conf
)
{
boolean
compress
=
isCompressionEmulationEnabled
(
conf
)
;
if
(
compress
)
{
float
ratio
=
getMapInputCompressionEmulationRatio
(
conf
)
;
int
numCompressedFiles
=
0
;
FileStatus
[
]
outFileStatuses
=
fs
.
listStatus
(
inputDir
,
new
Utils
.
OutputFileUtils
.
OutputFilesFilter
(
)
)
;
for
(
FileStatus
status
:
outFileStatuses
)
{
if
(
compressionCodecs
!=
null
)
{
CompressionCodec
codec
=
compressionCodecs
.
getCodec
(
status
.
getPath
(
)
)
;
if
(
codec
!=
null
)
{
++
numCompressedFiles
;
compressedDataSize
+=
status
.
getLen
(
)
;
}
}
}
LOG
.
info
(
)
;
LOG
.
info
(
+
StringUtils
.
humanReadableInt
(
compressedDataSize
)
)
;
LOG
.
info
(
+
numCompressedFiles
)
;
if
(
numCompressedFiles
==
0
)
{
throw
new
RuntimeException
(
+
+
inputDir
.
toString
(
)
+
+
+
+
+
+
COMPRESSION_EMULATION_ENABLE
+
)
;
}
if
(
uncompressedDataSize
>
0
)
{
double
ratio
=
(
(
double
)
compressedDataSize
)
/
uncompressedDataSize
;
long
byteCount
=
0
;
long
bytesSync
=
0
;
for
(
Iterator
it
=
dcFiles
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
Map
.
Entry
entry
=
(
Map
.
Entry
)
it
.
next
(
)
;
LongWritable
fileSize
=
new
LongWritable
(
Long
.
parseLong
(
entry
.
getValue
(
)
.
toString
(
)
)
)
;
BytesWritable
filePath
=
new
BytesWritable
(
entry
.
getKey
(
)
.
toString
(
)
.
getBytes
(
charsetUTF8
)
)
;
byteCount
+=
fileSize
.
get
(
)
;
bytesSync
+=
fileSize
.
get
(
)
;
if
(
bytesSync
>
AVG_BYTES_PER_MAP
)
{
src_writer
.
sync
(
)
;
bytesSync
=
fileSize
.
get
(
)
;
}
src_writer
.
append
(
fileSize
,
filePath
)
;
}
if
(
src_writer
!=
null
)
{
src_writer
.
close
(
)
;
}
fs
.
deleteOnExit
(
distCacheFilesList
)
;
for
(
Iterator
it
=
dcFiles
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
Map
.
Entry
entry
=
(
Map
.
Entry
)
it
.
next
(
)
;
LongWritable
fileSize
=
new
LongWritable
(
Long
.
parseLong
(
entry
.
getValue
(
)
.
toString
(
)
)
)
;
BytesWritable
filePath
=
new
BytesWritable
(
entry
.
getKey
(
)
.
toString
(
)
.
getBytes
(
charsetUTF8
)
)
;
byteCount
+=
fileSize
.
get
(
)
;
bytesSync
+=
fileSize
.
get
(
)
;
if
(
bytesSync
>
AVG_BYTES_PER_MAP
)
{
src_writer
.
sync
(
)
;
bytesSync
=
fileSize
.
get
(
)
;
}
src_writer
.
append
(
fileSize
,
filePath
)
;
}
if
(
src_writer
!=
null
)
{
src_writer
.
close
(
)
;
}
fs
.
deleteOnExit
(
distCacheFilesList
)
;
conf
.
setInt
(
GenerateDistCacheData
.
GRIDMIX_DISTCACHE_FILE_COUNT
,
fileCount
)
;
conf
.
setLong
(
GenerateDistCacheData
.
GRIDMIX_DISTCACHE_BYTE_COUNT
,
byteCount
)
;
}
else
{
configureRandomBytesDataGenerator
(
)
;
}
job
.
submit
(
)
;
return
job
;
}
private
void
configureRandomBytesDataGenerator
(
)
{
job
.
setMapperClass
(
GenDataMapper
.
class
)
;
job
.
setNumReduceTasks
(
0
)
;
job
.
setMapOutputKeyClass
(
NullWritable
.
class
)
;
job
.
setMapOutputValueClass
(
BytesWritable
.
class
)
;
job
.
setInputFormatClass
(
GenDataFormat
.
class
)
;
job
.
setOutputFormatClass
(
RawBytesOutputFormat
.
class
)
;
job
.
setJarByClass
(
GenerateData
.
class
)
;
try
{
FileInputFormat
.
addInputPath
(
job
,
new
Path
(
)
)
;
}
catch
(
IOException
e
)
{
protected
int
writeInputData
(
long
genbytes
,
Path
inputDir
)
throws
IOException
,
InterruptedException
{
if
(
genbytes
>
0
)
{
final
Configuration
conf
=
getConf
(
)
;
if
(
inputDir
.
getFileSystem
(
conf
)
.
exists
(
inputDir
)
)
{
protected
int
writeInputData
(
long
genbytes
,
Path
inputDir
)
throws
IOException
,
InterruptedException
{
if
(
genbytes
>
0
)
{
final
Configuration
conf
=
getConf
(
)
;
if
(
inputDir
.
getFileSystem
(
conf
)
.
exists
(
inputDir
)
)
{
LOG
.
error
(
+
,
inputDir
)
;
return
STARTUP_FAILED_ERROR
;
}
CompressionEmulationUtil
.
setupDataGeneratorConfig
(
conf
)
;
final
GenerateData
genData
=
new
GenerateData
(
conf
,
inputDir
,
genbytes
)
;
LOG
.
info
(
,
StringUtils
.
TraditionalBinaryPrefix
.
long2String
(
genbytes
,
,
1
)
)
;
launchGridmixJob
(
genData
)
;
FsShell
shell
=
new
FsShell
(
conf
)
;
try
{
LOG
.
info
(
,
inputDir
)
;
shell
.
run
(
new
String
[
]
{
,
,
,
inputDir
.
toString
(
)
}
)
;
}
catch
(
Exception
e
)
{
private
void
startThreads
(
Configuration
conf
,
String
traceIn
,
Path
ioPath
,
Path
scratchDir
,
CountDownLatch
startFlag
,
UserResolver
userResolver
)
throws
IOException
{
try
{
Path
inputDir
=
getGridmixInputDataPath
(
ioPath
)
;
GridmixJobSubmissionPolicy
policy
=
getJobSubmissionPolicy
(
conf
)
;
int
numThreads
=
conf
.
getInt
(
GRIDMIX_SUB_THR
,
noOfSubmitterThreads
)
;
int
queueDep
=
conf
.
getInt
(
GRIDMIX_QUE_DEP
,
5
)
;
submitter
=
createJobSubmitter
(
monitor
,
numThreads
,
queueDep
,
new
FilePool
(
conf
,
inputDir
)
,
userResolver
,
statistics
)
;
distCacheEmulator
=
new
DistributedCacheEmulator
(
conf
,
ioPath
)
;
factory
=
createJobFactory
(
submitter
,
traceIn
,
scratchDir
,
conf
,
startFlag
,
userResolver
)
;
factory
.
jobCreator
.
setDistCacheEmulator
(
distCacheEmulator
)
;
if
(
policy
==
GridmixJobSubmissionPolicy
.
SERIAL
)
{
statistics
.
addJobStatsListeners
(
factory
)
;
}
else
{
statistics
.
addClusterStatsObservers
(
factory
)
;
}
statistics
.
addJobStatsListeners
(
summarizer
.
getExecutionSummarizer
(
)
)
;
statistics
.
addClusterStatsObservers
(
summarizer
.
getClusterSummarizer
(
)
)
;
monitor
.
start
(
)
;
submitter
.
start
(
)
;
}
catch
(
Exception
e
)
{
if
(
.
equals
(
argv
[
i
]
)
)
{
genbytes
=
StringUtils
.
TraditionalBinaryPrefix
.
string2long
(
argv
[
++
i
]
)
;
if
(
genbytes
<=
0
)
{
LOG
.
error
(
+
)
;
return
ARGS_ERROR
;
}
}
else
if
(
.
equals
(
argv
[
i
]
)
)
{
userRsrc
=
new
URI
(
argv
[
++
i
]
)
;
}
else
{
LOG
.
error
(
+
argv
[
i
]
+
)
;
printUsage
(
System
.
err
)
;
return
ARGS_ERROR
;
}
}
if
(
userResolver
.
needsTargetUsersList
(
)
)
{
if
(
userRsrc
!=
null
)
{
if
(
!
userResolver
.
setTargetUsers
(
userRsrc
,
conf
)
)
{
LOG
.
warn
(
+
userRsrc
+
)
;
}
else
{
LOG
.
error
(
userResolver
.
getClass
(
)
+
)
;
printUsage
(
System
.
err
)
;
return
ARGS_ERROR
;
}
}
else
if
(
userRsrc
!=
null
)
{
LOG
.
warn
(
+
userRsrc
+
)
;
}
ioPath
=
new
Path
(
argv
[
argv
.
length
-
2
]
)
;
traceIn
=
argv
[
argv
.
length
-
1
]
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
e
.
toString
(
)
+
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
e
.
printStackTrace
(
)
;
}
printUsage
(
System
.
err
)
;
return
ARGS_ERROR
;
}
final
FileSystem
inputFs
=
ioPath
.
getFileSystem
(
conf
)
;
startThreads
(
conf
,
traceIn
,
ioPath
,
scratchDir
,
startFlag
,
userResolver
)
;
Path
inputDir
=
getGridmixInputDataPath
(
ioPath
)
;
exitCode
=
writeInputData
(
genbytes
,
inputDir
)
;
if
(
exitCode
!=
0
)
{
return
exitCode
;
}
stats
=
GenerateData
.
publishDataStatistics
(
inputDir
,
genbytes
,
conf
)
;
submitter
.
refreshFilePool
(
)
;
boolean
shouldGenerate
=
(
genbytes
>
0
)
;
exitCode
=
setupEmulation
(
conf
,
traceIn
,
scratchDir
,
ioPath
,
shouldGenerate
)
;
if
(
exitCode
!=
0
)
{
return
exitCode
;
}
summarizer
.
start
(
conf
)
;
factory
.
start
(
)
;
statistics
.
start
(
)
;
}
catch
(
Throwable
e
)
{
Path
inputDir
=
getGridmixInputDataPath
(
ioPath
)
;
exitCode
=
writeInputData
(
genbytes
,
inputDir
)
;
if
(
exitCode
!=
0
)
{
return
exitCode
;
}
stats
=
GenerateData
.
publishDataStatistics
(
inputDir
,
genbytes
,
conf
)
;
submitter
.
refreshFilePool
(
)
;
boolean
shouldGenerate
=
(
genbytes
>
0
)
;
exitCode
=
setupEmulation
(
conf
,
traceIn
,
scratchDir
,
ioPath
,
shouldGenerate
)
;
if
(
exitCode
!=
0
)
{
return
exitCode
;
}
summarizer
.
start
(
conf
)
;
factory
.
start
(
)
;
statistics
.
start
(
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
e
.
toString
(
)
+
)
;
if
(
exitCode
!=
0
)
{
return
exitCode
;
}
summarizer
.
start
(
conf
)
;
factory
.
start
(
)
;
statistics
.
start
(
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
e
.
toString
(
)
+
)
;
LOG
.
debug
(
,
e
)
;
if
(
factory
!=
null
)
factory
.
abort
(
)
;
exitCode
=
STARTUP_FAILED_ERROR
;
}
finally
{
startFlag
.
countDown
(
)
;
}
if
(
factory
!=
null
)
{
factory
.
join
(
Long
.
MAX_VALUE
)
;
final
Throwable
badTraceException
=
factory
.
error
(
)
;
private
static
void
scaleConfigParameter
(
Configuration
sourceConf
,
Configuration
destConf
,
String
clusterValueKey
,
String
jobValueKey
,
long
defaultValue
)
{
long
simulatedClusterDefaultValue
=
destConf
.
getLong
(
clusterValueKey
,
defaultValue
)
;
long
originalClusterDefaultValue
=
sourceConf
.
getLong
(
clusterValueKey
,
defaultValue
)
;
long
originalJobValue
=
sourceConf
.
getLong
(
jobValueKey
,
defaultValue
)
;
double
scaleFactor
=
(
double
)
originalJobValue
/
originalClusterDefaultValue
;
long
simulatedJobValue
=
(
long
)
(
scaleFactor
*
simulatedClusterDefaultValue
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
submissionFailed
(
JobStats
job
)
{
String
jobID
=
job
.
getJob
(
)
.
getConfiguration
(
)
.
get
(
Gridmix
.
ORIGINAL_JOB_ID
)
;
protected
void
onSuccess
(
Job
job
)
{
protected
void
onFailure
(
Job
job
)
{
for
(
int
i
=
0
;
i
<
reds
;
++
i
)
{
final
TaskInfo
info
=
jobdesc
.
getTaskInfo
(
TaskType
.
REDUCE
,
i
)
;
reduceByteRatio
[
i
]
=
info
.
getInputBytes
(
)
/
(
1.0
*
mapOutputBytesTotal
)
;
reduceRecordRatio
[
i
]
=
info
.
getInputRecords
(
)
/
(
1.0
*
mapOutputRecordsTotal
)
;
}
final
InputStriper
striper
=
new
InputStriper
(
inputDir
,
mapInputBytesTotal
)
;
final
List
<
InputSplit
>
splits
=
new
ArrayList
<
InputSplit
>
(
)
;
for
(
int
i
=
0
;
i
<
maps
;
++
i
)
{
final
int
nSpec
=
reds
/
maps
+
(
(
reds
%
maps
)
>
i
?
1
:
0
)
;
final
long
[
]
specBytes
=
new
long
[
nSpec
]
;
final
long
[
]
specRecords
=
new
long
[
nSpec
]
;
final
ResourceUsageMetrics
[
]
metrics
=
new
ResourceUsageMetrics
[
nSpec
]
;
for
(
int
j
=
0
;
j
<
nSpec
;
++
j
)
{
final
TaskInfo
info
=
jobdesc
.
getTaskInfo
(
TaskType
.
REDUCE
,
i
+
j
*
maps
)
;
specBytes
[
j
]
=
info
.
getOutputBytes
(
)
;
specRecords
[
j
]
=
info
.
getOutputRecords
(
)
;
static
void
setRandomTextDataGeneratorListSize
(
Configuration
conf
,
int
listSize
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
static
void
setRandomTextDataGeneratorWordSize
(
Configuration
conf
,
int
wordSize
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
void
buildSplits
(
FilePool
inputDir
)
throws
IOException
{
final
List
<
InputSplit
>
splits
=
new
ArrayList
<
InputSplit
>
(
)
;
final
int
reds
=
(
mapTasksOnly
)
?
0
:
jobdesc
.
getNumberReduces
(
)
;
final
int
maps
=
jobdesc
.
getNumberMaps
(
)
;
for
(
int
i
=
0
;
i
<
maps
;
++
i
)
{
final
int
nSpec
=
reds
/
maps
+
(
(
reds
%
maps
)
>
i
?
1
:
0
)
;
final
long
[
]
redDurations
=
new
long
[
nSpec
]
;
for
(
int
j
=
0
;
j
<
nSpec
;
++
j
)
{
final
ReduceTaskAttemptInfo
info
=
(
ReduceTaskAttemptInfo
)
getSuccessfulAttemptInfo
(
TaskType
.
REDUCE
,
i
+
j
*
maps
)
;
redDurations
[
j
]
=
Math
.
min
(
reduceMaxSleepTime
,
info
.
getMergeRuntime
(
)
+
info
.
getReduceRuntime
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
addJobStats
(
JobStats
stats
)
{
int
seq
=
GridmixJob
.
getJobSeqId
(
stats
.
getJob
(
)
)
;
if
(
seq
<
0
)
{
protected
void
checkLoadAndGetSlotsToBackfill
(
)
throws
IOException
,
InterruptedException
{
if
(
loadStatus
.
getJobLoad
(
)
<=
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
System
.
currentTimeMillis
(
)
+
+
Boolean
.
TRUE
.
toString
(
)
+
+
loadStatus
.
getJobLoad
(
)
)
;
}
return
;
}
int
mapCapacity
=
loadStatus
.
getMapCapacity
(
)
;
int
reduceCapacity
=
loadStatus
.
getReduceCapacity
(
)
;
if
(
mapCapacity
<
0
||
reduceCapacity
<
0
)
{
return
;
}
int
maxMapLoad
=
(
int
)
(
overloadMapTaskMapSlotRatio
*
mapCapacity
)
;
int
maxReduceLoad
=
(
int
)
(
overloadReduceTaskReduceSlotRatio
*
reduceCapacity
)
;
int
totalMapTasks
=
ClusterStats
.
getSubmittedMapTasks
(
)
;
int
totalReduceTasks
=
ClusterStats
.
getSubmittedReduceTasks
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
totalMapTasks
)
;
if
(
loadStatus
.
getJobLoad
(
)
<=
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
System
.
currentTimeMillis
(
)
+
+
Boolean
.
TRUE
.
toString
(
)
+
+
loadStatus
.
getJobLoad
(
)
)
;
}
return
;
}
int
mapCapacity
=
loadStatus
.
getMapCapacity
(
)
;
int
reduceCapacity
=
loadStatus
.
getReduceCapacity
(
)
;
if
(
mapCapacity
<
0
||
reduceCapacity
<
0
)
{
return
;
}
int
maxMapLoad
=
(
int
)
(
overloadMapTaskMapSlotRatio
*
mapCapacity
)
;
int
maxReduceLoad
=
(
int
)
(
overloadReduceTaskReduceSlotRatio
*
reduceCapacity
)
;
int
totalMapTasks
=
ClusterStats
.
getSubmittedMapTasks
(
)
;
int
totalReduceTasks
=
ClusterStats
.
getSubmittedReduceTasks
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
totalMapTasks
)
;
LOG
.
debug
(
+
totalReduceTasks
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
System
.
currentTimeMillis
(
)
+
+
Boolean
.
TRUE
.
toString
(
)
+
+
loadStatus
.
getJobLoad
(
)
)
;
}
return
;
}
int
mapCapacity
=
loadStatus
.
getMapCapacity
(
)
;
int
reduceCapacity
=
loadStatus
.
getReduceCapacity
(
)
;
if
(
mapCapacity
<
0
||
reduceCapacity
<
0
)
{
return
;
}
int
maxMapLoad
=
(
int
)
(
overloadMapTaskMapSlotRatio
*
mapCapacity
)
;
int
maxReduceLoad
=
(
int
)
(
overloadReduceTaskReduceSlotRatio
*
reduceCapacity
)
;
int
totalMapTasks
=
ClusterStats
.
getSubmittedMapTasks
(
)
;
int
totalReduceTasks
=
ClusterStats
.
getSubmittedReduceTasks
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
totalMapTasks
)
;
LOG
.
debug
(
+
totalReduceTasks
)
;
LOG
.
debug
(
+
maxMapLoad
)
;
int
currentReduceSlotsBackFill
=
(
int
)
(
maxReduceLoad
-
incompleteReduceTasks
)
;
if
(
currentReduceSlotsBackFill
<=
0
)
{
incompleteMapTasks
=
totalMapTasks
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
}
break
;
}
}
else
{
LOG
.
warn
(
+
id
)
;
blacklistedJobs
.
add
(
id
)
;
}
}
mapSlotsBackFill
=
(
int
)
(
maxMapLoad
-
incompleteMapTasks
)
;
reduceSlotsBackFill
=
(
int
)
(
maxReduceLoad
-
incompleteReduceTasks
)
;
blacklistedJobs
.
retainAll
(
seenJobIDs
)
;
if
(
LOG
.
isDebugEnabled
(
)
&&
blacklistedJobs
.
size
(
)
>
0
)
{
LOG
.
debug
(
+
blacklistedJobs
.
size
(
)
)
;
}
}
loadStatus
.
updateMapLoad
(
mapSlotsBackFill
)
;
public
static
void
createHomeAndStagingDirectory
(
String
user
,
Configuration
conf
)
{
try
{
FileSystem
fs
=
dfsCluster
.
getFileSystem
(
)
;
String
path
=
+
user
;
Path
homeDirectory
=
new
Path
(
path
)
;
if
(
!
fs
.
exists
(
homeDirectory
)
)
{
@
BeforeClass
public
static
void
setup
(
)
throws
IOException
{
final
Configuration
conf
=
new
Configuration
(
)
;
final
FileSystem
fs
=
FileSystem
.
getLocal
(
conf
)
.
getRaw
(
)
;
fs
.
delete
(
base
,
true
)
;
final
Random
r
=
new
Random
(
)
;
final
long
seed
=
r
.
nextLong
(
)
;
r
.
setSeed
(
seed
)
;
@
SuppressWarnings
(
{
,
}
)
@
Test
(
timeout
=
30000
)
public
void
testSleepMapper
(
)
throws
Exception
{
SleepJob
.
SleepMapper
test
=
new
SleepJob
.
SleepMapper
(
)
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setInt
(
JobContext
.
NUM_REDUCES
,
2
)
;
CompressionEmulationUtil
.
setCompressionEmulationEnabled
(
conf
,
true
)
;
conf
.
setBoolean
(
MRJobConfig
.
MAP_OUTPUT_COMPRESS
,
true
)
;
TaskAttemptID
taskId
=
new
TaskAttemptID
(
)
;
FakeRecordLLReader
reader
=
new
FakeRecordLLReader
(
)
;
LoadRecordGkNullWriter
writer
=
new
LoadRecordGkNullWriter
(
)
;
OutputCommitter
committer
=
new
CustomOutputCommitter
(
)
;
StatusReporter
reporter
=
new
TaskAttemptContextImpl
.
DummyReporter
(
)
;
SleepSplit
split
=
getSleepSplit
(
)
;
MapContext
<
LongWritable
,
LongWritable
,
GridmixKey
,
NullWritable
>
mapcontext
=
new
MapContextImpl
<
LongWritable
,
LongWritable
,
GridmixKey
,
NullWritable
>
(
conf
,
taskId
,
reader
,
writer
,
committer
,
reporter
,
split
)
;
Context
context
=
new
WrappedMapper
<
LongWritable
,
LongWritable
,
GridmixKey
,
NullWritable
>
(
)
.
getMapContext
(
mapcontext
)
;
long
start
=
System
.
currentTimeMillis
(
)
;
CompressionEmulationUtil
.
setCompressionEmulationEnabled
(
conf
,
true
)
;
conf
.
setBoolean
(
MRJobConfig
.
MAP_OUTPUT_COMPRESS
,
true
)
;
TaskAttemptID
taskId
=
new
TaskAttemptID
(
)
;
FakeRecordLLReader
reader
=
new
FakeRecordLLReader
(
)
;
LoadRecordGkNullWriter
writer
=
new
LoadRecordGkNullWriter
(
)
;
OutputCommitter
committer
=
new
CustomOutputCommitter
(
)
;
StatusReporter
reporter
=
new
TaskAttemptContextImpl
.
DummyReporter
(
)
;
SleepSplit
split
=
getSleepSplit
(
)
;
MapContext
<
LongWritable
,
LongWritable
,
GridmixKey
,
NullWritable
>
mapcontext
=
new
MapContextImpl
<
LongWritable
,
LongWritable
,
GridmixKey
,
NullWritable
>
(
conf
,
taskId
,
reader
,
writer
,
committer
,
reporter
,
split
)
;
Context
context
=
new
WrappedMapper
<
LongWritable
,
LongWritable
,
GridmixKey
,
NullWritable
>
(
)
.
getMapContext
(
mapcontext
)
;
long
start
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
start
)
;
LongWritable
key
=
new
LongWritable
(
start
+
2000
)
;
LongWritable
value
=
new
LongWritable
(
start
+
2000
)
;
test
.
map
(
key
,
value
,
context
)
;
static
void
lengthTest
(
GridmixRecord
x
,
GridmixRecord
y
,
int
min
,
int
max
)
throws
Exception
{
final
Random
r
=
new
Random
(
)
;
final
long
seed
=
r
.
nextLong
(
)
;
r
.
setSeed
(
seed
)
;
static
void
randomReplayTest
(
GridmixRecord
x
,
GridmixRecord
y
,
int
min
,
int
max
)
throws
Exception
{
final
Random
r
=
new
Random
(
)
;
final
long
seed
=
r
.
nextLong
(
)
;
r
.
setSeed
(
seed
)
;
static
void
eqSeedTest
(
GridmixRecord
x
,
GridmixRecord
y
,
int
max
)
throws
Exception
{
final
Random
r
=
new
Random
(
)
;
final
long
s
=
r
.
nextLong
(
)
;
r
.
setSeed
(
s
)
;
static
void
binSortTest
(
GridmixRecord
x
,
GridmixRecord
y
,
int
min
,
int
max
,
WritableComparator
cmp
)
throws
Exception
{
final
Random
r
=
new
Random
(
)
;
final
long
s
=
r
.
nextLong
(
)
;
r
.
setSeed
(
s
)
;
static
void
checkSpec
(
GridmixKey
a
,
GridmixKey
b
)
throws
Exception
{
final
Random
r
=
new
Random
(
)
;
final
long
s
=
r
.
nextLong
(
)
;
r
.
setSeed
(
s
)
;
@
Test
(
timeout
=
500000
)
public
void
testReplaySubmit
(
)
throws
Exception
{
policy
=
GridmixJobSubmissionPolicy
.
REPLAY
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
)
;
doSubmission
(
null
,
false
)
;
@
Test
(
timeout
=
500000
)
public
void
testStressSubmit
(
)
throws
Exception
{
policy
=
GridmixJobSubmissionPolicy
.
STRESS
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
)
;
doSubmission
(
null
,
false
)
;
@
Test
(
timeout
=
500000
)
public
void
testSerialSubmit
(
)
throws
Exception
{
policy
=
GridmixJobSubmissionPolicy
.
SERIAL
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
)
;
doSubmission
(
JobCreator
.
LOADJOB
.
name
(
)
,
false
)
;
@
Test
(
timeout
=
500000
)
public
void
testReplaySubmit
(
)
throws
Exception
{
policy
=
GridmixJobSubmissionPolicy
.
REPLAY
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
)
;
doSubmission
(
JobCreator
.
LOADJOB
.
name
(
)
,
false
)
;
public
static
void
testFactory
(
long
targetBytes
,
long
targetRecs
)
throws
Exception
{
final
Configuration
conf
=
new
Configuration
(
)
;
final
GridmixKey
key
=
new
GridmixKey
(
)
;
final
GridmixRecord
val
=
new
GridmixRecord
(
)
;
@
Test
public
void
testSerialSubmit
(
)
throws
Exception
{
policy
=
GridmixJobSubmissionPolicy
.
SERIAL
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
)
;
doSubmission
(
JobCreator
.
SLEEPJOB
.
name
(
)
,
false
)
;
@
Test
public
void
testReplaySubmit
(
)
throws
Exception
{
policy
=
GridmixJobSubmissionPolicy
.
REPLAY
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
)
;
doSubmission
(
JobCreator
.
SLEEPJOB
.
name
(
)
,
false
)
;
@
Test
public
void
testStressSubmit
(
)
throws
Exception
{
policy
=
GridmixJobSubmissionPolicy
.
STRESS
;
LOG
.
info
(
+
System
.
currentTimeMillis
(
)
)
;
doSubmission
(
JobCreator
.
SLEEPJOB
.
name
(
)
,
false
)
;
@
Override
public
void
init
(
SubsetConfiguration
conf
)
{
Properties
props
=
new
Properties
(
)
;
brokerList
=
conf
.
getString
(
BROKER_LIST
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
Instant
instant
=
Instant
.
ofEpochMilli
(
timestamp
)
;
LocalDateTime
ldt
=
LocalDateTime
.
ofInstant
(
instant
,
zoneId
)
;
String
date
=
ldt
.
format
(
dateFormat
)
;
String
time
=
ldt
.
format
(
timeFormat
)
;
jsonLines
.
append
(
+
hostname
)
;
jsonLines
.
append
(
+
timestamp
)
;
jsonLines
.
append
(
+
date
)
;
jsonLines
.
append
(
+
time
)
;
jsonLines
.
append
(
+
record
.
name
(
)
+
)
;
for
(
MetricsTag
tag
:
record
.
tags
(
)
)
{
jsonLines
.
append
(
+
tag
.
name
(
)
.
toString
(
)
.
replaceAll
(
,
)
+
)
;
jsonLines
.
append
(
+
tag
.
value
(
)
.
toString
(
)
+
)
;
}
for
(
AbstractMetric
metric
:
record
.
metrics
(
)
)
{
jsonLines
.
append
(
+
metric
.
name
(
)
.
toString
(
)
.
replaceAll
(
,
)
+
)
;
jsonLines
.
append
(
+
metric
.
value
(
)
.
toString
(
)
+
)
;
return
null
;
}
@
Override
public
void
visit
(
MetricsVisitor
visitor
)
{
}
}
;
Iterable
<
AbstractMetric
>
metrics
=
Lists
.
newArrayList
(
metric
)
;
when
(
record
.
name
(
)
)
.
thenReturn
(
)
;
when
(
record
.
metrics
(
)
)
.
thenReturn
(
metrics
)
;
SubsetConfiguration
conf
=
mock
(
SubsetConfiguration
.
class
)
;
when
(
conf
.
getString
(
KafkaSink
.
BROKER_LIST
)
)
.
thenReturn
(
)
;
String
topic
=
;
when
(
conf
.
getString
(
KafkaSink
.
TOPIC
)
)
.
thenReturn
(
topic
)
;
kafkaSink
=
new
KafkaSink
(
)
;
kafkaSink
.
init
(
conf
)
;
Producer
<
Integer
,
byte
[
]
>
mockProducer
=
mock
(
KafkaProducer
.
class
)
;
kafkaSink
.
setProducer
(
mockProducer
)
;
StringBuilder
jsonLines
=
recordToJson
(
record
)
;
private
synchronized
boolean
release
(
String
reason
,
Exception
ex
)
throws
IOException
{
if
(
!
released
)
{
reasonClosed
=
reason
;
try
{
private
void
setAuthDetails
(
URI
endpoint
,
URI
objectLocation
,
AccessToken
authToken
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
StringEntity
getAuthenticationRequst
(
AuthenticationRequest
authenticationRequest
)
throws
IOException
{
final
String
data
=
JSONUtil
.
toJSON
(
new
AuthenticationRequestWrapper
(
authenticationRequest
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
<
M
extends
HttpUriRequest
>
IOException
buildException
(
URI
uri
,
M
req
,
HttpResponse
resp
,
int
statusCode
)
{
IOException
fault
;
String
errorMessage
=
String
.
format
(
+
,
req
.
getMethod
(
)
,
uri
,
statusCode
,
resp
.
getStatusLine
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
<
M
extends
HttpUriRequest
>
HttpResponse
exec
(
HttpClient
client
,
M
req
)
throws
IOException
{
HttpResponse
resp
=
execWithDebugOutput
(
req
,
client
)
;
int
statusCode
=
resp
.
getStatusLine
(
)
.
getStatusCode
(
)
;
if
(
(
statusCode
==
HttpStatus
.
SC_UNAUTHORIZED
||
statusCode
==
HttpStatus
.
SC_BAD_REQUEST
)
&&
req
instanceof
AuthPostRequest
&&
!
useKeystoneAuthentication
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
setWorkingDirectory
(
Path
dir
)
{
workingDir
=
makeAbsolute
(
dir
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
return
null
;
}
if
(
start
<
0
||
len
<
0
)
{
throw
new
IllegalArgumentException
(
+
)
;
}
if
(
file
.
getLen
(
)
<=
start
)
{
return
new
BlockLocation
[
0
]
;
}
final
FileStatus
[
]
listOfFileBlocks
=
store
.
listSubPaths
(
file
.
getPath
(
)
,
false
,
true
)
;
List
<
URI
>
locations
=
new
ArrayList
<
URI
>
(
)
;
if
(
listOfFileBlocks
.
length
>
1
)
{
for
(
FileStatus
fileStatus
:
listOfFileBlocks
)
{
if
(
SwiftObjectPath
.
fromPath
(
uri
,
fileStatus
.
getPath
(
)
)
.
equals
(
SwiftObjectPath
.
fromPath
(
uri
,
file
.
getPath
(
)
)
)
)
{
continue
;
}
locations
.
addAll
(
store
.
getObjectLocation
(
fileStatus
.
getPath
(
)
)
)
;
}
}
else
{
locations
=
store
.
getObjectLocation
(
file
.
getPath
(
)
)
;
}
if
(
locations
.
isEmpty
(
)
)
{
@
Override
public
boolean
mkdirs
(
Path
path
,
FsPermission
permission
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
forceMkdir
(
Path
absolutePath
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
FileStatus
[
]
listStatus
(
Path
path
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
rename
(
Path
src
,
Path
dst
)
throws
FileNotFoundException
,
SwiftOperationFailedException
,
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
throw
new
SwiftOperationFailedException
(
)
;
}
final
SwiftFileStatus
srcMetadata
;
srcMetadata
=
getObjectMetadata
(
src
)
;
SwiftFileStatus
dstMetadata
;
try
{
dstMetadata
=
getObjectMetadata
(
dst
)
;
}
catch
(
FileNotFoundException
e
)
{
LOG
.
debug
(
)
;
dstMetadata
=
null
;
}
Path
srcParent
=
src
.
getParent
(
)
;
Path
dstParent
=
dst
.
getParent
(
)
;
if
(
dstParent
!=
null
&&
!
dstParent
.
equals
(
srcParent
)
)
{
SwiftFileStatus
fileStatus
;
try
{
fileStatus
=
getObjectMetadata
(
dstParent
)
;
return
;
}
}
}
else
{
destPath
=
toObjectPath
(
dst
)
;
}
int
childCount
=
childStats
.
size
(
)
;
if
(
childCount
==
0
)
{
copyThenDeleteObject
(
srcObject
,
destPath
)
;
}
else
{
SwiftUtils
.
debug
(
LOG
,
+
)
;
copyObject
(
srcObject
,
destPath
)
;
for
(
FileStatus
stat
:
childStats
)
{
SwiftUtils
.
debug
(
LOG
,
,
stat
)
;
deleteObject
(
stat
.
getPath
(
)
)
;
}
swiftRestClient
.
delete
(
srcObject
)
;
}
}
else
{
if
(
destExists
&&
!
destIsDir
)
{
private
void
logDirectory
(
String
message
,
SwiftObjectPath
objectPath
,
Iterable
<
FileStatus
>
statuses
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
innerClose
(
String
reason
)
throws
IOException
{
try
{
if
(
httpStream
!=
null
)
{
reasonClosed
=
reason
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
finalize
(
)
throws
Throwable
{
if
(
httpStream
!=
null
)
{
throw
new
EOFException
(
FSExceptionMessages
.
NEGATIVE_SEEK
)
;
}
long
offset
=
targetPos
-
pos
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
targetPos
+
+
pos
+
+
offset
)
;
}
if
(
offset
==
0
)
{
LOG
.
debug
(
)
;
return
;
}
if
(
offset
<
0
)
{
LOG
.
debug
(
)
;
}
else
if
(
(
rangeOffset
+
offset
<
bufferSize
)
)
{
SwiftUtils
.
debug
(
LOG
,
+
+
,
pos
,
targetPos
,
offset
,
rangeOffset
)
;
try
{
LOG
.
debug
(
)
;
chompBytes
(
offset
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
)
;
return
;
}
if
(
offset
<
0
)
{
LOG
.
debug
(
)
;
}
else
if
(
(
rangeOffset
+
offset
<
bufferSize
)
)
{
SwiftUtils
.
debug
(
LOG
,
+
+
,
pos
,
targetPos
,
offset
,
rangeOffset
)
;
try
{
LOG
.
debug
(
)
;
chompBytes
(
offset
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
e
)
;
}
if
(
targetPos
-
pos
==
0
)
{
LOG
.
trace
(
)
;
return
;
}
LOG
.
trace
(
)
;
if
(
backupStream
!=
null
)
{
backupStream
.
close
(
)
;
}
if
(
closingUpload
&&
partUpload
&&
backupFile
.
length
(
)
==
0
)
{
SwiftUtils
.
debug
(
LOG
,
)
;
delete
(
backupFile
)
;
}
else
{
partUpload
=
true
;
boolean
uploadSuccess
=
false
;
int
attempt
=
0
;
while
(
!
uploadSuccess
)
{
try
{
++
attempt
;
bytesUploaded
+=
uploadFilePartAttempt
(
attempt
)
;
uploadSuccess
=
true
;
}
catch
(
IOException
e
)
{
public
static
void
noteAction
(
String
action
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
static
void
debug
(
Logger
log
,
String
text
,
Object
...
args
)
{
if
(
log
.
isDebugEnabled
(
)
)
{
public
static
void
debugEx
(
Logger
log
,
String
text
,
Exception
ex
)
{
if
(
log
.
isDebugEnabled
(
)
)
{
@
AfterClass
public
static
void
classTearDown
(
)
throws
Exception
{
if
(
lastFs
!=
null
)
{
List
<
DurationStats
>
statistics
=
lastFs
.
getOperationStatistics
(
)
;
for
(
DurationStats
stat
:
statistics
)
{
private
void
printf
(
String
format
,
Object
...
args
)
{
String
msg
=
String
.
format
(
format
,
args
)
;
System
.
out
.
printf
(
msg
+
)
;
private
void
assertLocationValid
(
BlockLocation
location
)
throws
IOException
{
@
Test
(
timeout
=
SWIFT_TEST_TIMEOUT
)
public
void
testLocateDirectory
(
)
throws
Throwable
{
describe
(
)
;
createFile
(
path
(
)
)
;
FileStatus
status
=
fs
.
getFileStatus
(
path
(
)
)
;
int
firstWriteLen
=
2048
;
out
.
write
(
src
,
0
,
firstWriteLen
)
;
long
expected
=
getExpectedPartitionsWritten
(
firstWriteLen
,
PART_SIZE_BYTES
,
false
)
;
SwiftUtils
.
debug
(
LOG
,
,
expected
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
int
remainder
=
len
-
firstWriteLen
;
SwiftUtils
.
debug
(
LOG
,
,
remainder
)
;
out
.
write
(
src
,
firstWriteLen
,
remainder
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
false
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
out
.
close
(
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
true
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
Header
[
]
headers
=
fs
.
getStore
(
)
.
getObjectHeaders
(
path
,
true
)
;
for
(
Header
header
:
headers
)
{
long
expected
=
getExpectedPartitionsWritten
(
firstWriteLen
,
PART_SIZE_BYTES
,
false
)
;
SwiftUtils
.
debug
(
LOG
,
,
expected
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
int
remainder
=
len
-
firstWriteLen
;
SwiftUtils
.
debug
(
LOG
,
,
remainder
)
;
out
.
write
(
src
,
firstWriteLen
,
remainder
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
false
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
out
.
close
(
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
true
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
Header
[
]
headers
=
fs
.
getStore
(
)
.
getObjectHeaders
(
path
,
true
)
;
for
(
Header
header
:
headers
)
{
LOG
.
info
(
header
.
toString
(
)
)
;
}
byte
[
]
dest
=
readDataset
(
fs
,
path
,
len
)
;
int
firstWriteLen
=
2048
;
out
.
write
(
src
,
0
,
firstWriteLen
)
;
long
expected
=
getExpectedPartitionsWritten
(
firstWriteLen
,
PART_SIZE_BYTES
,
false
)
;
SwiftUtils
.
debug
(
LOG
,
,
expected
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
int
remainder
=
len
-
firstWriteLen
;
SwiftUtils
.
debug
(
LOG
,
,
remainder
)
;
out
.
write
(
src
,
firstWriteLen
,
remainder
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
false
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
out
.
close
(
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
true
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
Header
[
]
headers
=
fs
.
getStore
(
)
.
getObjectHeaders
(
path
,
true
)
;
for
(
Header
header
:
headers
)
{
long
expected
=
getExpectedPartitionsWritten
(
firstWriteLen
,
PART_SIZE_BYTES
,
false
)
;
SwiftUtils
.
debug
(
LOG
,
,
expected
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
int
remainder
=
len
-
firstWriteLen
;
SwiftUtils
.
debug
(
LOG
,
,
remainder
)
;
out
.
write
(
src
,
firstWriteLen
,
remainder
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
false
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
out
.
close
(
)
;
expected
=
getExpectedPartitionsWritten
(
len
,
PART_SIZE_BYTES
,
true
)
;
assertPartitionsWritten
(
,
out
,
expected
)
;
Header
[
]
headers
=
fs
.
getStore
(
)
.
getObjectHeaders
(
path
,
true
)
;
for
(
Header
header
:
headers
)
{
LOG
.
info
(
header
.
toString
(
)
)
;
}
byte
[
]
dest
=
readDataset
(
fs
,
path
,
len
)
;
@
Test
(
timeout
=
SWIFT_TEST_TIMEOUT
)
public
void
testConvertToPath
(
)
throws
Throwable
{
String
initialpath
=
;
Path
ipath
=
new
Path
(
initialpath
)
;
SwiftObjectPath
objectPath
=
SwiftObjectPath
.
fromPath
(
new
URI
(
initialpath
)
,
ipath
)
;
URI
endpoint
=
new
URI
(
ENDPOINT
)
;
URI
uri
=
SwiftRestClient
.
pathToURI
(
objectPath
,
endpoint
)
;
@
Test
(
timeout
=
SWIFT_TEST_TIMEOUT
)
public
void
testConvertToPath
(
)
throws
Throwable
{
String
initialpath
=
;
Path
ipath
=
new
Path
(
initialpath
)
;
SwiftObjectPath
objectPath
=
SwiftObjectPath
.
fromPath
(
new
URI
(
initialpath
)
,
ipath
)
;
URI
endpoint
=
new
URI
(
ENDPOINT
)
;
URI
uri
=
SwiftRestClient
.
pathToURI
(
objectPath
,
endpoint
)
;
LOG
.
info
(
+
initialpath
)
;
@
Test
(
timeout
=
SWIFT_TEST_TIMEOUT
)
public
void
testPutAndDelete
(
)
throws
Throwable
{
assumeEnabled
(
)
;
SwiftRestClient
client
=
createClient
(
)
;
client
.
authenticate
(
)
;
Path
path
=
new
Path
(
)
;
SwiftObjectPath
sobject
=
SwiftObjectPath
.
fromPath
(
serviceURI
,
path
)
;
byte
[
]
stuff
=
new
byte
[
1
]
;
stuff
[
0
]
=
'a'
;
client
.
upload
(
sobject
,
new
ByteArrayInputStream
(
stuff
)
,
stuff
.
length
)
;
Duration
head
=
new
Duration
(
)
;
Header
[
]
responseHeaders
=
client
.
headRequest
(
,
sobject
,
SwiftRestClient
.
NEWEST
)
;
head
.
finished
(
)
;
@
Test
(
timeout
=
SWIFT_TEST_TIMEOUT
)
public
void
testPutAndDelete
(
)
throws
Throwable
{
assumeEnabled
(
)
;
SwiftRestClient
client
=
createClient
(
)
;
client
.
authenticate
(
)
;
Path
path
=
new
Path
(
)
;
SwiftObjectPath
sobject
=
SwiftObjectPath
.
fromPath
(
serviceURI
,
path
)
;
byte
[
]
stuff
=
new
byte
[
1
]
;
stuff
[
0
]
=
'a'
;
client
.
upload
(
sobject
,
new
ByteArrayInputStream
(
stuff
)
,
stuff
.
length
)
;
Duration
head
=
new
Duration
(
)
;
Header
[
]
responseHeaders
=
client
.
headRequest
(
,
sobject
,
SwiftRestClient
.
NEWEST
)
;
head
.
finished
(
)
;
LOG
.
info
(
+
head
)
;
for
(
Header
header
:
responseHeaders
)
{
byte
[
]
stuff
=
new
byte
[
1
]
;
stuff
[
0
]
=
'a'
;
client
.
upload
(
sobject
,
new
ByteArrayInputStream
(
stuff
)
,
stuff
.
length
)
;
Duration
head
=
new
Duration
(
)
;
Header
[
]
responseHeaders
=
client
.
headRequest
(
,
sobject
,
SwiftRestClient
.
NEWEST
)
;
head
.
finished
(
)
;
LOG
.
info
(
+
head
)
;
for
(
Header
header
:
responseHeaders
)
{
LOG
.
info
(
header
.
toString
(
)
)
;
}
client
.
delete
(
sobject
)
;
try
{
Header
[
]
headers
=
client
.
headRequest
(
,
sobject
,
SwiftRestClient
.
NEWEST
)
;
Assert
.
fail
(
+
sobject
)
;
}
catch
(
FileNotFoundException
e
)
{
}
for
(
DurationStats
stats
:
client
.
getOperationStatistics
(
)
)
{
FileStatus
[
]
status2
=
(
FileStatus
[
]
)
fs
.
listStatus
(
dir
)
;
ls2
.
finished
(
)
;
assertEquals
(
,
count
,
status2
.
length
)
;
SwiftTestUtils
.
noteAction
(
)
;
for
(
long
l
=
0
;
l
<
count
;
l
++
)
{
String
name
=
String
.
format
(
format
,
l
)
;
Path
p
=
new
Path
(
dir
,
+
name
)
;
Duration
d
=
new
Duration
(
)
;
String
result
=
SwiftTestUtils
.
readBytesToString
(
fs
,
p
,
name
.
length
(
)
)
;
assertEquals
(
name
,
result
)
;
d
.
finished
(
)
;
readStats
.
add
(
d
)
;
}
SwiftTestUtils
.
noteAction
(
)
;
Duration
rm2
=
new
Duration
(
)
;
fs
.
delete
(
dir
,
true
)
;
assertEquals
(
,
count
,
status2
.
length
)
;
SwiftTestUtils
.
noteAction
(
)
;
for
(
long
l
=
0
;
l
<
count
;
l
++
)
{
String
name
=
String
.
format
(
format
,
l
)
;
Path
p
=
new
Path
(
dir
,
+
name
)
;
Duration
d
=
new
Duration
(
)
;
String
result
=
SwiftTestUtils
.
readBytesToString
(
fs
,
p
,
name
.
length
(
)
)
;
assertEquals
(
name
,
result
)
;
d
.
finished
(
)
;
readStats
.
add
(
d
)
;
}
SwiftTestUtils
.
noteAction
(
)
;
Duration
rm2
=
new
Duration
(
)
;
fs
.
delete
(
dir
,
true
)
;
rm2
.
finished
(
)
;
LOG
.
info
(
String
.
format
(
,
fs
.
getUri
(
)
)
)
;
for
(
long
l
=
0
;
l
<
count
;
l
++
)
{
String
name
=
String
.
format
(
format
,
l
)
;
Path
p
=
new
Path
(
dir
,
+
name
)
;
Duration
d
=
new
Duration
(
)
;
String
result
=
SwiftTestUtils
.
readBytesToString
(
fs
,
p
,
name
.
length
(
)
)
;
assertEquals
(
name
,
result
)
;
d
.
finished
(
)
;
readStats
.
add
(
d
)
;
}
SwiftTestUtils
.
noteAction
(
)
;
Duration
rm2
=
new
Duration
(
)
;
fs
.
delete
(
dir
,
true
)
;
rm2
.
finished
(
)
;
LOG
.
info
(
String
.
format
(
,
fs
.
getUri
(
)
)
)
;
LOG
.
info
(
writeStats
.
toString
(
)
)
;
LOG
.
info
(
readStats
.
toString
(
)
)
;
@
POST
@
Path
(
)
public
void
parseFile
(
@
PathParam
(
)
String
logFile
)
throws
IOException
,
SkylineStoreException
,
ResourceEstimatorException
{
logParserUtil
.
parseLog
(
logFile
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
)
public
String
getHistoryResourceSkyline
(
@
PathParam
(
)
String
pipelineId
,
@
PathParam
(
)
String
runId
)
throws
SkylineStoreException
{
RecurrenceId
recurrenceId
=
new
RecurrenceId
(
pipelineId
,
runId
)
;
Map
<
RecurrenceId
,
List
<
ResourceSkyline
>>
jobHistory
=
skylineStore
.
getHistory
(
recurrenceId
)
;
final
String
skyline
=
gson
.
toJson
(
jobHistory
,
skylineStoreType
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
)
public
String
getEstimatedResourceAllocation
(
@
PathParam
(
)
String
pipelineId
)
throws
SkylineStoreException
{
RLESparseResourceAllocation
result
=
skylineStore
.
getEstimation
(
pipelineId
)
;
final
String
skyline
=
gson
.
toJson
(
result
,
rleType
)
;
@
DELETE
@
Path
(
)
public
void
deleteHistoryResourceSkyline
(
@
PathParam
(
)
String
pipelineId
,
@
PathParam
(
)
String
runId
)
throws
SkylineStoreException
{
RecurrenceId
recurrenceId
=
new
RecurrenceId
(
pipelineId
,
runId
)
;
skylineStore
.
deleteHistory
(
recurrenceId
)
;
inputValidator
.
validate
(
recurrenceId
,
resourceSkylines
)
;
writeLock
.
lock
(
)
;
try
{
final
List
<
ResourceSkyline
>
filteredInput
=
eliminateNull
(
resourceSkylines
)
;
if
(
filteredInput
.
size
(
)
>
0
)
{
if
(
skylineStore
.
containsKey
(
recurrenceId
)
)
{
final
List
<
ResourceSkyline
>
jobHistory
=
skylineStore
.
get
(
recurrenceId
)
;
final
List
<
String
>
oldJobIds
=
new
ArrayList
<
>
(
)
;
for
(
final
ResourceSkyline
resourceSkyline
:
jobHistory
)
{
oldJobIds
.
add
(
resourceSkyline
.
getJobId
(
)
)
;
}
if
(
!
oldJobIds
.
isEmpty
(
)
)
{
for
(
ResourceSkyline
elem
:
filteredInput
)
{
if
(
oldJobIds
.
contains
(
elem
.
getJobId
(
)
)
)
{
StringBuilder
errMsg
=
new
StringBuilder
(
)
;
errMsg
.
append
(
+
recurrenceId
+
)
;
final
List
<
ResourceSkyline
>
filteredInput
=
eliminateNull
(
resourceSkylines
)
;
if
(
filteredInput
.
size
(
)
>
0
)
{
if
(
skylineStore
.
containsKey
(
recurrenceId
)
)
{
final
List
<
ResourceSkyline
>
jobHistory
=
skylineStore
.
get
(
recurrenceId
)
;
final
List
<
String
>
oldJobIds
=
new
ArrayList
<
>
(
)
;
for
(
final
ResourceSkyline
resourceSkyline
:
jobHistory
)
{
oldJobIds
.
add
(
resourceSkyline
.
getJobId
(
)
)
;
}
if
(
!
oldJobIds
.
isEmpty
(
)
)
{
for
(
ResourceSkyline
elem
:
filteredInput
)
{
if
(
oldJobIds
.
contains
(
elem
.
getJobId
(
)
)
)
{
StringBuilder
errMsg
=
new
StringBuilder
(
)
;
errMsg
.
append
(
+
recurrenceId
+
)
;
LOGGER
.
error
(
errMsg
.
toString
(
)
)
;
throw
new
DuplicateRecurrenceIdException
(
errMsg
.
toString
(
)
)
;
}
}
}
skylineStore
.
get
(
recurrenceId
)
.
addAll
(
filteredInput
)
;
final
List
<
ResourceSkyline
>
jobHistory
=
skylineStore
.
get
(
recurrenceId
)
;
final
List
<
String
>
oldJobIds
=
new
ArrayList
<
>
(
)
;
for
(
final
ResourceSkyline
resourceSkyline
:
jobHistory
)
{
oldJobIds
.
add
(
resourceSkyline
.
getJobId
(
)
)
;
}
if
(
!
oldJobIds
.
isEmpty
(
)
)
{
for
(
ResourceSkyline
elem
:
filteredInput
)
{
if
(
oldJobIds
.
contains
(
elem
.
getJobId
(
)
)
)
{
StringBuilder
errMsg
=
new
StringBuilder
(
)
;
errMsg
.
append
(
+
recurrenceId
+
)
;
LOGGER
.
error
(
errMsg
.
toString
(
)
)
;
throw
new
DuplicateRecurrenceIdException
(
errMsg
.
toString
(
)
)
;
}
}
}
skylineStore
.
get
(
recurrenceId
)
.
addAll
(
filteredInput
)
;
LOGGER
.
info
(
,
recurrenceId
)
;
}
else
{
skylineStore
.
put
(
recurrenceId
,
filteredInput
)
;
@
Override
public
void
addEstimation
(
String
pipelineId
,
RLESparseResourceAllocation
resourceSkyline
)
throws
SkylineStoreException
{
inputValidator
.
validate
(
pipelineId
,
resourceSkyline
)
;
writeLock
.
lock
(
)
;
try
{
estimationStore
.
put
(
pipelineId
,
resourceSkyline
)
;
@
Override
public
final
void
updateHistory
(
final
RecurrenceId
recurrenceId
,
final
List
<
ResourceSkyline
>
resourceSkylines
)
throws
SkylineStoreException
{
inputValidator
.
validate
(
recurrenceId
,
resourceSkylines
)
;
writeLock
.
lock
(
)
;
try
{
if
(
skylineStore
.
containsKey
(
recurrenceId
)
)
{
List
<
ResourceSkyline
>
filteredInput
=
eliminateNull
(
resourceSkylines
)
;
if
(
filteredInput
.
size
(
)
>
0
)
{
skylineStore
.
put
(
recurrenceId
,
filteredInput
)
;
writeLock
.
lock
(
)
;
try
{
if
(
skylineStore
.
containsKey
(
recurrenceId
)
)
{
List
<
ResourceSkyline
>
filteredInput
=
eliminateNull
(
resourceSkylines
)
;
if
(
filteredInput
.
size
(
)
>
0
)
{
skylineStore
.
put
(
recurrenceId
,
filteredInput
)
;
LOGGER
.
info
(
,
recurrenceId
)
;
}
else
{
StringBuilder
errMsg
=
new
StringBuilder
(
)
;
errMsg
.
append
(
+
recurrenceId
+
)
;
LOGGER
.
error
(
errMsg
.
toString
(
)
)
;
throw
new
EmptyResourceSkylineException
(
errMsg
.
toString
(
)
)
;
}
}
else
{
StringBuilder
errMsg
=
new
StringBuilder
(
)
;
errMsg
.
append
(
+
recurrenceId
)
;
@
Override
public
final
Map
<
RecurrenceId
,
List
<
ResourceSkyline
>>
getHistory
(
final
RecurrenceId
recurrenceId
)
throws
SkylineStoreException
{
inputValidator
.
validate
(
recurrenceId
)
;
readLock
.
lock
(
)
;
try
{
String
pipelineId
=
recurrenceId
.
getPipelineId
(
)
;
if
(
pipelineId
.
equals
(
)
)
{
inputValidator
.
validate
(
recurrenceId
)
;
readLock
.
lock
(
)
;
try
{
String
pipelineId
=
recurrenceId
.
getPipelineId
(
)
;
if
(
pipelineId
.
equals
(
)
)
{
LOGGER
.
info
(
,
recurrenceId
)
;
return
Collections
.
unmodifiableMap
(
skylineStore
)
;
}
String
runId
=
recurrenceId
.
getRunId
(
)
;
Map
<
RecurrenceId
,
List
<
ResourceSkyline
>>
result
=
new
HashMap
<
RecurrenceId
,
List
<
ResourceSkyline
>>
(
)
;
if
(
runId
.
equals
(
)
)
{
for
(
Map
.
Entry
<
RecurrenceId
,
List
<
ResourceSkyline
>>
entry
:
skylineStore
.
entrySet
(
)
)
{
RecurrenceId
index
=
entry
.
getKey
(
)
;
if
(
index
.
getPipelineId
(
)
.
equals
(
pipelineId
)
)
{
result
.
put
(
index
,
entry
.
getValue
(
)
)
;
}
}
if
(
result
.
size
(
)
>
0
)
{
for
(
Map
.
Entry
<
RecurrenceId
,
List
<
ResourceSkyline
>>
entry
:
skylineStore
.
entrySet
(
)
)
{
RecurrenceId
index
=
entry
.
getKey
(
)
;
if
(
index
.
getPipelineId
(
)
.
equals
(
pipelineId
)
)
{
result
.
put
(
index
,
entry
.
getValue
(
)
)
;
}
}
if
(
result
.
size
(
)
>
0
)
{
LOGGER
.
info
(
,
recurrenceId
)
;
return
Collections
.
unmodifiableMap
(
result
)
;
}
else
{
LOGGER
.
warn
(
,
recurrenceId
)
;
return
null
;
}
}
if
(
skylineStore
.
containsKey
(
recurrenceId
)
)
{
result
.
put
(
recurrenceId
,
skylineStore
.
get
(
recurrenceId
)
)
;
}
else
{
LOGGER
.
warn
(
,
recurrenceId
)
;
return
null
;
public
final
void
validate
(
final
RecurrenceId
recurrenceId
)
throws
SkylineStoreException
{
if
(
recurrenceId
==
null
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
+
)
;
public
final
void
validate
(
final
String
pipelineId
)
throws
SkylineStoreException
{
if
(
pipelineId
==
null
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
+
)
;
public
final
void
validate
(
final
RecurrenceId
recurrenceId
,
final
List
<
ResourceSkyline
>
resourceSkylines
)
throws
SkylineStoreException
{
validate
(
recurrenceId
)
;
if
(
resourceSkylines
==
null
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
+
recurrenceId
+
+
)
;
public
final
void
validate
(
final
String
pipelineId
,
final
RLESparseResourceAllocation
resourceOverTime
)
throws
SkylineStoreException
{
validate
(
pipelineId
)
;
if
(
resourceOverTime
==
null
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
+
pipelineId
+
)
;
indexJobITimeK
=
indexJobI
*
jobLen
+
timeK
;
cJobITimeK
=
containerNums
[
timeK
]
;
regularizationConstraint
.
set
(
uaPredict
[
indexJobITimeK
]
,
1
/
cJobI
)
;
generateOverAllocationConstraints
(
lpModel
,
cJobITimeK
,
oa
,
x
,
indexJobITimeK
,
timeK
)
;
generateUnderAllocationConstraints
(
lpModel
,
cJobITimeK
,
uaPredict
,
ua
,
x
,
indexJobITimeK
,
timeK
)
;
}
}
Expression
objective
=
lpModel
.
addExpression
(
)
;
generateObjective
(
objective
,
numJobs
,
jobLen
,
oa
,
ua
,
eps
)
;
final
Result
lpResult
=
lpModel
.
minimise
(
)
;
final
TreeMap
<
Long
,
Resource
>
treeMap
=
new
TreeMap
<
>
(
)
;
RLESparseResourceAllocation
result
=
new
RLESparseResourceAllocation
(
treeMap
,
new
DefaultResourceCalculator
(
)
)
;
ReservationInterval
riAdd
;
Resource
containerSpec
=
resourceSkylines
.
get
(
0
)
.
getContainerSpec
(
)
;
String
pipelineId
=
(
(
RecurrenceId
)
jobHistory
.
keySet
(
)
.
toArray
(
)
[
0
]
)
.
getPipelineId
(
)
;
Resource
resource
;
for
(
int
indexTimeK
=
0
;
indexTimeK
<
jobLen
;
indexTimeK
++
)
{
}
Queue
<
Pair
<
LoggedJob
,
JobTraceReader
>>
heap
=
new
PriorityQueue
<
Pair
<
LoggedJob
,
JobTraceReader
>>
(
)
;
try
{
LoggedJob
job
=
reader
.
nextJob
(
)
;
if
(
job
==
null
)
{
LOG
.
error
(
)
;
return
EMPTY_JOB_TRACE
;
}
if
(
startsAfter
>
0
)
{
LOG
.
info
(
+
job
.
getSubmitTime
(
)
)
;
long
approximateTime
=
job
.
getSubmitTime
(
)
+
startsAfter
;
job
=
reader
.
nextJob
(
)
;
long
skippedCount
=
0
;
while
(
job
!=
null
&&
job
.
getSubmitTime
(
)
<
approximateTime
)
{
job
=
reader
.
nextJob
(
)
;
skippedCount
++
;
}
LOG
.
debug
(
+
startsAfter
+
+
skippedCount
+
)
;
LoggedJob
job
=
reader
.
nextJob
(
)
;
if
(
job
==
null
)
{
LOG
.
error
(
)
;
return
EMPTY_JOB_TRACE
;
}
if
(
startsAfter
>
0
)
{
LOG
.
info
(
+
job
.
getSubmitTime
(
)
)
;
long
approximateTime
=
job
.
getSubmitTime
(
)
+
startsAfter
;
job
=
reader
.
nextJob
(
)
;
long
skippedCount
=
0
;
while
(
job
!=
null
&&
job
.
getSubmitTime
(
)
<
approximateTime
)
{
job
=
reader
.
nextJob
(
)
;
skippedCount
++
;
}
LOG
.
debug
(
+
startsAfter
+
+
skippedCount
+
)
;
if
(
job
==
null
)
{
LOG
.
error
(
+
+
startsAfter
+
)
;
long
approximateTime
=
job
.
getSubmitTime
(
)
+
startsAfter
;
job
=
reader
.
nextJob
(
)
;
long
skippedCount
=
0
;
while
(
job
!=
null
&&
job
.
getSubmitTime
(
)
<
approximateTime
)
{
job
=
reader
.
nextJob
(
)
;
skippedCount
++
;
}
LOG
.
debug
(
+
startsAfter
+
+
skippedCount
+
)
;
if
(
job
==
null
)
{
LOG
.
error
(
+
+
startsAfter
+
)
;
return
EMPTY_JOB_TRACE
;
}
LOG
.
info
(
+
job
.
getSubmitTime
(
)
)
;
}
firstJobSubmitTime
=
job
.
getSubmitTime
(
)
;
long
lastJobSubmitTime
=
firstJobSubmitTime
;
int
numberJobs
=
0
;
long
currentIntervalEnd
=
Long
.
MIN_VALUE
;
long
lastJobSubmitTime
=
firstJobSubmitTime
;
int
numberJobs
=
0
;
long
currentIntervalEnd
=
Long
.
MIN_VALUE
;
Path
nextSegment
=
null
;
Outputter
<
LoggedJob
>
tempGen
=
null
;
if
(
debug
)
{
LOG
.
debug
(
+
firstJobSubmitTime
)
;
}
final
Configuration
conf
=
getConf
(
)
;
try
{
while
(
job
!=
null
)
{
final
Random
tempNameGenerator
=
new
Random
(
)
;
lastJobSubmitTime
=
job
.
getSubmitTime
(
)
;
++
numberJobs
;
if
(
job
.
getSubmitTime
(
)
>=
currentIntervalEnd
)
{
if
(
tempGen
!=
null
)
{
final
Random
tempNameGenerator
=
new
Random
(
)
;
lastJobSubmitTime
=
job
.
getSubmitTime
(
)
;
++
numberJobs
;
if
(
job
.
getSubmitTime
(
)
>=
currentIntervalEnd
)
{
if
(
tempGen
!=
null
)
{
tempGen
.
close
(
)
;
}
nextSegment
=
null
;
for
(
int
i
=
0
;
i
<
3
&&
nextSegment
==
null
;
++
i
)
{
try
{
nextSegment
=
new
Path
(
tempDir
,
+
tempNameGenerator
.
nextLong
(
)
+
)
;
if
(
debug
)
{
LOG
.
debug
(
+
nextSegment
)
;
}
FileSystem
fs
=
nextSegment
.
getFileSystem
(
conf
)
;
try
{
if
(
!
fs
.
exists
(
nextSegment
)
)
{
private
boolean
setNextDirectoryInputStream
(
)
throws
FileNotFoundException
,
IOException
{
if
(
input
!=
null
)
{
input
.
close
(
)
;
LOG
.
info
(
+
currentFileName
)
;
input
=
null
;
}
if
(
inputCodec
!=
null
)
{
CodecPool
.
returnDecompressor
(
inputDecompressor
)
;
inputDecompressor
=
null
;
inputCodec
=
null
;
}
++
inputDirectoryCursor
;
if
(
inputDirectoryCursor
>=
inputDirectoryFiles
.
length
)
{
return
false
;
}
fileFirstLine
=
true
;
currentFileName
=
inputDirectoryFiles
[
inputDirectoryCursor
]
;
LOG
.
info
(
+
currentFileName
+
)
;
}
}
task
.
setPreferredLocations
(
locations
)
;
}
task
.
setTaskID
(
taskID
)
;
if
(
startTime
!=
null
)
{
task
.
setStartTime
(
Long
.
parseLong
(
startTime
)
)
;
}
if
(
finishTime
!=
null
)
{
task
.
setFinishTime
(
Long
.
parseLong
(
finishTime
)
)
;
}
Pre21JobHistoryConstants
.
Values
typ
;
Pre21JobHistoryConstants
.
Values
stat
;
try
{
stat
=
status
==
null
?
null
:
Pre21JobHistoryConstants
.
Values
.
valueOf
(
status
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
error
(
+
status
+
,
e
)
;
stat
=
null
;
}
task
.
setTaskStatus
(
stat
)
;
try
{
jobBeingTraced
.
getMapTasks
(
)
.
add
(
task
)
;
tasksInCurrentJob
.
put
(
taskID
,
task
)
;
}
task
.
setTaskID
(
taskID
)
;
LoggedTaskAttempt
attempt
=
attemptsInCurrentJob
.
get
(
attemptID
)
;
boolean
attemptAlreadyExists
=
attempt
!=
null
;
if
(
attempt
==
null
)
{
attempt
=
new
LoggedTaskAttempt
(
)
;
attempt
.
setAttemptID
(
attemptID
)
;
}
if
(
!
attemptAlreadyExists
)
{
attemptsInCurrentJob
.
put
(
attemptID
,
attempt
)
;
task
.
getAttempts
(
)
.
add
(
attempt
)
;
}
Pre21JobHistoryConstants
.
Values
stat
=
null
;
try
{
stat
=
status
==
null
?
null
:
Pre21JobHistoryConstants
.
Values
.
valueOf
(
status
)
;
}
catch
(
IllegalArgumentException
e
)
{
int
run
(
)
throws
IOException
{
Pair
<
String
,
String
>
line
=
readBalancedLine
(
)
;
while
(
line
!=
null
)
{
if
(
debug
&&
(
lineNumber
<
1000000L
&&
lineNumber
%
1000L
==
0
||
lineNumber
%
1000000L
==
0
)
)
{
try
{
HadoopLogsAnalyzer
analyzer
=
new
HadoopLogsAnalyzer
(
)
;
int
result
=
ToolRunner
.
run
(
analyzer
,
args
)
;
if
(
result
==
0
)
{
return
;
}
System
.
exit
(
result
)
;
}
catch
(
FileNotFoundException
e
)
{
LOG
.
error
(
,
e
)
;
e
.
printStackTrace
(
staticDebugOutput
)
;
System
.
exit
(
1
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
e
.
printStackTrace
(
staticDebugOutput
)
;
System
.
exit
(
2
)
;
}
catch
(
Exception
e
)
{
void
dumpParsedTask
(
)
{
LOG
.
info
(
+
obtainCounters
(
)
+
+
obtainFailedDueToAttemptId
(
)
+
)
;
List
<
LoggedLocation
>
loc
=
getPreferredLocations
(
)
;
for
(
LoggedLocation
l
:
loc
)
{
private
void
printSimulationInfo
(
)
{
if
(
printSimulation
)
{
LOG
.
info
(
)
;
LOG
.
info
(
+
,
numNMs
,
numRacks
,
nodeManagerResource
)
;
LOG
.
info
(
)
;
LOG
.
info
(
+
,
numAMs
,
numTasks
,
(
int
)
(
Math
.
ceil
(
(
numTasks
+
0.0
)
/
numAMs
)
)
)
;
LOG
.
info
(
)
;
for
(
Map
.
Entry
<
String
,
AMSimulator
>
entry
:
amMap
.
entrySet
(
)
)
{
AMSimulator
am
=
entry
.
getValue
(
)
;
private
ReservationId
submitReservationWhenSpecified
(
)
throws
IOException
,
InterruptedException
{
if
(
reservationRequest
!=
null
)
{
UserGroupInformation
ugi
=
UserGroupInformation
.
createRemoteUser
(
user
)
;
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
Object
>
(
)
{
@
Override
public
Object
run
(
)
throws
YarnException
,
IOException
{
rm
.
getClientRMService
(
)
.
submitReservation
(
reservationRequest
)
;
@
SuppressWarnings
(
)
public
void
init
(
int
heartbeatInterval
,
List
<
ContainerSimulator
>
containerList
,
ResourceManager
resourceManager
,
SLSRunner
slsRunnner
,
long
startTime
,
long
finishTime
,
String
simUser
,
String
simQueue
,
boolean
tracked
,
String
oldApp
,
long
baseTimeMS
,
Resource
amResource
,
String
nodeLabelExpr
,
Map
<
String
,
String
>
params
,
Map
<
ApplicationId
,
AMSimulator
>
appIdAMSim
)
{
super
.
init
(
heartbeatInterval
,
containerList
,
resourceManager
,
slsRunnner
,
startTime
,
finishTime
,
simUser
,
simQueue
,
tracked
,
oldApp
,
baseTimeMS
,
amResource
,
nodeLabelExpr
,
params
,
appIdAMSim
)
;
super
.
amtype
=
;
allContainers
.
addAll
(
containerList
)
;
pendingContainers
.
addAll
(
containerList
)
;
totalContainers
=
allContainers
.
size
(
)
;
protected
void
processResponseQueue
(
)
throws
Exception
{
while
(
!
responseQueue
.
isEmpty
(
)
)
{
AllocateResponse
response
=
responseQueue
.
take
(
)
;
if
(
!
response
.
getCompletedContainersStatuses
(
)
.
isEmpty
(
)
)
{
for
(
ContainerStatus
cs
:
response
.
getCompletedContainersStatuses
(
)
)
{
ContainerId
containerId
=
cs
.
getContainerId
(
)
;
if
(
cs
.
getExitStatus
(
)
==
ContainerExitStatus
.
SUCCESS
)
{
if
(
assignedContainers
.
containsKey
(
containerId
)
)
{
for
(
ContainerStatus
cs
:
response
.
getCompletedContainersStatuses
(
)
)
{
ContainerId
containerId
=
cs
.
getContainerId
(
)
;
if
(
cs
.
getExitStatus
(
)
==
ContainerExitStatus
.
SUCCESS
)
{
if
(
assignedContainers
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
ContainerSimulator
containerSimulator
=
assignedContainers
.
remove
(
containerId
)
;
finishedContainers
++
;
completedContainers
.
add
(
containerSimulator
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
if
(
finishedContainers
>=
totalContainers
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedContainers
.
containsKey
(
containerId
)
)
{
if
(
assignedContainers
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
ContainerSimulator
containerSimulator
=
assignedContainers
.
remove
(
containerId
)
;
finishedContainers
++
;
completedContainers
.
add
(
containerSimulator
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
if
(
finishedContainers
>=
totalContainers
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedContainers
.
containsKey
(
containerId
)
)
{
LOG
.
error
(
,
appId
,
containerId
)
;
pendingContainers
.
add
(
assignedContainers
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
finishedContainers
++
;
completedContainers
.
add
(
containerSimulator
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
if
(
finishedContainers
>=
totalContainers
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedContainers
.
containsKey
(
containerId
)
)
{
LOG
.
error
(
,
appId
,
containerId
)
;
pendingContainers
.
add
(
assignedContainers
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
LOG
.
error
(
+
,
appId
)
;
}
}
}
}
if
(
isAMContainerRunning
&&
(
finishedContainers
>=
totalContainers
)
)
{
isAMContainerRunning
=
false
;
}
if
(
finishedContainers
>=
totalContainers
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedContainers
.
containsKey
(
containerId
)
)
{
LOG
.
error
(
,
appId
,
containerId
)
;
pendingContainers
.
add
(
assignedContainers
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
LOG
.
error
(
+
,
appId
)
;
}
}
}
}
if
(
isAMContainerRunning
&&
(
finishedContainers
>=
totalContainers
)
)
{
isAMContainerRunning
=
false
;
LOG
.
info
(
+
,
appId
)
;
isFinished
=
true
;
break
;
}
for
(
Container
container
:
response
.
getAllocatedContainers
(
)
)
{
if
(
!
scheduledContainers
.
isEmpty
(
)
)
{
@
Override
@
SuppressWarnings
(
)
protected
void
processResponseQueue
(
)
throws
Exception
{
while
(
!
responseQueue
.
isEmpty
(
)
)
{
AllocateResponse
response
=
responseQueue
.
take
(
)
;
if
(
!
response
.
getCompletedContainersStatuses
(
)
.
isEmpty
(
)
)
{
for
(
ContainerStatus
cs
:
response
.
getCompletedContainersStatuses
(
)
)
{
ContainerId
containerId
=
cs
.
getContainerId
(
)
;
if
(
cs
.
getExitStatus
(
)
==
ContainerExitStatus
.
SUCCESS
)
{
if
(
assignedMaps
.
containsKey
(
containerId
)
)
{
for
(
ContainerStatus
cs
:
response
.
getCompletedContainersStatuses
(
)
)
{
ContainerId
containerId
=
cs
.
getContainerId
(
)
;
if
(
cs
.
getExitStatus
(
)
==
ContainerExitStatus
.
SUCCESS
)
{
if
(
assignedMaps
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
assignedMaps
.
remove
(
containerId
)
;
mapFinished
++
;
finishedContainers
++
;
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
assignedReduces
.
remove
(
containerId
)
;
reduceFinished
++
;
finishedContainers
++
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
assignedMaps
.
remove
(
containerId
)
;
mapFinished
++
;
finishedContainers
++
;
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
assignedReduces
.
remove
(
containerId
)
;
reduceFinished
++
;
finishedContainers
++
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
if
(
mapFinished
>=
mapTotal
&&
reduceFinished
>=
reduceTotal
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedMaps
.
containsKey
(
containerId
)
)
{
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
assignedReduces
.
remove
(
containerId
)
;
reduceFinished
++
;
finishedContainers
++
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
if
(
mapFinished
>=
mapTotal
&&
reduceFinished
>=
reduceTotal
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedMaps
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedMaps
.
add
(
assignedMaps
.
remove
(
containerId
)
)
;
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
assignedReduces
.
remove
(
containerId
)
;
reduceFinished
++
;
finishedContainers
++
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
if
(
mapFinished
>=
mapTotal
&&
reduceFinished
>=
reduceTotal
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedMaps
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedMaps
.
add
(
assignedMaps
.
remove
(
containerId
)
)
;
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedReduces
.
add
(
assignedReduces
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
if
(
mapFinished
>=
mapTotal
&&
reduceFinished
>=
reduceTotal
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedMaps
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedMaps
.
add
(
assignedMaps
.
remove
(
containerId
)
)
;
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedReduces
.
add
(
assignedReduces
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
LOG
.
info
(
+
,
appId
)
;
}
}
}
}
if
(
isAMContainerRunning
&&
(
mapFinished
>=
mapTotal
)
&&
(
reduceFinished
>=
reduceTotal
)
)
{
lastStep
(
)
;
}
}
else
{
if
(
assignedMaps
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedMaps
.
add
(
assignedMaps
.
remove
(
containerId
)
)
;
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedReduces
.
add
(
assignedReduces
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
LOG
.
info
(
+
,
appId
)
;
}
}
}
}
if
(
isAMContainerRunning
&&
(
mapFinished
>=
mapTotal
)
&&
(
reduceFinished
>=
reduceTotal
)
)
{
isAMContainerRunning
=
false
;
LOG
.
debug
(
+
,
appId
)
;
isFinished
=
true
;
break
;
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedMaps
.
add
(
assignedMaps
.
remove
(
containerId
)
)
;
}
else
if
(
assignedReduces
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingFailedReduces
.
add
(
assignedReduces
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
LOG
.
info
(
+
,
appId
)
;
}
}
}
}
if
(
isAMContainerRunning
&&
(
mapFinished
>=
mapTotal
)
&&
(
reduceFinished
>=
reduceTotal
)
)
{
isAMContainerRunning
=
false
;
LOG
.
debug
(
+
,
appId
)
;
isFinished
=
true
;
break
;
}
for
(
Container
container
:
response
.
getAllocatedContainers
(
)
)
{
if
(
!
scheduledMaps
.
isEmpty
(
)
)
{
ContainerSimulator
cs
=
scheduledMaps
.
remove
(
)
;
}
List
<
ResourceRequest
>
ask
=
null
;
if
(
mapFinished
!=
mapTotal
)
{
if
(
!
pendingMaps
.
isEmpty
(
)
)
{
ask
=
packageRequests
(
mergeLists
(
pendingMaps
,
scheduledMaps
)
,
PRIORITY_MAP
)
;
LOG
.
debug
(
,
appId
,
pendingMaps
.
size
(
)
)
;
scheduledMaps
.
addAll
(
pendingMaps
)
;
pendingMaps
.
clear
(
)
;
}
else
if
(
!
pendingFailedMaps
.
isEmpty
(
)
)
{
ask
=
packageRequests
(
mergeLists
(
pendingFailedMaps
,
scheduledMaps
)
,
PRIORITY_MAP
)
;
LOG
.
debug
(
,
appId
,
pendingFailedMaps
.
size
(
)
)
;
scheduledMaps
.
addAll
(
pendingFailedMaps
)
;
pendingFailedMaps
.
clear
(
)
;
}
}
else
if
(
reduceFinished
!=
reduceTotal
)
{
if
(
!
pendingReduces
.
isEmpty
(
)
)
{
ask
=
packageRequests
(
mergeLists
(
pendingReduces
,
scheduledReduces
)
,
PRIORITY_REDUCE
)
;
LOG
.
debug
(
,
appId
,
pendingMaps
.
size
(
)
)
;
scheduledMaps
.
addAll
(
pendingMaps
)
;
pendingMaps
.
clear
(
)
;
}
else
if
(
!
pendingFailedMaps
.
isEmpty
(
)
)
{
ask
=
packageRequests
(
mergeLists
(
pendingFailedMaps
,
scheduledMaps
)
,
PRIORITY_MAP
)
;
LOG
.
debug
(
,
appId
,
pendingFailedMaps
.
size
(
)
)
;
scheduledMaps
.
addAll
(
pendingFailedMaps
)
;
pendingFailedMaps
.
clear
(
)
;
}
}
else
if
(
reduceFinished
!=
reduceTotal
)
{
if
(
!
pendingReduces
.
isEmpty
(
)
)
{
ask
=
packageRequests
(
mergeLists
(
pendingReduces
,
scheduledReduces
)
,
PRIORITY_REDUCE
)
;
LOG
.
debug
(
,
appId
,
pendingReduces
.
size
(
)
)
;
scheduledReduces
.
addAll
(
pendingReduces
)
;
pendingReduces
.
clear
(
)
;
}
else
if
(
!
pendingFailedReduces
.
isEmpty
(
)
)
{
@
SuppressWarnings
(
)
public
void
init
(
int
heartbeatInterval
,
List
<
ContainerSimulator
>
containerList
,
ResourceManager
rm
,
SLSRunner
se
,
long
traceStartTime
,
long
traceFinishTime
,
String
user
,
String
queue
,
boolean
isTracked
,
String
oldAppId
,
long
baselineStartTimeMS
,
Resource
amContainerResource
,
String
nodeLabelExpr
,
Map
<
String
,
String
>
params
,
Map
<
ApplicationId
,
AMSimulator
>
appIdAMSim
)
{
super
.
init
(
heartbeatInterval
,
containerList
,
rm
,
se
,
traceStartTime
,
traceFinishTime
,
user
,
queue
,
isTracked
,
oldAppId
,
baselineStartTimeMS
,
amContainerResource
,
nodeLabelExpr
,
params
,
appIdAMSim
)
;
amtype
=
;
allStreams
.
addAll
(
containerList
)
;
duration
=
traceFinishTime
-
traceStartTime
;
@
Override
@
SuppressWarnings
(
)
protected
void
processResponseQueue
(
)
throws
Exception
{
while
(
!
responseQueue
.
isEmpty
(
)
)
{
AllocateResponse
response
=
responseQueue
.
take
(
)
;
if
(
!
response
.
getCompletedContainersStatuses
(
)
.
isEmpty
(
)
)
{
for
(
ContainerStatus
cs
:
response
.
getCompletedContainersStatuses
(
)
)
{
ContainerId
containerId
=
cs
.
getContainerId
(
)
;
if
(
assignedStreams
.
containsKey
(
containerId
)
)
{
@
Override
@
SuppressWarnings
(
)
protected
void
processResponseQueue
(
)
throws
Exception
{
while
(
!
responseQueue
.
isEmpty
(
)
)
{
AllocateResponse
response
=
responseQueue
.
take
(
)
;
if
(
!
response
.
getCompletedContainersStatuses
(
)
.
isEmpty
(
)
)
{
for
(
ContainerStatus
cs
:
response
.
getCompletedContainersStatuses
(
)
)
{
ContainerId
containerId
=
cs
.
getContainerId
(
)
;
if
(
assignedStreams
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingStreams
.
add
(
assignedStreams
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
if
(
cs
.
getExitStatus
(
)
==
ContainerExitStatus
.
SUCCESS
)
{
isAMContainerRunning
=
false
;
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
else
{
if
(
!
response
.
getCompletedContainersStatuses
(
)
.
isEmpty
(
)
)
{
for
(
ContainerStatus
cs
:
response
.
getCompletedContainersStatuses
(
)
)
{
ContainerId
containerId
=
cs
.
getContainerId
(
)
;
if
(
assignedStreams
.
containsKey
(
containerId
)
)
{
LOG
.
debug
(
,
appId
,
containerId
)
;
pendingStreams
.
add
(
assignedStreams
.
remove
(
containerId
)
)
;
}
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
if
(
cs
.
getExitStatus
(
)
==
ContainerExitStatus
.
SUCCESS
)
{
isAMContainerRunning
=
false
;
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
else
{
LOG
.
info
(
+
,
appId
)
;
isAMContainerRunning
=
false
;
}
}
}
}
if
(
isAMContainerRunning
&&
(
System
.
currentTimeMillis
(
)
-
simulateStartTimeMS
>=
duration
)
)
{
else
if
(
amContainer
.
getId
(
)
.
equals
(
containerId
)
)
{
if
(
cs
.
getExitStatus
(
)
==
ContainerExitStatus
.
SUCCESS
)
{
isAMContainerRunning
=
false
;
isFinished
=
true
;
LOG
.
info
(
,
appId
)
;
}
else
{
LOG
.
info
(
+
,
appId
)
;
isAMContainerRunning
=
false
;
}
}
}
}
if
(
isAMContainerRunning
&&
(
System
.
currentTimeMillis
(
)
-
simulateStartTimeMS
>=
duration
)
)
{
LOG
.
debug
(
+
,
appId
)
;
isAMContainerRunning
=
false
;
isFinished
=
true
;
break
;
}
for
(
Container
container
:
response
.
getAllocatedContainers
(
)
)
{
if
(
!
scheduledStreams
.
isEmpty
(
)
)
{
@
Override
protected
void
sendContainerRequest
(
)
throws
YarnException
,
IOException
,
InterruptedException
{
List
<
ResourceRequest
>
ask
=
new
ArrayList
<
>
(
)
;
List
<
ContainerId
>
release
=
new
ArrayList
<
>
(
)
;
if
(
!
isFinished
)
{
if
(
!
pendingStreams
.
isEmpty
(
)
)
{
ask
=
packageRequests
(
mergeLists
(
pendingStreams
,
scheduledStreams
)
,
PRIORITY_MAP
)
;
@
Override
public
void
middleStep
(
)
throws
Exception
{
ContainerSimulator
cs
=
null
;
synchronized
(
completedContainerList
)
{
while
(
(
cs
=
containerQueue
.
poll
(
)
)
!=
null
)
{
runningContainers
.
remove
(
cs
.
getId
(
)
)
;
completedContainerList
.
add
(
cs
.
getId
(
)
)
;
ns
.
setResponseId
(
responseId
++
)
;
ns
.
setNodeHealthStatus
(
NodeHealthStatus
.
newInstance
(
true
,
,
0
)
)
;
if
(
resourceUtilizationRatio
>
0
&&
resourceUtilizationRatio
<=
1
)
{
int
pMemUsed
=
Math
.
round
(
node
.
getTotalCapability
(
)
.
getMemorySize
(
)
*
resourceUtilizationRatio
)
;
float
cpuUsed
=
node
.
getTotalCapability
(
)
.
getVirtualCores
(
)
*
resourceUtilizationRatio
;
ResourceUtilization
resourceUtilization
=
ResourceUtilization
.
newInstance
(
pMemUsed
,
pMemUsed
,
cpuUsed
)
;
ns
.
setContainersUtilization
(
resourceUtilization
)
;
ns
.
setNodeUtilization
(
resourceUtilization
)
;
}
beatRequest
.
setNodeStatus
(
ns
)
;
NodeHeartbeatResponse
beatResponse
=
rm
.
getResourceTrackerService
(
)
.
nodeHeartbeat
(
beatRequest
)
;
if
(
!
beatResponse
.
getContainersToCleanup
(
)
.
isEmpty
(
)
)
{
synchronized
(
releasedContainerList
)
{
for
(
ContainerId
containerId
:
beatResponse
.
getContainersToCleanup
(
)
)
{
if
(
amContainerList
.
contains
(
containerId
)
)
{
synchronized
(
amContainerList
)
{
float
cpuUsed
=
node
.
getTotalCapability
(
)
.
getVirtualCores
(
)
*
resourceUtilizationRatio
;
ResourceUtilization
resourceUtilization
=
ResourceUtilization
.
newInstance
(
pMemUsed
,
pMemUsed
,
cpuUsed
)
;
ns
.
setContainersUtilization
(
resourceUtilization
)
;
ns
.
setNodeUtilization
(
resourceUtilization
)
;
}
beatRequest
.
setNodeStatus
(
ns
)
;
NodeHeartbeatResponse
beatResponse
=
rm
.
getResourceTrackerService
(
)
.
nodeHeartbeat
(
beatRequest
)
;
if
(
!
beatResponse
.
getContainersToCleanup
(
)
.
isEmpty
(
)
)
{
synchronized
(
releasedContainerList
)
{
for
(
ContainerId
containerId
:
beatResponse
.
getContainersToCleanup
(
)
)
{
if
(
amContainerList
.
contains
(
containerId
)
)
{
synchronized
(
amContainerList
)
{
amContainerList
.
remove
(
containerId
)
;
}
LOG
.
debug
(
,
node
.
getNodeID
(
)
,
containerId
)
;
}
else
{
cs
=
runningContainers
.
remove
(
containerId
)
;
public
void
addNewContainer
(
Container
container
,
long
lifeTimeMS
)
{
private
void
initMetricsCSVOutput
(
)
{
int
timeIntervalMS
=
conf
.
getInt
(
SLSConfiguration
.
METRICS_RECORD_INTERVAL_MS
,
SLSConfiguration
.
METRICS_RECORD_INTERVAL_MS_DEFAULT
)
;
File
dir
=
new
File
(
metricsOutputDir
+
)
;
if
(
!
dir
.
exists
(
)
&&
!
dir
.
mkdirs
(
)
)
{
@
Test
public
void
testWorkloadGenerateTime
(
)
throws
IllegalArgumentException
,
IOException
{
String
workloadJson
=
+
+
+
+
+
;
ObjectMapper
mapper
=
new
ObjectMapper
(
)
;
mapper
.
configure
(
INTERN_FIELD_NAMES
,
true
)
;
mapper
.
configure
(
FAIL_ON_UNKNOWN_PROPERTIES
,
false
)
;
SynthTraceJobProducer
.
Workload
wl
=
mapper
.
readValue
(
workloadJson
,
SynthTraceJobProducer
.
Workload
.
class
)
;
JDKRandomGenerator
rand
=
new
JDKRandomGenerator
(
)
;
rand
.
setSeed
(
0
)
;
wl
.
init
(
rand
)
;
int
bucket0
=
0
;
int
bucket1
=
0
;
int
bucket2
=
0
;
int
bucket3
=
0
;
for
(
int
i
=
0
;
i
<
1000
;
++
i
)
{
long
time
=
wl
.
generateSubmissionTime
(
)
;
LOG
.
info
(
+
time
)
;
if
(
time
<
30
)
{
bucket0
++
;
}
else
if
(
time
<
60
)
{
bucket1
++
;
}
else
if
(
time
<
90
)
{
bucket2
++
;
}
else
{
bucket3
++
;
}
}
Assert
.
assertTrue
(
bucket0
>
0
)
;
Assert
.
assertTrue
(
bucket1
==
0
)
;
Assert
.
assertTrue
(
bucket2
>
0
)
;
Assert
.
assertTrue
(
bucket3
>
0
)
;
Assert
.
assertTrue
(
bucket2
>
bucket0
)
;
Assert
.
assertTrue
(
bucket2
>
bucket3
)
;
@
Test
public
void
testMapReduce
(
)
throws
IllegalArgumentException
,
IOException
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
SynthTraceJobProducer
.
SLS_SYNTHETIC_TRACE_FILE
,
)
;
SynthTraceJobProducer
stjp
=
new
SynthTraceJobProducer
(
conf
)
;
@
Test
public
void
testMapReduce
(
)
throws
IllegalArgumentException
,
IOException
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
SynthTraceJobProducer
.
SLS_SYNTHETIC_TRACE_FILE
,
)
;
SynthTraceJobProducer
stjp
=
new
SynthTraceJobProducer
(
conf
)
;
LOG
.
info
(
stjp
.
toString
(
)
)
;
SynthJob
js
=
(
SynthJob
)
stjp
.
getNextJob
(
)
;
int
jobCount
=
0
;
while
(
js
!=
null
)
{
@
Test
public
void
testGeneric
(
)
throws
IllegalArgumentException
,
IOException
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
SynthTraceJobProducer
.
SLS_SYNTHETIC_TRACE_FILE
,
)
;
SynthTraceJobProducer
stjp
=
new
SynthTraceJobProducer
(
conf
)
;
@
Test
public
void
testGeneric
(
)
throws
IllegalArgumentException
,
IOException
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
SynthTraceJobProducer
.
SLS_SYNTHETIC_TRACE_FILE
,
)
;
SynthTraceJobProducer
stjp
=
new
SynthTraceJobProducer
(
conf
)
;
LOG
.
info
(
stjp
.
toString
(
)
)
;
SynthJob
js
=
(
SynthJob
)
stjp
.
getNextJob
(
)
;
int
jobCount
=
0
;
while
(
js
!=
null
)
{
@
Test
public
void
testStream
(
)
throws
IllegalArgumentException
,
IOException
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
SynthTraceJobProducer
.
SLS_SYNTHETIC_TRACE_FILE
,
)
;
SynthTraceJobProducer
stjp
=
new
SynthTraceJobProducer
(
conf
)
;
@
Test
public
void
testStream
(
)
throws
IllegalArgumentException
,
IOException
{
Configuration
conf
=
new
Configuration
(
)
;
conf
.
set
(
SynthTraceJobProducer
.
SLS_SYNTHETIC_TRACE_FILE
,
)
;
SynthTraceJobProducer
stjp
=
new
SynthTraceJobProducer
(
conf
)
;
LOG
.
info
(
stjp
.
toString
(
)
)
;
SynthJob
js
=
(
SynthJob
)
stjp
.
getNextJob
(
)
;
int
jobCount
=
0
;
while
(
js
!=
null
)
{
}
f
=
null
;
}
LOG
.
info
(
+
Arrays
.
asList
(
argvSplit
)
)
;
Environment
childEnv
=
(
Environment
)
StreamUtil
.
env
(
)
.
clone
(
)
;
addJobConfToEnvironment
(
job_
,
childEnv
)
;
addEnvironment
(
childEnv
,
job_
.
get
(
)
)
;
envPut
(
childEnv
,
,
System
.
getProperty
(
)
)
;
ProcessBuilder
builder
=
new
ProcessBuilder
(
argvSplit
)
;
builder
.
environment
(
)
.
putAll
(
childEnv
.
toMap
(
)
)
;
sim
=
builder
.
start
(
)
;
clientOut_
=
new
DataOutputStream
(
new
BufferedOutputStream
(
sim
.
getOutputStream
(
)
,
BUFFER_SIZE
)
)
;
clientIn_
=
new
DataInputStream
(
new
BufferedInputStream
(
sim
.
getInputStream
(
)
,
BUFFER_SIZE
)
)
;
clientErr_
=
new
DataInputStream
(
new
BufferedInputStream
(
sim
.
getErrorStream
(
)
)
)
;
startTime_
=
System
.
currentTimeMillis
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
void
setStreamJobDetails
(
JobConf
job
)
{
String
s
=
job
.
get
(
)
;
if
(
s
!=
null
)
{
minRecWrittenToEnableSkip_
=
Long
.
parseLong
(
s
)
;
void
envPut
(
Properties
env
,
String
name
,
String
value
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
info
(
)
;
return
;
}
if
(
clientOut_
!=
null
)
{
try
{
clientOut_
.
flush
(
)
;
clientOut_
.
close
(
)
;
}
catch
(
IOException
io
)
{
LOG
.
warn
(
,
io
)
;
}
}
try
{
waitOutputThreads
(
)
;
}
catch
(
IOException
io
)
{
LOG
.
warn
(
,
io
)
;
}
if
(
sim
!=
null
)
sim
.
destroy
(
)
;
LOG
.
info
(
)
;
}
catch
(
RuntimeException
e
)
{
void
maybeLogRecord
(
)
{
if
(
numRecRead_
>=
nextRecReadLog_
)
{
String
info
=
numRecInfo
(
)
;
throw
new
IOException
(
,
outerrThreadsThrowable
)
;
}
try
{
numRecRead_
++
;
maybeLogRecord
(
)
;
if
(
numExceptions_
==
0
)
{
if
(
!
this
.
ignoreKey
)
{
inWriter_
.
writeKey
(
key
)
;
}
inWriter_
.
writeValue
(
value
)
;
if
(
skipping
)
{
clientOut_
.
flush
(
)
;
}
}
else
{
numRecSkipped_
++
;
}
}
catch
(
IOException
io
)
{
numExceptions_
++
;
if
(
numExceptions_
>
1
||
numRecWritten_
<
minRecWrittenToEnableSkip_
)
{
void
numRecStats
(
byte
[
]
record
,
int
start
,
int
len
)
throws
IOException
{
numRec_
++
;
if
(
numRec_
==
nextStatusRec_
)
{
String
recordStr
=
new
String
(
record
,
start
,
Math
.
min
(
len
,
statusMaxRecordChars_
)
,
)
;
nextStatusRec_
+=
100
;
String
status
=
getStatus
(
recordStr
)
;
if
(
jar_
!=
null
&&
isLocalHadoop
(
)
)
{
File
wd
=
new
File
(
)
.
getAbsoluteFile
(
)
;
RunJar
.
unJar
(
new
File
(
jar_
)
,
wd
,
MATCH_ANY
)
;
}
jc_
=
new
JobClient
(
jobConf_
)
;
running_
=
null
;
try
{
running_
=
jc_
.
submitJob
(
jobConf_
)
;
jobId_
=
running_
.
getID
(
)
;
if
(
background_
)
{
LOG
.
info
(
)
;
}
else
if
(
!
jc_
.
monitorAndPrintJob
(
jobConf_
,
running_
)
)
{
LOG
.
error
(
)
;
return
1
;
}
LOG
.
info
(
+
output_
)
;
}
catch
(
FileNotFoundException
fe
)
{
}
jc_
=
new
JobClient
(
jobConf_
)
;
running_
=
null
;
try
{
running_
=
jc_
.
submitJob
(
jobConf_
)
;
jobId_
=
running_
.
getID
(
)
;
if
(
background_
)
{
LOG
.
info
(
)
;
}
else
if
(
!
jc_
.
monitorAndPrintJob
(
jobConf_
,
running_
)
)
{
LOG
.
error
(
)
;
return
1
;
}
LOG
.
info
(
+
output_
)
;
}
catch
(
FileNotFoundException
fe
)
{
LOG
.
error
(
+
fe
.
getMessage
(
)
)
;
return
2
;
}
catch
(
InvalidJobConfException
je
)
{
void
numRecStats
(
byte
[
]
record
,
int
start
,
int
len
)
throws
IOException
{
numRec_
++
;
if
(
numRec_
==
nextStatusRec_
)
{
String
recordStr
=
new
String
(
record
,
start
,
Math
.
min
(
len
,
statusMaxRecordChars_
)
,
)
;
nextStatusRec_
+=
100
;
String
status
=
getStatus
(
recordStr
)
;
private
static
void
addMandatoryResources
(
Map
<
String
,
ResourceInformation
>
res
)
{
ResourceInformation
ri
;
if
(
!
res
.
containsKey
(
MEMORY
)
)
{
private
static
long
getAllocation
(
Configuration
conf
,
String
resourceTypesKey
,
String
schedulerKey
,
long
schedulerDefault
)
{
long
value
=
conf
.
getLong
(
resourceTypesKey
,
-
1L
)
;
if
(
value
==
-
1
)
{
private
static
Map
<
String
,
ResourceInformation
>
getResourceInformationMapFromConfig
(
Configuration
conf
)
{
Map
<
String
,
ResourceInformation
>
resourceInformationMap
=
new
HashMap
<
>
(
)
;
String
[
]
resourceNames
=
conf
.
getStrings
(
YarnConfiguration
.
RESOURCE_TYPES
)
;
if
(
resourceNames
!=
null
&&
resourceNames
.
length
!=
0
)
{
for
(
String
resourceName
:
resourceNames
)
{
String
resourceUnits
=
conf
.
get
(
YarnConfiguration
.
RESOURCE_TYPES
+
+
resourceName
+
UNITS
,
)
;
String
resourceTypeName
=
conf
.
get
(
YarnConfiguration
.
RESOURCE_TYPES
+
+
resourceName
+
TYPE
,
ResourceTypes
.
COUNTABLE
.
toString
(
)
)
;
Long
minimumAllocation
=
conf
.
getLong
(
YarnConfiguration
.
RESOURCE_TYPES
+
+
resourceName
+
MINIMUM_ALLOCATION
,
0L
)
;
Long
maximumAllocation
=
conf
.
getLong
(
YarnConfiguration
.
RESOURCE_TYPES
+
+
resourceName
+
MAXIMUM_ALLOCATION
,
Long
.
MAX_VALUE
)
;
if
(
resourceName
==
null
||
resourceName
.
isEmpty
(
)
||
resourceUnits
==
null
||
resourceTypeName
==
null
)
{
throw
new
YarnRuntimeException
(
+
resourceName
+
)
;
}
ResourceTypes
resourceType
=
ResourceTypes
.
valueOf
(
resourceTypeName
)
;
String
[
]
resourceTags
=
conf
.
getTrimmedStrings
(
YarnConfiguration
.
RESOURCE_TYPES
+
+
resourceName
+
TAGS
)
;
Set
<
String
>
resourceTagSet
=
new
HashSet
<
>
(
)
;
Collections
.
addAll
(
resourceTagSet
,
resourceTags
)
;
private
static
void
addResourcesFileToConf
(
String
resourceFile
,
Configuration
conf
)
{
try
{
InputStream
ris
=
getConfInputStream
(
resourceFile
,
conf
)
;
private
static
void
addResourceTypeInformation
(
String
prop
,
String
value
,
Map
<
String
,
ResourceInformation
>
nodeResources
)
{
if
(
prop
.
startsWith
(
YarnConfiguration
.
NM_RESOURCES_PREFIX
)
)
{
public
static
List
<
ResourceInformation
>
getRequestedResourcesFromConfig
(
Configuration
configuration
,
String
prefix
)
{
List
<
ResourceInformation
>
result
=
new
ArrayList
<
>
(
)
;
Map
<
String
,
String
>
customResourcesMap
=
configuration
.
getValByRegex
(
+
Pattern
.
quote
(
prefix
)
+
YARN_IO_OPTIONAL
+
)
;
for
(
Entry
<
String
,
String
>
resource
:
customResourcesMap
.
entrySet
(
)
)
{
String
resourceName
=
resource
.
getKey
(
)
.
substring
(
prefix
.
length
(
)
)
;
Matcher
matcher
=
RESOURCE_REQUEST_VALUE_PATTERN
.
matcher
(
resource
.
getValue
(
)
)
;
if
(
!
matcher
.
matches
(
)
)
{
String
errorMsg
=
+
resource
.
getKey
(
)
+
+
resource
.
getValue
(
)
+
;
response
=
solr
.
query
(
query
)
;
Iterator
<
SolrDocument
>
list
=
response
.
getResults
(
)
.
listIterator
(
)
;
while
(
list
.
hasNext
(
)
)
{
SolrDocument
d
=
list
.
next
(
)
;
AppStoreEntry
entry
=
new
AppStoreEntry
(
)
;
entry
.
setId
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setOrg
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setName
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setDesc
(
d
.
get
(
)
.
toString
(
)
)
;
if
(
d
.
get
(
)
!=
null
)
{
entry
.
setIcon
(
d
.
get
(
)
.
toString
(
)
)
;
}
entry
.
setLike
(
Integer
.
parseInt
(
d
.
get
(
)
.
toString
(
)
)
)
;
entry
.
setDownload
(
Integer
.
parseInt
(
d
.
get
(
)
.
toString
(
)
)
)
;
apps
.
add
(
entry
)
;
}
}
catch
(
SolrServerException
|
IOException
e
)
{
query
.
setFilterQueries
(
)
;
query
.
setRows
(
40
)
;
QueryResponse
response
;
try
{
response
=
solr
.
query
(
query
)
;
Iterator
<
SolrDocument
>
appList
=
response
.
getResults
(
)
.
listIterator
(
)
;
while
(
appList
.
hasNext
(
)
)
{
SolrDocument
d
=
appList
.
next
(
)
;
AppEntry
entry
=
new
AppEntry
(
)
;
entry
.
setId
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setName
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setApp
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setYarnfile
(
mapper
.
readValue
(
d
.
get
(
)
.
toString
(
)
,
Service
.
class
)
)
;
list
.
add
(
entry
)
;
}
}
catch
(
SolrServerException
|
IOException
e
)
{
entry
.
setDownload
(
Integer
.
parseInt
(
d
.
get
(
)
.
toString
(
)
)
)
;
Service
yarnApp
=
mapper
.
readValue
(
d
.
get
(
)
.
toString
(
)
,
Service
.
class
)
;
String
name
;
try
{
Random
r
=
new
Random
(
)
;
int
low
=
3
;
int
high
=
10
;
int
seed
=
r
.
nextInt
(
high
-
low
)
+
low
;
int
seed2
=
r
.
nextInt
(
high
-
low
)
+
low
;
name
=
RandomWord
.
getNewWord
(
seed
)
.
toLowerCase
(
)
+
+
RandomWord
.
getNewWord
(
seed2
)
.
toLowerCase
(
)
;
}
catch
(
WordLengthException
e
)
{
name
=
+
java
.
util
.
UUID
.
randomUUID
(
)
.
toString
(
)
.
substring
(
0
,
11
)
;
}
yarnApp
.
setName
(
name
)
;
entry
.
setApp
(
yarnApp
)
;
}
}
catch
(
SolrServerException
|
IOException
e
)
{
SolrQuery
query
=
new
SolrQuery
(
)
;
query
.
setQuery
(
+
id
)
;
query
.
setFilterQueries
(
)
;
query
.
setRows
(
1
)
;
QueryResponse
response
;
try
{
response
=
solr
.
query
(
query
)
;
Iterator
<
SolrDocument
>
appList
=
response
.
getResults
(
)
.
listIterator
(
)
;
while
(
appList
.
hasNext
(
)
)
{
SolrDocument
d
=
appList
.
next
(
)
;
entry
.
setId
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setApp
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setName
(
d
.
get
(
)
.
toString
(
)
)
;
entry
.
setYarnfile
(
mapper
.
readValue
(
d
.
get
(
)
.
toString
(
)
,
Service
.
class
)
)
;
}
}
catch
(
SolrServerException
|
IOException
e
)
{
SolrClient
solr
=
getSolrClient
(
)
;
ObjectMapper
mapper
=
new
ObjectMapper
(
)
;
mapper
.
configure
(
DeserializationFeature
.
FAIL_ON_UNKNOWN_PROPERTIES
,
false
)
;
try
{
SolrInputDocument
buffer
=
new
SolrInputDocument
(
)
;
buffer
.
setField
(
,
java
.
util
.
UUID
.
randomUUID
(
)
.
toString
(
)
.
substring
(
0
,
11
)
)
;
buffer
.
setField
(
,
app
.
getOrganization
(
)
)
;
buffer
.
setField
(
,
app
.
getName
(
)
)
;
buffer
.
setField
(
,
app
.
getDescription
(
)
)
;
if
(
app
.
getIcon
(
)
!=
null
)
{
buffer
.
setField
(
,
app
.
getIcon
(
)
)
;
}
buffer
.
setField
(
,
)
;
buffer
.
setField
(
,
0
)
;
buffer
.
setField
(
,
0
)
;
String
yarnFile
=
mapper
.
writeValueAsString
(
app
)
;
SolrClient
solr
=
getSolrClient
(
)
;
ObjectMapper
mapper
=
new
ObjectMapper
(
)
;
mapper
.
configure
(
DeserializationFeature
.
FAIL_ON_UNKNOWN_PROPERTIES
,
false
)
;
try
{
SolrInputDocument
buffer
=
new
SolrInputDocument
(
)
;
buffer
.
setField
(
,
java
.
util
.
UUID
.
randomUUID
(
)
.
toString
(
)
.
substring
(
0
,
11
)
)
;
buffer
.
setField
(
,
app
.
getOrg
(
)
)
;
buffer
.
setField
(
,
app
.
getName
(
)
)
;
buffer
.
setField
(
,
app
.
getDesc
(
)
)
;
if
(
app
.
getIcon
(
)
!=
null
)
{
buffer
.
setField
(
,
app
.
getIcon
(
)
)
;
}
buffer
.
setField
(
,
)
;
buffer
.
setField
(
,
app
.
getLike
(
)
)
;
buffer
.
setField
(
,
app
.
getDownload
(
)
)
;
String
yarnFile
=
mapper
.
writeValueAsString
(
app
)
;
if
(
service
!=
null
)
{
String
name
=
service
.
getName
(
)
;
String
app
=
;
SolrQuery
query
=
new
SolrQuery
(
)
;
query
.
setQuery
(
+
name
)
;
query
.
setFilterQueries
(
)
;
query
.
setRows
(
1
)
;
QueryResponse
response
;
try
{
response
=
solr
.
query
(
query
)
;
Iterator
<
SolrDocument
>
appList
=
response
.
getResults
(
)
.
listIterator
(
)
;
while
(
appList
.
hasNext
(
)
)
{
SolrDocument
d
=
appList
.
next
(
)
;
app
=
d
.
get
(
)
.
toString
(
)
;
}
}
catch
(
SolrServerException
|
IOException
e
)
{
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
temp
[
1
]
)
;
String
keytab
=
System
.
getenv
(
)
;
if
(
!
keytab
.
startsWith
(
)
)
{
keytab
=
+
keytab
;
}
kerberos
.
setPrincipalName
(
sb
.
toString
(
)
)
;
kerberos
.
setKeytab
(
keytab
)
;
app
.
setKerberosPrincipal
(
kerberos
)
;
}
response
=
asc
.
getApiClient
(
)
.
post
(
ClientResponse
.
class
,
mapper
.
writeValueAsString
(
app
)
)
;
if
(
response
.
getStatus
(
)
>=
299
)
{
String
message
=
response
.
getEntity
(
String
.
class
)
;
throw
new
RuntimeException
(
+
response
.
getStatus
(
)
+
+
message
)
;
}
}
catch
(
UniformInterfaceException
|
ClientHandlerException
|
IOException
e
)
{
private
void
dumpOutDebugInfo
(
)
{
LOG
.
info
(
)
;
Map
<
String
,
String
>
envs
=
System
.
getenv
(
)
;
for
(
Map
.
Entry
<
String
,
String
>
env
:
envs
.
entrySet
(
)
)
{
}
if
(
fileExist
(
log4jPath
)
)
{
try
{
Log4jPropertyHelper
.
updateLog4jConfiguration
(
ApplicationMaster
.
class
,
log4jPath
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
e
)
;
}
}
appName
=
cliParser
.
getOptionValue
(
,
)
;
if
(
cliParser
.
hasOption
(
)
)
{
printUsage
(
opts
)
;
return
false
;
}
if
(
cliParser
.
hasOption
(
)
)
{
dumpOutDebugInfo
(
)
;
}
homeDirectory
=
cliParser
.
hasOption
(
)
?
new
Path
(
cliParser
.
getOptionValue
(
)
)
:
new
Path
(
+
System
.
getenv
(
ApplicationConstants
.
Environment
.
USER
.
name
(
)
)
)
;
if
(
cliParser
.
hasOption
(
)
)
{
String
placementSpec
=
cliParser
.
getOptionValue
(
)
;
String
decodedSpec
=
getDecodedPlacementSpec
(
placementSpec
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
e
)
;
}
}
appName
=
cliParser
.
getOptionValue
(
,
)
;
if
(
cliParser
.
hasOption
(
)
)
{
printUsage
(
opts
)
;
return
false
;
}
if
(
cliParser
.
hasOption
(
)
)
{
dumpOutDebugInfo
(
)
;
}
homeDirectory
=
cliParser
.
hasOption
(
)
?
new
Path
(
cliParser
.
getOptionValue
(
)
)
:
new
Path
(
+
System
.
getenv
(
ApplicationConstants
.
Environment
.
USER
.
name
(
)
)
)
;
if
(
cliParser
.
hasOption
(
)
)
{
String
placementSpec
=
cliParser
.
getOptionValue
(
)
;
String
decodedSpec
=
getDecodedPlacementSpec
(
placementSpec
)
;
LOG
.
info
(
,
decodedSpec
)
;
this
.
numTotalContainers
=
0
;
int
globalNumOfContainers
=
Integer
.
parseInt
(
cliParser
.
getOptionValue
(
,
)
)
;
private
String
getDecodedPlacementSpec
(
String
placementSpecifications
)
{
Base64
.
Decoder
decoder
=
Base64
.
getDecoder
(
)
;
byte
[
]
decodedBytes
=
decoder
.
decode
(
placementSpecifications
.
getBytes
(
StandardCharsets
.
UTF_8
)
)
;
String
decodedSpec
=
new
String
(
decodedBytes
,
StandardCharsets
.
UTF_8
)
;
@
SuppressWarnings
(
{
}
)
public
void
run
(
)
throws
YarnException
,
IOException
,
InterruptedException
{
LOG
.
info
(
)
;
Credentials
credentials
=
UserGroupInformation
.
getCurrentUser
(
)
.
getCredentials
(
)
;
DataOutputBuffer
dob
=
new
DataOutputBuffer
(
)
;
credentials
.
writeTokenStorageToStream
(
dob
)
;
Iterator
<
Token
<
?
>>
iter
=
credentials
.
getAllTokens
(
)
.
iterator
(
)
;
LOG
.
info
(
)
;
while
(
iter
.
hasNext
(
)
)
{
Token
<
?
>
token
=
iter
.
next
(
)
;
if
(
timelineServiceV1Enabled
)
{
publishApplicationAttemptEvent
(
timelineClient
,
appAttemptID
.
toString
(
)
,
DSEvent
.
DS_APP_ATTEMPT_START
,
domainId
,
appSubmitterUgi
)
;
}
appMasterHostname
=
NetUtils
.
getHostname
(
)
;
Map
<
Set
<
String
>
,
PlacementConstraint
>
placementConstraintMap
=
null
;
if
(
this
.
placementSpecs
!=
null
)
{
placementConstraintMap
=
new
HashMap
<
>
(
)
;
for
(
PlacementSpec
spec
:
this
.
placementSpecs
.
values
(
)
)
{
if
(
spec
.
constraint
!=
null
)
{
Set
<
String
>
allocationTags
=
Strings
.
isNullOrEmpty
(
spec
.
sourceTag
)
?
Collections
.
emptySet
(
)
:
Collections
.
singleton
(
spec
.
sourceTag
)
;
placementConstraintMap
.
put
(
allocationTags
,
spec
.
constraint
)
;
}
}
}
RegisterApplicationMasterResponse
response
=
amRMClient
.
registerApplicationMaster
(
appMasterHostname
,
appMasterRpcPort
,
appMasterTrackingUrl
,
placementConstraintMap
)
;
resourceProfiles
=
response
.
getResourceProfiles
(
)
;
ResourceUtils
.
reinitializeResources
(
response
.
getResourceTypes
(
)
)
;
long
maxMem
=
response
.
getMaximumResourceCapability
(
)
.
getMemorySize
(
)
;
LOG
.
info
(
+
maxMem
)
;
appMasterHostname
=
NetUtils
.
getHostname
(
)
;
Map
<
Set
<
String
>
,
PlacementConstraint
>
placementConstraintMap
=
null
;
if
(
this
.
placementSpecs
!=
null
)
{
placementConstraintMap
=
new
HashMap
<
>
(
)
;
for
(
PlacementSpec
spec
:
this
.
placementSpecs
.
values
(
)
)
{
if
(
spec
.
constraint
!=
null
)
{
Set
<
String
>
allocationTags
=
Strings
.
isNullOrEmpty
(
spec
.
sourceTag
)
?
Collections
.
emptySet
(
)
:
Collections
.
singleton
(
spec
.
sourceTag
)
;
placementConstraintMap
.
put
(
allocationTags
,
spec
.
constraint
)
;
}
}
}
RegisterApplicationMasterResponse
response
=
amRMClient
.
registerApplicationMaster
(
appMasterHostname
,
appMasterRpcPort
,
appMasterTrackingUrl
,
placementConstraintMap
)
;
resourceProfiles
=
response
.
getResourceProfiles
(
)
;
ResourceUtils
.
reinitializeResources
(
response
.
getResourceTypes
(
)
)
;
long
maxMem
=
response
.
getMaximumResourceCapability
(
)
.
getMemorySize
(
)
;
LOG
.
info
(
+
maxMem
)
;
int
maxVCores
=
response
.
getMaximumResourceCapability
(
)
.
getVirtualCores
(
)
;
LOG
.
info
(
+
maxVCores
)
;
private
ContainerRequest
setupContainerAskForRM
(
)
{
Priority
pri
=
Priority
.
newInstance
(
requestPriority
)
;
ContainerRequest
request
=
new
ContainerRequest
(
getTaskResourceCapability
(
)
,
null
,
null
,
pri
,
0
,
true
,
null
,
ExecutionTypeRequest
.
newInstance
(
containerType
,
enforceExecType
)
,
containerResourceProfile
)
;
private
SchedulingRequest
setupSchedulingRequest
(
PlacementSpec
spec
)
{
long
allocId
=
allocIdCounter
.
incrementAndGet
(
)
;
SchedulingRequest
sReq
=
SchedulingRequest
.
newInstance
(
allocId
,
Priority
.
newInstance
(
requestPriority
)
,
ExecutionTypeRequest
.
newInstance
(
)
,
Collections
.
singleton
(
spec
.
sourceTag
)
,
ResourceSizing
.
newInstance
(
getTaskResourceCapability
(
)
)
,
null
)
;
sReq
.
setPlacementConstraint
(
spec
.
constraint
)
;
final
TimelineEntity
entity
=
new
TimelineEntity
(
)
;
entity
.
setEntityId
(
container
.
getId
(
)
.
toString
(
)
)
;
entity
.
setEntityType
(
DSEntity
.
DS_CONTAINER
.
toString
(
)
)
;
entity
.
setDomainId
(
domainId
)
;
entity
.
addPrimaryFilter
(
USER_TIMELINE_FILTER_NAME
,
ugi
.
getShortUserName
(
)
)
;
entity
.
addPrimaryFilter
(
APPID_TIMELINE_FILTER_NAME
,
container
.
getId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
.
toString
(
)
)
;
TimelineEvent
event
=
new
TimelineEvent
(
)
;
event
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
event
.
setEventType
(
DSEvent
.
DS_CONTAINER_START
.
toString
(
)
)
;
event
.
addEventInfo
(
,
container
.
getNodeId
(
)
.
toString
(
)
)
;
event
.
addEventInfo
(
,
container
.
getResource
(
)
.
toString
(
)
)
;
entity
.
addEvent
(
event
)
;
try
{
processTimelineResponseErrors
(
putContainerEntity
(
timelineClient
,
container
.
getId
(
)
.
getApplicationAttemptId
(
)
,
entity
)
)
;
}
catch
(
YarnException
|
IOException
|
ClientHandlerException
e
)
{
entity
.
setEntityId
(
container
.
getContainerId
(
)
.
toString
(
)
)
;
entity
.
setEntityType
(
DSEntity
.
DS_CONTAINER
.
toString
(
)
)
;
entity
.
setDomainId
(
domainId
)
;
entity
.
addPrimaryFilter
(
USER_TIMELINE_FILTER_NAME
,
ugi
.
getShortUserName
(
)
)
;
entity
.
addPrimaryFilter
(
APPID_TIMELINE_FILTER_NAME
,
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
.
toString
(
)
)
;
TimelineEvent
event
=
new
TimelineEvent
(
)
;
event
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
event
.
setEventType
(
DSEvent
.
DS_CONTAINER_END
.
toString
(
)
)
;
event
.
addEventInfo
(
,
container
.
getState
(
)
.
name
(
)
)
;
event
.
addEventInfo
(
,
container
.
getExitStatus
(
)
)
;
event
.
addEventInfo
(
DIAGNOSTICS
,
container
.
getDiagnostics
(
)
)
;
entity
.
addEvent
(
event
)
;
try
{
processTimelineResponseErrors
(
putContainerEntity
(
timelineClient
,
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
,
entity
)
)
;
}
catch
(
YarnException
|
IOException
|
ClientHandlerException
e
)
{
final
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEntity
entity
=
new
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEntity
(
)
;
entity
.
setId
(
containerId
.
toString
(
)
)
;
entity
.
setType
(
DSEntity
.
DS_CONTAINER
.
toString
(
)
)
;
entity
.
addInfo
(
,
appSubmitterUgi
.
getShortUserName
(
)
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEvent
event
=
new
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEvent
(
)
;
event
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
event
.
setId
(
DSEvent
.
DS_CONTAINER_END
.
toString
(
)
)
;
event
.
addInfo
(
DIAGNOSTICS
,
diagnostics
)
;
entity
.
addEvent
(
event
)
;
try
{
appSubmitterUgi
.
doAs
(
(
PrivilegedExceptionAction
<
Object
>
)
(
)
->
{
timelineV2Client
.
putEntitiesAsync
(
entity
)
;
return
null
;
}
)
;
}
catch
(
Exception
e
)
{
private
void
publishContainerStartFailedEvent
(
final
ContainerId
containerId
,
String
diagnostics
)
{
final
TimelineEntity
entityV1
=
new
TimelineEntity
(
)
;
entityV1
.
setEntityId
(
containerId
.
toString
(
)
)
;
entityV1
.
setEntityType
(
DSEntity
.
DS_CONTAINER
.
toString
(
)
)
;
entityV1
.
setDomainId
(
domainId
)
;
entityV1
.
addPrimaryFilter
(
USER_TIMELINE_FILTER_NAME
,
appSubmitterUgi
.
getShortUserName
(
)
)
;
entityV1
.
addPrimaryFilter
(
APPID_TIMELINE_FILTER_NAME
,
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
.
toString
(
)
)
;
TimelineEvent
eventV1
=
new
TimelineEvent
(
)
;
eventV1
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
eventV1
.
setEventType
(
DSEvent
.
DS_CONTAINER_END
.
toString
(
)
)
;
eventV1
.
addEventInfo
(
DIAGNOSTICS
,
diagnostics
)
;
entityV1
.
addEvent
(
eventV1
)
;
try
{
processTimelineResponseErrors
(
putContainerEntity
(
timelineClient
,
containerId
.
getApplicationAttemptId
(
)
,
entityV1
)
)
;
}
catch
(
YarnException
|
IOException
|
ClientHandlerException
e
)
{
if
(
appEvent
==
DSEvent
.
DS_APP_ATTEMPT_START
)
{
entity
.
setCreatedTime
(
ts
)
;
}
entity
.
addInfo
(
,
appSubmitterUgi
.
getShortUserName
(
)
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEvent
event
=
new
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
timelineservice
.
TimelineEvent
(
)
;
event
.
setId
(
appEvent
.
toString
(
)
)
;
event
.
setTimestamp
(
ts
)
;
entity
.
addEvent
(
event
)
;
entity
.
setIdPrefix
(
TimelineServiceHelper
.
invertLong
(
appAttemptID
.
getAttemptId
(
)
)
)
;
try
{
appSubmitterUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Object
>
(
)
{
@
Override
public
TimelinePutResponse
run
(
)
throws
Exception
{
timelineV2Client
.
putEntitiesAsync
(
entity
)
;
return
null
;
}
}
)
;
}
catch
(
Exception
e
)
{
public
static
void
main
(
String
[
]
args
)
{
boolean
result
=
false
;
try
{
Client
client
=
new
Client
(
)
;
LOG
.
info
(
)
;
try
{
boolean
doRun
=
client
.
init
(
args
)
;
if
(
!
doRun
)
{
System
.
exit
(
0
)
;
}
}
catch
(
IllegalArgumentException
e
)
{
System
.
err
.
println
(
e
.
getLocalizedMessage
(
)
)
;
client
.
printUsage
(
)
;
System
.
exit
(
-
1
)
;
}
result
=
client
.
run
(
)
;
}
catch
(
Throwable
t
)
{
public
boolean
run
(
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
yarnClient
.
start
(
)
;
YarnClusterMetrics
clusterMetrics
=
yarnClient
.
getYarnClusterMetrics
(
)
;
public
boolean
run
(
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
yarnClient
.
start
(
)
;
YarnClusterMetrics
clusterMetrics
=
yarnClient
.
getYarnClusterMetrics
(
)
;
LOG
.
info
(
+
+
clusterMetrics
.
getNumNodeManagers
(
)
)
;
List
<
NodeReport
>
clusterNodeReports
=
yarnClient
.
getNodeReports
(
NodeState
.
RUNNING
)
;
LOG
.
info
(
)
;
for
(
NodeReport
node
:
clusterNodeReports
)
{
LOG
.
info
(
)
;
yarnClient
.
start
(
)
;
YarnClusterMetrics
clusterMetrics
=
yarnClient
.
getYarnClusterMetrics
(
)
;
LOG
.
info
(
+
+
clusterMetrics
.
getNumNodeManagers
(
)
)
;
List
<
NodeReport
>
clusterNodeReports
=
yarnClient
.
getNodeReports
(
NodeState
.
RUNNING
)
;
LOG
.
info
(
)
;
for
(
NodeReport
node
:
clusterNodeReports
)
{
LOG
.
info
(
+
+
node
.
getNodeId
(
)
+
+
node
.
getHttpAddress
(
)
+
+
node
.
getRackName
(
)
+
+
node
.
getNumContainers
(
)
)
;
}
QueueInfo
queueInfo
=
yarnClient
.
getQueueInfo
(
this
.
amQueue
)
;
if
(
queueInfo
==
null
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
,
this
.
amQueue
)
)
;
}
LOG
.
info
(
+
+
queueInfo
.
getQueueName
(
)
+
+
queueInfo
.
getCurrentCapacity
(
)
+
+
queueInfo
.
getMaximumCapacity
(
)
+
+
queueInfo
.
getApplications
(
)
.
size
(
)
+
+
queueInfo
.
getChildQueues
(
)
.
size
(
)
)
;
List
<
QueueUserACLInfo
>
listAclInfo
=
yarnClient
.
getQueueAclsInfo
(
)
;
for
(
QueueUserACLInfo
aclInfo
:
listAclInfo
)
{
for
(
QueueACL
userAcl
:
aclInfo
.
getUserAcls
(
)
)
{
}
if
(
domainId
!=
null
&&
domainId
.
length
(
)
>
0
&&
toCreateDomain
)
{
prepareTimelineDomain
(
)
;
}
Map
<
String
,
Resource
>
profiles
;
try
{
profiles
=
yarnClient
.
getResourceProfiles
(
)
;
}
catch
(
YARNFeatureNotEnabledException
re
)
{
profiles
=
null
;
}
List
<
String
>
appProfiles
=
new
ArrayList
<
>
(
2
)
;
appProfiles
.
add
(
amResourceProfile
)
;
appProfiles
.
add
(
containerResourceProfile
)
;
for
(
String
appProfile
:
appProfiles
)
{
if
(
appProfile
!=
null
&&
!
appProfile
.
isEmpty
(
)
)
{
if
(
profiles
==
null
)
{
String
message
=
;
LOG
.
error
(
message
)
;
private
boolean
monitorApplication
(
ApplicationId
appId
)
throws
YarnException
,
IOException
{
while
(
true
)
{
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
debug
(
)
;
}
ApplicationReport
report
=
yarnClient
.
getApplicationReport
(
appId
)
;
LOG
.
info
(
+
+
appId
.
getId
(
)
+
+
report
.
getClientToAMToken
(
)
+
+
report
.
getDiagnostics
(
)
+
+
report
.
getHost
(
)
+
+
report
.
getQueue
(
)
+
+
report
.
getRpcPort
(
)
+
+
report
.
getStartTime
(
)
+
+
report
.
getYarnApplicationState
(
)
.
toString
(
)
+
+
report
.
getFinalApplicationStatus
(
)
.
toString
(
)
+
+
report
.
getTrackingUrl
(
)
+
+
report
.
getUser
(
)
)
;
YarnApplicationState
state
=
report
.
getYarnApplicationState
(
)
;
FinalApplicationStatus
dsStatus
=
report
.
getFinalApplicationStatus
(
)
;
if
(
YarnApplicationState
.
FINISHED
==
state
)
{
if
(
FinalApplicationStatus
.
SUCCEEDED
==
dsStatus
)
{
LOG
.
info
(
)
;
return
true
;
}
else
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
debug
(
)
;
}
ApplicationReport
report
=
yarnClient
.
getApplicationReport
(
appId
)
;
LOG
.
info
(
+
+
appId
.
getId
(
)
+
+
report
.
getClientToAMToken
(
)
+
+
report
.
getDiagnostics
(
)
+
+
report
.
getHost
(
)
+
+
report
.
getQueue
(
)
+
+
report
.
getRpcPort
(
)
+
+
report
.
getStartTime
(
)
+
+
report
.
getYarnApplicationState
(
)
.
toString
(
)
+
+
report
.
getFinalApplicationStatus
(
)
.
toString
(
)
+
+
report
.
getTrackingUrl
(
)
+
+
report
.
getUser
(
)
)
;
YarnApplicationState
state
=
report
.
getYarnApplicationState
(
)
;
FinalApplicationStatus
dsStatus
=
report
.
getFinalApplicationStatus
(
)
;
if
(
YarnApplicationState
.
FINISHED
==
state
)
{
if
(
FinalApplicationStatus
.
SUCCEEDED
==
dsStatus
)
{
LOG
.
info
(
)
;
return
true
;
}
else
{
LOG
.
info
(
+
+
state
.
toString
(
)
+
+
dsStatus
.
toString
(
)
+
)
;
return
false
;
}
}
else
if
(
YarnApplicationState
.
KILLED
==
state
||
YarnApplicationState
.
FAILED
==
state
)
{
private
void
uploadFile
(
FileSystem
fs
,
String
fileSrcPath
,
String
fileDstPath
,
String
appId
)
throws
IOException
{
String
relativePath
=
ApplicationMaster
.
getRelativePath
(
appName
,
appId
,
fileDstPath
)
;
Path
dst
=
new
Path
(
fs
.
getHomeDirectory
(
)
,
relativePath
)
;
private
void
prepareTimelineDomain
(
)
{
TimelineClient
timelineClient
=
null
;
if
(
conf
.
getBoolean
(
YarnConfiguration
.
TIMELINE_SERVICE_ENABLED
,
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_ENABLED
)
)
{
timelineClient
=
TimelineClient
.
createTimelineClient
(
)
;
timelineClient
.
init
(
conf
)
;
timelineClient
.
start
(
)
;
}
else
{
LOG
.
warn
(
+
domainId
+
)
;
return
;
}
try
{
TimelineDomain
domain
=
new
TimelineDomain
(
)
;
domain
.
setId
(
domainId
)
;
domain
.
setReaders
(
viewACLs
!=
null
&&
viewACLs
.
length
(
)
>
0
?
viewACLs
:
)
;
domain
.
setWriters
(
modifyACLs
!=
null
&&
modifyACLs
.
length
(
)
>
0
?
modifyACLs
:
)
;
timelineClient
.
putDomain
(
domain
)
;
if
(
conf
.
getBoolean
(
YarnConfiguration
.
TIMELINE_SERVICE_ENABLED
,
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_ENABLED
)
)
{
timelineClient
=
TimelineClient
.
createTimelineClient
(
)
;
timelineClient
.
init
(
conf
)
;
timelineClient
.
start
(
)
;
}
else
{
LOG
.
warn
(
+
domainId
+
)
;
return
;
}
try
{
TimelineDomain
domain
=
new
TimelineDomain
(
)
;
domain
.
setId
(
domainId
)
;
domain
.
setReaders
(
viewACLs
!=
null
&&
viewACLs
.
length
(
)
>
0
?
viewACLs
:
)
;
domain
.
setWriters
(
modifyACLs
!=
null
&&
modifyACLs
.
length
(
)
>
0
?
modifyACLs
:
)
;
timelineClient
.
putDomain
(
domain
)
;
LOG
.
info
(
+
TimelineUtils
.
dumpTimelineRecordtoJSON
(
domain
)
)
;
}
catch
(
Exception
e
)
{
public
static
Map
<
String
,
PlacementSpec
>
parse
(
String
specs
)
throws
IllegalArgumentException
{
@
Override
public
void
run
(
)
throws
YarnException
,
IOException
,
InterruptedException
{
super
.
run
(
)
;
if
(
appAttemptID
.
getAttemptId
(
)
==
2
)
{
if
(
numAllocatedContainers
.
get
(
)
!=
1
||
numRequestedContainers
.
get
(
)
!=
numTotalContainers
)
{
@
Test
(
timeout
=
90000
)
public
void
testDSShellWithNodeLabelExpression
(
)
throws
Exception
{
initializeNodeLabels
(
)
;
NMContainerMonitor
mon
=
new
NMContainerMonitor
(
)
;
Thread
t
=
new
Thread
(
mon
)
;
t
.
start
(
)
;
String
[
]
args
=
{
,
TestDistributedShell
.
APPMASTER_JAR
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
,
}
;
LOG
.
info
(
)
;
final
Client
client
=
new
Client
(
new
Configuration
(
distShellTest
.
yarnCluster
.
getConfig
(
)
)
)
;
boolean
initSuccess
=
client
.
init
(
args
)
;
Assert
.
assertTrue
(
initSuccess
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
@
Test
(
timeout
=
90000
)
public
void
testDistributedShellWithPlacementConstraint
(
)
throws
Exception
{
NMContainerMonitor
mon
=
new
NMContainerMonitor
(
)
;
Thread
t
=
new
Thread
(
mon
)
;
t
.
start
(
)
;
String
[
]
args
=
{
,
distShellTest
.
APPMASTER_JAR
,
,
,
distShellTest
.
getSleepCommand
(
15
)
,
,
}
;
LOG
.
info
(
)
;
final
Client
client
=
new
Client
(
new
Configuration
(
distShellTest
.
yarnCluster
.
getConfig
(
)
)
)
;
boolean
initSuccess
=
client
.
init
(
args
)
;
Assert
.
assertTrue
(
initSuccess
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
private
void
checkTimelineV2
(
boolean
haveDomain
,
ApplicationId
appId
,
boolean
defaultFlow
,
ApplicationReport
appReport
)
throws
Exception
{
LOG
.
info
(
)
;
String
tmpRoot
=
timelineV2StorageDir
+
File
.
separator
+
+
File
.
separator
;
File
tmpRootFolder
=
new
File
(
tmpRoot
)
;
try
{
Assert
.
assertTrue
(
tmpRootFolder
.
isDirectory
(
)
)
;
String
basePath
=
tmpRoot
+
YarnConfiguration
.
DEFAULT_RM_CLUSTER_ID
+
File
.
separator
+
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
+
(
defaultFlow
?
File
.
separator
+
appReport
.
getName
(
)
+
File
.
separator
+
TimelineUtils
.
DEFAULT_FLOW_VERSION
+
File
.
separator
+
appReport
.
getStartTime
(
)
+
File
.
separator
:
File
.
separator
+
+
File
.
separator
+
+
File
.
separator
+
+
File
.
separator
)
+
appId
.
toString
(
)
;
private
File
verifyEntityTypeFileExists
(
String
basePath
,
String
entityType
,
String
entityfileName
)
{
String
outputDirPathForEntity
=
basePath
+
File
.
separator
+
entityType
+
File
.
separator
;
@
Test
public
void
testDSRestartWithPreviousRunningContainers
(
)
throws
Exception
{
String
[
]
args
=
{
,
APPMASTER_JAR
,
,
,
,
getSleepCommand
(
8
)
,
,
,
,
,
}
;
LOG
.
info
(
)
;
Client
client
=
new
Client
(
TestDSFailedAppMaster
.
class
.
getName
(
)
,
new
Configuration
(
yarnCluster
.
getConfig
(
)
)
)
;
client
.
init
(
args
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
@
Test
public
void
testDSAttemptFailuresValidityIntervalSucess
(
)
throws
Exception
{
String
[
]
args
=
{
,
APPMASTER_JAR
,
,
,
,
getSleepCommand
(
8
)
,
,
,
,
,
,
}
;
LOG
.
info
(
)
;
Configuration
conf
=
yarnCluster
.
getConfig
(
)
;
conf
.
setInt
(
YarnConfiguration
.
RM_AM_MAX_ATTEMPTS
,
2
)
;
Client
client
=
new
Client
(
TestDSSleepingAppMaster
.
class
.
getName
(
)
,
new
Configuration
(
conf
)
)
;
client
.
init
(
args
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
@
Test
public
void
testDSAttemptFailuresValidityIntervalFailed
(
)
throws
Exception
{
String
[
]
args
=
{
,
APPMASTER_JAR
,
,
,
,
getSleepCommand
(
8
)
,
,
,
,
,
,
}
;
LOG
.
info
(
)
;
Configuration
conf
=
yarnCluster
.
getConfig
(
)
;
conf
.
setInt
(
YarnConfiguration
.
RM_AM_MAX_ATTEMPTS
,
2
)
;
Client
client
=
new
Client
(
TestDSSleepingAppMaster
.
class
.
getName
(
)
,
new
Configuration
(
conf
)
)
;
client
.
init
(
args
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
public
void
testDSShellWithCommands
(
)
throws
Exception
{
String
[
]
args
=
{
,
APPMASTER_JAR
,
,
,
,
,
,
,
,
,
,
,
,
}
;
LOG
.
info
(
)
;
final
Client
client
=
new
Client
(
new
Configuration
(
yarnCluster
.
getConfig
(
)
)
)
;
boolean
initSuccess
=
client
.
init
(
args
)
;
Assert
.
assertTrue
(
initSuccess
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
@
Test
public
void
testDSShellWithMultipleArgs
(
)
throws
Exception
{
String
[
]
args
=
{
,
APPMASTER_JAR
,
,
,
,
,
,
,
,
,
,
,
,
,
,
}
;
LOG
.
info
(
)
;
final
Client
client
=
new
Client
(
new
Configuration
(
yarnCluster
.
getConfig
(
)
)
)
;
boolean
initSuccess
=
client
.
init
(
args
)
;
Assert
.
assertTrue
(
initSuccess
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
if
(
customShellScript
.
exists
(
)
)
{
customShellScript
.
delete
(
)
;
}
if
(
!
customShellScript
.
createNewFile
(
)
)
{
Assert
.
fail
(
)
;
}
PrintWriter
fileWriter
=
new
PrintWriter
(
customShellScript
)
;
fileWriter
.
write
(
)
;
fileWriter
.
close
(
)
;
System
.
out
.
println
(
customShellScript
.
getAbsolutePath
(
)
)
;
String
[
]
args
=
{
,
APPMASTER_JAR
,
,
,
,
customShellScript
.
getAbsolutePath
(
)
,
,
,
,
,
,
,
,
}
;
LOG
.
info
(
)
;
final
Client
client
=
new
Client
(
new
Configuration
(
yarnCluster
.
getConfig
(
)
)
)
;
boolean
initSuccess
=
client
.
init
(
args
)
;
Assert
.
assertTrue
(
initSuccess
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
@
Test
public
void
testContainerLaunchFailureHandling
(
)
throws
Exception
{
String
[
]
args
=
{
,
APPMASTER_JAR
,
,
,
,
Shell
.
WINDOWS
?
:
,
,
,
,
}
;
LOG
.
info
(
)
;
Client
client
=
new
Client
(
ContainerLaunchFailAppMaster
.
class
.
getName
(
)
,
new
Configuration
(
yarnCluster
.
getConfig
(
)
)
)
;
boolean
initSuccess
=
client
.
init
(
args
)
;
Assert
.
assertTrue
(
initSuccess
)
;
LOG
.
info
(
)
;
boolean
result
=
client
.
run
(
)
;
appContext
.
setApplicationName
(
appName
)
;
Priority
pri
=
Records
.
newRecord
(
Priority
.
class
)
;
pri
.
setPriority
(
amPriority
)
;
appContext
.
setPriority
(
pri
)
;
appContext
.
setQueue
(
amQueue
)
;
ContainerLaunchContext
amContainer
=
Records
.
newRecord
(
ContainerLaunchContext
.
class
)
;
appContext
.
setAMContainerSpec
(
amContainer
)
;
appContext
.
setUnmanagedAM
(
true
)
;
LOG
.
info
(
)
;
LOG
.
info
(
)
;
rmClient
.
submitApplication
(
appContext
)
;
ApplicationReport
appReport
=
monitorApplication
(
appId
,
EnumSet
.
of
(
YarnApplicationState
.
ACCEPTED
,
YarnApplicationState
.
KILLED
,
YarnApplicationState
.
FAILED
,
YarnApplicationState
.
FINISHED
)
)
;
if
(
appReport
.
getYarnApplicationState
(
)
==
YarnApplicationState
.
ACCEPTED
)
{
ApplicationAttemptReport
attemptReport
=
monitorCurrentAppAttempt
(
appId
,
YarnApplicationAttemptState
.
LAUNCHED
)
;
ApplicationAttemptId
attemptId
=
attemptReport
.
getApplicationAttemptId
(
)
;
ContainerLaunchContext
amContainer
=
Records
.
newRecord
(
ContainerLaunchContext
.
class
)
;
appContext
.
setAMContainerSpec
(
amContainer
)
;
appContext
.
setUnmanagedAM
(
true
)
;
LOG
.
info
(
)
;
LOG
.
info
(
)
;
rmClient
.
submitApplication
(
appContext
)
;
ApplicationReport
appReport
=
monitorApplication
(
appId
,
EnumSet
.
of
(
YarnApplicationState
.
ACCEPTED
,
YarnApplicationState
.
KILLED
,
YarnApplicationState
.
FAILED
,
YarnApplicationState
.
FINISHED
)
)
;
if
(
appReport
.
getYarnApplicationState
(
)
==
YarnApplicationState
.
ACCEPTED
)
{
ApplicationAttemptReport
attemptReport
=
monitorCurrentAppAttempt
(
appId
,
YarnApplicationAttemptState
.
LAUNCHED
)
;
ApplicationAttemptId
attemptId
=
attemptReport
.
getApplicationAttemptId
(
)
;
LOG
.
info
(
+
attemptId
)
;
launchAM
(
attemptId
)
;
appReport
=
monitorApplication
(
appId
,
EnumSet
.
of
(
YarnApplicationState
.
KILLED
,
YarnApplicationState
.
FAILED
,
YarnApplicationState
.
FINISHED
)
)
;
}
YarnApplicationState
appState
=
appReport
.
getYarnApplicationState
(
)
;
FinalApplicationStatus
appStatus
=
appReport
.
getFinalApplicationStatus
(
)
;
ApplicationReport
appReport
=
monitorApplication
(
appId
,
EnumSet
.
of
(
YarnApplicationState
.
ACCEPTED
,
YarnApplicationState
.
KILLED
,
YarnApplicationState
.
FAILED
,
YarnApplicationState
.
FINISHED
)
)
;
if
(
appReport
.
getYarnApplicationState
(
)
==
YarnApplicationState
.
ACCEPTED
)
{
ApplicationAttemptReport
attemptReport
=
monitorCurrentAppAttempt
(
appId
,
YarnApplicationAttemptState
.
LAUNCHED
)
;
ApplicationAttemptId
attemptId
=
attemptReport
.
getApplicationAttemptId
(
)
;
LOG
.
info
(
+
attemptId
)
;
launchAM
(
attemptId
)
;
appReport
=
monitorApplication
(
appId
,
EnumSet
.
of
(
YarnApplicationState
.
KILLED
,
YarnApplicationState
.
FAILED
,
YarnApplicationState
.
FINISHED
)
)
;
}
YarnApplicationState
appState
=
appReport
.
getYarnApplicationState
(
)
;
FinalApplicationStatus
appStatus
=
appReport
.
getFinalApplicationStatus
(
)
;
LOG
.
info
(
+
appReport
.
getYarnApplicationState
(
)
+
+
appStatus
)
;
boolean
success
;
if
(
YarnApplicationState
.
FINISHED
==
appState
&&
FinalApplicationStatus
.
SUCCEEDED
==
appStatus
)
{
LOG
.
info
(
)
;
success
=
true
;
}
else
{
while
(
true
)
{
if
(
attemptId
==
null
)
{
attemptId
=
rmClient
.
getApplicationReport
(
appId
)
.
getCurrentApplicationAttemptId
(
)
;
}
ApplicationAttemptReport
attemptReport
=
null
;
if
(
attemptId
!=
null
)
{
attemptReport
=
rmClient
.
getApplicationAttemptReport
(
attemptId
)
;
if
(
attemptState
.
equals
(
attemptReport
.
getYarnApplicationAttemptState
(
)
)
)
{
return
attemptReport
;
}
}
LOG
.
info
(
+
appId
+
+
(
attemptReport
==
null
?
:
attemptReport
.
getYarnApplicationAttemptState
(
)
)
+
+
attemptState
)
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
warn
(
+
appId
+
+
attemptState
)
;
}
if
(
System
.
currentTimeMillis
(
)
-
startTime
>
AM_STATE_WAIT_TIMEOUT_MS
)
{
String
errmsg
=
+
appId
+
+
attemptState
;
long
foundAMCompletedTime
=
0
;
StringBuilder
expectedFinalState
=
new
StringBuilder
(
)
;
boolean
first
=
true
;
for
(
YarnApplicationState
state
:
finalState
)
{
if
(
first
)
{
first
=
false
;
expectedFinalState
.
append
(
state
.
name
(
)
)
;
}
else
{
expectedFinalState
.
append
(
+
state
.
name
(
)
)
;
}
}
while
(
true
)
{
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
debug
(
)
;
}
ApplicationReport
report
=
rmClient
.
getApplicationReport
(
appId
)
;
@
BeforeClass
public
static
void
setup
(
)
throws
InterruptedException
,
IOException
{
LOG
.
info
(
)
;
conf
.
setInt
(
YarnConfiguration
.
RM_SCHEDULER_MINIMUM_ALLOCATION_MB
,
128
)
;
if
(
yarnCluster
==
null
)
{
yarnCluster
=
new
MiniYARNCluster
(
TestUnmanagedAMLauncher
.
class
.
getSimpleName
(
)
,
1
,
1
,
1
)
;
yarnCluster
.
init
(
conf
)
;
yarnCluster
.
start
(
)
;
Configuration
yarnClusterConfig
=
yarnCluster
.
getConfig
(
)
;
@
BeforeClass
public
static
void
setup
(
)
throws
InterruptedException
,
IOException
{
LOG
.
info
(
)
;
conf
.
setInt
(
YarnConfiguration
.
RM_SCHEDULER_MINIMUM_ALLOCATION_MB
,
128
)
;
if
(
yarnCluster
==
null
)
{
yarnCluster
=
new
MiniYARNCluster
(
TestUnmanagedAMLauncher
.
class
.
getSimpleName
(
)
,
1
,
1
,
1
)
;
yarnCluster
.
init
(
conf
)
;
yarnCluster
.
start
(
)
;
Configuration
yarnClusterConfig
=
yarnCluster
.
getConfig
(
)
;
LOG
.
info
(
+
yarnClusterConfig
.
get
(
YarnConfiguration
.
RM_ADDRESS
)
)
;
@
BeforeClass
public
static
void
setup
(
)
throws
InterruptedException
,
IOException
{
LOG
.
info
(
)
;
conf
.
setInt
(
YarnConfiguration
.
RM_SCHEDULER_MINIMUM_ALLOCATION_MB
,
128
)
;
if
(
yarnCluster
==
null
)
{
yarnCluster
=
new
MiniYARNCluster
(
TestUnmanagedAMLauncher
.
class
.
getSimpleName
(
)
,
1
,
1
,
1
)
;
yarnCluster
.
init
(
conf
)
;
yarnCluster
.
start
(
)
;
Configuration
yarnClusterConfig
=
yarnCluster
.
getConfig
(
)
;
LOG
.
info
(
+
yarnClusterConfig
.
get
(
YarnConfiguration
.
RM_ADDRESS
)
)
;
LOG
.
info
(
+
yarnClusterConfig
.
get
(
YarnConfiguration
.
RM_WEBAPP_ADDRESS
)
)
;
String
webapp
=
yarnClusterConfig
.
get
(
YarnConfiguration
.
RM_WEBAPP_ADDRESS
)
;
assertTrue
(
+
webapp
,
!
webapp
.
startsWith
(
)
)
;
String
webapp
=
yarnClusterConfig
.
get
(
YarnConfiguration
.
RM_WEBAPP_ADDRESS
)
;
assertTrue
(
+
webapp
,
!
webapp
.
startsWith
(
)
)
;
LOG
.
info
(
+
webapp
)
;
URL
url
=
Thread
.
currentThread
(
)
.
getContextClassLoader
(
)
.
getResource
(
)
;
if
(
url
==
null
)
{
throw
new
RuntimeException
(
)
;
}
ByteArrayOutputStream
bytesOut
=
new
ByteArrayOutputStream
(
)
;
yarnClusterConfig
.
writeXml
(
bytesOut
)
;
bytesOut
.
close
(
)
;
OutputStream
os
=
new
FileOutputStream
(
new
File
(
url
.
getPath
(
)
)
)
;
os
.
write
(
bytesOut
.
toByteArray
(
)
)
;
os
.
close
(
)
;
}
try
{
Thread
.
sleep
(
2000
)
;
}
catch
(
InterruptedException
e
)
{
if
(
javaHome
==
null
)
{
LOG
.
error
(
)
;
return
;
}
String
[
]
args
=
{
,
classpath
,
,
,
,
javaHome
+
+
TestUnmanagedAMLauncher
.
class
.
getCanonicalName
(
)
+
}
;
LOG
.
info
(
)
;
UnmanagedAMLauncher
launcher
=
new
UnmanagedAMLauncher
(
new
Configuration
(
yarnCluster
.
getConfig
(
)
)
)
{
public
void
launchAM
(
ApplicationAttemptId
attemptId
)
throws
IOException
,
YarnException
{
YarnApplicationAttemptState
attemptState
=
rmClient
.
getApplicationAttemptReport
(
attemptId
)
.
getYarnApplicationAttemptState
(
)
;
Assert
.
assertTrue
(
attemptState
.
equals
(
YarnApplicationAttemptState
.
LAUNCHED
)
)
;
super
.
launchAM
(
attemptId
)
;
}
}
;
boolean
initSuccess
=
launcher
.
init
(
args
)
;
Assert
.
assertTrue
(
initSuccess
)
;
LOG
.
info
(
)
;
boolean
result
=
launcher
.
run
(
)
;
}
if
(
HAUtil
.
isHAEnabled
(
conf
)
)
{
boolean
useKerberos
=
UserGroupInformation
.
isSecurityEnabled
(
)
;
List
<
String
>
rmServers
=
getRMHAWebAddresses
(
conf
)
;
StringBuilder
diagnosticsMsg
=
new
StringBuilder
(
)
;
for
(
String
host
:
rmServers
)
{
try
{
Client
client
=
Client
.
create
(
)
;
client
.
setFollowRedirects
(
false
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
scheme
)
.
append
(
host
)
.
append
(
path
)
;
if
(
!
useKerberos
)
{
try
{
String
username
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
sb
.
append
(
)
.
append
(
username
)
;
}
catch
(
IOException
e
)
{
try
{
Client
client
=
Client
.
create
(
)
;
client
.
setFollowRedirects
(
false
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
scheme
)
.
append
(
host
)
.
append
(
path
)
;
if
(
!
useKerberos
)
{
try
{
String
username
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
sb
.
append
(
)
.
append
(
username
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
e
)
;
}
}
Builder
builder
=
client
.
resource
(
sb
.
toString
(
)
)
.
type
(
MediaType
.
APPLICATION_JSON
)
;
if
(
useKerberos
)
{
String
[
]
server
=
host
.
split
(
)
;
String
challenge
=
YarnClientUtils
.
generateToken
(
server
[
0
]
)
;
if
(
!
useKerberos
)
{
try
{
String
username
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
sb
.
append
(
)
.
append
(
username
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
e
)
;
}
}
Builder
builder
=
client
.
resource
(
sb
.
toString
(
)
)
.
type
(
MediaType
.
APPLICATION_JSON
)
;
if
(
useKerberos
)
{
String
[
]
server
=
host
.
split
(
)
;
String
challenge
=
YarnClientUtils
.
generateToken
(
server
[
0
]
)
;
builder
.
header
(
HttpHeaders
.
AUTHORIZATION
,
+
challenge
)
;
LOG
.
debug
(
,
challenge
)
;
}
ClientResponse
test
=
builder
.
get
(
ClientResponse
.
class
)
;
if
(
test
.
getStatus
(
)
==
200
)
{
return
scheme
+
host
;
try
{
String
username
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
sb
.
append
(
)
.
append
(
username
)
;
}
catch
(
IOException
e
)
{
LOG
.
debug
(
,
e
)
;
}
}
Builder
builder
=
client
.
resource
(
sb
.
toString
(
)
)
.
type
(
MediaType
.
APPLICATION_JSON
)
;
if
(
useKerberos
)
{
String
[
]
server
=
host
.
split
(
)
;
String
challenge
=
YarnClientUtils
.
generateToken
(
server
[
0
]
)
;
builder
.
header
(
HttpHeaders
.
AUTHORIZATION
,
+
challenge
)
;
LOG
.
debug
(
,
challenge
)
;
}
ClientResponse
test
=
builder
.
get
(
ClientResponse
.
class
)
;
if
(
test
.
getStatus
(
)
==
200
)
{
return
scheme
+
host
;
}
}
catch
(
Exception
e
)
{
String
output
;
if
(
response
.
getStatus
(
)
==
401
)
{
LOG
.
error
(
)
;
return
EXIT_EXCEPTION_THROWN
;
}
if
(
response
.
getStatus
(
)
==
503
)
{
LOG
.
error
(
)
;
return
EXIT_EXCEPTION_THROWN
;
}
try
{
ServiceStatus
ss
=
response
.
getEntity
(
ServiceStatus
.
class
)
;
output
=
ss
.
getDiagnostics
(
)
;
}
catch
(
Throwable
t
)
{
output
=
response
.
getEntity
(
String
.
class
)
;
}
if
(
output
==
null
)
{
output
=
response
.
getEntity
(
String
.
class
)
;
}
if
(
response
.
getStatus
(
)
<=
299
)
{
return
EXIT_EXCEPTION_THROWN
;
}
if
(
response
.
getStatus
(
)
==
503
)
{
LOG
.
error
(
)
;
return
EXIT_EXCEPTION_THROWN
;
}
try
{
ServiceStatus
ss
=
response
.
getEntity
(
ServiceStatus
.
class
)
;
output
=
ss
.
getDiagnostics
(
)
;
}
catch
(
Throwable
t
)
{
output
=
response
.
getEntity
(
String
.
class
)
;
}
if
(
output
==
null
)
{
output
=
response
.
getEntity
(
String
.
class
)
;
}
if
(
response
.
getStatus
(
)
<=
299
)
{
LOG
.
info
(
output
)
;
return
EXIT_SUCCESS
;
}
else
{
if
(
examplesDirStr
==
null
)
{
String
yarnHome
=
System
.
getenv
(
ApplicationConstants
.
Environment
.
HADOOP_YARN_HOME
.
key
(
)
)
;
examplesDirs
=
new
String
[
]
{
yarnHome
+
,
yarnHome
+
}
;
}
else
{
examplesDirs
=
StringUtils
.
split
(
examplesDirStr
,
)
;
}
for
(
String
dir
:
examplesDirs
)
{
file
=
new
File
(
MessageFormat
.
format
(
,
dir
,
fileName
,
fileName
)
)
;
if
(
file
.
exists
(
)
)
{
break
;
}
file
=
new
File
(
MessageFormat
.
format
(
,
dir
,
fileName
)
)
;
if
(
file
.
exists
(
)
)
{
break
;
}
}
}
if
(
!
file
.
exists
(
)
)
{
throw
new
YarnException
(
+
fileName
)
;
}
Path
filePath
=
new
Path
(
file
.
getAbsolutePath
(
)
)
;
int
result
=
EXIT_SUCCESS
;
try
{
Service
service
=
new
Service
(
)
;
service
.
setName
(
appName
)
;
service
.
setState
(
ServiceState
.
FLEX
)
;
for
(
Map
.
Entry
<
String
,
String
>
entry
:
componentCounts
.
entrySet
(
)
)
{
Component
component
=
new
Component
(
)
;
component
.
setName
(
entry
.
getKey
(
)
)
;
Long
numberOfContainers
=
Long
.
parseLong
(
entry
.
getValue
(
)
)
;
component
.
setNumberOfContainers
(
numberOfContainers
)
;
service
.
addComponent
(
component
)
;
}
String
buffer
=
jsonSerDeser
.
toJson
(
service
)
;
ClientResponse
response
=
getApiClient
(
getServicePath
(
appName
)
)
.
put
(
ClientResponse
.
class
,
buffer
)
;
result
=
processResponse
(
response
)
;
}
catch
(
Exception
e
)
{
}
catch
(
IllegalArgumentException
e
)
{
appName
=
appIdOrName
;
ServiceApiUtil
.
validateNameFormat
(
appName
,
getConfig
(
)
)
;
}
try
{
ClientResponse
response
=
getApiClient
(
getServicePath
(
appName
)
)
.
get
(
ClientResponse
.
class
)
;
if
(
response
.
getStatus
(
)
==
404
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
)
.
append
(
appName
)
.
append
(
)
;
return
sb
.
toString
(
)
;
}
if
(
response
.
getStatus
(
)
!=
200
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
appName
)
.
append
(
)
.
append
(
response
.
getStatus
(
)
)
;
return
sb
.
toString
(
)
;
}
output
=
response
.
getEntity
(
String
.
class
)
;
}
catch
(
Exception
e
)
{
try
{
Service
service
=
new
Service
(
)
;
service
.
setName
(
appName
)
;
for
(
String
instance
:
componentInstances
)
{
String
componentName
=
ServiceApiUtil
.
parseComponentName
(
instance
)
;
Component
component
=
service
.
getComponent
(
componentName
)
;
if
(
component
==
null
)
{
component
=
new
Component
(
)
;
component
.
setName
(
componentName
)
;
service
.
addComponent
(
component
)
;
}
component
.
addDecommissionedInstance
(
instance
)
;
}
String
buffer
=
jsonSerDeser
.
toJson
(
service
)
;
ClientResponse
response
=
getApiClient
(
getServicePath
(
appName
)
)
.
put
(
ClientResponse
.
class
,
buffer
)
;
result
=
processResponse
(
response
)
;
}
catch
(
Exception
e
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
String
dirPath
=
conf
.
get
(
YarnServiceConf
.
YARN_SERVICES_SYSTEM_SERVICE_DIRECTORY
)
;
if
(
dirPath
!=
null
)
{
systemServiceDir
=
new
Path
(
dirPath
)
;
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
String
dirPath
=
conf
.
get
(
YarnServiceConf
.
YARN_SERVICES_SYSTEM_SERVICE_DIRECTORY
)
;
if
(
dirPath
!=
null
)
{
systemServiceDir
=
new
Path
(
dirPath
)
;
LOG
.
info
(
,
systemServiceDir
)
;
fs
=
systemServiceDir
.
getFileSystem
(
conf
)
;
this
.
loginUGI
=
UserGroupInformation
.
isSecurityEnabled
(
)
?
UserGroupInformation
.
getLoginUser
(
)
:
UserGroupInformation
.
getCurrentUser
(
)
;
if
(
services
.
isEmpty
(
)
)
{
continue
;
}
ServiceClient
serviceClient
=
null
;
try
{
UserGroupInformation
userUgi
=
getProxyUser
(
user
)
;
serviceClient
=
createServiceClient
(
userUgi
)
;
for
(
Service
service
:
services
)
{
LOG
.
info
(
,
service
,
userUgi
)
;
try
{
launchServices
(
userUgi
,
serviceClient
,
service
)
;
}
catch
(
IOException
|
UndeclaredThrowableException
e
)
{
if
(
e
.
getCause
(
)
!=
null
)
{
LOG
.
warn
(
e
.
getCause
(
)
.
getMessage
(
)
)
;
}
else
{
String
message
=
+
service
.
getName
(
)
+
;
UserGroupInformation
userUgi
=
getProxyUser
(
user
)
;
serviceClient
=
createServiceClient
(
userUgi
)
;
for
(
Service
service
:
services
)
{
LOG
.
info
(
,
service
,
userUgi
)
;
try
{
launchServices
(
userUgi
,
serviceClient
,
service
)
;
}
catch
(
IOException
|
UndeclaredThrowableException
e
)
{
if
(
e
.
getCause
(
)
!=
null
)
{
LOG
.
warn
(
e
.
getCause
(
)
.
getMessage
(
)
)
;
}
else
{
String
message
=
+
service
.
getName
(
)
+
;
LOG
.
error
(
message
,
e
)
;
}
}
}
}
catch
(
InterruptedException
e
)
{
LOG
.
warn
(
,
e
)
;
break
;
if
(
service
.
getState
(
)
==
ServiceState
.
STOPPED
)
{
userUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
IOException
,
YarnException
{
serviceClient
.
actionBuild
(
service
)
;
return
null
;
}
}
)
;
LOG
.
info
(
,
service
.
getName
(
)
,
service
.
getVersion
(
)
)
;
}
else
{
ApplicationId
applicationId
=
userUgi
.
doAs
(
new
PrivilegedExceptionAction
<
ApplicationId
>
(
)
{
@
Override
public
ApplicationId
run
(
)
throws
IOException
,
YarnException
{
boolean
tryStart
=
true
;
try
{
serviceClient
.
actionBuild
(
service
)
;
}
catch
(
Exception
e
)
{
if
(
e
instanceof
SliderException
&&
(
(
SliderException
)
e
)
.
getExitCode
(
)
==
SliderExitCodes
.
EXIT_INSTANCE_EXISTS
)
{
serviceClient
.
actionBuild
(
service
)
;
return
null
;
}
}
)
;
LOG
.
info
(
,
service
.
getName
(
)
,
service
.
getVersion
(
)
)
;
}
else
{
ApplicationId
applicationId
=
userUgi
.
doAs
(
new
PrivilegedExceptionAction
<
ApplicationId
>
(
)
{
@
Override
public
ApplicationId
run
(
)
throws
IOException
,
YarnException
{
boolean
tryStart
=
true
;
try
{
serviceClient
.
actionBuild
(
service
)
;
}
catch
(
Exception
e
)
{
if
(
e
instanceof
SliderException
&&
(
(
SliderException
)
e
)
.
getExitCode
(
)
==
SliderExitCodes
.
EXIT_INSTANCE_EXISTS
)
{
LOG
.
info
(
+
,
service
.
getName
(
)
)
;
}
else
{
tryStart
=
false
;
else
{
ApplicationId
applicationId
=
userUgi
.
doAs
(
new
PrivilegedExceptionAction
<
ApplicationId
>
(
)
{
@
Override
public
ApplicationId
run
(
)
throws
IOException
,
YarnException
{
boolean
tryStart
=
true
;
try
{
serviceClient
.
actionBuild
(
service
)
;
}
catch
(
Exception
e
)
{
if
(
e
instanceof
SliderException
&&
(
(
SliderException
)
e
)
.
getExitCode
(
)
==
SliderExitCodes
.
EXIT_INSTANCE_EXISTS
)
{
LOG
.
info
(
+
,
service
.
getName
(
)
)
;
}
else
{
tryStart
=
false
;
LOG
.
info
(
+
,
service
.
getName
(
)
,
e
)
;
}
}
if
(
tryStart
)
{
return
serviceClient
.
actionStartAndGetId
(
service
.
getName
(
)
)
;
}
else
{
return
;
}
try
{
LOG
.
info
(
,
systemServiceDir
)
;
RemoteIterator
<
FileStatus
>
iterLaunchType
=
list
(
systemServiceDir
)
;
while
(
iterLaunchType
.
hasNext
(
)
)
{
FileStatus
launchType
=
iterLaunchType
.
next
(
)
;
if
(
!
launchType
.
isDirectory
(
)
)
{
LOG
.
debug
(
,
launchType
.
getPath
(
)
)
;
continue
;
}
if
(
launchType
.
getPath
(
)
.
getName
(
)
.
equals
(
SYNC
)
)
{
scanForUserServiceDefinition
(
launchType
.
getPath
(
)
,
syncUserServices
)
;
}
else
if
(
launchType
.
getPath
(
)
.
getName
(
)
.
equals
(
ASYNC
)
)
{
scanForUserServiceDefinition
(
launchType
.
getPath
(
)
,
asyncUserServices
)
;
}
else
{
badDirSkipCounter
++
;
private
void
scanForUserServiceDefinition
(
Path
userDirPath
,
Map
<
String
,
Set
<
Service
>>
userServices
)
throws
IOException
{
private
void
scanForUserServiceDefinition
(
Path
userDirPath
,
Map
<
String
,
Set
<
Service
>>
userServices
)
throws
IOException
{
LOG
.
info
(
,
userDirPath
)
;
RemoteIterator
<
FileStatus
>
iterUsers
=
list
(
userDirPath
)
;
while
(
iterUsers
.
hasNext
(
)
)
{
FileStatus
userDir
=
iterUsers
.
next
(
)
;
if
(
!
userDir
.
isDirectory
(
)
)
{
LOG
.
info
(
,
userDir
.
getPath
(
)
.
getName
(
)
)
;
continue
;
}
String
userName
=
userDir
.
getPath
(
)
.
getName
(
)
;
LOG
.
info
(
,
userName
)
;
RemoteIterator
<
FileStatus
>
iterServices
=
list
(
userDir
.
getPath
(
)
)
;
while
(
iterServices
.
hasNext
(
)
)
{
FileStatus
serviceCache
=
iterServices
.
next
(
)
;
String
filename
=
serviceCache
.
getPath
(
)
.
getName
(
)
;
if
(
!
serviceCache
.
isFile
(
)
)
{
while
(
iterUsers
.
hasNext
(
)
)
{
FileStatus
userDir
=
iterUsers
.
next
(
)
;
if
(
!
userDir
.
isDirectory
(
)
)
{
LOG
.
info
(
,
userDir
.
getPath
(
)
.
getName
(
)
)
;
continue
;
}
String
userName
=
userDir
.
getPath
(
)
.
getName
(
)
;
LOG
.
info
(
,
userName
)
;
RemoteIterator
<
FileStatus
>
iterServices
=
list
(
userDir
.
getPath
(
)
)
;
while
(
iterServices
.
hasNext
(
)
)
{
FileStatus
serviceCache
=
iterServices
.
next
(
)
;
String
filename
=
serviceCache
.
getPath
(
)
.
getName
(
)
;
if
(
!
serviceCache
.
isFile
(
)
)
{
LOG
.
info
(
,
filename
)
;
continue
;
}
if
(
!
filename
.
endsWith
(
YARN_FILE_SUFFIX
)
)
{
continue
;
}
if
(
!
filename
.
endsWith
(
YARN_FILE_SUFFIX
)
)
{
LOG
.
info
(
,
filename
)
;
badFileNameExtensionSkipCounter
++
;
continue
;
}
Service
service
=
getServiceDefinition
(
serviceCache
.
getPath
(
)
)
;
if
(
service
!=
null
)
{
Set
<
Service
>
services
=
userServices
.
get
(
userName
)
;
if
(
services
==
null
)
{
services
=
new
HashSet
<
>
(
)
;
userServices
.
put
(
userName
,
services
)
;
}
if
(
!
services
.
add
(
service
)
)
{
int
count
=
ignoredUserServices
.
containsKey
(
userName
)
?
ignoredUserServices
.
get
(
userName
)
:
0
;
ignoredUserServices
.
put
(
userName
,
count
+
1
)
;
LOG
.
warn
(
+
,
service
.
getName
(
)
,
userName
,
filename
)
;
private
Service
getServiceDefinition
(
Path
filePath
)
{
Service
service
=
null
;
try
{
@
GET
@
Path
(
VERSION
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
@
Produces
(
{
MediaType
.
APPLICATION_JSON
+
}
)
public
Response
getVersion
(
)
{
String
version
=
VersionInfo
.
getBuildVersion
(
)
;
@
POST
@
Path
(
SERVICE_ROOT_PATH
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
@
Produces
(
{
MediaType
.
APPLICATION_JSON
+
}
)
public
Response
createService
(
@
Context
HttpServletRequest
request
,
Service
service
)
{
ServiceStatus
serviceStatus
=
new
ServiceStatus
(
)
;
try
{
UserGroupInformation
ugi
=
getProxyUser
(
request
)
;
ServiceClient
sc
=
getServiceClient
(
)
;
try
{
sc
.
init
(
YARN_CONFIG
)
;
sc
.
start
(
)
;
ApplicationId
applicationId
=
sc
.
actionCreate
(
service
)
;
return
applicationId
;
}
finally
{
sc
.
close
(
)
;
}
}
}
)
;
serviceStatus
.
setDiagnostics
(
+
applicationId
)
;
}
serviceStatus
.
setState
(
ACCEPTED
)
;
serviceStatus
.
setUri
(
CONTEXT_ROOT
+
SERVICE_ROOT_PATH
+
+
service
.
getName
(
)
)
;
return
formatResponse
(
Status
.
ACCEPTED
,
serviceStatus
)
;
}
catch
(
AccessControlException
e
)
{
serviceStatus
.
setDiagnostics
(
e
.
getMessage
(
)
)
;
ApplicationId
applicationId
=
sc
.
actionCreate
(
service
)
;
return
applicationId
;
}
finally
{
sc
.
close
(
)
;
}
}
}
)
;
serviceStatus
.
setDiagnostics
(
+
applicationId
)
;
}
serviceStatus
.
setState
(
ACCEPTED
)
;
serviceStatus
.
setUri
(
CONTEXT_ROOT
+
SERVICE_ROOT_PATH
+
+
service
.
getName
(
)
)
;
return
formatResponse
(
Status
.
ACCEPTED
,
serviceStatus
)
;
}
catch
(
AccessControlException
e
)
{
serviceStatus
.
setDiagnostics
(
e
.
getMessage
(
)
)
;
return
formatResponse
(
Status
.
FORBIDDEN
,
e
.
getCause
(
)
.
getMessage
(
)
)
;
}
catch
(
IllegalArgumentException
e
)
{
return
formatResponse
(
Status
.
BAD_REQUEST
,
e
.
getMessage
(
)
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
}
UserGroupInformation
ugi
=
getProxyUser
(
request
)
;
LOG
.
info
(
,
appName
,
ugi
)
;
Service
app
=
getServiceFromClient
(
ugi
,
appName
)
;
return
Response
.
ok
(
app
)
.
build
(
)
;
}
catch
(
AccessControlException
e
)
{
return
formatResponse
(
Status
.
FORBIDDEN
,
e
.
getMessage
(
)
)
;
}
catch
(
IllegalArgumentException
e
)
{
serviceStatus
.
setDiagnostics
(
e
.
getMessage
(
)
)
;
serviceStatus
.
setCode
(
ERROR_CODE_APP_NAME_INVALID
)
;
return
Response
.
status
(
Status
.
NOT_FOUND
)
.
entity
(
serviceStatus
)
.
build
(
)
;
}
catch
(
FileNotFoundException
e
)
{
serviceStatus
.
setDiagnostics
(
+
appName
+
)
;
serviceStatus
.
setCode
(
ERROR_CODE_APP_NAME_INVALID
)
;
return
Response
.
status
(
Status
.
NOT_FOUND
)
.
entity
(
serviceStatus
)
.
build
(
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
Service
app
=
getServiceFromClient
(
ugi
,
appName
)
;
return
Response
.
ok
(
app
)
.
build
(
)
;
}
catch
(
AccessControlException
e
)
{
return
formatResponse
(
Status
.
FORBIDDEN
,
e
.
getMessage
(
)
)
;
}
catch
(
IllegalArgumentException
e
)
{
serviceStatus
.
setDiagnostics
(
e
.
getMessage
(
)
)
;
serviceStatus
.
setCode
(
ERROR_CODE_APP_NAME_INVALID
)
;
return
Response
.
status
(
Status
.
NOT_FOUND
)
.
entity
(
serviceStatus
)
.
build
(
)
;
}
catch
(
FileNotFoundException
e
)
{
serviceStatus
.
setDiagnostics
(
+
appName
+
)
;
serviceStatus
.
setCode
(
ERROR_CODE_APP_NAME_INVALID
)
;
return
Response
.
status
(
Status
.
NOT_FOUND
)
.
entity
(
serviceStatus
)
.
build
(
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
LOG
.
error
(
,
e
)
;
return
formatResponse
(
Status
.
INTERNAL_SERVER_ERROR
,
e
.
getMessage
(
)
)
;
if
(
appName
==
null
)
{
throw
new
IllegalArgumentException
(
)
;
}
UserGroupInformation
ugi
=
getProxyUser
(
request
)
;
LOG
.
info
(
,
appName
,
ugi
)
;
return
stopService
(
appName
,
true
,
ugi
)
;
}
catch
(
AccessControlException
e
)
{
return
formatResponse
(
Status
.
FORBIDDEN
,
e
.
getMessage
(
)
)
;
}
catch
(
IllegalArgumentException
e
)
{
return
formatResponse
(
Status
.
BAD_REQUEST
,
e
.
getMessage
(
)
)
;
}
catch
(
UndeclaredThrowableException
e
)
{
LOG
.
error
(
,
e
)
;
return
formatResponse
(
Status
.
BAD_REQUEST
,
e
.
getCause
(
)
.
getMessage
(
)
)
;
}
catch
(
YarnException
|
FileNotFoundException
e
)
{
return
formatResponse
(
Status
.
NOT_FOUND
,
e
.
getMessage
(
)
)
;
}
catch
(
Exception
e
)
{
private
Response
stopService
(
String
appName
,
boolean
destroy
,
final
UserGroupInformation
ugi
)
throws
Exception
{
int
result
=
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
throws
Exception
{
int
result
=
0
;
ServiceClient
sc
=
getServiceClient
(
)
;
try
{
sc
.
init
(
YARN_CONFIG
)
;
sc
.
start
(
)
;
Exception
stopException
=
null
;
try
{
result
=
sc
.
actionStop
(
appName
,
destroy
)
;
if
(
result
==
EXIT_SUCCESS
)
{
ServiceClient
sc
=
getServiceClient
(
)
;
try
{
sc
.
init
(
YARN_CONFIG
)
;
sc
.
start
(
)
;
Exception
stopException
=
null
;
try
{
result
=
sc
.
actionStop
(
appName
,
destroy
)
;
if
(
result
==
EXIT_SUCCESS
)
{
LOG
.
info
(
,
appName
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
e
)
;
stopException
=
e
;
}
if
(
destroy
)
{
result
=
sc
.
actionDestroy
(
appName
)
;
if
(
result
==
EXIT_SUCCESS
)
{
@
PUT
@
Path
(
SERVICE_PATH
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
@
Produces
(
{
MediaType
.
APPLICATION_JSON
+
}
)
public
Response
updateService
(
@
Context
HttpServletRequest
request
,
@
PathParam
(
SERVICE_NAME
)
String
appName
,
Service
updateServiceData
)
{
try
{
UserGroupInformation
ugi
=
getProxyUser
(
request
)
;
if
(
updateServiceData
.
getState
(
)
!=
null
&&
updateServiceData
.
getState
(
)
==
ServiceState
.
STARTED
)
{
return
startService
(
appName
,
ugi
)
;
}
if
(
updateServiceData
.
getState
(
)
!=
null
&&
(
updateServiceData
.
getState
(
)
==
ServiceState
.
UPGRADING
||
updateServiceData
.
getState
(
)
==
ServiceState
.
UPGRADING_AUTO_FINALIZE
)
||
updateServiceData
.
getState
(
)
==
ServiceState
.
EXPRESS_UPGRADING
)
{
return
upgradeService
(
updateServiceData
,
ugi
)
;
}
if
(
updateServiceData
.
getState
(
)
!=
null
&&
updateServiceData
.
getState
(
)
==
CANCEL_UPGRADING
)
{
return
cancelUpgradeService
(
appName
,
ugi
)
;
}
if
(
updateServiceData
.
getLifetime
(
)
!=
null
&&
updateServiceData
.
getLifetime
(
)
>
0
)
{
return
updateLifetime
(
appName
,
updateServiceData
,
ugi
)
;
}
for
(
Component
c
:
updateServiceData
.
getComponents
(
)
)
{
if
(
c
.
getDecommissionedInstances
(
)
.
size
(
)
>
0
)
{
return
decommissionInstances
(
updateServiceData
,
ugi
)
;
}
}
}
catch
(
UndeclaredThrowableException
e
)
{
return
formatResponse
(
Status
.
BAD_REQUEST
,
e
.
getCause
(
)
.
getMessage
(
)
)
;
}
catch
(
AccessControlException
e
)
{
return
formatResponse
(
Status
.
FORBIDDEN
,
e
.
getMessage
(
)
)
;
if
(
updateServiceData
.
getState
(
)
!=
null
&&
(
updateServiceData
.
getState
(
)
==
ServiceState
.
UPGRADING
||
updateServiceData
.
getState
(
)
==
ServiceState
.
UPGRADING_AUTO_FINALIZE
)
||
updateServiceData
.
getState
(
)
==
ServiceState
.
EXPRESS_UPGRADING
)
{
return
upgradeService
(
updateServiceData
,
ugi
)
;
}
if
(
updateServiceData
.
getState
(
)
!=
null
&&
updateServiceData
.
getState
(
)
==
CANCEL_UPGRADING
)
{
return
cancelUpgradeService
(
appName
,
ugi
)
;
}
if
(
updateServiceData
.
getLifetime
(
)
!=
null
&&
updateServiceData
.
getLifetime
(
)
>
0
)
{
return
updateLifetime
(
appName
,
updateServiceData
,
ugi
)
;
}
for
(
Component
c
:
updateServiceData
.
getComponents
(
)
)
{
if
(
c
.
getDecommissionedInstances
(
)
.
size
(
)
>
0
)
{
return
decommissionInstances
(
updateServiceData
,
ugi
)
;
}
}
}
catch
(
UndeclaredThrowableException
e
)
{
return
formatResponse
(
Status
.
BAD_REQUEST
,
e
.
getCause
(
)
.
getMessage
(
)
)
;
}
catch
(
AccessControlException
e
)
{
return
formatResponse
(
Status
.
FORBIDDEN
,
e
.
getMessage
(
)
)
;
}
catch
(
FileNotFoundException
e
)
{
String
message
=
+
appName
;
return
cancelUpgradeService
(
appName
,
ugi
)
;
}
if
(
updateServiceData
.
getLifetime
(
)
!=
null
&&
updateServiceData
.
getLifetime
(
)
>
0
)
{
return
updateLifetime
(
appName
,
updateServiceData
,
ugi
)
;
}
for
(
Component
c
:
updateServiceData
.
getComponents
(
)
)
{
if
(
c
.
getDecommissionedInstances
(
)
.
size
(
)
>
0
)
{
return
decommissionInstances
(
updateServiceData
,
ugi
)
;
}
}
}
catch
(
UndeclaredThrowableException
e
)
{
return
formatResponse
(
Status
.
BAD_REQUEST
,
e
.
getCause
(
)
.
getMessage
(
)
)
;
}
catch
(
AccessControlException
e
)
{
return
formatResponse
(
Status
.
FORBIDDEN
,
e
.
getMessage
(
)
)
;
}
catch
(
FileNotFoundException
e
)
{
String
message
=
+
appName
;
LOG
.
error
(
message
,
e
)
;
return
formatResponse
(
Status
.
NOT_FOUND
,
e
.
getMessage
(
)
)
;
}
catch
(
YarnException
e
)
{
@
PUT
@
Path
(
COMP_INSTANCE_LONG_PATH
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
@
Produces
(
{
RestApiConstants
.
MEDIA_TYPE_JSON_UTF8
,
MediaType
.
TEXT_PLAIN
}
)
public
Response
updateComponentInstance
(
@
Context
HttpServletRequest
request
,
@
PathParam
(
SERVICE_NAME
)
String
serviceName
,
@
PathParam
(
COMPONENT_NAME
)
String
componentName
,
@
PathParam
(
COMP_INSTANCE_NAME
)
String
compInstanceName
,
Container
reqContainer
)
{
try
{
UserGroupInformation
ugi
=
getProxyUser
(
request
)
;
@
GET
@
Path
(
COMP_INSTANCES_PATH
)
@
Produces
(
{
RestApiConstants
.
MEDIA_TYPE_JSON_UTF8
}
)
public
Response
getComponentInstances
(
@
Context
HttpServletRequest
request
,
@
PathParam
(
SERVICE_NAME
)
String
serviceName
,
@
QueryParam
(
PARAM_COMP_NAME
)
List
<
String
>
componentNames
,
@
QueryParam
(
PARAM_VERSION
)
String
version
,
@
QueryParam
(
PARAM_CONTAINER_STATE
)
List
<
String
>
containerStates
)
{
try
{
UserGroupInformation
ugi
=
getProxyUser
(
request
)
;
componentCountStrings
.
put
(
c
.
getName
(
)
,
c
.
getNumberOfContainers
(
)
.
toString
(
)
)
;
}
Integer
result
=
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
throws
YarnException
,
IOException
{
int
result
=
0
;
ServiceClient
sc
=
new
ServiceClient
(
)
;
try
{
sc
.
init
(
YARN_CONFIG
)
;
sc
.
start
(
)
;
result
=
sc
.
actionFlex
(
appName
,
componentCountStrings
)
;
return
Integer
.
valueOf
(
result
)
;
}
finally
{
sc
.
close
(
)
;
}
}
}
)
;
if
(
result
==
EXIT_SUCCESS
)
{
String
message
=
+
appName
+
;
private
Response
upgradeService
(
Service
service
,
final
UserGroupInformation
ugi
)
throws
IOException
,
InterruptedException
{
ServiceStatus
status
=
new
ServiceStatus
(
)
;
ugi
.
doAs
(
(
PrivilegedExceptionAction
<
Void
>
)
(
)
->
{
ServiceClient
sc
=
getServiceClient
(
)
;
try
{
sc
.
init
(
YARN_CONFIG
)
;
sc
.
start
(
)
;
if
(
service
.
getState
(
)
.
equals
(
ServiceState
.
EXPRESS_UPGRADING
)
)
{
sc
.
actionUpgradeExpress
(
service
)
;
}
else
{
sc
.
initiateUpgrade
(
service
)
;
}
}
finally
{
sc
.
close
(
)
;
}
return
null
;
}
)
;
instances
.
addAll
(
c
.
getDecommissionedInstances
(
)
)
;
}
Integer
result
=
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
Integer
>
(
)
{
@
Override
public
Integer
run
(
)
throws
YarnException
,
IOException
{
int
result
=
0
;
ServiceClient
sc
=
new
ServiceClient
(
)
;
try
{
sc
.
init
(
YARN_CONFIG
)
;
sc
.
start
(
)
;
result
=
sc
.
actionDecommissionInstances
(
appName
,
instances
)
;
return
Integer
.
valueOf
(
result
)
;
}
finally
{
sc
.
close
(
)
;
}
}
}
)
;
if
(
result
==
EXIT_SUCCESS
)
{
String
message
=
+
appName
+
+
;
@
Override
protected
void
serviceStart
(
)
throws
Exception
{
bindAddress
=
getConfig
(
)
.
getSocketAddr
(
API_SERVER_ADDRESS
,
DEFAULT_API_SERVER_ADDRESS
,
DEFAULT_API_SERVER_PORT
)
;
private
void
startWebApp
(
)
throws
IOException
{
URI
uri
=
URI
.
create
(
+
NetUtils
.
getHostPortString
(
bindAddress
)
)
;
apiServer
=
new
HttpServer2
.
Builder
(
)
.
setName
(
)
.
setConf
(
getConfig
(
)
)
.
setSecurityEnabled
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
.
setUsernameConfKey
(
RM_WEBAPP_SPNEGO_USER_NAME_KEY
)
.
setKeytabConfKey
(
RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY
)
.
addEndpoint
(
uri
)
.
build
(
)
;
String
apiPackages
=
ApiServer
.
class
.
getPackage
(
)
.
getName
(
)
+
SEP
+
GenericExceptionHandler
.
class
.
getPackage
(
)
.
getName
(
)
+
SEP
+
YarnJacksonJaxbJsonProvider
.
class
.
getPackage
(
)
.
getName
(
)
;
apiServer
.
addJerseyResourcePackage
(
apiPackages
,
)
;
try
{
logger
.
info
(
)
;
apiServer
.
start
(
)
;
private
void
startWebApp
(
)
throws
IOException
{
URI
uri
=
URI
.
create
(
+
NetUtils
.
getHostPortString
(
bindAddress
)
)
;
apiServer
=
new
HttpServer2
.
Builder
(
)
.
setName
(
)
.
setConf
(
getConfig
(
)
)
.
setSecurityEnabled
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
.
setUsernameConfKey
(
RM_WEBAPP_SPNEGO_USER_NAME_KEY
)
.
setKeytabConfKey
(
RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY
)
.
addEndpoint
(
uri
)
.
build
(
)
;
String
apiPackages
=
ApiServer
.
class
.
getPackage
(
)
.
getName
(
)
+
SEP
+
GenericExceptionHandler
.
class
.
getPackage
(
)
.
getName
(
)
+
SEP
+
YarnJacksonJaxbJsonProvider
.
class
.
getPackage
(
)
.
getName
(
)
;
apiServer
.
addJerseyResourcePackage
(
apiPackages
,
)
;
try
{
logger
.
info
(
)
;
apiServer
.
start
(
)
;
logger
.
info
(
,
apiServer
.
toString
(
)
)
;
for
(
Configuration
conf
:
apiServer
.
getWebAppContext
(
)
.
getConfigurations
(
)
)
{
private
void
startWebApp
(
)
throws
IOException
{
URI
uri
=
URI
.
create
(
+
NetUtils
.
getHostPortString
(
bindAddress
)
)
;
apiServer
=
new
HttpServer2
.
Builder
(
)
.
setName
(
)
.
setConf
(
getConfig
(
)
)
.
setSecurityEnabled
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
.
setUsernameConfKey
(
RM_WEBAPP_SPNEGO_USER_NAME_KEY
)
.
setKeytabConfKey
(
RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY
)
.
addEndpoint
(
uri
)
.
build
(
)
;
String
apiPackages
=
ApiServer
.
class
.
getPackage
(
)
.
getName
(
)
+
SEP
+
GenericExceptionHandler
.
class
.
getPackage
(
)
.
getName
(
)
+
SEP
+
YarnJacksonJaxbJsonProvider
.
class
.
getPackage
(
)
.
getName
(
)
;
apiServer
.
addJerseyResourcePackage
(
apiPackages
,
)
;
try
{
logger
.
info
(
)
;
apiServer
.
start
(
)
;
logger
.
info
(
,
apiServer
.
toString
(
)
)
;
for
(
Configuration
conf
:
apiServer
.
getWebAppContext
(
)
.
getConfigurations
(
)
)
{
logger
.
info
(
,
conf
)
;
}
logger
.
info
(
,
Collections
.
singletonList
(
apiServer
.
getWebAppContext
(
)
.
getContextPath
(
)
)
)
;
logger
.
info
(
,
Collections
.
singletonList
(
apiServer
.
getWebAppContext
(
)
.
getResourceBase
(
)
)
)
;
logger
.
info
(
,
Collections
.
singletonList
(
apiServer
.
getWebAppContext
(
)
.
getWar
(
)
)
)
;
}
catch
(
Exception
ex
)
{
@
Override
public
FlexComponentsResponseProto
flexComponents
(
FlexComponentsRequestProto
request
)
throws
IOException
{
if
(
!
request
.
getComponentsList
(
)
.
isEmpty
(
)
)
{
for
(
ComponentCountProto
component
:
request
.
getComponentsList
(
)
)
{
ComponentEvent
event
=
new
ComponentEvent
(
component
.
getName
(
)
,
FLEX
)
.
setDesired
(
component
.
getNumberOfContainers
(
)
)
;
context
.
scheduler
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
event
)
;
@
Override
public
UpgradeServiceResponseProto
upgrade
(
UpgradeServiceRequestProto
request
)
throws
IOException
{
try
{
@
Override
public
CompInstancesUpgradeResponseProto
upgrade
(
CompInstancesUpgradeRequestProto
request
)
throws
IOException
,
YarnException
{
if
(
!
request
.
getContainerIdsList
(
)
.
isEmpty
(
)
)
{
for
(
String
containerId
:
request
.
getContainerIdsList
(
)
)
{
ComponentInstanceEvent
event
=
new
ComponentInstanceEvent
(
ContainerId
.
fromString
(
containerId
)
,
ComponentInstanceEventType
.
UPGRADE
)
;
@
Override
public
DecommissionCompInstancesResponseProto
decommissionCompInstances
(
DecommissionCompInstancesRequestProto
request
)
throws
IOException
,
YarnException
{
if
(
!
request
.
getCompInstancesList
(
)
.
isEmpty
(
)
)
{
for
(
String
instance
:
request
.
getCompInstancesList
(
)
)
{
String
componentName
=
ServiceApiUtil
.
parseComponentName
(
instance
)
;
ComponentEvent
event
=
new
ComponentEvent
(
componentName
,
DECOMMISSION_INSTANCE
)
.
setInstanceName
(
instance
)
;
context
.
scheduler
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
event
)
;
public
synchronized
void
resetContainerFailures
(
)
{
failureCountPerNode
.
clear
(
)
;
context
.
scheduler
.
getAmRMClient
(
)
.
updateBlacklist
(
null
,
new
ArrayList
<
>
(
blackListedNodes
)
)
;
private
void
upgradeNextCompIfAny
(
boolean
cancelUpgrade
)
{
if
(
!
componentsToUpgrade
.
isEmpty
(
)
)
{
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
Component
component
=
componentsToUpgrade
.
get
(
0
)
;
serviceSpec
.
getComponent
(
component
.
getName
(
)
)
.
getContainers
(
)
.
forEach
(
container
->
{
ComponentInstanceEvent
upgradeEvent
=
new
ComponentInstanceEvent
(
ContainerId
.
fromString
(
container
.
getId
(
)
)
,
!
cancelUpgrade
?
ComponentInstanceEventType
.
UPGRADE
:
ComponentInstanceEventType
.
CANCEL_UPGRADE
)
;
private
void
dispatchNeedUpgradeEvents
(
boolean
cancelUpgrade
)
{
if
(
componentsToUpgrade
!=
null
)
{
componentsToUpgrade
.
forEach
(
component
->
{
ComponentEvent
needUpgradeEvent
=
new
ComponentEvent
(
component
.
getName
(
)
,
!
cancelUpgrade
?
ComponentEventType
.
UPGRADE
:
ComponentEventType
.
CANCEL_UPGRADE
)
.
setTargetSpec
(
component
)
.
setUpgradeVersion
(
upgradeVersion
)
;
try
{
Service
targetSpec
=
ServiceApiUtil
.
loadServiceUpgrade
(
fs
,
getName
(
)
,
upgradeVersion
)
;
targetSpec
.
setId
(
serviceSpec
.
getId
(
)
)
;
targetSpec
.
setState
(
ServiceState
.
STABLE
)
;
Map
<
String
,
Component
>
allComps
=
scheduler
.
getAllComponents
(
)
;
targetSpec
.
getComponents
(
)
.
forEach
(
compSpec
->
{
Component
comp
=
allComps
.
get
(
compSpec
.
getName
(
)
)
;
compSpec
.
setState
(
comp
.
getComponentSpec
(
)
.
getState
(
)
)
;
}
)
;
jsonSerDeser
.
save
(
fs
.
getFileSystem
(
)
,
ServiceApiUtil
.
getServiceJsonPath
(
fs
,
getName
(
)
)
,
targetSpec
,
true
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
,
e
)
;
return
false
;
}
}
try
{
String
upgradeVersionToDel
=
cancelUpgrade
?
cancelledVersion
:
upgradeVersion
;
Map
<
String
,
Component
>
allComps
=
scheduler
.
getAllComponents
(
)
;
targetSpec
.
getComponents
(
)
.
forEach
(
compSpec
->
{
Component
comp
=
allComps
.
get
(
compSpec
.
getName
(
)
)
;
compSpec
.
setState
(
comp
.
getComponentSpec
(
)
.
getState
(
)
)
;
}
)
;
jsonSerDeser
.
save
(
fs
.
getFileSystem
(
)
,
ServiceApiUtil
.
getServiceJsonPath
(
fs
,
getName
(
)
)
,
targetSpec
,
true
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
,
e
)
;
return
false
;
}
}
try
{
String
upgradeVersionToDel
=
cancelUpgrade
?
cancelledVersion
:
upgradeVersion
;
LOG
.
info
(
,
upgradeVersionToDel
)
;
fs
.
deleteClusterUpgradeDir
(
getName
(
)
,
upgradeVersionToDel
)
;
for
(
String
comp
:
compsAffectedByUpgrade
)
{
String
compDirVersionToDel
=
cancelUpgrade
?
cancelledVersion
:
serviceSpec
.
getVersion
(
)
;
private
List
<
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
Component
>
resolveCompsToUpgrade
(
Service
sourceSpec
,
Service
targetSpec
)
{
List
<
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
Component
>
compsNeedUpgradeList
=
componentsFinder
.
findTargetComponentSpecs
(
sourceSpec
,
targetSpec
)
;
if
(
compsNeedUpgradeList
!=
null
)
{
compsNeedUpgradeList
.
removeIf
(
component
->
{
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
Component
.
RestartPolicyEnum
restartPolicy
=
component
.
getRestartPolicy
(
)
;
final
ComponentRestartPolicy
restartPolicyHandler
=
Component
.
getRestartPolicyHandler
(
restartPolicy
)
;
if
(
!
restartPolicyHandler
.
allowUpgrades
(
)
)
{
private
void
setServiceState
(
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ServiceState
state
)
{
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ServiceState
curState
=
serviceSpec
.
getState
(
)
;
if
(
!
curState
.
equals
(
state
)
)
{
serviceSpec
.
setState
(
state
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
credentials
=
UserGroupInformation
.
getCurrentUser
(
)
.
getCredentials
(
)
;
doSecureLogin
(
)
;
}
SliderFileSystem
fs
=
new
SliderFileSystem
(
conf
)
;
fs
.
setAppDir
(
appDir
)
;
context
.
fs
=
fs
;
loadApplicationJson
(
context
,
fs
)
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
if
(
credentials
!=
null
)
{
UserGroupInformation
.
getCurrentUser
(
)
.
addCredentials
(
credentials
)
;
}
removeHdfsDelegationToken
(
UserGroupInformation
.
getLoginUser
(
)
)
;
}
for
(
Map
.
Entry
<
String
,
String
>
entry
:
context
.
service
.
getConfiguration
(
)
.
getProperties
(
)
.
entrySet
(
)
)
{
conf
.
set
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
ContainerId
amContainerId
=
getAMContainerId
(
)
;
ApplicationAttemptId
attemptId
=
amContainerId
.
getApplicationAttemptId
(
)
;
@
VisibleForTesting
protected
ByteBuffer
recordTokensForContainers
(
)
throws
IOException
{
Credentials
copy
=
new
Credentials
(
UserGroupInformation
.
getCurrentUser
(
)
.
getCredentials
(
)
)
;
Iterator
<
Token
<
?
>>
iter
=
copy
.
getAllTokens
(
)
.
iterator
(
)
;
while
(
iter
.
hasNext
(
)
)
{
Token
<
?
>
token
=
iter
.
next
(
)
;
private
void
doSecureLogin
(
)
throws
IOException
,
URISyntaxException
{
File
keytab
=
new
File
(
String
.
format
(
KEYTAB_LOCATION
,
getServiceName
(
)
)
)
;
if
(
!
keytab
.
exists
(
)
)
{
private
void
doSecureLogin
(
)
throws
IOException
,
URISyntaxException
{
File
keytab
=
new
File
(
String
.
format
(
KEYTAB_LOCATION
,
getServiceName
(
)
)
)
;
if
(
!
keytab
.
exists
(
)
)
{
LOG
.
info
(
+
keytab
)
;
String
preInstalledKeytab
=
context
.
service
==
null
?
this
.
serviceKeytab
:
context
.
service
.
getKerberosPrincipal
(
)
.
getKeytab
(
)
;
if
(
!
StringUtils
.
isEmpty
(
preInstalledKeytab
)
)
{
URI
uri
=
new
URI
(
preInstalledKeytab
)
;
if
(
uri
.
getScheme
(
)
.
equals
(
)
)
{
keytab
=
new
File
(
uri
)
;
File
keytab
=
new
File
(
String
.
format
(
KEYTAB_LOCATION
,
getServiceName
(
)
)
)
;
if
(
!
keytab
.
exists
(
)
)
{
LOG
.
info
(
+
keytab
)
;
String
preInstalledKeytab
=
context
.
service
==
null
?
this
.
serviceKeytab
:
context
.
service
.
getKerberosPrincipal
(
)
.
getKeytab
(
)
;
if
(
!
StringUtils
.
isEmpty
(
preInstalledKeytab
)
)
{
URI
uri
=
new
URI
(
preInstalledKeytab
)
;
if
(
uri
.
getScheme
(
)
.
equals
(
)
)
{
keytab
=
new
File
(
uri
)
;
LOG
.
info
(
+
preInstalledKeytab
)
;
}
}
}
if
(
!
keytab
.
exists
(
)
)
{
LOG
.
info
(
+
keytab
)
;
return
;
}
String
principal
=
context
.
service
==
null
?
this
.
servicePrincipalName
:
context
.
service
.
getKerberosPrincipal
(
)
.
getPrincipalName
(
)
;
if
(
StringUtils
.
isEmpty
(
(
principal
)
)
)
{
principal
=
UserGroupInformation
.
getLoginUser
(
)
.
getShortUserName
(
)
;
if
(
!
StringUtils
.
isEmpty
(
preInstalledKeytab
)
)
{
URI
uri
=
new
URI
(
preInstalledKeytab
)
;
if
(
uri
.
getScheme
(
)
.
equals
(
)
)
{
keytab
=
new
File
(
uri
)
;
LOG
.
info
(
+
preInstalledKeytab
)
;
}
}
}
if
(
!
keytab
.
exists
(
)
)
{
LOG
.
info
(
+
keytab
)
;
return
;
}
String
principal
=
context
.
service
==
null
?
this
.
servicePrincipalName
:
context
.
service
.
getKerberosPrincipal
(
)
.
getPrincipalName
(
)
;
if
(
StringUtils
.
isEmpty
(
(
principal
)
)
)
{
principal
=
UserGroupInformation
.
getLoginUser
(
)
.
getShortUserName
(
)
;
LOG
.
info
(
+
,
principal
)
;
}
LOG
.
info
(
+
UserGroupInformation
.
getCurrentUser
(
)
)
;
String
principalName
=
SecurityUtil
.
getServerPrincipal
(
principal
,
ServiceUtils
.
getLocalHostName
(
getConfig
(
)
)
)
;
UserGroupInformation
.
loginUserFromKeytab
(
principalName
,
keytab
.
getAbsolutePath
(
)
)
;
private
void
printSystemEnv
(
)
{
for
(
Map
.
Entry
<
String
,
String
>
envs
:
System
.
getenv
(
)
.
entrySet
(
)
)
{
opts
.
addOption
(
YARNFILE_OPTION
,
true
,
+
)
;
opts
.
getOption
(
YARNFILE_OPTION
)
.
setRequired
(
true
)
;
opts
.
addOption
(
SERVICE_NAME_OPTION
,
true
,
)
;
opts
.
getOption
(
SERVICE_NAME_OPTION
)
.
setRequired
(
true
)
;
opts
.
addOption
(
KEYTAB_OPTION
,
true
,
)
;
opts
.
addOption
(
PRINCIPAL_NAME_OPTION
,
true
,
)
;
GenericOptionsParser
parser
=
new
GenericOptionsParser
(
conf
,
opts
,
args
)
;
CommandLine
cmdLine
=
parser
.
getCommandLine
(
)
;
serviceMaster
.
serviceDefPath
=
cmdLine
.
getOptionValue
(
YARNFILE_OPTION
)
;
serviceMaster
.
serviceName
=
cmdLine
.
getOptionValue
(
SERVICE_NAME_OPTION
)
;
serviceMaster
.
serviceKeytab
=
cmdLine
.
getOptionValue
(
KEYTAB_OPTION
)
;
serviceMaster
.
servicePrincipalName
=
cmdLine
.
getOptionValue
(
PRINCIPAL_NAME_OPTION
)
;
serviceMaster
.
init
(
conf
)
;
serviceMaster
.
start
(
)
;
}
catch
(
Throwable
t
)
{
public
void
buildInstance
(
ServiceContext
context
,
Configuration
configuration
)
throws
YarnException
,
IOException
{
app
=
context
.
service
;
executorService
=
Executors
.
newScheduledThreadPool
(
10
)
;
RegistryOperations
registryClient
=
null
;
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
&&
!
StringUtils
.
isEmpty
(
context
.
principal
)
&&
!
StringUtils
.
isEmpty
(
context
.
keytab
)
)
{
Configuration
conf
=
getConfig
(
)
;
String
username
=
new
HadoopKerberosName
(
context
.
principal
.
trim
(
)
)
.
getServiceName
(
)
;
@
Override
public
void
serviceStop
(
)
throws
Exception
{
LOG
.
info
(
)
;
if
(
executorService
!=
null
)
{
executorService
.
shutdownNow
(
)
;
}
DefaultMetricsSystem
.
shutdown
(
)
;
if
(
gracefulStop
)
{
if
(
YarnConfiguration
.
timelineServiceV2Enabled
(
getConfig
(
)
)
)
{
final
Map
<
ContainerId
,
ComponentInstance
>
liveInst
=
getLiveInstances
(
)
;
for
(
Map
.
Entry
<
ContainerId
,
ComponentInstance
>
instance
:
liveInst
.
entrySet
(
)
)
{
if
(
!
ComponentInstance
.
isFinalState
(
instance
.
getValue
(
)
.
getContainerSpec
(
)
.
getState
(
)
)
)
{
LOG
.
info
(
,
instance
.
getValue
(
)
.
getCompInstanceName
(
)
,
instance
.
getValue
(
)
.
getContainerSpec
(
)
.
getState
(
)
,
ContainerState
.
STOPPED
)
;
serviceTimelinePublisher
.
componentInstanceFinished
(
instance
.
getKey
(
)
,
KILLED_AFTER_APP_COMPLETION
,
ContainerState
.
STOPPED
,
getDiagnostics
(
)
.
toString
(
)
)
;
}
}
LOG
.
info
(
,
finalApplicationStatus
)
;
serviceTimelinePublisher
.
serviceAttemptUnregistered
(
context
,
finalApplicationStatus
,
diagnostics
.
toString
(
)
)
;
}
amRMClient
.
unregisterApplicationMaster
(
finalApplicationStatus
,
diagnostics
.
toString
(
)
,
)
;
private
void
recoverComponents
(
RegisterApplicationMasterResponse
response
)
{
List
<
Container
>
containersFromPrevAttempt
=
response
.
getContainersFromPreviousAttempts
(
)
;
private
void
recoverComponents
(
RegisterApplicationMasterResponse
response
)
{
List
<
Container
>
containersFromPrevAttempt
=
response
.
getContainersFromPreviousAttempts
(
)
;
LOG
.
info
(
,
containersFromPrevAttempt
.
size
(
)
)
;
Map
<
String
,
ServiceRecord
>
existingRecords
=
new
HashMap
<
>
(
)
;
List
<
String
>
existingComps
=
null
;
try
{
existingComps
=
yarnRegistryOperations
.
listComponents
(
)
;
Map
<
String
,
ServiceRecord
>
existingRecords
=
new
HashMap
<
>
(
)
;
List
<
String
>
existingComps
=
null
;
try
{
existingComps
=
yarnRegistryOperations
.
listComponents
(
)
;
LOG
.
info
(
,
existingComps
.
size
(
)
,
existingComps
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
e
.
getMessage
(
)
)
;
}
if
(
existingComps
!=
null
)
{
for
(
String
existingComp
:
existingComps
)
{
try
{
ServiceRecord
record
=
yarnRegistryOperations
.
getComponent
(
existingComp
)
;
existingRecords
.
put
(
existingComp
,
record
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
existingComp
,
e
)
;
}
}
}
for
(
Container
container
:
containersFromPrevAttempt
)
{
LOG
.
info
(
,
e
.
getMessage
(
)
)
;
}
if
(
existingComps
!=
null
)
{
for
(
String
existingComp
:
existingComps
)
{
try
{
ServiceRecord
record
=
yarnRegistryOperations
.
getComponent
(
existingComp
)
;
existingRecords
.
put
(
existingComp
,
record
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
existingComp
,
e
)
;
}
}
}
for
(
Container
container
:
containersFromPrevAttempt
)
{
LOG
.
info
(
,
container
.
getId
(
)
)
;
ServiceRecord
record
=
existingRecords
.
remove
(
RegistryPathUtils
.
encodeYarnID
(
container
.
getId
(
)
.
toString
(
)
)
)
;
if
(
record
!=
null
)
{
Component
comp
=
componentsById
.
get
(
container
.
getAllocationRequestId
(
)
)
;
ComponentEvent
event
=
new
ComponentEvent
(
comp
.
getName
(
)
,
CONTAINER_RECOVERED
)
.
setContainer
(
container
)
.
setInstance
(
comp
.
getComponentInstance
(
record
.
description
)
)
;
comp
.
handle
(
event
)
;
ComponentEvent
event
=
new
ComponentEvent
(
comp
.
getName
(
)
,
CONTAINER_RECOVERED
)
.
setContainer
(
container
)
.
setInstance
(
comp
.
getComponentInstance
(
record
.
description
)
)
;
comp
.
handle
(
event
)
;
}
else
{
LOG
.
info
(
+
,
container
.
getId
(
)
)
;
amRMClient
.
releaseAssignedContainer
(
container
.
getId
(
)
)
;
}
}
ApplicationId
appId
=
ApplicationId
.
fromString
(
app
.
getId
(
)
)
;
existingRecords
.
forEach
(
(
encodedContainerId
,
record
)
->
{
String
componentName
=
record
.
get
(
YarnRegistryAttributes
.
YARN_COMPONENT
)
;
if
(
componentName
!=
null
)
{
Component
component
=
componentsByName
.
get
(
componentName
)
;
if
(
component
!=
null
)
{
ComponentInstance
compInstance
=
component
.
getComponentInstance
(
record
.
description
)
;
ContainerId
containerId
=
ContainerId
.
fromString
(
record
.
get
(
YarnRegistryAttributes
.
YARN_ID
)
)
;
if
(
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
.
equals
(
appId
)
)
{
unRecoveredInstances
.
put
(
containerId
,
compInstance
)
;
private
void
registerServiceInstance
(
ApplicationAttemptId
attemptId
,
Service
service
)
throws
IOException
{
private
void
registerServiceInstance
(
ApplicationAttemptId
attemptId
,
Service
service
)
throws
IOException
{
LOG
.
info
(
+
attemptId
+
+
service
.
getName
(
)
+
)
;
ServiceRecord
serviceRecord
=
new
ServiceRecord
(
)
;
serviceRecord
.
set
(
YarnRegistryAttributes
.
YARN_ID
,
attemptId
.
getApplicationId
(
)
.
toString
(
)
)
;
serviceRecord
.
set
(
YarnRegistryAttributes
.
YARN_PERSISTENCE
,
PersistencePolicies
.
APPLICATION
)
;
serviceRecord
.
description
=
;
executorService
.
submit
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
yarnRegistryOperations
.
registerSelf
(
serviceRecord
,
false
)
;
private
void
registerServiceInstance
(
ApplicationAttemptId
attemptId
,
Service
service
)
throws
IOException
{
LOG
.
info
(
+
attemptId
+
+
service
.
getName
(
)
+
)
;
ServiceRecord
serviceRecord
=
new
ServiceRecord
(
)
;
serviceRecord
.
set
(
YarnRegistryAttributes
.
YARN_ID
,
attemptId
.
getApplicationId
(
)
.
toString
(
)
)
;
serviceRecord
.
set
(
YarnRegistryAttributes
.
YARN_PERSISTENCE
,
PersistencePolicies
.
APPLICATION
)
;
serviceRecord
.
description
=
;
executorService
.
submit
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
yarnRegistryOperations
.
registerSelf
(
serviceRecord
,
false
)
;
LOG
.
info
(
,
yarnRegistryOperations
.
getSelfRegistrationPath
(
)
,
yarnRegistryOperations
.
getAbsoluteSelfRegistrationPath
(
)
)
;
boolean
isFirstAttempt
=
1
==
attemptId
.
getAttemptId
(
)
;
if
(
isFirstAttempt
)
{
yarnRegistryOperations
.
deleteChildren
(
yarnRegistryOperations
.
getSelfRegistrationPath
(
)
,
true
)
;
}
}
catch
(
IOException
e
)
{
private
boolean
terminateServiceIfDominantComponentFinished
(
Component
component
)
{
boolean
shouldTerminate
=
false
;
boolean
componentIsDominant
=
component
.
getComponentSpec
(
)
.
getConfiguration
(
)
.
getPropertyBool
(
CONTAINER_STATE_REPORT_AS_SERVICE_STATE
,
false
)
;
if
(
componentIsDominant
)
{
ComponentRestartPolicy
restartPolicy
=
component
.
getRestartPolicyHandler
(
)
;
if
(
restartPolicy
.
shouldTerminate
(
component
)
)
{
shouldTerminate
=
true
;
boolean
isSucceeded
=
restartPolicy
.
hasCompletedSuccessfully
(
component
)
;
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
state
=
isSucceeded
?
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
.
SUCCEEDED
:
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
.
FAILED
;
private
boolean
terminateServiceIfDominantComponentFinished
(
Component
component
)
{
boolean
shouldTerminate
=
false
;
boolean
componentIsDominant
=
component
.
getComponentSpec
(
)
.
getConfiguration
(
)
.
getPropertyBool
(
CONTAINER_STATE_REPORT_AS_SERVICE_STATE
,
false
)
;
if
(
componentIsDominant
)
{
ComponentRestartPolicy
restartPolicy
=
component
.
getRestartPolicyHandler
(
)
;
if
(
restartPolicy
.
shouldTerminate
(
component
)
)
{
shouldTerminate
=
true
;
boolean
isSucceeded
=
restartPolicy
.
hasCompletedSuccessfully
(
component
)
;
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
state
=
isSucceeded
?
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
.
SUCCEEDED
:
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
.
FAILED
;
LOG
.
info
(
,
component
.
getName
(
)
,
component
.
getComponentSpec
(
)
.
getState
(
)
,
state
)
;
component
.
getComponentSpec
(
)
.
setState
(
state
)
;
private
boolean
terminateServiceIfAllComponentsFinished
(
)
{
boolean
shouldTerminate
=
true
;
Set
<
String
>
succeededComponents
=
new
HashSet
<
>
(
)
;
Set
<
String
>
failedComponents
=
new
HashSet
<
>
(
)
;
for
(
Component
comp
:
getAllComponents
(
)
.
values
(
)
)
{
ComponentRestartPolicy
restartPolicy
=
comp
.
getRestartPolicyHandler
(
)
;
if
(
restartPolicy
.
shouldTerminate
(
comp
)
)
{
if
(
restartPolicy
.
hasCompletedSuccessfully
(
comp
)
)
{
comp
.
getComponentSpec
(
)
.
setState
(
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
.
SUCCEEDED
)
;
}
else
{
LOG
.
info
(
,
comp
.
getName
(
)
,
comp
.
getComponentSpec
(
)
.
getState
(
)
,
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
.
FAILED
)
;
comp
.
getComponentSpec
(
)
.
setState
(
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
.
FAILED
)
;
}
if
(
isTimelineServiceEnabled
(
)
)
{
serviceTimelinePublisher
.
componentFinished
(
comp
.
getComponentSpec
(
)
,
comp
.
getComponentSpec
(
)
.
getState
(
)
,
systemClock
.
getTime
(
)
)
;
}
}
else
{
shouldTerminate
=
false
;
break
;
}
long
nFailed
=
comp
.
getNumFailedInstances
(
)
;
if
(
nFailed
>
0
)
{
failedComponents
.
add
(
comp
.
getName
(
)
)
;
}
else
{
succeededComponents
.
add
(
comp
.
getName
(
)
)
;
}
}
if
(
shouldTerminate
)
{
if
(
examplesDirStr
==
null
)
{
String
yarnHome
=
System
.
getenv
(
ApplicationConstants
.
Environment
.
HADOOP_YARN_HOME
.
key
(
)
)
;
examplesDirs
=
new
String
[
]
{
yarnHome
+
,
yarnHome
+
}
;
}
else
{
examplesDirs
=
StringUtils
.
split
(
examplesDirStr
,
)
;
}
for
(
String
dir
:
examplesDirs
)
{
file
=
new
File
(
MessageFormat
.
format
(
,
dir
,
fileName
,
fileName
)
)
;
if
(
file
.
exists
(
)
)
{
break
;
}
file
=
new
File
(
MessageFormat
.
format
(
,
dir
,
fileName
)
)
;
if
(
file
.
exists
(
)
)
{
break
;
}
}
}
if
(
!
file
.
exists
(
)
)
{
throw
new
YarnException
(
+
fileName
)
;
}
Path
filePath
=
new
Path
(
file
.
getAbsolutePath
(
)
)
;
throw
new
YarnException
(
ErrorStrings
.
SERVICE_UPGRADE_DISABLED
)
;
}
Service
persistedService
=
ServiceApiUtil
.
loadService
(
fs
,
service
.
getName
(
)
)
;
if
(
!
StringUtils
.
isEmpty
(
persistedService
.
getId
(
)
)
)
{
cachedAppInfo
.
put
(
persistedService
.
getName
(
)
,
new
AppInfo
(
ApplicationId
.
fromString
(
persistedService
.
getId
(
)
)
,
persistedService
.
getKerberosPrincipal
(
)
.
getPrincipalName
(
)
)
)
;
}
if
(
persistedService
.
getVersion
(
)
.
equals
(
service
.
getVersion
(
)
)
)
{
String
message
=
service
.
getName
(
)
+
+
service
.
getVersion
(
)
+
;
LOG
.
error
(
message
)
;
throw
new
YarnException
(
message
)
;
}
boolean
foundNotNeverComp
=
false
;
for
(
Component
comp
:
persistedService
.
getComponents
(
)
)
{
if
(
!
comp
.
getRestartPolicy
(
)
.
equals
(
Component
.
RestartPolicyEnum
.
NEVER
)
)
{
foundNotNeverComp
=
true
;
break
;
}
}
if
(
!
foundNotNeverComp
)
{
String
message
=
+
service
.
getName
(
)
+
+
Component
.
RestartPolicyEnum
.
NEVER
+
+
;
}
if
(
persistedService
.
getVersion
(
)
.
equals
(
service
.
getVersion
(
)
)
)
{
String
message
=
service
.
getName
(
)
+
+
service
.
getVersion
(
)
+
;
LOG
.
error
(
message
)
;
throw
new
YarnException
(
message
)
;
}
boolean
foundNotNeverComp
=
false
;
for
(
Component
comp
:
persistedService
.
getComponents
(
)
)
{
if
(
!
comp
.
getRestartPolicy
(
)
.
equals
(
Component
.
RestartPolicyEnum
.
NEVER
)
)
{
foundNotNeverComp
=
true
;
break
;
}
}
if
(
!
foundNotNeverComp
)
{
String
message
=
+
service
.
getName
(
)
+
+
Component
.
RestartPolicyEnum
.
NEVER
+
+
;
LOG
.
error
(
message
)
;
throw
new
YarnException
(
message
)
;
}
Service
liveService
=
getStatus
(
service
.
getName
(
)
)
;
if
(
!
liveService
.
getState
(
)
.
equals
(
ServiceState
.
STABLE
)
)
{
if
(
StringUtils
.
isEmpty
(
persistedService
.
getId
(
)
)
)
{
throw
new
YarnException
(
persistedService
.
getName
(
)
+
+
)
;
}
cachedAppInfo
.
put
(
persistedService
.
getName
(
)
,
new
AppInfo
(
ApplicationId
.
fromString
(
persistedService
.
getId
(
)
)
,
persistedService
.
getKerberosPrincipal
(
)
.
getPrincipalName
(
)
)
)
;
for
(
String
instance
:
componentInstances
)
{
String
componentName
=
ServiceApiUtil
.
parseComponentName
(
ServiceApiUtil
.
parseAndValidateComponentInstanceName
(
instance
,
appName
,
getConfig
(
)
)
)
;
Component
component
=
persistedService
.
getComponent
(
componentName
)
;
if
(
component
==
null
)
{
throw
new
IllegalArgumentException
(
instance
+
)
;
}
if
(
!
component
.
getDecommissionedInstances
(
)
.
contains
(
instance
)
)
{
component
.
addDecommissionedInstance
(
instance
)
;
component
.
setNumberOfContainers
(
Math
.
max
(
0
,
component
.
getNumberOfContainers
(
)
-
1
)
)
;
}
}
ServiceApiUtil
.
writeAppDefinition
(
fs
,
persistedService
)
;
ApplicationReport
appReport
=
yarnClient
.
getApplicationReport
(
ApplicationId
.
fromString
(
persistedService
.
getId
(
)
)
)
;
if
(
appReport
.
getYarnApplicationState
(
)
!=
RUNNING
)
{
String
message
=
persistedService
.
getName
(
)
+
+
appReport
.
getYarnApplicationState
(
)
+
+
;
public
int
actionUpgrade
(
Service
service
,
List
<
Container
>
compInstances
)
throws
IOException
,
YarnException
{
ApplicationReport
appReport
=
yarnClient
.
getApplicationReport
(
getAppId
(
service
.
getName
(
)
)
)
;
if
(
appReport
.
getYarnApplicationState
(
)
!=
RUNNING
)
{
String
message
=
service
.
getName
(
)
+
+
appReport
.
getYarnApplicationState
(
)
+
;
FlexComponentsRequestProto
.
Builder
requestBuilder
=
FlexComponentsRequestProto
.
newBuilder
(
)
;
for
(
Component
persistedComp
:
persistedService
.
getComponents
(
)
)
{
String
name
=
persistedComp
.
getName
(
)
;
if
(
componentCounts
.
containsKey
(
persistedComp
.
getName
(
)
)
)
{
original
.
put
(
name
,
persistedComp
.
getNumberOfContainers
(
)
)
;
persistedComp
.
setNumberOfContainers
(
componentCounts
.
get
(
name
)
)
;
countBuilder
.
setName
(
persistedComp
.
getName
(
)
)
.
setNumberOfContainers
(
persistedComp
.
getNumberOfContainers
(
)
)
;
requestBuilder
.
addComponents
(
countBuilder
.
build
(
)
)
;
}
}
if
(
original
.
size
(
)
<
componentCounts
.
size
(
)
)
{
componentCounts
.
keySet
(
)
.
removeAll
(
original
.
keySet
(
)
)
;
throw
new
YarnException
(
+
componentCounts
.
keySet
(
)
+
)
;
}
ServiceApiUtil
.
writeAppDefinition
(
fs
,
persistedService
)
;
ApplicationId
appId
=
getAppId
(
serviceName
)
;
if
(
appId
==
null
)
{
String
message
=
+
serviceName
;
persistedComp
.
setNumberOfContainers
(
componentCounts
.
get
(
name
)
)
;
countBuilder
.
setName
(
persistedComp
.
getName
(
)
)
.
setNumberOfContainers
(
persistedComp
.
getNumberOfContainers
(
)
)
;
requestBuilder
.
addComponents
(
countBuilder
.
build
(
)
)
;
}
}
if
(
original
.
size
(
)
<
componentCounts
.
size
(
)
)
{
componentCounts
.
keySet
(
)
.
removeAll
(
original
.
keySet
(
)
)
;
throw
new
YarnException
(
+
componentCounts
.
keySet
(
)
+
)
;
}
ServiceApiUtil
.
writeAppDefinition
(
fs
,
persistedService
)
;
ApplicationId
appId
=
getAppId
(
serviceName
)
;
if
(
appId
==
null
)
{
String
message
=
+
serviceName
;
LOG
.
error
(
message
)
;
throw
new
YarnException
(
message
)
;
}
ApplicationReport
appReport
=
yarnClient
.
getApplicationReport
(
appId
)
;
if
(
appReport
.
getYarnApplicationState
(
)
!=
RUNNING
)
{
String
message
=
serviceName
+
+
appReport
.
getYarnApplicationState
(
)
+
;
if
(
original
.
size
(
)
<
componentCounts
.
size
(
)
)
{
componentCounts
.
keySet
(
)
.
removeAll
(
original
.
keySet
(
)
)
;
throw
new
YarnException
(
+
componentCounts
.
keySet
(
)
+
)
;
}
ServiceApiUtil
.
writeAppDefinition
(
fs
,
persistedService
)
;
ApplicationId
appId
=
getAppId
(
serviceName
)
;
if
(
appId
==
null
)
{
String
message
=
+
serviceName
;
LOG
.
error
(
message
)
;
throw
new
YarnException
(
message
)
;
}
ApplicationReport
appReport
=
yarnClient
.
getApplicationReport
(
appId
)
;
if
(
appReport
.
getYarnApplicationState
(
)
!=
RUNNING
)
{
String
message
=
serviceName
+
+
appReport
.
getYarnApplicationState
(
)
+
;
LOG
.
error
(
message
)
;
throw
new
YarnException
(
message
)
;
}
Service
liveService
=
getStatus
(
serviceName
)
;
public
int
actionStop
(
String
serviceName
,
boolean
waitForAppStopped
)
throws
YarnException
,
IOException
{
ServiceApiUtil
.
validateNameFormat
(
serviceName
,
getConfig
(
)
)
;
ApplicationId
currentAppId
=
getAppId
(
serviceName
)
;
if
(
currentAppId
==
null
)
{
public
int
actionStop
(
String
serviceName
,
boolean
waitForAppStopped
)
throws
YarnException
,
IOException
{
ServiceApiUtil
.
validateNameFormat
(
serviceName
,
getConfig
(
)
)
;
ApplicationId
currentAppId
=
getAppId
(
serviceName
)
;
if
(
currentAppId
==
null
)
{
LOG
.
info
(
,
serviceName
)
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_COMMAND_ARGUMENT_ERROR
;
}
ApplicationReport
report
=
yarnClient
.
getApplicationReport
(
currentAppId
)
;
if
(
terminatedStates
.
contains
(
report
.
getYarnApplicationState
(
)
)
)
{
LOG
.
info
(
,
serviceName
,
report
.
getYarnApplicationState
(
)
)
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_COMMAND_ARGUMENT_ERROR
;
}
if
(
preRunningStates
.
contains
(
report
.
getYarnApplicationState
(
)
)
)
{
String
msg
=
serviceName
+
+
report
.
getYarnApplicationState
(
)
+
;
yarnClient
.
killApplication
(
currentAppId
,
msg
)
;
return
EXIT_COMMAND_ARGUMENT_ERROR
;
}
if
(
preRunningStates
.
contains
(
report
.
getYarnApplicationState
(
)
)
)
{
String
msg
=
serviceName
+
+
report
.
getYarnApplicationState
(
)
+
;
yarnClient
.
killApplication
(
currentAppId
,
msg
)
;
LOG
.
info
(
msg
)
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
if
(
StringUtils
.
isEmpty
(
report
.
getHost
(
)
)
)
{
throw
new
YarnException
(
serviceName
+
)
;
}
LOG
.
info
(
,
serviceName
,
currentAppId
)
;
try
{
ClientAMProtocol
proxy
=
createAMProxy
(
serviceName
,
report
)
;
cachedAppInfo
.
remove
(
serviceName
)
;
if
(
proxy
!=
null
)
{
StopRequestProto
request
=
StopRequestProto
.
newBuilder
(
)
.
build
(
)
;
String
msg
=
serviceName
+
+
report
.
getYarnApplicationState
(
)
+
;
yarnClient
.
killApplication
(
currentAppId
,
msg
)
;
LOG
.
info
(
msg
)
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
if
(
StringUtils
.
isEmpty
(
report
.
getHost
(
)
)
)
{
throw
new
YarnException
(
serviceName
+
)
;
}
LOG
.
info
(
,
serviceName
,
currentAppId
)
;
try
{
ClientAMProtocol
proxy
=
createAMProxy
(
serviceName
,
report
)
;
cachedAppInfo
.
remove
(
serviceName
)
;
if
(
proxy
!=
null
)
{
StopRequestProto
request
=
StopRequestProto
.
newBuilder
(
)
.
build
(
)
;
proxy
.
stop
(
request
)
;
LOG
.
info
(
+
serviceName
+
)
;
cachedAppInfo
.
remove
(
serviceName
)
;
if
(
proxy
!=
null
)
{
StopRequestProto
request
=
StopRequestProto
.
newBuilder
(
)
.
build
(
)
;
proxy
.
stop
(
request
)
;
LOG
.
info
(
+
serviceName
+
)
;
}
else
{
yarnClient
.
killApplication
(
currentAppId
,
serviceName
+
)
;
LOG
.
info
(
+
serviceName
)
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
if
(
!
waitForAppStopped
)
{
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
long
startTime
=
System
.
currentTimeMillis
(
)
;
int
pollCount
=
0
;
proxy
.
stop
(
request
)
;
LOG
.
info
(
+
serviceName
+
)
;
}
else
{
yarnClient
.
killApplication
(
currentAppId
,
serviceName
+
)
;
LOG
.
info
(
+
serviceName
)
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
if
(
!
waitForAppStopped
)
{
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
long
startTime
=
System
.
currentTimeMillis
(
)
;
int
pollCount
=
0
;
while
(
true
)
{
Thread
.
sleep
(
2000
)
;
report
=
yarnClient
.
getApplicationReport
(
currentAppId
)
;
yarnClient
.
killApplication
(
currentAppId
,
serviceName
+
)
;
LOG
.
info
(
+
serviceName
)
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
if
(
!
waitForAppStopped
)
{
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
long
startTime
=
System
.
currentTimeMillis
(
)
;
int
pollCount
=
0
;
while
(
true
)
{
Thread
.
sleep
(
2000
)
;
report
=
yarnClient
.
getApplicationReport
(
currentAppId
)
;
if
(
terminatedStates
.
contains
(
report
.
getYarnApplicationState
(
)
)
)
{
LOG
.
info
(
+
serviceName
+
)
;
break
;
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
if
(
!
waitForAppStopped
)
{
cleanUpRegistry
(
serviceName
)
;
return
EXIT_SUCCESS
;
}
long
startTime
=
System
.
currentTimeMillis
(
)
;
int
pollCount
=
0
;
while
(
true
)
{
Thread
.
sleep
(
2000
)
;
report
=
yarnClient
.
getApplicationReport
(
currentAppId
)
;
if
(
terminatedStates
.
contains
(
report
.
getYarnApplicationState
(
)
)
)
{
LOG
.
info
(
+
serviceName
+
)
;
break
;
}
if
(
(
System
.
currentTimeMillis
(
)
-
startTime
)
>
10000
)
{
LOG
.
info
(
+
serviceName
)
;
@
Override
public
int
actionDestroy
(
String
serviceName
)
throws
YarnException
,
IOException
{
ServiceApiUtil
.
validateNameFormat
(
serviceName
,
getConfig
(
)
)
;
verifyNoLiveAppInRM
(
serviceName
,
)
;
Path
appDir
=
fs
.
buildClusterDirPath
(
serviceName
)
;
FileSystem
fileSystem
=
fs
.
getFileSystem
(
)
;
cachedAppInfo
.
remove
(
serviceName
)
;
int
ret
=
EXIT_SUCCESS
;
if
(
fileSystem
.
exists
(
appDir
)
)
{
if
(
fileSystem
.
delete
(
appDir
,
true
)
)
{
@
Override
public
int
actionDestroy
(
String
serviceName
)
throws
YarnException
,
IOException
{
ServiceApiUtil
.
validateNameFormat
(
serviceName
,
getConfig
(
)
)
;
verifyNoLiveAppInRM
(
serviceName
,
)
;
Path
appDir
=
fs
.
buildClusterDirPath
(
serviceName
)
;
FileSystem
fileSystem
=
fs
.
getFileSystem
(
)
;
cachedAppInfo
.
remove
(
serviceName
)
;
int
ret
=
EXIT_SUCCESS
;
if
(
fileSystem
.
exists
(
appDir
)
)
{
if
(
fileSystem
.
delete
(
appDir
,
true
)
)
{
LOG
.
info
(
+
serviceName
+
+
appDir
)
;
}
else
{
String
message
=
+
serviceName
+
+
appDir
;
LOG
.
info
(
message
)
;
throw
new
YarnException
(
message
)
;
}
}
else
{
cachedAppInfo
.
remove
(
serviceName
)
;
int
ret
=
EXIT_SUCCESS
;
if
(
fileSystem
.
exists
(
appDir
)
)
{
if
(
fileSystem
.
delete
(
appDir
,
true
)
)
{
LOG
.
info
(
+
serviceName
+
+
appDir
)
;
}
else
{
String
message
=
+
serviceName
+
+
appDir
;
LOG
.
info
(
message
)
;
throw
new
YarnException
(
message
)
;
}
}
else
{
LOG
.
info
(
+
serviceName
+
+
appDir
)
;
ret
=
EXIT_NOT_FOUND
;
}
Path
publicResourceDir
=
new
Path
(
fs
.
getBasePath
(
)
,
serviceName
)
;
if
(
fileSystem
.
exists
(
publicResourceDir
)
)
{
if
(
fileSystem
.
delete
(
publicResourceDir
,
true
)
)
{
if
(
fileSystem
.
delete
(
appDir
,
true
)
)
{
LOG
.
info
(
+
serviceName
+
+
appDir
)
;
}
else
{
String
message
=
+
serviceName
+
+
appDir
;
LOG
.
info
(
message
)
;
throw
new
YarnException
(
message
)
;
}
}
else
{
LOG
.
info
(
+
serviceName
+
+
appDir
)
;
ret
=
EXIT_NOT_FOUND
;
}
Path
publicResourceDir
=
new
Path
(
fs
.
getBasePath
(
)
,
serviceName
)
;
if
(
fileSystem
.
exists
(
publicResourceDir
)
)
{
if
(
fileSystem
.
delete
(
publicResourceDir
,
true
)
)
{
LOG
.
info
(
+
serviceName
+
+
publicResourceDir
)
;
}
else
{
String
message
=
+
serviceName
+
+
publicResourceDir
;
LOG
.
info
(
+
serviceName
+
+
appDir
)
;
ret
=
EXIT_NOT_FOUND
;
}
Path
publicResourceDir
=
new
Path
(
fs
.
getBasePath
(
)
,
serviceName
)
;
if
(
fileSystem
.
exists
(
publicResourceDir
)
)
{
if
(
fileSystem
.
delete
(
publicResourceDir
,
true
)
)
{
LOG
.
info
(
+
serviceName
+
+
publicResourceDir
)
;
}
else
{
String
message
=
+
serviceName
+
+
publicResourceDir
;
LOG
.
info
(
message
)
;
throw
new
YarnException
(
message
)
;
}
}
try
{
deleteZKNode
(
serviceName
)
;
}
catch
(
Exception
e
)
{
throw
new
IOException
(
+
serviceName
,
e
)
;
}
if
(
!
cleanUpRegistry
(
serviceName
)
)
{
Path
publicResourceDir
=
new
Path
(
fs
.
getBasePath
(
)
,
serviceName
)
;
if
(
fileSystem
.
exists
(
publicResourceDir
)
)
{
if
(
fileSystem
.
delete
(
publicResourceDir
,
true
)
)
{
LOG
.
info
(
+
serviceName
+
+
publicResourceDir
)
;
}
else
{
String
message
=
+
serviceName
+
+
publicResourceDir
;
LOG
.
info
(
message
)
;
throw
new
YarnException
(
message
)
;
}
}
try
{
deleteZKNode
(
serviceName
)
;
}
catch
(
Exception
e
)
{
throw
new
IOException
(
+
serviceName
,
e
)
;
}
if
(
!
cleanUpRegistry
(
serviceName
)
)
{
if
(
ret
==
EXIT_SUCCESS
)
{
ret
=
EXIT_OTHER_FAILURE
;
LOG
.
info
(
+
serviceName
+
+
publicResourceDir
)
;
}
else
{
String
message
=
+
serviceName
+
+
publicResourceDir
;
LOG
.
info
(
message
)
;
throw
new
YarnException
(
message
)
;
}
}
try
{
deleteZKNode
(
serviceName
)
;
}
catch
(
Exception
e
)
{
throw
new
IOException
(
+
serviceName
,
e
)
;
}
if
(
!
cleanUpRegistry
(
serviceName
)
)
{
if
(
ret
==
EXIT_SUCCESS
)
{
ret
=
EXIT_OTHER_FAILURE
;
}
}
if
(
ret
==
EXIT_SUCCESS
)
{
LOG
.
info
(
,
serviceName
)
;
return
ret
;
private
boolean
deleteZKNode
(
String
serviceName
)
throws
Exception
{
CuratorFramework
curatorFramework
=
getCuratorClient
(
)
;
String
user
=
RegistryUtils
.
currentUser
(
)
;
String
zkPath
=
ServiceRegistryUtils
.
mkServiceHomePath
(
user
,
serviceName
)
;
if
(
curatorFramework
.
checkExists
(
)
.
forPath
(
zkPath
)
!=
null
)
{
curatorFramework
.
delete
(
)
.
deletingChildrenIfNeeded
(
)
.
forPath
(
zkPath
)
;
protected
Path
addJarResource
(
String
serviceName
,
Map
<
String
,
LocalResource
>
localResources
)
throws
IOException
,
YarnException
{
Path
libPath
=
fs
.
buildClusterDirPath
(
serviceName
)
;
ProviderUtils
.
addProviderJar
(
localResources
,
ServiceMaster
.
class
,
SERVICE_CORE_JAR
,
fs
,
libPath
,
,
false
)
;
Path
dependencyLibTarGzip
=
fs
.
getDependencyTarGzip
(
)
;
if
(
actionDependency
(
null
,
false
)
==
EXIT_SUCCESS
)
{
public
ApplicationId
actionStartAndGetId
(
String
serviceName
)
throws
YarnException
,
IOException
{
ServiceApiUtil
.
validateNameFormat
(
serviceName
,
getConfig
(
)
)
;
Service
liveService
=
getStatus
(
serviceName
)
;
if
(
liveService
==
null
||
!
liveService
.
getState
(
)
.
equals
(
ServiceState
.
UPGRADING
)
)
{
Path
appDir
=
checkAppExistOnHdfs
(
serviceName
)
;
Service
service
=
ServiceApiUtil
.
loadService
(
fs
,
serviceName
)
;
ServiceApiUtil
.
validateAndResolveService
(
service
,
fs
,
getConfig
(
)
)
;
verifyNoLiveAppInRM
(
serviceName
,
)
;
ApplicationId
appId
=
submitApp
(
service
)
;
cachedAppInfo
.
put
(
serviceName
,
new
AppInfo
(
appId
,
service
.
getKerberosPrincipal
(
)
.
getPrincipalName
(
)
)
)
;
service
.
setId
(
appId
.
toString
(
)
)
;
Path
appJson
=
ServiceApiUtil
.
writeAppDefinition
(
fs
,
appDir
,
service
)
;
public
ApplicationId
actionStartAndGetId
(
String
serviceName
)
throws
YarnException
,
IOException
{
ServiceApiUtil
.
validateNameFormat
(
serviceName
,
getConfig
(
)
)
;
Service
liveService
=
getStatus
(
serviceName
)
;
if
(
liveService
==
null
||
!
liveService
.
getState
(
)
.
equals
(
ServiceState
.
UPGRADING
)
)
{
Path
appDir
=
checkAppExistOnHdfs
(
serviceName
)
;
Service
service
=
ServiceApiUtil
.
loadService
(
fs
,
serviceName
)
;
ServiceApiUtil
.
validateAndResolveService
(
service
,
fs
,
getConfig
(
)
)
;
verifyNoLiveAppInRM
(
serviceName
,
)
;
ApplicationId
appId
=
submitApp
(
service
)
;
cachedAppInfo
.
put
(
serviceName
,
new
AppInfo
(
appId
,
service
.
getKerberosPrincipal
(
)
.
getPrincipalName
(
)
)
)
;
service
.
setId
(
appId
.
toString
(
)
)
;
Path
appJson
=
ServiceApiUtil
.
writeAppDefinition
(
fs
,
appDir
,
service
)
;
LOG
.
info
(
+
service
.
getName
(
)
+
+
appJson
)
;
return
appId
;
}
else
{
if
(
!
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
return
;
}
String
principalName
=
service
.
getKerberosPrincipal
(
)
.
getPrincipalName
(
)
;
if
(
StringUtils
.
isEmpty
(
principalName
)
)
{
LOG
.
warn
(
+
service
.
getName
(
)
)
;
return
;
}
if
(
StringUtils
.
isEmpty
(
service
.
getKerberosPrincipal
(
)
.
getKeytab
(
)
)
)
{
LOG
.
warn
(
+
service
.
getName
(
)
)
;
return
;
}
URI
keytabURI
;
try
{
keytabURI
=
new
URI
(
service
.
getKerberosPrincipal
(
)
.
getKeytab
(
)
)
;
}
catch
(
URISyntaxException
e
)
{
throw
new
YarnException
(
e
)
;
}
if
(
.
equals
(
keytabURI
.
getScheme
(
)
)
)
{
if
(
StringUtils
.
isEmpty
(
service
.
getKerberosPrincipal
(
)
.
getKeytab
(
)
)
)
{
LOG
.
warn
(
+
service
.
getName
(
)
)
;
return
;
}
URI
keytabURI
;
try
{
keytabURI
=
new
URI
(
service
.
getKerberosPrincipal
(
)
.
getKeytab
(
)
)
;
}
catch
(
URISyntaxException
e
)
{
throw
new
YarnException
(
e
)
;
}
if
(
.
equals
(
keytabURI
.
getScheme
(
)
)
)
{
LOG
.
info
(
+
keytabURI
)
;
}
else
{
Path
keytabPath
=
new
Path
(
keytabURI
)
;
if
(
!
fileSystem
.
getFileSystem
(
)
.
exists
(
keytabPath
)
)
{
LOG
.
warn
(
service
.
getName
(
)
+
+
principalName
+
+
keytabPath
)
;
return
;
public
Service
getStatus
(
String
serviceName
)
throws
IOException
,
YarnException
{
ServiceApiUtil
.
validateNameFormat
(
serviceName
,
getConfig
(
)
)
;
Service
appSpec
=
new
Service
(
)
;
appSpec
.
setName
(
serviceName
)
;
appSpec
.
setState
(
ServiceState
.
STOPPED
)
;
ApplicationId
currentAppId
=
getAppId
(
serviceName
)
;
if
(
currentAppId
==
null
)
{
}
appSpec
.
setId
(
currentAppId
.
toString
(
)
)
;
ApplicationReport
appReport
=
null
;
try
{
appReport
=
yarnClient
.
getApplicationReport
(
currentAppId
)
;
}
catch
(
ApplicationNotFoundException
e
)
{
LOG
.
info
(
,
currentAppId
)
;
return
appSpec
;
}
if
(
appReport
==
null
)
{
LOG
.
warn
(
,
currentAppId
)
;
return
appSpec
;
}
appSpec
.
setState
(
convertState
(
appReport
.
getYarnApplicationState
(
)
)
)
;
ApplicationTimeout
lifetime
=
appReport
.
getApplicationTimeouts
(
)
.
get
(
ApplicationTimeoutType
.
LIFETIME
)
;
if
(
lifetime
!=
null
)
{
appSpec
.
setLifetime
(
lifetime
.
getRemainingTime
(
)
)
;
}
if
(
appReport
.
getYarnApplicationState
(
)
!=
RUNNING
)
{
public
int
actionDependency
(
String
destinationFolder
,
boolean
overwrite
)
{
String
currentUser
=
RegistryUtils
.
currentUser
(
)
;
dependencyLibTarGzip
=
fs
.
getDependencyTarGzip
(
)
;
}
else
{
dependencyLibTarGzip
=
new
Path
(
destinationFolder
,
YarnServiceConstants
.
DEPENDENCY_TAR_GZ_FILE_NAME
+
YarnServiceConstants
.
DEPENDENCY_TAR_GZ_FILE_EXT
)
;
}
if
(
fs
.
isFile
(
dependencyLibTarGzip
)
&&
!
overwrite
)
{
System
.
out
.
println
(
String
.
format
(
,
dependencyLibTarGzip
.
toUri
(
)
)
)
;
return
EXIT_SUCCESS
;
}
String
[
]
libDirs
=
ServiceUtils
.
getLibDirs
(
)
;
if
(
libDirs
.
length
>
0
)
{
File
tempLibTarGzipFile
=
null
;
try
{
if
(
!
checkPermissions
(
dependencyLibTarGzip
)
)
{
return
EXIT_UNAUTHORIZED
;
}
tempLibTarGzipFile
=
File
.
createTempFile
(
YarnServiceConstants
.
DEPENDENCY_TAR_GZ_FILE_NAME
+
,
YarnServiceConstants
.
DEPENDENCY_TAR_GZ_FILE_EXT
)
;
tarGzipFolder
(
libDirs
,
tempLibTarGzipFile
,
createJarFilter
(
)
)
;
fs
.
copyLocalFileToHdfs
(
tempLibTarGzipFile
,
dependencyLibTarGzip
,
new
FsPermission
(
YarnServiceConstants
.
DEPENDENCY_DIR_PERMISSIONS
)
)
;
private
boolean
checkPermissions
(
Path
dependencyLibTarGzip
)
throws
IOException
{
AccessControlList
yarnAdminAcl
=
new
AccessControlList
(
getConfig
(
)
.
get
(
YarnConfiguration
.
YARN_ADMIN_ACL
,
YarnConfiguration
.
DEFAULT_YARN_ADMIN_ACL
)
)
;
AccessControlList
dfsAdminAcl
=
new
AccessControlList
(
getConfig
(
)
.
get
(
DFSConfigKeys
.
DFS_ADMIN
,
)
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
if
(
!
yarnAdminAcl
.
isUserAllowed
(
ugi
)
&&
!
dfsAdminAcl
.
isUserAllowed
(
ugi
)
)
{
private
void
checkAndScheduleHealthThresholdMonitor
(
)
{
int
healthThresholdPercent
=
YarnServiceConf
.
getInt
(
CONTAINER_HEALTH_THRESHOLD_PERCENT
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_PERCENT
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
if
(
healthThresholdPercent
==
CONTAINER_HEALTH_THRESHOLD_PERCENT_DISABLED
)
{
private
void
checkAndScheduleHealthThresholdMonitor
(
)
{
int
healthThresholdPercent
=
YarnServiceConf
.
getInt
(
CONTAINER_HEALTH_THRESHOLD_PERCENT
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_PERCENT
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
if
(
healthThresholdPercent
==
CONTAINER_HEALTH_THRESHOLD_PERCENT_DISABLED
)
{
LOG
.
info
(
,
componentSpec
.
getName
(
)
)
;
return
;
}
if
(
healthThresholdPercent
<=
0
||
healthThresholdPercent
>
100
)
{
LOG
.
error
(
+
,
healthThresholdPercent
,
componentSpec
.
getName
(
)
)
;
return
;
}
long
window
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
long
initDelay
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
long
pollFrequency
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
if
(
window
<=
0
)
{
LOG
.
error
(
+
,
window
,
componentSpec
.
getName
(
)
)
;
return
;
}
if
(
initDelay
<
0
)
{
LOG
.
info
(
,
componentSpec
.
getName
(
)
)
;
return
;
}
if
(
healthThresholdPercent
<=
0
||
healthThresholdPercent
>
100
)
{
LOG
.
error
(
+
,
healthThresholdPercent
,
componentSpec
.
getName
(
)
)
;
return
;
}
long
window
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
long
initDelay
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
long
pollFrequency
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
if
(
window
<=
0
)
{
LOG
.
error
(
+
,
window
,
componentSpec
.
getName
(
)
)
;
return
;
}
if
(
initDelay
<
0
)
{
LOG
.
error
(
+
,
initDelay
,
componentSpec
.
getName
(
)
)
;
return
;
}
if
(
pollFrequency
<=
0
)
{
if
(
healthThresholdPercent
<=
0
||
healthThresholdPercent
>
100
)
{
LOG
.
error
(
+
,
healthThresholdPercent
,
componentSpec
.
getName
(
)
)
;
return
;
}
long
window
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_WINDOW_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
long
initDelay
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_INIT_DELAY_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
long
pollFrequency
=
YarnServiceConf
.
getLong
(
CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC
,
DEFAULT_CONTAINER_HEALTH_THRESHOLD_POLL_FREQUENCY_SEC
,
componentSpec
.
getConfiguration
(
)
,
scheduler
.
getConfig
(
)
)
;
if
(
window
<=
0
)
{
LOG
.
error
(
+
,
window
,
componentSpec
.
getName
(
)
)
;
return
;
}
if
(
initDelay
<
0
)
{
LOG
.
error
(
+
,
initDelay
,
componentSpec
.
getName
(
)
)
;
return
;
}
if
(
pollFrequency
<=
0
)
{
LOG
.
error
(
+
,
pollFrequency
,
componentSpec
.
getName
(
)
)
;
return
;
private
void
assignContainerToCompInstance
(
Container
container
)
{
if
(
pendingInstances
.
size
(
)
==
0
)
{
@
SuppressWarnings
(
{
}
)
public
void
requestContainers
(
long
count
)
{
LOG
.
info
(
,
componentSpec
.
getName
(
)
,
count
)
;
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
Resource
componentResource
=
componentSpec
.
getResource
(
)
;
Resource
resource
=
Resource
.
newInstance
(
componentResource
.
calcMemoryMB
(
)
,
componentResource
.
getCpus
(
)
)
;
if
(
componentResource
.
getAdditional
(
)
!=
null
)
{
for
(
Map
.
Entry
<
String
,
ResourceInformation
>
entry
:
componentResource
.
getAdditional
(
)
.
entrySet
(
)
)
{
String
resourceName
=
entry
.
getKey
(
)
;
if
(
resourceName
.
equals
(
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
ResourceInformation
.
MEMORY_URI
)
||
resourceName
.
equals
(
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
ResourceInformation
.
VCORES_URI
)
)
{
LOG
.
warn
(
+
+
resourceName
)
;
continue
;
}
ResourceInformation
specInfo
=
entry
.
getValue
(
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
ResourceInformation
ri
=
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
ResourceInformation
.
newInstance
(
entry
.
getKey
(
)
,
specInfo
.
getUnit
(
)
,
specInfo
.
getValue
(
)
,
specInfo
.
getTags
(
)
,
specInfo
.
getAttributes
(
)
)
;
resource
.
setResourceInformation
(
resourceName
,
ri
)
;
}
}
if
(
!
scheduler
.
hasAtLeastOnePlacementConstraint
(
)
)
{
for
(
int
i
=
0
;
i
<
count
;
i
++
)
{
ContainerRequest
request
=
ContainerRequest
.
newBuilder
(
)
.
capability
(
resource
)
.
priority
(
priority
)
.
allocationRequestId
(
allocateId
)
.
relaxLocality
(
true
)
.
build
(
)
;
private
void
setComponentState
(
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
state
)
{
org
.
apache
.
hadoop
.
yarn
.
service
.
api
.
records
.
ComponentState
curState
=
componentSpec
.
getState
(
)
;
if
(
!
curState
.
equals
(
state
)
)
{
componentSpec
.
setState
(
state
)
;
@
VisibleForTesting
static
void
handleComponentInstanceRelaunch
(
ComponentInstance
compInstance
,
ComponentInstanceEvent
event
,
boolean
failureBeforeLaunch
,
String
containerDiag
)
{
Component
comp
=
compInstance
.
getComponent
(
)
;
boolean
hasContainerFailed
=
failureBeforeLaunch
||
hasContainerFailed
(
event
.
getStatus
(
)
)
;
ComponentRestartPolicy
restartPolicy
=
comp
.
getRestartPolicyHandler
(
)
;
ContainerState
containerState
=
hasContainerFailed
?
ContainerState
.
FAILED
:
ContainerState
.
SUCCEEDED
;
if
(
compInstance
.
getContainerSpec
(
)
!=
null
)
{
compInstance
.
getContainerSpec
(
)
.
setState
(
containerState
)
;
}
if
(
restartPolicy
.
shouldRelaunchInstance
(
compInstance
,
event
.
getStatus
(
)
)
)
{
comp
.
requestContainers
(
1
)
;
comp
.
reInsertPendingInstance
(
compInstance
)
;
StringBuilder
builder
=
new
StringBuilder
(
)
;
builder
.
append
(
compInstance
.
getCompInstanceId
(
)
)
.
append
(
)
.
append
(
event
.
getContainerId
(
)
)
.
append
(
)
.
append
(
)
.
append
(
System
.
lineSeparator
(
)
)
.
append
(
)
.
append
(
failureBeforeLaunch
||
event
.
getStatus
(
)
==
null
?
null
:
event
.
getStatus
(
)
.
getExitStatus
(
)
)
.
append
(
)
.
append
(
failureBeforeLaunch
?
FAILED_BEFORE_LAUNCH_DIAG
:
(
event
.
getStatus
(
)
!=
null
?
event
.
getStatus
(
)
.
getDiagnostics
(
)
:
UPGRADE_FAILED
)
)
;
if
(
event
.
getStatus
(
)
!=
null
&&
event
.
getStatus
(
)
.
getExitStatus
(
)
!=
0
)
{
LOG
.
error
(
builder
.
toString
(
)
)
;
}
else
{
boolean
hasContainerFailed
=
failureBeforeLaunch
||
hasContainerFailed
(
event
.
getStatus
(
)
)
;
ComponentRestartPolicy
restartPolicy
=
comp
.
getRestartPolicyHandler
(
)
;
ContainerState
containerState
=
hasContainerFailed
?
ContainerState
.
FAILED
:
ContainerState
.
SUCCEEDED
;
if
(
compInstance
.
getContainerSpec
(
)
!=
null
)
{
compInstance
.
getContainerSpec
(
)
.
setState
(
containerState
)
;
}
if
(
restartPolicy
.
shouldRelaunchInstance
(
compInstance
,
event
.
getStatus
(
)
)
)
{
comp
.
requestContainers
(
1
)
;
comp
.
reInsertPendingInstance
(
compInstance
)
;
StringBuilder
builder
=
new
StringBuilder
(
)
;
builder
.
append
(
compInstance
.
getCompInstanceId
(
)
)
.
append
(
)
.
append
(
event
.
getContainerId
(
)
)
.
append
(
)
.
append
(
)
.
append
(
System
.
lineSeparator
(
)
)
.
append
(
)
.
append
(
failureBeforeLaunch
||
event
.
getStatus
(
)
==
null
?
null
:
event
.
getStatus
(
)
.
getExitStatus
(
)
)
.
append
(
)
.
append
(
failureBeforeLaunch
?
FAILED_BEFORE_LAUNCH_DIAG
:
(
event
.
getStatus
(
)
!=
null
?
event
.
getStatus
(
)
.
getDiagnostics
(
)
:
UPGRADE_FAILED
)
)
;
if
(
event
.
getStatus
(
)
!=
null
&&
event
.
getStatus
(
)
.
getExitStatus
(
)
!=
0
)
{
LOG
.
error
(
builder
.
toString
(
)
)
;
}
else
{
LOG
.
info
(
builder
.
toString
(
)
)
;
}
if
(
compInstance
.
timelineServiceEnabled
)
{
LOG
.
error
(
builder
.
toString
(
)
)
;
}
else
{
LOG
.
info
(
builder
.
toString
(
)
)
;
}
if
(
compInstance
.
timelineServiceEnabled
)
{
LOG
.
info
(
,
event
.
getContainerId
(
)
,
containerState
)
;
int
exitStatus
=
failureBeforeLaunch
||
event
.
getStatus
(
)
==
null
?
ContainerExitStatus
.
INVALID
:
event
.
getStatus
(
)
.
getExitStatus
(
)
;
compInstance
.
serviceTimelinePublisher
.
componentInstanceFinished
(
event
.
getContainerId
(
)
,
exitStatus
,
containerState
,
containerDiag
)
;
}
}
else
{
if
(
hasContainerFailed
)
{
comp
.
markAsFailed
(
compInstance
)
;
}
else
{
comp
.
markAsSucceeded
(
compInstance
)
;
}
if
(
compInstance
.
timelineServiceEnabled
)
{
int
exitStatus
=
failureBeforeLaunch
||
event
.
getStatus
(
)
==
null
?
ContainerExitStatus
.
INVALID
:
event
.
getStatus
(
)
.
getExitStatus
(
)
;
compInstance
.
serviceTimelinePublisher
.
componentInstanceFinished
(
event
.
getContainerId
(
)
,
exitStatus
,
containerState
,
containerDiag
)
;
public
void
setContainerState
(
ContainerState
state
)
{
this
.
writeLock
.
lock
(
)
;
try
{
ContainerState
curState
=
containerSpec
.
getState
(
)
;
if
(
!
curState
.
equals
(
state
)
)
{
containerSpec
.
setState
(
state
)
;
if
(
existingIP
!=
null
&&
newIP
.
equals
(
existingIP
)
)
{
doRegistryUpdate
=
false
;
}
}
ObjectMapper
mapper
=
new
ObjectMapper
(
)
;
try
{
Map
<
String
,
List
<
Map
<
String
,
String
>>>
ports
=
null
;
ports
=
mapper
.
readValue
(
status
.
getExposedPorts
(
)
,
new
TypeReference
<
Map
<
String
,
List
<
Map
<
String
,
String
>>>
>
(
)
{
}
)
;
container
.
setExposedPorts
(
ports
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
,
e
)
;
}
setContainerStatus
(
status
.
getContainerId
(
)
,
status
)
;
if
(
containerRec
!=
null
&&
timelineServiceEnabled
&&
doRegistryUpdate
)
{
serviceTimelinePublisher
.
componentInstanceIPHostUpdated
(
containerRec
)
;
}
if
(
doRegistryUpdate
)
{
cleanupRegistry
(
status
.
getContainerId
(
)
)
;
public
void
cleanupRegistryAndCompHdfsDir
(
ContainerId
containerId
)
{
cleanupRegistry
(
containerId
)
;
try
{
if
(
compInstanceDir
!=
null
&&
fs
.
exists
(
compInstanceDir
)
)
{
boolean
deleted
=
fs
.
delete
(
compInstanceDir
,
true
)
;
if
(
!
deleted
)
{
public
ContainerLaunchContext
completeContainerLaunch
(
)
throws
IOException
{
String
cmdStr
=
ServiceUtils
.
join
(
commands
,
,
false
)
;
public
ContainerLaunchContext
completeContainerLaunch
(
)
throws
IOException
{
String
cmdStr
=
ServiceUtils
.
join
(
commands
,
,
false
)
;
log
.
debug
(
,
cmdStr
)
;
containerLaunchContext
.
setCommands
(
commands
)
;
if
(
log
.
isDebugEnabled
(
)
)
{
log
.
debug
(
)
;
for
(
Map
.
Entry
<
String
,
String
>
envPair
:
envVars
.
entrySet
(
)
)
{
private
void
dumpLocalResources
(
)
{
if
(
log
.
isDebugEnabled
(
)
)
{
log
.
debug
(
,
localResources
.
size
(
)
)
;
for
(
Map
.
Entry
<
String
,
LocalResource
>
entry
:
localResources
.
entrySet
(
)
)
{
String
key
=
entry
.
getKey
(
)
;
LocalResource
val
=
entry
.
getValue
(
)
;
if
(
desiredContainerCount
==
0
)
{
return
;
}
long
readyContainerCount
=
component
.
getNumReadyInstances
(
)
;
float
thresholdFraction
=
(
float
)
healthThresholdPercent
/
100
;
float
readyContainerFraction
=
(
float
)
readyContainerCount
/
desiredContainerCount
;
boolean
healthChanged
=
false
;
if
(
Math
.
abs
(
readyContainerFraction
-
prevReadyContainerFraction
)
>
.0000001
)
{
prevReadyContainerFraction
=
readyContainerFraction
;
healthChanged
=
true
;
}
String
readyContainerPercentStr
=
String
.
format
(
,
readyContainerFraction
*
100
)
;
if
(
readyContainerFraction
<
thresholdFraction
)
{
long
currentTimestamp
=
System
.
nanoTime
(
)
;
if
(
firstOccurrenceTimestamp
==
0
)
{
firstOccurrenceTimestamp
=
currentTimestamp
;
Date
date
=
new
Date
(
)
;
LOG
.
info
(
+
,
component
.
getName
(
)
,
date
.
getTime
(
)
,
date
)
;
}
long
elapsedTime
=
currentTimestamp
-
firstOccurrenceTimestamp
;
long
elapsedTimeSecs
=
TimeUnit
.
SECONDS
.
convert
(
elapsedTime
,
TimeUnit
.
NANOSECONDS
)
;
LOG
.
warn
(
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
if
(
elapsedTime
>
healthThresholdWindowNanos
)
{
LOG
.
warn
(
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
String
exitDiag
=
String
.
format
(
+
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
component
.
getScheduler
(
)
.
getDiagnostics
(
)
.
append
(
exitDiag
)
;
LOG
.
warn
(
exitDiag
)
;
try
{
Thread
.
sleep
(
5000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
error
(
,
e
)
;
}
ExitUtil
.
terminate
(
-
1
)
;
}
}
else
{
long
elapsedTime
=
currentTimestamp
-
firstOccurrenceTimestamp
;
long
elapsedTimeSecs
=
TimeUnit
.
SECONDS
.
convert
(
elapsedTime
,
TimeUnit
.
NANOSECONDS
)
;
LOG
.
warn
(
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
if
(
elapsedTime
>
healthThresholdWindowNanos
)
{
LOG
.
warn
(
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
String
exitDiag
=
String
.
format
(
+
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
component
.
getScheduler
(
)
.
getDiagnostics
(
)
.
append
(
exitDiag
)
;
LOG
.
warn
(
exitDiag
)
;
try
{
Thread
.
sleep
(
5000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
error
(
,
e
)
;
}
ExitUtil
.
terminate
(
-
1
)
;
}
}
else
{
String
logMsg
=
+
;
if
(
elapsedTime
>
healthThresholdWindowNanos
)
{
LOG
.
warn
(
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
String
exitDiag
=
String
.
format
(
+
+
,
component
.
getName
(
)
,
readyContainerPercentStr
,
healthThresholdPercent
,
elapsedTimeSecs
,
healthThresholdWindowSecs
)
;
component
.
getScheduler
(
)
.
getDiagnostics
(
)
.
append
(
exitDiag
)
;
LOG
.
warn
(
exitDiag
)
;
try
{
Thread
.
sleep
(
5000
)
;
}
catch
(
InterruptedException
e
)
{
LOG
.
error
(
,
e
)
;
}
ExitUtil
.
terminate
(
-
1
)
;
}
}
else
{
String
logMsg
=
+
;
if
(
healthChanged
)
{
LOG
.
info
(
logMsg
,
component
.
getName
(
)
,
healthThresholdPercent
,
readyContainerPercentStr
,
readyContainerCount
,
desiredContainerCount
)
;
}
else
{
return
status
;
}
String
ip
=
instance
.
getContainerStatus
(
)
.
getIPs
(
)
.
get
(
0
)
;
HttpURLConnection
connection
=
null
;
try
{
URL
url
=
new
URL
(
urlString
.
replace
(
HOST_TOKEN
,
ip
)
)
;
connection
=
getConnection
(
url
,
this
.
timeout
)
;
int
rc
=
connection
.
getResponseCode
(
)
;
if
(
rc
<
min
||
rc
>
max
)
{
String
error
=
+
url
+
+
rc
;
log
.
info
(
error
)
;
status
.
fail
(
this
,
new
IOException
(
error
)
)
;
}
else
{
status
.
succeed
(
this
)
;
}
}
catch
(
Throwable
e
)
{
String
error
=
+
urlString
+
+
ip
+
+
e
;
public
static
synchronized
void
createConfigFileAndAddLocalResource
(
AbstractLauncher
launcher
,
SliderFileSystem
fs
,
ContainerLaunchService
.
ComponentLaunchContext
compLaunchContext
,
Map
<
String
,
String
>
tokensForSubstitution
,
ComponentInstance
instance
,
ServiceContext
context
,
ProviderService
.
ResolvedLaunchParams
resolvedParams
)
throws
IOException
{
Path
compInstanceDir
=
initCompInstanceDir
(
fs
,
compLaunchContext
,
instance
)
;
if
(
!
fs
.
getFileSystem
(
)
.
exists
(
compInstanceDir
)
)
{
}
else
{
log
.
info
(
+
compPublicResourceDir
)
;
}
log
.
debug
(
+
instance
.
getCompInstanceName
(
)
,
System
.
lineSeparator
(
)
,
tokensForSubstitution
)
;
for
(
ConfigFile
originalFile
:
compLaunchContext
.
getConfiguration
(
)
.
getFiles
(
)
)
{
if
(
isStaticFile
(
originalFile
)
)
{
continue
;
}
ConfigFile
configFile
=
originalFile
.
copy
(
)
;
String
fileName
=
new
Path
(
configFile
.
getDestFile
(
)
)
.
getName
(
)
;
for
(
Map
.
Entry
<
String
,
String
>
token
:
tokensForSubstitution
.
entrySet
(
)
)
{
configFile
.
setDestFile
(
configFile
.
getDestFile
(
)
.
replaceAll
(
Pattern
.
quote
(
token
.
getKey
(
)
)
,
token
.
getValue
(
)
)
)
;
}
Path
remoteFile
=
null
;
LocalResourceVisibility
visibility
=
configFile
.
getVisibility
(
)
;
if
(
visibility
!=
null
&&
visibility
.
equals
(
LocalResourceVisibility
.
PUBLIC
)
)
{
remoteFile
=
new
Path
(
compPublicResourceDir
,
fileName
)
;
}
else
{
private
static
void
addLocalResource
(
AbstractLauncher
launcher
,
String
symlink
,
LocalResource
localResource
,
Path
destFile
,
ProviderService
.
ResolvedLaunchParams
resolvedParams
)
{
if
(
destFile
.
isAbsolute
(
)
)
{
launcher
.
addLocalResource
(
symlink
,
localResource
,
destFile
.
toString
(
)
)
;
catch
(
ExecutionException
e
)
{
log
.
info
(
+
configFile
,
e
)
;
return
;
}
org
.
apache
.
hadoop
.
conf
.
Configuration
confCopy
=
new
org
.
apache
.
hadoop
.
conf
.
Configuration
(
false
)
;
for
(
Map
.
Entry
<
String
,
String
>
entry
:
conf
.
entrySet
(
)
)
{
confCopy
.
set
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
for
(
Map
.
Entry
<
String
,
String
>
entry
:
configFile
.
getProperties
(
)
.
entrySet
(
)
)
{
confCopy
.
set
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
for
(
Map
.
Entry
<
String
,
String
>
entry
:
confCopy
)
{
String
val
=
entry
.
getValue
(
)
;
if
(
val
!=
null
)
{
for
(
Map
.
Entry
<
String
,
String
>
token
:
tokensForSubstitution
.
entrySet
(
)
)
{
val
=
val
.
replaceAll
(
Pattern
.
quote
(
token
.
getKey
(
)
)
,
token
.
getValue
(
)
)
;
confCopy
.
set
(
entry
.
getKey
(
)
,
val
)
;
}
}
}
try
(
OutputStream
output
=
fs
.
create
(
remoteFile
)
)
{
public
ServiceRecord
getComponent
(
String
componentName
)
throws
IOException
{
String
path
=
RegistryUtils
.
componentPath
(
user
,
serviceClass
,
instanceName
,
componentName
)
;
public
void
deleteComponent
(
ComponentInstanceId
instanceId
,
String
containerId
)
throws
IOException
{
String
path
=
RegistryUtils
.
componentPath
(
user
,
serviceClass
,
instanceName
,
containerId
)
;
if
(
serviceTimelinePublisher
.
isStopped
(
)
)
{
log
.
warn
(
+
)
;
return
;
}
boolean
isServiceMetrics
=
false
;
boolean
isComponentMetrics
=
false
;
String
appId
=
null
;
for
(
MetricsTag
tag
:
record
.
tags
(
)
)
{
if
(
tag
.
name
(
)
.
equals
(
)
&&
tag
.
value
(
)
.
equals
(
)
)
{
isServiceMetrics
=
true
;
}
else
if
(
tag
.
name
(
)
.
equals
(
)
&&
tag
.
value
(
)
.
equals
(
)
)
{
isComponentMetrics
=
true
;
break
;
}
else
if
(
tag
.
name
(
)
.
equals
(
)
)
{
appId
=
tag
.
value
(
)
;
}
}
if
(
isServiceMetrics
&&
appId
!=
null
)
{
}
boolean
isServiceMetrics
=
false
;
boolean
isComponentMetrics
=
false
;
String
appId
=
null
;
for
(
MetricsTag
tag
:
record
.
tags
(
)
)
{
if
(
tag
.
name
(
)
.
equals
(
)
&&
tag
.
value
(
)
.
equals
(
)
)
{
isServiceMetrics
=
true
;
}
else
if
(
tag
.
name
(
)
.
equals
(
)
&&
tag
.
value
(
)
.
equals
(
)
)
{
isComponentMetrics
=
true
;
break
;
}
else
if
(
tag
.
name
(
)
.
equals
(
)
)
{
appId
=
tag
.
value
(
)
;
}
}
if
(
isServiceMetrics
&&
appId
!=
null
)
{
log
.
debug
(
,
record
)
;
serviceTimelinePublisher
.
publishMetrics
(
record
.
metrics
(
)
,
appId
,
ServiceTimelineEntityType
.
SERVICE_ATTEMPT
.
toString
(
)
,
record
.
timestamp
(
)
)
;
}
else
if
(
isComponentMetrics
)
{
private
void
putEntity
(
TimelineEntity
entity
)
{
try
{
if
(
log
.
isDebugEnabled
(
)
)
{
public
static
Configuration
loadFromResource
(
String
resource
)
{
Configuration
conf
=
new
Configuration
(
false
)
;
URL
resURL
=
getResourceUrl
(
resource
)
;
if
(
resURL
!=
null
)
{
public
void
verifyDirectoryNonexistent
(
Path
clusterDirectory
)
throws
IOException
,
SliderException
{
if
(
fileSystem
.
exists
(
clusterDirectory
)
)
{
public
LocalResource
submitFile
(
File
localFile
,
Path
tempPath
,
String
subdir
,
String
destFileName
)
throws
IOException
{
Path
src
=
new
Path
(
localFile
.
toString
(
)
)
;
Path
subdirPath
=
new
Path
(
tempPath
,
subdir
)
;
fileSystem
.
mkdirs
(
subdirPath
)
;
Path
destPath
=
new
Path
(
subdirPath
,
destFileName
)
;
public
static
String
generateToken
(
String
server
)
throws
IOException
,
InterruptedException
{
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
public
static
String
generateToken
(
String
server
)
throws
IOException
,
InterruptedException
{
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
LOG
.
debug
(
,
currentUser
)
;
String
challenge
=
currentUser
.
doAs
(
new
PrivilegedExceptionAction
<
String
>
(
)
{
@
Override
public
String
run
(
)
throws
Exception
{
try
{
Oid
mechOid
=
KerberosUtil
.
getOidInstance
(
)
;
GSSManager
manager
=
GSSManager
.
getInstance
(
)
;
GSSName
serverName
=
manager
.
createName
(
+
server
,
GSSName
.
NT_HOSTBASED_SERVICE
)
;
GSSContext
gssContext
=
manager
.
createContext
(
serverName
.
canonicalize
(
mechOid
)
,
mechOid
,
null
,
GSSContext
.
DEFAULT_LIFETIME
)
;
gssContext
.
requestMutualAuth
(
true
)
;
gssContext
.
requestCredDeleg
(
true
)
;
byte
[
]
inToken
=
new
byte
[
0
]
;
byte
[
]
outToken
=
gssContext
.
initSecContext
(
inToken
,
0
,
inToken
.
length
)
;
gssContext
.
dispose
(
)
;
String
challenge
=
currentUser
.
doAs
(
new
PrivilegedExceptionAction
<
String
>
(
)
{
@
Override
public
String
run
(
)
throws
Exception
{
try
{
Oid
mechOid
=
KerberosUtil
.
getOidInstance
(
)
;
GSSManager
manager
=
GSSManager
.
getInstance
(
)
;
GSSName
serverName
=
manager
.
createName
(
+
server
,
GSSName
.
NT_HOSTBASED_SERVICE
)
;
GSSContext
gssContext
=
manager
.
createContext
(
serverName
.
canonicalize
(
mechOid
)
,
mechOid
,
null
,
GSSContext
.
DEFAULT_LIFETIME
)
;
gssContext
.
requestMutualAuth
(
true
)
;
gssContext
.
requestCredDeleg
(
true
)
;
byte
[
]
inToken
=
new
byte
[
0
]
;
byte
[
]
outToken
=
gssContext
.
initSecContext
(
inToken
,
0
,
inToken
.
length
)
;
gssContext
.
dispose
(
)
;
LOG
.
debug
(
,
serverName
)
;
return
new
String
(
BASE_64_CODEC
.
encode
(
outToken
)
,
StandardCharsets
.
US_ASCII
)
;
}
catch
(
GSSException
|
IllegalAccessException
|
NoSuchFieldException
|
ClassNotFoundException
e
)
{
public
static
Builder
connect
(
String
url
)
throws
URISyntaxException
,
IOException
,
InterruptedException
{
boolean
useKerberos
=
UserGroupInformation
.
isSecurityEnabled
(
)
;
URI
resource
=
new
URI
(
url
)
;
Client
client
=
Client
.
create
(
)
;
Builder
builder
=
client
.
resource
(
url
)
.
type
(
MediaType
.
APPLICATION_JSON
)
;
if
(
useKerberos
)
{
String
challenge
=
generateToken
(
resource
.
getHost
(
)
)
;
builder
.
header
(
HttpHeaders
.
AUTHORIZATION
,
+
challenge
)
;
try
{
validateDockerClientConfiguration
(
service
,
conf
)
;
}
catch
(
IOException
e
)
{
throw
new
IllegalArgumentException
(
e
)
;
}
Configuration
globalConf
=
service
.
getConfiguration
(
)
;
Set
<
String
>
componentNames
=
new
HashSet
<
>
(
)
;
List
<
Component
>
componentsToRemove
=
new
ArrayList
<
>
(
)
;
List
<
Component
>
componentsToAdd
=
new
ArrayList
<
>
(
)
;
for
(
Component
comp
:
service
.
getComponents
(
)
)
{
int
maxCompLength
=
RegistryConstants
.
MAX_FQDN_LABEL_LENGTH
;
maxCompLength
=
maxCompLength
-
Long
.
toString
(
Long
.
MAX_VALUE
)
.
length
(
)
;
if
(
dnsEnabled
&&
comp
.
getName
(
)
.
length
(
)
>
maxCompLength
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
RestApiErrorMessages
.
ERROR_COMPONENT_NAME_INVALID
,
maxCompLength
,
comp
.
getName
(
)
)
)
;
}
if
(
service
.
getName
(
)
.
equals
(
comp
.
getName
(
)
)
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
RestApiErrorMessages
.
ERROR_COMPONENT_NAME_CONFLICTS_WITH_SERVICE_NAME
,
comp
.
getName
(
)
,
service
.
getName
(
)
)
)
;
Configuration
globalConf
=
service
.
getConfiguration
(
)
;
Set
<
String
>
componentNames
=
new
HashSet
<
>
(
)
;
List
<
Component
>
componentsToRemove
=
new
ArrayList
<
>
(
)
;
List
<
Component
>
componentsToAdd
=
new
ArrayList
<
>
(
)
;
for
(
Component
comp
:
service
.
getComponents
(
)
)
{
int
maxCompLength
=
RegistryConstants
.
MAX_FQDN_LABEL_LENGTH
;
maxCompLength
=
maxCompLength
-
Long
.
toString
(
Long
.
MAX_VALUE
)
.
length
(
)
;
if
(
dnsEnabled
&&
comp
.
getName
(
)
.
length
(
)
>
maxCompLength
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
RestApiErrorMessages
.
ERROR_COMPONENT_NAME_INVALID
,
maxCompLength
,
comp
.
getName
(
)
)
)
;
}
if
(
service
.
getName
(
)
.
equals
(
comp
.
getName
(
)
)
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
RestApiErrorMessages
.
ERROR_COMPONENT_NAME_CONFLICTS_WITH_SERVICE_NAME
,
comp
.
getName
(
)
,
service
.
getName
(
)
)
)
;
}
if
(
componentNames
.
contains
(
comp
.
getName
(
)
)
)
{
throw
new
IllegalArgumentException
(
+
comp
.
getName
(
)
)
;
}
if
(
comp
.
getArtifact
(
)
!=
null
&&
comp
.
getArtifact
(
)
.
getType
(
)
==
Artifact
.
TypeEnum
.
SERVICE
)
{
if
(
StringUtils
.
isEmpty
(
comp
.
getArtifact
(
)
.
getId
(
)
)
)
{
maxCompLength
=
maxCompLength
-
Long
.
toString
(
Long
.
MAX_VALUE
)
.
length
(
)
;
if
(
dnsEnabled
&&
comp
.
getName
(
)
.
length
(
)
>
maxCompLength
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
RestApiErrorMessages
.
ERROR_COMPONENT_NAME_INVALID
,
maxCompLength
,
comp
.
getName
(
)
)
)
;
}
if
(
service
.
getName
(
)
.
equals
(
comp
.
getName
(
)
)
)
{
throw
new
IllegalArgumentException
(
String
.
format
(
RestApiErrorMessages
.
ERROR_COMPONENT_NAME_CONFLICTS_WITH_SERVICE_NAME
,
comp
.
getName
(
)
,
service
.
getName
(
)
)
)
;
}
if
(
componentNames
.
contains
(
comp
.
getName
(
)
)
)
{
throw
new
IllegalArgumentException
(
+
comp
.
getName
(
)
)
;
}
if
(
comp
.
getArtifact
(
)
!=
null
&&
comp
.
getArtifact
(
)
.
getType
(
)
==
Artifact
.
TypeEnum
.
SERVICE
)
{
if
(
StringUtils
.
isEmpty
(
comp
.
getArtifact
(
)
.
getId
(
)
)
)
{
throw
new
IllegalArgumentException
(
RestApiErrorMessages
.
ERROR_ARTIFACT_ID_INVALID
)
;
}
LOG
.
info
(
,
comp
.
getName
(
)
)
;
componentsToRemove
.
add
(
comp
)
;
List
<
Component
>
externalComponents
=
getComponents
(
fs
,
comp
.
getArtifact
(
)
.
getId
(
)
)
;
for
(
Component
c
:
externalComponents
)
{
Component
override
=
service
.
getComponent
(
c
.
getName
(
)
)
;
private
static
void
validateDockerClientConfiguration
(
Service
service
,
org
.
apache
.
hadoop
.
conf
.
Configuration
conf
)
throws
IOException
{
String
dockerClientConfig
=
service
.
getDockerClientConfig
(
)
;
if
(
!
StringUtils
.
isEmpty
(
dockerClientConfig
)
)
{
Path
dockerClientConfigPath
=
new
Path
(
dockerClientConfig
)
;
FileSystem
fs
=
dockerClientConfigPath
.
getFileSystem
(
conf
)
;
public
static
Service
loadService
(
SliderFileSystem
fs
,
String
serviceName
)
throws
IOException
{
Path
serviceJson
=
getServiceJsonPath
(
fs
,
serviceName
)
;
public
static
Service
loadServiceUpgrade
(
SliderFileSystem
fs
,
String
serviceName
,
String
version
)
throws
IOException
{
Path
versionPath
=
fs
.
buildClusterUpgradeDirPath
(
serviceName
,
version
)
;
Path
versionedDef
=
new
Path
(
versionPath
,
serviceName
+
)
;
public
static
Service
loadServiceFrom
(
SliderFileSystem
fs
,
Path
appDefPath
)
throws
IOException
{
public
static
void
createDirAndPersistApp
(
SliderFileSystem
fs
,
Path
appDir
,
Service
service
)
throws
IOException
,
SliderException
{
FsPermission
appDirPermission
=
new
FsPermission
(
)
;
fs
.
createWithPermissions
(
appDir
,
appDirPermission
)
;
Path
appJson
=
writeAppDefinition
(
fs
,
appDir
,
service
)
;
private
static
boolean
serviceDependencySatisfied
(
Service
service
)
{
boolean
result
=
true
;
try
{
List
<
String
>
dependencies
=
service
.
getDependencies
(
)
;
org
.
apache
.
hadoop
.
conf
.
Configuration
conf
=
new
org
.
apache
.
hadoop
.
conf
.
Configuration
(
)
;
if
(
dependencies
!=
null
&&
dependencies
.
size
(
)
>
0
)
{
ServiceClient
sc
=
new
ServiceClient
(
)
;
sc
.
init
(
conf
)
;
sc
.
start
(
)
;
for
(
String
dependent
:
dependencies
)
{
Service
dependentService
=
sc
.
getStatus
(
dependent
)
;
if
(
dependentService
.
getState
(
)
==
null
||
!
dependentService
.
getState
(
)
.
equals
(
ServiceState
.
STABLE
)
)
{
result
=
false
;
InetAddress
.
getByName
(
name
)
;
return
true
;
}
catch
(
UnknownHostException
e
)
{
return
false
;
}
}
String
dnsURI
=
String
.
format
(
,
addr
)
;
Hashtable
<
String
,
Object
>
env
=
new
Hashtable
<
>
(
)
;
env
.
put
(
Context
.
INITIAL_CONTEXT_FACTORY
,
)
;
env
.
put
(
Context
.
PROVIDER_URL
,
dnsURI
)
;
try
{
DirContext
ictx
=
new
InitialDirContext
(
env
)
;
Attributes
attrs
=
ictx
.
getAttributes
(
name
,
new
String
[
]
{
}
)
;
if
(
attrs
.
size
(
)
>
0
)
{
return
true
;
}
}
catch
(
NameNotFoundException
e
)
{
}
catch
(
NamingException
e
)
{
if
(
loader
==
null
)
{
throw
new
IOException
(
+
my_class
+
)
;
}
String
class_file
=
my_class
.
getName
(
)
.
replaceAll
(
,
)
+
;
Enumeration
<
URL
>
urlEnumeration
=
loader
.
getResources
(
class_file
)
;
for
(
;
urlEnumeration
.
hasMoreElements
(
)
;
)
{
URL
url
=
urlEnumeration
.
nextElement
(
)
;
if
(
.
equals
(
url
.
getProtocol
(
)
)
)
{
String
toReturn
=
url
.
getPath
(
)
;
if
(
toReturn
.
startsWith
(
)
)
{
toReturn
=
toReturn
.
substring
(
.
length
(
)
)
;
}
toReturn
=
toReturn
.
replaceAll
(
,
)
;
toReturn
=
URLDecoder
.
decode
(
toReturn
,
)
;
String
jarFilePath
=
toReturn
.
replaceAll
(
,
)
;
return
new
File
(
jarFilePath
)
;
}
else
{
public
static
void
putAllJars
(
Map
<
String
,
LocalResource
>
providerResources
,
SliderFileSystem
sliderFileSystem
,
Path
tempPath
,
String
libDir
,
String
srcPath
)
throws
IOException
,
SliderException
{
public
static
void
tarGzipFolder
(
String
[
]
libDirs
,
File
tarGzipFile
,
FilenameFilter
filter
)
throws
IOException
{
public
void
deleteComponentDir
(
String
serviceVersion
,
String
compName
)
throws
IOException
{
Path
path
=
getComponentDir
(
serviceVersion
,
compName
)
;
if
(
fileSystem
.
exists
(
path
)
)
{
fileSystem
.
delete
(
path
,
true
)
;
public
void
deleteComponentsVersionDirIfEmpty
(
String
serviceVersion
)
throws
IOException
{
Path
path
=
new
Path
(
new
Path
(
getAppDir
(
)
,
)
,
serviceVersion
)
;
if
(
fileSystem
.
exists
(
path
)
&&
fileSystem
.
listStatus
(
path
)
.
length
==
0
)
{
fileSystem
.
delete
(
path
,
true
)
;
@
Override
protected
Path
getAppDir
(
)
{
Path
path
=
new
Path
(
new
Path
(
,
)
,
service
.
getName
(
)
)
;
throw
new
RuntimeException
(
e
)
;
}
}
return
yarnRegistryView
;
}
@
Override
protected
AMRMClientAsync
<
AMRMClient
.
ContainerRequest
>
createAMRMClient
(
)
{
AMRMClientImpl
client1
=
new
AMRMClientImpl
(
)
{
@
Override
public
AllocateResponse
allocate
(
float
progressIndicator
)
throws
YarnException
,
IOException
{
AllocateResponse
.
AllocateResponseBuilder
builder
=
AllocateResponse
.
newBuilder
(
)
;
synchronized
(
feedContainers
)
{
if
(
feedContainers
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
}
else
{
List
<
Container
>
allocatedContainers
=
new
LinkedList
<
>
(
)
;
Iterator
<
Container
>
itor
=
feedContainers
.
iterator
(
)
;
while
(
itor
.
hasNext
(
)
)
{
Container
c
=
itor
.
next
(
)
;
org
.
apache
.
hadoop
.
yarn
.
service
.
component
.
Component
component
=
componentsById
.
get
(
c
.
getAllocationRequestId
(
)
)
;
conf
.
set
(
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
,
)
;
conf
.
set
(
YarnConfiguration
.
NM_CONTAINER_MON_RESOURCE_CALCULATOR
,
LinuxResourceCalculatorPlugin
.
class
.
getName
(
)
)
;
conf
.
set
(
YarnConfiguration
.
NM_CONTAINER_MON_PROCESS_TREE
,
ProcfsBasedProcessTree
.
class
.
getName
(
)
)
;
conf
.
setBoolean
(
YarnConfiguration
.
YARN_MINICLUSTER_CONTROL_RESOURCE_MONITORING
,
true
)
;
conf
.
setBoolean
(
TIMELINE_SERVICE_ENABLED
,
false
)
;
conf
.
setInt
(
YarnConfiguration
.
NM_MAX_PER_DISK_UTILIZATION_PERCENTAGE
,
100
)
;
conf
.
setLong
(
DEBUG_NM_DELETE_DELAY_SEC
,
60000
)
;
conf
.
setLong
(
AM_RESOURCE_MEM
,
526
)
;
conf
.
setLong
(
YarnServiceConf
.
READINESS_CHECK_INTERVAL
,
5
)
;
conf
.
setBoolean
(
NM_VMEM_CHECK_ENABLED
,
false
)
;
conf
.
setBoolean
(
NM_PMEM_CHECK_ENABLED
,
false
)
;
conf
.
set
(
HttpServer2
.
FILTER_INITIALIZER_PROPERTY
,
+
)
;
zkCluster
=
new
TestingCluster
(
1
)
;
zkCluster
.
start
(
)
;
conf
.
set
(
YarnConfiguration
.
RM_ZK_ADDRESS
,
zkCluster
.
getConnectString
(
)
)
;
}
conf
.
set
(
YARN_SERVICE_BASE_PATH
,
basedir
.
getAbsolutePath
(
)
)
;
if
(
yarnCluster
==
null
)
{
yarnCluster
=
new
MiniYARNCluster
(
this
.
getClass
(
)
.
getSimpleName
(
)
,
1
,
numNodeManager
,
1
,
1
)
;
yarnCluster
.
init
(
conf
)
;
yarnCluster
.
start
(
)
;
waitForNMsToRegister
(
)
;
URL
url
=
Thread
.
currentThread
(
)
.
getContextClassLoader
(
)
.
getResource
(
)
;
if
(
url
==
null
)
{
throw
new
RuntimeException
(
)
;
}
Configuration
yarnClusterConfig
=
yarnCluster
.
getConfig
(
)
;
yarnClusterConfig
.
set
(
YarnConfiguration
.
YARN_APPLICATION_CLASSPATH
,
new
File
(
url
.
getPath
(
)
)
.
getParent
(
)
)
;
ByteArrayOutputStream
bytesOut
=
new
ByteArrayOutputStream
(
)
;
yarnClusterConfig
.
writeXml
(
bytesOut
)
;
bytesOut
.
close
(
)
;
OutputStream
os
=
new
FileOutputStream
(
new
File
(
url
.
getPath
(
)
)
)
;
protected
Multimap
<
String
,
String
>
waitForAllCompToBeReady
(
ServiceClient
client
,
Service
exampleApp
)
throws
TimeoutException
,
InterruptedException
{
int
expectedTotalContainers
=
countTotalContainers
(
exampleApp
)
;
Multimap
<
String
,
String
>
allContainers
=
HashMultimap
.
create
(
)
;
GenericTestUtils
.
waitFor
(
(
)
->
{
try
{
Service
retrievedApp
=
client
.
getStatus
(
exampleApp
.
getName
(
)
)
;
int
totalReadyContainers
=
0
;
allContainers
.
clear
(
)
;
LOG
.
info
(
+
retrievedApp
.
getComponents
(
)
.
size
(
)
)
;
for
(
Component
component
:
retrievedApp
.
getComponents
(
)
)
{
protected
Multimap
<
String
,
String
>
waitForAllCompToBeReady
(
ServiceClient
client
,
Service
exampleApp
)
throws
TimeoutException
,
InterruptedException
{
int
expectedTotalContainers
=
countTotalContainers
(
exampleApp
)
;
Multimap
<
String
,
String
>
allContainers
=
HashMultimap
.
create
(
)
;
GenericTestUtils
.
waitFor
(
(
)
->
{
try
{
Service
retrievedApp
=
client
.
getStatus
(
exampleApp
.
getName
(
)
)
;
int
totalReadyContainers
=
0
;
allContainers
.
clear
(
)
;
LOG
.
info
(
+
retrievedApp
.
getComponents
(
)
.
size
(
)
)
;
for
(
Component
component
:
retrievedApp
.
getComponents
(
)
)
{
LOG
.
info
(
+
component
.
getName
(
)
)
;
protected
Multimap
<
String
,
String
>
waitForAllCompToBeReady
(
ServiceClient
client
,
Service
exampleApp
)
throws
TimeoutException
,
InterruptedException
{
int
expectedTotalContainers
=
countTotalContainers
(
exampleApp
)
;
Multimap
<
String
,
String
>
allContainers
=
HashMultimap
.
create
(
)
;
GenericTestUtils
.
waitFor
(
(
)
->
{
try
{
Service
retrievedApp
=
client
.
getStatus
(
exampleApp
.
getName
(
)
)
;
int
totalReadyContainers
=
0
;
allContainers
.
clear
(
)
;
LOG
.
info
(
+
retrievedApp
.
getComponents
(
)
.
size
(
)
)
;
for
(
Component
component
:
retrievedApp
.
getComponents
(
)
)
{
LOG
.
info
(
+
component
.
getName
(
)
)
;
LOG
.
info
(
component
.
toString
(
)
)
;
if
(
component
.
getContainers
(
)
!=
null
)
{
if
(
component
.
getContainers
(
)
.
size
(
)
==
exampleApp
.
getComponent
(
component
.
getName
(
)
)
.
getNumberOfContainers
(
)
)
{
for
(
Container
container
:
component
.
getContainers
(
)
)
{
try
{
Service
retrievedApp
=
client
.
getStatus
(
exampleApp
.
getName
(
)
)
;
int
totalReadyContainers
=
0
;
allContainers
.
clear
(
)
;
LOG
.
info
(
+
retrievedApp
.
getComponents
(
)
.
size
(
)
)
;
for
(
Component
component
:
retrievedApp
.
getComponents
(
)
)
{
LOG
.
info
(
+
component
.
getName
(
)
)
;
LOG
.
info
(
component
.
toString
(
)
)
;
if
(
component
.
getContainers
(
)
!=
null
)
{
if
(
component
.
getContainers
(
)
.
size
(
)
==
exampleApp
.
getComponent
(
component
.
getName
(
)
)
.
getNumberOfContainers
(
)
)
{
for
(
Container
container
:
component
.
getContainers
(
)
)
{
LOG
.
info
(
+
container
.
getState
(
)
+
+
component
.
getName
(
)
)
;
if
(
container
.
getState
(
)
==
ContainerState
.
READY
)
{
totalReadyContainers
++
;
allContainers
.
put
(
component
.
getName
(
)
,
container
.
getId
(
)
)
;
int
totalReadyContainers
=
0
;
allContainers
.
clear
(
)
;
LOG
.
info
(
+
retrievedApp
.
getComponents
(
)
.
size
(
)
)
;
for
(
Component
component
:
retrievedApp
.
getComponents
(
)
)
{
LOG
.
info
(
+
component
.
getName
(
)
)
;
LOG
.
info
(
component
.
toString
(
)
)
;
if
(
component
.
getContainers
(
)
!=
null
)
{
if
(
component
.
getContainers
(
)
.
size
(
)
==
exampleApp
.
getComponent
(
component
.
getName
(
)
)
.
getNumberOfContainers
(
)
)
{
for
(
Container
container
:
component
.
getContainers
(
)
)
{
LOG
.
info
(
+
container
.
getState
(
)
+
+
component
.
getName
(
)
)
;
if
(
container
.
getState
(
)
==
ContainerState
.
READY
)
{
totalReadyContainers
++
;
allContainers
.
put
(
component
.
getName
(
)
,
container
.
getId
(
)
)
;
LOG
.
info
(
+
container
.
getId
(
)
)
;
}
}
}
else
{
compA
.
getConfiguration
(
)
.
getEnv
(
)
.
put
(
,
)
;
Artifact
artifact
=
new
Artifact
(
)
;
artifact
.
setType
(
Artifact
.
TypeEnum
.
TARBALL
)
;
compA
.
artifact
(
artifact
)
;
exampleApp
.
addComponent
(
compA
)
;
try
{
MockServiceAM
am
=
new
MockServiceAM
(
exampleApp
)
;
am
.
init
(
conf
)
;
am
.
start
(
)
;
ServiceScheduler
scheduler
=
am
.
context
.
scheduler
;
scheduler
.
syncSysFs
(
exampleApp
)
;
scheduler
.
close
(
)
;
am
.
stop
(
)
;
am
.
close
(
)
;
}
catch
(
Exception
e
)
{
conf
.
setBoolean
(
YarnConfiguration
.
YARN_MINICLUSTER_FIXED_PORTS
,
true
)
;
conf
.
setBoolean
(
YarnConfiguration
.
YARN_MINICLUSTER_USE_RPC
,
true
)
;
conf
.
setInt
(
YarnConfiguration
.
RM_MAX_COMPLETED_APPLICATIONS
,
YarnConfiguration
.
DEFAULT_RM_MAX_COMPLETED_APPLICATIONS
)
;
setConf
(
conf
)
;
setupInternal
(
NUM_NMS
)
;
ServiceClient
client
=
createClient
(
getConf
(
)
)
;
Service
exampleApp
=
createExampleApplication
(
)
;
client
.
actionCreate
(
exampleApp
)
;
Multimap
<
String
,
String
>
containersBeforeFailure
=
waitForAllCompToBeReady
(
client
,
exampleApp
)
;
LOG
.
info
(
)
;
getYarnCluster
(
)
.
restartResourceManager
(
getYarnCluster
(
)
.
getActiveRMIndex
(
)
)
;
GenericTestUtils
.
waitFor
(
(
)
->
getYarnCluster
(
)
.
getResourceManager
(
)
.
getServiceState
(
)
==
org
.
apache
.
hadoop
.
service
.
Service
.
STATE
.
STARTED
,
2000
,
200000
)
;
Assert
.
assertTrue
(
,
getYarnCluster
(
)
.
waitForNodeManagersToConnect
(
5000
)
)
;
ApplicationId
exampleAppId
=
ApplicationId
.
fromString
(
exampleApp
.
getId
(
)
)
;
ApplicationAttemptId
applicationAttemptId
=
client
.
getYarnClient
(
)
.
getApplicationReport
(
exampleAppId
)
.
getCurrentApplicationAttemptId
(
)
;
ApplicationId
exampleAppId
=
ApplicationId
.
fromString
(
exampleApp
.
getId
(
)
)
;
ApplicationAttemptId
applicationAttemptId
=
client
.
getYarnClient
(
)
.
getApplicationReport
(
exampleAppId
)
.
getCurrentApplicationAttemptId
(
)
;
LOG
.
info
(
,
applicationAttemptId
)
;
client
.
getYarnClient
(
)
.
failApplicationAttempt
(
applicationAttemptId
)
;
GenericTestUtils
.
waitFor
(
(
)
->
{
try
{
ApplicationReport
ar
=
client
.
getYarnClient
(
)
.
getApplicationReport
(
exampleAppId
)
;
return
ar
.
getCurrentApplicationAttemptId
(
)
.
getAttemptId
(
)
==
2
&&
ar
.
getYarnApplicationState
(
)
==
YarnApplicationState
.
RUNNING
;
}
catch
(
YarnException
|
IOException
e
)
{
throw
new
RuntimeException
(
,
e
)
;
}
}
,
2000
,
200000
)
;
Multimap
<
String
,
String
>
containersAfterFailure
=
waitForAllCompToBeReady
(
client
,
exampleApp
)
;
containersBeforeFailure
.
keys
(
)
.
forEach
(
compName
->
{
Assert
.
assertEquals
(
+
compName
,
containersBeforeFailure
.
get
(
compName
)
.
size
(
)
,
containersAfterFailure
.
get
(
compName
)
==
null
?
0
:
containersAfterFailure
.
get
(
compName
)
.
size
(
)
)
;
}
)
;
component
.
getConfiguration
(
)
.
getEnv
(
)
.
put
(
,
)
;
client
.
initiateUpgrade
(
service
)
;
waitForServiceToBeInState
(
client
,
service
,
ServiceState
.
UPGRADING
)
;
SliderFileSystem
fs
=
new
SliderFileSystem
(
getConf
(
)
)
;
Service
fromFs
=
ServiceApiUtil
.
loadServiceUpgrade
(
fs
,
service
.
getName
(
)
,
service
.
getVersion
(
)
)
;
Assert
.
assertEquals
(
service
.
getName
(
)
,
fromFs
.
getName
(
)
)
;
Assert
.
assertEquals
(
service
.
getVersion
(
)
,
fromFs
.
getVersion
(
)
)
;
Service
liveService
=
client
.
getStatus
(
service
.
getName
(
)
)
;
client
.
actionUpgrade
(
service
,
liveService
.
getComponent
(
component
.
getName
(
)
)
.
getContainers
(
)
)
;
waitForAllCompToBeReady
(
client
,
service
)
;
client
.
actionStart
(
service
.
getName
(
)
)
;
waitForServiceToBeStable
(
client
,
service
)
;
Service
active
=
client
.
getStatus
(
service
.
getName
(
)
)
;
Assert
.
assertEquals
(
,
ComponentState
.
STABLE
,
active
.
getComponent
(
component
.
getName
(
)
)
.
getState
(
)
)
;
Assert
.
assertEquals
(
,
,
active
.
getComponent
(
component
.
getName
(
)
)
.
getConfiguration
(
)
.
getEnv
(
)
)
;
waitForServiceToBeStable
(
client
,
service
)
;
Component
component
=
service
.
getComponents
(
)
.
iterator
(
)
.
next
(
)
;
service
.
setState
(
ServiceState
.
EXPRESS_UPGRADING
)
;
service
.
setVersion
(
)
;
component
.
getConfiguration
(
)
.
getEnv
(
)
.
put
(
,
)
;
Component
component2
=
service
.
getComponent
(
)
;
component2
.
getConfiguration
(
)
.
getEnv
(
)
.
put
(
,
)
;
client
.
actionUpgradeExpress
(
service
)
;
waitForServiceToBeExpressUpgrading
(
client
,
service
)
;
waitForServiceToBeStable
(
client
,
service
)
;
Service
active
=
client
.
getStatus
(
service
.
getName
(
)
)
;
Assert
.
assertEquals
(
,
service
.
getVersion
(
)
,
active
.
getVersion
(
)
)
;
Assert
.
assertEquals
(
,
ComponentState
.
STABLE
,
active
.
getComponent
(
component
.
getName
(
)
)
.
getState
(
)
)
;
Assert
.
assertEquals
(
,
,
active
.
getComponent
(
component
.
getName
(
)
)
.
getConfiguration
(
)
.
getEnv
(
)
)
;
Assert
.
assertEquals
(
,
,
active
.
getComponent
(
component2
.
getName
(
)
)
.
getConfiguration
(
)
.
getEnv
(
)
)
;
waitForServiceToBeStable
(
client
,
service
)
;
service
.
setState
(
ServiceState
.
UPGRADING
)
;
service
.
setVersion
(
)
;
component
.
getConfiguration
(
)
.
getEnv
(
)
.
put
(
,
)
;
client
.
initiateUpgrade
(
service
)
;
waitForServiceToBeInState
(
client
,
service
,
ServiceState
.
UPGRADING
)
;
Service
liveService
=
client
.
getStatus
(
service
.
getName
(
)
)
;
Container
container
=
liveService
.
getComponent
(
component
.
getName
(
)
)
.
getContainers
(
)
.
iterator
(
)
.
next
(
)
;
client
.
actionUpgrade
(
service
,
Lists
.
newArrayList
(
container
)
)
;
Thread
.
sleep
(
500
)
;
client
.
actionCancelUpgrade
(
service
.
getName
(
)
)
;
waitForServiceToBeStable
(
client
,
service
)
;
Service
active
=
client
.
getStatus
(
service
.
getName
(
)
)
;
Assert
.
assertEquals
(
,
ComponentState
.
STABLE
,
active
.
getComponent
(
component
.
getName
(
)
)
.
getState
(
)
)
;
Assert
.
assertEquals
(
,
,
active
.
getComponent
(
component
.
getName
(
)
)
.
getConfiguration
(
)
.
getEnv
(
)
)
;
Assert
.
assertNotEquals
(
,
ServiceState
.
STABLE
,
service
.
getState
(
)
)
;
Assert
.
assertEquals
(
,
ComponentState
.
FLEXING
,
component
.
getState
(
)
)
;
Assert
.
assertEquals
(
,
3
,
component
.
getContainers
(
)
.
size
(
)
)
;
}
compCounts
=
new
HashMap
<
>
(
)
;
compCounts
.
put
(
,
4L
)
;
exampleApp
.
getComponent
(
)
.
setNumberOfContainers
(
4L
)
;
client
.
flexByRestService
(
exampleApp
.
getName
(
)
,
compCounts
)
;
try
{
waitForServiceToBeStable
(
client
,
exampleApp
,
10000
)
;
Assert
.
fail
(
+
)
;
}
catch
(
Exception
e
)
{
service
=
client
.
getStatus
(
exampleApp
.
getName
(
)
)
;
component
=
service
.
getComponent
(
)
;
Assert
.
assertNotEquals
(
,
ServiceState
.
STABLE
,
service
.
getState
(
)
)
;
Assert
.
assertEquals
(
,
ComponentState
.
FLEXING
,
component
.
getState
(
)
)
;
private
int
runCLI
(
String
[
]
args
)
throws
Exception
{
@
Test
public
void
testOverride
(
)
throws
Throwable
{
Service
orig
=
ExampleAppJson
.
loadResource
(
OVERRIDE_JSON
)
;
Configuration
global
=
orig
.
getConfiguration
(
)
;
assertEquals
(
,
global
.
getProperty
(
)
)
;
assertEquals
(
,
global
.
getProperty
(
)
)
;
assertEquals
(
2
,
global
.
getFiles
(
)
.
size
(
)
)
;
Configuration
simple
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertEquals
(
0
,
simple
.
getProperties
(
)
.
size
(
)
)
;
assertEquals
(
1
,
simple
.
getFiles
(
)
.
size
(
)
)
;
Configuration
master
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
0
,
master
.
getFiles
(
)
.
size
(
)
)
;
Configuration
worker
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
Configuration
master
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
0
,
master
.
getFiles
(
)
.
size
(
)
)
;
Configuration
worker
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
LOG
.
info
(
,
worker
)
;
assertEquals
(
3
,
worker
.
getProperties
(
)
.
size
(
)
)
;
assertEquals
(
0
,
worker
.
getFiles
(
)
.
size
(
)
)
;
assertEquals
(
,
worker
.
getProperty
(
)
)
;
assertEquals
(
,
worker
.
getProperty
(
)
)
;
assertNull
(
worker
.
getProperty
(
)
)
;
assertEquals
(
,
worker
.
getProperty
(
)
)
;
SliderFileSystem
sfs
=
ServiceTestUtils
.
initMockFs
(
)
;
ServiceApiUtil
.
validateAndResolveService
(
orig
,
sfs
,
new
YarnConfiguration
(
)
)
;
global
=
orig
.
getConfiguration
(
)
;
assertEquals
(
2
,
global
.
getFiles
(
)
.
size
(
)
)
;
simple
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertEquals
(
2
,
simple
.
getProperties
(
)
.
size
(
)
)
;
assertEquals
(
,
simple
.
getProperty
(
)
)
;
assertEquals
(
,
simple
.
getProperty
(
)
)
;
assertEquals
(
2
,
simple
.
getFiles
(
)
.
size
(
)
)
;
Set
<
ConfigFile
>
files
=
new
HashSet
<
>
(
)
;
Map
<
String
,
String
>
props
=
new
HashMap
<
>
(
)
;
props
.
put
(
,
)
;
props
.
put
(
,
)
;
files
.
add
(
new
ConfigFile
(
)
.
destFile
(
)
.
type
(
ConfigFile
.
TypeEnum
.
PROPERTIES
)
.
properties
(
props
)
)
;
files
.
add
(
new
ConfigFile
(
)
.
destFile
(
)
.
type
(
ConfigFile
.
TypeEnum
.
XML
)
.
properties
(
Collections
.
singletonMap
(
,
)
)
)
;
assertTrue
(
files
.
contains
(
simple
.
getFiles
(
)
.
get
(
0
)
)
)
;
assertTrue
(
files
.
contains
(
simple
.
getFiles
(
)
.
get
(
1
)
)
)
;
master
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertTrue
(
files
.
contains
(
simple
.
getFiles
(
)
.
get
(
1
)
)
)
;
master
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
LOG
.
info
(
,
master
)
;
assertEquals
(
3
,
master
.
getProperties
(
)
.
size
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
2
,
master
.
getFiles
(
)
.
size
(
)
)
;
props
.
put
(
,
)
;
files
.
clear
(
)
;
files
.
add
(
new
ConfigFile
(
)
.
destFile
(
)
.
type
(
ConfigFile
.
TypeEnum
.
PROPERTIES
)
.
properties
(
props
)
)
;
files
.
add
(
new
ConfigFile
(
)
.
destFile
(
)
.
type
(
ConfigFile
.
TypeEnum
.
XML
)
.
properties
(
Collections
.
singletonMap
(
,
)
)
)
;
assertTrue
(
files
.
contains
(
master
.
getFiles
(
)
.
get
(
0
)
)
)
;
assertTrue
(
files
.
contains
(
master
.
getFiles
(
)
.
get
(
1
)
)
)
;
worker
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertEquals
(
0
,
global
.
getProperties
(
)
.
size
(
)
)
;
assertEquals
(
4
,
orig
.
getComponents
(
)
.
size
(
)
)
;
simple
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertEquals
(
3
,
simple
.
getProperties
(
)
.
size
(
)
)
;
assertEquals
(
,
simple
.
getProperty
(
)
)
;
assertEquals
(
,
simple
.
getProperty
(
)
)
;
assertEquals
(
,
simple
.
getProperty
(
)
)
;
master
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
assertEquals
(
5
,
master
.
getProperties
(
)
.
size
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
master
.
getProperty
(
)
)
;
assertEquals
(
,
simple
.
getProperty
(
)
)
;
Configuration
worker
=
orig
.
getComponent
(
)
.
getConfiguration
(
)
;
artifact
.
setType
(
Artifact
.
TypeEnum
.
SERVICE
)
;
try
{
ServiceApiUtil
.
validateAndResolveService
(
app
,
sfs
,
CONF_DNS_ENABLED
)
;
Assert
.
fail
(
EXCEPTION_PREFIX
+
)
;
}
catch
(
IllegalArgumentException
e
)
{
assertEquals
(
ERROR_ARTIFACT_ID_INVALID
,
e
.
getMessage
(
)
)
;
}
artifact
.
setType
(
Artifact
.
TypeEnum
.
TARBALL
)
;
try
{
ServiceApiUtil
.
validateAndResolveService
(
app
,
sfs
,
CONF_DNS_ENABLED
)
;
Assert
.
fail
(
EXCEPTION_PREFIX
+
)
;
}
catch
(
IllegalArgumentException
e
)
{
assertEquals
(
String
.
format
(
ERROR_ARTIFACT_ID_FOR_COMP_INVALID
,
compName
)
,
e
.
getMessage
(
)
)
;
}
artifact
.
setType
(
Artifact
.
TypeEnum
.
DOCKER
)
;
artifact
.
setId
(
)
;
try
{
@
OnWebSocketClose
public
void
onClose
(
Session
session
,
int
status
,
String
reason
)
{
if
(
status
==
1000
)
{
while
(
mySession
.
isOpen
(
)
)
{
mySession
.
getRemote
(
)
.
flush
(
)
;
if
(
consoleReader
.
hasData
(
)
)
{
String
message
=
consoleReader
.
read
(
)
;
mySession
.
getRemote
(
)
.
sendString
(
message
)
;
mySession
.
getRemote
(
)
.
sendString
(
)
;
}
String
message
=
;
mySession
.
getRemote
(
)
.
sendString
(
message
)
;
Thread
.
sleep
(
100
)
;
mySession
.
getRemote
(
)
.
flush
(
)
;
}
inputThread
.
join
(
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
try
{
mySession
.
disconnect
(
)
;
}
catch
(
IOException
e1
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
this
.
maxThreadPoolSize
=
conf
.
getInt
(
YarnConfiguration
.
NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE
,
YarnConfiguration
.
DEFAULT_NM_CLIENT_ASYNC_THREAD_POOL_MAX_SIZE
)
;
Set
<
String
>
allNodes
=
new
HashSet
<
String
>
(
)
;
while
(
!
stopped
.
get
(
)
&&
!
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
try
{
event
=
events
.
take
(
)
;
}
catch
(
InterruptedException
e
)
{
if
(
!
stopped
.
get
(
)
)
{
LOG
.
error
(
,
e
)
;
}
return
;
}
allNodes
.
add
(
event
.
getNodeId
(
)
.
toString
(
)
)
;
int
threadPoolSize
=
threadPool
.
getCorePoolSize
(
)
;
if
(
threadPoolSize
!=
maxThreadPoolSize
)
{
int
nodeNum
=
allNodes
.
size
(
)
;
int
idealThreadPoolSize
=
Math
.
min
(
maxThreadPoolSize
,
nodeNum
)
;
if
(
threadPoolSize
<
idealThreadPoolSize
)
{
int
newThreadPoolSize
=
Math
.
min
(
maxThreadPoolSize
,
idealThreadPoolSize
+
INITIAL_THREAD_POOL_SIZE
)
;
@
Private
@
VisibleForTesting
protected
void
populateNMTokens
(
List
<
NMToken
>
nmTokens
)
{
for
(
NMToken
token
:
nmTokens
)
{
String
nodeId
=
token
.
getNodeId
(
)
.
toString
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
getNMTokenCache
(
)
.
containsToken
(
nodeId
)
)
{
@
Override
public
synchronized
void
requestContainerUpdate
(
Container
container
,
UpdateContainerRequest
updateContainerRequest
)
{
Preconditions
.
checkNotNull
(
container
,
)
;
Preconditions
.
checkNotNull
(
updateContainerRequest
,
)
;
public
synchronized
ContainerManagementProtocolProxyData
getProxy
(
String
containerManagerBindAddr
,
ContainerId
containerId
)
throws
InvalidToken
{
ContainerManagementProtocolProxyData
proxy
=
cmProxy
.
get
(
containerManagerBindAddr
)
;
while
(
proxy
!=
null
&&
!
proxy
.
token
.
getIdentifier
(
)
.
equals
(
nmTokenCache
.
getToken
(
containerManagerBindAddr
)
.
getIdentifier
(
)
)
)
{
private
void
addProxyToCache
(
String
containerManagerBindAddr
,
ContainerManagementProtocolProxyData
proxy
)
{
while
(
cmProxy
.
size
(
)
>=
maxConnectedNMs
)
{
private
boolean
tryCloseProxy
(
ContainerManagementProtocolProxyData
proxy
)
{
proxy
.
activeCallers
--
;
if
(
proxy
.
scheduledForClose
&&
proxy
.
activeCallers
<
0
)
{
@
SuppressWarnings
(
)
void
put
(
Priority
priority
,
String
resourceName
,
ExecutionType
execType
,
Resource
capability
,
ResourceRequestInfo
resReqInfo
)
{
Map
<
String
,
Map
<
ExecutionType
,
TreeMap
<
Resource
,
ResourceRequestInfo
>>>
locationMap
=
remoteRequestsTable
.
get
(
priority
)
;
if
(
locationMap
==
null
)
{
locationMap
=
new
HashMap
<
>
(
)
;
this
.
remoteRequestsTable
.
put
(
priority
,
locationMap
)
;
@
SuppressWarnings
(
)
void
put
(
Priority
priority
,
String
resourceName
,
ExecutionType
execType
,
Resource
capability
,
ResourceRequestInfo
resReqInfo
)
{
Map
<
String
,
Map
<
ExecutionType
,
TreeMap
<
Resource
,
ResourceRequestInfo
>>>
locationMap
=
remoteRequestsTable
.
get
(
priority
)
;
if
(
locationMap
==
null
)
{
locationMap
=
new
HashMap
<
>
(
)
;
this
.
remoteRequestsTable
.
put
(
priority
,
locationMap
)
;
LOG
.
debug
(
,
priority
)
;
}
Map
<
ExecutionType
,
TreeMap
<
Resource
,
ResourceRequestInfo
>>
execTypeMap
=
locationMap
.
get
(
resourceName
)
;
if
(
execTypeMap
==
null
)
{
execTypeMap
=
new
HashMap
<
>
(
)
;
locationMap
.
put
(
resourceName
,
execTypeMap
)
;
LOG
.
debug
(
,
resourceName
)
;
}
TreeMap
<
Resource
,
ResourceRequestInfo
>
capabilityMap
=
execTypeMap
.
get
(
execType
)
;
if
(
capabilityMap
==
null
)
{
capabilityMap
=
new
TreeMap
<
>
(
new
AMRMClientImpl
.
ResourceReverseComparator
(
)
)
;
execTypeMap
.
put
(
execType
,
capabilityMap
)
;
ResourceRequestInfo
remove
(
Priority
priority
,
String
resourceName
,
ExecutionType
execType
,
Resource
capability
)
{
ResourceRequestInfo
retVal
=
null
;
Map
<
String
,
Map
<
ExecutionType
,
TreeMap
<
Resource
,
ResourceRequestInfo
>>>
locationMap
=
remoteRequestsTable
.
get
(
priority
)
;
if
(
locationMap
==
null
)
{
ResourceRequestInfo
decResourceRequest
(
Priority
priority
,
String
resourceName
,
ExecutionTypeRequest
execTypeReq
,
Resource
capability
,
T
req
)
{
ResourceRequestInfo
resourceRequestInfo
=
get
(
priority
,
resourceName
,
execTypeReq
.
getExecutionType
(
)
,
capability
)
;
if
(
resourceRequestInfo
==
null
)
{
if
(
isSecurityEnabled
(
)
)
{
addLogAggregationDelegationToken
(
appContext
.
getAMContainerSpec
(
)
)
;
}
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
e
)
;
}
rmClient
.
submitApplication
(
request
)
;
int
pollCount
=
0
;
long
startTime
=
System
.
currentTimeMillis
(
)
;
EnumSet
<
YarnApplicationState
>
waitingStates
=
EnumSet
.
of
(
YarnApplicationState
.
NEW
,
YarnApplicationState
.
NEW_SAVING
,
YarnApplicationState
.
SUBMITTED
)
;
EnumSet
<
YarnApplicationState
>
failToSubmitStates
=
EnumSet
.
of
(
YarnApplicationState
.
FAILED
,
YarnApplicationState
.
KILLED
)
;
while
(
true
)
{
try
{
ApplicationReport
appReport
=
getApplicationReport
(
applicationId
)
;
YarnApplicationState
state
=
appReport
.
getYarnApplicationState
(
)
;
if
(
!
waitingStates
.
contains
(
state
)
)
{
if
(
failToSubmitStates
.
contains
(
state
)
)
{
LOG
.
warn
(
,
e
)
;
}
rmClient
.
submitApplication
(
request
)
;
int
pollCount
=
0
;
long
startTime
=
System
.
currentTimeMillis
(
)
;
EnumSet
<
YarnApplicationState
>
waitingStates
=
EnumSet
.
of
(
YarnApplicationState
.
NEW
,
YarnApplicationState
.
NEW_SAVING
,
YarnApplicationState
.
SUBMITTED
)
;
EnumSet
<
YarnApplicationState
>
failToSubmitStates
=
EnumSet
.
of
(
YarnApplicationState
.
FAILED
,
YarnApplicationState
.
KILLED
)
;
while
(
true
)
{
try
{
ApplicationReport
appReport
=
getApplicationReport
(
applicationId
)
;
YarnApplicationState
state
=
appReport
.
getYarnApplicationState
(
)
;
if
(
!
waitingStates
.
contains
(
state
)
)
{
if
(
failToSubmitStates
.
contains
(
state
)
)
{
throw
new
YarnException
(
+
applicationId
+
+
appReport
.
getDiagnostics
(
)
)
;
}
LOG
.
info
(
+
applicationId
)
;
break
;
try
{
ApplicationReport
appReport
=
getApplicationReport
(
applicationId
)
;
YarnApplicationState
state
=
appReport
.
getYarnApplicationState
(
)
;
if
(
!
waitingStates
.
contains
(
state
)
)
{
if
(
failToSubmitStates
.
contains
(
state
)
)
{
throw
new
YarnException
(
+
applicationId
+
+
appReport
.
getDiagnostics
(
)
)
;
}
LOG
.
info
(
+
applicationId
)
;
break
;
}
long
elapsedMillis
=
System
.
currentTimeMillis
(
)
-
startTime
;
if
(
enforceAsyncAPITimeout
(
)
&&
elapsedMillis
>=
asyncApiPollTimeoutMillis
)
{
throw
new
YarnException
(
+
applicationId
+
)
;
}
if
(
++
pollCount
%
10
==
0
)
{
LOG
.
info
(
+
+
applicationId
+
+
state
)
;
}
try
{
Thread
.
sleep
(
submitPollIntervalMillis
)
;
dibb
.
reset
(
tokens
)
;
credentials
.
readTokenStorageStream
(
dibb
)
;
tokens
.
rewind
(
)
;
}
Configuration
conf
=
getConfig
(
)
;
String
masterPrincipal
=
YarnClientUtils
.
getRmPrincipal
(
conf
)
;
if
(
StringUtils
.
isEmpty
(
masterPrincipal
)
)
{
throw
new
IOException
(
)
;
}
LOG
.
debug
(
+
masterPrincipal
)
;
LogAggregationFileControllerFactory
factory
=
new
LogAggregationFileControllerFactory
(
conf
)
;
LogAggregationFileController
fileController
=
factory
.
getFileControllerForWrite
(
)
;
Path
remoteRootLogDir
=
fileController
.
getRemoteRootLogDir
(
)
;
FileSystem
fs
=
remoteRootLogDir
.
getFileSystem
(
conf
)
;
final
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
>
[
]
finalTokens
=
fs
.
addDelegationTokens
(
masterPrincipal
,
credentials
)
;
if
(
finalTokens
!=
null
)
{
for
(
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
>
token
:
finalTokens
)
{
private
void
addTimelineDelegationToken
(
ContainerLaunchContext
clc
)
throws
YarnException
,
IOException
{
Credentials
credentials
=
new
Credentials
(
)
;
DataInputByteBuffer
dibb
=
new
DataInputByteBuffer
(
)
;
ByteBuffer
tokens
=
clc
.
getTokens
(
)
;
if
(
tokens
!=
null
)
{
dibb
.
reset
(
tokens
)
;
credentials
.
readTokenStorageStream
(
dibb
)
;
tokens
.
rewind
(
)
;
}
for
(
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
?
extends
TokenIdentifier
>
token
:
credentials
.
getAllTokens
(
)
)
{
if
(
token
.
getKind
(
)
.
equals
(
TimelineDelegationTokenIdentifier
.
KIND_NAME
)
)
{
return
;
}
}
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
TimelineDelegationTokenIdentifier
>
timelineDelegationToken
=
getTimelineDelegationToken
(
)
;
if
(
timelineDelegationToken
==
null
)
{
return
;
}
credentials
.
addToken
(
timelineService
,
timelineDelegationToken
)
;
@
Override
public
void
failApplicationAttempt
(
ApplicationAttemptId
attemptId
)
throws
YarnException
,
IOException
{
request
.
setApplicationId
(
applicationId
)
;
if
(
diagnostics
!=
null
)
{
request
.
setDiagnostics
(
diagnostics
)
;
}
try
{
int
pollCount
=
0
;
long
startTime
=
System
.
currentTimeMillis
(
)
;
while
(
true
)
{
KillApplicationResponse
response
=
rmClient
.
forceKillApplication
(
request
)
;
if
(
response
.
getIsKillCompleted
(
)
)
{
LOG
.
info
(
+
applicationId
)
;
break
;
}
long
elapsedMillis
=
System
.
currentTimeMillis
(
)
-
startTime
;
if
(
enforceAsyncAPITimeout
(
)
&&
elapsedMillis
>=
this
.
asyncApiPollTimeoutMillis
)
{
throw
new
YarnException
(
+
applicationId
+
)
;
}
if
(
++
pollCount
%
10
==
0
)
{
@
Override
public
void
signalToContainer
(
ContainerId
containerId
,
SignalContainerCommand
command
)
throws
YarnException
,
IOException
{
QueueMetrics
queueMetrics
=
new
QueueMetrics
(
)
;
List
<
QueueInfo
>
queuesInfo
;
if
(
queues
.
isEmpty
(
)
)
{
try
{
queuesInfo
=
client
.
getRootQueueInfos
(
)
;
}
catch
(
Exception
ie
)
{
LOG
.
error
(
,
ie
)
;
return
queueMetrics
;
}
}
else
{
queuesInfo
=
new
ArrayList
<
>
(
)
;
for
(
String
queueName
:
queues
)
{
try
{
QueueInfo
qInfo
=
client
.
getQueueInfo
(
queueName
)
;
queuesInfo
.
add
(
qInfo
)
;
}
catch
(
Exception
ie
)
{
public
static
String
generateToken
(
String
server
)
throws
IOException
,
InterruptedException
{
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
public
static
String
generateToken
(
String
server
)
throws
IOException
,
InterruptedException
{
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
LOG
.
debug
(
,
currentUser
)
;
String
challenge
=
currentUser
.
doAs
(
new
PrivilegedExceptionAction
<
String
>
(
)
{
@
Override
public
String
run
(
)
throws
Exception
{
try
{
Oid
mechOid
=
KerberosUtil
.
getOidInstance
(
)
;
GSSManager
manager
=
GSSManager
.
getInstance
(
)
;
GSSName
serverName
=
manager
.
createName
(
+
server
,
GSSName
.
NT_HOSTBASED_SERVICE
)
;
GSSContext
gssContext
=
manager
.
createContext
(
serverName
.
canonicalize
(
mechOid
)
,
mechOid
,
null
,
GSSContext
.
DEFAULT_LIFETIME
)
;
gssContext
.
requestMutualAuth
(
true
)
;
gssContext
.
requestCredDeleg
(
true
)
;
byte
[
]
inToken
=
new
byte
[
0
]
;
byte
[
]
outToken
=
gssContext
.
initSecContext
(
inToken
,
0
,
inToken
.
length
)
;
gssContext
.
dispose
(
)
;
String
challenge
=
currentUser
.
doAs
(
new
PrivilegedExceptionAction
<
String
>
(
)
{
@
Override
public
String
run
(
)
throws
Exception
{
try
{
Oid
mechOid
=
KerberosUtil
.
getOidInstance
(
)
;
GSSManager
manager
=
GSSManager
.
getInstance
(
)
;
GSSName
serverName
=
manager
.
createName
(
+
server
,
GSSName
.
NT_HOSTBASED_SERVICE
)
;
GSSContext
gssContext
=
manager
.
createContext
(
serverName
.
canonicalize
(
mechOid
)
,
mechOid
,
null
,
GSSContext
.
DEFAULT_LIFETIME
)
;
gssContext
.
requestMutualAuth
(
true
)
;
gssContext
.
requestCredDeleg
(
true
)
;
byte
[
]
inToken
=
new
byte
[
0
]
;
byte
[
]
outToken
=
gssContext
.
initSecContext
(
inToken
,
0
,
inToken
.
length
)
;
gssContext
.
dispose
(
)
;
LOG
.
debug
(
,
serverName
)
;
return
new
String
(
BASE_64_CODEC
.
encode
(
outToken
)
,
StandardCharsets
.
US_ASCII
)
;
}
catch
(
GSSException
|
IllegalAccessException
|
NoSuchFieldException
|
ClassNotFoundException
e
)
{
@
Override
public
ContainerReport
getContainerReport
(
String
containerIdStr
)
throws
YarnException
,
IOException
{
ContainerReport
mockReport
=
mock
(
ContainerReport
.
class
)
;
doReturn
(
nodeId
)
.
when
(
mockReport
)
.
getAssignedNode
(
)
;
doReturn
(
)
.
when
(
mockReport
)
.
getNodeHttpAddress
(
)
;
return
mockReport
;
}
}
;
cli
.
setConf
(
conf
)
;
int
exitCode
=
cli
.
run
(
new
String
[
]
{
,
appId
.
toString
(
)
}
)
;
LOG
.
info
(
sysOutStream
.
toString
(
)
)
;
assertTrue
(
exitCode
==
0
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId1
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId2
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId3
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId3
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId3
,
)
)
)
;
LOG
.
info
(
sysOutStream
.
toString
(
)
)
;
assertTrue
(
exitCode
==
0
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId1
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId2
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId3
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId3
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId3
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
createEmptyLog
(
)
)
)
;
sysOutStream
.
reset
(
)
;
exitCode
=
cli
.
run
(
new
String
[
]
{
,
appId
.
toString
(
)
,
,
appAttemptId1
.
toString
(
)
}
)
;
LOG
.
info
(
sysOutStream
.
toString
(
)
)
;
assertTrue
(
exitCode
==
0
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId1
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId2
,
)
)
)
;
assertTrue
(
sysOutStream
.
toString
(
)
.
contains
(
logMessage
(
containerId3
,
)
)
)
;
private
int
runTool
(
String
...
args
)
throws
Exception
{
errOutBytes
.
reset
(
)
;
sysOutBytes
.
reset
(
)
;
LOG
.
info
(
+
Joiner
.
on
(
)
.
join
(
args
)
)
;
int
ret
=
nodeAttributesCLI
.
run
(
args
)
;
errOutput
=
new
String
(
errOutBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
sysOutput
=
new
String
(
sysOutBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
private
int
runTool
(
String
...
args
)
throws
Exception
{
errOutBytes
.
reset
(
)
;
sysOutBytes
.
reset
(
)
;
LOG
.
info
(
+
Joiner
.
on
(
)
.
join
(
args
)
)
;
int
ret
=
nodeAttributesCLI
.
run
(
args
)
;
errOutput
=
new
String
(
errOutBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
sysOutput
=
new
String
(
sysOutBytes
.
toByteArray
(
)
,
Charsets
.
UTF_8
)
;
LOG
.
info
(
+
errOutput
)
;
@
Override
public
void
uncaughtException
(
Thread
t
,
Throwable
e
)
{
if
(
ShutdownHookManager
.
get
(
)
.
isShutdownInProgress
(
)
)
{
@
Override
public
void
uncaughtException
(
Thread
t
,
Throwable
e
)
{
if
(
ShutdownHookManager
.
get
(
)
.
isShutdownInProgress
(
)
)
{
LOG
.
error
(
+
t
+
+
,
e
)
;
}
else
if
(
e
instanceof
Error
)
{
try
{
LOG
.
error
(
FATAL
,
+
t
+
,
e
)
;
}
catch
(
Throwable
err
)
{
}
if
(
e
instanceof
OutOfMemoryError
)
{
try
{
System
.
err
.
println
(
)
;
}
catch
(
Throwable
err
)
{
}
ExitUtil
.
halt
(
-
1
)
;
}
else
{
ExitUtil
.
terminate
(
-
1
)
;
}
}
else
{
public
static
<
T
>
T
createAHSProxy
(
final
Configuration
conf
,
final
Class
<
T
>
protocol
,
InetSocketAddress
ahsAddress
)
throws
IOException
{
@
Public
@
Unstable
public
static
<
T
>
T
createRMProxy
(
final
Configuration
configuration
,
final
Class
<
T
>
protocol
,
UserGroupInformation
user
,
final
Token
<
?
extends
TokenIdentifier
>
token
)
throws
IOException
{
try
{
String
rmClusterId
=
configuration
.
get
(
YarnConfiguration
.
RM_CLUSTER_ID
,
YarnConfiguration
.
DEFAULT_RM_CLUSTER_ID
)
;
@
Override
public
void
init
(
Configuration
conf
,
RMProxy
<
T
>
proxy
,
Class
<
T
>
protocol
)
{
this
.
protocol
=
protocol
;
try
{
YarnConfiguration
yarnConf
=
new
YarnConfiguration
(
conf
)
;
InetSocketAddress
rmAddress
=
proxy
.
getRMAddress
(
yarnConf
,
protocol
)
;
return
putEntities
(
entities
)
;
}
List
<
TimelineEntity
>
entitiesToDBStore
=
new
ArrayList
<
TimelineEntity
>
(
)
;
List
<
TimelineEntity
>
entitiesToSummaryCache
=
new
ArrayList
<
TimelineEntity
>
(
)
;
List
<
TimelineEntity
>
entitiesToEntityCache
=
new
ArrayList
<
TimelineEntity
>
(
)
;
Path
attemptDir
=
attemptDirCache
.
getAppAttemptDir
(
appAttemptId
)
;
for
(
TimelineEntity
entity
:
entities
)
{
if
(
summaryEntityTypes
.
contains
(
entity
.
getEntityType
(
)
)
)
{
entitiesToSummaryCache
.
add
(
entity
)
;
}
else
{
if
(
groupId
!=
null
)
{
entitiesToEntityCache
.
add
(
entity
)
;
}
else
{
entitiesToDBStore
.
add
(
entity
)
;
}
}
}
if
(
!
entitiesToSummaryCache
.
isEmpty
(
)
)
{
Path
summaryLogPath
=
new
Path
(
attemptDir
,
SUMMARY_LOG_PREFIX
+
appAttemptId
.
toString
(
)
)
;
List
<
TimelineEntity
>
entitiesToEntityCache
=
new
ArrayList
<
TimelineEntity
>
(
)
;
Path
attemptDir
=
attemptDirCache
.
getAppAttemptDir
(
appAttemptId
)
;
for
(
TimelineEntity
entity
:
entities
)
{
if
(
summaryEntityTypes
.
contains
(
entity
.
getEntityType
(
)
)
)
{
entitiesToSummaryCache
.
add
(
entity
)
;
}
else
{
if
(
groupId
!=
null
)
{
entitiesToEntityCache
.
add
(
entity
)
;
}
else
{
entitiesToDBStore
.
add
(
entity
)
;
}
}
}
if
(
!
entitiesToSummaryCache
.
isEmpty
(
)
)
{
Path
summaryLogPath
=
new
Path
(
attemptDir
,
SUMMARY_LOG_PREFIX
+
appAttemptId
.
toString
(
)
)
;
LOG
.
debug
(
,
appAttemptId
,
summaryLogPath
)
;
this
.
logFDsCache
.
writeSummaryEntityLogs
(
fs
,
summaryLogPath
,
objMapper
,
appAttemptId
,
entitiesToSummaryCache
,
isAppendSupported
)
;
}
if
(
!
entitiesToEntityCache
.
isEmpty
(
)
)
{
private
void
writeDomain
(
ApplicationAttemptId
appAttemptId
,
TimelineDomain
domain
)
throws
IOException
{
Path
domainLogPath
=
new
Path
(
attemptDirCache
.
getAppAttemptDir
(
appAttemptId
)
,
DOMAIN_LOG_PREFIX
+
appAttemptId
.
toString
(
)
)
;
private
static
void
putTimelineDataInJSONFile
(
String
path
,
String
type
)
{
File
jsonFile
=
new
File
(
path
)
;
if
(
!
jsonFile
.
exists
(
)
)
{
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
e
.
getMessage
(
)
)
;
e
.
printStackTrace
(
System
.
err
)
;
return
;
}
Configuration
conf
=
new
YarnConfiguration
(
)
;
TimelineClient
client
=
TimelineClient
.
createTimelineClient
(
)
;
client
.
init
(
conf
)
;
client
.
start
(
)
;
try
{
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
&&
conf
.
getBoolean
(
YarnConfiguration
.
TIMELINE_SERVICE_ENABLED
,
false
)
&&
conf
.
get
(
YarnConfiguration
.
TIMELINE_HTTP_AUTH_TYPE
)
.
equals
(
KerberosAuthenticationHandler
.
TYPE
)
)
{
Token
<
TimelineDelegationTokenIdentifier
>
token
=
client
.
getDelegationToken
(
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
)
;
UserGroupInformation
.
getCurrentUser
(
)
.
addToken
(
token
)
;
}
if
(
type
.
equals
(
ENTITY_DATA_TYPE
)
)
{
TimelinePutResponse
response
=
client
.
putEntities
(
entities
.
getEntities
(
)
.
toArray
(
new
TimelineEntity
[
entities
.
getEntities
(
)
.
size
(
)
]
)
)
;
if
(
response
.
getErrors
(
)
.
size
(
)
==
0
)
{
Token
<
TimelineDelegationTokenIdentifier
>
token
=
client
.
getDelegationToken
(
UserGroupInformation
.
getCurrentUser
(
)
.
getUserName
(
)
)
;
UserGroupInformation
.
getCurrentUser
(
)
.
addToken
(
token
)
;
}
if
(
type
.
equals
(
ENTITY_DATA_TYPE
)
)
{
TimelinePutResponse
response
=
client
.
putEntities
(
entities
.
getEntities
(
)
.
toArray
(
new
TimelineEntity
[
entities
.
getEntities
(
)
.
size
(
)
]
)
)
;
if
(
response
.
getErrors
(
)
.
size
(
)
==
0
)
{
LOG
.
info
(
)
;
}
else
{
for
(
TimelinePutResponse
.
TimelinePutError
error
:
response
.
getErrors
(
)
)
{
LOG
.
error
(
+
error
.
getEntityType
(
)
+
+
error
.
getEntityId
(
)
+
+
error
.
getErrorCode
(
)
)
;
}
}
}
else
if
(
type
.
equals
(
DOMAIN_DATA_TYPE
)
&&
domains
!=
null
)
{
boolean
hasError
=
false
;
for
(
TimelineDomain
domain
:
domains
.
getDomains
(
)
)
{
try
{
client
.
putDomain
(
domain
)
;
}
catch
(
Exception
e
)
{
}
if
(
type
.
equals
(
ENTITY_DATA_TYPE
)
)
{
TimelinePutResponse
response
=
client
.
putEntities
(
entities
.
getEntities
(
)
.
toArray
(
new
TimelineEntity
[
entities
.
getEntities
(
)
.
size
(
)
]
)
)
;
if
(
response
.
getErrors
(
)
.
size
(
)
==
0
)
{
LOG
.
info
(
)
;
}
else
{
for
(
TimelinePutResponse
.
TimelinePutError
error
:
response
.
getErrors
(
)
)
{
LOG
.
error
(
+
error
.
getEntityType
(
)
+
+
error
.
getEntityId
(
)
+
+
error
.
getErrorCode
(
)
)
;
}
}
}
else
if
(
type
.
equals
(
DOMAIN_DATA_TYPE
)
&&
domains
!=
null
)
{
boolean
hasError
=
false
;
for
(
TimelineDomain
domain
:
domains
.
getDomains
(
)
)
{
try
{
client
.
putDomain
(
domain
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
domain
.
getId
(
)
,
e
)
;
hasError
=
true
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
UserGroupInformation
realUgi
=
ugi
.
getRealUser
(
)
;
String
doAsUser
;
UserGroupInformation
authUgi
;
if
(
realUgi
!=
null
)
{
authUgi
=
realUgi
;
doAsUser
=
ugi
.
getShortUserName
(
)
;
}
else
{
authUgi
=
ugi
;
doAsUser
=
null
;
}
DelegationTokenAuthenticatedURL
.
Token
token
=
new
DelegationTokenAuthenticatedURL
.
Token
(
)
;
connector
=
new
TimelineConnector
(
false
,
authUgi
,
doAsUser
,
token
)
;
addIfService
(
connector
)
;
String
timelineReaderWebAppAddress
=
WebAppUtils
.
getTimelineReaderWebAppURLWithoutScheme
(
conf
)
;
baseUri
=
TimelineConnector
.
constructResURI
(
conf
,
timelineReaderWebAppAddress
,
RESOURCE_URI_STR_V2
)
;
@
VisibleForTesting
protected
ClientResponse
doGetUri
(
URI
base
,
String
path
,
MultivaluedMap
<
String
,
String
>
params
)
throws
IOException
{
ClientResponse
resp
=
connector
.
getClient
(
)
.
resource
(
base
)
.
path
(
path
)
.
queryParams
(
params
)
.
accept
(
MediaType
.
APPLICATION_JSON
)
.
get
(
ClientResponse
.
class
)
;
if
(
resp
==
null
||
resp
.
getStatusInfo
(
)
.
getStatusCode
(
)
!=
ClientResponse
.
Status
.
OK
.
getStatusCode
(
)
)
{
String
msg
=
+
(
(
resp
==
null
)
?
:
+
+
resp
.
getStatus
(
)
+
+
resp
.
getEntity
(
String
.
class
)
)
;
LOG
.
warn
(
+
TimelineDelegationTokenIdentifier
.
KIND_NAME
)
;
return
;
}
if
(
collectorAddr
==
null
||
collectorAddr
.
isEmpty
(
)
)
{
collectorAddr
=
timelineServiceAddress
;
}
String
service
=
delegationToken
.
getService
(
)
;
if
(
(
service
==
null
||
service
.
isEmpty
(
)
)
&&
(
collectorAddr
==
null
||
collectorAddr
.
isEmpty
(
)
)
)
{
LOG
.
warn
(
+
)
;
return
;
}
if
(
currentTimelineToken
!=
null
&&
currentTimelineToken
.
equals
(
delegationToken
)
)
{
return
;
}
currentTimelineToken
=
delegationToken
;
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
TimelineDelegationTokenIdentifier
>
timelineToken
=
new
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
TimelineDelegationTokenIdentifier
>
(
delegationToken
.
getIdentifier
(
)
.
array
(
)
,
delegationToken
.
getPassword
(
)
.
array
(
)
,
new
Text
(
delegationToken
.
getKind
(
)
)
,
service
==
null
?
new
Text
(
)
:
new
Text
(
service
)
)
;
InetSocketAddress
serviceAddr
=
(
collectorAddr
!=
null
&&
!
collectorAddr
.
isEmpty
(
)
)
?
NetUtils
.
createSocketAddr
(
collectorAddr
)
:
SecurityUtil
.
getTokenServiceAddr
(
timelineToken
)
;
SecurityUtil
.
setTokenService
(
timelineToken
,
serviceAddr
)
;
authUgi
.
addToken
(
timelineToken
)
;
try
{
resp
=
authUgi
.
doAs
(
new
PrivilegedExceptionAction
<
ClientResponse
>
(
)
{
@
Override
public
ClientResponse
run
(
)
throws
Exception
{
return
doPutObjects
(
base
,
path
,
params
,
obj
)
;
}
}
)
;
}
catch
(
UndeclaredThrowableException
ue
)
{
Throwable
cause
=
ue
.
getCause
(
)
;
if
(
cause
instanceof
IOException
)
{
throw
(
IOException
)
cause
;
}
else
{
throw
new
IOException
(
cause
)
;
}
}
catch
(
InterruptedException
ie
)
{
throw
(
IOException
)
new
InterruptedIOException
(
)
.
initCause
(
ie
)
;
}
if
(
resp
==
null
)
{
String
msg
=
;
throw
(
IOException
)
new
InterruptedIOException
(
)
.
initCause
(
ie
)
;
}
if
(
resp
==
null
)
{
String
msg
=
;
LOG
.
error
(
msg
)
;
throw
new
YarnException
(
msg
)
;
}
else
if
(
resp
.
getStatusInfo
(
)
.
getStatusCode
(
)
==
ClientResponse
.
Status
.
OK
.
getStatusCode
(
)
)
{
try
{
resp
.
close
(
)
;
}
catch
(
ClientHandlerException
che
)
{
LOG
.
warn
(
,
che
)
;
}
}
else
{
String
msg
=
;
try
{
String
stringType
=
resp
.
getEntity
(
String
.
class
)
;
msg
=
+
stringType
;
private
int
verifyRestEndPointAvailable
(
)
throws
YarnException
{
int
retries
=
pollTimelineServiceAddress
(
this
.
maxServiceRetries
)
;
if
(
timelineServiceAddress
==
null
)
{
String
errMessage
=
+
this
.
maxServiceRetries
+
+
;
try
{
resp
=
authUgi
.
doAs
(
new
PrivilegedExceptionAction
<
ClientResponse
>
(
)
{
@
Override
public
ClientResponse
run
(
)
throws
Exception
{
return
doPostingObject
(
obj
,
path
)
;
}
}
)
;
}
catch
(
UndeclaredThrowableException
e
)
{
Throwable
cause
=
e
.
getCause
(
)
;
if
(
cause
instanceof
IOException
)
{
throw
(
IOException
)
cause
;
}
else
{
throw
new
IOException
(
cause
)
;
}
}
catch
(
InterruptedException
ie
)
{
throw
(
IOException
)
new
InterruptedIOException
(
)
.
initCause
(
ie
)
;
}
if
(
resp
==
null
||
resp
.
getStatusInfo
(
)
.
getStatusCode
(
)
!=
ClientResponse
.
Status
.
OK
.
getStatusCode
(
)
)
{
String
msg
=
;
return
doPostingObject
(
obj
,
path
)
;
}
}
)
;
}
catch
(
UndeclaredThrowableException
e
)
{
Throwable
cause
=
e
.
getCause
(
)
;
if
(
cause
instanceof
IOException
)
{
throw
(
IOException
)
cause
;
}
else
{
throw
new
IOException
(
cause
)
;
}
}
catch
(
InterruptedException
ie
)
{
throw
(
IOException
)
new
InterruptedIOException
(
)
.
initCause
(
ie
)
;
}
if
(
resp
==
null
||
resp
.
getStatusInfo
(
)
.
getStatusCode
(
)
!=
ClientResponse
.
Status
.
OK
.
getStatusCode
(
)
)
{
String
msg
=
;
LOG
.
error
(
msg
)
;
if
(
resp
!=
null
)
{
msg
+=
+
resp
.
getStatus
(
)
;
@
Private
@
VisibleForTesting
public
ClientResponse
doPostingObject
(
Object
object
,
String
path
)
{
WebResource
webResource
=
client
.
resource
(
resURI
)
;
if
(
path
==
null
)
{
drained
=
eventQueue
.
isEmpty
(
)
;
if
(
blockNewEvents
)
{
synchronized
(
waitForDrained
)
{
if
(
drained
)
{
waitForDrained
.
notify
(
)
;
}
}
}
Event
event
;
try
{
event
=
eventQueue
.
take
(
)
;
}
catch
(
InterruptedException
ie
)
{
if
(
!
stopped
)
{
LOG
.
warn
(
,
ie
)
;
}
return
;
}
if
(
event
!=
null
)
{
dispatch
(
event
)
;
if
(
printTrigger
)
{
@
Override
protected
void
serviceStop
(
)
throws
Exception
{
if
(
drainEventsOnStop
)
{
blockNewEvents
=
true
;
LOG
.
info
(
)
;
long
endTime
=
System
.
currentTimeMillis
(
)
+
getConfig
(
)
.
getLong
(
YarnConfiguration
.
DISPATCHER_DRAIN_EVENTS_TIMEOUT
,
YarnConfiguration
.
DEFAULT_DISPATCHER_DRAIN_EVENTS_TIMEOUT
)
;
synchronized
(
waitForDrained
)
{
while
(
!
isDrained
(
)
&&
eventHandlingThread
!=
null
&&
eventHandlingThread
.
isAlive
(
)
&&
System
.
currentTimeMillis
(
)
<
endTime
)
{
waitForDrained
.
wait
(
100
)
;
@
SuppressWarnings
(
)
protected
void
dispatch
(
Event
event
)
{
@
SuppressWarnings
(
)
@
Override
public
void
register
(
Class
<
?
extends
Enum
>
eventType
,
EventHandler
handler
)
{
EventHandler
<
Event
>
registeredHandler
=
(
EventHandler
<
Event
>
)
eventDispatchers
.
get
(
eventType
)
;
@
Override
public
void
handle
(
T
event
)
{
try
{
int
qSize
=
eventQueue
.
size
(
)
;
if
(
qSize
!=
0
&&
qSize
%
1000
==
0
)
{
private
Server
createServer
(
Class
<
?
>
pbProtocol
,
InetSocketAddress
addr
,
Configuration
conf
,
SecretManager
<
?
extends
TokenIdentifier
>
secretManager
,
int
numHandlers
,
BlockingService
blockingService
,
String
portRangeConfig
)
throws
IOException
{
RPC
.
setProtocolEngine
(
conf
,
pbProtocol
,
ProtobufRpcEngine2
.
class
)
;
RPC
.
Server
server
=
new
RPC
.
Builder
(
conf
)
.
setProtocol
(
pbProtocol
)
.
setInstance
(
blockingService
)
.
setBindAddress
(
addr
.
getHostName
(
)
)
.
setPort
(
addr
.
getPort
(
)
)
.
setNumHandlers
(
numHandlers
)
.
setVerbose
(
false
)
.
setSecretManager
(
secretManager
)
.
setPortRangeConfig
(
portRangeConfig
)
.
build
(
)
;
@
Override
public
Object
getProxy
(
Class
protocol
,
InetSocketAddress
addr
,
Configuration
conf
)
{
@
Override
public
Server
getServer
(
Class
protocol
,
Object
instance
,
InetSocketAddress
addr
,
Configuration
conf
,
SecretManager
<
?
extends
TokenIdentifier
>
secretManager
,
int
numHandlers
,
String
portRangeConfig
)
{
Path
remoteRootLogDir
=
getRemoteRootLogDir
(
)
;
try
{
FsPermission
perms
=
remoteFS
.
getFileStatus
(
remoteRootLogDir
)
.
getPermission
(
)
;
if
(
!
perms
.
equals
(
TLDIR_PERMISSIONS
)
)
{
LOG
.
warn
(
+
remoteRootLogDir
+
+
+
TLDIR_PERMISSIONS
+
+
perms
+
+
)
;
}
}
catch
(
FileNotFoundException
e
)
{
remoteExists
=
false
;
}
catch
(
IOException
e
)
{
throw
new
YarnRuntimeException
(
+
remoteRootLogDir
+
,
e
)
;
}
Path
qualified
=
remoteRootLogDir
.
makeQualified
(
remoteFS
.
getUri
(
)
,
remoteFS
.
getWorkingDirectory
(
)
)
;
if
(
!
remoteExists
)
{
LOG
.
warn
(
+
remoteRootLogDir
+
)
;
try
{
remoteFS
.
mkdirs
(
qualified
,
new
FsPermission
(
TLDIR_PERMISSIONS
)
)
;
try
{
throw
new
YarnRuntimeException
(
+
remoteRootLogDir
+
,
e
)
;
}
Path
qualified
=
remoteRootLogDir
.
makeQualified
(
remoteFS
.
getUri
(
)
,
remoteFS
.
getWorkingDirectory
(
)
)
;
if
(
!
remoteExists
)
{
LOG
.
warn
(
+
remoteRootLogDir
+
)
;
try
{
remoteFS
.
mkdirs
(
qualified
,
new
FsPermission
(
TLDIR_PERMISSIONS
)
)
;
try
{
remoteFS
.
setPermission
(
qualified
,
new
FsPermission
(
TLDIR_PERMISSIONS
)
)
;
}
catch
(
UnsupportedOperationException
use
)
{
LOG
.
info
(
+
,
remoteFS
.
getScheme
(
)
)
;
fsSupportsChmod
=
false
;
}
UserGroupInformation
loginUser
=
UserGroupInformation
.
getLoginUser
(
)
;
String
primaryGroupName
=
conf
.
get
(
YarnConfiguration
.
NM_REMOTE_APP_LOG_DIR_GROUPNAME
)
;
if
(
primaryGroupName
==
null
||
primaryGroupName
.
isEmpty
(
)
)
{
try
{
try
{
remoteFS
.
mkdirs
(
qualified
,
new
FsPermission
(
TLDIR_PERMISSIONS
)
)
;
try
{
remoteFS
.
setPermission
(
qualified
,
new
FsPermission
(
TLDIR_PERMISSIONS
)
)
;
}
catch
(
UnsupportedOperationException
use
)
{
LOG
.
info
(
+
,
remoteFS
.
getScheme
(
)
)
;
fsSupportsChmod
=
false
;
}
UserGroupInformation
loginUser
=
UserGroupInformation
.
getLoginUser
(
)
;
String
primaryGroupName
=
conf
.
get
(
YarnConfiguration
.
NM_REMOTE_APP_LOG_DIR_GROUPNAME
)
;
if
(
primaryGroupName
==
null
||
primaryGroupName
.
isEmpty
(
)
)
{
try
{
primaryGroupName
=
loginUser
.
getPrimaryGroupName
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
+
+
)
;
}
}
else
{
fsSupportsChmod
=
false
;
}
UserGroupInformation
loginUser
=
UserGroupInformation
.
getLoginUser
(
)
;
String
primaryGroupName
=
conf
.
get
(
YarnConfiguration
.
NM_REMOTE_APP_LOG_DIR_GROUPNAME
)
;
if
(
primaryGroupName
==
null
||
primaryGroupName
.
isEmpty
(
)
)
{
try
{
primaryGroupName
=
loginUser
.
getPrimaryGroupName
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
+
+
)
;
}
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
primaryGroupName
)
;
}
}
if
(
primaryGroupName
!=
null
)
{
try
{
remoteFS
.
setOwner
(
qualified
,
loginUser
.
getShortUserName
(
)
,
primaryGroupName
)
;
}
catch
(
UnsupportedOperationException
use
)
{
try
{
FileSystem
remoteFS
=
getFileSystem
(
conf
)
;
Path
appDir
=
LogAggregationUtils
.
getRemoteAppLogDir
(
remoteRootLogDir
,
appId
,
user
,
remoteRootLogDirSuffix
)
;
Path
curDir
=
appDir
.
makeQualified
(
remoteFS
.
getUri
(
)
,
remoteFS
.
getWorkingDirectory
(
)
)
;
Path
rootLogDir
=
remoteRootLogDir
.
makeQualified
(
remoteFS
.
getUri
(
)
,
remoteFS
.
getWorkingDirectory
(
)
)
;
LinkedList
<
Path
>
pathsToCreate
=
new
LinkedList
<
>
(
)
;
while
(
!
curDir
.
equals
(
rootLogDir
)
)
{
if
(
!
checkExists
(
remoteFS
,
curDir
,
APP_DIR_PERMISSIONS
)
)
{
pathsToCreate
.
addFirst
(
curDir
)
;
curDir
=
curDir
.
getParent
(
)
;
}
else
{
break
;
}
}
for
(
Path
path
:
pathsToCreate
)
{
createDir
(
remoteFS
,
path
,
APP_DIR_PERMISSIONS
)
;
}
}
catch
(
IOException
e
)
{
if
(
status
.
size
(
)
>=
this
.
retentionSize
)
{
List
<
FileStatus
>
statusList
=
new
ArrayList
<
FileStatus
>
(
status
)
;
Collections
.
sort
(
statusList
,
new
Comparator
<
FileStatus
>
(
)
{
public
int
compare
(
FileStatus
s1
,
FileStatus
s2
)
{
return
s1
.
getModificationTime
(
)
<
s2
.
getModificationTime
(
)
?
-
1
:
s1
.
getModificationTime
(
)
>
s2
.
getModificationTime
(
)
?
1
:
0
;
}
}
)
;
for
(
int
i
=
0
;
i
<=
statusList
.
size
(
)
-
this
.
retentionSize
;
i
++
)
{
final
FileStatus
remove
=
statusList
.
get
(
i
)
;
try
{
userUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Object
>
(
)
{
@
Override
public
Object
run
(
)
throws
Exception
{
remoteFS
.
delete
(
remove
.
getPath
(
)
,
false
)
;
return
null
;
}
}
)
;
}
catch
(
Exception
e
)
{
Collections
.
sort
(
statusList
,
new
Comparator
<
FileStatus
>
(
)
{
public
int
compare
(
FileStatus
s1
,
FileStatus
s2
)
{
return
s1
.
getModificationTime
(
)
<
s2
.
getModificationTime
(
)
?
-
1
:
s1
.
getModificationTime
(
)
>
s2
.
getModificationTime
(
)
?
1
:
0
;
}
}
)
;
for
(
int
i
=
0
;
i
<=
statusList
.
size
(
)
-
this
.
retentionSize
;
i
++
)
{
final
FileStatus
remove
=
statusList
.
get
(
i
)
;
try
{
userUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Object
>
(
)
{
@
Override
public
Object
run
(
)
throws
Exception
{
remoteFS
.
delete
(
remove
.
getPath
(
)
,
false
)
;
return
null
;
}
}
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
remove
.
getPath
(
)
,
e
)
;
}
}
}
}
catch
(
Exception
e
)
{
return
;
}
ApplicationId
appId
=
params
.
getAppId
(
)
;
ContainerId
containerId
=
params
.
getContainerId
(
)
;
NodeId
nodeId
=
params
.
getNodeId
(
)
;
String
appOwner
=
params
.
getAppOwner
(
)
;
String
logEntity
=
params
.
getLogEntity
(
)
;
long
start
=
params
.
getStartIndex
(
)
;
long
end
=
params
.
getEndIndex
(
)
;
long
startTime
=
params
.
getStartTime
(
)
;
long
endTime
=
params
.
getEndTime
(
)
;
List
<
FileStatus
>
nodeFiles
=
null
;
try
{
nodeFiles
=
LogAggregationUtils
.
getRemoteNodeFileList
(
conf
,
appId
,
appOwner
,
this
.
fileController
.
getRemoteRootLogDir
(
)
,
this
.
fileController
.
getRemoteRootLogDirSuffix
(
)
)
;
}
catch
(
Exception
ex
)
{
html
.
h1
(
+
containerId
.
toString
(
)
)
;
String
logEntity
=
params
.
getLogEntity
(
)
;
long
start
=
params
.
getStartIndex
(
)
;
long
end
=
params
.
getEndIndex
(
)
;
long
startTime
=
params
.
getStartTime
(
)
;
long
endTime
=
params
.
getEndTime
(
)
;
List
<
FileStatus
>
nodeFiles
=
null
;
try
{
nodeFiles
=
LogAggregationUtils
.
getRemoteNodeFileList
(
conf
,
appId
,
appOwner
,
this
.
fileController
.
getRemoteRootLogDir
(
)
,
this
.
fileController
.
getRemoteRootLogDirSuffix
(
)
)
;
}
catch
(
Exception
ex
)
{
html
.
h1
(
+
containerId
.
toString
(
)
)
;
LOG
.
error
(
ex
.
getMessage
(
)
)
;
return
;
}
Map
<
String
,
Long
>
checkSumFiles
;
try
{
checkSumFiles
=
fileController
.
parseCheckSumFiles
(
nodeFiles
)
;
nodeFiles
=
LogAggregationUtils
.
getRemoteNodeFileList
(
conf
,
appId
,
appOwner
,
this
.
fileController
.
getRemoteRootLogDir
(
)
,
this
.
fileController
.
getRemoteRootLogDirSuffix
(
)
)
;
}
catch
(
Exception
ex
)
{
html
.
h1
(
+
containerId
.
toString
(
)
)
;
LOG
.
error
(
ex
.
getMessage
(
)
)
;
return
;
}
Map
<
String
,
Long
>
checkSumFiles
;
try
{
checkSumFiles
=
fileController
.
parseCheckSumFiles
(
nodeFiles
)
;
}
catch
(
IOException
ex
)
{
LOG
.
error
(
+
logEntity
,
ex
)
;
html
.
h1
(
+
logEntity
)
;
return
;
}
List
<
FileStatus
>
fileToRead
;
try
{
fileToRead
=
fileController
.
getNodeLogFileToRead
(
nodeFiles
,
nodeId
.
toString
(
)
,
appId
)
;
if
(
!
checkAcls
(
conf
,
appId
,
user
,
appAcls
,
remoteUser
)
)
{
html
.
h1
(
)
.
__
(
+
remoteUser
+
+
logEntity
+
+
thisNodeFile
.
getPath
(
)
.
getName
(
)
+
)
.
__
(
)
;
LOG
.
error
(
+
remoteUser
+
+
logEntity
)
;
continue
;
}
String
compressAlgo
=
indexedLogsMeta
.
getCompressName
(
)
;
List
<
IndexedFileLogMeta
>
candidates
=
new
ArrayList
<
>
(
)
;
for
(
IndexedPerAggregationLogMeta
logMeta
:
indexedLogsMeta
.
getLogMetas
(
)
)
{
for
(
Entry
<
String
,
List
<
IndexedFileLogMeta
>>
meta
:
logMeta
.
getLogMetas
(
)
.
entrySet
(
)
)
{
for
(
IndexedFileLogMeta
log
:
meta
.
getValue
(
)
)
{
if
(
!
log
.
getContainerId
(
)
.
equals
(
containerId
.
toString
(
)
)
)
{
continue
;
}
if
(
desiredLogType
!=
null
&&
!
desiredLogType
.
isEmpty
(
)
&&
!
desiredLogType
.
equals
(
log
.
getFileName
(
)
)
)
{
continue
;
}
candidates
.
add
(
log
)
;
}
}
}
if
(
candidates
.
isEmpty
(
)
)
{
if
(
candidate
.
getLastModifiedTime
(
)
<
startTime
||
candidate
.
getLastModifiedTime
(
)
>
endTime
)
{
continue
;
}
byte
[
]
cbuf
=
new
byte
[
bufferSize
]
;
InputStream
in
=
null
;
try
{
in
=
compressName
.
createDecompressionStream
(
new
BoundedRangeFileInputStream
(
fsin
,
candidate
.
getStartIndex
(
)
,
candidate
.
getFileCompressedSize
(
)
)
,
decompressor
,
LogAggregationIndexedFileController
.
getFSInputBufferSize
(
conf
)
)
;
long
logLength
=
candidate
.
getFileSize
(
)
;
html
.
pre
(
)
.
__
(
)
.
__
(
)
;
html
.
p
(
)
.
__
(
+
candidate
.
getFileName
(
)
)
.
__
(
)
;
html
.
p
(
)
.
__
(
+
Times
.
format
(
candidate
.
getLastModifiedTime
(
)
)
)
.
__
(
)
;
html
.
p
(
)
.
__
(
+
Long
.
toString
(
logLength
)
)
.
__
(
)
;
long
[
]
range
=
checkParseRange
(
html
,
start
,
end
,
startTime
,
endTime
,
logLength
,
candidate
.
getFileName
(
)
)
;
processContainerLog
(
html
,
range
,
in
,
bufferSize
,
cbuf
)
;
foundLog
=
true
;
}
catch
(
Exception
ex
)
{
try
{
fsDataIStream
=
fileContext
.
open
(
remoteLogPath
)
;
if
(
end
==
0
)
{
return
null
;
}
long
fileLength
=
end
<
0
?
fileContext
.
getFileStatus
(
remoteLogPath
)
.
getLen
(
)
:
end
;
fsDataIStream
.
seek
(
fileLength
-
Integer
.
SIZE
/
Byte
.
SIZE
-
UUID_LENGTH
)
;
int
offset
=
fsDataIStream
.
readInt
(
)
;
if
(
offset
>
64
*
1024
*
1024
)
{
LOG
.
warn
(
+
remoteLogPath
+
+
offset
)
;
}
byte
[
]
uuidRead
=
new
byte
[
UUID_LENGTH
]
;
int
uuidReadLen
=
fsDataIStream
.
read
(
uuidRead
)
;
if
(
this
.
uuid
==
null
)
{
this
.
uuid
=
createUUID
(
appId
)
;
}
if
(
uuidReadLen
!=
UUID_LENGTH
||
!
Arrays
.
equals
(
this
.
uuid
,
uuidRead
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
String
logErrorMessage
(
File
logFile
,
Exception
e
)
{
String
message
=
+
logFile
.
getAbsolutePath
(
)
+
+
e
.
getMessage
(
)
;
record
.
increcleanupOldLogTimes
(
)
;
}
closeWriter
(
)
;
final
Path
renamedPath
=
record
.
getRollingMonitorInterval
(
)
<=
0
?
record
.
getRemoteNodeLogFileForApp
(
)
:
new
Path
(
record
.
getRemoteNodeLogFileForApp
(
)
.
getParent
(
)
,
record
.
getRemoteNodeLogFileForApp
(
)
.
getName
(
)
+
+
record
.
getLogUploadTimeStamp
(
)
)
;
final
boolean
rename
=
record
.
isUploadedLogsInThisCycle
(
)
;
try
{
record
.
getUserUgi
(
)
.
doAs
(
new
PrivilegedExceptionAction
<
Object
>
(
)
{
@
Override
public
Object
run
(
)
throws
Exception
{
FileSystem
remoteFS
=
record
.
getRemoteNodeLogFileForApp
(
)
.
getFileSystem
(
conf
)
;
if
(
rename
)
{
remoteFS
.
rename
(
record
.
getRemoteNodeTmpLogFileForApp
(
)
,
renamedPath
)
;
}
else
{
remoteFS
.
delete
(
record
.
getRemoteNodeTmpLogFileForApp
(
)
,
false
)
;
}
return
null
;
}
}
)
;
}
catch
(
Exception
e
)
{
nodeFiles
=
HarFs
.
get
(
p
.
toUri
(
)
,
conf
)
.
listStatusIterator
(
p
)
;
continue
;
}
if
(
!
thisNodeFile
.
getPath
(
)
.
getName
(
)
.
contains
(
LogAggregationUtils
.
getNodeString
(
nodeId
)
)
||
thisNodeFile
.
getPath
(
)
.
getName
(
)
.
endsWith
(
LogAggregationUtils
.
TMP_FILE_SUFFIX
)
)
{
continue
;
}
long
logUploadedTime
=
thisNodeFile
.
getModificationTime
(
)
;
if
(
logUploadedTime
<
startTime
||
logUploadedTime
>
endTime
)
{
continue
;
}
reader
=
new
AggregatedLogFormat
.
LogReader
(
conf
,
thisNodeFile
.
getPath
(
)
)
;
String
owner
=
null
;
Map
<
ApplicationAccessType
,
String
>
appAcls
=
null
;
try
{
owner
=
reader
.
getApplicationOwner
(
)
;
appAcls
=
reader
.
getApplicationAcls
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
logEntity
,
e
)
;
if
(
logUploadedTime
<
startTime
||
logUploadedTime
>
endTime
)
{
continue
;
}
reader
=
new
AggregatedLogFormat
.
LogReader
(
conf
,
thisNodeFile
.
getPath
(
)
)
;
String
owner
=
null
;
Map
<
ApplicationAccessType
,
String
>
appAcls
=
null
;
try
{
owner
=
reader
.
getApplicationOwner
(
)
;
appAcls
=
reader
.
getApplicationAcls
(
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
logEntity
,
e
)
;
continue
;
}
String
remoteUser
=
request
(
)
.
getRemoteUser
(
)
;
if
(
!
checkAcls
(
conf
,
appId
,
owner
,
appAcls
,
remoteUser
)
)
{
html
.
h1
(
)
.
__
(
+
remoteUser
+
+
logEntity
+
+
thisNodeFile
.
getPath
(
)
.
getName
(
)
+
)
.
__
(
)
;
LOG
.
error
(
+
remoteUser
+
+
logEntity
)
;
continue
;
}
String
remoteUser
=
request
(
)
.
getRemoteUser
(
)
;
if
(
!
checkAcls
(
conf
,
appId
,
owner
,
appAcls
,
remoteUser
)
)
{
html
.
h1
(
)
.
__
(
+
remoteUser
+
+
logEntity
+
+
thisNodeFile
.
getPath
(
)
.
getName
(
)
+
)
.
__
(
)
;
LOG
.
error
(
+
remoteUser
+
+
logEntity
)
;
continue
;
}
AggregatedLogFormat
.
ContainerLogsReader
logReader
=
reader
.
getContainerLogsReader
(
containerId
)
;
if
(
logReader
==
null
)
{
continue
;
}
foundLog
=
readContainerLogs
(
html
,
logReader
,
start
,
end
,
desiredLogType
,
logUploadedTime
,
startTime
,
endTime
)
;
}
catch
(
IOException
ex
)
{
LOG
.
error
(
+
logEntity
,
ex
)
;
continue
;
}
finally
{
if
(
reader
!=
null
)
{
if
(
null
==
addedLabelsToNode
||
addedLabelsToNode
.
isEmpty
(
)
)
{
return
;
}
Set
<
String
>
knownLabels
=
labelCollections
.
keySet
(
)
;
for
(
Entry
<
NodeId
,
Set
<
String
>>
entry
:
addedLabelsToNode
.
entrySet
(
)
)
{
NodeId
nodeId
=
entry
.
getKey
(
)
;
Set
<
String
>
labels
=
entry
.
getValue
(
)
;
if
(
!
knownLabels
.
containsAll
(
labels
)
)
{
String
msg
=
+
+
+
StringUtils
.
join
(
labels
,
)
+
;
LOG
.
error
(
msg
)
;
throw
new
IOException
(
msg
)
;
}
if
(
!
labels
.
isEmpty
(
)
)
{
Set
<
String
>
newLabels
=
new
HashSet
<
String
>
(
getLabelsByNode
(
nodeId
)
)
;
newLabels
.
addAll
(
labels
)
;
if
(
newLabels
.
size
(
)
>
1
)
{
String
msg
=
String
.
format
(
+
+
,
newLabels
.
size
(
)
,
nodeId
.
getHost
(
)
)
;
protected
void
checkRemoveLabelsFromNode
(
Map
<
NodeId
,
Set
<
String
>>
removeLabelsFromNode
)
throws
IOException
{
Set
<
String
>
knownLabels
=
labelCollections
.
keySet
(
)
;
for
(
Entry
<
NodeId
,
Set
<
String
>>
entry
:
removeLabelsFromNode
.
entrySet
(
)
)
{
NodeId
nodeId
=
entry
.
getKey
(
)
;
Set
<
String
>
labels
=
entry
.
getValue
(
)
;
if
(
!
knownLabels
.
containsAll
(
labels
)
)
{
String
msg
=
+
+
+
StringUtils
.
join
(
labels
,
)
+
;
throw
new
IOException
(
msg
)
;
}
Set
<
String
>
originalLabels
=
null
;
boolean
nodeExisted
=
false
;
if
(
WILDCARD_PORT
!=
nodeId
.
getPort
(
)
)
{
Node
nm
=
getNMInNodeSet
(
nodeId
)
;
if
(
nm
!=
null
)
{
originalLabels
=
nm
.
labels
;
nodeExisted
=
true
;
}
}
else
{
Host
host
=
nodeCollections
.
get
(
nodeId
.
getHost
(
)
)
;
if
(
null
!=
host
)
{
originalLabels
=
host
.
labels
;
nodeExisted
=
true
;
}
}
if
(
!
nodeExisted
)
{
String
msg
=
+
nodeId
+
;
if
(
nm
!=
null
)
{
originalLabels
=
nm
.
labels
;
nodeExisted
=
true
;
}
}
else
{
Host
host
=
nodeCollections
.
get
(
nodeId
.
getHost
(
)
)
;
if
(
null
!=
host
)
{
originalLabels
=
host
.
labels
;
nodeExisted
=
true
;
}
}
if
(
!
nodeExisted
)
{
String
msg
=
+
nodeId
+
;
LOG
.
error
(
msg
)
;
throw
new
IOException
(
msg
)
;
}
if
(
labels
.
isEmpty
(
)
)
{
continue
;
}
if
(
originalLabels
==
null
||
!
originalLabels
.
containsAll
(
labels
)
)
{
protected
void
initStore
(
Configuration
conf
,
Path
fsStorePath
,
StoreSchema
schma
,
M
mgr
)
throws
IOException
{
this
.
schema
=
schma
;
this
.
fsWorkingPath
=
fsStorePath
;
this
.
manager
=
mgr
;
initFileSystem
(
conf
)
;
fs
.
mkdirs
(
fsWorkingPath
)
;
loadFromMirror
(
mirrorPath
,
oldMirrorPath
)
;
editLogPath
=
new
Path
(
fsWorkingPath
,
schema
.
editLogName
)
;
loadManagerFromEditLog
(
editLogPath
)
;
Path
writingMirrorPath
=
new
Path
(
fsWorkingPath
,
schema
.
mirrorName
+
)
;
try
(
FSDataOutputStream
os
=
fs
.
create
(
writingMirrorPath
,
true
)
)
{
StoreOp
op
=
FSStoreOpHandler
.
getMirrorOp
(
storeType
)
;
op
.
write
(
os
,
manager
)
;
}
if
(
fs
.
exists
(
mirrorPath
)
)
{
fs
.
delete
(
oldMirrorPath
,
false
)
;
fs
.
rename
(
mirrorPath
,
oldMirrorPath
)
;
}
fs
.
rename
(
writingMirrorPath
,
mirrorPath
)
;
fs
.
delete
(
writingMirrorPath
,
false
)
;
fs
.
delete
(
oldMirrorPath
,
false
)
;
editlogOs
=
fs
.
create
(
editLogPath
,
true
)
;
editlogOs
.
close
(
)
;
editLogPath
=
new
Path
(
fsWorkingPath
,
schema
.
editLogName
)
;
loadManagerFromEditLog
(
editLogPath
)
;
Path
writingMirrorPath
=
new
Path
(
fsWorkingPath
,
schema
.
mirrorName
+
)
;
try
(
FSDataOutputStream
os
=
fs
.
create
(
writingMirrorPath
,
true
)
)
{
StoreOp
op
=
FSStoreOpHandler
.
getMirrorOp
(
storeType
)
;
op
.
write
(
os
,
manager
)
;
}
if
(
fs
.
exists
(
mirrorPath
)
)
{
fs
.
delete
(
oldMirrorPath
,
false
)
;
fs
.
rename
(
mirrorPath
,
oldMirrorPath
)
;
}
fs
.
rename
(
writingMirrorPath
,
mirrorPath
)
;
fs
.
delete
(
writingMirrorPath
,
false
)
;
fs
.
delete
(
oldMirrorPath
,
false
)
;
editlogOs
=
fs
.
create
(
editLogPath
,
true
)
;
editlogOs
.
close
(
)
;
LOG
.
info
(
+
mirrorPath
.
toString
(
)
)
;
public
static
YarnAuthorizationProvider
getInstance
(
Configuration
conf
)
{
synchronized
(
YarnAuthorizationProvider
.
class
)
{
if
(
authorizer
==
null
)
{
Class
<
?
>
authorizerClass
=
conf
.
getClass
(
YarnConfiguration
.
YARN_AUTHORIZATION_PROVIDER
,
ConfiguredYarnAuthorizer
.
class
)
;
authorizer
=
(
YarnAuthorizationProvider
)
ReflectionUtils
.
newInstance
(
authorizerClass
,
conf
)
;
authorizer
.
init
(
conf
)
;
public
boolean
checkAccess
(
UserGroupInformation
callerUGI
,
ApplicationAccessType
applicationAccessType
,
String
applicationOwner
,
ApplicationId
applicationId
)
{
FileAppender
fApp
;
File
file
=
new
File
(
System
.
getProperty
(
)
,
targetFilename
)
;
try
{
fApp
=
new
FileAppender
(
layout
,
file
.
getAbsolutePath
(
)
,
false
)
;
}
catch
(
IOException
ie
)
{
LOG
.
warn
(
+
file
.
getAbsolutePath
(
)
,
ie
)
;
throw
ie
;
}
fApp
.
setName
(
AdHocLogDumper
.
AD_HOC_DUMPER_APPENDER
)
;
fApp
.
setThreshold
(
targetLevel
)
;
for
(
Enumeration
appenders
=
Logger
.
getRootLogger
(
)
.
getAllAppenders
(
)
;
appenders
.
hasMoreElements
(
)
;
)
{
Object
obj
=
appenders
.
nextElement
(
)
;
if
(
obj
instanceof
AppenderSkeleton
)
{
AppenderSkeleton
appender
=
(
AppenderSkeleton
)
obj
;
appenderLevels
.
put
(
appender
.
getName
(
)
,
appender
.
getThreshold
(
)
)
;
appender
.
setThreshold
(
currentEffectiveLevel
)
;
}
}
if
(
contents
==
null
)
{
throw
new
IOException
(
+
configFile
)
;
}
ObjectMapper
mapper
=
new
ObjectMapper
(
)
;
JsonFactory
factory
=
mapper
.
getJsonFactory
(
)
;
JsonParser
parser
=
factory
.
createJsonParser
(
contents
)
;
JsonNode
rootNode
=
mapper
.
readTree
(
parser
)
;
Credentials
credentials
=
new
Credentials
(
)
;
if
(
rootNode
.
has
(
CONFIG_AUTHS_KEY
)
)
{
Iterator
<
String
>
iter
=
rootNode
.
get
(
CONFIG_AUTHS_KEY
)
.
getFieldNames
(
)
;
for
(
;
iter
.
hasNext
(
)
;
)
{
String
registryUrl
=
iter
.
next
(
)
;
String
registryCred
=
rootNode
.
get
(
CONFIG_AUTHS_KEY
)
.
get
(
registryUrl
)
.
get
(
CONFIG_AUTH_KEY
)
.
asText
(
)
;
TokenIdentifier
tokenId
=
new
DockerCredentialTokenIdentifier
(
registryUrl
,
applicationId
)
;
Token
<
DockerCredentialTokenIdentifier
>
token
=
new
Token
<
>
(
tokenId
.
getBytes
(
)
,
registryCred
.
getBytes
(
Charset
.
forName
(
)
)
,
tokenId
.
getKind
(
)
,
new
Text
(
registryUrl
)
)
;
credentials
.
addToken
(
new
Text
(
registryUrl
+
+
applicationId
)
,
token
)
;
public
static
Credentials
getCredentialsFromTokensByteBuffer
(
ByteBuffer
tokens
)
throws
IOException
{
Credentials
credentials
=
new
Credentials
(
)
;
DataInputByteBuffer
dibb
=
new
DataInputByteBuffer
(
)
;
tokens
.
rewind
(
)
;
dibb
.
reset
(
tokens
)
;
credentials
.
readTokenStorageStream
(
dibb
)
;
tokens
.
rewind
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
for
(
Token
token
:
credentials
.
getAllTokens
(
)
)
{
public
static
boolean
writeDockerCredentialsToPath
(
File
outConfigFile
,
Credentials
credentials
)
throws
IOException
{
boolean
foundDockerCred
=
false
;
if
(
credentials
.
numberOfTokens
(
)
>
0
)
{
ObjectMapper
mapper
=
new
ObjectMapper
(
)
;
ObjectNode
rootNode
=
mapper
.
createObjectNode
(
)
;
ObjectNode
registryUrlNode
=
mapper
.
createObjectNode
(
)
;
for
(
Token
<
?
extends
TokenIdentifier
>
tk
:
credentials
.
getAllTokens
(
)
)
{
if
(
tk
.
getKind
(
)
.
equals
(
DockerCredentialTokenIdentifier
.
KIND
)
)
{
foundDockerCred
=
true
;
DockerCredentialTokenIdentifier
ti
=
(
DockerCredentialTokenIdentifier
)
tk
.
decodeIdentifier
(
)
;
ObjectNode
registryCredNode
=
mapper
.
createObjectNode
(
)
;
registryUrlNode
.
put
(
ti
.
getRegistryUrl
(
)
,
registryCredNode
)
;
registryCredNode
.
put
(
CONFIG_AUTH_KEY
,
new
String
(
tk
.
getPassword
(
)
,
Charset
.
forName
(
)
)
)
;
throw
new
IOException
(
,
e
)
;
}
LOG
.
debug
(
,
sCopy
,
resource
.
getType
(
)
,
resource
.
getPattern
(
)
)
;
final
Path
destinationTmp
=
new
Path
(
destDirPath
+
)
;
createDir
(
destinationTmp
,
cachePerms
)
;
Path
dFinal
=
files
.
makeQualified
(
new
Path
(
destinationTmp
,
sCopy
.
getName
(
)
)
)
;
try
{
if
(
userUgi
==
null
)
{
verifyAndCopy
(
dFinal
)
;
}
else
{
userUgi
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
verifyAndCopy
(
dFinal
)
;
return
null
;
}
}
)
;
}
changePermissions
(
dFinal
.
getFileSystem
(
conf
)
,
dFinal
)
;
@
Override
public
float
getCpuUsagePercent
(
)
{
BigInteger
processTotalJiffies
=
getTotalProcessJiffies
(
)
;
in
=
new
BufferedReader
(
fReader
)
;
ProcessSmapMemoryInfo
memoryMappingInfo
=
null
;
List
<
String
>
lines
=
IOUtils
.
readLines
(
in
)
;
for
(
String
line
:
lines
)
{
line
=
line
.
trim
(
)
;
try
{
Matcher
address
=
ADDRESS_PATTERN
.
matcher
(
line
)
;
if
(
address
.
find
(
)
)
{
memoryMappingInfo
=
new
ProcessSmapMemoryInfo
(
line
)
;
memoryMappingInfo
.
setPermission
(
address
.
group
(
4
)
)
;
pInfo
.
getMemoryInfoList
(
)
.
add
(
memoryMappingInfo
)
;
continue
;
}
Matcher
memInfo
=
MEM_INFO_PATTERN
.
matcher
(
line
)
;
if
(
memInfo
.
find
(
)
)
{
String
key
=
memInfo
.
group
(
1
)
.
trim
(
)
;
pInfo
.
getMemoryInfoList
(
)
.
add
(
memoryMappingInfo
)
;
continue
;
}
Matcher
memInfo
=
MEM_INFO_PATTERN
.
matcher
(
line
)
;
if
(
memInfo
.
find
(
)
)
{
String
key
=
memInfo
.
group
(
1
)
.
trim
(
)
;
String
value
=
memInfo
.
group
(
2
)
.
replace
(
KB
,
)
.
trim
(
)
;
LOG
.
debug
(
,
key
,
value
)
;
if
(
memoryMappingInfo
!=
null
)
{
memoryMappingInfo
.
setMemInfo
(
key
,
value
)
;
}
}
}
catch
(
Throwable
t
)
{
LOG
.
warn
(
+
line
+
+
t
.
getMessage
(
)
)
;
}
}
}
catch
(
FileNotFoundException
f
)
{
LOG
.
error
(
f
.
toString
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
e
.
toString
(
)
)
;
String
[
]
processesStr
=
processesInfoStr
.
split
(
)
;
Map
<
String
,
ProcessInfo
>
allProcs
=
new
HashMap
<
String
,
ProcessInfo
>
(
)
;
final
int
procInfoSplitCount
=
4
;
for
(
String
processStr
:
processesStr
)
{
if
(
processStr
!=
null
)
{
String
[
]
procInfo
=
processStr
.
split
(
)
;
if
(
procInfo
.
length
==
procInfoSplitCount
)
{
try
{
ProcessInfo
pInfo
=
new
ProcessInfo
(
)
;
pInfo
.
pid
=
procInfo
[
0
]
;
pInfo
.
vmem
=
Long
.
parseLong
(
procInfo
[
1
]
)
;
pInfo
.
workingSet
=
Long
.
parseLong
(
procInfo
[
2
]
)
;
pInfo
.
cpuTimeMs
=
Long
.
parseLong
(
procInfo
[
3
]
)
;
allProcs
.
put
(
pInfo
.
pid
,
pInfo
)
;
}
catch
(
NumberFormatException
nfe
)
{
final
int
procInfoSplitCount
=
4
;
for
(
String
processStr
:
processesStr
)
{
if
(
processStr
!=
null
)
{
String
[
]
procInfo
=
processStr
.
split
(
)
;
if
(
procInfo
.
length
==
procInfoSplitCount
)
{
try
{
ProcessInfo
pInfo
=
new
ProcessInfo
(
)
;
pInfo
.
pid
=
procInfo
[
0
]
;
pInfo
.
vmem
=
Long
.
parseLong
(
procInfo
[
1
]
)
;
pInfo
.
workingSet
=
Long
.
parseLong
(
procInfo
[
2
]
)
;
pInfo
.
cpuTimeMs
=
Long
.
parseLong
(
procInfo
[
3
]
)
;
allProcs
.
put
(
pInfo
.
pid
,
pInfo
)
;
}
catch
(
NumberFormatException
nfe
)
{
LOG
.
debug
(
,
nfe
)
;
}
}
else
{
@
Override
public
Resource
normalize
(
Resource
r
,
Resource
minimumResource
,
Resource
maximumResource
,
Resource
stepFactor
)
{
if
(
stepFactor
.
getMemorySize
(
)
==
0
)
{
double
[
]
rhsShares
=
new
double
[
maxLength
]
;
double
diff
;
try
{
if
(
singleType
)
{
double
[
]
max
=
new
double
[
2
]
;
calculateShares
(
clusterRes
,
lhs
,
rhs
,
lhsShares
,
rhsShares
,
max
)
;
diff
=
max
[
0
]
-
max
[
1
]
;
}
else
if
(
maxLength
==
2
)
{
diff
=
calculateSharesForTwoMandatoryResources
(
clusterRes
,
lhs
,
rhs
,
lhsShares
,
rhsShares
)
;
}
else
{
calculateShares
(
clusterRes
,
lhs
,
rhs
,
lhsShares
,
rhsShares
)
;
Arrays
.
sort
(
lhsShares
)
;
Arrays
.
sort
(
rhsShares
)
;
diff
=
compareShares
(
lhsShares
,
rhsShares
)
;
}
}
catch
(
ArrayIndexOutOfBoundsException
ex
)
{
protected
void
renderJSON
(
Object
object
)
{
protected
void
renderText
(
String
s
)
{
}
}
rc
.
prefix
=
webApp
.
name
(
)
;
Router
.
Dest
dest
=
null
;
try
{
dest
=
router
.
resolve
(
method
,
pathInfo
)
;
}
catch
(
WebAppException
e
)
{
rc
.
error
=
e
;
if
(
!
e
.
getMessage
(
)
.
contains
(
)
)
{
rc
.
setStatus
(
res
.
SC_INTERNAL_SERVER_ERROR
)
;
render
(
ErrorPage
.
class
)
;
return
;
}
}
if
(
dest
==
null
)
{
rc
.
setStatus
(
res
.
SC_NOT_FOUND
)
;
render
(
ErrorPage
.
class
)
;
return
;
}
rc
.
devMode
=
devMode
;
public
static
void
removeCookie
(
HttpServletResponse
res
,
String
name
,
String
path
)
{
private
Dest
lookupRoute
(
WebApp
.
HTTP
method
,
String
path
)
{
String
key
=
path
;
do
{
Dest
dest
=
routes
.
get
(
key
)
;
if
(
dest
!=
null
&&
methodAllowed
(
method
,
dest
)
)
{
if
(
(
Object
)
key
==
path
)
{
if
(
dest
!=
null
&&
methodAllowed
(
method
,
dest
)
)
{
if
(
(
Object
)
key
==
path
)
{
LOG
.
debug
(
,
key
,
dest
.
action
)
;
return
dest
;
}
else
if
(
isGoodMatch
(
dest
,
path
)
)
{
LOG
.
debug
(
,
key
,
dest
.
action
)
;
return
dest
;
}
return
resolveAction
(
method
,
dest
,
path
)
;
}
Map
.
Entry
<
String
,
Dest
>
lower
=
routes
.
lowerEntry
(
key
)
;
if
(
lower
==
null
)
{
return
null
;
}
dest
=
lower
.
getValue
(
)
;
if
(
prefixMatches
(
dest
,
path
)
)
{
if
(
methodAllowed
(
method
,
dest
)
)
{
if
(
isGoodMatch
(
dest
,
path
)
)
{
@
SuppressWarnings
(
)
private
<
T
>
Class
<
?
extends
T
>
load
(
Class
<
T
>
cls
,
String
className
)
{
@
SuppressWarnings
(
)
private
<
T
>
Class
<
?
extends
T
>
load
(
Class
<
T
>
cls
,
String
className
)
{
LOG
.
debug
(
,
className
)
;
try
{
Class
<
?
>
found
=
Class
.
forName
(
className
)
;
if
(
cls
.
isAssignableFrom
(
found
)
)
{
LOG
.
info
(
,
new
Object
[
]
{
outputName
,
specClass
,
implClass
}
)
;
out
=
new
PrintWriter
(
outputName
+
,
)
;
hamlet
=
basename
(
outputName
)
;
String
pkg
=
pkgName
(
outputPkg
,
implClass
.
getPackage
(
)
.
getName
(
)
)
;
puts
(
0
,
,
,
pkg
,
,
,
,
,
,
implClass
.
getName
(
)
,
,
)
;
String
implClassName
=
implClass
.
getSimpleName
(
)
;
if
(
!
implClass
.
getPackage
(
)
.
getName
(
)
.
equals
(
pkg
)
)
{
puts
(
0
,
,
implClass
.
getName
(
)
,
';'
)
;
}
puts
(
0
,
,
,
hamlet
,
,
implClassName
,
,
specClass
.
getSimpleName
(
)
,
,
,
hamlet
,
,
,
,
,
,
,
,
,
,
,
,
)
;
initLut
(
specClass
)
;
genImpl
(
specClass
,
implClassName
,
1
)
;
LOG
.
info
(
,
hamlet
)
;
genMethods
(
hamlet
,
top
,
1
)
;
puts
(
0
,
)
;
out
.
close
(
)
;
LOG
.
info
(
,
new
Object
[
]
{
outputName
,
specClass
,
implClass
}
)
;
out
=
new
PrintWriter
(
outputName
+
,
)
;
hamlet
=
basename
(
outputName
)
;
String
pkg
=
pkgName
(
outputPkg
,
implClass
.
getPackage
(
)
.
getName
(
)
)
;
puts
(
0
,
,
,
pkg
,
,
,
,
,
,
implClass
.
getName
(
)
,
,
)
;
String
implClassName
=
implClass
.
getSimpleName
(
)
;
if
(
!
implClass
.
getPackage
(
)
.
getName
(
)
.
equals
(
pkg
)
)
{
puts
(
0
,
,
implClass
.
getName
(
)
,
';'
)
;
}
puts
(
0
,
,
,
hamlet
,
,
implClassName
,
,
specClass
.
getSimpleName
(
)
,
,
,
hamlet
,
,
,
,
,
,
,
,
,
,
,
,
)
;
initLut
(
specClass
)
;
genImpl
(
specClass
,
implClassName
,
1
)
;
LOG
.
info
(
,
hamlet
)
;
genMethods
(
hamlet
,
top
,
1
)
;
puts
(
0
,
)
;
out
.
close
(
)
;
@
Override
public
void
render
(
)
{
int
nestLevel
=
context
(
)
.
nestLevel
(
)
;
ContainerLaunchContext
containerLaunchContext
=
recordFactory
.
newRecordInstance
(
ContainerLaunchContext
.
class
)
;
ApplicationId
applicationId
=
ApplicationId
.
newInstance
(
0
,
0
)
;
ApplicationAttemptId
applicationAttemptId
=
ApplicationAttemptId
.
newInstance
(
applicationId
,
0
)
;
ContainerId
containerId
=
ContainerId
.
newContainerId
(
applicationAttemptId
,
100
)
;
NodeId
nodeId
=
NodeId
.
newInstance
(
,
1234
)
;
Resource
resource
=
Resource
.
newInstance
(
1234
,
2
)
;
ContainerTokenIdentifier
containerTokenIdentifier
=
new
ContainerTokenIdentifier
(
containerId
,
,
,
resource
,
System
.
currentTimeMillis
(
)
+
10000
,
42
,
42
,
Priority
.
newInstance
(
0
)
,
0
)
;
Token
containerToken
=
newContainerToken
(
nodeId
,
.
getBytes
(
)
,
containerTokenIdentifier
)
;
StartContainerRequest
scRequest
=
StartContainerRequest
.
newInstance
(
containerLaunchContext
,
containerToken
)
;
List
<
StartContainerRequest
>
list
=
new
ArrayList
<
StartContainerRequest
>
(
)
;
list
.
add
(
scRequest
)
;
StartContainersRequest
allRequests
=
StartContainersRequest
.
newInstance
(
list
)
;
try
{
proxy
.
startContainers
(
allRequests
)
;
}
catch
(
Exception
e
)
{
try
{
ContainerManagementProtocol
proxy
=
(
ContainerManagementProtocol
)
rpc
.
getProxy
(
ContainerManagementProtocol
.
class
,
server
.
getListenerAddress
(
)
,
conf
)
;
ApplicationId
applicationId
=
ApplicationId
.
newInstance
(
0
,
0
)
;
ApplicationAttemptId
applicationAttemptId
=
ApplicationAttemptId
.
newInstance
(
applicationId
,
0
)
;
ContainerId
containerId
=
ContainerId
.
newContainerId
(
applicationAttemptId
,
100
)
;
NodeId
nodeId
=
NodeId
.
newInstance
(
,
1234
)
;
Resource
resource
=
Resource
.
newInstance
(
1234
,
2
)
;
ContainerTokenIdentifier
containerTokenIdentifier
=
new
ContainerTokenIdentifier
(
containerId
,
,
,
resource
,
System
.
currentTimeMillis
(
)
+
10000
,
42
,
42
,
Priority
.
newInstance
(
0
)
,
0
)
;
Token
containerToken
=
newContainerToken
(
nodeId
,
.
getBytes
(
)
,
containerTokenIdentifier
)
;
List
<
Token
>
increaseTokens
=
new
ArrayList
<
>
(
)
;
increaseTokens
.
add
(
containerToken
)
;
ContainerUpdateRequest
request
=
ContainerUpdateRequest
.
newInstance
(
increaseTokens
)
;
try
{
proxy
.
updateContainer
(
request
)
;
}
catch
(
Exception
e
)
{
}
}
}
for
(
int
i
=
0
;
i
<
methods
.
length
;
i
++
)
{
Method
m
=
methods
[
i
]
;
int
mod
=
m
.
getModifiers
(
)
;
if
(
m
.
getDeclaringClass
(
)
.
equals
(
recordClass
)
&&
Modifier
.
isPublic
(
mod
)
&&
(
!
Modifier
.
isStatic
(
mod
)
)
)
{
String
name
=
m
.
getName
(
)
;
if
(
name
.
startsWith
(
)
&&
(
m
.
getParameterTypes
(
)
.
length
==
1
)
)
{
String
propertyName
=
name
.
substring
(
3
)
;
Type
valueType
=
m
.
getGenericParameterTypes
(
)
[
0
]
;
GetSetPair
p
=
ret
.
get
(
propertyName
)
;
if
(
p
!=
null
&&
p
.
type
.
equals
(
valueType
)
)
{
p
.
setMethod
=
m
;
}
}
}
}
Iterator
<
Map
.
Entry
<
String
,
GetSetPair
>>
itr
=
ret
.
entrySet
(
)
.
iterator
(
)
;
while
(
itr
.
hasNext
(
)
)
{
Map
.
Entry
<
String
,
GetSetPair
>
cur
=
itr
.
next
(
)
;
GetSetPair
gsp
=
cur
.
getValue
(
)
;
for
(
int
i
=
0
;
i
<
methods
.
length
;
i
++
)
{
Method
m
=
methods
[
i
]
;
int
mod
=
m
.
getModifiers
(
)
;
if
(
m
.
getDeclaringClass
(
)
.
equals
(
recordClass
)
&&
Modifier
.
isPublic
(
mod
)
&&
(
!
Modifier
.
isStatic
(
mod
)
)
)
{
String
name
=
m
.
getName
(
)
;
if
(
name
.
startsWith
(
)
&&
(
m
.
getParameterTypes
(
)
.
length
==
1
)
)
{
String
propertyName
=
name
.
substring
(
3
)
;
Type
valueType
=
m
.
getGenericParameterTypes
(
)
[
0
]
;
GetSetPair
p
=
ret
.
get
(
propertyName
)
;
if
(
p
!=
null
&&
p
.
type
.
equals
(
valueType
)
)
{
p
.
setMethod
=
m
;
}
}
}
}
Iterator
<
Map
.
Entry
<
String
,
GetSetPair
>>
itr
=
ret
.
entrySet
(
)
.
iterator
(
)
;
while
(
itr
.
hasNext
(
)
)
{
Map
.
Entry
<
String
,
GetSetPair
>
cur
=
itr
.
next
(
)
;
GetSetPair
gsp
=
cur
.
getValue
(
)
;
int
mod
=
m
.
getModifiers
(
)
;
if
(
m
.
getDeclaringClass
(
)
.
equals
(
recordClass
)
&&
Modifier
.
isPublic
(
mod
)
&&
(
!
Modifier
.
isStatic
(
mod
)
)
)
{
String
name
=
m
.
getName
(
)
;
if
(
name
.
startsWith
(
)
&&
(
m
.
getParameterTypes
(
)
.
length
==
1
)
)
{
String
propertyName
=
name
.
substring
(
3
)
;
Type
valueType
=
m
.
getGenericParameterTypes
(
)
[
0
]
;
GetSetPair
p
=
ret
.
get
(
propertyName
)
;
if
(
p
!=
null
&&
p
.
type
.
equals
(
valueType
)
)
{
p
.
setMethod
=
m
;
}
}
}
}
Iterator
<
Map
.
Entry
<
String
,
GetSetPair
>>
itr
=
ret
.
entrySet
(
)
.
iterator
(
)
;
while
(
itr
.
hasNext
(
)
)
{
Map
.
Entry
<
String
,
GetSetPair
>
cur
=
itr
.
next
(
)
;
GetSetPair
gsp
=
cur
.
getValue
(
)
;
if
(
(
gsp
.
getMethod
==
null
)
||
(
gsp
.
setMethod
==
null
)
)
{
LOG
.
info
(
String
.
format
(
,
gsp
.
propertyName
)
)
;
protected
<
R
,
P
>
void
validatePBImplRecord
(
Class
<
R
>
recordClass
,
Class
<
P
>
protoClass
)
throws
Exception
{
TimelineEvent
event
=
new
TimelineEvent
(
)
;
event
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
event
.
setEventType
(
+
i
)
;
event
.
addEventInfo
(
,
)
;
event
.
addEventInfo
(
,
)
;
entity
.
addEvent
(
event
)
;
}
entity
.
addRelatedEntity
(
,
)
;
entity
.
addRelatedEntity
(
,
)
;
entity
.
addPrimaryFilter
(
,
)
;
entity
.
addPrimaryFilter
(
,
)
;
entity
.
addOtherInfo
(
,
)
;
entity
.
addOtherInfo
(
,
)
;
entity
.
setDomainId
(
+
j
)
;
entities
.
addEntity
(
entity
)
;
}
LOG
.
info
(
)
;
@
Test
public
void
testEvents
(
)
throws
Exception
{
TimelineEvents
events
=
new
TimelineEvents
(
)
;
for
(
int
j
=
0
;
j
<
2
;
++
j
)
{
TimelineEvents
.
EventsOfOneEntity
partEvents
=
new
TimelineEvents
.
EventsOfOneEntity
(
)
;
partEvents
.
setEntityId
(
+
j
)
;
partEvents
.
setEntityType
(
+
j
)
;
for
(
int
i
=
0
;
i
<
2
;
++
i
)
{
TimelineEvent
event
=
new
TimelineEvent
(
)
;
event
.
setTimestamp
(
System
.
currentTimeMillis
(
)
)
;
event
.
setEventType
(
+
i
)
;
event
.
addEventInfo
(
,
)
;
event
.
addEventInfo
(
,
)
;
partEvents
.
addEvent
(
event
)
;
}
events
.
addEvent
(
partEvents
)
;
}
LOG
.
info
(
)
;
TimelinePutResponse
TimelinePutErrors
=
new
TimelinePutResponse
(
)
;
TimelinePutError
error1
=
new
TimelinePutError
(
)
;
error1
.
setEntityId
(
)
;
error1
.
setEntityId
(
)
;
error1
.
setErrorCode
(
TimelinePutError
.
NO_START_TIME
)
;
TimelinePutErrors
.
addError
(
error1
)
;
List
<
TimelinePutError
>
response
=
new
ArrayList
<
TimelinePutError
>
(
)
;
response
.
add
(
error1
)
;
TimelinePutError
error2
=
new
TimelinePutError
(
)
;
error2
.
setEntityId
(
)
;
error2
.
setEntityId
(
)
;
error2
.
setErrorCode
(
TimelinePutError
.
IO_EXCEPTION
)
;
response
.
add
(
error2
)
;
TimelinePutErrors
.
addErrors
(
response
)
;
LOG
.
info
(
)
;
TimelineEvent
event2
=
new
TimelineEvent
(
)
;
event2
.
setId
(
)
;
event2
.
addInfo
(
,
)
;
event2
.
addInfo
(
,
Arrays
.
asList
(
,
)
)
;
event2
.
addInfo
(
,
true
)
;
Assert
.
assertTrue
(
event2
.
getInfo
(
)
.
get
(
)
instanceof
Boolean
)
;
event2
.
setTimestamp
(
2L
)
;
entity
.
addEvent
(
event2
)
;
Assert
.
assertFalse
(
,
event1
.
equals
(
event2
)
)
;
TimelineEvent
event3
=
new
TimelineEvent
(
)
;
event3
.
setId
(
)
;
event3
.
setTimestamp
(
1L
)
;
Assert
.
assertEquals
(
,
event3
,
event1
)
;
Assert
.
assertNotEquals
(
,
event1
,
event2
)
;
entity
.
setCreatedTime
(
0L
)
;
event2
.
setTimestamp
(
2L
)
;
entity
.
addEvent
(
event2
)
;
Assert
.
assertFalse
(
,
event1
.
equals
(
event2
)
)
;
TimelineEvent
event3
=
new
TimelineEvent
(
)
;
event3
.
setId
(
)
;
event3
.
setTimestamp
(
1L
)
;
Assert
.
assertEquals
(
,
event3
,
event1
)
;
Assert
.
assertNotEquals
(
,
event1
,
event2
)
;
entity
.
setCreatedTime
(
0L
)
;
entity
.
addRelatesToEntity
(
,
)
;
entity
.
addRelatesToEntity
(
,
)
;
entity
.
addIsRelatedToEntity
(
,
)
;
entity
.
addIsRelatedToEntity
(
,
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
entity
,
true
)
)
;
TimelineEntities
entities
=
new
TimelineEntities
(
)
;
appAttempt
.
setId
(
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0
,
1
)
,
1
)
.
toString
(
)
)
;
ContainerEntity
container
=
new
ContainerEntity
(
)
;
container
.
setId
(
ContainerId
.
newContainerId
(
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0
,
1
)
,
1
)
,
1
)
.
toString
(
)
)
;
cluster
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow1
.
setParent
(
TimelineEntityType
.
YARN_CLUSTER
.
toString
(
)
,
cluster
.
getId
(
)
)
;
flow1
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
flow2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app2
.
getId
(
)
)
;
app1
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
app1
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
appAttempt
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
app2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
appAttempt
.
addChild
(
TimelineEntityType
.
YARN_CONTAINER
.
toString
(
)
,
container
.
getId
(
)
)
;
container
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
ContainerEntity
container
=
new
ContainerEntity
(
)
;
container
.
setId
(
ContainerId
.
newContainerId
(
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0
,
1
)
,
1
)
,
1
)
.
toString
(
)
)
;
cluster
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow1
.
setParent
(
TimelineEntityType
.
YARN_CLUSTER
.
toString
(
)
,
cluster
.
getId
(
)
)
;
flow1
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
flow2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app2
.
getId
(
)
)
;
app1
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
app1
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
appAttempt
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
app2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
appAttempt
.
addChild
(
TimelineEntityType
.
YARN_CONTAINER
.
toString
(
)
,
container
.
getId
(
)
)
;
container
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
cluster
,
true
)
)
;
container
.
setId
(
ContainerId
.
newContainerId
(
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0
,
1
)
,
1
)
,
1
)
.
toString
(
)
)
;
cluster
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow1
.
setParent
(
TimelineEntityType
.
YARN_CLUSTER
.
toString
(
)
,
cluster
.
getId
(
)
)
;
flow1
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
flow2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app2
.
getId
(
)
)
;
app1
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
app1
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
appAttempt
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
app2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
appAttempt
.
addChild
(
TimelineEntityType
.
YARN_CONTAINER
.
toString
(
)
,
container
.
getId
(
)
)
;
container
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
cluster
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow1
,
true
)
)
;
cluster
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow1
.
setParent
(
TimelineEntityType
.
YARN_CLUSTER
.
toString
(
)
,
cluster
.
getId
(
)
)
;
flow1
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
flow2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app2
.
getId
(
)
)
;
app1
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
app1
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
appAttempt
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
app2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
appAttempt
.
addChild
(
TimelineEntityType
.
YARN_CONTAINER
.
toString
(
)
,
container
.
getId
(
)
)
;
container
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
cluster
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow1
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow2
,
true
)
)
;
flow1
.
setParent
(
TimelineEntityType
.
YARN_CLUSTER
.
toString
(
)
,
cluster
.
getId
(
)
)
;
flow1
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
flow2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app2
.
getId
(
)
)
;
app1
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
app1
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
appAttempt
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
app2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
appAttempt
.
addChild
(
TimelineEntityType
.
YARN_CONTAINER
.
toString
(
)
,
container
.
getId
(
)
)
;
container
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
cluster
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow1
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow2
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
app1
,
true
)
)
;
flow1
.
addChild
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
flow2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app2
.
getId
(
)
)
;
app1
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
app1
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
appAttempt
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
app2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
appAttempt
.
addChild
(
TimelineEntityType
.
YARN_CONTAINER
.
toString
(
)
,
container
.
getId
(
)
)
;
container
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
cluster
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow1
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow2
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
app1
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
app2
,
true
)
)
;
flow2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
flow2
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app2
.
getId
(
)
)
;
app1
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
app1
.
addChild
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
appAttempt
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
app1
.
getId
(
)
)
;
app2
.
setParent
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
flow2
.
getId
(
)
)
;
appAttempt
.
addChild
(
TimelineEntityType
.
YARN_CONTAINER
.
toString
(
)
,
container
.
getId
(
)
)
;
container
.
setParent
(
TimelineEntityType
.
YARN_APPLICATION_ATTEMPT
.
toString
(
)
,
appAttempt
.
getId
(
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
cluster
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow1
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
flow2
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
app1
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
app2
,
true
)
)
;
LOG
.
info
(
TimelineUtils
.
dumpTimelineRecordtoJSON
(
appAttempt
,
true
)
)
;
@
Test
public
void
testUser
(
)
throws
Exception
{
UserEntity
user
=
new
UserEntity
(
)
;
user
.
setId
(
)
;
user
.
addInfo
(
,
)
;
user
.
addInfo
(
,
)
;
@
Test
public
void
testQueue
(
)
throws
Exception
{
QueueEntity
queue
=
new
QueueEntity
(
)
;
queue
.
setId
(
)
;
queue
.
addInfo
(
,
)
;
queue
.
addInfo
(
,
)
;
queue
.
setParent
(
TimelineEntityType
.
YARN_QUEUE
.
toString
(
)
,
)
;
queue
.
addChild
(
TimelineEntityType
.
YARN_QUEUE
.
toString
(
)
,
)
;
queue
.
addChild
(
TimelineEntityType
.
YARN_QUEUE
.
toString
(
)
,
)
;
@
Before
public
void
setup
(
)
throws
Exception
{
localFS
=
FileContext
.
getLocalFSFileContext
(
)
;
localActiveDir
=
new
File
(
,
this
.
getClass
(
)
.
getSimpleName
(
)
+
)
.
getAbsoluteFile
(
)
;
localFS
.
delete
(
new
Path
(
localActiveDir
.
getAbsolutePath
(
)
)
,
true
)
;
localActiveDir
.
mkdir
(
)
;
@
Before
@
After
public
void
cleanupTestDir
(
)
throws
Exception
{
Path
workDirPath
=
new
Path
(
testWorkDir
.
getAbsolutePath
(
)
)
;
String
stderr
=
;
writeSrcFile
(
srcFilePath1
,
stdout
,
data
+
testContainerId1
.
toString
(
)
+
stdout
)
;
writeSrcFile
(
srcFilePath1
,
stderr
,
data
+
testContainerId1
.
toString
(
)
+
stderr
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
getCurrentUser
(
)
;
try
(
LogWriter
logWriter
=
new
LogWriter
(
)
)
{
logWriter
.
initialize
(
conf
,
remoteAppLogFile
,
ugi
)
;
LogKey
logKey
=
new
LogKey
(
testContainerId1
)
;
String
randomUser
=
;
LogValue
logValue
=
spy
(
new
LogValue
(
Collections
.
singletonList
(
srcFileRoot
.
toString
(
)
)
,
testContainerId1
,
randomUser
)
)
;
when
(
logValue
.
getUser
(
)
)
.
thenReturn
(
randomUser
)
.
thenReturn
(
ugi
.
getShortUserName
(
)
)
;
logWriter
.
append
(
logKey
,
logValue
)
;
}
BufferedReader
in
=
new
BufferedReader
(
new
FileReader
(
new
File
(
remoteAppLogFile
.
toUri
(
)
.
getRawPath
(
)
)
)
)
;
String
line
;
StringBuffer
sb
=
new
StringBuffer
(
)
;
while
(
(
line
=
in
.
readLine
(
)
)
!=
null
)
{
private
void
verifyFileControllerInstance
(
LogAggregationFileControllerFactory
factory
,
Class
<
?
extends
LogAggregationFileController
>
className
)
throws
IOException
{
List
<
LogAggregationFileController
>
fileControllers
=
factory
.
getConfiguredLogAggregationFileControllerList
(
)
;
FileSystem
fs
=
FileSystem
.
get
(
getConf
(
)
)
;
Path
logPath
=
fileControllers
.
get
(
0
)
.
getRemoteAppLogDir
(
appId
,
APP_OWNER
)
;
static
LocalResource
createJar
(
FileContext
files
,
Path
p
,
LocalResourceVisibility
vis
)
throws
IOException
{
lostDescendant
=
TEST_ROOT_DIR
+
File
.
separator
+
;
File
file
=
new
File
(
shellScript
)
;
FileUtils
.
writeStringToFile
(
file
,
+
+
+
+
+
+
shellScript
+
+
+
+
lowestDescendant
+
+
+
+
lostDescendant
+
+
+
+
+
,
StandardCharsets
.
UTF_8
)
;
Thread
t
=
new
RogueTaskThread
(
)
;
t
.
start
(
)
;
String
pid
=
getRogueTaskPID
(
)
;
LOG
.
info
(
+
pid
)
;
ProcfsBasedProcessTree
p
=
createProcessTree
(
pid
)
;
p
.
updateProcessTree
(
)
;
LOG
.
info
(
+
p
)
;
File
leaf
=
new
File
(
lowestDescendant
)
;
while
(
!
leaf
.
exists
(
)
)
{
try
{
Thread
.
sleep
(
500
)
;
}
catch
(
InterruptedException
ie
)
{
}
p
.
updateProcessTree
(
)
;
LOG
.
info
(
+
p
)
;
String
lostpid
=
getPidFromPidFile
(
lostDescendant
)
;
LOG
.
info
(
+
lostpid
)
;
Assert
.
assertTrue
(
,
p
.
contains
(
lostpid
)
)
;
String
processTreeDump
=
p
.
getProcessTreeDump
(
)
;
destroyProcessTree
(
pid
)
;
boolean
isAlive
=
true
;
for
(
int
tries
=
100
;
tries
>
0
;
tries
--
)
{
if
(
isSetsidAvailable
(
)
)
{
isAlive
=
isAnyProcessInTreeAlive
(
p
)
;
}
else
{
isAlive
=
isAlive
(
pid
)
;
}
if
(
!
isAlive
)
{
break
;
memInfos
[
4
]
=
new
ProcessTreeSmapMemInfo
(
)
;
memInfos
[
5
]
=
new
ProcessTreeSmapMemInfo
(
)
;
String
[
]
cmdLines
=
new
String
[
numProcesses
]
;
cmdLines
[
0
]
=
;
cmdLines
[
1
]
=
;
cmdLines
[
2
]
=
;
cmdLines
[
3
]
=
;
cmdLines
[
4
]
=
;
cmdLines
[
5
]
=
;
createMemoryMappingInfo
(
memInfos
)
;
writeStatFiles
(
procfsRootDir
,
pids
,
procInfos
,
memInfos
)
;
writeCmdLineFiles
(
procfsRootDir
,
pids
,
cmdLines
)
;
ProcfsBasedProcessTree
processTree
=
createProcessTree
(
,
procfsRootDir
.
getAbsolutePath
(
)
,
SystemClock
.
getInstance
(
)
)
;
processTree
.
updateProcessTree
(
)
;
String
processTreeDump
=
processTree
.
getProcessTreeDump
(
)
;
public
static
void
setupPidDirs
(
File
procfsRootDir
,
String
[
]
pids
)
throws
IOException
{
for
(
String
pid
:
pids
)
{
File
pidDir
=
new
File
(
procfsRootDir
,
pid
)
;
FileUtils
.
forceMkdir
(
pidDir
)
;
private
void
logInstances
(
HttpServletRequest
req
,
HttpServletResponse
res
,
PrintWriter
out
)
{
private
void
logInstances
(
HttpServletRequest
req
,
HttpServletResponse
res
,
PrintWriter
out
)
{
LOG
.
info
(
,
req
)
;
private
void
logInstances
(
HttpServletRequest
req
,
HttpServletResponse
res
,
PrintWriter
out
)
{
LOG
.
info
(
,
req
)
;
LOG
.
info
(
,
res
)
;
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
String
[
]
names
=
CsiConfigUtils
.
getCsiDriverNames
(
conf
)
;
if
(
names
!=
null
&&
names
.
length
>
0
)
{
for
(
String
driverName
:
names
)
{
@
Override
public
void
init
(
String
driverName
,
Configuration
conf
)
throws
YarnException
{
String
driverEndpoint
=
CsiConfigUtils
.
getCsiDriverEndpoint
(
driverName
,
conf
)
;
@
Override
public
NodePublishVolumeResponse
nodePublishVolume
(
NodePublishVolumeRequest
request
)
throws
YarnException
,
IOException
{
@
Override
public
NodePublishVolumeResponse
nodePublishVolume
(
NodePublishVolumeRequest
request
)
throws
YarnException
,
IOException
{
LOG
.
debug
(
,
request
)
;
Csi
.
NodePublishVolumeRequest
req
=
ProtoTranslatorFactory
.
getTranslator
(
NodePublishVolumeRequest
.
class
,
Csi
.
NodePublishVolumeRequest
.
class
)
.
convertTo
(
request
)
;
@
Override
public
NodeUnpublishVolumeResponse
nodeUnpublishVolume
(
NodeUnpublishVolumeRequest
request
)
throws
YarnException
,
IOException
{
@
Override
public
NodeUnpublishVolumeResponse
nodeUnpublishVolume
(
NodeUnpublishVolumeRequest
request
)
throws
YarnException
,
IOException
{
LOG
.
debug
(
,
request
)
;
Csi
.
NodeUnpublishVolumeRequest
req
=
ProtoTranslatorFactory
.
getTranslator
(
NodeUnpublishVolumeRequest
.
class
,
Csi
.
NodeUnpublishVolumeRequest
.
class
)
.
convertTo
(
request
)
;
public
void
start
(
)
throws
IOException
{
EpollEventLoopGroup
group
=
new
EpollEventLoopGroup
(
)
;
server
=
NettyServerBuilder
.
forAddress
(
GrpcHelper
.
getSocketAddress
(
socketAddress
)
)
.
channelType
(
EpollServerDomainSocketChannel
.
class
)
.
workerEventLoopGroup
(
group
)
.
bossEventLoopGroup
(
group
)
.
addService
(
new
FakeCsiIdentityService
(
)
)
.
build
(
)
;
server
.
start
(
)
;
ApplicationReportExt
app
=
convertToApplicationReport
(
entity
,
field
)
;
if
(
field
==
ApplicationReportField
.
USER_AND_ACLS
)
{
return
app
;
}
try
{
checkAccess
(
app
)
;
if
(
app
.
appReport
.
getCurrentApplicationAttemptId
(
)
!=
null
)
{
ApplicationAttemptReport
appAttempt
=
getApplicationAttempt
(
app
.
appReport
.
getCurrentApplicationAttemptId
(
)
,
false
)
;
app
.
appReport
.
setHost
(
appAttempt
.
getHost
(
)
)
;
app
.
appReport
.
setRpcPort
(
appAttempt
.
getRpcPort
(
)
)
;
app
.
appReport
.
setTrackingUrl
(
appAttempt
.
getTrackingUrl
(
)
)
;
app
.
appReport
.
setOriginalTrackingUrl
(
appAttempt
.
getOriginalTrackingUrl
(
)
)
;
}
}
catch
(
AuthorizationException
|
ApplicationAttemptNotFoundException
e
)
{
if
(
e
instanceof
AuthorizationException
)
{
LOG
.
warn
(
+
app
.
appReport
.
getApplicationId
(
)
+
,
e
)
;
}
else
{
webApp
=
WebApps
.
$for
(
,
ApplicationHistoryClientService
.
class
,
ahsClientService
,
)
.
with
(
conf
)
.
withAttribute
(
YarnConfiguration
.
TIMELINE_SERVICE_WEBAPP_ADDRESS
,
conf
.
get
(
YarnConfiguration
.
TIMELINE_SERVICE_WEBAPP_ADDRESS
)
)
.
withCSRFProtection
(
YarnConfiguration
.
TIMELINE_CSRF_PREFIX
)
.
withXFSProtection
(
YarnConfiguration
.
TIMELINE_XFS_PREFIX
)
.
at
(
bindAddress
)
.
build
(
ahsWebApp
)
;
HttpServer2
httpServer
=
webApp
.
httpServer
(
)
;
String
[
]
names
=
conf
.
getTrimmedStrings
(
YarnConfiguration
.
TIMELINE_SERVICE_UI_NAMES
)
;
WebAppContext
webAppContext
=
httpServer
.
getWebAppContext
(
)
;
for
(
String
name
:
names
)
{
String
webPath
=
conf
.
get
(
YarnConfiguration
.
TIMELINE_SERVICE_UI_WEB_PATH_PREFIX
+
name
)
;
String
onDiskPath
=
conf
.
get
(
YarnConfiguration
.
TIMELINE_SERVICE_UI_ON_DISK_PATH_PREFIX
+
name
)
;
WebAppContext
uiWebAppContext
=
new
WebAppContext
(
)
;
uiWebAppContext
.
setContextPath
(
webPath
)
;
if
(
onDiskPath
.
endsWith
(
)
)
{
uiWebAppContext
.
setWar
(
onDiskPath
)
;
}
else
{
uiWebAppContext
.
setResourceBase
(
onDiskPath
)
;
}
final
String
[
]
ALL_URLS
=
{
}
;
FilterHolder
[
]
filterHolders
=
webAppContext
.
getServletHandler
(
)
.
getFilters
(
)
;
mergeApplicationHistoryData
(
historyData
,
startData
)
;
readStartData
=
true
;
}
else
if
(
entry
.
key
.
suffix
.
equals
(
FINISH_DATA_SUFFIX
)
)
{
ApplicationFinishData
finishData
=
parseApplicationFinishData
(
entry
.
value
)
;
mergeApplicationHistoryData
(
historyData
,
finishData
)
;
readFinishData
=
true
;
}
}
}
if
(
!
readStartData
&&
!
readFinishData
)
{
return
null
;
}
if
(
!
readStartData
)
{
LOG
.
warn
(
+
appId
)
;
}
if
(
!
readFinishData
)
{
LOG
.
warn
(
+
appId
)
;
}
LOG
.
info
(
+
appId
)
;
return
historyData
;
}
catch
(
IOException
e
)
{
while
(
hfReader
.
hasNext
(
)
)
{
HistoryFileReader
.
Entry
entry
=
hfReader
.
next
(
)
;
if
(
entry
.
key
.
id
.
startsWith
(
ConverterUtils
.
APPLICATION_ATTEMPT_PREFIX
)
)
{
ApplicationAttemptId
appAttemptId
=
ApplicationAttemptId
.
fromString
(
entry
.
key
.
id
)
;
if
(
appAttemptId
.
getApplicationId
(
)
.
equals
(
appId
)
)
{
ApplicationAttemptHistoryData
historyData
=
historyDataMap
.
get
(
appAttemptId
)
;
if
(
historyData
==
null
)
{
historyData
=
ApplicationAttemptHistoryData
.
newInstance
(
appAttemptId
,
null
,
-
1
,
null
,
null
,
null
,
FinalApplicationStatus
.
UNDEFINED
,
null
)
;
historyDataMap
.
put
(
appAttemptId
,
historyData
)
;
}
if
(
entry
.
key
.
suffix
.
equals
(
START_DATA_SUFFIX
)
)
{
mergeApplicationAttemptHistoryData
(
historyData
,
parseApplicationAttemptStartData
(
entry
.
value
)
)
;
}
else
if
(
entry
.
key
.
suffix
.
equals
(
FINISH_DATA_SUFFIX
)
)
{
mergeApplicationAttemptHistoryData
(
historyData
,
parseApplicationAttemptFinishData
(
entry
.
value
)
)
;
}
}
}
}
LOG
.
info
(
+
+
appId
)
;
}
catch
(
IOException
e
)
{
mergeApplicationAttemptHistoryData
(
historyData
,
startData
)
;
readStartData
=
true
;
}
else
if
(
entry
.
key
.
suffix
.
equals
(
FINISH_DATA_SUFFIX
)
)
{
ApplicationAttemptFinishData
finishData
=
parseApplicationAttemptFinishData
(
entry
.
value
)
;
mergeApplicationAttemptHistoryData
(
historyData
,
finishData
)
;
readFinishData
=
true
;
}
}
}
if
(
!
readStartData
&&
!
readFinishData
)
{
return
null
;
}
if
(
!
readStartData
)
{
LOG
.
warn
(
+
appAttemptId
)
;
}
if
(
!
readFinishData
)
{
LOG
.
warn
(
+
appAttemptId
)
;
}
LOG
.
info
(
+
appAttemptId
)
;
return
historyData
;
}
catch
(
IOException
e
)
{
mergeContainerHistoryData
(
historyData
,
startData
)
;
readStartData
=
true
;
}
else
if
(
entry
.
key
.
suffix
.
equals
(
FINISH_DATA_SUFFIX
)
)
{
ContainerFinishData
finishData
=
parseContainerFinishData
(
entry
.
value
)
;
mergeContainerHistoryData
(
historyData
,
finishData
)
;
readFinishData
=
true
;
}
}
}
if
(
!
readStartData
&&
!
readFinishData
)
{
return
null
;
}
if
(
!
readStartData
)
{
LOG
.
warn
(
+
containerId
)
;
}
if
(
!
readFinishData
)
{
LOG
.
warn
(
+
containerId
)
;
}
LOG
.
info
(
+
containerId
)
;
return
historyData
;
}
catch
(
IOException
e
)
{
while
(
hfReader
.
hasNext
(
)
)
{
HistoryFileReader
.
Entry
entry
=
hfReader
.
next
(
)
;
if
(
entry
.
key
.
id
.
startsWith
(
ConverterUtils
.
CONTAINER_PREFIX
)
)
{
ContainerId
containerId
=
ContainerId
.
fromString
(
entry
.
key
.
id
)
;
if
(
containerId
.
getApplicationAttemptId
(
)
.
equals
(
appAttemptId
)
)
{
ContainerHistoryData
historyData
=
historyDataMap
.
get
(
containerId
)
;
if
(
historyData
==
null
)
{
historyData
=
ContainerHistoryData
.
newInstance
(
containerId
,
null
,
null
,
null
,
Long
.
MIN_VALUE
,
Long
.
MAX_VALUE
,
null
,
Integer
.
MAX_VALUE
,
null
)
;
historyDataMap
.
put
(
containerId
,
historyData
)
;
}
if
(
entry
.
key
.
suffix
.
equals
(
START_DATA_SUFFIX
)
)
{
mergeContainerHistoryData
(
historyData
,
parseContainerStartData
(
entry
.
value
)
)
;
}
else
if
(
entry
.
key
.
suffix
.
equals
(
FINISH_DATA_SUFFIX
)
)
{
mergeContainerHistoryData
(
historyData
,
parseContainerFinishData
(
entry
.
value
)
)
;
}
}
}
}
LOG
.
info
(
+
+
appAttemptId
)
;
}
catch
(
IOException
e
)
{
}
}
}
if
(
primaryFilters
!=
null
&&
!
primaryFilters
.
isEmpty
(
)
)
{
for
(
Entry
<
String
,
Set
<
Object
>>
primaryFilter
:
primaryFilters
.
entrySet
(
)
)
{
for
(
Object
primaryFilterValue
:
primaryFilter
.
getValue
(
)
)
{
byte
[
]
key
=
createPrimaryFilterKey
(
entity
.
getEntityId
(
)
,
entity
.
getEntityType
(
)
,
revStartTime
,
primaryFilter
.
getKey
(
)
,
primaryFilterValue
)
;
writeBatch
.
put
(
key
,
EMPTY_BYTES
)
;
writePrimaryFilterEntries
(
writeBatch
,
primaryFilters
,
key
,
EMPTY_BYTES
)
;
}
}
}
Map
<
String
,
Object
>
otherInfo
=
entity
.
getOtherInfo
(
)
;
if
(
otherInfo
!=
null
&&
!
otherInfo
.
isEmpty
(
)
)
{
for
(
Entry
<
String
,
Object
>
i
:
otherInfo
.
entrySet
(
)
)
{
byte
[
]
key
=
createOtherInfoKey
(
entity
.
getEntityId
(
)
,
entity
.
getEntityType
(
)
,
revStartTime
,
i
.
getKey
(
)
)
;
byte
[
]
value
=
GenericObjectMapper
.
write
(
i
.
getValue
(
)
)
;
writeBatch
.
put
(
key
,
value
)
;
writePrimaryFilterEntries
(
writeBatch
,
primaryFilters
,
key
,
value
)
;
}
}
byte
[
]
key
=
createDomainIdKey
(
entity
.
getEntityId
(
)
,
entity
.
getEntityType
(
)
,
revStartTime
)
;
if
(
entity
.
getDomainId
(
)
==
null
||
entity
.
getDomainId
(
)
.
length
(
)
==
0
)
{
if
(
primaryFilters
!=
null
&&
!
primaryFilters
.
isEmpty
(
)
)
{
for
(
Entry
<
String
,
Set
<
Object
>>
primaryFilter
:
primaryFilters
.
entrySet
(
)
)
{
for
(
Object
primaryFilterValue
:
primaryFilter
.
getValue
(
)
)
{
byte
[
]
key
=
createPrimaryFilterKey
(
entity
.
getEntityId
(
)
,
entity
.
getEntityType
(
)
,
revStartTime
,
primaryFilter
.
getKey
(
)
,
primaryFilterValue
)
;
writeBatch
.
put
(
key
,
EMPTY_BYTES
)
;
writePrimaryFilterEntries
(
writeBatch
,
primaryFilters
,
key
,
EMPTY_BYTES
)
;
}
}
}
Map
<
String
,
Object
>
otherInfo
=
entity
.
getOtherInfo
(
)
;
if
(
otherInfo
!=
null
&&
!
otherInfo
.
isEmpty
(
)
)
{
for
(
Entry
<
String
,
Object
>
i
:
otherInfo
.
entrySet
(
)
)
{
byte
[
]
key
=
createOtherInfoKey
(
entity
.
getEntityId
(
)
,
entity
.
getEntityType
(
)
,
revStartTime
,
i
.
getKey
(
)
)
;
byte
[
]
value
=
GenericObjectMapper
.
write
(
i
.
getValue
(
)
)
;
writeBatch
.
put
(
key
,
value
)
;
writePrimaryFilterEntries
(
writeBatch
,
primaryFilters
,
key
,
value
)
;
}
}
byte
[
]
key
=
createDomainIdKey
(
entity
.
getEntityId
(
)
,
entity
.
getEntityType
(
)
,
revStartTime
)
;
if
(
entity
.
getDomainId
(
)
==
null
||
entity
.
getDomainId
(
)
.
length
(
)
==
0
)
{
if
(
entity
.
getDomainId
(
)
==
null
||
entity
.
getDomainId
(
)
.
length
(
)
==
0
)
{
if
(
!
allowEmptyDomainId
)
{
handleError
(
entity
,
response
,
TimelinePutError
.
NO_DOMAIN
)
;
return
;
}
}
else
{
writeBatch
.
put
(
key
,
entity
.
getDomainId
(
)
.
getBytes
(
Charset
.
forName
(
)
)
)
;
writePrimaryFilterEntries
(
writeBatch
,
primaryFilters
,
key
,
entity
.
getDomainId
(
)
.
getBytes
(
Charset
.
forName
(
)
)
)
;
}
db
.
write
(
writeBatch
)
;
}
catch
(
DBException
de
)
{
LOG
.
error
(
+
entity
.
getEntityId
(
)
+
+
entity
.
getEntityType
(
)
,
de
)
;
handleError
(
entity
,
response
,
TimelinePutError
.
IO_EXCEPTION
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
entity
.
getEntityId
(
)
+
+
entity
.
getEntityType
(
)
,
e
)
;
handleError
(
entity
,
response
,
TimelinePutError
.
IO_EXCEPTION
)
;
}
finally
{
return
;
}
}
else
{
writeBatch
.
put
(
key
,
entity
.
getDomainId
(
)
.
getBytes
(
Charset
.
forName
(
)
)
)
;
writePrimaryFilterEntries
(
writeBatch
,
primaryFilters
,
key
,
entity
.
getDomainId
(
)
.
getBytes
(
Charset
.
forName
(
)
)
)
;
}
db
.
write
(
writeBatch
)
;
}
catch
(
DBException
de
)
{
LOG
.
error
(
+
entity
.
getEntityId
(
)
+
+
entity
.
getEntityType
(
)
,
de
)
;
handleError
(
entity
,
response
,
TimelinePutError
.
IO_EXCEPTION
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
entity
.
getEntityId
(
)
+
+
entity
.
getEntityType
(
)
,
e
)
;
handleError
(
entity
,
response
,
TimelinePutError
.
IO_EXCEPTION
)
;
}
finally
{
lock
.
unlock
(
)
;
writeLocks
.
returnLock
(
lock
)
;
IOUtils
.
cleanupWithLogger
(
LOG
,
writeBatch
)
;
byte
[
]
typePrefix
=
kb
.
getBytesForLookup
(
)
;
kb
.
add
(
reverseTimestamp
)
;
if
(
!
seeked
)
{
iterator
.
seek
(
kb
.
getBytesForLookup
(
)
)
;
}
if
(
!
iterator
.
hasNext
(
)
)
{
return
false
;
}
byte
[
]
entityKey
=
iterator
.
peekNext
(
)
.
getKey
(
)
;
if
(
!
prefixMatches
(
typePrefix
,
typePrefix
.
length
,
entityKey
)
)
{
return
false
;
}
KeyParser
kp
=
new
KeyParser
(
entityKey
,
typePrefix
.
length
+
8
)
;
String
entityId
=
kp
.
getNextString
(
)
;
int
prefixlen
=
kp
.
getOffset
(
)
;
byte
[
]
deletePrefix
=
new
byte
[
prefixlen
]
;
System
.
arraycopy
(
entityKey
,
0
,
deletePrefix
,
0
,
prefixlen
)
;
writeBatch
=
db
.
createWriteBatch
(
)
;
writeBatch
=
db
.
createWriteBatch
(
)
;
LOG
.
debug
(
,
entityType
,
entityId
)
;
writeBatch
.
delete
(
createStartTimeLookupKey
(
entityId
,
entityType
)
)
;
EntityIdentifier
entityIdentifier
=
new
EntityIdentifier
(
entityId
,
entityType
)
;
startTimeReadCache
.
remove
(
entityIdentifier
)
;
startTimeWriteCache
.
remove
(
entityIdentifier
)
;
for
(
;
iterator
.
hasNext
(
)
;
iterator
.
next
(
)
)
{
byte
[
]
key
=
iterator
.
peekNext
(
)
.
getKey
(
)
;
if
(
!
prefixMatches
(
entityKey
,
prefixlen
,
key
)
)
{
break
;
}
writeBatch
.
delete
(
key
)
;
if
(
key
.
length
==
prefixlen
)
{
continue
;
}
if
(
key
[
prefixlen
]
==
PRIMARY_FILTERS_COLUMN
[
0
]
)
{
kp
=
new
KeyParser
(
key
,
prefixlen
+
PRIMARY_FILTERS_COLUMN
.
length
)
;
}
writeBatch
.
delete
(
key
)
;
if
(
key
.
length
==
prefixlen
)
{
continue
;
}
if
(
key
[
prefixlen
]
==
PRIMARY_FILTERS_COLUMN
[
0
]
)
{
kp
=
new
KeyParser
(
key
,
prefixlen
+
PRIMARY_FILTERS_COLUMN
.
length
)
;
String
name
=
kp
.
getNextString
(
)
;
Object
value
=
GenericObjectMapper
.
read
(
key
,
kp
.
getOffset
(
)
)
;
deleteKeysWithPrefix
(
writeBatch
,
addPrimaryFilterToKey
(
name
,
value
,
deletePrefix
)
,
pfIterator
)
;
LOG
.
debug
(
,
entityType
,
entityId
,
name
,
value
)
;
}
else
if
(
key
[
prefixlen
]
==
RELATED_ENTITIES_COLUMN
[
0
]
)
{
kp
=
new
KeyParser
(
key
,
prefixlen
+
RELATED_ENTITIES_COLUMN
.
length
)
;
String
type
=
kp
.
getNextString
(
)
;
String
id
=
kp
.
getNextString
(
)
;
byte
[
]
relatedEntityStartTime
=
getStartTime
(
id
,
type
)
;
if
(
relatedEntityStartTime
==
null
)
{
LOG
.
debug
(
,
entityType
,
entityId
,
name
,
value
)
;
}
else
if
(
key
[
prefixlen
]
==
RELATED_ENTITIES_COLUMN
[
0
]
)
{
kp
=
new
KeyParser
(
key
,
prefixlen
+
RELATED_ENTITIES_COLUMN
.
length
)
;
String
type
=
kp
.
getNextString
(
)
;
String
id
=
kp
.
getNextString
(
)
;
byte
[
]
relatedEntityStartTime
=
getStartTime
(
id
,
type
)
;
if
(
relatedEntityStartTime
==
null
)
{
LOG
.
warn
(
+
+
id
+
+
type
+
+
+
entityId
+
+
entityType
)
;
continue
;
}
writeBatch
.
delete
(
createReverseRelatedEntityKey
(
id
,
type
,
relatedEntityStartTime
,
entityId
,
entityType
)
)
;
LOG
.
debug
(
+
,
entityType
,
entityId
,
type
,
id
)
;
}
else
if
(
key
[
prefixlen
]
==
INVISIBLE_REVERSE_RELATED_ENTITIES_COLUMN
[
0
]
)
{
kp
=
new
KeyParser
(
key
,
prefixlen
+
INVISIBLE_REVERSE_RELATED_ENTITIES_COLUMN
.
length
)
;
String
type
=
kp
.
getNextString
(
)
;
String
id
=
kp
.
getNextString
(
)
;
long
typeCount
=
0
;
deleteLock
.
writeLock
(
)
.
lock
(
)
;
try
{
iterator
=
getDbIterator
(
false
)
;
pfIterator
=
getDbIterator
(
false
)
;
if
(
deletionThread
!=
null
&&
deletionThread
.
isInterrupted
(
)
)
{
throw
new
InterruptedException
(
)
;
}
boolean
seeked
=
false
;
while
(
deleteNextEntity
(
entityType
,
reverseTimestamp
,
iterator
,
pfIterator
,
seeked
)
)
{
typeCount
++
;
totalCount
++
;
seeked
=
true
;
if
(
deletionThread
!=
null
&&
deletionThread
.
isInterrupted
(
)
)
{
throw
new
InterruptedException
(
)
;
}
}
}
catch
(
IOException
e
)
{
if
(
deletionThread
!=
null
&&
deletionThread
.
isInterrupted
(
)
)
{
throw
new
InterruptedException
(
)
;
}
boolean
seeked
=
false
;
while
(
deleteNextEntity
(
entityType
,
reverseTimestamp
,
iterator
,
pfIterator
,
seeked
)
)
{
typeCount
++
;
totalCount
++
;
seeked
=
true
;
if
(
deletionThread
!=
null
&&
deletionThread
.
isInterrupted
(
)
)
{
throw
new
InterruptedException
(
)
;
}
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
entityType
+
,
e
)
;
}
finally
{
IOUtils
.
cleanupWithLogger
(
LOG
,
iterator
,
pfIterator
)
;
deleteLock
.
writeLock
(
)
.
unlock
(
)
;
if
(
typeCount
>
0
)
{
private
void
checkVersion
(
)
throws
IOException
{
Version
loadedVersion
=
loadVersion
(
)
;
protected
void
setNextRollingTimeMillis
(
final
long
timestamp
)
{
this
.
nextRollingCheckMillis
=
timestamp
;
private
synchronized
void
scheduleOldDBsForEviction
(
)
{
long
evictionThreshold
=
computeCurrentCheckMillis
(
currentTimeMillis
(
)
-
getTimeToLive
(
)
)
;
private
synchronized
void
scheduleOldDBsForEviction
(
)
{
long
evictionThreshold
=
computeCurrentCheckMillis
(
currentTimeMillis
(
)
-
getTimeToLive
(
)
)
;
LOG
.
info
(
+
getName
(
)
+
+
fdf
.
format
(
evictionThreshold
)
+
)
;
Iterator
<
Entry
<
Long
,
DB
>>
iterator
=
rollingdbs
.
entrySet
(
)
.
iterator
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
Entry
<
Long
,
DB
>
entry
=
iterator
.
next
(
)
;
if
(
entry
.
getKey
(
)
<
evictionThreshold
)
{
public
synchronized
void
evictOldDBs
(
)
{
LOG
.
info
(
+
getName
(
)
+
)
;
Iterator
<
Entry
<
Long
,
DB
>>
iterator
=
rollingdbsToEvict
.
entrySet
(
)
.
iterator
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
Entry
<
Long
,
DB
>
entry
=
iterator
.
next
(
)
;
IOUtils
.
cleanupWithLogger
(
LOG
,
entry
.
getValue
(
)
)
;
String
dbName
=
fdf
.
format
(
entry
.
getKey
(
)
)
;
Path
path
=
new
Path
(
rollingDBPath
,
getName
(
)
+
+
dbName
)
;
try
{
}
localFS
.
setPermission
(
dbPath
,
LEVELDB_DIR_UMASK
)
;
}
if
(
!
localFS
.
exists
(
domainDBPath
)
)
{
if
(
!
localFS
.
mkdirs
(
domainDBPath
)
)
{
throw
new
IOException
(
+
+
domainDBPath
)
;
}
localFS
.
setPermission
(
domainDBPath
,
LEVELDB_DIR_UMASK
)
;
}
if
(
!
localFS
.
exists
(
starttimeDBPath
)
)
{
if
(
!
localFS
.
mkdirs
(
starttimeDBPath
)
)
{
throw
new
IOException
(
+
+
starttimeDBPath
)
;
}
localFS
.
setPermission
(
starttimeDBPath
,
LEVELDB_DIR_UMASK
)
;
}
if
(
!
localFS
.
exists
(
ownerDBPath
)
)
{
if
(
!
localFS
.
mkdirs
(
ownerDBPath
)
)
{
throw
new
IOException
(
+
+
ownerDBPath
)
;
}
localFS
.
setPermission
(
ownerDBPath
,
LEVELDB_DIR_UMASK
)
;
}
}
options
.
maxOpenFiles
(
conf
.
getInt
(
TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES
,
DEFAULT_TIMELINE_SERVICE_LEVELDB_MAX_OPEN_FILES
)
)
;
options
.
writeBufferSize
(
conf
.
getInt
(
TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE
,
DEFAULT_TIMELINE_SERVICE_LEVELDB_WRITE_BUFFER_SIZE
)
)
;
@
Override
public
TimelineEntity
getEntity
(
String
entityId
,
String
entityType
,
EnumSet
<
Field
>
fields
)
throws
IOException
{
Long
revStartTime
=
getStartTimeLong
(
entityId
,
entityType
)
;
if
(
revStartTime
==
null
)
{
domainId
=
TimelineDataManager
.
DEFAULT_DOMAIN_ID
;
}
else
{
domainId
=
new
String
(
relatedDomainIdBytes
,
UTF_8
)
;
}
if
(
!
domainId
.
equals
(
entity
.
getDomainId
(
)
)
)
{
TimelinePutError
error
=
new
TimelinePutError
(
)
;
error
.
setEntityId
(
entity
.
getEntityId
(
)
)
;
error
.
setEntityType
(
entity
.
getEntityType
(
)
)
;
error
.
setErrorCode
(
TimelinePutError
.
FORBIDDEN_RELATION
)
;
response
.
addError
(
error
)
;
continue
;
}
byte
[
]
key
=
createRelatedEntityKey
(
relatedEntityId
,
relatedEntityType
,
relatedEntityStartTime
,
entity
.
getEntityId
(
)
,
entity
.
getEntityType
(
)
)
;
WriteBatch
relatedWriteBatch
=
relatedRollingWriteBatch
.
getWriteBatch
(
)
;
relatedWriteBatch
.
put
(
key
,
EMPTY_BYTES
)
;
++
putCount
;
}
}
}
RollingWriteBatch
indexRollingWriteBatch
=
indexUpdates
.
get
(
roundedStartTime
)
;
WriteBatch
indexWriteBatch
=
indexRollingWriteBatch
.
getWriteBatch
(
)
;
putCount
+=
writePrimaryFilterEntries
(
indexWriteBatch
,
primaryFilters
,
markerKey
,
EMPTY_BYTES
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
entity
.
getEntityId
(
)
+
+
entity
.
getEntityType
(
)
,
e
)
;
TimelinePutError
error
=
new
TimelinePutError
(
)
;
error
.
setEntityId
(
entity
.
getEntityId
(
)
)
;
error
.
setEntityType
(
entity
.
getEntityType
(
)
)
;
error
.
setErrorCode
(
TimelinePutError
.
IO_EXCEPTION
)
;
response
.
addError
(
error
)
;
}
for
(
EntityIdentifier
relatedEntity
:
relatedEntitiesWithoutStartTimes
)
{
try
{
Long
relatedEntityStartAndInsertTime
=
getAndSetStartTime
(
relatedEntity
.
getId
(
)
,
relatedEntity
.
getType
(
)
,
readReverseOrderedLong
(
revStartTime
,
0
)
,
null
)
;
if
(
relatedEntityStartAndInsertTime
==
null
)
{
throw
new
IOException
(
)
;
}
long
relatedStartTimeLong
=
relatedEntityStartAndInsertTime
;
@
VisibleForTesting
long
evictOldStartTimes
(
long
minStartTime
)
throws
IOException
{
ReadOptions
readOptions
=
new
ReadOptions
(
)
;
readOptions
.
fillCache
(
false
)
;
try
(
DBIterator
iterator
=
starttimedb
.
iterator
(
readOptions
)
)
{
iterator
.
seekToFirst
(
)
;
writeBatch
=
starttimedb
.
createWriteBatch
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
Map
.
Entry
<
byte
[
]
,
byte
[
]
>
current
=
iterator
.
next
(
)
;
byte
[
]
entityKey
=
current
.
getKey
(
)
;
byte
[
]
entityValue
=
current
.
getValue
(
)
;
long
startTime
=
readReverseOrderedLong
(
entityValue
,
0
)
;
if
(
startTime
<
minStartTime
)
{
++
batchSize
;
++
startTimesCount
;
writeBatch
.
delete
(
entityKey
)
;
if
(
batchSize
>=
writeBatchSize
)
{
try
(
DBIterator
iterator
=
starttimedb
.
iterator
(
readOptions
)
)
{
iterator
.
seekToFirst
(
)
;
writeBatch
=
starttimedb
.
createWriteBatch
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
Map
.
Entry
<
byte
[
]
,
byte
[
]
>
current
=
iterator
.
next
(
)
;
byte
[
]
entityKey
=
current
.
getKey
(
)
;
byte
[
]
entityValue
=
current
.
getValue
(
)
;
long
startTime
=
readReverseOrderedLong
(
entityValue
,
0
)
;
if
(
startTime
<
minStartTime
)
{
++
batchSize
;
++
startTimesCount
;
writeBatch
.
delete
(
entityKey
)
;
if
(
batchSize
>=
writeBatchSize
)
{
LOG
.
debug
(
,
batchSize
)
;
starttimedb
.
write
(
writeBatch
)
;
long
startTime
=
readReverseOrderedLong
(
entityValue
,
0
)
;
if
(
startTime
<
minStartTime
)
{
++
batchSize
;
++
startTimesCount
;
writeBatch
.
delete
(
entityKey
)
;
if
(
batchSize
>=
writeBatchSize
)
{
LOG
.
debug
(
,
batchSize
)
;
starttimedb
.
write
(
writeBatch
)
;
LOG
.
debug
(
+
,
batchSize
,
startTimesCount
)
;
IOUtils
.
cleanupWithLogger
(
LOG
,
writeBatch
)
;
writeBatch
=
starttimedb
.
createWriteBatch
(
)
;
batchSize
=
0
;
}
}
++
totalCount
;
}
LOG
.
debug
(
,
batchSize
)
;
starttimedb
.
write
(
writeBatch
)
;
if
(
startTime
<
minStartTime
)
{
++
batchSize
;
++
startTimesCount
;
writeBatch
.
delete
(
entityKey
)
;
if
(
batchSize
>=
writeBatchSize
)
{
LOG
.
debug
(
,
batchSize
)
;
starttimedb
.
write
(
writeBatch
)
;
LOG
.
debug
(
+
,
batchSize
,
startTimesCount
)
;
IOUtils
.
cleanupWithLogger
(
LOG
,
writeBatch
)
;
writeBatch
=
starttimedb
.
createWriteBatch
(
)
;
batchSize
=
0
;
}
}
++
totalCount
;
}
LOG
.
debug
(
,
batchSize
)
;
starttimedb
.
write
(
writeBatch
)
;
LOG
.
debug
(
+
,
batchSize
,
startTimesCount
)
;
private
void
checkVersion
(
)
throws
IOException
{
Version
loadedVersion
=
loadVersion
(
)
;
Options
options
=
new
Options
(
)
;
Path
dbPath
=
new
Path
(
getConfig
(
)
.
get
(
YarnConfiguration
.
TIMELINE_SERVICE_LEVELDB_STATE_STORE_PATH
)
,
DB_NAME
)
;
FileSystem
localFS
=
null
;
try
{
localFS
=
FileSystem
.
getLocal
(
getConfig
(
)
)
;
if
(
!
localFS
.
exists
(
dbPath
)
)
{
if
(
!
localFS
.
mkdirs
(
dbPath
)
)
{
throw
new
IOException
(
+
+
dbPath
)
;
}
localFS
.
setPermission
(
dbPath
,
LEVELDB_DIR_UMASK
)
;
}
}
finally
{
IOUtils
.
cleanupWithLogger
(
LOG
,
localFS
)
;
}
JniDBFactory
factory
=
new
JniDBFactory
(
)
;
try
{
options
.
createIfMissing
(
false
)
;
db
=
factory
.
open
(
new
File
(
dbPath
.
toString
(
)
)
,
options
)
;
throw
new
IOException
(
+
+
dbPath
)
;
}
localFS
.
setPermission
(
dbPath
,
LEVELDB_DIR_UMASK
)
;
}
}
finally
{
IOUtils
.
cleanupWithLogger
(
LOG
,
localFS
)
;
}
JniDBFactory
factory
=
new
JniDBFactory
(
)
;
try
{
options
.
createIfMissing
(
false
)
;
db
=
factory
.
open
(
new
File
(
dbPath
.
toString
(
)
)
,
options
)
;
LOG
.
info
(
+
dbPath
.
toString
(
)
)
;
checkVersion
(
)
;
}
catch
(
NativeDB
.
DBException
e
)
{
if
(
e
.
isNotFound
(
)
||
e
.
getMessage
(
)
.
contains
(
)
)
{
try
{
options
.
createIfMissing
(
true
)
;
db
=
factory
.
open
(
new
File
(
dbPath
.
toString
(
)
)
,
options
)
;
@
Override
public
TimelineServiceState
loadState
(
)
throws
IOException
{
LOG
.
info
(
)
;
TimelineServiceState
state
=
new
TimelineServiceState
(
)
;
int
numKeys
=
loadTokenMasterKeys
(
state
)
;
int
numTokens
=
loadTokens
(
state
)
;
loadLatestSequenceNumber
(
state
)
;
private
void
checkVersion
(
)
throws
IOException
{
Version
loadedVersion
=
loadVersion
(
)
;
public
boolean
checkAccess
(
UserGroupInformation
callerUGI
,
TimelineDomain
domain
)
throws
YarnException
,
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
POST
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
@
Produces
(
{
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
}
)
public
TimelinePutResponse
postEntities
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
TimelineEntities
entities
)
{
init
(
res
)
;
UserGroupInformation
callerUGI
=
getUser
(
req
)
;
if
(
callerUGI
==
null
)
{
String
msg
=
;
@
PUT
@
Path
(
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
@
Produces
(
{
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
}
)
public
TimelinePutResponse
putDomain
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
TimelineDomain
domain
)
{
init
(
res
)
;
UserGroupInformation
callerUGI
=
getUser
(
req
)
;
if
(
callerUGI
==
null
)
{
String
msg
=
;
UserGroupInformation
callerUGI
=
getUser
(
req
)
;
if
(
callerUGI
==
null
)
{
String
msg
=
;
LOG
.
error
(
msg
)
;
throw
new
ForbiddenException
(
msg
)
;
}
domain
.
setOwner
(
callerUGI
.
getShortUserName
(
)
)
;
try
{
timelineDataManager
.
putDomain
(
domain
,
callerUGI
)
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
e
.
getMessage
(
)
,
e
)
;
throw
new
ForbiddenException
(
e
)
;
}
catch
(
RuntimeException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
WebApplicationException
(
e
,
Response
.
Status
.
INTERNAL_SERVER_ERROR
)
;
}
catch
(
IOException
e
)
{
this
.
metrics
.
incrRMMasterSlaveSwitch
(
this
.
rmId
)
;
synchronized
(
this
)
{
for
(
ResourceRequestSet
requestSet
:
this
.
remotePendingAsks
.
values
(
)
)
{
for
(
ResourceRequest
rr
:
requestSet
.
getRRs
(
)
)
{
addResourceRequestToAsk
(
rr
)
;
}
}
this
.
release
.
addAll
(
this
.
remotePendingRelease
)
;
this
.
blacklistAdditions
.
addAll
(
this
.
remoteBlacklistedNodes
)
;
this
.
change
.
putAll
(
this
.
remotePendingChange
)
;
for
(
List
<
SchedulingRequest
>
reqs
:
this
.
remotePendingSchedRequest
.
values
(
)
)
{
this
.
schedulingRequest
.
addAll
(
reqs
)
;
}
}
reRegisterApplicationMaster
(
this
.
amRegistrationRequest
)
;
allocateRequest
.
setResponseId
(
0
)
;
allocateResponse
=
allocate
(
allocateRequest
)
;
return
allocateResponse
;
}
catch
(
Throwable
t
)
{
private
T
getProxyInternal
(
boolean
isFailover
)
{
SubClusterInfo
subClusterInfo
;
T
proxy
=
this
.
current
;
try
{
LOG
.
info
(
,
subClusterId
)
;
subClusterInfo
=
facade
.
getSubCluster
(
subClusterId
,
this
.
flushFacadeCacheForYarnRMAddr
&&
isFailover
)
;
updateRMAddress
(
subClusterInfo
)
;
if
(
this
.
originalUser
==
null
)
{
InetSocketAddress
rmAddress
=
rmProxy
.
getRMAddress
(
conf
,
protocol
)
;
private
T
getProxyInternal
(
boolean
isFailover
)
{
SubClusterInfo
subClusterInfo
;
T
proxy
=
this
.
current
;
try
{
LOG
.
info
(
,
subClusterId
)
;
subClusterInfo
=
facade
.
getSubCluster
(
subClusterId
,
this
.
flushFacadeCacheForYarnRMAddr
&&
isFailover
)
;
updateRMAddress
(
subClusterInfo
)
;
if
(
this
.
originalUser
==
null
)
{
InetSocketAddress
rmAddress
=
rmProxy
.
getRMAddress
(
conf
,
protocol
)
;
LOG
.
info
(
+
,
rmAddress
,
subClusterId
,
protocol
.
getSimpleName
(
)
)
;
proxy
=
createRMProxy
(
rmAddress
)
;
}
else
{
proxy
=
this
.
originalUser
.
doAs
(
new
PrivilegedExceptionAction
<
T
>
(
)
{
@
Override
public
T
run
(
)
throws
IOException
{
InetSocketAddress
rmAddress
=
rmProxy
.
getRMAddress
(
conf
,
protocol
)
;
LOG
.
info
(
,
subClusterId
)
;
subClusterInfo
=
facade
.
getSubCluster
(
subClusterId
,
this
.
flushFacadeCacheForYarnRMAddr
&&
isFailover
)
;
updateRMAddress
(
subClusterInfo
)
;
if
(
this
.
originalUser
==
null
)
{
InetSocketAddress
rmAddress
=
rmProxy
.
getRMAddress
(
conf
,
protocol
)
;
LOG
.
info
(
+
,
rmAddress
,
subClusterId
,
protocol
.
getSimpleName
(
)
)
;
proxy
=
createRMProxy
(
rmAddress
)
;
}
else
{
proxy
=
this
.
originalUser
.
doAs
(
new
PrivilegedExceptionAction
<
T
>
(
)
{
@
Override
public
T
run
(
)
throws
IOException
{
InetSocketAddress
rmAddress
=
rmProxy
.
getRMAddress
(
conf
,
protocol
)
;
LOG
.
info
(
,
rmAddress
,
subClusterId
,
protocol
.
getSimpleName
(
)
,
originalUser
)
;
return
createRMProxy
(
rmAddress
)
;
}
}
)
;
}
}
catch
(
Exception
e
)
{
public
static
SubClusterPolicyConfiguration
loadPolicyConfiguration
(
String
queue
,
Configuration
conf
,
FederationStateStoreFacade
federationFacade
)
{
SubClusterPolicyConfiguration
configuration
=
null
;
if
(
queue
!=
null
)
{
try
{
configuration
=
federationFacade
.
getPolicyConfiguration
(
queue
)
;
}
catch
(
YarnException
e
)
{
LOG
.
warn
(
+
queue
+
+
e
.
getMessage
(
)
)
;
}
}
if
(
configuration
==
null
)
{
LOG
.
info
(
+
,
queue
)
;
queue
=
YarnConfiguration
.
DEFAULT_FEDERATION_POLICY_KEY
;
try
{
configuration
=
federationFacade
.
getPolicyConfiguration
(
queue
)
;
}
catch
(
YarnException
e
)
{
LOG
.
warn
(
+
)
;
}
}
if
(
configuration
==
null
)
{
public
static
FederationAMRMProxyPolicy
loadAMRMPolicy
(
String
queue
,
FederationAMRMProxyPolicy
oldPolicy
,
Configuration
conf
,
FederationStateStoreFacade
federationFacade
,
SubClusterId
homeSubClusterId
)
throws
FederationPolicyInitializationException
{
SubClusterPolicyConfiguration
configuration
=
loadPolicyConfiguration
(
queue
,
conf
,
federationFacade
)
;
FederationPolicyInitializationContext
context
=
new
FederationPolicyInitializationContext
(
configuration
,
federationFacade
.
getSubClusterResolver
(
)
,
federationFacade
,
homeSubClusterId
)
;
@
Override
public
void
notifyOfResponse
(
SubClusterId
subClusterId
,
AllocateResponse
response
)
throws
YarnException
{
if
(
response
.
getAvailableResources
(
)
!=
null
)
{
headroom
.
put
(
subClusterId
,
response
.
getAvailableResources
(
)
)
;
}
catch
(
YarnException
e
)
{
}
if
(
bookkeeper
.
isActiveAndEnabled
(
targetId
)
)
{
bookkeeper
.
addLocalizedNodeRR
(
targetId
,
rr
)
;
continue
;
}
try
{
targetIds
=
resolver
.
getSubClustersForRack
(
rr
.
getResourceName
(
)
)
;
}
catch
(
YarnException
e
)
{
}
if
(
targetIds
!=
null
&&
targetIds
.
size
(
)
>
0
)
{
boolean
hasActive
=
false
;
for
(
SubClusterId
tid
:
targetIds
)
{
if
(
bookkeeper
.
isActiveAndEnabled
(
tid
)
)
{
bookkeeper
.
addRackRR
(
tid
,
rr
)
;
hasActive
=
true
;
}
}
if
(
hasActive
)
{
continue
;
if
(
blackListSubClusters
!=
null
)
{
validSubClusters
.
removeAll
(
blackListSubClusters
)
;
}
try
{
SubClusterId
targetId
=
null
;
ResourceRequest
nodeRequest
=
null
;
ResourceRequest
rackRequest
=
null
;
ResourceRequest
anyRequest
=
null
;
for
(
ResourceRequest
rr
:
rrList
)
{
try
{
targetId
=
resolver
.
getSubClusterForNode
(
rr
.
getResourceName
(
)
)
;
nodeRequest
=
rr
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
.
getLocalizedMessage
(
)
)
;
}
try
{
resolver
.
getSubClustersForRack
(
rr
.
getResourceName
(
)
)
;
nodeRequest
=
rr
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
.
getLocalizedMessage
(
)
)
;
}
try
{
resolver
.
getSubClustersForRack
(
rr
.
getResourceName
(
)
)
;
rackRequest
=
rr
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
.
getLocalizedMessage
(
)
)
;
}
if
(
ResourceRequest
.
isAnyLocation
(
rr
.
getResourceName
(
)
)
)
{
anyRequest
=
rr
;
continue
;
}
}
if
(
nodeRequest
==
null
)
{
throw
new
YarnException
(
)
;
}
if
(
rackRequest
==
null
)
{
throw
new
YarnException
(
)
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
.
getLocalizedMessage
(
)
)
;
}
try
{
resolver
.
getSubClustersForRack
(
rr
.
getResourceName
(
)
)
;
rackRequest
=
rr
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
.
getLocalizedMessage
(
)
)
;
}
if
(
ResourceRequest
.
isAnyLocation
(
rr
.
getResourceName
(
)
)
)
{
anyRequest
=
rr
;
continue
;
}
}
if
(
nodeRequest
==
null
)
{
throw
new
YarnException
(
)
;
}
if
(
rackRequest
==
null
)
{
throw
new
YarnException
(
)
;
}
if
(
anyRequest
==
null
)
{
resolver
.
getSubClustersForRack
(
rr
.
getResourceName
(
)
)
;
rackRequest
=
rr
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
.
getLocalizedMessage
(
)
)
;
}
if
(
ResourceRequest
.
isAnyLocation
(
rr
.
getResourceName
(
)
)
)
{
anyRequest
=
rr
;
continue
;
}
}
if
(
nodeRequest
==
null
)
{
throw
new
YarnException
(
)
;
}
if
(
rackRequest
==
null
)
{
throw
new
YarnException
(
)
;
}
if
(
anyRequest
==
null
)
{
throw
new
YarnException
(
)
;
}
LOG
.
info
(
+
nodeRequest
.
getResourceName
(
)
+
+
rackRequest
.
getResourceName
(
)
+
+
anyRequest
.
getResourceName
(
)
)
;
if
(
validSubClusters
.
contains
(
targetId
)
&&
enabledSCs
.
contains
(
targetId
)
)
{
BufferedReader
reader
=
null
;
try
{
file
=
Paths
.
get
(
fileName
)
;
}
catch
(
InvalidPathException
e
)
{
LOG
.
info
(
,
fileName
)
;
return
;
}
try
{
reader
=
Files
.
newBufferedReader
(
file
,
Charset
.
defaultCharset
(
)
)
;
String
line
=
null
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
String
[
]
tokens
=
line
.
split
(
)
;
if
(
tokens
.
length
==
3
)
{
String
nodeName
=
tokens
[
NODE_NAME_INDEX
]
.
trim
(
)
.
toUpperCase
(
)
;
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
tokens
[
SUBCLUSTER_ID_INDEX
]
.
trim
(
)
)
;
String
rackName
=
tokens
[
RACK_NAME_INDEX
]
.
trim
(
)
.
toUpperCase
(
)
;
try
{
file
=
Paths
.
get
(
fileName
)
;
}
catch
(
InvalidPathException
e
)
{
LOG
.
info
(
,
fileName
)
;
return
;
}
try
{
reader
=
Files
.
newBufferedReader
(
file
,
Charset
.
defaultCharset
(
)
)
;
String
line
=
null
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
String
[
]
tokens
=
line
.
split
(
)
;
if
(
tokens
.
length
==
3
)
{
String
nodeName
=
tokens
[
NODE_NAME_INDEX
]
.
trim
(
)
.
toUpperCase
(
)
;
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
tokens
[
SUBCLUSTER_ID_INDEX
]
.
trim
(
)
)
;
String
rackName
=
tokens
[
RACK_NAME_INDEX
]
.
trim
(
)
.
toUpperCase
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
driverClass
=
conf
.
get
(
YarnConfiguration
.
FEDERATION_STATESTORE_SQL_JDBC_CLASS
,
YarnConfiguration
.
DEFAULT_FEDERATION_STATESTORE_SQL_JDBC_CLASS
)
;
maximumPoolSize
=
conf
.
getInt
(
YarnConfiguration
.
FEDERATION_STATESTORE_SQL_MAXCONNECTIONS
,
YarnConfiguration
.
DEFAULT_FEDERATION_STATESTORE_SQL_MAXCONNECTIONS
)
;
userName
=
conf
.
get
(
YarnConfiguration
.
FEDERATION_STATESTORE_SQL_USERNAME
)
;
password
=
conf
.
get
(
YarnConfiguration
.
FEDERATION_STATESTORE_SQL_PASSWORD
)
;
url
=
conf
.
get
(
YarnConfiguration
.
FEDERATION_STATESTORE_SQL_URL
)
;
try
{
Class
.
forName
(
driverClass
)
;
}
catch
(
ClassNotFoundException
e
)
{
FederationStateStoreUtils
.
logAndThrowException
(
LOG
,
,
e
)
;
}
dataSource
=
new
HikariDataSource
(
)
;
dataSource
.
setDataSourceClassName
(
driverClass
)
;
FederationStateStoreUtils
.
setUsername
(
dataSource
,
userName
)
;
FederationStateStoreUtils
.
setPassword
(
dataSource
,
password
)
;
FederationStateStoreUtils
.
setProperty
(
dataSource
,
FederationStateStoreUtils
.
FEDERATION_STORE_URL
,
url
)
;
dataSource
.
setMaximumPoolSize
(
maximumPoolSize
)
;
SubClusterId
subClusterId
=
request
.
getApplicationHomeSubCluster
(
)
.
getHomeSubCluster
(
)
;
try
{
cstmt
=
getCallableStatement
(
CALL_SP_ADD_APPLICATION_HOME_SUBCLUSTER
)
;
cstmt
.
setString
(
1
,
appId
.
toString
(
)
)
;
cstmt
.
setString
(
2
,
subClusterId
.
getId
(
)
)
;
cstmt
.
registerOutParameter
(
3
,
java
.
sql
.
Types
.
VARCHAR
)
;
cstmt
.
registerOutParameter
(
4
,
java
.
sql
.
Types
.
INTEGER
)
;
long
startTime
=
clock
.
getTime
(
)
;
cstmt
.
executeUpdate
(
)
;
long
stopTime
=
clock
.
getTime
(
)
;
subClusterHome
=
cstmt
.
getString
(
3
)
;
SubClusterId
subClusterIdHome
=
SubClusterId
.
newInstance
(
subClusterHome
)
;
FederationStateStoreClientMetrics
.
succeededStateStoreCall
(
stopTime
-
startTime
)
;
if
(
subClusterId
.
equals
(
subClusterIdHome
)
)
{
if
(
cstmt
.
getInt
(
4
)
==
0
)
{
@
Override
public
GetSubClusterPolicyConfigurationResponse
getPolicyConfiguration
(
GetSubClusterPolicyConfigurationRequest
request
)
throws
YarnException
{
FederationPolicyStoreInputValidator
.
validate
(
request
)
;
CallableStatement
cstmt
=
null
;
SubClusterPolicyConfiguration
subClusterPolicyConfiguration
=
null
;
try
{
cstmt
=
getCallableStatement
(
CALL_SP_GET_POLICY_CONFIGURATION
)
;
cstmt
.
setString
(
1
,
request
.
getQueue
(
)
)
;
cstmt
.
registerOutParameter
(
2
,
java
.
sql
.
Types
.
VARCHAR
)
;
cstmt
.
registerOutParameter
(
3
,
java
.
sql
.
Types
.
VARBINARY
)
;
long
startTime
=
clock
.
getTime
(
)
;
cstmt
.
executeUpdate
(
)
;
long
stopTime
=
clock
.
getTime
(
)
;
if
(
cstmt
.
getString
(
2
)
!=
null
&&
cstmt
.
getBytes
(
3
)
!=
null
)
{
subClusterPolicyConfiguration
=
SubClusterPolicyConfiguration
.
newInstance
(
request
.
getQueue
(
)
,
cstmt
.
getString
(
2
)
,
ByteBuffer
.
wrap
(
cstmt
.
getBytes
(
3
)
)
)
;
public
static
void
failedStateStoreCall
(
)
{
String
methodName
=
Thread
.
currentThread
(
)
.
getStackTrace
(
)
[
2
]
.
getMethodName
(
)
;
MutableCounterLong
methodMetric
=
API_TO_FAILED_CALLS
.
get
(
methodName
)
;
if
(
methodMetric
==
null
)
{
public
static
void
succeededStateStoreCall
(
long
duration
)
{
String
methodName
=
Thread
.
currentThread
(
)
.
getStackTrace
(
)
[
2
]
.
getMethodName
(
)
;
MutableRate
methodMetric
=
API_TO_SUCCESSFUL_CALLS
.
get
(
methodName
)
;
MutableQuantiles
methodQuantileMetric
=
API_TO_QUANTILE_METRICS
.
get
(
methodName
)
;
if
(
methodMetric
==
null
||
methodQuantileMetric
==
null
)
{
public
static
void
logAndThrowException
(
Logger
log
,
String
errMsg
,
Throwable
t
)
throws
YarnException
{
if
(
t
!=
null
)
{
public
static
void
logAndThrowStoreException
(
Logger
log
,
String
errMsg
)
throws
YarnException
{
public
static
void
logAndThrowInvalidInputException
(
Logger
log
,
String
errMsg
)
throws
YarnException
{
public
static
void
logAndThrowRetriableException
(
Logger
log
,
String
errMsg
,
Throwable
t
)
throws
YarnException
{
if
(
t
!=
null
)
{
public
static
void
setProperty
(
HikariDataSource
dataSource
,
String
property
,
String
value
)
{
LOG
.
warn
(
,
e
)
;
}
if
(
subclusters
==
null
)
{
LOG
.
info
(
,
appId
)
;
return
retMap
;
}
for
(
String
scId
:
subclusters
)
{
LOG
.
info
(
,
scId
,
appId
)
;
String
key
=
getRegistryKey
(
appId
,
scId
)
;
try
{
String
tokenString
=
readRegistry
(
this
.
registry
,
this
.
user
,
key
,
true
)
;
if
(
tokenString
==
null
)
{
throw
new
YarnException
(
+
key
)
;
}
Token
<
AMRMTokenIdentifier
>
amrmToken
=
new
Token
<
>
(
)
;
amrmToken
.
decodeFromUrlString
(
tokenString
)
;
amrmToken
.
setService
(
new
Text
(
)
)
;
retMap
.
put
(
scId
,
amrmToken
)
;
public
synchronized
void
removeAppFromRegistry
(
ApplicationId
appId
)
{
Map
<
String
,
Token
<
AMRMTokenIdentifier
>>
subClusterTokenMap
=
this
.
appSubClusterTokenMap
.
get
(
appId
)
;
updateBlacklist
(
blackList
,
opportContext
)
;
opportContext
.
addToOutstandingReqs
(
oppResourceReqs
)
;
Set
<
String
>
nodeBlackList
=
new
HashSet
<
>
(
opportContext
.
getBlacklist
(
)
)
;
Set
<
String
>
allocatedNodes
=
new
HashSet
<
>
(
)
;
List
<
Container
>
allocatedContainers
=
new
ArrayList
<
>
(
)
;
boolean
continueLoop
=
true
;
while
(
continueLoop
)
{
continueLoop
=
false
;
List
<
Map
<
Resource
,
List
<
Allocation
>>>
allocations
=
new
ArrayList
<
>
(
)
;
for
(
SchedulerRequestKey
schedulerKey
:
opportContext
.
getOutstandingOpReqs
(
)
.
descendingKeySet
(
)
)
{
int
remAllocs
=
-
1
;
int
maxAllocationsPerAMHeartbeat
=
getMaxAllocationsPerAMHeartbeat
(
)
;
if
(
maxAllocationsPerAMHeartbeat
>
0
)
{
remAllocs
=
maxAllocationsPerAMHeartbeat
-
allocatedContainers
.
size
(
)
-
getTotalAllocations
(
allocations
)
;
if
(
remAllocs
<=
0
)
{
private
Map
<
Resource
,
List
<
Allocation
>>
allocate
(
long
rmIdentifier
,
OpportunisticContainerContext
appContext
,
SchedulerRequestKey
schedKey
,
ApplicationAttemptId
appAttId
,
String
userName
,
Set
<
String
>
blackList
,
Set
<
String
>
allocatedNodes
,
int
maxAllocations
)
throws
YarnException
{
Map
<
Resource
,
List
<
Allocation
>>
containers
=
new
HashMap
<
>
(
)
;
for
(
EnrichedResourceRequest
enrichedAsk
:
appContext
.
getOutstandingOpReqs
(
)
.
get
(
schedKey
)
.
values
(
)
)
{
int
remainingAllocs
=
-
1
;
if
(
maxAllocations
>
0
)
{
int
totalAllocated
=
0
;
for
(
List
<
Allocation
>
allocs
:
containers
.
values
(
)
)
{
totalAllocated
+=
allocs
.
size
(
)
;
}
remainingAllocs
=
maxAllocations
-
totalAllocated
;
if
(
remainingAllocs
<=
0
)
{
LOG
.
info
(
+
,
getMaxAllocationsPerAMHeartbeat
(
)
)
;
break
;
}
}
allocateContainersInternal
(
rmIdentifier
,
appContext
.
getAppParams
(
)
,
appContext
.
getContainerIdGenerator
(
)
,
blackList
,
allocatedNodes
,
appAttId
,
appContext
.
getNodeMap
(
)
,
userName
,
containers
,
enrichedAsk
,
remainingAllocs
)
;
ResourceRequest
anyAsk
=
enrichedAsk
.
getRequest
(
)
;
if
(
!
containers
.
isEmpty
(
)
)
{
return
;
}
ResourceRequest
anyAsk
=
enrichedAsk
.
getRequest
(
)
;
int
toAllocate
=
anyAsk
.
getNumContainers
(
)
-
(
allocations
.
isEmpty
(
)
?
0
:
allocations
.
get
(
anyAsk
.
getCapability
(
)
)
.
size
(
)
)
;
toAllocate
=
Math
.
min
(
toAllocate
,
appParams
.
getMaxAllocationsPerSchedulerKeyPerRound
(
)
)
;
if
(
maxAllocations
>=
0
)
{
toAllocate
=
Math
.
min
(
maxAllocations
,
toAllocate
)
;
}
int
numAllocated
=
0
;
int
loopIndex
=
OFF_SWITCH_LOOP
;
if
(
enrichedAsk
.
getNodeMap
(
)
.
size
(
)
>
0
)
{
loopIndex
=
NODE_LOCAL_LOOP
;
}
while
(
numAllocated
<
toAllocate
)
{
Collection
<
RemoteNode
>
nodeCandidates
=
findNodeCandidates
(
loopIndex
,
allNodes
,
blacklist
,
allocatedNodes
,
enrichedAsk
)
;
for
(
RemoteNode
rNode
:
nodeCandidates
)
{
String
rNodeHost
=
rNode
.
getNodeId
(
)
.
getHost
(
)
;
if
(
blacklist
.
contains
(
rNodeHost
)
)
{
int
loopIndex
=
OFF_SWITCH_LOOP
;
if
(
enrichedAsk
.
getNodeMap
(
)
.
size
(
)
>
0
)
{
loopIndex
=
NODE_LOCAL_LOOP
;
}
while
(
numAllocated
<
toAllocate
)
{
Collection
<
RemoteNode
>
nodeCandidates
=
findNodeCandidates
(
loopIndex
,
allNodes
,
blacklist
,
allocatedNodes
,
enrichedAsk
)
;
for
(
RemoteNode
rNode
:
nodeCandidates
)
{
String
rNodeHost
=
rNode
.
getNodeId
(
)
.
getHost
(
)
;
if
(
blacklist
.
contains
(
rNodeHost
)
)
{
LOG
.
info
(
+
+
rNodeHost
+
)
;
continue
;
}
String
location
=
ResourceRequest
.
ANY
;
if
(
loopIndex
==
NODE_LOCAL_LOOP
)
{
if
(
enrichedAsk
.
getNodeMap
(
)
.
containsKey
(
rNodeHost
)
)
{
location
=
rNodeHost
;
}
else
{
}
String
location
=
ResourceRequest
.
ANY
;
if
(
loopIndex
==
NODE_LOCAL_LOOP
)
{
if
(
enrichedAsk
.
getNodeMap
(
)
.
containsKey
(
rNodeHost
)
)
{
location
=
rNodeHost
;
}
else
{
continue
;
}
}
else
if
(
allocatedNodes
.
contains
(
rNodeHost
)
)
{
LOG
.
info
(
,
rNodeHost
)
;
continue
;
}
if
(
loopIndex
==
RACK_LOCAL_LOOP
)
{
if
(
enrichedAsk
.
getRackMap
(
)
.
containsKey
(
rNode
.
getRackName
(
)
)
)
{
location
=
rNode
.
getRackName
(
)
;
}
else
{
continue
;
}
}
Container
container
=
createContainer
(
rmIdentifier
,
appParams
,
idCounter
,
id
,
userName
,
allocations
,
location
,
anyAsk
,
rNode
)
;
SchedulerRequestKey
schedulerKey
=
SchedulerRequestKey
.
create
(
request
)
;
Map
<
Resource
,
EnrichedResourceRequest
>
reqMap
=
outstandingOpReqs
.
get
(
schedulerKey
)
;
if
(
reqMap
==
null
)
{
reqMap
=
new
HashMap
<
>
(
)
;
outstandingOpReqs
.
put
(
schedulerKey
,
reqMap
)
;
}
EnrichedResourceRequest
eReq
=
reqMap
.
get
(
request
.
getCapability
(
)
)
;
if
(
eReq
==
null
)
{
eReq
=
new
EnrichedResourceRequest
(
request
)
;
reqMap
.
put
(
request
.
getCapability
(
)
,
eReq
)
;
}
if
(
ResourceRequest
.
isAnyLocation
(
request
.
getResourceName
(
)
)
)
{
eReq
.
getRequest
(
)
.
setResourceName
(
ResourceRequest
.
ANY
)
;
eReq
.
getRequest
(
)
.
setNumContainers
(
request
.
getNumContainers
(
)
)
;
}
else
{
eReq
.
addLocation
(
request
.
getResourceName
(
)
,
request
.
getNumContainers
(
)
)
;
}
if
(
ResourceRequest
.
isAnyLocation
(
request
.
getResourceName
(
)
)
)
{
@
Override
public
byte
[
]
createPassword
(
ContainerTokenIdentifier
identifier
)
{
protected
byte
[
]
retrievePasswordInternal
(
ContainerTokenIdentifier
identifier
,
MasterKeyData
masterKey
)
throws
org
.
apache
.
hadoop
.
security
.
token
.
SecretManager
.
InvalidToken
{
@
Override
protected
byte
[
]
createPassword
(
NMTokenIdentifier
identifier
)
{
protected
byte
[
]
retrivePasswordInternal
(
NMTokenIdentifier
identifier
,
MasterKeyData
masterKey
)
{
LOG
.
warn
(
,
addressList
.
size
(
)
)
;
for
(
final
String
uamId
:
addressList
)
{
completionService
.
submit
(
new
Callable
<
KillApplicationResponse
>
(
)
{
@
Override
public
KillApplicationResponse
call
(
)
throws
Exception
{
try
{
LOG
.
info
(
+
uamId
+
+
appIdMap
.
get
(
uamId
)
)
;
return
unmanagedAppMasterMap
.
remove
(
uamId
)
.
forceKillApplication
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
)
;
return
null
;
}
}
}
)
;
}
for
(
int
i
=
0
;
i
<
addressList
.
size
(
)
;
++
i
)
{
try
{
Future
<
KillApplicationResponse
>
future
=
completionService
.
take
(
)
;
future
.
get
(
)
;
public
RegisterApplicationMasterResponse
registerApplicationMaster
(
RegisterApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
this
.
registerRequest
=
request
;
LOG
.
info
(
,
this
.
applicationId
)
;
RegisterApplicationMasterResponse
response
=
this
.
rmProxyRelayer
.
registerApplicationMaster
(
this
.
registerRequest
)
;
this
.
heartbeatHandler
.
resetLastResponseId
(
)
;
for
(
Container
container
:
response
.
getContainersFromPreviousAttempts
(
)
)
{
SubmitApplicationRequest
submitRequest
=
this
.
recordFactory
.
newRecordInstance
(
SubmitApplicationRequest
.
class
)
;
ApplicationSubmissionContext
context
=
this
.
recordFactory
.
newRecordInstance
(
ApplicationSubmissionContext
.
class
)
;
context
.
setApplicationId
(
appId
)
;
context
.
setApplicationName
(
APP_NAME
+
+
appNameSuffix
)
;
if
(
StringUtils
.
isBlank
(
this
.
queueName
)
)
{
context
.
setQueue
(
this
.
conf
.
get
(
DEFAULT_QUEUE_CONFIG
,
YarnConfiguration
.
DEFAULT_QUEUE_NAME
)
)
;
}
else
{
context
.
setQueue
(
this
.
queueName
)
;
}
ContainerLaunchContext
amContainer
=
this
.
recordFactory
.
newRecordInstance
(
ContainerLaunchContext
.
class
)
;
Resource
resource
=
BuilderUtils
.
newResource
(
1024
,
1
)
;
context
.
setResource
(
resource
)
;
context
.
setAMContainerSpec
(
amContainer
)
;
submitRequest
.
setApplicationSubmissionContext
(
context
)
;
context
.
setUnmanagedAM
(
true
)
;
context
.
setKeepContainersAcrossApplicationAttempts
(
this
.
keepContainersAcrossApplicationAttempts
)
;
public
static
Credentials
parseCredentials
(
ContainerLaunchContext
launchContext
)
throws
IOException
{
Credentials
credentials
=
new
Credentials
(
)
;
ByteBuffer
tokens
=
launchContext
.
getTokens
(
)
;
if
(
tokens
!=
null
)
{
DataInputByteBuffer
buf
=
new
DataInputByteBuffer
(
)
;
tokens
.
rewind
(
)
;
buf
.
reset
(
tokens
)
;
credentials
.
readTokenStorageStream
(
buf
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
for
(
Token
<
?
extends
TokenIdentifier
>
tk
:
credentials
.
getAllTokens
(
)
)
{
}
catch
(
IllegalArgumentException
e
)
{
puts
(
+
attemptid
)
;
return
;
}
UserGroupInformation
callerUGI
=
getCallerUGI
(
)
;
ApplicationAttemptReport
appAttemptReport
;
try
{
final
GetApplicationAttemptReportRequest
request
=
GetApplicationAttemptReportRequest
.
newInstance
(
appAttemptId
)
;
if
(
callerUGI
==
null
)
{
appAttemptReport
=
getApplicationAttemptReport
(
request
)
;
}
else
{
appAttemptReport
=
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
ApplicationAttemptReport
>
(
)
{
@
Override
public
ApplicationAttemptReport
run
(
)
throws
Exception
{
return
getApplicationAttemptReport
(
request
)
;
}
}
)
;
}
}
catch
(
Exception
e
)
{
}
catch
(
Exception
e
)
{
puts
(
+
aid
)
;
return
;
}
UserGroupInformation
callerUGI
=
getCallerUGI
(
)
;
ApplicationReport
appReport
;
try
{
final
GetApplicationReportRequest
request
=
GetApplicationReportRequest
.
newInstance
(
appID
)
;
if
(
callerUGI
==
null
)
{
appReport
=
getApplicationReport
(
request
)
;
}
else
{
appReport
=
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
ApplicationReport
>
(
)
{
@
Override
public
ApplicationReport
run
(
)
throws
Exception
{
return
getApplicationReport
(
request
)
;
}
}
)
;
}
}
catch
(
Exception
e
)
{
}
catch
(
Exception
e
)
{
String
message
=
+
appID
+
;
LOG
.
error
(
message
,
e
)
;
html
.
p
(
)
.
__
(
message
)
.
__
(
)
;
return
;
}
if
(
appReport
==
null
)
{
puts
(
+
aid
)
;
return
;
}
AppInfo
app
=
new
AppInfo
(
appReport
)
;
setTitle
(
join
(
,
aid
)
)
;
Collection
<
ApplicationAttemptReport
>
attempts
;
try
{
final
GetApplicationAttemptsRequest
request
=
GetApplicationAttemptsRequest
.
newInstance
(
appID
)
;
if
(
callerUGI
==
null
)
{
attempts
=
getApplicationAttemptsReport
(
request
)
;
if
(
callerUGI
==
null
)
{
containerReport
=
getContainerReport
(
request
)
;
}
else
{
containerReport
=
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
ContainerReport
>
(
)
{
@
Override
public
ContainerReport
run
(
)
throws
Exception
{
ContainerReport
report
=
null
;
if
(
request
.
getContainerId
(
)
!=
null
)
{
try
{
report
=
getContainerReport
(
request
)
;
}
catch
(
ContainerNotFoundException
ex
)
{
LOG
.
warn
(
ex
.
getMessage
(
)
)
;
}
}
return
report
;
}
}
)
;
}
}
catch
(
Exception
e
)
{
String
message
=
+
appAttemptReport
.
getApplicationAttemptId
(
)
+
;
}
catch
(
IllegalArgumentException
e
)
{
puts
(
+
containerid
)
;
return
;
}
UserGroupInformation
callerUGI
=
getCallerUGI
(
)
;
ContainerReport
containerReport
=
null
;
try
{
final
GetContainerReportRequest
request
=
GetContainerReportRequest
.
newInstance
(
containerId
)
;
if
(
callerUGI
==
null
)
{
containerReport
=
getContainerReport
(
request
)
;
}
else
{
containerReport
=
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
ContainerReport
>
(
)
{
@
Override
public
ContainerReport
run
(
)
throws
Exception
{
return
getContainerReport
(
request
)
;
}
}
)
;
}
}
catch
(
Exception
e
)
{
appInfo
=
appInfoProvider
.
getApp
(
req
,
builder
.
getAppId
(
)
,
clusterId
)
;
}
catch
(
Exception
ex
)
{
LOG
.
warn
(
,
ex
)
;
return
getContainerLogMeta
(
builder
.
build
(
)
,
false
)
;
}
if
(
Apps
.
isApplicationFinalState
(
appInfo
.
getAppState
(
)
)
)
{
return
getContainerLogMeta
(
builder
.
build
(
)
,
false
)
;
}
if
(
LogWebServiceUtils
.
isRunningState
(
appInfo
.
getAppState
(
)
)
)
{
String
appOwner
=
appInfo
.
getUser
(
)
;
builder
.
setAppOwner
(
appOwner
)
;
WrappedLogMetaRequest
request
=
builder
.
build
(
)
;
String
nodeHttpAddress
=
null
;
if
(
nmId
!=
null
&&
!
nmId
.
isEmpty
(
)
)
{
try
{
nodeHttpAddress
=
getNMWebAddressFromRM
(
nmId
)
;
}
catch
(
Exception
ex
)
{
final
long
length
=
LogWebServiceUtils
.
parseLongParam
(
size
)
;
ApplicationId
appId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
BasicAppInfo
appInfo
;
try
{
appInfo
=
appInfoProvider
.
getApp
(
req
,
appId
.
toString
(
)
,
clusterId
)
;
}
catch
(
Exception
ex
)
{
LOG
.
warn
(
,
ex
)
;
return
LogWebServiceUtils
.
sendStreamOutputResponse
(
factory
,
appId
,
null
,
null
,
containerIdStr
,
filename
,
format
,
length
,
false
)
;
}
String
appOwner
=
appInfo
.
getUser
(
)
;
if
(
Apps
.
isApplicationFinalState
(
appInfo
.
getAppState
(
)
)
)
{
return
LogWebServiceUtils
.
sendStreamOutputResponse
(
factory
,
appId
,
appOwner
,
null
,
containerIdStr
,
filename
,
format
,
length
,
false
)
;
}
if
(
LogWebServiceUtils
.
isRunningState
(
appInfo
.
getAppState
(
)
)
)
{
String
nodeHttpAddress
=
null
;
if
(
nmId
!=
null
&&
!
nmId
.
isEmpty
(
)
)
{
try
{
private
static
void
init
(
)
{
factory
=
new
LogAggregationFileControllerFactory
(
yarnConf
)
;
base
=
JOINER
.
join
(
WebAppUtils
.
getHttpSchemePrefix
(
yarnConf
)
,
WebAppUtils
.
getTimelineReaderWebAppURLWithoutScheme
(
yarnConf
)
,
RESOURCE_URI_STR_V2
)
;
defaultClusterid
=
yarnConf
.
get
(
YarnConfiguration
.
RM_CLUSTER_ID
,
YarnConfiguration
.
DEFAULT_RM_CLUSTER_ID
)
;
@
VisibleForTesting
protected
TimelineEntity
getEntity
(
String
path
,
MultivaluedMap
<
String
,
String
>
params
)
throws
IOException
{
ClientResponse
resp
=
getClient
(
)
.
resource
(
base
)
.
path
(
path
)
.
queryParams
(
params
)
.
accept
(
MediaType
.
APPLICATION_JSON
)
.
type
(
MediaType
.
APPLICATION_JSON
)
.
get
(
ClientResponse
.
class
)
;
if
(
resp
==
null
||
resp
.
getStatusInfo
(
)
.
getStatusCode
(
)
!=
ClientResponse
.
Status
.
OK
.
getStatusCode
(
)
)
{
String
msg
=
+
(
(
resp
==
null
)
?
:
+
+
resp
.
getStatus
(
)
+
+
resp
.
getEntity
(
String
.
class
)
)
;
@
Override
public
RegisterApplicationMasterResponse
registerApplicationMaster
(
RegisterApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
validateRunning
(
)
;
ApplicationAttemptId
attemptId
=
getAppIdentifier
(
)
;
for
(
ContainerId
containerId
:
applicationContainerIdMap
.
get
(
appId
)
)
{
containersFromPreviousAttempt
.
add
(
Container
.
newInstance
(
containerId
,
null
,
null
,
null
,
null
,
null
)
)
;
}
}
else
if
(
!
shouldReRegisterNext
)
{
throw
new
InvalidApplicationMasterRequestException
(
AMRMClientUtils
.
APP_ALREADY_REGISTERED_MESSAGE
)
;
}
}
else
{
applicationContainerIdMap
.
put
(
appId
,
new
ArrayList
<
ContainerId
>
(
)
)
;
}
}
shouldReRegisterNext
=
false
;
synchronized
(
registerSyncObj
)
{
registerSyncObj
.
notifyAll
(
)
;
if
(
request
.
getRpcPort
(
)
>
1000
)
{
LOG
.
info
(
)
;
try
{
registerSyncObj
.
wait
(
)
;
LOG
.
info
(
)
;
}
catch
(
InterruptedException
e
)
{
@
Override
public
FinishApplicationMasterResponse
finishApplicationMaster
(
FinishApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
validateRunning
(
)
;
ApplicationAttemptId
attemptId
=
getAppIdentifier
(
)
;
@
SuppressWarnings
(
)
@
Override
public
AllocateResponse
allocate
(
AllocateRequest
request
)
throws
YarnException
,
IOException
{
validateRunning
(
)
;
ApplicationAttemptId
attemptId
=
getAppIdentifier
(
)
;
ApplicationAttemptId
attemptId
=
getAppIdentifier
(
)
;
LOG
.
info
(
+
attemptId
)
;
ApplicationId
appId
=
attemptId
.
getApplicationId
(
)
;
if
(
shouldReRegisterNext
)
{
String
message
=
;
LOG
.
warn
(
message
)
;
throw
new
ApplicationMasterNotRegisteredException
(
message
)
;
}
synchronized
(
allocateSyncObj
)
{
if
(
shouldWaitForSyncNextAllocate
)
{
shouldWaitForSyncNextAllocate
=
false
;
LOG
.
info
(
)
;
try
{
allocateSyncObj
.
wait
(
)
;
LOG
.
info
(
)
;
}
catch
(
InterruptedException
e
)
{
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
try
{
SubClusterRegisterRequest
request
=
null
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
null
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
null
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterIdNull
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterIdNull
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterIdInvalid
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterIdInvalid
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
@
Test
public
void
testValidateSubClusterRegisterRequestTimestamp
(
)
{
SubClusterInfo
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeatNegative
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTimeNegative
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
@
Test
public
void
testValidateSubClusterRegisterRequestAddress
(
)
{
SubClusterInfo
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
addressNull
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
addressEmpty
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
addressEmpty
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressNull
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressNull
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressEmpty
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressEmpty
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressNull
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressNull
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressNull
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressEmpty
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressEmpty
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
addressNull
,
lastHeartBeat
,
stateNew
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
@
Test
public
void
testValidateSubClusterRegisterRequestAddressInvalid
(
)
{
SubClusterInfo
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
addressWrong
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressWrong
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressWrong
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressWrong
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressWrong
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
addressWrong
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
addressWrong
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
addressWrongPort
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
addressWrongPort
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
addressWrongPort
,
clientRMServiceAddress
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressWrongPort
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
addressWrongPort
,
rmAdminServiceAddress
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMServiceAddress
,
clientRMServiceAddress
,
addressWrongPort
,
rmWebServiceAddress
,
lastHeartBeat
,
stateNull
,
lastStartTime
,
capability
)
;
try
{
SubClusterRegisterRequest
request
=
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
try
{
SubClusterDeregisterRequest
request
=
null
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterDeregisterRequest
request
=
SubClusterDeregisterRequest
.
newInstance
(
subClusterIdNull
,
stateLost
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterDeregisterRequest
request
=
SubClusterDeregisterRequest
.
newInstance
(
subClusterIdNull
,
stateLost
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterDeregisterRequest
request
=
SubClusterDeregisterRequest
.
newInstance
(
subClusterIdInvalid
,
stateLost
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
try
{
SubClusterDeregisterRequest
request
=
SubClusterDeregisterRequest
.
newInstance
(
subClusterIdNull
,
stateLost
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterDeregisterRequest
request
=
SubClusterDeregisterRequest
.
newInstance
(
subClusterIdInvalid
,
stateLost
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterDeregisterRequest
request
=
SubClusterDeregisterRequest
.
newInstance
(
subClusterIdInvalid
,
stateLost
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterDeregisterRequest
request
=
SubClusterDeregisterRequest
.
newInstance
(
subClusterId
,
stateNull
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
null
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterIdNull
,
lastHeartBeat
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterIdNull
,
lastHeartBeat
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterIdInvalid
,
lastHeartBeat
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterIdNull
,
lastHeartBeat
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterIdInvalid
,
lastHeartBeat
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterIdInvalid
,
lastHeartBeat
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterId
,
lastHeartBeat
,
stateNull
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterId
,
lastHeartBeat
,
stateNull
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterId
,
lastHeartBeatNegative
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterId
,
lastHeartBeatNegative
,
stateLost
,
capability
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterId
,
lastHeartBeat
,
stateLost
,
capabilityNull
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
try
{
GetSubClusterInfoRequest
request
=
null
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
GetSubClusterInfoRequest
request
=
GetSubClusterInfoRequest
.
newInstance
(
subClusterIdNull
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
GetSubClusterInfoRequest
request
=
GetSubClusterInfoRequest
.
newInstance
(
subClusterIdNull
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
try
{
GetSubClusterInfoRequest
request
=
GetSubClusterInfoRequest
.
newInstance
(
subClusterIdInvalid
)
;
FederationMembershipStateStoreInputValidator
.
validate
(
request
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
null
;
try
{
AddApplicationHomeSubClusterRequest
request
=
AddApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
appId
,
subClusterIdNull
)
;
try
{
AddApplicationHomeSubClusterRequest
request
=
AddApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
AddApplicationHomeSubClusterRequest
request
=
AddApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
appId
,
subClusterIdNull
)
;
try
{
AddApplicationHomeSubClusterRequest
request
=
AddApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
appId
,
subClusterIdInvalid
)
;
try
{
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
null
;
try
{
UpdateApplicationHomeSubClusterRequest
request
=
UpdateApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
appId
,
subClusterIdNull
)
;
try
{
UpdateApplicationHomeSubClusterRequest
request
=
UpdateApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
UpdateApplicationHomeSubClusterRequest
request
=
UpdateApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
appId
,
subClusterIdNull
)
;
try
{
UpdateApplicationHomeSubClusterRequest
request
=
UpdateApplicationHomeSubClusterRequest
.
newInstance
(
applicationHomeSubCluster
)
;
FederationApplicationHomeSubClusterStoreInputValidator
.
validate
(
request
)
;
Assert
.
fail
(
)
;
}
catch
(
FederationStateStoreInvalidInputException
e
)
{
LOG
.
info
(
e
.
getMessage
(
)
)
;
Assert
.
assertTrue
(
e
.
getMessage
(
)
.
startsWith
(
)
)
;
}
applicationHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
appId
,
subClusterIdInvalid
)
;
try
{
@
Test
public
void
testRoundRobinSimpleAllocation
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
1
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
ResourceRequest
.
ANY
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
2
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
ResourceRequest
.
ANY
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
3
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
ResourceRequest
.
ANY
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testNodeLocalAllocation
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
1
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
ResourceRequest
.
ANY
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
2
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
2
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
2
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
ResourceRequest
.
ANY
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
3
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
3
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
3
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
ResourceRequest
.
ANY
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testNodeLocalAllocationSameSchedKey
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
2
)
.
numContainers
(
2
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
2
)
.
numContainers
(
2
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
,
ResourceRequest
.
newBuilder
(
)
.
allocationRequestId
(
2
)
.
numContainers
(
2
)
.
priority
(
PRIORITY_NORMAL
)
.
resourceName
(
ResourceRequest
.
ANY
)
.
capability
(
CAPABILITY_1GB
)
.
relaxLocality
(
true
)
.
executionType
(
ExecutionType
.
OPPORTUNISTIC
)
.
build
(
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testOffSwitchAllocationWhenNoNodeOrRack
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeat
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
3
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
3
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
3
,
true
,
null
,
OPPORTUNISTIC_REQ
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeat
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
3
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
3
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
3
,
true
,
null
,
OPPORTUNISTIC_REQ
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
Assert
.
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeatDifferentSchedKey
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
final
ExecutionTypeRequest
oppRequest
=
ExecutionTypeRequest
.
newInstance
(
ExecutionType
.
OPPORTUNISTIC
,
true
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
1
)
,
,
CAPABILITY_1GB
,
1
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
2
)
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
3
)
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeatDifferentSchedKey
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
final
ExecutionTypeRequest
oppRequest
=
ExecutionTypeRequest
.
newInstance
(
ExecutionType
.
OPPORTUNISTIC
,
true
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
1
)
,
,
CAPABILITY_1GB
,
1
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
2
)
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
3
)
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
Assert
.
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeatDifferentSchedKey
(
)
throws
Exception
{
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
new
ArrayList
<
>
(
)
,
new
ArrayList
<
>
(
)
)
;
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
final
ExecutionTypeRequest
oppRequest
=
ExecutionTypeRequest
.
newInstance
(
ExecutionType
.
OPPORTUNISTIC
,
true
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
1
)
,
,
CAPABILITY_1GB
,
1
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
2
)
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
Priority
.
newInstance
(
3
)
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
Assert
.
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
Assert
.
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testAllocationLatencyMetrics
(
)
throws
Exception
{
oppCntxt
=
spy
(
oppCntxt
)
;
OpportunisticSchedulerMetrics
metrics
=
mock
(
OpportunisticSchedulerMetrics
.
class
)
;
when
(
oppCntxt
.
getOppSchedulerMetrics
(
)
)
.
thenReturn
(
metrics
)
;
ResourceBlacklistRequest
blacklistRequest
=
ResourceBlacklistRequest
.
newInstance
(
Collections
.
emptyList
(
)
,
Collections
.
emptyList
(
)
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
,
ResourceRequest
.
newInstance
(
PRIORITY_NORMAL
,
,
CAPABILITY_1GB
,
2
,
true
,
null
,
OPPORTUNISTIC_REQ
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
oppCntxt
.
updateNodeList
(
Arrays
.
asList
(
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
,
RemoteNode
.
newInstance
(
NodeId
.
newInstance
(
,
1234
)
,
,
)
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
blacklistRequest
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
try
{
launchUAM
(
attemptId
)
;
registerApplicationMaster
(
RegisterApplicationMasterRequest
.
newInstance
(
null
,
1001
,
null
)
,
attemptId
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
,
e
)
;
}
}
}
)
;
Object
syncObj
=
MockResourceManagerFacade
.
getRegisterSyncObj
(
)
;
synchronized
(
syncObj
)
{
LOG
.
info
(
)
;
registerAMThread
.
start
(
)
;
try
{
LOG
.
info
(
)
;
syncObj
.
wait
(
)
;
LOG
.
info
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
,
containerId
)
;
return
ExitCode
.
TERMINATED
.
getExitCode
(
)
;
}
String
pid
=
ProcessIdFileReader
.
getProcessId
(
pidPath
)
;
if
(
pid
==
null
)
{
throw
new
IOException
(
+
containerId
)
;
}
LOG
.
info
(
,
containerId
,
pid
)
;
ContainerLivenessContext
livenessContext
=
new
ContainerLivenessContext
.
Builder
(
)
.
setContainer
(
container
)
.
setUser
(
user
)
.
setPid
(
pid
)
.
build
(
)
;
while
(
isContainerAlive
(
livenessContext
)
)
{
Thread
.
sleep
(
1000
)
;
}
final
int
sleepMsec
=
100
;
int
msecLeft
=
this
.
exitCodeFileTimeout
;
String
exitCodeFile
=
ContainerLaunch
.
getExitCodeFile
(
pidPath
.
toString
(
)
)
;
File
file
=
new
File
(
exitCodeFile
)
;
while
(
!
file
.
exists
(
)
&&
msecLeft
>=
0
)
{
if
(
!
isContainerActive
(
containerId
)
)
{
protected
void
logOutput
(
String
output
)
{
String
shExecOutput
=
output
;
if
(
shExecOutput
!=
null
)
{
for
(
String
str
:
shExecOutput
.
split
(
)
)
{
public
void
cleanupBeforeRelaunch
(
Container
container
)
throws
IOException
,
InterruptedException
{
if
(
container
.
getLocalizedResources
(
)
!=
null
)
{
Map
<
Path
,
Path
>
symLinks
=
resolveSymLinks
(
container
.
getLocalizedResources
(
)
,
container
.
getUser
(
)
)
;
for
(
Map
.
Entry
<
Path
,
Path
>
symLink
:
symLinks
.
entrySet
(
)
)
{
InetSocketAddress
nmAddr
=
ctx
.
getNmAddr
(
)
;
String
user
=
ctx
.
getUser
(
)
;
String
appId
=
ctx
.
getAppId
(
)
;
String
locId
=
ctx
.
getLocId
(
)
;
LocalDirsHandlerService
dirsHandler
=
ctx
.
getDirsHandler
(
)
;
List
<
String
>
localDirs
=
dirsHandler
.
getLocalDirs
(
)
;
List
<
String
>
logDirs
=
dirsHandler
.
getLogDirs
(
)
;
createUserLocalDirs
(
localDirs
,
user
)
;
createUserCacheDirs
(
localDirs
,
user
)
;
createAppDirs
(
localDirs
,
user
,
appId
)
;
createAppLogDirs
(
appId
,
logDirs
,
user
)
;
Path
appStorageDir
=
getWorkingDir
(
localDirs
,
user
,
appId
)
;
String
tokenFn
=
String
.
format
(
TOKEN_FILE_NAME_FMT
,
locId
)
;
Path
tokenDst
=
new
Path
(
appStorageDir
,
tokenFn
)
;
copyFile
(
nmPrivateContainerTokensPath
,
tokenDst
,
user
)
;
LocalDirsHandlerService
dirsHandler
=
ctx
.
getDirsHandler
(
)
;
List
<
String
>
localDirs
=
dirsHandler
.
getLocalDirs
(
)
;
List
<
String
>
logDirs
=
dirsHandler
.
getLogDirs
(
)
;
createUserLocalDirs
(
localDirs
,
user
)
;
createUserCacheDirs
(
localDirs
,
user
)
;
createAppDirs
(
localDirs
,
user
,
appId
)
;
createAppLogDirs
(
appId
,
logDirs
,
user
)
;
Path
appStorageDir
=
getWorkingDir
(
localDirs
,
user
,
appId
)
;
String
tokenFn
=
String
.
format
(
TOKEN_FILE_NAME_FMT
,
locId
)
;
Path
tokenDst
=
new
Path
(
appStorageDir
,
tokenFn
)
;
copyFile
(
nmPrivateContainerTokensPath
,
tokenDst
,
user
)
;
LOG
.
info
(
,
nmPrivateContainerTokensPath
,
tokenDst
)
;
FileContext
localizerFc
=
FileContext
.
getFileContext
(
lfs
.
getDefaultFileSystem
(
)
,
getConf
(
)
)
;
localizerFc
.
setUMask
(
lfs
.
getUMask
(
)
)
;
localizerFc
.
setWorkingDirectory
(
appStorageDir
)
;
copyFile
(
nmPrivateTokensPath
,
tokenDst
,
user
)
;
if
(
nmPrivateKeystorePath
!=
null
)
{
Path
keystoreDst
=
new
Path
(
containerWorkDir
,
ContainerLaunch
.
KEYSTORE_FILE
)
;
copyFile
(
nmPrivateKeystorePath
,
keystoreDst
,
user
)
;
}
if
(
nmPrivateTruststorePath
!=
null
)
{
Path
truststoreDst
=
new
Path
(
containerWorkDir
,
ContainerLaunch
.
TRUSTSTORE_FILE
)
;
copyFile
(
nmPrivateTruststorePath
,
truststoreDst
,
user
)
;
}
Path
launchDst
=
new
Path
(
containerWorkDir
,
ContainerLaunch
.
CONTAINER_SCRIPT
)
;
copyFile
(
nmPrivateContainerScriptPath
,
launchDst
,
user
)
;
LocalWrapperScriptBuilder
sb
=
getLocalWrapperScriptBuilder
(
containerIdStr
,
containerWorkDir
)
;
if
(
Shell
.
WINDOWS
&&
sb
.
getWrapperScriptPath
(
)
.
toString
(
)
.
length
(
)
>
WIN_MAX_PATH
)
{
throw
new
IOException
(
String
.
format
(
+
+
,
sb
.
getWrapperScriptPath
(
)
,
WIN_MAX_PATH
,
YarnConfiguration
.
NM_LOCAL_DIRS
)
)
;
}
Path
pidFile
=
getPidFilePath
(
containerId
)
;
if
(
pidFile
!=
null
)
{
sb
.
writeLocalWrapperScript
(
launchDst
,
pidFile
)
;
copyFile
(
nmPrivateContainerScriptPath
,
launchDst
,
user
)
;
LocalWrapperScriptBuilder
sb
=
getLocalWrapperScriptBuilder
(
containerIdStr
,
containerWorkDir
)
;
if
(
Shell
.
WINDOWS
&&
sb
.
getWrapperScriptPath
(
)
.
toString
(
)
.
length
(
)
>
WIN_MAX_PATH
)
{
throw
new
IOException
(
String
.
format
(
+
+
,
sb
.
getWrapperScriptPath
(
)
,
WIN_MAX_PATH
,
YarnConfiguration
.
NM_LOCAL_DIRS
)
)
;
}
Path
pidFile
=
getPidFilePath
(
containerId
)
;
if
(
pidFile
!=
null
)
{
sb
.
writeLocalWrapperScript
(
launchDst
,
pidFile
)
;
}
else
{
LOG
.
info
(
,
containerIdStr
)
;
return
ExitCode
.
TERMINATED
.
getExitCode
(
)
;
}
Shell
.
CommandExecutor
shExec
=
null
;
try
{
setScriptExecutable
(
launchDst
,
user
)
;
setScriptExecutable
(
sb
.
getWrapperScriptPath
(
)
,
user
)
;
shExec
=
buildCommandExecutor
(
sb
.
getWrapperScriptPath
(
)
.
toString
(
)
,
containerIdStr
,
user
,
pidFile
,
container
.
getResource
(
)
,
new
File
(
containerWorkDir
.
toUri
(
)
.
getPath
(
)
)
,
container
.
getLaunchContext
(
)
.
getEnvironment
(
)
)
;
protected
CommandExecutor
buildCommandExecutor
(
String
wrapperScriptPath
,
String
containerIdStr
,
String
user
,
Path
pidFile
,
Resource
resource
,
File
workDir
,
Map
<
String
,
String
>
environment
)
{
String
[
]
command
=
getRunCommand
(
wrapperScriptPath
,
containerIdStr
,
user
,
pidFile
,
this
.
getConf
(
)
,
resource
)
;
@
Override
public
boolean
signalContainer
(
ContainerSignalContext
ctx
)
throws
IOException
{
String
user
=
ctx
.
getUser
(
)
;
String
pid
=
ctx
.
getPid
(
)
;
Signal
signal
=
ctx
.
getSignal
(
)
;
@
Override
public
void
deleteAsUser
(
DeletionAsUserContext
ctx
)
throws
IOException
,
InterruptedException
{
Path
subDir
=
ctx
.
getSubDir
(
)
;
List
<
Path
>
baseDirs
=
ctx
.
getBasedirs
(
)
;
if
(
baseDirs
==
null
||
baseDirs
.
size
(
)
==
0
)
{
void
createUserCacheDirs
(
List
<
String
>
localDirs
,
String
user
)
throws
IOException
{
public
void
delete
(
DeletionTask
deletionTask
)
{
if
(
debugDelay
!=
-
1
)
{
switch
(
entry
.
getValue
(
)
.
cause
)
{
case
DISK_FULL
:
fullDirs
.
add
(
entry
.
getKey
(
)
)
;
break
;
case
OTHER
:
errorDirs
.
add
(
entry
.
getKey
(
)
)
;
break
;
default
:
LOG
.
warn
(
entry
.
getValue
(
)
.
cause
+
)
;
break
;
}
directoryErrorInfo
.
put
(
entry
.
getKey
(
)
,
errorInformation
)
;
if
(
preCheckGoodDirs
.
contains
(
dir
)
)
{
LOG
.
warn
(
+
dir
+
+
errorInformation
.
message
+
)
;
setChanged
=
true
;
numFailures
++
;
}
}
for
(
String
dir
:
allLocalDirs
)
{
if
(
!
dirsFailedCheck
.
containsKey
(
dir
)
)
{
localDirs
.
add
(
dir
)
;
@
Override
public
void
init
(
Context
context
)
throws
IOException
{
Configuration
conf
=
super
.
getConf
(
)
;
this
.
nmContext
=
context
;
try
{
PrivilegedOperation
checkSetupOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
CHECK_SETUP
)
;
PrivilegedOperationExecutor
privilegedOperationExecutor
=
getPrivilegedOperationExecutor
(
)
;
privilegedOperationExecutor
.
executePrivilegedOperation
(
checkSetupOp
,
false
)
;
}
catch
(
PrivilegedOperationException
e
)
{
int
exitCode
=
e
.
getExitCode
(
)
;
LOG
.
warn
(
,
exitCode
,
e
)
;
throw
new
IOException
(
+
+
exitCode
+
,
e
)
;
}
try
{
resourceHandlerChain
=
ResourceHandlerModule
.
getConfiguredResourceHandlerChain
(
conf
,
nmContext
)
;
LOG
.
debug
(
,
(
resourceHandlerChain
!=
null
)
)
;
if
(
resourceHandlerChain
!=
null
)
{
try
{
PrivilegedOperation
checkSetupOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
CHECK_SETUP
)
;
PrivilegedOperationExecutor
privilegedOperationExecutor
=
getPrivilegedOperationExecutor
(
)
;
privilegedOperationExecutor
.
executePrivilegedOperation
(
checkSetupOp
,
false
)
;
}
catch
(
PrivilegedOperationException
e
)
{
int
exitCode
=
e
.
getExitCode
(
)
;
LOG
.
warn
(
,
exitCode
,
e
)
;
throw
new
IOException
(
+
+
exitCode
+
,
e
)
;
}
try
{
resourceHandlerChain
=
ResourceHandlerModule
.
getConfiguredResourceHandlerChain
(
conf
,
nmContext
)
;
LOG
.
debug
(
,
(
resourceHandlerChain
!=
null
)
)
;
if
(
resourceHandlerChain
!=
null
)
{
LOG
.
debug
(
,
resourceHandlerChain
)
;
resourceHandlerChain
.
bootstrap
(
conf
)
;
}
}
catch
(
ResourceHandlerException
e
)
{
throw
new
IOException
(
+
+
exitCode
+
,
e
)
;
}
try
{
resourceHandlerChain
=
ResourceHandlerModule
.
getConfiguredResourceHandlerChain
(
conf
,
nmContext
)
;
LOG
.
debug
(
,
(
resourceHandlerChain
!=
null
)
)
;
if
(
resourceHandlerChain
!=
null
)
{
LOG
.
debug
(
,
resourceHandlerChain
)
;
resourceHandlerChain
.
bootstrap
(
conf
)
;
}
}
catch
(
ResourceHandlerException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
IOException
(
)
;
}
try
{
if
(
linuxContainerRuntime
==
null
)
{
LinuxContainerRuntime
runtime
=
new
DelegatingLinuxContainerRuntime
(
)
;
runtime
.
initialize
(
conf
,
nmContext
)
;
this
.
linuxContainerRuntime
=
runtime
;
resourceOps
.
add
(
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
ADD_PID_TO_CGROUP
,
resourcesOptions
)
)
;
for
(
PrivilegedOperation
op
:
ops
)
{
switch
(
op
.
getOperationType
(
)
)
{
case
ADD_PID_TO_CGROUP
:
resourceOps
.
add
(
op
)
;
break
;
case
TC_MODIFY_STATE
:
tcCommandFile
=
op
.
getArguments
(
)
.
get
(
0
)
;
break
;
case
ADD_NUMA_PARAMS
:
numaArgs
=
op
.
getArguments
(
)
;
break
;
default
:
LOG
.
warn
(
,
op
.
getOperationType
(
)
)
;
}
}
if
(
resourceOps
.
size
(
)
>
1
)
{
try
{
PrivilegedOperation
operation
=
PrivilegedOperationExecutor
.
squashCGroupOperations
(
resourceOps
)
;
resourcesOptions
=
operation
.
getArguments
(
)
.
get
(
0
)
;
}
catch
(
PrivilegedOperationException
e
)
{
case
ADD_PID_TO_CGROUP
:
resourceOps
.
add
(
op
)
;
break
;
case
TC_MODIFY_STATE
:
tcCommandFile
=
op
.
getArguments
(
)
.
get
(
0
)
;
break
;
case
ADD_NUMA_PARAMS
:
numaArgs
=
op
.
getArguments
(
)
;
break
;
default
:
LOG
.
warn
(
,
op
.
getOperationType
(
)
)
;
}
}
if
(
resourceOps
.
size
(
)
>
1
)
{
try
{
PrivilegedOperation
operation
=
PrivilegedOperationExecutor
.
squashCGroupOperations
(
resourceOps
)
;
resourcesOptions
=
operation
.
getArguments
(
)
.
get
(
0
)
;
}
catch
(
PrivilegedOperationException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
ResourceHandlerException
(
)
;
}
}
}
}
}
catch
(
ResourceHandlerException
e
)
{
@
Override
public
void
deleteAsUser
(
DeletionAsUserContext
ctx
)
{
String
user
=
ctx
.
getUser
(
)
;
Path
dir
=
ctx
.
getSubDir
(
)
;
List
<
Path
>
baseDirs
=
ctx
.
getBasedirs
(
)
;
verifyUsernamePattern
(
user
)
;
String
runAsUser
=
getRunAsUser
(
user
)
;
String
dirString
=
dir
==
null
?
:
dir
.
toUri
(
)
.
getPath
(
)
;
PrivilegedOperation
deleteAsUserOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
DELETE_AS_USER
,
(
String
)
null
)
;
deleteAsUserOp
.
appendArgs
(
runAsUser
,
user
,
Integer
.
toString
(
PrivilegedOperation
.
RunAsUserCommand
.
DELETE_AS_USER
.
getValue
(
)
)
,
dirString
)
;
List
<
String
>
pathsToDelete
=
new
ArrayList
<
String
>
(
)
;
if
(
baseDirs
==
null
||
baseDirs
.
size
(
)
==
0
)
{
String
user
=
ctx
.
getUser
(
)
;
Path
dir
=
ctx
.
getSubDir
(
)
;
List
<
Path
>
baseDirs
=
ctx
.
getBasedirs
(
)
;
verifyUsernamePattern
(
user
)
;
String
runAsUser
=
getRunAsUser
(
user
)
;
String
dirString
=
dir
==
null
?
:
dir
.
toUri
(
)
.
getPath
(
)
;
PrivilegedOperation
deleteAsUserOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
DELETE_AS_USER
,
(
String
)
null
)
;
deleteAsUserOp
.
appendArgs
(
runAsUser
,
user
,
Integer
.
toString
(
PrivilegedOperation
.
RunAsUserCommand
.
DELETE_AS_USER
.
getValue
(
)
)
,
dirString
)
;
List
<
String
>
pathsToDelete
=
new
ArrayList
<
String
>
(
)
;
if
(
baseDirs
==
null
||
baseDirs
.
size
(
)
==
0
)
{
LOG
.
info
(
,
dir
)
;
pathsToDelete
.
add
(
dirString
)
;
}
else
{
for
(
Path
baseDir
:
baseDirs
)
{
Path
del
=
dir
==
null
?
baseDir
:
new
Path
(
baseDir
,
dir
)
;
if
(
baseDirs
==
null
||
baseDirs
.
size
(
)
==
0
)
{
LOG
.
info
(
,
dir
)
;
pathsToDelete
.
add
(
dirString
)
;
}
else
{
for
(
Path
baseDir
:
baseDirs
)
{
Path
del
=
dir
==
null
?
baseDir
:
new
Path
(
baseDir
,
dir
)
;
LOG
.
info
(
,
del
)
;
pathsToDelete
.
add
(
del
.
toString
(
)
)
;
deleteAsUserOp
.
appendArgs
(
baseDir
.
toUri
(
)
.
getPath
(
)
)
;
}
}
try
{
Configuration
conf
=
super
.
getConf
(
)
;
PrivilegedOperationExecutor
privilegedOperationExecutor
=
getPrivilegedOperationExecutor
(
)
;
privilegedOperationExecutor
.
executePrivilegedOperation
(
deleteAsUserOp
,
false
)
;
}
catch
(
PrivilegedOperationException
e
)
{
int
exitCode
=
e
.
getExitCode
(
)
;
@
Override
protected
File
[
]
readDirAsUser
(
String
user
,
Path
dir
)
{
List
<
File
>
files
=
new
ArrayList
<
>
(
)
;
PrivilegedOperation
listAsUserOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
LIST_AS_USER
,
(
String
)
null
)
;
String
runAsUser
=
getRunAsUser
(
user
)
;
String
dirString
=
;
if
(
dir
!=
null
)
{
dirString
=
dir
.
toUri
(
)
.
getPath
(
)
;
}
listAsUserOp
.
appendArgs
(
runAsUser
,
user
,
Integer
.
toString
(
PrivilegedOperation
.
RunAsUserCommand
.
LIST_AS_USER
.
getValue
(
)
)
,
dirString
)
;
try
{
PrivilegedOperationExecutor
privOpExecutor
=
getPrivilegedOperationExecutor
(
)
;
String
results
=
privOpExecutor
.
executePrivilegedOperation
(
listAsUserOp
,
true
)
;
for
(
String
file
:
results
.
split
(
)
)
{
if
(
!
file
.
startsWith
(
)
)
{
files
.
add
(
new
File
(
new
File
(
dirString
)
,
file
)
)
;
}
}
}
catch
(
PrivilegedOperationException
e
)
{
public
void
removeDockerContainer
(
String
containerId
)
{
try
{
PrivilegedOperationExecutor
privOpExecutor
=
PrivilegedOperationExecutor
.
getInstance
(
super
.
getConf
(
)
)
;
if
(
DockerCommandExecutor
.
isRemovable
(
DockerCommandExecutor
.
getContainerStatus
(
containerId
,
privOpExecutor
,
nmContext
)
)
)
{
@
VisibleForTesting
void
postComplete
(
final
ContainerId
containerId
)
{
try
{
if
(
resourceHandlerChain
!=
null
)
{
private
void
logDiskStatus
(
boolean
newDiskFailure
,
boolean
diskTurnedGood
)
{
if
(
newDiskFailure
)
{
String
report
=
getDisksHealthReport
(
false
)
;
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
ContainerId
containerId
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
private
void
stopRecoveryStore
(
)
throws
IOException
{
if
(
null
!=
nmStore
)
{
nmStore
.
stop
(
)
;
if
(
null
!=
context
)
{
if
(
context
.
getDecommissioned
(
)
&&
nmStore
.
canRecover
(
)
)
{
LOG
.
info
(
)
;
Configuration
conf
=
getConfig
(
)
;
Path
recoveryRoot
=
new
Path
(
conf
.
get
(
YarnConfiguration
.
NM_RECOVERY_DIR
)
)
;
if
(
this
.
resyncingWithRM
.
getAndSet
(
true
)
)
{
}
else
{
new
Thread
(
)
{
@
Override
public
void
run
(
)
{
try
{
if
(
!
rmWorkPreservingRestartEnabled
)
{
LOG
.
info
(
)
;
containerManager
.
cleanupContainersOnNMResync
(
)
;
if
(
context
.
getKnownCollectors
(
)
!=
null
)
{
context
.
getKnownCollectors
(
)
.
clear
(
)
;
}
}
else
{
LOG
.
info
(
)
;
reregisterCollectors
(
)
;
}
(
(
NodeStatusUpdaterImpl
)
nodeStatusUpdater
)
.
rebootNodeStatusUpdaterAndRegisterWithRM
(
)
;
}
catch
(
YarnRuntimeException
e
)
{
private
void
initAndStartNodeManager
(
Configuration
conf
,
boolean
hasToReboot
)
{
try
{
if
(
!
Shell
.
WINDOWS
)
{
if
(
!
Shell
.
checkIsBashSupported
(
)
)
{
String
message
=
+
;
private
void
initAndStartNodeManager
(
Configuration
conf
,
boolean
hasToReboot
)
{
try
{
if
(
!
Shell
.
WINDOWS
)
{
if
(
!
Shell
.
checkIsBashSupported
(
)
)
{
String
message
=
+
;
LOG
.
error
(
message
)
;
throw
new
YarnRuntimeException
(
message
)
;
}
}
if
(
hasToReboot
&&
null
!=
nodeManagerShutdownHook
)
{
ShutdownHookManager
.
get
(
)
.
removeShutdownHook
(
nodeManagerShutdownHook
)
;
}
nodeManagerShutdownHook
=
new
CompositeServiceShutdownHook
(
this
)
;
ShutdownHookManager
.
get
(
)
.
addShutdownHook
(
nodeManagerShutdownHook
,
SHUTDOWN_HOOK_PRIORITY
)
;
this
.
shouldExitOnShutdownEvent
=
true
;
this
.
init
(
conf
)
;
this
.
start
(
)
;
}
catch
(
Throwable
t
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
this
.
totalResource
=
NodeManagerHardwareUtils
.
getNodeResources
(
conf
)
;
long
memoryMb
=
totalResource
.
getMemorySize
(
)
;
float
vMemToPMem
=
conf
.
getFloat
(
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
,
YarnConfiguration
.
DEFAULT_NM_VMEM_PMEM_RATIO
)
;
long
virtualMemoryMb
=
(
long
)
Math
.
ceil
(
memoryMb
*
vMemToPMem
)
;
int
virtualCores
=
totalResource
.
getVirtualCores
(
)
;
updateConfiguredResourcesViaPlugins
(
totalResource
)
;
long
physicalMemoryMb
=
memoryMb
;
int
physicalCores
=
virtualCores
;
ResourceCalculatorPlugin
rcp
=
ResourceCalculatorPlugin
.
getNodeResourceMonitorPlugin
(
conf
)
;
if
(
rcp
!=
null
)
{
physicalMemoryMb
=
rcp
.
getPhysicalMemorySize
(
)
/
(
1024
*
1024
)
;
physicalCores
=
rcp
.
getNumProcessors
(
)
;
}
this
.
physicalResource
=
Resource
.
newInstance
(
physicalMemoryMb
,
physicalCores
)
;
this
.
tokenKeepAliveEnabled
=
isTokenKeepAliveEnabled
(
conf
)
;
this
.
tokenRemovalDelayMs
=
conf
.
getInt
(
YarnConfiguration
.
RM_NM_EXPIRY_INTERVAL_MS
,
YarnConfiguration
.
DEFAULT_RM_NM_EXPIRY_INTERVAL_MS
)
;
this
.
minimumResourceManagerVersion
=
conf
.
get
(
YarnConfiguration
.
NM_RESOURCEMANAGER_MINIMUM_VERSION
,
YarnConfiguration
.
DEFAULT_NM_RESOURCEMANAGER_MINIMUM_VERSION
)
;
nodeLabelsHandler
=
createNMNodeLabelsHandler
(
nodeLabelsProvider
)
;
nodeAttributesHandler
=
createNMNodeAttributesHandler
(
nodeAttributesProvider
)
;
durationToTrackStoppedContainers
=
conf
.
getLong
(
YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS
,
600000
)
;
if
(
durationToTrackStoppedContainers
<
0
)
{
String
message
=
+
YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS
+
+
;
synchronized
(
shutdownMonitor
)
{
if
(
this
.
isStopped
)
{
LOG
.
info
(
)
;
return
;
}
this
.
isStopped
=
true
;
sendOutofBandHeartBeat
(
)
;
try
{
statusUpdater
.
join
(
)
;
registerWithRM
(
)
;
statusUpdater
=
new
Thread
(
statusUpdaterRunnable
,
)
;
this
.
isStopped
=
false
;
statusUpdater
.
start
(
)
;
LOG
.
info
(
)
;
}
catch
(
Exception
e
)
{
String
errorMessage
=
;
@
VisibleForTesting
protected
void
registerWithRM
(
)
throws
YarnException
,
IOException
{
RegisterNodeManagerResponse
regNMResponse
;
Set
<
NodeLabel
>
nodeLabels
=
nodeLabelsHandler
.
getNodeLabelsForRegistration
(
)
;
Set
<
NodeAttribute
>
nodeAttributes
=
nodeAttributesHandler
.
getNodeAttributesForRegistration
(
)
;
synchronized
(
this
.
context
)
{
List
<
NMContainerStatus
>
containerReports
=
getNMContainerStatuses
(
)
;
NodeStatus
nodeStatus
=
getNodeStatus
(
0
)
;
RegisterNodeManagerRequest
request
=
RegisterNodeManagerRequest
.
newInstance
(
nodeId
,
httpPort
,
totalResource
,
nodeManagerVersionId
,
containerReports
,
getRunningApplications
(
)
,
nodeLabels
,
physicalResource
,
nodeAttributes
,
nodeStatus
)
;
if
(
containerReports
!=
null
&&
!
containerReports
.
isEmpty
(
)
)
{
throw
new
YarnRuntimeException
(
+
message
)
;
}
if
(
VersionUtil
.
compareVersions
(
rmVersion
,
minimumResourceManagerVersion
)
<
0
)
{
String
message
=
+
rmVersion
+
+
+
minimumResourceManagerVersion
;
throw
new
YarnRuntimeException
(
+
+
message
)
;
}
}
this
.
registeredWithRM
=
true
;
MasterKey
masterKey
=
regNMResponse
.
getContainerTokenMasterKey
(
)
;
if
(
masterKey
!=
null
)
{
this
.
context
.
getContainerTokenSecretManager
(
)
.
setMasterKey
(
masterKey
)
;
}
masterKey
=
regNMResponse
.
getNMTokenMasterKey
(
)
;
if
(
masterKey
!=
null
)
{
this
.
context
.
getNMTokenSecretManager
(
)
.
setMasterKey
(
masterKey
)
;
}
StringBuilder
successfullRegistrationMsg
=
new
StringBuilder
(
)
;
successfullRegistrationMsg
.
append
(
)
.
append
(
this
.
nodeId
)
;
Resource
newResource
=
regNMResponse
.
getResource
(
)
;
if
(
newResource
!=
null
)
{
@
VisibleForTesting
protected
NodeStatus
getNodeStatus
(
int
responseId
)
throws
IOException
{
NodeHealthStatus
nodeHealthStatus
=
this
.
context
.
getNodeHealthStatus
(
)
;
nodeHealthStatus
.
setHealthReport
(
healthChecker
.
getHealthReport
(
)
)
;
nodeHealthStatus
.
setIsNodeHealthy
(
healthChecker
.
isHealthy
(
)
)
;
nodeHealthStatus
.
setLastHealthReportTime
(
healthChecker
.
getLastHealthReportTime
(
)
)
;
@
VisibleForTesting
protected
List
<
ContainerStatus
>
getContainerStatuses
(
)
throws
IOException
{
List
<
ContainerStatus
>
containerStatuses
=
new
ArrayList
<
ContainerStatus
>
(
)
;
for
(
Container
container
:
this
.
context
.
getContainers
(
)
.
values
(
)
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
ApplicationId
applicationId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
ContainerStatus
containerStatus
=
container
.
cloneAndGetContainerStatus
(
)
;
if
(
containerStatus
.
getState
(
)
==
ContainerState
.
COMPLETE
)
{
if
(
isApplicationStopped
(
applicationId
)
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
ApplicationId
applicationId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
ContainerStatus
containerStatus
=
container
.
cloneAndGetContainerStatus
(
)
;
if
(
containerStatus
.
getState
(
)
==
ContainerState
.
COMPLETE
)
{
if
(
isApplicationStopped
(
applicationId
)
)
{
LOG
.
debug
(
,
applicationId
,
containerId
)
;
context
.
getContainers
(
)
.
remove
(
containerId
)
;
pendingCompletedContainers
.
put
(
containerId
,
containerStatus
)
;
}
else
{
if
(
!
isContainerRecentlyStopped
(
containerId
)
)
{
pendingCompletedContainers
.
put
(
containerId
,
containerStatus
)
;
}
}
addCompletedContainer
(
containerId
)
;
}
else
{
containerStatuses
.
add
(
containerStatus
)
;
}
}
containerStatuses
.
addAll
(
pendingCompletedContainers
.
values
(
)
)
;
synchronized
(
recentlyStoppedContainers
)
{
long
currentTime
=
System
.
currentTimeMillis
(
)
;
Iterator
<
Entry
<
ContainerId
,
Long
>>
i
=
recentlyStoppedContainers
.
entrySet
(
)
.
iterator
(
)
;
while
(
i
.
hasNext
(
)
)
{
Entry
<
ContainerId
,
Long
>
mapEntry
=
i
.
next
(
)
;
ContainerId
cid
=
mapEntry
.
getKey
(
)
;
if
(
mapEntry
.
getValue
(
)
>=
currentTime
)
{
break
;
}
if
(
!
context
.
getContainers
(
)
.
containsKey
(
cid
)
)
{
ApplicationId
appId
=
cid
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
if
(
isApplicationStopped
(
appId
)
)
{
i
.
remove
(
)
;
try
{
context
.
getNMStateStore
(
)
.
removeContainer
(
cid
)
;
}
catch
(
IOException
e
)
{
@
Override
protected
String
[
]
getRunCommand
(
String
command
,
String
groupId
,
String
userName
,
Path
pidFile
,
Configuration
conf
)
{
File
f
=
new
File
(
command
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
void
copyFile
(
Path
src
,
Path
dst
,
String
owner
)
throws
IOException
{
@
Override
protected
void
createDir
(
Path
dirPath
,
FsPermission
perms
,
boolean
createParent
,
String
owner
)
throws
IOException
{
perms
=
new
FsPermission
(
DIR_PERM
)
;
@
Override
protected
void
setScriptExecutable
(
Path
script
,
String
owner
)
throws
IOException
{
@
Override
public
Path
localizeClasspathJar
(
Path
jarPath
,
Path
target
,
String
owner
)
throws
IOException
{
String
appId
=
ctx
.
getAppId
(
)
;
String
locId
=
ctx
.
getLocId
(
)
;
LocalDirsHandlerService
dirsHandler
=
ctx
.
getDirsHandler
(
)
;
List
<
String
>
localDirs
=
dirsHandler
.
getLocalDirs
(
)
;
List
<
String
>
logDirs
=
dirsHandler
.
getLogDirs
(
)
;
Path
classpathJarPrivateDir
=
dirsHandler
.
getLocalPathForWrite
(
ResourceLocalizationService
.
NM_PRIVATE_DIR
)
;
createUserLocalDirs
(
localDirs
,
user
)
;
createUserCacheDirs
(
localDirs
,
user
)
;
createAppDirs
(
localDirs
,
user
,
appId
)
;
createAppLogDirs
(
appId
,
logDirs
,
user
)
;
Path
appStorageDir
=
getWorkingDir
(
localDirs
,
user
,
appId
)
;
String
tokenFn
=
String
.
format
(
ContainerExecutor
.
TOKEN_FILE_NAME_FMT
,
locId
)
;
Path
tokenDst
=
new
Path
(
appStorageDir
,
tokenFn
)
;
copyFile
(
nmPrivateContainerTokensPath
,
tokenDst
,
user
)
;
File
cwdApp
=
new
File
(
appStorageDir
.
toString
(
)
)
;
public
void
recover
(
)
throws
IOException
{
LOG
.
info
(
)
;
RecoveredAMRMProxyState
state
=
this
.
nmContext
.
getNMStateStore
(
)
.
loadAMRMProxyState
(
)
;
this
.
secretManager
.
recover
(
state
)
;
LOG
.
info
(
,
state
.
getAppContexts
(
)
.
size
(
)
)
;
for
(
Map
.
Entry
<
ApplicationAttemptId
,
Map
<
String
,
byte
[
]
>>
entry
:
state
.
getAppContexts
(
)
.
entrySet
(
)
)
{
ApplicationAttemptId
attemptId
=
entry
.
getKey
(
)
;
Token
<
AMRMTokenIdentifier
>
amrmToken
=
null
;
for
(
Map
.
Entry
<
String
,
byte
[
]
>
contextEntry
:
entry
.
getValue
(
)
.
entrySet
(
)
)
{
if
(
contextEntry
.
getKey
(
)
.
equals
(
NMSS_USER_KEY
)
)
{
user
=
new
String
(
contextEntry
.
getValue
(
)
,
)
;
}
else
if
(
contextEntry
.
getKey
(
)
.
equals
(
NMSS_AMRMTOKEN_KEY
)
)
{
amrmToken
=
new
Token
<
>
(
)
;
amrmToken
.
decodeFromUrlString
(
new
String
(
contextEntry
.
getValue
(
)
,
)
)
;
amrmToken
.
setService
(
new
Text
(
)
)
;
}
}
if
(
amrmToken
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
if
(
user
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
Token
<
AMRMTokenIdentifier
>
localToken
=
this
.
secretManager
.
createAndGetAMRMToken
(
attemptId
)
;
Credentials
amCred
=
null
;
for
(
Container
container
:
this
.
nmContext
.
getContainers
(
)
.
values
(
)
)
{
}
else
if
(
contextEntry
.
getKey
(
)
.
equals
(
NMSS_AMRMTOKEN_KEY
)
)
{
amrmToken
=
new
Token
<
>
(
)
;
amrmToken
.
decodeFromUrlString
(
new
String
(
contextEntry
.
getValue
(
)
,
)
)
;
amrmToken
.
setService
(
new
Text
(
)
)
;
}
}
if
(
amrmToken
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
if
(
user
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
Token
<
AMRMTokenIdentifier
>
localToken
=
this
.
secretManager
.
createAndGetAMRMToken
(
attemptId
)
;
Credentials
amCred
=
null
;
for
(
Container
container
:
this
.
nmContext
.
getContainers
(
)
.
values
(
)
)
{
LOG
.
debug
(
,
container
.
getContainerId
(
)
)
;
if
(
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
equals
(
attemptId
)
&&
container
.
getContainerTokenIdentifier
(
)
!=
null
)
{
LOG
.
debug
(
,
container
.
getContainerTokenIdentifier
(
)
.
getContainerType
(
)
)
;
if
(
container
.
getContainerTokenIdentifier
(
)
.
getContainerType
(
)
==
ContainerType
.
APPLICATION_MASTER
)
{
amrmToken
.
decodeFromUrlString
(
new
String
(
contextEntry
.
getValue
(
)
,
)
)
;
amrmToken
.
setService
(
new
Text
(
)
)
;
}
}
if
(
amrmToken
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
if
(
user
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
Token
<
AMRMTokenIdentifier
>
localToken
=
this
.
secretManager
.
createAndGetAMRMToken
(
attemptId
)
;
Credentials
amCred
=
null
;
for
(
Container
container
:
this
.
nmContext
.
getContainers
(
)
.
values
(
)
)
{
LOG
.
debug
(
,
container
.
getContainerId
(
)
)
;
if
(
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
equals
(
attemptId
)
&&
container
.
getContainerTokenIdentifier
(
)
!=
null
)
{
LOG
.
debug
(
,
container
.
getContainerTokenIdentifier
(
)
.
getContainerType
(
)
)
;
if
(
container
.
getContainerTokenIdentifier
(
)
.
getContainerType
(
)
==
ContainerType
.
APPLICATION_MASTER
)
{
LOG
.
info
(
,
container
.
getContainerId
(
)
,
(
container
.
getCredentials
(
)
!=
null
)
)
;
amCred
=
container
.
getCredentials
(
)
;
}
if
(
amrmToken
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
if
(
user
==
null
)
{
throw
new
IOException
(
+
attemptId
)
;
}
Token
<
AMRMTokenIdentifier
>
localToken
=
this
.
secretManager
.
createAndGetAMRMToken
(
attemptId
)
;
Credentials
amCred
=
null
;
for
(
Container
container
:
this
.
nmContext
.
getContainers
(
)
.
values
(
)
)
{
LOG
.
debug
(
,
container
.
getContainerId
(
)
)
;
if
(
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
equals
(
attemptId
)
&&
container
.
getContainerTokenIdentifier
(
)
!=
null
)
{
LOG
.
debug
(
,
container
.
getContainerTokenIdentifier
(
)
.
getContainerType
(
)
)
;
if
(
container
.
getContainerTokenIdentifier
(
)
.
getContainerType
(
)
==
ContainerType
.
APPLICATION_MASTER
)
{
LOG
.
info
(
,
container
.
getContainerId
(
)
,
(
container
.
getCredentials
(
)
!=
null
)
)
;
amCred
=
container
.
getCredentials
(
)
;
}
}
}
if
(
amCred
==
null
)
{
LOG
.
error
(
+
,
attemptId
)
;
@
Override
public
RegisterApplicationMasterResponse
registerApplicationMaster
(
RegisterApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
long
startTime
=
clock
.
getTime
(
)
;
try
{
RequestInterceptorChainWrapper
pipeline
=
authorizeAndGetInterceptorChain
(
)
;
@
Override
public
RegisterApplicationMasterResponse
registerApplicationMaster
(
RegisterApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
long
startTime
=
clock
.
getTime
(
)
;
try
{
RequestInterceptorChainWrapper
pipeline
=
authorizeAndGetInterceptorChain
(
)
;
LOG
.
info
(
+
+
request
.
getHost
(
)
+
+
request
.
getRpcPort
(
)
+
+
request
.
getTrackingUrl
(
)
+
+
pipeline
.
getApplicationAttemptId
(
)
)
;
RegisterApplicationMasterResponse
response
=
pipeline
.
getRootInterceptor
(
)
.
registerApplicationMaster
(
request
)
;
long
endTime
=
clock
.
getTime
(
)
;
this
.
metrics
.
succeededRegisterAMRequests
(
endTime
-
startTime
)
;
@
Override
public
FinishApplicationMasterResponse
finishApplicationMaster
(
FinishApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
long
startTime
=
clock
.
getTime
(
)
;
try
{
RequestInterceptorChainWrapper
pipeline
=
authorizeAndGetInterceptorChain
(
)
;
@
Override
public
FinishApplicationMasterResponse
finishApplicationMaster
(
FinishApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
long
startTime
=
clock
.
getTime
(
)
;
try
{
RequestInterceptorChainWrapper
pipeline
=
authorizeAndGetInterceptorChain
(
)
;
LOG
.
info
(
,
pipeline
.
getApplicationAttemptId
(
)
,
request
.
getTrackingUrl
(
)
)
;
FinishApplicationMasterResponse
response
=
pipeline
.
getRootInterceptor
(
)
.
finishApplicationMaster
(
request
)
;
long
endTime
=
clock
.
getTime
(
)
;
this
.
metrics
.
succeededFinishAMRequests
(
endTime
-
startTime
)
;
@
Override
public
AllocateResponse
allocate
(
AllocateRequest
request
)
throws
YarnException
,
IOException
{
long
startTime
=
clock
.
getTime
(
)
;
try
{
AMRMTokenIdentifier
amrmTokenIdentifier
=
YarnServerSecurityUtils
.
authorizeRequest
(
)
;
RequestInterceptorChainWrapper
pipeline
=
getInterceptorChain
(
amrmTokenIdentifier
)
;
AllocateResponse
allocateResponse
=
pipeline
.
getRootInterceptor
(
)
.
allocate
(
request
)
;
updateAMRMTokens
(
amrmTokenIdentifier
,
pipeline
,
allocateResponse
)
;
long
endTime
=
clock
.
getTime
(
)
;
this
.
metrics
.
succeededAllocateRequests
(
endTime
-
startTime
)
;
protected
void
initializePipeline
(
ApplicationAttemptId
applicationAttemptId
,
String
user
,
Token
<
AMRMTokenIdentifier
>
amrmToken
,
Token
<
AMRMTokenIdentifier
>
localToken
,
Map
<
String
,
byte
[
]
>
recoveredDataMap
,
boolean
isRecovery
,
Credentials
credentials
)
{
RequestInterceptorChainWrapper
chainWrapper
=
null
;
synchronized
(
applPipelineMap
)
{
if
(
applPipelineMap
.
containsKey
(
applicationAttemptId
.
getApplicationId
(
)
)
)
{
LOG
.
warn
(
+
+
+
applicationAttemptId
.
toString
(
)
)
;
RequestInterceptorChainWrapper
chainWrapperBackup
=
this
.
applPipelineMap
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
if
(
chainWrapperBackup
!=
null
&&
chainWrapperBackup
.
getApplicationAttemptId
(
)
!=
null
&&
!
chainWrapperBackup
.
getApplicationAttemptId
(
)
.
equals
(
applicationAttemptId
)
)
{
LOG
.
warn
(
+
applicationAttemptId
.
getApplicationId
(
)
,
ex
)
;
}
}
else
{
return
;
}
}
chainWrapper
=
new
RequestInterceptorChainWrapper
(
)
;
this
.
applPipelineMap
.
put
(
applicationAttemptId
.
getApplicationId
(
)
,
chainWrapper
)
;
}
LOG
.
info
(
+
+
applicationAttemptId
+
+
user
)
;
try
{
RequestInterceptor
interceptorChain
=
this
.
createRequestInterceptorChain
(
)
;
interceptorChain
.
init
(
createApplicationMasterContext
(
this
.
nmContext
,
applicationAttemptId
,
user
,
amrmToken
,
localToken
,
credentials
,
this
.
registry
)
)
;
if
(
isRecovery
)
{
if
(
recoveredDataMap
==
null
)
{
throw
new
YarnRuntimeException
(
)
;
}
interceptorChain
.
recover
(
recoveredDataMap
)
;
}
chainWrapper
.
init
(
interceptorChain
,
applicationAttemptId
)
;
if
(
!
isRecovery
&&
this
.
nmContext
.
getNMStateStore
(
)
!=
null
)
{
protected
void
stopApplication
(
ApplicationId
applicationId
)
{
Preconditions
.
checkArgument
(
applicationId
!=
null
,
)
;
RequestInterceptorChainWrapper
pipeline
=
this
.
applPipelineMap
.
remove
(
applicationId
)
;
if
(
pipeline
==
null
)
{
Preconditions
.
checkArgument
(
applicationId
!=
null
,
)
;
RequestInterceptorChainWrapper
pipeline
=
this
.
applPipelineMap
.
remove
(
applicationId
)
;
if
(
pipeline
==
null
)
{
LOG
.
info
(
+
,
applicationId
)
;
}
else
{
this
.
secretManager
.
applicationMasterFinished
(
pipeline
.
getApplicationAttemptId
(
)
)
;
LOG
.
info
(
+
applicationId
)
;
try
{
pipeline
.
getRootInterceptor
(
)
.
shutdown
(
)
;
}
catch
(
Throwable
ex
)
{
LOG
.
warn
(
+
applicationId
,
ex
)
;
}
if
(
this
.
nmContext
.
getNMStateStore
(
)
!=
null
)
{
try
{
this
.
nmContext
.
getNMStateStore
(
)
.
removeAMRMProxyAppContext
(
pipeline
.
getApplicationAttemptId
(
)
)
;
}
catch
(
IOException
e
)
{
AMRMProxyApplicationContextImpl
context
=
(
AMRMProxyApplicationContextImpl
)
pipeline
.
getRootInterceptor
(
)
.
getApplicationContext
(
)
;
if
(
allocateResponse
.
getAMRMToken
(
)
!=
null
)
{
LOG
.
info
(
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
token
=
allocateResponse
.
getAMRMToken
(
)
;
allocateResponse
.
setAMRMToken
(
null
)
;
org
.
apache
.
hadoop
.
security
.
token
.
Token
<
AMRMTokenIdentifier
>
newToken
=
ConverterUtils
.
convertFromYarn
(
token
,
(
Text
)
null
)
;
if
(
context
.
setAMRMToken
(
newToken
)
&&
this
.
nmContext
.
getNMStateStore
(
)
!=
null
)
{
try
{
this
.
nmContext
.
getNMStateStore
(
)
.
storeAMRMProxyAppContextEntry
(
context
.
getApplicationAttemptId
(
)
,
NMSS_AMRMTOKEN_KEY
,
newToken
.
encodeToUrlString
(
)
.
getBytes
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
context
.
getApplicationAttemptId
(
)
,
e
)
;
}
}
}
MasterKeyData
nextMasterKey
=
this
.
secretManager
.
getNextMasterKeyData
(
)
;
if
(
nextMasterKey
!=
null
&&
nextMasterKey
.
getMasterKey
(
)
.
getKeyId
(
)
!=
amrmTokenIdentifier
.
getKeyId
(
)
)
{
Token
<
AMRMTokenIdentifier
>
localToken
=
context
.
getLocalAMRMToken
(
)
;
if
(
nextMasterKey
.
getMasterKey
(
)
.
getKeyId
(
)
!=
context
.
getLocalAMRMTokenKeyId
(
)
)
{
public
void
applicationMasterFinished
(
ApplicationAttemptId
appAttemptId
)
{
this
.
writeLock
.
lock
(
)
;
try
{
public
Token
<
AMRMTokenIdentifier
>
createAndGetAMRMToken
(
ApplicationAttemptId
appAttemptId
)
{
this
.
writeLock
.
lock
(
)
;
try
{
@
Override
public
byte
[
]
retrievePassword
(
AMRMTokenIdentifier
identifier
)
throws
InvalidToken
{
this
.
readLock
.
lock
(
)
;
try
{
ApplicationAttemptId
applicationAttemptId
=
identifier
.
getApplicationAttemptId
(
)
;
@
Override
@
Private
protected
byte
[
]
createPassword
(
AMRMTokenIdentifier
identifier
)
{
this
.
readLock
.
lock
(
)
;
try
{
ApplicationAttemptId
applicationAttemptId
=
identifier
.
getApplicationAttemptId
(
)
;
appOwner
.
addCredentials
(
appContext
.
getCredentials
(
)
)
;
}
}
this
.
attemptId
=
appContext
.
getApplicationAttemptId
(
)
;
ApplicationId
appId
=
this
.
attemptId
.
getApplicationId
(
)
;
this
.
homeSubClusterId
=
SubClusterId
.
newInstance
(
YarnConfiguration
.
getClusterId
(
conf
)
)
;
this
.
homeRMRelayer
=
new
AMRMClientRelayer
(
createHomeRMProxy
(
appContext
,
ApplicationMasterProtocol
.
class
,
appOwner
)
,
appId
,
this
.
homeSubClusterId
.
toString
(
)
)
;
this
.
homeHeartbeartHandler
=
createHomeHeartbeartHandler
(
conf
,
appId
,
this
.
homeRMRelayer
)
;
this
.
homeHeartbeartHandler
.
setUGI
(
appOwner
)
;
this
.
homeHeartbeartHandler
.
setDaemon
(
true
)
;
this
.
homeHeartbeartHandler
.
start
(
)
;
this
.
lastAllocateResponse
=
RECORD_FACTORY
.
newRecordInstance
(
AllocateResponse
.
class
)
;
this
.
lastAllocateResponse
.
setResponseId
(
AMRMClientUtils
.
PRE_REGISTER_RESPONSE_ID
)
;
this
.
federationFacade
=
FederationStateStoreFacade
.
getInstance
(
)
;
this
.
subClusterResolver
=
this
.
federationFacade
.
getSubClusterResolver
(
)
;
this
.
policyInterpreter
=
null
;
this
.
uamPool
.
init
(
conf
)
;
if
(
recoveredDataMap
==
null
)
{
return
;
}
try
{
if
(
recoveredDataMap
.
containsKey
(
NMSS_REG_REQUEST_KEY
)
)
{
RegisterApplicationMasterRequestProto
pb
=
RegisterApplicationMasterRequestProto
.
parseFrom
(
recoveredDataMap
.
get
(
NMSS_REG_REQUEST_KEY
)
)
;
this
.
amRegistrationRequest
=
new
RegisterApplicationMasterRequestPBImpl
(
pb
)
;
LOG
.
info
(
,
this
.
attemptId
)
;
this
.
homeRMRelayer
.
setAMRegistrationRequest
(
this
.
amRegistrationRequest
)
;
}
if
(
recoveredDataMap
.
containsKey
(
NMSS_REG_RESPONSE_KEY
)
)
{
RegisterApplicationMasterResponseProto
pb
=
RegisterApplicationMasterResponseProto
.
parseFrom
(
recoveredDataMap
.
get
(
NMSS_REG_RESPONSE_KEY
)
)
;
this
.
amRegistrationResponse
=
new
RegisterApplicationMasterResponsePBImpl
(
pb
)
;
LOG
.
info
(
,
this
.
attemptId
)
;
}
Map
<
String
,
Token
<
AMRMTokenIdentifier
>>
uamMap
;
if
(
this
.
registryClient
!=
null
)
{
uamMap
=
this
.
registryClient
.
loadStateFromRegistry
(
this
.
attemptId
.
getApplicationId
(
)
)
;
}
if
(
recoveredDataMap
.
containsKey
(
NMSS_REG_RESPONSE_KEY
)
)
{
RegisterApplicationMasterResponseProto
pb
=
RegisterApplicationMasterResponseProto
.
parseFrom
(
recoveredDataMap
.
get
(
NMSS_REG_RESPONSE_KEY
)
)
;
this
.
amRegistrationResponse
=
new
RegisterApplicationMasterResponsePBImpl
(
pb
)
;
LOG
.
info
(
,
this
.
attemptId
)
;
}
Map
<
String
,
Token
<
AMRMTokenIdentifier
>>
uamMap
;
if
(
this
.
registryClient
!=
null
)
{
uamMap
=
this
.
registryClient
.
loadStateFromRegistry
(
this
.
attemptId
.
getApplicationId
(
)
)
;
LOG
.
info
(
,
uamMap
.
size
(
)
,
this
.
attemptId
.
getApplicationId
(
)
)
;
}
else
{
uamMap
=
new
HashMap
<
>
(
)
;
for
(
Entry
<
String
,
byte
[
]
>
entry
:
recoveredDataMap
.
entrySet
(
)
)
{
if
(
entry
.
getKey
(
)
.
startsWith
(
NMSS_SECONDARY_SC_PREFIX
)
)
{
String
scId
=
entry
.
getKey
(
)
.
substring
(
NMSS_SECONDARY_SC_PREFIX
.
length
(
)
)
;
Token
<
AMRMTokenIdentifier
>
amrmToken
=
new
Token
<
>
(
)
;
amrmToken
.
decodeFromUrlString
(
new
String
(
entry
.
getValue
(
)
,
STRING_TO_BYTE_FORMAT
)
)
;
RegisterApplicationMasterResponseProto
pb
=
RegisterApplicationMasterResponseProto
.
parseFrom
(
recoveredDataMap
.
get
(
NMSS_REG_RESPONSE_KEY
)
)
;
this
.
amRegistrationResponse
=
new
RegisterApplicationMasterResponsePBImpl
(
pb
)
;
LOG
.
info
(
,
this
.
attemptId
)
;
}
Map
<
String
,
Token
<
AMRMTokenIdentifier
>>
uamMap
;
if
(
this
.
registryClient
!=
null
)
{
uamMap
=
this
.
registryClient
.
loadStateFromRegistry
(
this
.
attemptId
.
getApplicationId
(
)
)
;
LOG
.
info
(
,
uamMap
.
size
(
)
,
this
.
attemptId
.
getApplicationId
(
)
)
;
}
else
{
uamMap
=
new
HashMap
<
>
(
)
;
for
(
Entry
<
String
,
byte
[
]
>
entry
:
recoveredDataMap
.
entrySet
(
)
)
{
if
(
entry
.
getKey
(
)
.
startsWith
(
NMSS_SECONDARY_SC_PREFIX
)
)
{
String
scId
=
entry
.
getKey
(
)
.
substring
(
NMSS_SECONDARY_SC_PREFIX
.
length
(
)
)
;
Token
<
AMRMTokenIdentifier
>
amrmToken
=
new
Token
<
>
(
)
;
amrmToken
.
decodeFromUrlString
(
new
String
(
entry
.
getValue
(
)
,
STRING_TO_BYTE_FORMAT
)
)
;
uamMap
.
put
(
scId
,
amrmToken
)
;
String
scId
=
entry
.
getKey
(
)
.
substring
(
NMSS_SECONDARY_SC_PREFIX
.
length
(
)
)
;
Token
<
AMRMTokenIdentifier
>
amrmToken
=
new
Token
<
>
(
)
;
amrmToken
.
decodeFromUrlString
(
new
String
(
entry
.
getValue
(
)
,
STRING_TO_BYTE_FORMAT
)
)
;
uamMap
.
put
(
scId
,
amrmToken
)
;
LOG
.
debug
(
,
scId
)
;
}
}
LOG
.
info
(
,
uamMap
.
size
(
)
,
this
.
attemptId
.
getApplicationId
(
)
)
;
}
int
containers
=
0
;
for
(
Map
.
Entry
<
String
,
Token
<
AMRMTokenIdentifier
>>
entry
:
uamMap
.
entrySet
(
)
)
{
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
entry
.
getKey
(
)
)
;
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
try
{
this
.
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
this
.
attemptId
.
getApplicationId
(
)
,
this
.
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
this
.
homeSubClusterId
.
getId
(
)
,
entry
.
getValue
(
)
,
subClusterId
.
toString
(
)
)
;
this
.
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
this
.
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
RegisterApplicationMasterResponse
response
=
this
.
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
this
.
amRegistrationRequest
)
;
amrmToken
.
decodeFromUrlString
(
new
String
(
entry
.
getValue
(
)
,
STRING_TO_BYTE_FORMAT
)
)
;
uamMap
.
put
(
scId
,
amrmToken
)
;
LOG
.
debug
(
,
scId
)
;
}
}
LOG
.
info
(
,
uamMap
.
size
(
)
,
this
.
attemptId
.
getApplicationId
(
)
)
;
}
int
containers
=
0
;
for
(
Map
.
Entry
<
String
,
Token
<
AMRMTokenIdentifier
>>
entry
:
uamMap
.
entrySet
(
)
)
{
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
entry
.
getKey
(
)
)
;
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
try
{
this
.
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
this
.
attemptId
.
getApplicationId
(
)
,
this
.
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
this
.
homeSubClusterId
.
getId
(
)
,
entry
.
getValue
(
)
,
subClusterId
.
toString
(
)
)
;
this
.
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
this
.
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
RegisterApplicationMasterResponse
response
=
this
.
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
this
.
amRegistrationRequest
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
for
(
Container
container
:
response
.
getContainersFromPreviousAttempts
(
)
)
{
uamMap
.
put
(
scId
,
amrmToken
)
;
LOG
.
debug
(
,
scId
)
;
}
}
LOG
.
info
(
,
uamMap
.
size
(
)
,
this
.
attemptId
.
getApplicationId
(
)
)
;
}
int
containers
=
0
;
for
(
Map
.
Entry
<
String
,
Token
<
AMRMTokenIdentifier
>>
entry
:
uamMap
.
entrySet
(
)
)
{
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
entry
.
getKey
(
)
)
;
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
try
{
this
.
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
this
.
attemptId
.
getApplicationId
(
)
,
this
.
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
this
.
homeSubClusterId
.
getId
(
)
,
entry
.
getValue
(
)
,
subClusterId
.
toString
(
)
)
;
this
.
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
this
.
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
RegisterApplicationMasterResponse
response
=
this
.
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
this
.
amRegistrationRequest
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
for
(
Container
container
:
response
.
getContainersFromPreviousAttempts
(
)
)
{
containerIdToSubClusterIdMap
.
put
(
container
.
getId
(
)
,
subClusterId
)
;
for
(
Map
.
Entry
<
String
,
Token
<
AMRMTokenIdentifier
>>
entry
:
uamMap
.
entrySet
(
)
)
{
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
entry
.
getKey
(
)
)
;
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
try
{
this
.
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
this
.
attemptId
.
getApplicationId
(
)
,
this
.
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
this
.
homeSubClusterId
.
getId
(
)
,
entry
.
getValue
(
)
,
subClusterId
.
toString
(
)
)
;
this
.
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
this
.
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
RegisterApplicationMasterResponse
response
=
this
.
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
this
.
amRegistrationRequest
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
for
(
Container
container
:
response
.
getContainersFromPreviousAttempts
(
)
)
{
containerIdToSubClusterIdMap
.
put
(
container
.
getId
(
)
,
subClusterId
)
;
containers
++
;
LOG
.
debug
(
,
subClusterId
,
container
.
getId
(
)
)
;
}
LOG
.
info
(
,
response
.
getContainersFromPreviousAttempts
(
)
.
size
(
)
,
subClusterId
)
;
}
catch
(
Exception
e
)
{
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
try
{
this
.
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
this
.
attemptId
.
getApplicationId
(
)
,
this
.
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
this
.
homeSubClusterId
.
getId
(
)
,
entry
.
getValue
(
)
,
subClusterId
.
toString
(
)
)
;
this
.
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
this
.
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
RegisterApplicationMasterResponse
response
=
this
.
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
this
.
amRegistrationRequest
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
for
(
Container
container
:
response
.
getContainersFromPreviousAttempts
(
)
)
{
containerIdToSubClusterIdMap
.
put
(
container
.
getId
(
)
,
subClusterId
)
;
containers
++
;
LOG
.
debug
(
,
subClusterId
,
container
.
getId
(
)
)
;
}
LOG
.
info
(
,
response
.
getContainersFromPreviousAttempts
(
)
.
size
(
)
,
subClusterId
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
subClusterId
+
+
this
.
attemptId
,
e
)
;
}
}
UserGroupInformation
appSubmitter
=
UserGroupInformation
.
createRemoteUser
(
getApplicationContext
(
)
.
getUser
(
)
)
;
if
(
getNMStateStore
(
)
!=
null
)
{
try
{
RegisterApplicationMasterRequestPBImpl
pb
=
(
RegisterApplicationMasterRequestPBImpl
)
this
.
amRegistrationRequest
;
getNMStateStore
(
)
.
storeAMRMProxyAppContextEntry
(
this
.
attemptId
,
NMSS_REG_REQUEST_KEY
,
pb
.
getProto
(
)
.
toByteArray
(
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
this
.
attemptId
,
e
)
;
}
}
}
if
(
this
.
amRegistrationResponse
!=
null
)
{
return
this
.
amRegistrationResponse
;
}
this
.
amRegistrationResponse
=
this
.
homeRMRelayer
.
registerApplicationMaster
(
request
)
;
if
(
this
.
amRegistrationResponse
.
getContainersFromPreviousAttempts
(
)
!=
null
)
{
cacheAllocatedContainers
(
this
.
amRegistrationResponse
.
getContainersFromPreviousAttempts
(
)
,
this
.
homeSubClusterId
)
;
}
ApplicationId
appId
=
this
.
attemptId
.
getApplicationId
(
)
;
reAttachUAMAndMergeRegisterResponse
(
this
.
amRegistrationResponse
,
appId
)
;
if
(
getNMStateStore
(
)
!=
null
)
{
try
{
catch
(
Exception
e
)
{
LOG
.
error
(
+
this
.
attemptId
,
e
)
;
}
}
}
if
(
this
.
amRegistrationResponse
!=
null
)
{
return
this
.
amRegistrationResponse
;
}
this
.
amRegistrationResponse
=
this
.
homeRMRelayer
.
registerApplicationMaster
(
request
)
;
if
(
this
.
amRegistrationResponse
.
getContainersFromPreviousAttempts
(
)
!=
null
)
{
cacheAllocatedContainers
(
this
.
amRegistrationResponse
.
getContainersFromPreviousAttempts
(
)
,
this
.
homeSubClusterId
)
;
}
ApplicationId
appId
=
this
.
attemptId
.
getApplicationId
(
)
;
reAttachUAMAndMergeRegisterResponse
(
this
.
amRegistrationResponse
,
appId
)
;
if
(
getNMStateStore
(
)
!=
null
)
{
try
{
RegisterApplicationMasterResponsePBImpl
pb
=
(
RegisterApplicationMasterResponsePBImpl
)
this
.
amRegistrationResponse
;
getNMStateStore
(
)
.
storeAMRMProxyAppContextEntry
(
this
.
attemptId
,
NMSS_REG_RESPONSE_KEY
,
pb
.
getProto
(
)
.
toByteArray
(
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
+
this
.
attemptId
,
e
)
;
@
Override
public
FinishApplicationMasterResponse
finishApplicationMaster
(
FinishApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
this
.
finishAMCalled
=
true
;
boolean
failedToUnRegister
=
false
;
ExecutorCompletionService
<
FinishApplicationMasterResponseInfo
>
compSvc
=
null
;
Set
<
String
>
subClusterIds
=
this
.
uamPool
.
getAllUAMIds
(
)
;
if
(
subClusterIds
.
size
(
)
>
0
)
{
final
FinishApplicationMasterRequest
finishRequest
=
request
;
compSvc
=
new
ExecutorCompletionService
<
FinishApplicationMasterResponseInfo
>
(
this
.
threadpool
)
;
@
Override
public
FinishApplicationMasterResponse
finishApplicationMaster
(
FinishApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
this
.
finishAMCalled
=
true
;
boolean
failedToUnRegister
=
false
;
ExecutorCompletionService
<
FinishApplicationMasterResponseInfo
>
compSvc
=
null
;
Set
<
String
>
subClusterIds
=
this
.
uamPool
.
getAllUAMIds
(
)
;
if
(
subClusterIds
.
size
(
)
>
0
)
{
final
FinishApplicationMasterRequest
finishRequest
=
request
;
compSvc
=
new
ExecutorCompletionService
<
FinishApplicationMasterResponseInfo
>
(
this
.
threadpool
)
;
LOG
.
info
(
,
subClusterIds
.
size
(
)
)
;
for
(
final
String
subClusterId
:
subClusterIds
)
{
compSvc
.
submit
(
new
Callable
<
FinishApplicationMasterResponseInfo
>
(
)
{
@
Override
public
FinishApplicationMasterResponseInfo
call
(
)
throws
Exception
{
LOG
.
info
(
,
subClusterId
)
;
FinishApplicationMasterResponse
uamResponse
=
null
;
try
{
uamResponse
=
uamPool
.
finishApplicationMaster
(
subClusterId
,
finishRequest
)
;
if
(
uamResponse
.
getIsUnregistered
(
)
)
{
secondaryRelayers
.
remove
(
subClusterId
)
;
if
(
getNMStateStore
(
)
!=
null
)
{
getNMStateStore
(
)
.
removeAMRMProxyAppContextEntry
(
attemptId
,
NMSS_SECONDARY_SC_PREFIX
+
subClusterId
)
;
}
}
}
catch
(
Throwable
e
)
{
LOG
.
warn
(
+
+
subClusterId
+
+
attemptId
,
e
)
;
}
return
new
FinishApplicationMasterResponseInfo
(
uamResponse
,
subClusterId
)
;
}
}
)
;
}
}
FinishApplicationMasterResponse
homeResponse
=
this
.
homeRMRelayer
.
finishApplicationMaster
(
request
)
;
this
.
homeHeartbeartHandler
.
shutdown
(
)
;
if
(
subClusterIds
.
size
(
)
>
0
)
{
secondaryRelayers
.
remove
(
subClusterId
)
;
if
(
getNMStateStore
(
)
!=
null
)
{
getNMStateStore
(
)
.
removeAMRMProxyAppContextEntry
(
attemptId
,
NMSS_SECONDARY_SC_PREFIX
+
subClusterId
)
;
}
}
}
catch
(
Throwable
e
)
{
LOG
.
warn
(
+
+
subClusterId
+
+
attemptId
,
e
)
;
}
return
new
FinishApplicationMasterResponseInfo
(
uamResponse
,
subClusterId
)
;
}
}
)
;
}
}
FinishApplicationMasterResponse
homeResponse
=
this
.
homeRMRelayer
.
finishApplicationMaster
(
request
)
;
this
.
homeHeartbeartHandler
.
shutdown
(
)
;
if
(
subClusterIds
.
size
(
)
>
0
)
{
LOG
.
info
(
,
subClusterIds
.
size
(
)
)
;
for
(
int
i
=
0
;
i
<
subClusterIds
.
size
(
)
;
++
i
)
{
try
{
Future
<
FinishApplicationMasterResponseInfo
>
future
=
compSvc
.
take
(
)
;
FinishApplicationMasterResponseInfo
uamResponse
=
future
.
get
(
)
;
ExecutorCompletionService
<
RegisterApplicationMasterResponse
>
completionService
=
new
ExecutorCompletionService
<
>
(
this
.
threadpool
)
;
for
(
Entry
<
String
,
Token
<
AMRMTokenIdentifier
>>
entry
:
uamMap
.
entrySet
(
)
)
{
final
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
entry
.
getKey
(
)
)
;
final
Token
<
AMRMTokenIdentifier
>
amrmToken
=
entry
.
getValue
(
)
;
completionService
.
submit
(
new
Callable
<
RegisterApplicationMasterResponse
>
(
)
{
@
Override
public
RegisterApplicationMasterResponse
call
(
)
throws
Exception
{
RegisterApplicationMasterResponse
response
=
null
;
try
{
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
appId
,
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
homeSubClusterId
.
getId
(
)
,
amrmToken
,
subClusterId
.
toString
(
)
)
;
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
response
=
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
amRegistrationRequest
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
if
(
response
!=
null
&&
response
.
getContainersFromPreviousAttempts
(
)
!=
null
)
{
for
(
Entry
<
String
,
Token
<
AMRMTokenIdentifier
>>
entry
:
uamMap
.
entrySet
(
)
)
{
final
SubClusterId
subClusterId
=
SubClusterId
.
newInstance
(
entry
.
getKey
(
)
)
;
final
Token
<
AMRMTokenIdentifier
>
amrmToken
=
entry
.
getValue
(
)
;
completionService
.
submit
(
new
Callable
<
RegisterApplicationMasterResponse
>
(
)
{
@
Override
public
RegisterApplicationMasterResponse
call
(
)
throws
Exception
{
RegisterApplicationMasterResponse
response
=
null
;
try
{
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
appId
,
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
homeSubClusterId
.
getId
(
)
,
amrmToken
,
subClusterId
.
toString
(
)
)
;
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
response
=
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
amRegistrationRequest
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
if
(
response
!=
null
&&
response
.
getContainersFromPreviousAttempts
(
)
!=
null
)
{
cacheAllocatedContainers
(
response
.
getContainersFromPreviousAttempts
(
)
,
subClusterId
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
.
getId
(
)
)
;
uamPool
.
reAttachUAM
(
subClusterId
.
getId
(
)
,
config
,
appId
,
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
homeSubClusterId
.
getId
(
)
,
amrmToken
,
subClusterId
.
toString
(
)
)
;
secondaryRelayers
.
put
(
subClusterId
.
getId
(
)
,
uamPool
.
getAMRMClientRelayer
(
subClusterId
.
getId
(
)
)
)
;
response
=
uamPool
.
registerApplicationMaster
(
subClusterId
.
getId
(
)
,
amRegistrationRequest
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
if
(
response
!=
null
&&
response
.
getContainersFromPreviousAttempts
(
)
!=
null
)
{
cacheAllocatedContainers
(
response
.
getContainersFromPreviousAttempts
(
)
,
subClusterId
)
;
}
LOG
.
info
(
,
subClusterId
,
appId
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
subClusterId
+
+
appId
,
e
)
;
}
return
response
;
}
}
)
;
}
for
(
int
i
=
0
;
i
<
uamMap
.
size
(
)
;
i
++
)
{
try
{
Future
<
RegisterApplicationMasterResponse
>
future
=
completionService
.
take
(
)
;
lastSCResponseTime
.
put
(
subClusterId
,
clock
.
getTime
(
)
-
subClusterTimeOut
)
;
}
}
this
.
uamRegisterFutures
.
clear
(
)
;
for
(
final
SubClusterId
scId
:
newSubClusters
)
{
Future
<
?
>
future
=
this
.
threadpool
.
submit
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
String
subClusterId
=
scId
.
getId
(
)
;
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
)
;
RegisterApplicationMasterResponse
uamResponse
=
null
;
Token
<
AMRMTokenIdentifier
>
token
=
null
;
try
{
token
=
uamPool
.
launchUAM
(
subClusterId
,
config
,
attemptId
.
getApplicationId
(
)
,
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
homeSubClusterId
.
toString
(
)
,
true
,
subClusterId
)
;
secondaryRelayers
.
put
(
subClusterId
,
uamPool
.
getAMRMClientRelayer
(
subClusterId
)
)
;
uamResponse
=
uamPool
.
registerApplicationMaster
(
subClusterId
,
amRegistrationRequest
)
;
}
catch
(
Throwable
e
)
{
this
.
uamRegisterFutures
.
clear
(
)
;
for
(
final
SubClusterId
scId
:
newSubClusters
)
{
Future
<
?
>
future
=
this
.
threadpool
.
submit
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
String
subClusterId
=
scId
.
getId
(
)
;
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
)
;
RegisterApplicationMasterResponse
uamResponse
=
null
;
Token
<
AMRMTokenIdentifier
>
token
=
null
;
try
{
token
=
uamPool
.
launchUAM
(
subClusterId
,
config
,
attemptId
.
getApplicationId
(
)
,
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
homeSubClusterId
.
toString
(
)
,
true
,
subClusterId
)
;
secondaryRelayers
.
put
(
subClusterId
,
uamPool
.
getAMRMClientRelayer
(
subClusterId
)
)
;
uamResponse
=
uamPool
.
registerApplicationMaster
(
subClusterId
,
amRegistrationRequest
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
subClusterId
+
+
attemptId
,
e
)
;
String
subClusterId
=
scId
.
getId
(
)
;
YarnConfiguration
config
=
new
YarnConfiguration
(
getConf
(
)
)
;
FederationProxyProviderUtil
.
updateConfForFederation
(
config
,
subClusterId
)
;
RegisterApplicationMasterResponse
uamResponse
=
null
;
Token
<
AMRMTokenIdentifier
>
token
=
null
;
try
{
token
=
uamPool
.
launchUAM
(
subClusterId
,
config
,
attemptId
.
getApplicationId
(
)
,
amRegistrationResponse
.
getQueue
(
)
,
getApplicationContext
(
)
.
getUser
(
)
,
homeSubClusterId
.
toString
(
)
,
true
,
subClusterId
)
;
secondaryRelayers
.
put
(
subClusterId
,
uamPool
.
getAMRMClientRelayer
(
subClusterId
)
)
;
uamResponse
=
uamPool
.
registerApplicationMaster
(
subClusterId
,
amRegistrationRequest
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
subClusterId
+
+
attemptId
,
e
)
;
return
;
}
uamRegistrations
.
put
(
scId
,
uamResponse
)
;
LOG
.
info
(
+
subClusterId
+
+
attemptId
)
;
try
{
secondaryRelayers
.
put
(
subClusterId
,
uamPool
.
getAMRMClientRelayer
(
subClusterId
)
)
;
uamResponse
=
uamPool
.
registerApplicationMaster
(
subClusterId
,
amRegistrationRequest
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
subClusterId
+
+
attemptId
,
e
)
;
return
;
}
uamRegistrations
.
put
(
scId
,
uamResponse
)
;
LOG
.
info
(
+
subClusterId
+
+
attemptId
)
;
try
{
uamPool
.
allocateAsync
(
subClusterId
,
requests
.
get
(
scId
)
,
new
HeartbeatCallBack
(
scId
,
true
)
)
;
}
catch
(
Throwable
e
)
{
LOG
.
error
(
+
subClusterId
+
+
attemptId
,
e
)
;
}
try
{
if
(
registryClient
!=
null
)
{
registryClient
.
writeAMRMTokenForUAM
(
attemptId
.
getApplicationId
(
)
,
subClusterId
,
token
)
;
}
else
if
(
getNMStateStore
(
)
!=
null
)
{
private
void
removeFinishedContainersFromCache
(
List
<
ContainerStatus
>
finishedContainers
)
{
for
(
ContainerStatus
container
:
finishedContainers
)
{
private
void
cacheAllocatedContainers
(
List
<
Container
>
containers
,
SubClusterId
subClusterId
)
{
for
(
Container
container
:
containers
)
{
private
boolean
warnIfNotExists
(
ContainerId
containerId
,
String
actionName
)
{
if
(
!
this
.
containerIdToSubClusterIdMap
.
containsKey
(
containerId
)
)
{
protected
final
synchronized
void
addService
(
String
name
,
AuxiliaryService
service
,
AuxServiceRecord
serviceRecord
)
{
final
String
className
=
getClassName
(
service
)
;
if
(
className
==
null
||
className
.
isEmpty
(
)
)
{
throw
new
YarnRuntimeException
(
+
+
sName
)
;
}
if
(
fromConfiguration
)
{
final
String
appLocalClassPath
=
conf
.
get
(
String
.
format
(
YarnConfiguration
.
NM_AUX_SERVICES_CLASSPATH
,
sName
)
)
;
if
(
appLocalClassPath
!=
null
&&
!
appLocalClassPath
.
isEmpty
(
)
)
{
return
createAuxServiceFromLocalClasspath
(
service
,
appLocalClassPath
,
conf
)
;
}
}
AuxServiceConfiguration
serviceConf
=
service
.
getConfiguration
(
)
;
List
<
Path
>
destFiles
=
new
ArrayList
<
>
(
)
;
if
(
serviceConf
!=
null
)
{
List
<
AuxServiceFile
>
files
=
serviceConf
.
getFiles
(
)
;
if
(
files
!=
null
)
{
for
(
AuxServiceFile
file
:
files
)
{
destFiles
.
add
(
maybeDownloadJars
(
sName
,
className
,
file
.
getSrcFile
(
)
,
file
.
getType
(
)
,
conf
)
)
;
}
}
}
if
(
destFiles
.
size
(
)
>
0
)
{
private
synchronized
void
maybeRemoveAuxService
(
String
sName
)
{
AuxiliaryService
s
;
s
=
serviceMap
.
remove
(
sName
)
;
serviceRecordMap
.
remove
(
sName
)
;
serviceMetaData
.
remove
(
sName
)
;
if
(
s
!=
null
)
{
AuxiliaryService
s
;
try
{
Preconditions
.
checkArgument
(
validateAuxServiceName
(
sName
)
,
+
sName
+
+
+
)
;
s
=
createAuxService
(
service
,
conf
,
fromConfiguration
)
;
if
(
s
==
null
)
{
throw
new
YarnRuntimeException
(
+
+
sName
)
;
}
if
(
!
sName
.
equals
(
s
.
getName
(
)
)
)
{
LOG
.
warn
(
+
sName
+
+
+
s
.
getClass
(
)
+
+
+
s
.
getName
(
)
+
+
+
+
)
;
}
s
.
setAuxiliaryLocalPathHandler
(
auxiliaryLocalPathHandler
)
;
setStateStoreDir
(
sName
,
s
)
;
Configuration
customConf
=
new
Configuration
(
conf
)
;
if
(
service
.
getConfiguration
(
)
!=
null
)
{
for
(
Entry
<
String
,
String
>
entry
:
service
.
getConfiguration
(
)
.
getProperties
(
)
.
entrySet
(
)
)
{
customConf
.
set
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
}
s
.
init
(
customConf
)
;
Preconditions
.
checkArgument
(
validateAuxServiceName
(
sName
)
,
+
sName
+
+
+
)
;
s
=
createAuxService
(
service
,
conf
,
fromConfiguration
)
;
if
(
s
==
null
)
{
throw
new
YarnRuntimeException
(
+
+
sName
)
;
}
if
(
!
sName
.
equals
(
s
.
getName
(
)
)
)
{
LOG
.
warn
(
+
sName
+
+
+
s
.
getClass
(
)
+
+
+
s
.
getName
(
)
+
+
+
+
)
;
}
s
.
setAuxiliaryLocalPathHandler
(
auxiliaryLocalPathHandler
)
;
setStateStoreDir
(
sName
,
s
)
;
Configuration
customConf
=
new
Configuration
(
conf
)
;
if
(
service
.
getConfiguration
(
)
!=
null
)
{
for
(
Entry
<
String
,
String
>
entry
:
service
.
getConfiguration
(
)
.
getProperties
(
)
.
entrySet
(
)
)
{
customConf
.
set
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
}
s
.
init
(
customConf
)
;
LOG
.
info
(
+
sName
)
;
}
catch
(
RuntimeException
e
)
{
private
boolean
checkManifestPermissions
(
FileStatus
status
)
throws
IOException
{
if
(
(
status
.
getPermission
(
)
.
toShort
(
)
&
0022
)
!=
0
)
{
if
(
!
manifestFS
.
exists
(
manifest
)
)
{
LOG
.
warn
(
+
manifest
+
)
;
return
null
;
}
FileStatus
status
;
try
{
status
=
manifestFS
.
getFileStatus
(
manifest
)
;
}
catch
(
FileNotFoundException
e
)
{
LOG
.
warn
(
+
manifest
+
)
;
return
null
;
}
if
(
!
status
.
isFile
(
)
)
{
LOG
.
warn
(
+
manifest
+
)
;
}
if
(
!
checkManifestOwnerAndPermissions
(
status
)
)
{
return
null
;
}
if
(
status
.
getModificationTime
(
)
==
manifestModifyTS
)
{
return
null
;
boolean
foundChanges
=
false
;
if
(
services
.
getServices
(
)
!=
null
)
{
for
(
AuxServiceRecord
service
:
services
.
getServices
(
)
)
{
AuxServiceRecord
existingService
=
serviceRecordMap
.
get
(
service
.
getName
(
)
)
;
loadedAuxServices
.
add
(
service
.
getName
(
)
)
;
if
(
existingService
!=
null
&&
existingService
.
equals
(
service
)
)
{
LOG
.
debug
(
,
service
.
getName
(
)
)
;
continue
;
}
foundChanges
=
true
;
try
{
maybeRemoveAuxService
(
service
.
getName
(
)
)
;
AuxiliaryService
s
=
initAuxService
(
service
,
conf
,
false
)
;
if
(
startServices
)
{
startAuxService
(
service
.
getName
(
)
,
s
,
service
)
;
}
addService
(
service
.
getName
(
)
,
s
,
service
)
;
@
Override
public
void
stateChanged
(
Service
service
)
{
@
Override
public
void
handle
(
AuxServicesEvent
event
)
{
@
SuppressWarnings
(
)
private
void
recover
(
)
throws
IOException
,
URISyntaxException
{
NMStateStoreService
stateStore
=
context
.
getNMStateStore
(
)
;
if
(
stateStore
.
canRecover
(
)
)
{
rsrcLocalizationSrvc
.
recoverLocalizedResources
(
stateStore
.
loadLocalizationState
(
)
)
;
RecoveredApplicationsState
appsState
=
stateStore
.
loadApplicationsState
(
)
;
try
(
RecoveryIterator
<
ContainerManagerApplicationProto
>
rasIterator
=
appsState
.
getIterator
(
)
)
{
while
(
rasIterator
.
hasNext
(
)
)
{
ContainerManagerApplicationProto
proto
=
rasIterator
.
next
(
)
;
private
void
recoverApplication
(
ContainerManagerApplicationProto
p
)
throws
IOException
{
ApplicationId
appId
=
new
ApplicationIdPBImpl
(
p
.
getId
(
)
)
;
Credentials
creds
=
new
Credentials
(
)
;
creds
.
readTokenStorageStream
(
new
DataInputStream
(
p
.
getCredentials
(
)
.
newInput
(
)
)
)
;
List
<
ApplicationACLMapProto
>
aclProtoList
=
p
.
getAclsList
(
)
;
Map
<
ApplicationAccessType
,
String
>
acls
=
new
HashMap
<
ApplicationAccessType
,
String
>
(
aclProtoList
.
size
(
)
)
;
for
(
ApplicationACLMapProto
aclProto
:
aclProtoList
)
{
acls
.
put
(
ProtoUtils
.
convertFromProtoFormat
(
aclProto
.
getAccessType
(
)
)
,
aclProto
.
getAcl
(
)
)
;
}
LogAggregationContext
logAggregationContext
=
null
;
if
(
p
.
getLogAggregationContext
(
)
!=
null
)
{
logAggregationContext
=
new
LogAggregationContextPBImpl
(
p
.
getLogAggregationContext
(
)
)
;
}
FlowContext
fc
=
null
;
if
(
p
.
getFlowContext
(
)
!=
null
)
{
FlowContextProto
fcp
=
p
.
getFlowContext
(
)
;
fc
=
new
FlowContext
(
fcp
.
getFlowName
(
)
,
fcp
.
getFlowVersion
(
)
,
fcp
.
getFlowRunId
(
)
)
;
creds
.
readTokenStorageStream
(
new
DataInputStream
(
p
.
getCredentials
(
)
.
newInput
(
)
)
)
;
List
<
ApplicationACLMapProto
>
aclProtoList
=
p
.
getAclsList
(
)
;
Map
<
ApplicationAccessType
,
String
>
acls
=
new
HashMap
<
ApplicationAccessType
,
String
>
(
aclProtoList
.
size
(
)
)
;
for
(
ApplicationACLMapProto
aclProto
:
aclProtoList
)
{
acls
.
put
(
ProtoUtils
.
convertFromProtoFormat
(
aclProto
.
getAccessType
(
)
)
,
aclProto
.
getAcl
(
)
)
;
}
LogAggregationContext
logAggregationContext
=
null
;
if
(
p
.
getLogAggregationContext
(
)
!=
null
)
{
logAggregationContext
=
new
LogAggregationContextPBImpl
(
p
.
getLogAggregationContext
(
)
)
;
}
FlowContext
fc
=
null
;
if
(
p
.
getFlowContext
(
)
!=
null
)
{
FlowContextProto
fcp
=
p
.
getFlowContext
(
)
;
fc
=
new
FlowContext
(
fcp
.
getFlowName
(
)
,
fcp
.
getFlowVersion
(
)
,
fcp
.
getFlowRunId
(
)
)
;
LOG
.
debug
(
,
fc
,
appId
)
;
}
else
{
fc
=
new
FlowContext
(
TimelineUtils
.
generateDefaultFlowName
(
null
,
appId
)
,
YarnConfiguration
.
DEFAULT_FLOW_VERSION
,
appId
.
getClusterTimestamp
(
)
)
;
if
(
delayedRpcServerStart
)
{
connectAddress
=
NetUtils
.
getConnectAddress
(
initialAddress
)
;
}
else
{
server
.
start
(
)
;
connectAddress
=
NetUtils
.
getConnectAddress
(
server
)
;
}
NodeId
nodeId
=
buildNodeId
(
connectAddress
,
hostOverride
)
;
(
(
NodeManager
.
NMContext
)
context
)
.
setNodeId
(
nodeId
)
;
this
.
context
.
getNMTokenSecretManager
(
)
.
setNodeId
(
nodeId
)
;
this
.
context
.
getContainerTokenSecretManager
(
)
.
setNodeId
(
nodeId
)
;
super
.
serviceStart
(
)
;
if
(
delayedRpcServerStart
)
{
waitForRecoveredContainers
(
)
;
server
.
start
(
)
;
connectAddress
=
NetUtils
.
getConnectAddress
(
server
)
;
NodeId
serverNode
=
buildNodeId
(
connectAddress
,
hostOverride
)
;
if
(
delayedRpcServerStart
)
{
connectAddress
=
NetUtils
.
getConnectAddress
(
initialAddress
)
;
}
else
{
server
.
start
(
)
;
connectAddress
=
NetUtils
.
getConnectAddress
(
server
)
;
}
NodeId
nodeId
=
buildNodeId
(
connectAddress
,
hostOverride
)
;
(
(
NodeManager
.
NMContext
)
context
)
.
setNodeId
(
nodeId
)
;
this
.
context
.
getNMTokenSecretManager
(
)
.
setNodeId
(
nodeId
)
;
this
.
context
.
getContainerTokenSecretManager
(
)
.
setNodeId
(
nodeId
)
;
super
.
serviceStart
(
)
;
if
(
delayedRpcServerStart
)
{
waitForRecoveredContainers
(
)
;
server
.
start
(
)
;
connectAddress
=
NetUtils
.
getConnectAddress
(
server
)
;
NodeId
serverNode
=
buildNodeId
(
connectAddress
,
hostOverride
)
;
LOG
.
info
(
+
applications
.
keySet
(
)
)
;
if
(
this
.
context
.
getNMStateStore
(
)
.
canRecover
(
)
&&
!
this
.
context
.
getDecommissioned
(
)
)
{
if
(
getConfig
(
)
.
getBoolean
(
YarnConfiguration
.
NM_RECOVERY_SUPERVISED
,
YarnConfiguration
.
DEFAULT_NM_RECOVERY_SUPERVISED
)
)
{
return
;
}
}
List
<
ApplicationId
>
appIds
=
new
ArrayList
<
ApplicationId
>
(
applications
.
keySet
(
)
)
;
this
.
handle
(
new
CMgrCompletedAppsEvent
(
appIds
,
CMgrCompletedAppsEvent
.
Reason
.
ON_SHUTDOWN
)
)
;
LOG
.
info
(
)
;
long
waitStartTime
=
System
.
currentTimeMillis
(
)
;
while
(
!
applications
.
isEmpty
(
)
&&
System
.
currentTimeMillis
(
)
-
waitStartTime
<
waitForContainersOnShutdownMillis
)
{
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ex
)
{
LOG
.
warn
(
,
ex
)
;
}
}
if
(
applications
.
isEmpty
(
)
)
{
LOG
.
info
(
)
;
LOG
.
info
(
)
;
this
.
handle
(
new
CMgrCompletedContainersEvent
(
containerIds
,
CMgrCompletedContainersEvent
.
Reason
.
ON_NODEMANAGER_RESYNC
)
)
;
boolean
allContainersCompleted
=
false
;
while
(
!
containers
.
isEmpty
(
)
&&
!
allContainersCompleted
)
{
allContainersCompleted
=
true
;
for
(
Entry
<
ContainerId
,
Container
>
container
:
containers
.
entrySet
(
)
)
{
if
(
(
(
ContainerImpl
)
container
.
getValue
(
)
)
.
getCurrentState
(
)
!=
ContainerState
.
COMPLETE
)
{
allContainersCompleted
=
false
;
try
{
Thread
.
sleep
(
1000
)
;
}
catch
(
InterruptedException
ex
)
{
LOG
.
warn
(
,
ex
)
;
}
break
;
}
}
}
if
(
allContainersCompleted
)
{
LOG
.
info
(
)
;
@
SuppressWarnings
(
)
protected
void
startContainerInternal
(
ContainerTokenIdentifier
containerTokenIdentifier
,
StartContainerRequest
request
,
String
remoteUser
)
throws
YarnException
,
IOException
{
ContainerId
containerId
=
containerTokenIdentifier
.
getContainerID
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
String
user
=
containerTokenIdentifier
.
getApplicationSubmitter
(
)
;
}
else
if
(
rsrc
.
getValue
(
)
.
getVisibility
(
)
==
null
)
{
throw
new
YarnException
(
+
rsrc
.
getKey
(
)
+
+
rsrc
.
getValue
(
)
)
;
}
}
Credentials
credentials
=
YarnServerSecurityUtils
.
parseCredentials
(
launchContext
)
;
long
containerStartTime
=
SystemClock
.
getInstance
(
)
.
getTime
(
)
;
Container
container
=
new
ContainerImpl
(
getConfig
(
)
,
this
.
dispatcher
,
launchContext
,
credentials
,
metrics
,
containerTokenIdentifier
,
context
,
containerStartTime
)
;
ApplicationId
applicationID
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
if
(
context
.
getContainers
(
)
.
putIfAbsent
(
containerId
,
container
)
!=
null
)
{
NMAuditLogger
.
logFailure
(
remoteUser
,
AuditConstants
.
START_CONTAINER
,
,
,
applicationID
,
containerId
)
;
throw
RPCUtil
.
getRemoteException
(
+
containerIdStr
+
)
;
}
this
.
readLock
.
lock
(
)
;
try
{
if
(
!
isServiceStopped
(
)
)
{
if
(
!
context
.
getApplications
(
)
.
containsKey
(
applicationID
)
)
{
FlowContext
flowContext
=
getFlowContext
(
launchContext
,
applicationID
)
;
Application
application
=
new
ApplicationImpl
(
dispatcher
,
user
,
flowContext
,
applicationID
,
credentials
,
context
)
;
throw
RPCUtil
.
getRemoteException
(
+
containerIdStr
+
)
;
}
this
.
readLock
.
lock
(
)
;
try
{
if
(
!
isServiceStopped
(
)
)
{
if
(
!
context
.
getApplications
(
)
.
containsKey
(
applicationID
)
)
{
FlowContext
flowContext
=
getFlowContext
(
launchContext
,
applicationID
)
;
Application
application
=
new
ApplicationImpl
(
dispatcher
,
user
,
flowContext
,
applicationID
,
credentials
,
context
)
;
if
(
context
.
getApplications
(
)
.
putIfAbsent
(
applicationID
,
application
)
==
null
)
{
LOG
.
info
(
+
applicationID
)
;
LogAggregationContext
logAggregationContext
=
containerTokenIdentifier
.
getLogAggregationContext
(
)
;
Map
<
ApplicationAccessType
,
String
>
appAcls
=
container
.
getLaunchContext
(
)
.
getApplicationACLs
(
)
;
context
.
getNMStateStore
(
)
.
storeApplication
(
applicationID
,
buildAppProto
(
applicationID
,
user
,
credentials
,
appAcls
,
logAggregationContext
,
flowContext
)
)
;
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ApplicationInitEvent
(
applicationID
,
appAcls
,
logAggregationContext
)
)
;
}
}
else
if
(
containerTokenIdentifier
.
getContainerType
(
)
==
ContainerType
.
APPLICATION_MASTER
)
{
FlowContext
flowContext
=
getFlowContext
(
launchContext
,
applicationID
)
;
this
.
readLock
.
lock
(
)
;
try
{
if
(
!
isServiceStopped
(
)
)
{
if
(
!
context
.
getApplications
(
)
.
containsKey
(
applicationID
)
)
{
FlowContext
flowContext
=
getFlowContext
(
launchContext
,
applicationID
)
;
Application
application
=
new
ApplicationImpl
(
dispatcher
,
user
,
flowContext
,
applicationID
,
credentials
,
context
)
;
if
(
context
.
getApplications
(
)
.
putIfAbsent
(
applicationID
,
application
)
==
null
)
{
LOG
.
info
(
+
applicationID
)
;
LogAggregationContext
logAggregationContext
=
containerTokenIdentifier
.
getLogAggregationContext
(
)
;
Map
<
ApplicationAccessType
,
String
>
appAcls
=
container
.
getLaunchContext
(
)
.
getApplicationACLs
(
)
;
context
.
getNMStateStore
(
)
.
storeApplication
(
applicationID
,
buildAppProto
(
applicationID
,
user
,
credentials
,
appAcls
,
logAggregationContext
,
flowContext
)
)
;
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ApplicationInitEvent
(
applicationID
,
appAcls
,
logAggregationContext
)
)
;
}
}
else
if
(
containerTokenIdentifier
.
getContainerType
(
)
==
ContainerType
.
APPLICATION_MASTER
)
{
FlowContext
flowContext
=
getFlowContext
(
launchContext
,
applicationID
)
;
if
(
flowContext
!=
null
)
{
@
SuppressWarnings
(
)
protected
void
stopContainerInternal
(
ContainerId
containerID
,
String
remoteUser
)
throws
YarnException
,
IOException
{
String
containerIDStr
=
containerID
.
toString
(
)
;
Container
container
=
this
.
context
.
getContainers
(
)
.
get
(
containerID
)
;
protected
ContainerStatus
getContainerStatusInternal
(
ContainerId
containerID
,
NMTokenIdentifier
nmTokenIdentifier
,
String
remoteUser
)
throws
YarnException
{
String
containerIDStr
=
containerID
.
toString
(
)
;
Container
container
=
this
.
context
.
getContainers
(
)
.
get
(
containerID
)
;
sb
.
append
(
status
.
getCapability
(
)
)
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
LOG
.
isDebugEnabled
(
)
?
status
.
getDiagnostics
(
)
:
)
;
sb
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
status
.
getExitStatus
(
)
)
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
status
.
getIPs
(
)
)
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
status
.
getHost
(
)
)
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
status
.
getExposedPorts
(
)
)
.
append
(
)
;
sb
.
append
(
)
;
sb
.
append
(
status
.
getContainerSubState
(
)
)
;
sb
.
append
(
)
;
@
SuppressWarnings
(
)
@
Override
public
void
handle
(
ContainerManagerEvent
event
)
{
switch
(
event
.
getType
(
)
)
{
case
FINISH_APPS
:
CMgrCompletedAppsEvent
appsFinishedEvent
=
(
CMgrCompletedAppsEvent
)
event
;
for
(
ApplicationId
appID
:
appsFinishedEvent
.
getAppsToCleanup
(
)
)
{
Application
app
=
this
.
context
.
getApplications
(
)
.
get
(
appID
)
;
if
(
app
==
null
)
{
String
diagnostic
=
;
if
(
appsFinishedEvent
.
getReason
(
)
==
CMgrCompletedAppsEvent
.
Reason
.
ON_SHUTDOWN
)
{
diagnostic
=
;
}
else
if
(
appsFinishedEvent
.
getReason
(
)
==
CMgrCompletedAppsEvent
.
Reason
.
BY_RESOURCEMANAGER
)
{
diagnostic
=
;
}
this
.
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ApplicationFinishEvent
(
appID
,
diagnostic
)
)
;
}
break
;
case
FINISH_CONTAINERS
:
CMgrCompletedContainersEvent
containersFinishedEvent
=
(
CMgrCompletedContainersEvent
)
event
;
for
(
ContainerId
containerId
:
containersFinishedEvent
.
getContainersToCleanup
(
)
)
{
ApplicationId
appId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
Application
app
=
this
.
context
.
getApplications
(
)
.
get
(
appId
)
;
if
(
app
==
null
)
{
LOG
.
warn
(
+
appId
+
+
)
;
continue
;
}
Container
container
=
app
.
getContainers
(
)
.
get
(
containerId
)
;
public
void
reInitializeContainer
(
ContainerId
containerId
,
ContainerLaunchContext
reInitLaunchContext
,
boolean
autoCommit
)
throws
YarnException
{
@
SuppressWarnings
(
)
private
void
internalSignalToContainer
(
SignalContainerRequest
request
,
String
sentBy
)
{
ContainerId
containerId
=
request
.
getContainerId
(
)
;
Container
container
=
this
.
context
.
getContainers
(
)
.
get
(
containerId
)
;
if
(
container
!=
null
)
{
private
List
<
LocalizationStatus
>
getLocalizationStatusesInternal
(
ContainerId
containerID
,
NMTokenIdentifier
nmTokenIdentifier
,
String
remoteUser
)
throws
YarnException
{
Container
container
=
this
.
context
.
getContainers
(
)
.
get
(
containerID
)
;
@
Override
public
void
handle
(
ApplicationEvent
event
)
{
this
.
writeLock
.
lock
(
)
;
try
{
ApplicationId
applicationID
=
event
.
getApplicationID
(
)
;
@
Override
public
void
handle
(
ContainerEvent
event
)
{
this
.
writeLock
.
lock
(
)
;
try
{
ContainerId
containerID
=
event
.
getContainerID
(
)
;
lfs
.
delete
(
subDir
,
true
)
;
}
catch
(
IOException
e
)
{
error
=
true
;
LOG
.
warn
(
+
subDir
)
;
}
}
else
{
for
(
Path
baseDir
:
baseDirs
)
{
Path
del
=
subDir
==
null
?
baseDir
:
new
Path
(
baseDir
,
subDir
)
;
LOG
.
debug
(
,
del
)
;
try
{
lfs
.
delete
(
del
,
true
)
;
}
catch
(
IOException
e
)
{
error
=
true
;
LOG
.
warn
(
+
subDir
)
;
}
}
}
}
else
{
try
{
@
Override
public
void
run
(
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
@
Override
public
void
run
(
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
LOG
.
info
(
+
containerIdStr
)
;
try
{
context
.
getNMStateStore
(
)
.
storeContainerKilled
(
containerId
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
containerId
+
,
e
)
;
}
boolean
alreadyLaunched
=
!
launch
.
markLaunched
(
)
||
launch
.
isLaunchCompleted
(
)
;
if
(
!
alreadyLaunched
)
{
LOG
.
info
(
+
containerIdStr
+
+
)
;
return
;
}
LOG
.
debug
(
,
containerIdStr
)
;
exec
.
deactivateContainer
(
containerId
)
;
Path
pidFilePath
=
launch
.
getPidFilePath
(
)
;
private
void
signalProcess
(
String
processId
,
String
user
,
String
containerIdStr
)
throws
IOException
{
private
void
signalProcess
(
String
processId
,
String
user
,
String
containerIdStr
)
throws
IOException
{
LOG
.
debug
(
,
processId
,
user
,
containerIdStr
)
;
final
ContainerExecutor
.
Signal
signal
=
sleepDelayBeforeSigKill
>
0
?
ContainerExecutor
.
Signal
.
TERM
:
ContainerExecutor
.
Signal
.
KILL
;
boolean
result
=
sendSignal
(
user
,
processId
,
signal
)
;
protected
int
prepareForLaunch
(
ContainerStartContext
ctx
)
throws
IOException
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
if
(
container
.
isMarkedForKilling
(
)
)
{
protected
void
handleContainerExitCode
(
int
exitCode
,
Path
containerLogDir
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
public
void
signalContainer
(
SignalContainerCommand
command
)
throws
IOException
{
ContainerId
containerId
=
container
.
getContainerTokenIdentifier
(
)
.
getContainerID
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
String
user
=
container
.
getUser
(
)
;
Signal
signal
=
translateCommandToSignal
(
command
)
;
if
(
signal
.
equals
(
Signal
.
NULL
)
)
{
String
containerIdStr
=
containerId
.
toString
(
)
;
String
user
=
container
.
getUser
(
)
;
Signal
signal
=
translateCommandToSignal
(
command
)
;
if
(
signal
.
equals
(
Signal
.
NULL
)
)
{
LOG
.
info
(
+
command
)
;
return
;
}
LOG
.
info
(
+
command
+
+
containerIdStr
)
;
boolean
alreadyLaunched
=
!
containerAlreadyLaunched
.
compareAndSet
(
false
,
true
)
;
if
(
!
alreadyLaunched
)
{
LOG
.
info
(
+
containerIdStr
+
+
)
;
return
;
}
LOG
.
debug
(
+
,
containerIdStr
,
(
pidFilePath
!=
null
?
pidFilePath
.
toString
(
)
:
)
)
;
try
{
String
processId
=
getContainerPid
(
)
;
if
(
processId
!=
null
)
{
if
(
signal
.
equals
(
Signal
.
NULL
)
)
{
LOG
.
info
(
+
command
)
;
return
;
}
LOG
.
info
(
+
command
+
+
containerIdStr
)
;
boolean
alreadyLaunched
=
!
containerAlreadyLaunched
.
compareAndSet
(
false
,
true
)
;
if
(
!
alreadyLaunched
)
{
LOG
.
info
(
+
containerIdStr
+
+
)
;
return
;
}
LOG
.
debug
(
+
,
containerIdStr
,
(
pidFilePath
!=
null
?
pidFilePath
.
toString
(
)
:
)
)
;
try
{
String
processId
=
getContainerPid
(
)
;
if
(
processId
!=
null
)
{
LOG
.
debug
(
,
processId
,
user
,
containerIdStr
)
;
boolean
result
=
exec
.
signalContainer
(
new
ContainerSignalContext
.
Builder
(
)
.
setContainer
(
container
)
.
setUser
(
user
)
.
setPid
(
processId
)
.
setSignal
(
signal
)
.
build
(
)
)
;
String
diagnostics
=
+
command
+
+
signal
+
+
processId
+
+
user
+
+
containerIdStr
+
+
(
result
?
:
)
;
public
void
pauseContainer
(
)
throws
IOException
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
public
void
pauseContainer
(
)
throws
IOException
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
LOG
.
info
(
+
containerIdStr
)
;
if
(
!
shouldPauseContainer
.
compareAndSet
(
false
,
true
)
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
LOG
.
info
(
+
containerIdStr
)
;
if
(
!
shouldPauseContainer
.
compareAndSet
(
false
,
true
)
)
{
LOG
.
info
(
+
containerId
+
+
)
;
return
;
}
try
{
exec
.
pauseContainer
(
container
)
;
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ContainerEvent
(
containerId
,
ContainerEventType
.
CONTAINER_PAUSED
)
)
;
try
{
this
.
context
.
getNMStateStore
(
)
.
storeContainerPaused
(
container
.
getContainerId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
container
.
getContainerId
(
)
+
,
e
)
;
}
}
catch
(
Exception
e
)
{
String
message
=
+
containerIdStr
+
+
StringUtils
.
stringifyException
(
e
)
;
public
void
resumeContainer
(
)
throws
IOException
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
public
void
resumeContainer
(
)
throws
IOException
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
LOG
.
info
(
+
containerIdStr
)
;
boolean
alreadyPaused
=
!
shouldPauseContainer
.
compareAndSet
(
false
,
true
)
;
if
(
!
alreadyPaused
)
{
String
containerIdStr
=
containerId
.
toString
(
)
;
LOG
.
info
(
+
containerIdStr
)
;
boolean
alreadyPaused
=
!
shouldPauseContainer
.
compareAndSet
(
false
,
true
)
;
if
(
!
alreadyPaused
)
{
LOG
.
info
(
+
containerIdStr
+
+
)
;
return
;
}
try
{
exec
.
resumeContainer
(
container
)
;
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ContainerEvent
(
containerId
,
ContainerEventType
.
CONTAINER_RESUMED
)
)
;
try
{
this
.
context
.
getNMStateStore
(
)
.
removeContainerPaused
(
container
.
getContainerId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
container
.
getContainerId
(
)
+
,
e
)
;
}
}
catch
(
Exception
e
)
{
String
message
=
+
containerIdStr
+
+
StringUtils
.
stringifyException
(
e
)
;
protected
void
cleanupContainerFiles
(
Path
containerWorkDir
)
{
try
{
Path
containerWorkDir
=
getContainerWorkDir
(
)
;
cleanupContainerFiles
(
containerWorkDir
)
;
containerLogDir
=
getContainerLogDir
(
)
;
Map
<
Path
,
List
<
String
>>
localResources
=
getLocalizedResources
(
)
;
String
appIdStr
=
app
.
getAppId
(
)
.
toString
(
)
;
Path
nmPrivateContainerScriptPath
=
getNmPrivateContainerScriptPath
(
appIdStr
,
containerIdStr
)
;
Path
nmPrivateTokensPath
=
getNmPrivateTokensPath
(
appIdStr
,
containerIdStr
)
;
Path
nmPrivateKeystorePath
=
(
container
.
getCredentials
(
)
.
getSecretKey
(
AMSecretKeys
.
YARN_APPLICATION_AM_KEYSTORE
)
==
null
)
?
null
:
getNmPrivateKeystorePath
(
appIdStr
,
containerIdStr
)
;
Path
nmPrivateTruststorePath
=
(
container
.
getCredentials
(
)
.
getSecretKey
(
AMSecretKeys
.
YARN_APPLICATION_AM_TRUSTSTORE
)
==
null
)
?
null
:
getNmPrivateTruststorePath
(
appIdStr
,
containerIdStr
)
;
try
{
pidFilePath
=
getPidFilePath
(
appIdStr
,
containerIdStr
)
;
}
catch
(
IOException
e
)
{
String
pidFileSubpath
=
getPidFileSubpath
(
appIdStr
,
containerIdStr
)
;
pidFilePath
=
dirsHandler
.
getLocalPathForWrite
(
pidFileSubpath
)
;
catch
(
IOException
e
)
{
String
pidFileSubpath
=
getPidFileSubpath
(
appIdStr
,
containerIdStr
)
;
pidFilePath
=
dirsHandler
.
getLocalPathForWrite
(
pidFileSubpath
)
;
}
LOG
.
info
(
+
+
containerWorkDir
.
toString
(
)
+
+
containerLogDir
.
toString
(
)
+
+
nmPrivateContainerScriptPath
.
toString
(
)
+
+
nmPrivateTokensPath
.
toString
(
)
+
+
pidFilePath
.
toString
(
)
)
;
List
<
String
>
localDirs
=
dirsHandler
.
getLocalDirs
(
)
;
List
<
String
>
logDirs
=
dirsHandler
.
getLogDirs
(
)
;
List
<
String
>
containerLocalDirs
=
getContainerLocalDirs
(
localDirs
)
;
List
<
String
>
containerLogDirs
=
getContainerLogDirs
(
logDirs
)
;
List
<
String
>
filecacheDirs
=
getNMFilecacheDirs
(
localDirs
)
;
List
<
String
>
userLocalDirs
=
getUserLocalDirs
(
localDirs
)
;
List
<
String
>
userFilecacheDirs
=
getUserFilecacheDirs
(
localDirs
)
;
List
<
String
>
applicationLocalDirs
=
getApplicationLocalDirs
(
localDirs
,
appIdStr
)
;
if
(
!
dirsHandler
.
areDisksHealthy
(
)
)
{
ret
=
ContainerExitStatus
.
DISKS_FAILED
;
throw
new
IOException
(
+
dirsHandler
.
getDisksHealthReport
(
false
)
)
;
launch
=
new
RecoveredContainerLaunch
(
context
,
getConfig
(
)
,
dispatcher
,
exec
,
app
,
event
.
getContainer
(
)
,
dirsHandler
,
containerManager
)
;
containerLauncher
.
submit
(
launch
)
;
running
.
put
(
containerId
,
launch
)
;
break
;
case
RECOVER_PAUSED_CONTAINER
:
app
=
context
.
getApplications
(
)
.
get
(
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
)
;
launch
=
new
RecoverPausedContainerLaunch
(
context
,
getConfig
(
)
,
dispatcher
,
exec
,
app
,
event
.
getContainer
(
)
,
dirsHandler
,
containerManager
)
;
containerLauncher
.
submit
(
launch
)
;
break
;
case
CLEANUP_CONTAINER
:
cleanup
(
event
,
containerId
,
true
)
;
break
;
case
CLEANUP_CONTAINER_FOR_REINIT
:
cleanup
(
event
,
containerId
,
false
)
;
break
;
case
SIGNAL_CONTAINER
:
SignalContainersLauncherEvent
signalEvent
=
(
SignalContainersLauncherEvent
)
event
;
ContainerLaunch
runningContainer
=
running
.
get
(
containerId
)
;
if
(
runningContainer
==
null
)
{
ContainerLaunch
runningContainer
=
running
.
get
(
containerId
)
;
if
(
runningContainer
==
null
)
{
LOG
.
info
(
+
containerId
+
)
;
return
;
}
try
{
runningContainer
.
signalContainer
(
signalEvent
.
getCommand
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
warn
(
+
containerId
+
+
signalEvent
.
getCommand
(
)
)
;
}
break
;
case
PAUSE_CONTAINER
:
ContainerLaunch
launchedContainer
=
running
.
get
(
containerId
)
;
if
(
launchedContainer
==
null
)
{
return
;
}
try
{
launchedContainer
.
pauseContainer
(
)
;
}
catch
(
Exception
e
)
{
catch
(
IOException
e
)
{
LOG
.
warn
(
+
containerId
+
+
signalEvent
.
getCommand
(
)
)
;
}
break
;
case
PAUSE_CONTAINER
:
ContainerLaunch
launchedContainer
=
running
.
get
(
containerId
)
;
if
(
launchedContainer
==
null
)
{
return
;
}
try
{
launchedContainer
.
pauseContainer
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
StringUtils
.
stringifyException
(
e
)
)
;
}
break
;
case
RESUME_CONTAINER
:
ContainerLaunch
launchCont
=
running
.
get
(
containerId
)
;
if
(
launchCont
==
null
)
{
return
;
}
try
{
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ContainerEvent
(
containerId
,
ContainerEventType
.
RECOVER_PAUSED_CONTAINER
)
)
;
boolean
notInterrupted
=
true
;
try
{
File
pidFile
=
locatePidFile
(
appIdStr
,
containerIdStr
)
;
if
(
pidFile
!=
null
)
{
String
pidPathStr
=
pidFile
.
getPath
(
)
;
pidFilePath
=
new
Path
(
pidPathStr
)
;
exec
.
activateContainer
(
containerId
,
pidFilePath
)
;
retCode
=
exec
.
reacquireContainer
(
new
ContainerReacquisitionContext
.
Builder
(
)
.
setContainer
(
container
)
.
setUser
(
container
.
getUser
(
)
)
.
setContainerId
(
containerId
)
.
build
(
)
)
;
}
else
{
LOG
.
warn
(
+
containerIdStr
)
;
}
}
catch
(
InterruptedException
|
InterruptedIOException
e
)
{
LOG
.
warn
(
+
containerId
)
;
notInterrupted
=
false
;
}
catch
(
IOException
e
)
{
retCode
=
exec
.
reacquireContainer
(
new
ContainerReacquisitionContext
.
Builder
(
)
.
setContainer
(
container
)
.
setUser
(
container
.
getUser
(
)
)
.
setContainerId
(
containerId
)
.
build
(
)
)
;
}
else
{
LOG
.
warn
(
+
containerIdStr
)
;
}
}
catch
(
InterruptedException
|
InterruptedIOException
e
)
{
LOG
.
warn
(
+
containerId
)
;
notInterrupted
=
false
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
containerIdStr
,
e
)
;
}
finally
{
if
(
notInterrupted
)
{
this
.
completed
.
set
(
true
)
;
exec
.
deactivateContainer
(
containerId
)
;
try
{
getContext
(
)
.
getNMStateStore
(
)
.
storeContainerCompleted
(
containerId
,
retCode
)
;
}
catch
(
IOException
e
)
{
catch
(
InterruptedException
|
InterruptedIOException
e
)
{
LOG
.
warn
(
+
containerId
)
;
notInterrupted
=
false
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
containerIdStr
,
e
)
;
}
finally
{
if
(
notInterrupted
)
{
this
.
completed
.
set
(
true
)
;
exec
.
deactivateContainer
(
containerId
)
;
try
{
getContext
(
)
.
getNMStateStore
(
)
.
storeContainerCompleted
(
containerId
,
retCode
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
containerId
)
;
}
}
}
if
(
retCode
!=
0
)
{
LOG
.
warn
(
+
retCode
)
;
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ContainerEvent
(
containerId
,
ContainerEventType
.
CONTAINER_LAUNCHED
)
)
;
boolean
notInterrupted
=
true
;
try
{
File
pidFile
=
locatePidFile
(
appIdStr
,
containerIdStr
)
;
if
(
pidFile
!=
null
)
{
String
pidPathStr
=
pidFile
.
getPath
(
)
;
pidFilePath
=
new
Path
(
pidPathStr
)
;
exec
.
activateContainer
(
containerId
,
pidFilePath
)
;
retCode
=
exec
.
reacquireContainer
(
new
ContainerReacquisitionContext
.
Builder
(
)
.
setContainer
(
container
)
.
setUser
(
container
.
getUser
(
)
)
.
setContainerId
(
containerId
)
.
build
(
)
)
;
}
else
{
LOG
.
warn
(
+
containerIdStr
)
;
}
}
catch
(
InterruptedException
|
InterruptedIOException
e
)
{
LOG
.
warn
(
+
containerId
)
;
notInterrupted
=
false
;
}
catch
(
IOException
e
)
{
retCode
=
exec
.
reacquireContainer
(
new
ContainerReacquisitionContext
.
Builder
(
)
.
setContainer
(
container
)
.
setUser
(
container
.
getUser
(
)
)
.
setContainerId
(
containerId
)
.
build
(
)
)
;
}
else
{
LOG
.
warn
(
+
containerIdStr
)
;
}
}
catch
(
InterruptedException
|
InterruptedIOException
e
)
{
LOG
.
warn
(
+
containerId
)
;
notInterrupted
=
false
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
containerIdStr
,
e
)
;
}
finally
{
if
(
notInterrupted
)
{
this
.
completed
.
set
(
true
)
;
exec
.
deactivateContainer
(
containerId
)
;
try
{
getContext
(
)
.
getNMStateStore
(
)
.
storeContainerCompleted
(
containerId
,
retCode
)
;
}
catch
(
IOException
e
)
{
catch
(
InterruptedException
|
InterruptedIOException
e
)
{
LOG
.
warn
(
+
containerId
)
;
notInterrupted
=
false
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
containerIdStr
,
e
)
;
}
finally
{
if
(
notInterrupted
)
{
this
.
completed
.
set
(
true
)
;
exec
.
deactivateContainer
(
containerId
)
;
try
{
getContext
(
)
.
getNMStateStore
(
)
.
storeContainerCompleted
(
containerId
,
retCode
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
containerId
)
;
}
}
}
if
(
retCode
!=
0
)
{
LOG
.
warn
(
+
retCode
)
;
public
String
executePrivilegedOperation
(
List
<
String
>
prefixCommands
,
PrivilegedOperation
operation
,
File
workingDir
,
Map
<
String
,
String
>
env
,
boolean
grabOutput
,
boolean
inheritParentEnv
)
throws
PrivilegedOperationException
{
String
[
]
fullCommandArray
=
getPrivilegedOperationExecutionCommand
(
prefixCommands
,
operation
)
;
ShellCommandExecutor
exec
=
new
ShellCommandExecutor
(
fullCommandArray
,
workingDir
,
env
,
0L
,
inheritParentEnv
)
;
try
{
exec
.
execute
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
public
String
executePrivilegedOperation
(
List
<
String
>
prefixCommands
,
PrivilegedOperation
operation
,
File
workingDir
,
Map
<
String
,
String
>
env
,
boolean
grabOutput
,
boolean
inheritParentEnv
)
throws
PrivilegedOperationException
{
String
[
]
fullCommandArray
=
getPrivilegedOperationExecutionCommand
(
prefixCommands
,
operation
)
;
ShellCommandExecutor
exec
=
new
ShellCommandExecutor
(
fullCommandArray
,
workingDir
,
env
,
0L
,
inheritParentEnv
)
;
try
{
exec
.
execute
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
LOG
.
debug
(
Arrays
.
toString
(
fullCommandArray
)
)
;
LOG
.
debug
(
)
;
public
IOStreamPair
executePrivilegedInteractiveOperation
(
List
<
String
>
prefixCommands
,
PrivilegedOperation
operation
)
throws
PrivilegedOperationException
,
InterruptedException
{
String
[
]
fullCommandArray
=
getPrivilegedOperationExecutionCommand
(
prefixCommands
,
operation
)
;
ProcessBuilder
pb
=
new
ProcessBuilder
(
fullCommandArray
)
;
OutputStream
stdin
;
InputStream
stdout
;
try
{
pb
.
redirectErrorStream
(
true
)
;
Process
p
=
pb
.
start
(
)
;
stdin
=
p
.
getOutputStream
(
)
;
stdout
=
p
.
getInputStream
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
resetCGroupParameters
(
)
;
LOG
.
info
(
)
;
return
;
}
}
LOG
.
info
(
String
.
format
(
,
yarnCGroupPath
,
oomListenerPath
)
)
;
executor
=
Executors
.
newFixedThreadPool
(
2
)
;
Future
<
String
>
errorListener
=
executor
.
submit
(
(
)
->
IOUtils
.
toString
(
process
.
getErrorStream
(
)
,
Charset
.
defaultCharset
(
)
)
)
;
InputStream
events
=
process
.
getInputStream
(
)
;
byte
[
]
event
=
new
byte
[
8
]
;
int
read
;
while
(
(
read
=
events
.
read
(
event
)
)
==
event
.
length
)
{
resolveOOM
(
executor
)
;
}
if
(
read
!=
-
1
)
{
LOG
.
warn
(
String
.
format
(
,
read
)
)
;
}
int
exitCode
=
process
.
waitFor
(
)
;
String
error
=
errorListener
.
get
(
)
;
private
void
mountCGroupController
(
CGroupController
controller
)
throws
ResourceHandlerException
{
String
existingMountPath
=
getControllerPath
(
controller
)
;
String
requestedMountPath
=
new
File
(
cGroupsMountConfig
.
getMountPath
(
)
,
controller
.
getName
(
)
)
.
getAbsolutePath
(
)
;
if
(
existingMountPath
==
null
||
!
requestedMountPath
.
equals
(
existingMountPath
)
)
{
rwLock
.
writeLock
(
)
.
lock
(
)
;
try
{
String
mountOptions
;
if
(
existingMountPath
!=
null
)
{
mountOptions
=
Joiner
.
on
(
','
)
.
join
(
parsedMtab
.
get
(
existingMountPath
)
)
;
}
else
{
mountOptions
=
controller
.
getName
(
)
;
}
String
cGroupKV
=
mountOptions
+
+
requestedMountPath
;
PrivilegedOperation
.
OperationType
opType
=
PrivilegedOperation
.
OperationType
.
MOUNT_CGROUPS
;
PrivilegedOperation
op
=
new
PrivilegedOperation
(
opType
)
;
op
.
appendArgs
(
cGroupPrefix
,
cGroupKV
)
;
rwLock
.
writeLock
(
)
.
lock
(
)
;
try
{
String
mountOptions
;
if
(
existingMountPath
!=
null
)
{
mountOptions
=
Joiner
.
on
(
','
)
.
join
(
parsedMtab
.
get
(
existingMountPath
)
)
;
}
else
{
mountOptions
=
controller
.
getName
(
)
;
}
String
cGroupKV
=
mountOptions
+
+
requestedMountPath
;
PrivilegedOperation
.
OperationType
opType
=
PrivilegedOperation
.
OperationType
.
MOUNT_CGROUPS
;
PrivilegedOperation
op
=
new
PrivilegedOperation
(
opType
)
;
op
.
appendArgs
(
cGroupPrefix
,
cGroupKV
)
;
LOG
.
info
(
+
controller
.
getName
(
)
+
+
requestedMountPath
)
;
privilegedOperationExecutor
.
executePrivilegedOperation
(
op
,
false
)
;
controllerPaths
.
put
(
controller
,
requestedMountPath
)
;
}
catch
(
PrivilegedOperationException
e
)
{
@
Override
public
String
createCGroup
(
CGroupController
controller
,
String
cGroupId
)
throws
ResourceHandlerException
{
String
path
=
getPathForCGroup
(
controller
,
cGroupId
)
;
private
void
logLineFromTasksFile
(
File
cgf
)
{
String
str
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
(
BufferedReader
inl
=
new
BufferedReader
(
new
InputStreamReader
(
new
FileInputStream
(
cgf
+
)
,
)
)
)
{
str
=
inl
.
readLine
(
)
;
if
(
str
!=
null
)
{
@
Override
public
void
deleteCGroup
(
CGroupController
controller
,
String
cGroupId
)
throws
ResourceHandlerException
{
boolean
deleted
=
false
;
String
cGroupPath
=
getPathForCGroup
(
controller
,
cGroupId
)
;
@
Override
public
void
updateCGroupParam
(
CGroupController
controller
,
String
cGroupId
,
String
param
,
String
value
)
throws
ResourceHandlerException
{
String
cGroupParamPath
=
getPathForCGroupParam
(
controller
,
cGroupId
,
param
)
;
PrintWriter
pw
=
null
;
@
Override
public
float
getCpuUsagePercent
(
)
{
float
cgroupUsage
=
cgroup
.
getCpuUsagePercent
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
float
procfsUsage
=
procfs
.
getCpuUsagePercent
(
)
;
@
Override
public
float
getCpuUsagePercent
(
)
{
float
cgroupUsage
=
cgroup
.
getCpuUsagePercent
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
float
procfsUsage
=
procfs
.
getCpuUsagePercent
(
)
;
LOG
.
debug
(
+
procfsUsage
+
+
cgroupUsage
)
;
@
Override
public
long
getRssMemorySize
(
int
olderThanAge
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
long
getVirtualMemorySize
(
int
olderThanAge
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
boolean
sigKill
(
Container
container
)
{
boolean
containerKilled
=
false
;
boolean
finished
=
false
;
try
{
while
(
!
finished
)
{
String
[
]
pids
=
cgroups
.
getCGroupParam
(
CGroupsHandler
.
CGroupController
.
MEMORY
,
container
.
getContainerId
(
)
.
toString
(
)
,
CGROUP_PROCS_FILE
)
.
split
(
)
;
finished
=
true
;
for
(
String
pid
:
pids
)
{
if
(
pid
!=
null
&&
!
pid
.
isEmpty
(
)
)
{
@
Override
public
List
<
PrivilegedOperation
>
postComplete
(
ContainerId
containerId
)
throws
ResourceHandlerException
{
public
static
NetworkTagMappingManager
getManager
(
Configuration
conf
)
{
Class
<
?
extends
NetworkTagMappingManager
>
managerClass
=
conf
.
getClass
(
YarnConfiguration
.
NM_NETWORK_TAG_MAPPING_MANAGER
,
NetworkTagMappingJsonManager
.
class
,
NetworkTagMappingManager
.
class
)
;
private
static
CGroupsHandler
getInitializedCGroupsHandler
(
Configuration
conf
)
throws
ResourceHandlerException
{
if
(
cGroupsHandler
==
null
)
{
synchronized
(
CGroupsHandler
.
class
)
{
if
(
cGroupsHandler
==
null
)
{
cGroupsHandler
=
new
CGroupsHandlerImpl
(
conf
,
PrivilegedOperationExecutor
.
getInstance
(
conf
)
)
;
@
Override
public
List
<
PrivilegedOperation
>
reacquireContainer
(
ContainerId
containerId
)
throws
ResourceHandlerException
{
String
containerIdStr
=
containerId
.
toString
(
)
;
@
Override
public
List
<
PrivilegedOperation
>
reacquireContainer
(
ContainerId
containerId
)
throws
ResourceHandlerException
{
String
containerIdStr
=
containerId
.
toString
(
)
;
LOG
.
debug
(
,
containerIdStr
)
;
String
classIdStrFromFile
=
cGroupsHandler
.
getCGroupParam
(
CGroupsHandler
.
CGroupController
.
NET_CLS
,
containerIdStr
,
CGroupsHandler
.
CGROUP_PARAM_CLASSID
)
;
int
classId
=
trafficController
.
getClassIdFromFileContents
(
classIdStrFromFile
)
;
@
Override
public
List
<
PrivilegedOperation
>
postComplete
(
ContainerId
containerId
)
throws
ResourceHandlerException
{
private
boolean
checkIfAlreadyBootstrapped
(
String
state
)
throws
ResourceHandlerException
{
List
<
String
>
regexes
=
new
ArrayList
<
>
(
)
;
regexes
.
add
(
String
.
format
(
,
ROOT_QDISC_HANDLE
)
)
;
regexes
.
add
(
String
.
format
(
+
,
ROOT_QDISC_HANDLE
)
)
;
regexes
.
add
(
String
.
format
(
,
ROOT_QDISC_HANDLE
,
ROOT_CLASS_ID
)
)
;
regexes
.
add
(
String
.
format
(
,
ROOT_QDISC_HANDLE
,
DEFAULT_CLASS_ID
,
ROOT_QDISC_HANDLE
,
ROOT_CLASS_ID
)
)
;
regexes
.
add
(
String
.
format
(
,
ROOT_QDISC_HANDLE
,
YARN_ROOT_CLASS_ID
,
ROOT_QDISC_HANDLE
,
ROOT_CLASS_ID
)
)
;
for
(
String
regex
:
regexes
)
{
Pattern
pattern
=
Pattern
.
compile
(
regex
,
Pattern
.
MULTILINE
)
;
if
(
pattern
.
matcher
(
state
)
.
find
(
)
)
{
private
String
readState
(
)
throws
ResourceHandlerException
{
BatchBuilder
builder
=
new
BatchBuilder
(
PrivilegedOperation
.
OperationType
.
TC_READ_STATE
)
.
readState
(
)
;
PrivilegedOperation
op
=
builder
.
commitBatchToTempFile
(
)
;
try
{
String
output
=
privilegedOperationExecutor
.
executePrivilegedOperation
(
op
,
true
)
;
private
void
reacquireContainerClasses
(
String
state
)
{
String
tcClassesStr
=
state
.
substring
(
state
.
indexOf
(
)
)
;
String
[
]
tcClasses
=
Pattern
.
compile
(
,
Pattern
.
MULTILINE
)
.
split
(
tcClassesStr
)
;
Pattern
tcClassPattern
=
Pattern
.
compile
(
String
.
format
(
,
ROOT_QDISC_HANDLE
)
)
;
synchronized
(
classIdSet
)
{
for
(
String
tcClassSplit
:
tcClasses
)
{
String
tcClass
=
tcClassSplit
.
trim
(
)
;
if
(
!
tcClass
.
isEmpty
(
)
)
{
Matcher
classMatcher
=
tcClassPattern
.
matcher
(
tcClass
)
;
if
(
classMatcher
.
matches
(
)
)
{
int
classId
=
Integer
.
parseInt
(
classMatcher
.
group
(
1
)
)
;
if
(
classId
>=
MIN_CONTAINER_CLASS_ID
)
{
classIdSet
.
set
(
classId
-
MIN_CONTAINER_CLASS_ID
)
;
public
Map
<
Integer
,
Integer
>
readStats
(
)
throws
ResourceHandlerException
{
BatchBuilder
builder
=
new
BatchBuilder
(
PrivilegedOperation
.
OperationType
.
TC_READ_STATS
)
.
readClasses
(
)
;
PrivilegedOperation
op
=
builder
.
commitBatchToTempFile
(
)
;
try
{
String
output
=
privilegedOperationExecutor
.
executePrivilegedOperation
(
op
,
true
)
;
public
Map
<
Integer
,
Integer
>
readStats
(
)
throws
ResourceHandlerException
{
BatchBuilder
builder
=
new
BatchBuilder
(
PrivilegedOperation
.
OperationType
.
TC_READ_STATS
)
.
readClasses
(
)
;
PrivilegedOperation
op
=
builder
.
commitBatchToTempFile
(
)
;
try
{
String
output
=
privilegedOperationExecutor
.
executePrivilegedOperation
(
op
,
true
)
;
LOG
.
debug
(
,
output
)
;
Map
<
Integer
,
Integer
>
classIdBytesStats
=
parseStatsString
(
output
)
;
public
int
getClassIdFromFileContents
(
String
input
)
{
String
classIdStr
=
String
.
format
(
,
Integer
.
parseInt
(
input
)
)
;
public
synchronized
void
updateFpga
(
String
requestor
,
FpgaDevice
device
,
String
newIPID
,
String
newHash
)
{
device
.
setIPID
(
newIPID
)
;
device
.
setAocxHash
(
newHash
)
;
public
synchronized
void
updateFpga
(
String
requestor
,
FpgaDevice
device
,
String
newIPID
,
String
newHash
)
{
device
.
setIPID
(
newIPID
)
;
device
.
setAocxHash
(
newHash
)
;
LOG
.
info
(
+
newIPID
+
+
device
)
;
@
Override
public
List
<
PrivilegedOperation
>
preStart
(
Container
container
)
throws
ResourceHandlerException
{
List
<
PrivilegedOperation
>
ret
=
new
ArrayList
<
>
(
)
;
String
containerIdStr
=
container
.
getContainerId
(
)
.
toString
(
)
;
Resource
requestedResource
=
container
.
getResource
(
)
;
cGroupsHandler
.
createCGroup
(
CGroupsHandler
.
CGroupController
.
DEVICES
,
containerIdStr
)
;
long
deviceCount
=
requestedResource
.
getResourceValue
(
FPGA_URI
)
;
Resource
requestedResource
=
container
.
getResource
(
)
;
cGroupsHandler
.
createCGroup
(
CGroupsHandler
.
CGroupController
.
DEVICES
,
containerIdStr
)
;
long
deviceCount
=
requestedResource
.
getResourceValue
(
FPGA_URI
)
;
LOG
.
info
(
containerIdStr
+
+
deviceCount
+
)
;
String
ipFilePath
=
null
;
try
{
final
String
requestedIPID
=
getRequestedIPID
(
container
)
;
String
localizedIPIDHash
=
null
;
ipFilePath
=
vendorPlugin
.
retrieveIPfilePath
(
requestedIPID
,
container
.
getWorkDir
(
)
,
container
.
getResourceSet
(
)
.
getLocalizedResources
(
)
)
;
if
(
ipFilePath
!=
null
)
{
try
(
FileInputStream
fis
=
new
FileInputStream
(
ipFilePath
)
)
{
localizedIPIDHash
=
DigestUtils
.
sha256Hex
(
fis
)
;
}
catch
(
IOException
e
)
{
throw
new
ResourceHandlerException
(
,
e
)
;
}
}
FpgaResourceAllocator
.
FpgaAllocation
allocation
=
allocator
.
assignFpga
(
vendorPlugin
.
getFpgaType
(
)
,
deviceCount
,
container
,
localizedIPIDHash
)
;
}
catch
(
IOException
e
)
{
throw
new
ResourceHandlerException
(
,
e
)
;
}
}
FpgaResourceAllocator
.
FpgaAllocation
allocation
=
allocator
.
assignFpga
(
vendorPlugin
.
getFpgaType
(
)
,
deviceCount
,
container
,
localizedIPIDHash
)
;
LOG
.
info
(
+
allocation
)
;
PrivilegedOperation
privilegedOperation
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
FPGA
,
Arrays
.
asList
(
CONTAINER_ID_CLI_OPTION
,
containerIdStr
)
)
;
if
(
!
allocation
.
getDenied
(
)
.
isEmpty
(
)
)
{
List
<
Integer
>
denied
=
new
ArrayList
<
>
(
)
;
allocation
.
getDenied
(
)
.
forEach
(
device
->
denied
.
add
(
device
.
getMinor
(
)
)
)
;
privilegedOperation
.
appendArgs
(
Arrays
.
asList
(
EXCLUDED_FPGAS_CLI_OPTION
,
StringUtils
.
join
(
,
denied
)
)
)
;
}
privilegedOperationExecutor
.
executePrivilegedOperation
(
privilegedOperation
,
true
)
;
if
(
deviceCount
>
0
)
{
ipFilePath
=
vendorPlugin
.
retrieveIPfilePath
(
getRequestedIPID
(
container
)
,
container
.
getWorkDir
(
)
,
container
.
getResourceSet
(
)
.
getLocalizedResources
(
)
)
;
if
(
ipFilePath
==
null
)
{
LOG
.
warn
(
+
+
REQUEST_FPGA_IP_ID_KEY
+
)
;
}
else
{
private
synchronized
GpuAllocation
internalAssignGpus
(
Container
container
)
throws
ResourceHandlerException
{
Resource
requestedResource
=
container
.
getResource
(
)
;
ContainerId
containerId
=
container
.
getContainerId
(
)
;
int
numRequestedGpuDevices
=
getRequestedGpus
(
requestedResource
)
;
if
(
numRequestedGpuDevices
>
0
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
synchronized
void
unassignGpus
(
ContainerId
containerId
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
List
<
PrivilegedOperation
>
bootstrap
(
Configuration
configuration
)
throws
ResourceHandlerException
{
List
<
GpuDevice
>
usableGpus
;
try
{
usableGpus
=
gpuDiscoverer
.
getGpusUsableByYarn
(
)
;
if
(
usableGpus
==
null
||
usableGpus
.
isEmpty
(
)
)
{
String
message
=
+
;
public
boolean
isResourcesAvailable
(
Resource
resource
)
{
private
NumaResourceAllocation
allocate
(
ContainerId
containerId
,
Resource
resource
)
{
for
(
int
index
=
0
;
index
<
numaNodesList
.
size
(
)
;
index
++
)
{
NumaNodeResource
numaNode
=
numaNodesList
.
get
(
(
currentAssignNode
+
index
)
%
numaNodesList
.
size
(
)
)
;
if
(
numaNode
.
isResourcesAvailable
(
resource
)
)
{
numaNode
.
assignResources
(
resource
,
containerId
)
;
NumaNodeResource
numaNode
=
numaNodesList
.
get
(
(
currentAssignNode
+
index
)
%
numaNodesList
.
size
(
)
)
;
if
(
numaNode
.
isResourcesAvailable
(
resource
)
)
{
numaNode
.
assignResources
(
resource
,
containerId
)
;
LOG
.
info
(
+
numaNode
.
getNodeId
(
)
+
+
numaNode
.
getNodeId
(
)
+
+
containerId
)
;
currentAssignNode
=
(
currentAssignNode
+
index
+
1
)
%
numaNodesList
.
size
(
)
;
return
new
NumaResourceAllocation
(
numaNode
.
getNodeId
(
)
,
resource
.
getMemorySize
(
)
,
numaNode
.
getNodeId
(
)
,
resource
.
getVirtualCores
(
)
)
;
}
}
long
memoryRequirement
=
resource
.
getMemorySize
(
)
;
Map
<
String
,
Long
>
memoryAllocations
=
Maps
.
newHashMap
(
)
;
for
(
NumaNodeResource
numaNode
:
numaNodesList
)
{
long
memoryRemaining
=
numaNode
.
assignAvailableMemory
(
memoryRequirement
,
containerId
)
;
memoryAllocations
.
put
(
numaNode
.
getNodeId
(
)
,
memoryRequirement
-
memoryRemaining
)
;
memoryRequirement
=
memoryRemaining
;
if
(
memoryRequirement
==
0
)
{
break
;
}
}
if
(
memoryRequirement
!=
0
)
{
if
(
memoryRequirement
==
0
)
{
break
;
}
}
if
(
memoryRequirement
!=
0
)
{
LOG
.
info
(
+
resource
.
getMemorySize
(
)
+
+
containerId
)
;
releaseNumaResource
(
containerId
)
;
return
null
;
}
int
cpusRequirement
=
resource
.
getVirtualCores
(
)
;
Map
<
String
,
Integer
>
cpuAllocations
=
Maps
.
newHashMap
(
)
;
for
(
int
index
=
0
;
index
<
numaNodesList
.
size
(
)
;
index
++
)
{
NumaNodeResource
numaNode
=
numaNodesList
.
get
(
(
currentAssignNode
+
index
)
%
numaNodesList
.
size
(
)
)
;
int
cpusRemaining
=
numaNode
.
assignAvailableCpus
(
cpusRequirement
,
containerId
)
;
cpuAllocations
.
put
(
numaNode
.
getNodeId
(
)
,
cpusRequirement
-
cpusRemaining
)
;
cpusRequirement
=
cpusRemaining
;
if
(
cpusRequirement
==
0
)
{
currentAssignNode
=
(
currentAssignNode
+
index
+
1
)
%
numaNodesList
.
size
(
)
;
if
(
memoryRequirement
!=
0
)
{
LOG
.
info
(
+
resource
.
getMemorySize
(
)
+
+
containerId
)
;
releaseNumaResource
(
containerId
)
;
return
null
;
}
int
cpusRequirement
=
resource
.
getVirtualCores
(
)
;
Map
<
String
,
Integer
>
cpuAllocations
=
Maps
.
newHashMap
(
)
;
for
(
int
index
=
0
;
index
<
numaNodesList
.
size
(
)
;
index
++
)
{
NumaNodeResource
numaNode
=
numaNodesList
.
get
(
(
currentAssignNode
+
index
)
%
numaNodesList
.
size
(
)
)
;
int
cpusRemaining
=
numaNode
.
assignAvailableCpus
(
cpusRequirement
,
containerId
)
;
cpuAllocations
.
put
(
numaNode
.
getNodeId
(
)
,
cpusRequirement
-
cpusRemaining
)
;
cpusRequirement
=
cpusRemaining
;
if
(
cpusRequirement
==
0
)
{
currentAssignNode
=
(
currentAssignNode
+
index
+
1
)
%
numaNodesList
.
size
(
)
;
break
;
}
}
if
(
cpusRequirement
!=
0
)
{
public
synchronized
void
releaseNumaResource
(
ContainerId
containerId
)
{
private
String
runDockerVolumeCommand
(
DockerVolumeCommand
dockerVolumeCommand
,
Container
container
)
throws
ContainerExecutionException
{
try
{
String
commandFile
=
dockerClient
.
writeCommandToTempFile
(
dockerVolumeCommand
,
container
.
getContainerId
(
)
,
nmContext
)
;
PrivilegedOperation
privOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
RUN_DOCKER_CMD
)
;
privOp
.
appendArgs
(
commandFile
)
;
String
output
=
privilegedOperationExecutor
.
executePrivilegedOperation
(
null
,
privOp
,
null
,
null
,
true
,
false
)
;
private
void
setHostname
(
DockerRunCommand
runCommand
,
String
containerIdStr
,
String
network
,
String
name
)
throws
ContainerExecutionException
{
if
(
network
.
equalsIgnoreCase
(
)
)
{
if
(
name
!=
null
&&
!
name
.
isEmpty
(
)
)
{
dockerExecCommand
.
setTTY
(
)
;
List
<
String
>
command
=
new
ArrayList
<
String
>
(
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
)
;
sb
.
append
(
ctx
.
getShell
(
)
)
;
command
.
add
(
sb
.
toString
(
)
)
;
command
.
add
(
)
;
dockerExecCommand
.
setOverrideCommandWithArgs
(
command
)
;
String
commandFile
=
dockerClient
.
writeCommandToTempFile
(
dockerExecCommand
,
ContainerId
.
fromString
(
containerId
)
,
nmContext
)
;
PrivilegedOperation
privOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
EXEC_CONTAINER
)
;
privOp
.
appendArgs
(
commandFile
)
;
privOp
.
disableFailureLogging
(
)
;
IOStreamPair
output
;
try
{
output
=
privilegedOperationExecutor
.
executePrivilegedInteractiveOperation
(
null
,
privOp
)
;
@
Override
public
String
[
]
getIpAndHost
(
Container
container
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
String
containerIdStr
=
containerId
.
toString
(
)
;
DockerInspectCommand
inspectCommand
=
new
DockerInspectCommand
(
containerIdStr
)
.
getIpAndHost
(
)
;
try
{
String
output
=
executeDockerInspect
(
containerId
,
inspectCommand
)
;
network
=
defaultNetwork
;
}
}
catch
(
NullPointerException
e
)
{
network
=
defaultNetwork
;
}
boolean
useHostNetwork
=
network
.
equalsIgnoreCase
(
)
;
if
(
useHostNetwork
)
{
InetAddress
address
;
try
{
address
=
InetAddress
.
getLocalHost
(
)
;
ips
=
address
.
getHostAddress
(
)
;
}
catch
(
UnknownHostException
e
)
{
LOG
.
error
(
+
containerId
)
;
}
}
}
String
[
]
ipAndHost
=
new
String
[
2
]
;
ipAndHost
[
0
]
=
ips
;
ipAndHost
[
1
]
=
host
;
return
ipAndHost
;
}
catch
(
NullPointerException
e
)
{
network
=
defaultNetwork
;
}
boolean
useHostNetwork
=
network
.
equalsIgnoreCase
(
)
;
if
(
useHostNetwork
)
{
InetAddress
address
;
try
{
address
=
InetAddress
.
getLocalHost
(
)
;
ips
=
address
.
getHostAddress
(
)
;
}
catch
(
UnknownHostException
e
)
{
LOG
.
error
(
+
containerId
)
;
}
}
}
String
[
]
ipAndHost
=
new
String
[
2
]
;
ipAndHost
[
0
]
=
ips
;
ipAndHost
[
1
]
=
host
;
return
ipAndHost
;
}
catch
(
ContainerExecutionException
e
)
{
public
void
pullImageFromRemote
(
String
containerIdStr
,
String
imageName
)
throws
ContainerExecutionException
{
long
start
=
System
.
currentTimeMillis
(
)
;
DockerPullCommand
dockerPullCommand
=
new
DockerPullCommand
(
imageName
)
;
public
void
pullImageFromRemote
(
String
containerIdStr
,
String
imageName
)
throws
ContainerExecutionException
{
long
start
=
System
.
currentTimeMillis
(
)
;
DockerPullCommand
dockerPullCommand
=
new
DockerPullCommand
(
imageName
)
;
LOG
.
debug
(
,
imageName
,
containerIdStr
)
;
DockerCommandExecutor
.
executeDockerCommand
(
dockerPullCommand
,
containerIdStr
,
null
,
privilegedOperationExecutor
,
false
,
nmContext
)
;
long
end
=
System
.
currentTimeMillis
(
)
;
long
pullImageTimeMs
=
end
-
start
;
DockerInspectCommand
inspectCommand
=
new
DockerInspectCommand
(
containerId
.
toString
(
)
)
.
get
(
new
String
[
]
{
DockerInspectCommand
.
STATUS_TEMPLATE
,
DockerInspectCommand
.
STOPSIGNAL_TEMPLATE
}
,
delimiter
)
;
try
{
String
output
=
executeDockerInspect
(
containerId
,
inspectCommand
)
.
trim
(
)
;
if
(
!
output
.
isEmpty
(
)
)
{
String
[
]
statusAndSignal
=
StringUtils
.
split
(
output
,
delimiter
)
;
containerStatus
=
DockerCommandExecutor
.
parseContainerStatus
(
statusAndSignal
[
0
]
)
;
if
(
statusAndSignal
.
length
>
1
)
{
stopSignal
=
statusAndSignal
[
1
]
;
}
}
}
catch
(
ContainerExecutionException
|
PrivilegedOperationException
e
)
{
LOG
.
debug
(
,
containerId
,
e
)
;
return
;
}
if
(
DockerCommandExecutor
.
isStoppable
(
containerStatus
)
)
{
DockerKillCommand
dockerStopCommand
=
new
DockerKillCommand
(
containerId
.
toString
(
)
)
.
setSignal
(
stopSignal
)
;
DockerCommandExecutor
.
executeDockerCommand
(
dockerStopCommand
,
containerId
.
toString
(
)
,
env
,
privilegedOperationExecutor
,
false
,
nmContext
)
;
}
else
{
private
String
executeDockerInspect
(
ContainerId
containerId
,
DockerInspectCommand
inspectCommand
)
throws
ContainerExecutionException
,
PrivilegedOperationException
{
String
commandFile
=
dockerClient
.
writeCommandToTempFile
(
inspectCommand
,
containerId
,
nmContext
)
;
PrivilegedOperation
privOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
RUN_DOCKER_CMD
)
;
privOp
.
appendArgs
(
commandFile
)
;
String
output
=
privilegedOperationExecutor
.
executePrivilegedOperation
(
null
,
privOp
,
null
,
null
,
true
,
false
)
;
private
void
handleContainerKill
(
ContainerRuntimeContext
ctx
,
Map
<
String
,
String
>
env
,
ContainerExecutor
.
Signal
signal
)
throws
ContainerExecutionException
{
Container
container
=
ctx
.
getContainer
(
)
;
ContainerVolumePublisher
publisher
=
new
ContainerVolumePublisher
(
container
,
container
.
getCsiVolumesRootDir
(
)
,
this
)
;
try
{
publisher
.
unpublishVolumes
(
)
;
}
catch
(
YarnException
|
IOException
e
)
{
throw
new
ContainerExecutionException
(
e
)
;
}
boolean
serviceMode
=
Boolean
.
parseBoolean
(
env
.
get
(
ENV_DOCKER_CONTAINER_DOCKER_SERVICE_MODE
)
)
;
if
(
isContainerRequestedAsPrivileged
(
container
)
||
serviceMode
)
{
String
containerId
=
container
.
getContainerId
(
)
.
toString
(
)
;
DockerCommandExecutor
.
DockerContainerStatus
containerStatus
=
DockerCommandExecutor
.
getContainerStatus
(
containerId
,
privilegedOperationExecutor
,
nmContext
)
;
if
(
DockerCommandExecutor
.
isKillable
(
containerStatus
)
)
{
DockerKillCommand
dockerKillCommand
=
new
DockerKillCommand
(
containerId
)
.
setSignal
(
signal
.
name
(
)
)
;
DockerCommandExecutor
.
executeDockerCommand
(
dockerKillCommand
,
containerId
,
env
,
privilegedOperationExecutor
,
false
,
nmContext
)
;
}
else
{
private
void
handleContainerRemove
(
String
containerId
,
Map
<
String
,
String
>
env
)
throws
ContainerExecutionException
{
String
delayedRemoval
=
env
.
get
(
ENV_DOCKER_CONTAINER_DELAYED_REMOVAL
)
;
if
(
delayedRemovalAllowed
&&
delayedRemoval
!=
null
&&
delayedRemoval
.
equalsIgnoreCase
(
)
)
{
protected
void
initiateCsiClients
(
Configuration
config
)
throws
ContainerExecutionException
{
String
[
]
driverNames
=
CsiConfigUtils
.
getCsiDriverNames
(
config
)
;
if
(
driverNames
!=
null
&&
driverNames
.
length
>
0
)
{
for
(
String
driverName
:
driverNames
)
{
try
{
InetSocketAddress
adaptorServiceAddress
=
CsiConfigUtils
.
getCsiAdaptorAddressForDriver
(
driverName
,
config
)
;
@
Override
public
void
start
(
)
{
int
reapRuncLayerMountsInterval
=
conf
.
getInt
(
NM_REAP_RUNC_LAYER_MOUNTS_INTERVAL
,
DEFAULT_NM_REAP_RUNC_LAYER_MOUNTS_INTERVAL
)
;
exec
=
HadoopExecutors
.
newScheduledThreadPool
(
1
)
;
exec
.
scheduleAtFixedRate
(
new
Runnable
(
)
{
@
Override
public
void
run
(
)
{
try
{
PrivilegedOperation
launchOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
REAP_RUNC_LAYER_MOUNTS
)
;
launchOp
.
appendArgs
(
Integer
.
toString
(
layersToKeep
)
)
;
try
{
String
stdout
=
privilegedOperationExecutor
.
executePrivilegedOperation
(
null
,
launchOp
,
null
,
null
,
false
,
false
)
;
if
(
stdout
!=
null
)
{
protected
RuncManifestToResourcesPlugin
chooseManifestToResourcesPlugin
(
)
throws
ContainerExecutionException
{
String
pluginName
=
conf
.
get
(
NM_RUNC_MANIFEST_TO_RESOURCES_PLUGIN
,
DEFAULT_NM_RUNC_MANIFEST_TO_RESOURCES_PLUGIN
)
;
public
static
DockerContainerStatus
getContainerStatus
(
String
containerId
,
PrivilegedOperationExecutor
privilegedOperationExecutor
,
Context
nmContext
)
{
try
{
String
currentContainerStatus
=
executeStatusCommand
(
containerId
,
privilegedOperationExecutor
,
nmContext
)
;
DockerContainerStatus
dockerContainerStatus
=
parseContainerStatus
(
currentContainerStatus
)
;
LocalizerStatus
status
=
createStatus
(
)
;
LocalizerHeartbeatResponse
response
=
nodemanager
.
heartbeat
(
status
)
;
switch
(
response
.
getLocalizerAction
(
)
)
{
case
LIVE
:
List
<
ResourceLocalizationSpec
>
newRsrcs
=
response
.
getResourceSpecs
(
)
;
for
(
ResourceLocalizationSpec
newRsrc
:
newRsrcs
)
{
if
(
!
pendingResources
.
containsKey
(
newRsrc
.
getResource
(
)
)
)
{
pendingResources
.
put
(
newRsrc
.
getResource
(
)
,
cs
.
submit
(
download
(
new
Path
(
newRsrc
.
getDestinationDirectory
(
)
.
getFile
(
)
)
,
newRsrc
.
getResource
(
)
,
ugi
)
)
)
;
}
}
break
;
case
DIE
:
for
(
Future
<
Path
>
pending
:
pendingResources
.
values
(
)
)
{
pending
.
cancel
(
true
)
;
}
status
=
createStatus
(
)
;
try
{
nodemanager
.
heartbeat
(
status
)
;
}
catch
(
YarnException
e
)
{
e
.
printStackTrace
(
System
.
out
)
;
String
appId
=
argv
[
1
]
;
String
locId
=
argv
[
2
]
;
InetSocketAddress
nmAddr
=
new
InetSocketAddress
(
argv
[
3
]
,
Integer
.
parseInt
(
argv
[
4
]
)
)
;
String
tokenFileName
=
argv
[
5
]
;
String
[
]
sLocaldirs
=
Arrays
.
copyOfRange
(
argv
,
6
,
argv
.
length
)
;
ArrayList
<
Path
>
localDirs
=
new
ArrayList
<
>
(
sLocaldirs
.
length
)
;
for
(
String
sLocaldir
:
sLocaldirs
)
{
localDirs
.
add
(
new
Path
(
sLocaldir
)
)
;
}
final
String
uid
=
UserGroupInformation
.
getCurrentUser
(
)
.
getShortUserName
(
)
;
if
(
!
user
.
equals
(
uid
)
)
{
LOG
.
warn
(
+
uid
+
+
user
)
;
}
ContainerLocalizer
localizer
=
new
ContainerLocalizer
(
FileContext
.
getLocalFSFileContext
(
)
,
user
,
appId
,
locId
,
tokenFileName
,
localDirs
,
RecordFactoryProvider
.
getRecordFactory
(
null
)
)
;
localizer
.
runLocalization
(
nmAddr
)
;
}
catch
(
Throwable
e
)
{
e
.
printStackTrace
(
System
.
out
)
;
LocalizedResource
rsrc
=
localrsrc
.
get
(
req
)
;
switch
(
event
.
getType
(
)
)
{
case
LOCALIZED
:
if
(
useLocalCacheDirectoryManager
)
{
inProgressLocalResourcesMap
.
remove
(
req
)
;
}
break
;
case
REQUEST
:
if
(
rsrc
!=
null
&&
(
!
isResourcePresent
(
rsrc
)
)
)
{
LOG
.
info
(
+
rsrc
.
getLocalPath
(
)
+
)
;
removeResource
(
req
)
;
rsrc
=
null
;
}
if
(
null
==
rsrc
)
{
rsrc
=
new
LocalizedResource
(
req
,
dispatcher
)
;
localrsrc
.
put
(
req
,
rsrc
)
;
}
break
;
case
RELEASE
:
if
(
null
==
rsrc
)
{
ResourceReleaseEvent
relEvent
=
(
ResourceReleaseEvent
)
event
;
break
;
case
RECOVERED
:
if
(
rsrc
!=
null
)
{
LOG
.
warn
(
+
rsrc
)
;
return
;
}
rsrc
=
recoverResource
(
req
,
(
ResourceRecoveredEvent
)
event
)
;
localrsrc
.
put
(
req
,
rsrc
)
;
break
;
}
if
(
rsrc
==
null
)
{
LOG
.
warn
(
+
event
.
getType
(
)
+
+
req
+
)
;
return
;
}
rsrc
.
handle
(
event
)
;
if
(
event
.
getType
(
)
==
ResourceEventType
.
RELEASE
)
{
if
(
rsrc
.
getState
(
)
==
ResourceState
.
DOWNLOADING
&&
rsrc
.
getRefCount
(
)
<=
0
&&
rsrc
.
getRequest
(
)
.
getVisibility
(
)
!=
LocalResourceVisibility
.
PUBLIC
)
{
removeResource
(
req
)
;
}
}
if
(
event
.
getType
(
)
==
ResourceEventType
.
LOCALIZED
)
{
@
Override
public
boolean
remove
(
LocalizedResource
rem
,
DeletionService
delService
)
{
LocalizedResource
rsrc
=
localrsrc
.
get
(
rem
.
getRequest
(
)
)
;
if
(
null
==
rsrc
)
{
File
file
=
new
File
(
uniquePath
.
toUri
(
)
.
getRawPath
(
)
)
;
if
(
!
file
.
exists
(
)
)
{
rPath
=
uniquePath
;
break
;
}
LOG
.
warn
(
+
uniquePath
+
+
)
;
if
(
delService
!=
null
)
{
FileDeletionTask
deletionTask
=
new
FileDeletionTask
(
delService
,
getUser
(
)
,
uniquePath
,
null
)
;
delService
.
delete
(
deletionTask
)
;
}
}
Path
localPath
=
new
Path
(
rPath
,
req
.
getPath
(
)
.
getName
(
)
)
;
LocalizedResource
rsrc
=
localrsrc
.
get
(
req
)
;
if
(
rsrc
==
null
)
{
LOG
.
warn
(
+
req
+
+
)
;
return
null
;
}
rsrc
.
setLocalPath
(
localPath
)
;
LocalResource
lr
=
LocalResource
.
newInstance
(
req
.
getResource
(
)
,
req
.
getType
(
)
,
req
.
getVisibility
(
)
,
req
.
getSize
(
)
,
req
.
getTimestamp
(
)
)
;
@
Override
public
void
handle
(
ResourceEvent
event
)
{
this
.
writeLock
.
lock
(
)
;
try
{
Path
resourcePath
=
event
.
getLocalResourceRequest
(
)
.
getPath
(
)
;
private
void
validateConf
(
Configuration
conf
)
{
int
perDirFileLimit
=
conf
.
getInt
(
YarnConfiguration
.
NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY
,
YarnConfiguration
.
DEFAULT_NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY
)
;
if
(
perDirFileLimit
<=
36
)
{
private
void
recoverTrackerResources
(
LocalResourcesTracker
tracker
,
LocalResourceTrackerState
state
)
throws
URISyntaxException
,
IOException
{
try
(
RecoveryIterator
<
LocalizedResourceProto
>
it
=
state
.
getCompletedResourcesIterator
(
)
)
{
while
(
it
!=
null
&&
it
.
hasNext
(
)
)
{
LocalizedResourceProto
proto
=
it
.
next
(
)
;
LocalResource
rsrc
=
new
LocalResourcePBImpl
(
proto
.
getResource
(
)
)
;
LocalResourceRequest
req
=
new
LocalResourceRequest
(
rsrc
)
;
private
void
recoverTrackerResources
(
LocalResourcesTracker
tracker
,
LocalResourceTrackerState
state
)
throws
URISyntaxException
,
IOException
{
try
(
RecoveryIterator
<
LocalizedResourceProto
>
it
=
state
.
getCompletedResourcesIterator
(
)
)
{
while
(
it
!=
null
&&
it
.
hasNext
(
)
)
{
LocalizedResourceProto
proto
=
it
.
next
(
)
;
LocalResource
rsrc
=
new
LocalResourcePBImpl
(
proto
.
getResource
(
)
)
;
LocalResourceRequest
req
=
new
LocalResourceRequest
(
rsrc
)
;
LOG
.
debug
(
,
req
,
proto
.
getLocalPath
(
)
)
;
tracker
.
handle
(
new
ResourceRecoveredEvent
(
req
,
new
Path
(
proto
.
getLocalPath
(
)
)
,
proto
.
getSize
(
)
)
)
;
}
}
try
(
RecoveryIterator
<
Map
.
Entry
<
LocalResourceProto
,
Path
>>
it
=
state
.
getStartedResourcesIterator
(
)
)
{
while
(
it
!=
null
&&
it
.
hasNext
(
)
)
{
Map
.
Entry
<
LocalResourceProto
,
Path
>
entry
=
it
.
next
(
)
;
LocalResource
rsrc
=
new
LocalResourcePBImpl
(
entry
.
getKey
(
)
)
;
LocalResourceRequest
req
=
new
LocalResourceRequest
(
rsrc
)
;
Path
localPath
=
entry
.
getValue
(
)
;
tracker
.
handle
(
new
ResourceRecoveredEvent
(
req
,
localPath
,
0
)
)
;
@
Override
public
void
serviceStart
(
)
throws
Exception
{
cacheCleanup
.
scheduleWithFixedDelay
(
new
CacheCleanup
(
dispatcher
)
,
cacheCleanupPeriod
,
cacheCleanupPeriod
,
TimeUnit
.
MILLISECONDS
)
;
server
=
createServer
(
)
;
server
.
start
(
)
;
localizationServerAddress
=
getConfig
(
)
.
updateConnectAddr
(
YarnConfiguration
.
NM_BIND_HOST
,
YarnConfiguration
.
NM_LOCALIZER_ADDRESS
,
YarnConfiguration
.
DEFAULT_NM_LOCALIZER_ADDRESS
,
server
.
getListenerAddress
(
)
)
;
private
void
deleteAppLogDir
(
FileContext
fs
,
DeletionService
del
,
String
logDir
)
throws
IOException
{
RemoteIterator
<
FileStatus
>
fileStatuses
=
fs
.
listStatus
(
new
Path
(
logDir
)
)
;
if
(
fileStatuses
!=
null
)
{
while
(
fileStatuses
.
hasNext
(
)
)
{
FileStatus
fileStatus
=
fileStatuses
.
next
(
)
;
String
appName
=
fileStatus
.
getPath
(
)
.
getName
(
)
;
if
(
appName
.
matches
(
)
)
{
@
SuppressWarnings
(
)
@
Override
public
Token
<
LocalizerTokenIdentifier
>
selectToken
(
Text
service
,
Collection
<
Token
<
?
extends
TokenIdentifier
>>
tokens
)
{
LOG
.
debug
(
)
;
for
(
Token
<
?
extends
TokenIdentifier
>
token
:
tokens
)
{
fs
.
mkdirs
(
directoryPath
,
DIRECTORY_PERMISSION
)
;
tempPath
=
new
Path
(
directoryPath
,
getTemporaryFileName
(
actualPath
)
)
;
if
(
!
uploadFile
(
actualPath
,
tempPath
)
)
{
LOG
.
warn
(
+
tempPath
)
;
return
false
;
}
fs
.
setPermission
(
tempPath
,
FILE_PERMISSION
)
;
Path
finalPath
=
new
Path
(
directoryPath
,
actualPath
.
getName
(
)
)
;
if
(
!
fs
.
rename
(
tempPath
,
finalPath
)
)
{
LOG
.
warn
(
+
finalPath
+
)
;
deleteTempFile
(
tempPath
)
;
return
false
;
}
if
(
!
notifySharedCacheManager
(
checksumVal
,
actualPath
.
getName
(
)
)
)
{
fs
.
delete
(
finalPath
,
false
)
;
return
false
;
}
short
replication
=
(
short
)
conf
.
getInt
(
YarnConfiguration
.
SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR
,
YarnConfiguration
.
DEFAULT_SHARED_CACHE_NM_UPLOADER_REPLICATION_FACTOR
)
;
Class
<
?
extends
ContainerLogAggregationPolicy
>
policyClass
=
null
;
if
(
this
.
logAggregationContext
!=
null
)
{
String
className
=
this
.
logAggregationContext
.
getLogAggregationPolicyClassName
(
)
;
if
(
className
!=
null
)
{
try
{
Class
<
?
>
policyFromContext
=
conf
.
getClassByName
(
className
)
;
if
(
ContainerLogAggregationPolicy
.
class
.
isAssignableFrom
(
policyFromContext
)
)
{
policyClass
=
policyFromContext
.
asSubclass
(
ContainerLogAggregationPolicy
.
class
)
;
}
else
{
LOG
.
warn
(
this
.
appId
+
+
className
)
;
}
}
catch
(
ClassNotFoundException
cnfe
)
{
LOG
.
warn
(
this
.
appId
+
+
className
)
;
}
}
}
if
(
policyClass
==
null
)
{
policyClass
=
conf
.
getClass
(
YarnConfiguration
.
NM_LOG_AGG_POLICY_CLASS
,
AllContainerLogAggregationPolicy
.
class
,
ContainerLogAggregationPolicy
.
class
)
;
}
else
{
return
;
}
addCredentials
(
)
;
Set
<
ContainerId
>
pendingContainerInThisCycle
=
new
HashSet
<
ContainerId
>
(
)
;
this
.
pendingContainers
.
drainTo
(
pendingContainerInThisCycle
)
;
Set
<
ContainerId
>
finishedContainers
=
new
HashSet
<
ContainerId
>
(
pendingContainerInThisCycle
)
;
if
(
this
.
context
.
getApplications
(
)
.
get
(
this
.
appId
)
!=
null
)
{
for
(
Container
container
:
this
.
context
.
getApplications
(
)
.
get
(
this
.
appId
)
.
getContainers
(
)
.
values
(
)
)
{
ContainerType
containerType
=
container
.
getContainerTokenIdentifier
(
)
.
getContainerType
(
)
;
if
(
shouldUploadLogs
(
new
ContainerLogContext
(
container
.
getContainerId
(
)
,
containerType
,
0
)
)
)
{
pendingContainerInThisCycle
.
add
(
container
.
getContainerId
(
)
)
;
}
}
}
if
(
pendingContainerInThisCycle
.
isEmpty
(
)
)
{
LOG
.
debug
(
)
;
sendLogAggregationReport
(
true
,
,
appFinished
)
;
return
;
}
logAggregationTimes
++
;
if
(
shouldUploadLogs
(
new
ContainerLogContext
(
container
.
getContainerId
(
)
,
containerType
,
0
)
)
)
{
pendingContainerInThisCycle
.
add
(
container
.
getContainerId
(
)
)
;
}
}
}
if
(
pendingContainerInThisCycle
.
isEmpty
(
)
)
{
LOG
.
debug
(
)
;
sendLogAggregationReport
(
true
,
,
appFinished
)
;
return
;
}
logAggregationTimes
++
;
LOG
.
debug
(
,
logAggregationTimes
)
;
String
diagnosticMessage
=
;
boolean
logAggregationSucceedInThisCycle
=
true
;
DeletionTask
deletionTask
=
null
;
try
{
try
{
logAggregationFileController
.
initializeWriter
(
logControllerContext
)
;
}
catch
(
IOException
e1
)
{
LOG
.
error
(
+
e1
)
;
}
}
}
deletionTask
=
new
FileDeletionTask
(
delService
,
this
.
userUgi
.
getShortUserName
(
)
,
null
,
uploadedFilePathsInThisCycleList
)
;
}
if
(
finishedContainers
.
contains
(
container
)
)
{
containerLogAggregators
.
remove
(
container
)
;
}
}
logControllerContext
.
setUploadedLogsInThisCycle
(
uploadedLogsInThisCycle
)
;
logControllerContext
.
setLogUploadTimeStamp
(
System
.
currentTimeMillis
(
)
)
;
logControllerContext
.
increLogAggregationTimes
(
)
;
try
{
this
.
logAggregationFileController
.
postWrite
(
logControllerContext
)
;
diagnosticMessage
=
+
appId
+
+
LogAggregationUtils
.
getNodeString
(
nodeId
)
+
+
Times
.
format
(
logControllerContext
.
getLogUploadTimeStamp
(
)
)
+
;
}
catch
(
Exception
e
)
{
diagnosticMessage
=
e
.
getMessage
(
)
;
renameTemporaryLogFileFailed
=
true
;
logAggregationSucceedInThisCycle
=
false
;
}
}
finally
{
private
void
addCredentials
(
)
{
if
(
UserGroupInformation
.
isSecurityEnabled
(
)
)
{
Credentials
systemCredentials
=
context
.
getSystemCredentialsForApps
(
)
.
get
(
appId
)
;
if
(
systemCredentials
!=
null
)
{
}
uploadLogsForContainers
(
false
)
;
}
else
{
wait
(
THREAD_SLEEP_TIME
)
;
}
}
catch
(
InterruptedException
e
)
{
LOG
.
warn
(
)
;
this
.
appFinishing
.
set
(
true
)
;
}
catch
(
LogAggregationDFSException
e
)
{
this
.
appFinishing
.
set
(
true
)
;
throw
e
;
}
}
}
if
(
this
.
aborted
.
get
(
)
)
{
return
;
}
try
{
uploadLogsForContainers
(
true
)
;
doAppLogAggregationPostCleanUp
(
)
;
}
catch
(
LogAggregationDFSException
e
)
{
@
Override
public
void
startContainerLogAggregation
(
ContainerLogContext
logContext
)
{
if
(
shouldUploadLogs
(
logContext
)
)
{
private
static
long
calculateRollingMonitorInterval
(
Configuration
conf
)
{
long
interval
=
conf
.
getLong
(
YarnConfiguration
.
NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS
,
YarnConfiguration
.
DEFAULT_NM_LOG_AGGREGATION_ROLL_MONITORING_INTERVAL_SECONDS
)
;
if
(
interval
<=
0
)
{
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
int
threadPoolSize
=
getAggregatorThreadPoolSize
(
conf
)
;
this
.
threadPool
=
HadoopExecutors
.
newFixedThreadPool
(
threadPoolSize
,
new
ThreadFactoryBuilder
(
)
.
setNameFormat
(
)
.
build
(
)
)
;
rollingMonitorInterval
=
calculateRollingMonitorInterval
(
conf
)
;
private
void
recover
(
)
throws
IOException
{
if
(
stateStore
.
canRecover
(
)
)
{
RecoveredLogDeleterState
state
=
stateStore
.
loadLogDeleterState
(
)
;
long
now
=
System
.
currentTimeMillis
(
)
;
for
(
Map
.
Entry
<
ApplicationId
,
LogDeleterProto
>
entry
:
state
.
getLogDeleterMap
(
)
.
entrySet
(
)
)
{
ApplicationId
appId
=
entry
.
getKey
(
)
;
LogDeleterProto
proto
=
entry
.
getValue
(
)
;
long
deleteDelayMsec
=
proto
.
getDeletionTime
(
)
-
now
;
@
SuppressWarnings
(
)
@
Override
public
void
handle
(
LogHandlerEvent
event
)
{
switch
(
event
.
getType
(
)
)
{
case
APPLICATION_STARTED
:
LogHandlerAppStartedEvent
appStartedEvent
=
(
LogHandlerAppStartedEvent
)
event
;
this
.
appOwners
.
put
(
appStartedEvent
.
getApplicationId
(
)
,
appStartedEvent
.
getUser
(
)
)
;
this
.
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ApplicationEvent
(
appStartedEvent
.
getApplicationId
(
)
,
ApplicationEventType
.
APPLICATION_LOG_HANDLING_INITED
)
)
;
break
;
case
CONTAINER_FINISHED
:
break
;
case
APPLICATION_FINISHED
:
LogHandlerAppFinishedEvent
appFinishedEvent
=
(
LogHandlerAppFinishedEvent
)
event
;
ApplicationId
appId
=
appFinishedEvent
.
getApplicationId
(
)
;
@
SuppressWarnings
(
)
@
Override
public
void
handle
(
LogHandlerEvent
event
)
{
switch
(
event
.
getType
(
)
)
{
case
APPLICATION_STARTED
:
LogHandlerAppStartedEvent
appStartedEvent
=
(
LogHandlerAppStartedEvent
)
event
;
this
.
appOwners
.
put
(
appStartedEvent
.
getApplicationId
(
)
,
appStartedEvent
.
getUser
(
)
)
;
this
.
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ApplicationEvent
(
appStartedEvent
.
getApplicationId
(
)
,
ApplicationEventType
.
APPLICATION_LOG_HANDLING_INITED
)
)
;
break
;
case
CONTAINER_FINISHED
:
break
;
case
APPLICATION_FINISHED
:
LogHandlerAppFinishedEvent
appFinishedEvent
=
(
LogHandlerAppFinishedEvent
)
event
;
ApplicationId
appId
=
appFinishedEvent
.
getApplicationId
(
)
;
LOG
.
info
(
+
appId
+
+
this
.
deleteDelaySeconds
+
)
;
String
user
=
appOwners
.
remove
(
appId
)
;
if
(
user
==
null
)
{
case
CONTAINER_FINISHED
:
break
;
case
APPLICATION_FINISHED
:
LogHandlerAppFinishedEvent
appFinishedEvent
=
(
LogHandlerAppFinishedEvent
)
event
;
ApplicationId
appId
=
appFinishedEvent
.
getApplicationId
(
)
;
LOG
.
info
(
+
appId
+
+
this
.
deleteDelaySeconds
+
)
;
String
user
=
appOwners
.
remove
(
appId
)
;
if
(
user
==
null
)
{
LOG
.
error
(
+
appId
)
;
NonAggregatingLogHandler
.
this
.
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ApplicationEvent
(
appId
,
ApplicationEventType
.
APPLICATION_LOG_HANDLING_FAILED
)
)
;
break
;
}
LogDeleterRunnable
logDeleter
=
new
LogDeleterRunnable
(
user
,
appId
)
;
long
deletionTimestamp
=
System
.
currentTimeMillis
(
)
+
this
.
deleteDelaySeconds
*
1000
;
LogDeleterProto
deleterProto
=
LogDeleterProto
.
newBuilder
(
)
.
setUser
(
user
)
.
setDeletionTime
(
deletionTimestamp
)
.
build
(
)
;
try
{
stateStore
.
storeLogDeleter
(
appId
,
deleterProto
)
;
}
catch
(
IOException
e
)
{
processTreeClass
=
this
.
conf
.
getClass
(
YarnConfiguration
.
NM_CONTAINER_MON_PROCESS_TREE
,
null
,
ResourceCalculatorProcessTree
.
class
)
;
LOG
.
info
(
,
this
.
processTreeClass
)
;
this
.
containerMetricsEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_CONTAINER_METRICS_ENABLE
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_ENABLE
)
;
this
.
containerMetricsPeriodMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_PERIOD_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS
)
;
this
.
containerMetricsUnregisterDelayMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
)
;
long
configuredPMemForContainers
=
NodeManagerHardwareUtils
.
getContainerMemoryMB
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
int
configuredVCoresForContainers
=
NodeManagerHardwareUtils
.
getVCores
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
vmemRatio
=
this
.
conf
.
getFloat
(
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
,
YarnConfiguration
.
DEFAULT_NM_VMEM_PMEM_RATIO
)
;
Preconditions
.
checkArgument
(
vmemRatio
>
0.99f
,
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
+
)
;
Resource
resourcesForContainers
=
Resource
.
newInstance
(
configuredPMemForContainers
,
configuredVCoresForContainers
)
;
setAllocatedResourcesForContainers
(
resourcesForContainers
)
;
pmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_PMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_PMEM_CHECK_ENABLED
)
;
vmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_VMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_VMEM_CHECK_ENABLED
)
;
elasticMemoryEnforcement
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_ELASTIC_MEMORY_CONTROL_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED
)
;
strictMemoryEnforcement
=
conf
.
getBoolean
(
YarnConfiguration
.
NM_MEMORY_RESOURCE_ENFORCED
,
YarnConfiguration
.
DEFAULT_NM_MEMORY_RESOURCE_ENFORCED
)
;
LOG
.
info
(
,
this
.
processTreeClass
)
;
this
.
containerMetricsEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_CONTAINER_METRICS_ENABLE
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_ENABLE
)
;
this
.
containerMetricsPeriodMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_PERIOD_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS
)
;
this
.
containerMetricsUnregisterDelayMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
)
;
long
configuredPMemForContainers
=
NodeManagerHardwareUtils
.
getContainerMemoryMB
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
int
configuredVCoresForContainers
=
NodeManagerHardwareUtils
.
getVCores
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
vmemRatio
=
this
.
conf
.
getFloat
(
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
,
YarnConfiguration
.
DEFAULT_NM_VMEM_PMEM_RATIO
)
;
Preconditions
.
checkArgument
(
vmemRatio
>
0.99f
,
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
+
)
;
Resource
resourcesForContainers
=
Resource
.
newInstance
(
configuredPMemForContainers
,
configuredVCoresForContainers
)
;
setAllocatedResourcesForContainers
(
resourcesForContainers
)
;
pmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_PMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_PMEM_CHECK_ENABLED
)
;
vmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_VMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_VMEM_CHECK_ENABLED
)
;
elasticMemoryEnforcement
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_ELASTIC_MEMORY_CONTROL_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED
)
;
strictMemoryEnforcement
=
conf
.
getBoolean
(
YarnConfiguration
.
NM_MEMORY_RESOURCE_ENFORCED
,
YarnConfiguration
.
DEFAULT_NM_MEMORY_RESOURCE_ENFORCED
)
;
LOG
.
info
(
,
pmemCheckEnabled
)
;
this
.
containerMetricsEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_CONTAINER_METRICS_ENABLE
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_ENABLE
)
;
this
.
containerMetricsPeriodMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_PERIOD_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS
)
;
this
.
containerMetricsUnregisterDelayMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
)
;
long
configuredPMemForContainers
=
NodeManagerHardwareUtils
.
getContainerMemoryMB
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
int
configuredVCoresForContainers
=
NodeManagerHardwareUtils
.
getVCores
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
vmemRatio
=
this
.
conf
.
getFloat
(
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
,
YarnConfiguration
.
DEFAULT_NM_VMEM_PMEM_RATIO
)
;
Preconditions
.
checkArgument
(
vmemRatio
>
0.99f
,
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
+
)
;
Resource
resourcesForContainers
=
Resource
.
newInstance
(
configuredPMemForContainers
,
configuredVCoresForContainers
)
;
setAllocatedResourcesForContainers
(
resourcesForContainers
)
;
pmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_PMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_PMEM_CHECK_ENABLED
)
;
vmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_VMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_VMEM_CHECK_ENABLED
)
;
elasticMemoryEnforcement
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_ELASTIC_MEMORY_CONTROL_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED
)
;
strictMemoryEnforcement
=
conf
.
getBoolean
(
YarnConfiguration
.
NM_MEMORY_RESOURCE_ENFORCED
,
YarnConfiguration
.
DEFAULT_NM_MEMORY_RESOURCE_ENFORCED
)
;
LOG
.
info
(
,
pmemCheckEnabled
)
;
LOG
.
info
(
,
vmemCheckEnabled
)
;
this
.
containerMetricsPeriodMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_PERIOD_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_PERIOD_MS
)
;
this
.
containerMetricsUnregisterDelayMs
=
this
.
conf
.
getLong
(
YarnConfiguration
.
NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
,
YarnConfiguration
.
DEFAULT_NM_CONTAINER_METRICS_UNREGISTER_DELAY_MS
)
;
long
configuredPMemForContainers
=
NodeManagerHardwareUtils
.
getContainerMemoryMB
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
int
configuredVCoresForContainers
=
NodeManagerHardwareUtils
.
getVCores
(
this
.
resourceCalculatorPlugin
,
this
.
conf
)
;
vmemRatio
=
this
.
conf
.
getFloat
(
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
,
YarnConfiguration
.
DEFAULT_NM_VMEM_PMEM_RATIO
)
;
Preconditions
.
checkArgument
(
vmemRatio
>
0.99f
,
YarnConfiguration
.
NM_VMEM_PMEM_RATIO
+
)
;
Resource
resourcesForContainers
=
Resource
.
newInstance
(
configuredPMemForContainers
,
configuredVCoresForContainers
)
;
setAllocatedResourcesForContainers
(
resourcesForContainers
)
;
pmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_PMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_PMEM_CHECK_ENABLED
)
;
vmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_VMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_VMEM_CHECK_ENABLED
)
;
elasticMemoryEnforcement
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_ELASTIC_MEMORY_CONTROL_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED
)
;
strictMemoryEnforcement
=
conf
.
getBoolean
(
YarnConfiguration
.
NM_MEMORY_RESOURCE_ENFORCED
,
YarnConfiguration
.
DEFAULT_NM_MEMORY_RESOURCE_ENFORCED
)
;
LOG
.
info
(
,
pmemCheckEnabled
)
;
LOG
.
info
(
,
vmemCheckEnabled
)
;
LOG
.
info
(
,
elasticMemoryEnforcement
)
;
setAllocatedResourcesForContainers
(
resourcesForContainers
)
;
pmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_PMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_PMEM_CHECK_ENABLED
)
;
vmemCheckEnabled
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_VMEM_CHECK_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_VMEM_CHECK_ENABLED
)
;
elasticMemoryEnforcement
=
this
.
conf
.
getBoolean
(
YarnConfiguration
.
NM_ELASTIC_MEMORY_CONTROL_ENABLED
,
YarnConfiguration
.
DEFAULT_NM_ELASTIC_MEMORY_CONTROL_ENABLED
)
;
strictMemoryEnforcement
=
conf
.
getBoolean
(
YarnConfiguration
.
NM_MEMORY_RESOURCE_ENFORCED
,
YarnConfiguration
.
DEFAULT_NM_MEMORY_RESOURCE_ENFORCED
)
;
LOG
.
info
(
,
pmemCheckEnabled
)
;
LOG
.
info
(
,
vmemCheckEnabled
)
;
LOG
.
info
(
,
elasticMemoryEnforcement
)
;
LOG
.
info
(
,
strictMemoryEnforcement
)
;
if
(
elasticMemoryEnforcement
)
{
if
(
!
CGroupElasticMemoryController
.
isAvailable
(
)
)
{
throw
new
YarnException
(
+
)
;
}
else
{
this
.
oomListenerThread
=
new
CGroupElasticMemoryController
(
conf
,
context
,
ResourceHandlerModule
.
getCGroupsHandler
(
)
,
pmemCheckEnabled
,
vmemCheckEnabled
,
pmemCheckEnabled
?
maxPmemAllottedForContainers
:
maxVmemAllottedForContainers
)
;
}
}
containersMonitorEnabled
=
isContainerMonitorEnabled
(
)
&&
monitoringInterval
>
0
;
@
Override
public
void
setAllocatedResourcesForContainers
(
final
Resource
resource
)
{
private
void
onStopMonitoringContainer
(
ContainersMonitorEvent
monitoringEvent
,
ContainerId
containerId
)
{
private
void
onStartMonitoringContainer
(
ContainersMonitorEvent
monitoringEvent
,
ContainerId
containerId
)
{
ContainerStartMonitoringEvent
startEvent
=
(
ContainerStartMonitoringEvent
)
monitoringEvent
;
Map
<
String
,
ResourcePlugin
>
pluginMap
=
Maps
.
newHashMap
(
)
;
for
(
String
resourceName
:
plugins
)
{
resourceName
=
resourceName
.
trim
(
)
;
ensurePluginIsSupported
(
resourceName
)
;
if
(
!
isPluginDuplicate
(
pluginMap
,
resourceName
)
)
{
ResourcePlugin
plugin
=
null
;
if
(
resourceName
.
equals
(
GPU_URI
)
)
{
final
GpuDiscoverer
gpuDiscoverer
=
new
GpuDiscoverer
(
)
;
final
GpuNodeResourceUpdateHandler
updateHandler
=
new
GpuNodeResourceUpdateHandler
(
gpuDiscoverer
,
conf
)
;
plugin
=
new
GpuResourcePlugin
(
updateHandler
,
gpuDiscoverer
)
;
}
else
if
(
resourceName
.
equals
(
FPGA_URI
)
)
{
plugin
=
new
FpgaResourcePlugin
(
)
;
}
if
(
plugin
==
null
)
{
throw
new
YarnException
(
+
resourceName
+
)
;
}
plugin
.
initialize
(
context
)
;
private
void
ensurePluginIsSupported
(
String
resourceName
)
throws
YarnException
{
if
(
!
SUPPORTED_RESOURCE_PLUGINS
.
contains
(
resourceName
)
)
{
String
msg
=
+
resourceName
+
+
StringUtils
.
join
(
,
SUPPORTED_RESOURCE_PLUGINS
)
;
for
(
String
pluginClassName
:
pluginClassNames
)
{
Class
<
?
>
pluginClazz
=
Class
.
forName
(
pluginClassName
)
;
if
(
!
DevicePlugin
.
class
.
isAssignableFrom
(
pluginClazz
)
)
{
throw
new
YarnRuntimeException
(
+
pluginClassName
+
+
DevicePlugin
.
class
.
getCanonicalName
(
)
)
;
}
checkInterfaceCompatibility
(
DevicePlugin
.
class
,
pluginClazz
)
;
DevicePlugin
dpInstance
=
(
DevicePlugin
)
ReflectionUtils
.
newInstance
(
pluginClazz
,
configuration
)
;
DeviceRegisterRequest
request
=
null
;
try
{
request
=
dpInstance
.
getRegisterRequestInfo
(
)
;
}
catch
(
Exception
e
)
{
throw
new
YarnRuntimeException
(
+
+
e
.
getMessage
(
)
)
;
}
String
resourceName
=
request
.
getResourceName
(
)
;
if
(
pluginMap
.
containsKey
(
resourceName
)
)
{
throw
new
YarnRuntimeException
(
resourceName
+
+
+
+
pluginClassName
)
;
}
if
(
!
isConfiguredResourceName
(
resourceName
)
)
{
Class
<
?
>
pluginClazz
=
Class
.
forName
(
pluginClassName
)
;
if
(
!
DevicePlugin
.
class
.
isAssignableFrom
(
pluginClazz
)
)
{
throw
new
YarnRuntimeException
(
+
pluginClassName
+
+
DevicePlugin
.
class
.
getCanonicalName
(
)
)
;
}
checkInterfaceCompatibility
(
DevicePlugin
.
class
,
pluginClazz
)
;
DevicePlugin
dpInstance
=
(
DevicePlugin
)
ReflectionUtils
.
newInstance
(
pluginClazz
,
configuration
)
;
DeviceRegisterRequest
request
=
null
;
try
{
request
=
dpInstance
.
getRegisterRequestInfo
(
)
;
}
catch
(
Exception
e
)
{
throw
new
YarnRuntimeException
(
+
+
e
.
getMessage
(
)
)
;
}
String
resourceName
=
request
.
getResourceName
(
)
;
if
(
pluginMap
.
containsKey
(
resourceName
)
)
{
throw
new
YarnRuntimeException
(
resourceName
+
+
+
+
pluginClassName
)
;
}
if
(
!
isConfiguredResourceName
(
resourceName
)
)
{
throw
new
YarnRuntimeException
(
resourceName
+
+
YarnConfiguration
.
RESOURCE_TYPES_CONFIGURATION_FILE
+
)
;
DeviceRegisterRequest
request
=
null
;
try
{
request
=
dpInstance
.
getRegisterRequestInfo
(
)
;
}
catch
(
Exception
e
)
{
throw
new
YarnRuntimeException
(
+
+
e
.
getMessage
(
)
)
;
}
String
resourceName
=
request
.
getResourceName
(
)
;
if
(
pluginMap
.
containsKey
(
resourceName
)
)
{
throw
new
YarnRuntimeException
(
resourceName
+
+
+
+
pluginClassName
)
;
}
if
(
!
isConfiguredResourceName
(
resourceName
)
)
{
throw
new
YarnRuntimeException
(
resourceName
+
+
YarnConfiguration
.
RESOURCE_TYPES_CONFIGURATION_FILE
+
)
;
}
LOG
.
info
(
,
resourceName
,
pluginClassName
)
;
DevicePluginAdapter
pluginAdapter
=
new
DevicePluginAdapter
(
resourceName
,
dpInstance
,
deviceMappingManager
)
;
LOG
.
info
(
,
pluginClassName
)
;
try
{
pluginAdapter
.
initialize
(
context
)
;
}
catch
(
Exception
e
)
{
throw
new
YarnRuntimeException
(
+
+
e
.
getMessage
(
)
)
;
}
String
resourceName
=
request
.
getResourceName
(
)
;
if
(
pluginMap
.
containsKey
(
resourceName
)
)
{
throw
new
YarnRuntimeException
(
resourceName
+
+
+
+
pluginClassName
)
;
}
if
(
!
isConfiguredResourceName
(
resourceName
)
)
{
throw
new
YarnRuntimeException
(
resourceName
+
+
YarnConfiguration
.
RESOURCE_TYPES_CONFIGURATION_FILE
+
)
;
}
LOG
.
info
(
,
resourceName
,
pluginClassName
)
;
DevicePluginAdapter
pluginAdapter
=
new
DevicePluginAdapter
(
resourceName
,
dpInstance
,
deviceMappingManager
)
;
LOG
.
info
(
,
pluginClassName
)
;
try
{
pluginAdapter
.
initialize
(
context
)
;
}
catch
(
YarnException
e
)
{
throw
new
YarnRuntimeException
(
+
pluginClassName
+
)
;
}
LOG
.
info
(
,
pluginClassName
)
;
@
VisibleForTesting
public
void
checkInterfaceCompatibility
(
Class
<
?
>
expectedClass
,
Class
<
?
>
actualClass
)
throws
YarnRuntimeException
{
@
VisibleForTesting
public
void
checkInterfaceCompatibility
(
Class
<
?
>
expectedClass
,
Class
<
?
>
actualClass
)
throws
YarnRuntimeException
{
LOG
.
debug
(
,
expectedClass
.
getSimpleName
(
)
)
;
Method
[
]
expectedDevicePluginMethods
=
expectedClass
.
getMethods
(
)
;
boolean
found
;
for
(
Method
method
:
expectedDevicePluginMethods
)
{
found
=
false
;
@
VisibleForTesting
public
void
checkInterfaceCompatibility
(
Class
<
?
>
expectedClass
,
Class
<
?
>
actualClass
)
throws
YarnRuntimeException
{
LOG
.
debug
(
,
expectedClass
.
getSimpleName
(
)
)
;
Method
[
]
expectedDevicePluginMethods
=
expectedClass
.
getMethods
(
)
;
boolean
found
;
for
(
Method
method
:
expectedDevicePluginMethods
)
{
found
=
false
;
LOG
.
debug
(
,
method
.
getName
(
)
)
;
for
(
Method
m
:
actualClass
.
getDeclaredMethods
(
)
)
{
if
(
m
.
getName
(
)
.
equals
(
method
.
getName
(
)
)
)
{
LOG
.
info
(
,
binaryName
)
;
boolean
found
=
false
;
String
envBinaryPath
=
envProvider
.
apply
(
ENV_SCRIPT_PATH
)
;
if
(
envBinaryPath
!=
null
)
{
this
.
binaryPath
=
getScriptFromEnvSetting
(
envBinaryPath
)
;
found
=
binaryPath
!=
null
;
}
if
(
!
found
)
{
if
(
envBinaryPath
!=
null
)
{
LOG
.
warn
(
+
,
envBinaryPath
)
;
}
this
.
binaryPath
=
getScriptFromHadoopCommon
(
envProvider
,
binaryName
)
;
found
=
binaryPath
!=
null
;
}
if
(
!
found
)
{
LOG
.
info
(
+
+
)
;
this
.
binaryPath
=
getScriptFromSearchDirs
(
binaryName
,
scriptPaths
)
;
found
=
binaryPath
!=
null
;
private
Set
<
Device
>
parseOutput
(
String
output
)
{
Set
<
Device
>
devices
=
new
HashSet
<
>
(
)
;
private
String
getScriptFromEnvSetting
(
String
envBinaryPath
)
{
private
String
getScriptFromHadoopCommon
(
Function
<
String
,
String
>
envProvider
,
String
binaryName
)
{
String
scriptPath
=
null
;
String
hadoopCommon
=
envProvider
.
apply
(
HADOOP_COMMON_HOME
)
;
if
(
hadoopCommon
!=
null
)
{
String
targetPath
=
hadoopCommon
+
+
binaryName
;
private
String
getScriptFromHadoopCommon
(
Function
<
String
,
String
>
envProvider
,
String
binaryName
)
{
String
scriptPath
=
null
;
String
hadoopCommon
=
envProvider
.
apply
(
HADOOP_COMMON_HOME
)
;
if
(
hadoopCommon
!=
null
)
{
String
targetPath
=
hadoopCommon
+
+
binaryName
;
LOG
.
info
(
,
targetPath
)
;
if
(
new
File
(
targetPath
)
.
exists
(
)
)
{
private
String
getScriptFromSearchDirs
(
String
binaryName
,
String
[
]
scriptPaths
)
{
String
scriptPath
=
null
;
for
(
String
dir
:
scriptPaths
)
{
File
f
=
new
File
(
dir
,
binaryName
)
;
if
(
f
.
exists
(
)
)
{
private
Device
toDevice
(
Path
p
,
MutableInt
counter
)
{
CommandExecutor
executor
=
commandExecutorProvider
.
apply
(
new
String
[
]
{
,
,
,
,
p
.
toString
(
)
}
)
;
try
{
private
Device
toDevice
(
Path
p
,
MutableInt
counter
)
{
CommandExecutor
executor
=
commandExecutorProvider
.
apply
(
new
String
[
]
{
,
,
,
,
p
.
toString
(
)
}
)
;
try
{
LOG
.
info
(
,
p
)
;
executor
.
execute
(
)
;
String
statOutput
=
executor
.
getOutput
(
)
;
String
[
]
stat
=
statOutput
.
trim
(
)
.
split
(
)
;
int
major
=
Integer
.
parseInt
(
stat
[
0
]
,
16
)
;
int
minor
=
Integer
.
parseInt
(
stat
[
1
]
,
16
)
;
char
devType
=
getDevType
(
p
,
stat
[
2
]
)
;
int
deviceNumber
=
makeDev
(
major
,
minor
)
;
private
Device
toDevice
(
Path
p
,
MutableInt
counter
)
{
CommandExecutor
executor
=
commandExecutorProvider
.
apply
(
new
String
[
]
{
,
,
,
,
p
.
toString
(
)
}
)
;
try
{
LOG
.
info
(
,
p
)
;
executor
.
execute
(
)
;
String
statOutput
=
executor
.
getOutput
(
)
;
String
[
]
stat
=
statOutput
.
trim
(
)
.
split
(
)
;
int
major
=
Integer
.
parseInt
(
stat
[
0
]
,
16
)
;
int
minor
=
Integer
.
parseInt
(
stat
[
1
]
,
16
)
;
char
devType
=
getDevType
(
p
,
stat
[
2
]
)
;
int
deviceNumber
=
makeDev
(
major
,
minor
)
;
LOG
.
info
(
,
major
,
minor
,
deviceNumber
,
devType
)
;
String
sysPath
=
udev
.
getSysPath
(
deviceNumber
,
devType
)
;
@
Override
public
DeviceRuntimeSpec
onDevicesAllocated
(
Set
<
Device
>
allocatedDevices
,
YarnRuntimeType
yarnRuntime
)
throws
Exception
{
private
String
getMajorNumber
(
String
devName
)
{
String
output
=
null
;
try
{
private
String
getMajorNumber
(
String
devName
)
{
String
output
=
null
;
try
{
LOG
.
debug
(
,
devName
)
;
output
=
shellExecutor
.
getMajorMinorInfo
(
devName
)
;
String
[
]
strs
=
output
.
trim
(
)
.
split
(
)
;
int
num
=
0
;
String
policy
=
envs
.
get
(
TOPOLOGY_POLICY_ENV_KEY
)
;
if
(
policy
==
null
)
{
policy
=
TOPOLOGY_POLICY_PACK
;
}
if
(
cTable
==
null
)
{
LOG
.
error
(
)
;
return
;
}
List
<
Map
.
Entry
<
Set
<
Device
>
,
Integer
>>
combinationsToCost
=
cTable
.
get
(
count
)
;
Iterator
<
Map
.
Entry
<
Set
<
Device
>
,
Integer
>>
iterator
=
combinationsToCost
.
iterator
(
)
;
if
(
policy
.
equalsIgnoreCase
(
TOPOLOGY_POLICY_SPREAD
)
)
{
iterator
=
(
(
LinkedList
)
combinationsToCost
)
.
descendingIterator
(
)
;
}
while
(
iterator
.
hasNext
(
)
)
{
Map
.
Entry
<
Set
<
Device
>
,
Integer
>
element
=
iterator
.
next
(
)
;
if
(
availableDevices
.
containsAll
(
element
.
getKey
(
)
)
)
{
allocation
.
addAll
(
element
.
getKey
(
)
)
;
public
void
searchBinary
(
)
throws
Exception
{
if
(
pathOfGpuBinary
!=
null
)
{
return
;
}
String
envBinaryPath
=
System
.
getenv
(
ENV_BINARY_PATH
)
;
if
(
null
!=
envBinaryPath
)
{
if
(
new
File
(
envBinaryPath
)
.
exists
(
)
)
{
pathOfGpuBinary
=
envBinaryPath
;
LOG
.
info
(
+
pathOfGpuBinary
)
;
return
;
}
}
LOG
.
info
(
)
;
File
binaryFile
;
boolean
found
=
false
;
for
(
String
dir
:
DEFAULT_BINARY_SEARCH_DIRS
)
{
binaryFile
=
new
File
(
dir
,
DEFAULT_BINARY_NAME
)
;
if
(
binaryFile
.
exists
(
)
)
{
found
=
true
;
pathOfGpuBinary
=
binaryFile
.
getAbsolutePath
(
)
;
if
(
null
!=
envBinaryPath
)
{
if
(
new
File
(
envBinaryPath
)
.
exists
(
)
)
{
pathOfGpuBinary
=
envBinaryPath
;
LOG
.
info
(
+
pathOfGpuBinary
)
;
return
;
}
}
LOG
.
info
(
)
;
File
binaryFile
;
boolean
found
=
false
;
for
(
String
dir
:
DEFAULT_BINARY_SEARCH_DIRS
)
{
binaryFile
=
new
File
(
dir
,
DEFAULT_BINARY_NAME
)
;
if
(
binaryFile
.
exists
(
)
)
{
found
=
true
;
pathOfGpuBinary
=
binaryFile
.
getAbsolutePath
(
)
;
LOG
.
info
(
+
pathOfGpuBinary
)
;
break
;
public
synchronized
void
addDeviceSet
(
String
resourceName
,
Set
<
Device
>
deviceSet
)
{
private
synchronized
DeviceAllocation
internalAssignDevices
(
String
resourceName
,
Container
container
)
throws
ResourceHandlerException
{
Resource
requestedResource
=
container
.
getResource
(
)
;
ContainerId
containerId
=
container
.
getContainerId
(
)
;
int
requestedDeviceCount
=
getRequestedDeviceCount
(
resourceName
,
requestedResource
)
;
public
synchronized
void
cleanupAssignedDevices
(
String
resourceName
,
ContainerId
containerId
)
{
Iterator
<
Map
.
Entry
<
Device
,
ContainerId
>>
iter
=
allUsedDevices
.
get
(
resourceName
)
.
entrySet
(
)
.
iterator
(
)
;
Map
.
Entry
<
Device
,
ContainerId
>
entry
;
while
(
iter
.
hasNext
(
)
)
{
entry
=
iter
.
next
(
)
;
if
(
entry
.
getValue
(
)
.
equals
(
containerId
)
)
{
private
void
defaultScheduleAction
(
Set
<
Device
>
allowed
,
Map
<
Device
,
ContainerId
>
used
,
Set
<
Device
>
assigned
,
ContainerId
containerId
,
int
count
)
{
@
Override
public
void
initialize
(
Context
context
)
throws
YarnException
{
deviceDockerCommandPlugin
=
new
DeviceResourceDockerRuntimePluginImpl
(
resourceName
,
devicePlugin
,
this
)
;
deviceResourceUpdater
=
new
DeviceResourceUpdaterImpl
(
resourceName
,
devicePlugin
)
;
@
Override
public
void
updateDockerRunCommand
(
DockerRunCommand
dockerRunCommand
,
Container
container
)
throws
ContainerExecutionException
{
String
containerId
=
container
.
getContainerId
(
)
.
toString
(
)
;
String
containerId
=
container
.
getContainerId
(
)
.
toString
(
)
;
LOG
.
debug
(
,
containerId
)
;
if
(
!
requestedDevice
(
resourceName
,
container
)
)
{
return
;
}
DeviceRuntimeSpec
deviceRuntimeSpec
=
getRuntimeSpec
(
container
)
;
if
(
deviceRuntimeSpec
==
null
)
{
LOG
.
warn
(
+
devicePlugin
.
getClass
(
)
.
getCanonicalName
(
)
+
+
containerId
)
;
return
;
}
dockerRunCommand
.
addRuntime
(
deviceRuntimeSpec
.
getContainerRuntime
(
)
)
;
LOG
.
debug
(
,
deviceRuntimeSpec
.
getContainerRuntime
(
)
,
containerId
)
;
Set
<
MountDeviceSpec
>
deviceMounts
=
deviceRuntimeSpec
.
getDeviceMounts
(
)
;
LOG
.
debug
(
,
deviceMounts
,
containerId
)
;
for
(
MountDeviceSpec
mountDeviceSpec
:
deviceMounts
)
{
dockerRunCommand
.
addDevice
(
mountDeviceSpec
.
getDevicePathInHost
(
)
,
mountDeviceSpec
.
getDevicePathInContainer
(
)
)
;
}
Set
<
MountVolumeSpec
>
mountVolumeSpecs
=
deviceRuntimeSpec
.
getVolumeMounts
(
)
;
LOG
.
warn
(
+
devicePlugin
.
getClass
(
)
.
getCanonicalName
(
)
+
+
containerId
)
;
return
;
}
dockerRunCommand
.
addRuntime
(
deviceRuntimeSpec
.
getContainerRuntime
(
)
)
;
LOG
.
debug
(
,
deviceRuntimeSpec
.
getContainerRuntime
(
)
,
containerId
)
;
Set
<
MountDeviceSpec
>
deviceMounts
=
deviceRuntimeSpec
.
getDeviceMounts
(
)
;
LOG
.
debug
(
,
deviceMounts
,
containerId
)
;
for
(
MountDeviceSpec
mountDeviceSpec
:
deviceMounts
)
{
dockerRunCommand
.
addDevice
(
mountDeviceSpec
.
getDevicePathInHost
(
)
,
mountDeviceSpec
.
getDevicePathInContainer
(
)
)
;
}
Set
<
MountVolumeSpec
>
mountVolumeSpecs
=
deviceRuntimeSpec
.
getVolumeMounts
(
)
;
LOG
.
debug
(
,
mountVolumeSpecs
,
containerId
)
;
for
(
MountVolumeSpec
mountVolumeSpec
:
mountVolumeSpecs
)
{
if
(
mountVolumeSpec
.
getReadOnly
(
)
)
{
dockerRunCommand
.
addReadOnlyMountLocation
(
mountVolumeSpec
.
getHostPath
(
)
,
mountVolumeSpec
.
getMountPath
(
)
)
;
}
else
{
dockerRunCommand
.
addReadWriteMountLocation
(
mountVolumeSpec
.
getHostPath
(
)
,
mountVolumeSpec
.
getMountPath
(
)
)
;
public
synchronized
DeviceRuntimeSpec
getRuntimeSpec
(
Container
container
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
DeviceRuntimeSpec
deviceRuntimeSpec
=
cachedSpec
.
get
(
containerId
)
;
if
(
deviceRuntimeSpec
==
null
)
{
Set
<
Device
>
allocated
=
getAllocatedDevices
(
container
)
;
if
(
allocated
==
null
||
allocated
.
size
(
)
==
0
)
{
@
Override
public
synchronized
List
<
PrivilegedOperation
>
preStart
(
Container
container
)
throws
ResourceHandlerException
{
String
containerIdStr
=
container
.
getContainerId
(
)
.
toString
(
)
;
DeviceMappingManager
.
DeviceAllocation
allocation
=
deviceMappingManager
.
assignDevices
(
resourceName
,
container
)
;
LOG
.
error
(
)
;
return
Optional
.
empty
(
)
;
}
File
f
=
new
File
(
path
)
;
if
(
!
f
.
exists
(
)
)
{
LOG
.
error
(
)
;
return
Optional
.
empty
(
)
;
}
if
(
!
FileUtil
.
canExecute
(
f
)
)
{
LOG
.
error
(
)
;
return
Optional
.
empty
(
)
;
}
ShellCommandExecutor
shell
=
new
ShellCommandExecutor
(
new
String
[
]
{
path
}
,
null
,
null
,
MAX_EXEC_TIMEOUT_MS
)
;
try
{
shell
.
execute
(
)
;
String
output
=
shell
.
getOutput
(
)
;
return
Optional
.
of
(
output
)
;
}
catch
(
IOException
e
)
{
private
AbstractFpgaVendorPlugin
createFpgaVendorPlugin
(
Configuration
conf
)
{
String
vendorPluginClass
=
conf
.
get
(
YarnConfiguration
.
NM_FPGA_VENDOR_PLUGIN
,
YarnConfiguration
.
DEFAULT_NM_FPGA_VENDOR_PLUGIN
)
;
if
(
initialized
)
{
return
true
;
}
String
pluginDefaultBinaryName
=
DEFAULT_BINARY_NAME
;
String
executable
=
config
.
get
(
YarnConfiguration
.
NM_FPGA_PATH_TO_EXEC
,
pluginDefaultBinaryName
)
;
File
binaryPath
=
new
File
(
executable
)
;
if
(
!
binaryPath
.
exists
(
)
)
{
LOG
.
warn
(
+
YarnConfiguration
.
NM_FPGA_PATH_TO_EXEC
+
)
;
executable
=
pluginDefaultBinaryName
;
String
pluginDefaultPreferredPath
=
getDefaultPathToExecutable
(
)
;
if
(
null
==
pluginDefaultPreferredPath
)
{
LOG
.
warn
(
+
+
ALTERAOCLSDKROOT_NAME
+
)
;
}
else
{
binaryPath
=
new
File
(
pluginDefaultPreferredPath
+
,
pluginDefaultBinaryName
)
;
if
(
binaryPath
.
exists
(
)
)
{
executable
=
binaryPath
.
getAbsolutePath
(
)
;
@
Override
public
String
retrieveIPfilePath
(
String
id
,
String
dstDir
,
Map
<
Path
,
List
<
String
>>
localizedResources
)
{
String
ipFilePath
=
null
;
@
Override
public
boolean
configureIP
(
String
ipPath
,
FpgaDevice
device
)
{
Shell
.
ShellCommandExecutor
shexec
;
String
aclName
;
aclName
=
device
.
getAliasDevName
(
)
;
shexec
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
this
.
pathToExecutable
,
,
aclName
,
ipPath
}
)
;
try
{
shexec
.
execute
(
)
;
if
(
0
==
shexec
.
getExitCode
(
)
)
{
@
Override
public
boolean
configureIP
(
String
ipPath
,
FpgaDevice
device
)
{
Shell
.
ShellCommandExecutor
shexec
;
String
aclName
;
aclName
=
device
.
getAliasDevName
(
)
;
shexec
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
this
.
pathToExecutable
,
,
aclName
,
ipPath
}
)
;
try
{
shexec
.
execute
(
)
;
if
(
0
==
shexec
.
getExitCode
(
)
)
{
LOG
.
debug
(
,
shexec
.
getOutput
(
)
)
;
@
Override
public
boolean
configureIP
(
String
ipPath
,
FpgaDevice
device
)
{
Shell
.
ShellCommandExecutor
shexec
;
String
aclName
;
aclName
=
device
.
getAliasDevName
(
)
;
shexec
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
this
.
pathToExecutable
,
,
aclName
,
ipPath
}
)
;
try
{
shexec
.
execute
(
)
;
if
(
0
==
shexec
.
getExitCode
(
)
)
{
LOG
.
debug
(
,
shexec
.
getOutput
(
)
)
;
LOG
.
info
(
+
ipPath
+
+
aclName
+
)
;
}
else
{
LOG
.
error
(
)
;
LOG
.
error
(
shexec
.
getOutput
(
)
)
;
return
false
;
}
}
catch
(
IOException
e
)
{
Shell
.
ShellCommandExecutor
shexec
;
String
aclName
;
aclName
=
device
.
getAliasDevName
(
)
;
shexec
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
this
.
pathToExecutable
,
,
aclName
,
ipPath
}
)
;
try
{
shexec
.
execute
(
)
;
if
(
0
==
shexec
.
getExitCode
(
)
)
{
LOG
.
debug
(
,
shexec
.
getOutput
(
)
)
;
LOG
.
info
(
+
ipPath
+
+
aclName
+
)
;
}
else
{
LOG
.
error
(
)
;
LOG
.
error
(
shexec
.
getOutput
(
)
)
;
return
false
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
ipPath
+
+
aclName
+
,
e
)
;
public
synchronized
GpuDeviceInformation
getGpuDeviceInformation
(
)
throws
YarnException
{
if
(
numOfErrorExecutionSinceLastSucceed
==
MAX_REPEATED_ERROR_ALLOWED
)
{
String
msg
=
getErrorMessageOfScriptExecutionThresholdReached
(
)
;
public
synchronized
GpuDeviceInformation
getGpuDeviceInformation
(
)
throws
YarnException
{
if
(
numOfErrorExecutionSinceLastSucceed
==
MAX_REPEATED_ERROR_ALLOWED
)
{
String
msg
=
getErrorMessageOfScriptExecutionThresholdReached
(
)
;
LOG
.
error
(
msg
)
;
throw
new
YarnException
(
msg
)
;
}
try
{
lastDiscoveredGpuInformation
=
nvidiaBinaryHelper
.
getGpuDeviceInformation
(
pathOfGpuBinary
)
;
}
catch
(
IOException
e
)
{
numOfErrorExecutionSinceLastSucceed
++
;
String
msg
=
getErrorMessageOfScriptExecution
(
e
.
getMessage
(
)
)
;
LOG
.
debug
(
msg
)
;
throw
new
YarnException
(
msg
,
e
)
;
}
catch
(
YarnException
e
)
{
numOfErrorExecutionSinceLastSucceed
++
;
String
msg
=
getFailedToParseErrorMessage
(
e
.
getMessage
(
)
)
;
private
List
<
GpuDevice
>
parseGpuDevicesFromAutoDiscoveredGpuInfo
(
)
throws
YarnException
{
if
(
lastDiscoveredGpuInformation
==
null
)
{
String
msg
=
YarnConfiguration
.
NM_GPU_ALLOWED_DEVICES
+
+
YarnConfiguration
.
AUTOMATICALLY_DISCOVER_GPU_DEVICES
+
+
+
+
YarnConfiguration
.
NM_GPU_ALLOWED_DEVICES
+
;
if
(
splitByColon
.
length
!=
2
)
{
throwIfNecessary
(
GpuDeviceSpecificationException
.
createWithWrongValueSpecified
(
device
,
devices
)
,
getConf
(
)
)
;
LOG
.
warn
(
,
device
)
;
}
GpuDevice
gpuDevice
;
try
{
gpuDevice
=
parseGpuDevice
(
splitByColon
)
;
}
catch
(
NumberFormatException
e
)
{
throwIfNecessary
(
GpuDeviceSpecificationException
.
createWithWrongValueSpecified
(
device
,
devices
,
e
)
,
getConf
(
)
)
;
LOG
.
warn
(
,
device
)
;
continue
;
}
if
(
!
gpuDevices
.
contains
(
gpuDevice
)
)
{
gpuDevices
.
add
(
gpuDevice
)
;
}
else
{
throwIfNecessary
(
GpuDeviceSpecificationException
.
createWithDuplicateValueSpecified
(
device
,
devices
)
,
getConf
(
)
)
;
LOG
.
warn
(
,
device
)
;
public
synchronized
void
initialize
(
Configuration
config
,
NvidiaBinaryHelper
nvidiaHelper
)
throws
YarnException
{
setConf
(
config
)
;
this
.
nvidiaBinaryHelper
=
nvidiaHelper
;
if
(
isAutoDiscoveryEnabled
(
)
)
{
numOfErrorExecutionSinceLastSucceed
=
0
;
lookUpAutoDiscoveryBinary
(
config
)
;
try
{
LOG
.
info
(
)
;
GpuDeviceInformation
info
=
getGpuDeviceInformation
(
)
;
@
Override
public
void
updateConfiguredResource
(
Resource
res
)
throws
YarnException
{
LOG
.
info
(
)
;
List
<
GpuDevice
>
usableGpus
=
gpuDiscoverer
.
getGpusUsableByYarn
(
)
;
if
(
usableGpus
==
null
||
usableGpus
.
isEmpty
(
)
)
{
String
message
=
+
;
private
void
checkErrorCount
(
)
throws
YarnException
{
if
(
numOfErrorExecutionSinceLastSucceed
==
MAX_REPEATED_ERROR_ALLOWED
)
{
String
msg
=
+
MAX_REPEATED_ERROR_ALLOWED
+
;
private
void
init
(
)
throws
ContainerExecutionException
{
String
endpoint
=
conf
.
get
(
YarnConfiguration
.
NVIDIA_DOCKER_PLUGIN_V1_ENDPOINT
,
YarnConfiguration
.
DEFAULT_NVIDIA_DOCKER_PLUGIN_V1_ENDPOINT
)
;
if
(
null
==
endpoint
||
endpoint
.
isEmpty
(
)
)
{
}
String
cliOptions
;
try
{
URL
url
=
new
URL
(
endpoint
)
;
URLConnection
uc
=
url
.
openConnection
(
)
;
uc
.
setRequestProperty
(
,
)
;
StringWriter
writer
=
new
StringWriter
(
)
;
IOUtils
.
copy
(
uc
.
getInputStream
(
)
,
writer
,
)
;
cliOptions
=
writer
.
toString
(
)
;
LOG
.
info
(
+
+
cliOptions
)
;
for
(
String
str
:
cliOptions
.
split
(
)
)
{
str
=
str
.
trim
(
)
;
if
(
str
.
startsWith
(
DEVICE_OPTION
)
)
{
addToCommand
(
DEVICE_OPTION
,
getValue
(
str
)
)
;
}
else
if
(
str
.
startsWith
(
VOLUME_DRIVER_OPTION
)
)
{
volumeDriver
=
getValue
(
str
)
;
@
SuppressWarnings
(
)
private
void
reclaimOpportunisticContainerResources
(
Container
container
)
{
List
<
Container
>
extraOppContainersToReclaim
=
pickOpportunisticContainersToReclaimResources
(
container
.
getContainerId
(
)
)
;
for
(
Container
contToReclaim
:
extraOppContainersToReclaim
)
{
String
preemptionAction
=
usePauseEventForPreemption
==
true
?
:
;
private
void
startContainer
(
Container
container
)
{
private
void
shedQueuedOpportunisticContainers
(
)
{
int
numAllowed
=
this
.
queuingLimit
.
getMaxQueueLength
(
)
;
Iterator
<
Container
>
containerIter
=
queuedOpportunisticContainers
.
values
(
)
.
iterator
(
)
;
while
(
containerIter
.
hasNext
(
)
)
{
Container
container
=
containerIter
.
next
(
)
;
if
(
container
.
getContainerState
(
)
!=
ContainerState
.
PAUSED
)
{
if
(
numAllowed
<=
0
)
{
container
.
sendKillEvent
(
ContainerExitStatus
.
KILLED_BY_CONTAINER_SCHEDULER
,
)
;
containerIter
.
remove
(
)
;
public
Map
<
String
,
String
>
publishVolumes
(
)
throws
YarnException
,
IOException
{
LOG
.
info
(
)
;
Map
<
String
,
String
>
volumeMounts
=
new
HashMap
<
>
(
)
;
List
<
VolumeMetaData
>
volumes
=
getVolumes
(
)
;
public
void
unpublishVolumes
(
)
throws
YarnException
,
IOException
{
LOG
.
info
(
)
;
List
<
VolumeMetaData
>
volumes
=
getVolumes
(
)
;
private
Map
<
String
,
String
>
publishVolume
(
VolumeMetaData
volume
)
throws
IOException
,
YarnException
{
Map
<
String
,
String
>
bindVolumes
=
new
HashMap
<
>
(
)
;
File
localMount
=
getLocalVolumeMountPath
(
localMountRoot
,
volume
.
getVolumeId
(
)
.
toString
(
)
)
;
File
localStaging
=
getLocalVolumeStagingPath
(
localMountRoot
,
volume
.
getVolumeId
(
)
.
toString
(
)
)
;
static
boolean
shouldRun
(
String
script
,
String
healthScript
)
{
if
(
healthScript
==
null
||
healthScript
.
trim
(
)
.
isEmpty
(
)
)
{
private
void
markStoreUnHealthy
(
DBException
dbErr
)
{
@
Override
public
void
storeContainer
(
ContainerId
containerId
,
int
containerVersion
,
long
startTime
,
StartContainerRequest
startRequest
)
throws
IOException
{
String
idStr
=
containerId
.
toString
(
)
;
@
Override
public
void
storeContainerQueued
(
ContainerId
containerId
)
throws
IOException
{
private
void
removeContainerQueued
(
ContainerId
containerId
)
throws
IOException
{
@
Override
public
void
storeContainerPaused
(
ContainerId
containerId
)
throws
IOException
{
@
Override
public
void
removeContainerPaused
(
ContainerId
containerId
)
throws
IOException
{
@
Override
public
void
storeContainerDiagnostics
(
ContainerId
containerId
,
StringBuilder
diagnostics
)
throws
IOException
{
@
Override
public
void
storeContainerLaunched
(
ContainerId
containerId
)
throws
IOException
{
@
Override
public
void
storeContainerUpdateToken
(
ContainerId
containerId
,
ContainerTokenIdentifier
containerTokenIdentifier
)
throws
IOException
{
@
Override
public
void
storeContainerKilled
(
ContainerId
containerId
)
throws
IOException
{
@
Override
public
void
storeContainerCompleted
(
ContainerId
containerId
,
int
exitCode
)
throws
IOException
{
@
Override
public
void
removeContainer
(
ContainerId
containerId
)
throws
IOException
{
@
Override
public
void
storeApplication
(
ApplicationId
appId
,
ContainerManagerApplicationProto
p
)
throws
IOException
{
@
Override
public
void
removeApplication
(
ApplicationId
appId
)
throws
IOException
{
@
Override
public
void
finishResourceLocalization
(
String
user
,
ApplicationId
appId
,
LocalizedResourceProto
proto
)
throws
IOException
{
String
localPath
=
proto
.
getLocalPath
(
)
;
String
startedKey
=
getResourceStartedKey
(
user
,
appId
,
localPath
)
;
String
completedKey
=
getResourceCompletedKey
(
user
,
appId
,
localPath
)
;
@
Override
public
void
removeLocalizedResource
(
String
user
,
ApplicationId
appId
,
Path
localPath
)
throws
IOException
{
String
localPathStr
=
localPath
.
toString
(
)
;
String
startedKey
=
getResourceStartedKey
(
user
,
appId
,
localPathStr
)
;
String
completedKey
=
getResourceCompletedKey
(
user
,
appId
,
localPathStr
)
;
@
Override
public
void
storeAssignedResources
(
Container
container
,
String
resourceType
,
List
<
Serializable
>
assignedResources
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
iter
.
next
(
)
;
result
.
setNextMasterKey
(
parseMasterKey
(
entry
.
getValue
(
)
)
)
;
LOG
.
info
(
+
result
.
getNextMasterKey
(
)
.
getKeyId
(
)
)
;
}
else
{
int
idEndPos
;
ApplicationAttemptId
attemptId
;
try
{
idEndPos
=
key
.
indexOf
(
'/'
,
AMRMPROXY_KEY_PREFIX
.
length
(
)
)
;
if
(
idEndPos
<
0
)
{
throw
new
IOException
(
+
key
)
;
}
attemptId
=
ApplicationAttemptId
.
fromString
(
key
.
substring
(
AMRMPROXY_KEY_PREFIX
.
length
(
)
,
idEndPos
)
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
key
+
,
e
)
;
unknownKeys
.
add
(
key
)
;
continue
;
protected
DB
openDatabase
(
Configuration
conf
)
throws
IOException
{
Path
storeRoot
=
createStorageDir
(
conf
)
;
Options
options
=
new
Options
(
)
;
options
.
createIfMissing
(
false
)
;
protected
void
checkVersion
(
)
throws
IOException
{
Version
loadedVersion
=
loadVersion
(
)
;
@
Private
public
synchronized
void
setMasterKey
(
MasterKey
masterKeyRecord
)
{
if
(
super
.
currentMasterKey
==
null
||
super
.
currentMasterKey
.
getMasterKey
(
)
.
getKeyId
(
)
!=
masterKeyRecord
.
getKeyId
(
)
)
{
public
synchronized
void
setNodeId
(
NodeId
nodeId
)
{
nodeHostAddr
=
nodeId
.
toString
(
)
;
@
Private
public
synchronized
void
setMasterKey
(
MasterKey
masterKey
)
{
if
(
super
.
currentMasterKey
==
null
||
super
.
currentMasterKey
.
getMasterKey
(
)
.
getKeyId
(
)
!=
masterKey
.
getKeyId
(
)
)
{
public
synchronized
void
appFinished
(
ApplicationId
appId
)
{
List
<
ApplicationAttemptId
>
appAttemptList
=
appToAppAttemptMap
.
get
(
appId
)
;
if
(
appAttemptList
!=
null
)
{
public
synchronized
void
setNodeId
(
NodeId
nodeId
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
dispatcher
=
createDispatcher
(
)
;
dispatcher
.
register
(
NMTimelineEventType
.
class
,
new
ForwardingEventHandler
(
)
)
;
addIfService
(
dispatcher
)
;
this
.
nmLoginUGI
=
UserGroupInformation
.
isSecurityEnabled
(
)
?
UserGroupInformation
.
getLoginUser
(
)
:
UserGroupInformation
.
getCurrentUser
(
)
;
memoryMetric
.
setRealtimeAggregationOp
(
TimelineMetricOperation
.
SUM
)
;
memoryMetric
.
addValue
(
currentTimeMillis
,
pmemUsage
)
;
entity
.
addMetric
(
memoryMetric
)
;
}
if
(
cpuUsagePercentPerCore
!=
ResourceCalculatorProcessTree
.
UNAVAILABLE
)
{
TimelineMetric
cpuMetric
=
new
TimelineMetric
(
)
;
cpuMetric
.
setId
(
ContainerMetric
.
CPU
.
toString
(
)
)
;
cpuMetric
.
setRealtimeAggregationOp
(
TimelineMetricOperation
.
SUM
)
;
cpuMetric
.
addValue
(
currentTimeMillis
,
Math
.
round
(
cpuUsagePercentPerCore
)
)
;
entity
.
addMetric
(
cpuMetric
)
;
}
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
entity
.
addMetric
(
memoryMetric
)
;
}
if
(
cpuUsagePercentPerCore
!=
ResourceCalculatorProcessTree
.
UNAVAILABLE
)
{
TimelineMetric
cpuMetric
=
new
TimelineMetric
(
)
;
cpuMetric
.
setId
(
ContainerMetric
.
CPU
.
toString
(
)
)
;
cpuMetric
.
setRealtimeAggregationOp
(
TimelineMetricOperation
.
SUM
)
;
cpuMetric
.
addValue
(
currentTimeMillis
,
Math
.
round
(
cpuUsagePercentPerCore
)
)
;
entity
.
addMetric
(
cpuMetric
)
;
}
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
}
if
(
cpuUsagePercentPerCore
!=
ResourceCalculatorProcessTree
.
UNAVAILABLE
)
{
TimelineMetric
cpuMetric
=
new
TimelineMetric
(
)
;
cpuMetric
.
setId
(
ContainerMetric
.
CPU
.
toString
(
)
)
;
cpuMetric
.
setRealtimeAggregationOp
(
TimelineMetricOperation
.
SUM
)
;
cpuMetric
.
addValue
(
currentTimeMillis
,
Math
.
round
(
cpuUsagePercentPerCore
)
)
;
entity
.
addMetric
(
cpuMetric
)
;
}
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
container
.
getContainerId
(
)
)
;
TimelineMetric
cpuMetric
=
new
TimelineMetric
(
)
;
cpuMetric
.
setId
(
ContainerMetric
.
CPU
.
toString
(
)
)
;
cpuMetric
.
setRealtimeAggregationOp
(
TimelineMetricOperation
.
SUM
)
;
cpuMetric
.
addValue
(
currentTimeMillis
,
Math
.
round
(
cpuUsagePercentPerCore
)
)
;
entity
.
addMetric
(
cpuMetric
)
;
}
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
container
.
getContainerId
(
)
)
;
LOG
.
debug
(
,
container
.
getContainerId
(
)
,
e
)
;
cpuMetric
.
setId
(
ContainerMetric
.
CPU
.
toString
(
)
)
;
cpuMetric
.
setRealtimeAggregationOp
(
TimelineMetricOperation
.
SUM
)
;
cpuMetric
.
addValue
(
currentTimeMillis
,
Math
.
round
(
cpuUsagePercentPerCore
)
)
;
entity
.
addMetric
(
cpuMetric
)
;
}
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
container
.
getContainerId
(
)
)
;
LOG
.
debug
(
,
container
.
getContainerId
(
)
,
e
)
;
}
catch
(
YarnException
e
)
{
private
void
publishContainerLocalizationEvent
(
ContainerLocalizationEvent
event
,
String
eventType
)
{
if
(
publishNMContainerEvents
)
{
Container
container
=
event
.
getContainer
(
)
;
ContainerId
containerId
=
container
.
getContainerId
(
)
;
TimelineEntity
entity
=
createContainerEntity
(
containerId
)
;
TimelineEvent
tEvent
=
new
TimelineEvent
(
)
;
tEvent
.
setId
(
eventType
)
;
tEvent
.
setTimestamp
(
event
.
getTimestamp
(
)
)
;
entity
.
addEvent
(
tEvent
)
;
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
Container
container
=
event
.
getContainer
(
)
;
ContainerId
containerId
=
container
.
getContainerId
(
)
;
TimelineEntity
entity
=
createContainerEntity
(
containerId
)
;
TimelineEvent
tEvent
=
new
TimelineEvent
(
)
;
tEvent
.
setId
(
eventType
)
;
tEvent
.
setTimestamp
(
event
.
getTimestamp
(
)
)
;
entity
.
addEvent
(
tEvent
)
;
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
ContainerId
containerId
=
container
.
getContainerId
(
)
;
TimelineEntity
entity
=
createContainerEntity
(
containerId
)
;
TimelineEvent
tEvent
=
new
TimelineEvent
(
)
;
tEvent
.
setId
(
eventType
)
;
tEvent
.
setTimestamp
(
event
.
getTimestamp
(
)
)
;
entity
.
addEvent
(
tEvent
)
;
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
container
.
getContainerId
(
)
)
;
TimelineEvent
tEvent
=
new
TimelineEvent
(
)
;
tEvent
.
setId
(
eventType
)
;
tEvent
.
setTimestamp
(
event
.
getTimestamp
(
)
)
;
entity
.
addEvent
(
tEvent
)
;
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
container
.
getContainerId
(
)
)
;
LOG
.
debug
(
,
container
.
getContainerId
(
)
,
e
)
;
}
catch
(
YarnException
e
)
{
tEvent
.
setId
(
eventType
)
;
tEvent
.
setTimestamp
(
event
.
getTimestamp
(
)
)
;
entity
.
addEvent
(
tEvent
)
;
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
try
{
TimelineV2Client
timelineClient
=
getTimelineClient
(
appId
)
;
if
(
timelineClient
!=
null
)
{
timelineClient
.
putEntitiesAsync
(
entity
)
;
}
else
{
LOG
.
error
(
+
+
container
.
getContainerId
(
)
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
container
.
getContainerId
(
)
)
;
LOG
.
debug
(
,
container
.
getContainerId
(
)
,
e
)
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
+
container
.
getContainerId
(
)
,
e
.
getMessage
(
)
)
;
private
void
putEntity
(
TimelineEntity
entity
,
ApplicationId
appId
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
createCgroup
(
String
controller
,
String
groupName
)
throws
IOException
{
String
path
=
pathForCgroup
(
controller
,
groupName
)
;
private
void
updateCgroup
(
String
controller
,
String
groupName
,
String
param
,
String
value
)
throws
IOException
{
String
path
=
pathForCgroup
(
controller
,
groupName
)
;
param
=
controller
+
+
param
;
private
void
logLineFromTasksFile
(
File
cgf
)
{
String
str
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
(
BufferedReader
inl
=
new
BufferedReader
(
new
InputStreamReader
(
new
FileInputStream
(
cgf
+
)
,
)
)
)
{
str
=
inl
.
readLine
(
)
;
if
(
str
!=
null
)
{
@
VisibleForTesting
boolean
deleteCgroup
(
String
cgroupPath
)
{
boolean
deleted
=
false
;
public
static
Resource
getNodeResources
(
Configuration
configuration
)
{
Configuration
conf
=
new
Configuration
(
configuration
)
;
String
memory
=
ResourceInformation
.
MEMORY_MB
.
getName
(
)
;
String
vcores
=
ResourceInformation
.
VCORES
.
getName
(
)
;
Resource
ret
=
Resource
.
newInstance
(
0
,
0
)
;
Map
<
String
,
ResourceInformation
>
resourceInformation
=
ResourceUtils
.
getNodeResourceInformation
(
conf
)
;
for
(
Map
.
Entry
<
String
,
ResourceInformation
>
entry
:
resourceInformation
.
entrySet
(
)
)
{
ret
.
setResourceInformation
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
Configuration
conf
=
new
Configuration
(
configuration
)
;
String
memory
=
ResourceInformation
.
MEMORY_MB
.
getName
(
)
;
String
vcores
=
ResourceInformation
.
VCORES
.
getName
(
)
;
Resource
ret
=
Resource
.
newInstance
(
0
,
0
)
;
Map
<
String
,
ResourceInformation
>
resourceInformation
=
ResourceUtils
.
getNodeResourceInformation
(
conf
)
;
for
(
Map
.
Entry
<
String
,
ResourceInformation
>
entry
:
resourceInformation
.
entrySet
(
)
)
{
ret
.
setResourceInformation
(
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
LOG
.
debug
(
,
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
if
(
resourceInformation
.
containsKey
(
memory
)
)
{
Long
value
=
resourceInformation
.
get
(
memory
)
.
getValue
(
)
;
if
(
value
>
Integer
.
MAX_VALUE
)
{
throw
new
YarnRuntimeException
(
+
value
+
)
;
}
ResourceInformation
memResInfo
=
resourceInformation
.
get
(
memory
)
;
if
(
memResInfo
.
getValue
(
)
==
0
)
{
ret
.
setMemorySize
(
getContainerMemoryMB
(
conf
)
)
;
}
if
(
resourceInformation
.
containsKey
(
memory
)
)
{
Long
value
=
resourceInformation
.
get
(
memory
)
.
getValue
(
)
;
if
(
value
>
Integer
.
MAX_VALUE
)
{
throw
new
YarnRuntimeException
(
+
value
+
)
;
}
ResourceInformation
memResInfo
=
resourceInformation
.
get
(
memory
)
;
if
(
memResInfo
.
getValue
(
)
==
0
)
{
ret
.
setMemorySize
(
getContainerMemoryMB
(
conf
)
)
;
LOG
.
debug
(
,
ret
.
getMemorySize
(
)
)
;
}
}
if
(
resourceInformation
.
containsKey
(
vcores
)
)
{
Long
value
=
resourceInformation
.
get
(
vcores
)
.
getValue
(
)
;
if
(
value
>
Integer
.
MAX_VALUE
)
{
throw
new
YarnRuntimeException
(
+
value
+
)
;
}
ResourceInformation
vcoresResInfo
=
resourceInformation
.
get
(
vcores
)
;
if
(
vcoresResInfo
.
getValue
(
)
==
0
)
{
ret
.
setVirtualCores
(
getVCores
(
conf
)
)
;
Long
value
=
resourceInformation
.
get
(
memory
)
.
getValue
(
)
;
if
(
value
>
Integer
.
MAX_VALUE
)
{
throw
new
YarnRuntimeException
(
+
value
+
)
;
}
ResourceInformation
memResInfo
=
resourceInformation
.
get
(
memory
)
;
if
(
memResInfo
.
getValue
(
)
==
0
)
{
ret
.
setMemorySize
(
getContainerMemoryMB
(
conf
)
)
;
LOG
.
debug
(
,
ret
.
getMemorySize
(
)
)
;
}
}
if
(
resourceInformation
.
containsKey
(
vcores
)
)
{
Long
value
=
resourceInformation
.
get
(
vcores
)
.
getValue
(
)
;
if
(
value
>
Integer
.
MAX_VALUE
)
{
throw
new
YarnRuntimeException
(
+
value
+
)
;
}
ResourceInformation
vcoresResInfo
=
resourceInformation
.
get
(
vcores
)
;
if
(
vcoresResInfo
.
getValue
(
)
==
0
)
{
ret
.
setVirtualCores
(
getVCores
(
conf
)
)
;
LOG
.
debug
(
,
ret
.
getVirtualCores
(
)
)
;
String
command
=
;
String
[
]
containerPath
=
containerURI
.
getPath
(
)
.
split
(
)
;
String
cId
=
containerPath
[
2
]
;
if
(
containerPath
.
length
==
4
)
{
for
(
ShellContainerCommand
c
:
ShellContainerCommand
.
values
(
)
)
{
if
(
c
.
name
(
)
.
equalsIgnoreCase
(
containerPath
[
3
]
)
)
{
command
=
containerPath
[
3
]
.
toLowerCase
(
)
;
}
}
}
Container
container
=
nmContext
.
getContainers
(
)
.
get
(
ContainerId
.
fromString
(
cId
)
)
;
if
(
!
checkAuthorization
(
session
,
container
)
)
{
session
.
close
(
1008
,
)
;
return
;
}
if
(
checkInsecureSetup
(
)
)
{
session
.
close
(
1003
,
)
;
return
;
}
LOG
.
info
(
session
.
getRemoteAddress
(
)
.
getHostString
(
)
+
)
;
for
(
ShellContainerCommand
c
:
ShellContainerCommand
.
values
(
)
)
{
if
(
c
.
name
(
)
.
equalsIgnoreCase
(
containerPath
[
3
]
)
)
{
command
=
containerPath
[
3
]
.
toLowerCase
(
)
;
}
}
}
Container
container
=
nmContext
.
getContainers
(
)
.
get
(
ContainerId
.
fromString
(
cId
)
)
;
if
(
!
checkAuthorization
(
session
,
container
)
)
{
session
.
close
(
1008
,
)
;
return
;
}
if
(
checkInsecureSetup
(
)
)
{
session
.
close
(
1003
,
)
;
return
;
}
LOG
.
info
(
session
.
getRemoteAddress
(
)
.
getHostString
(
)
+
)
;
LOG
.
info
(
+
cId
)
;
ContainerExecContext
execContext
=
new
ContainerExecContext
.
Builder
(
)
.
setContainer
(
container
)
.
setNMLocalPath
(
nmContext
.
getLocalDirsHandler
(
)
)
.
setShell
(
command
)
.
build
(
)
;
pair
=
exec
.
execContainer
(
execContext
)
;
}
catch
(
Exception
e
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
String
endOfFile
=
+
outputFileName
;
sb
.
append
(
endOfFile
+
)
;
if
(
isRunning
)
{
sb
.
append
(
+
containerIdStr
+
+
)
;
}
else
{
sb
.
append
(
)
;
}
sb
.
append
(
StringUtils
.
repeat
(
,
endOfFile
.
length
(
)
+
50
)
+
)
;
os
.
write
(
sb
.
toString
(
)
.
getBytes
(
Charset
.
forName
(
)
)
)
;
ApplicationId
appId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
Application
app
=
nmContext
.
getApplications
(
)
.
get
(
appId
)
;
String
appOwner
=
app
==
null
?
null
:
app
.
getUser
(
)
;
try
{
ContainerLogsRequest
logRequest
=
new
ContainerLogsRequest
(
)
;
logRequest
.
setAppId
(
appId
)
;
if
(
initializersClasses
!=
null
)
{
for
(
Class
<
?
>
initializer
:
initializersClasses
)
{
if
(
initializer
.
getName
(
)
.
equals
(
AuthenticationFilterInitializer
.
class
.
getName
(
)
)
)
{
hasHadoopAuthFilterInitializer
=
true
;
break
;
}
targets
.
add
(
initializer
.
getName
(
)
)
;
}
}
if
(
!
hasHadoopAuthFilterInitializer
)
{
targets
.
add
(
AuthenticationFilterInitializer
.
class
.
getName
(
)
)
;
conf
.
set
(
filterInitializerConfKey
,
StringUtils
.
join
(
,
targets
)
)
;
}
ContainerShellWebSocket
.
init
(
nmContext
)
;
LOG
.
info
(
+
bindAddress
)
;
try
{
this
.
webApp
=
WebApps
.
$for
(
,
Context
.
class
,
this
.
nmContext
,
)
.
at
(
bindAddress
)
.
withServlet
(
,
,
ContainerShellWebSocketServlet
.
class
,
params
,
false
)
.
withServlet
(
,
,
TerminalServlet
.
class
,
terminalParams
,
false
)
.
with
(
conf
)
.
withHttpSpnegoPrincipalKey
(
YarnConfiguration
.
NM_WEBAPP_SPNEGO_USER_NAME_KEY
)
.
withHttpSpnegoKeytabKey
(
YarnConfiguration
.
NM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY
)
.
withCSRFProtection
(
YarnConfiguration
.
NM_CSRF_PREFIX
)
.
withXFSProtection
(
YarnConfiguration
.
NM_XFS_PREFIX
)
.
start
(
this
.
nmWebApp
)
;
this
.
port
=
this
.
webApp
.
httpServer
(
)
.
getConnectorAddress
(
0
)
.
getPort
(
)
;
}
catch
(
Exception
e
)
{
@
Override
@
SuppressWarnings
(
)
protected
ResourceLocalizationService
createResourceLocalizationService
(
ContainerExecutor
exec
,
DeletionService
deletionContext
,
Context
context
,
NodeManagerMetrics
metrics
)
{
return
new
ResourceLocalizationService
(
super
.
dispatcher
,
exec
,
deletionContext
,
super
.
dirsHandler
,
context
,
metrics
)
{
@
Override
public
void
handle
(
LocalizationEvent
event
)
{
switch
(
event
.
getType
(
)
)
{
case
INIT_APPLICATION_RESOURCES
:
Application
app
=
(
(
ApplicationLocalizationEvent
)
event
)
.
getApplication
(
)
;
dispatcher
.
getEventHandler
(
)
.
handle
(
new
ApplicationInitedEvent
(
app
.
getAppId
(
)
)
)
;
break
;
case
LOCALIZE_CONTAINER_RESOURCES
:
ContainerLocalizationRequestEvent
rsrcReqs
=
(
ContainerLocalizationRequestEvent
)
event
;
for
(
Collection
<
LocalResourceRequest
>
rc
:
rsrcReqs
.
getRequestedResources
(
)
.
values
(
)
)
{
for
(
LocalResourceRequest
req
:
rc
)
{
Path
workSpacePath
=
new
Path
(
workSpace
.
getAbsolutePath
(
)
)
;
files
.
mkdir
(
workSpacePath
,
null
,
true
)
;
FileUtil
.
chmod
(
workSpace
.
getAbsolutePath
(
)
,
)
;
File
localDir
=
new
File
(
workSpace
.
getAbsoluteFile
(
)
,
)
;
files
.
mkdir
(
new
Path
(
localDir
.
getAbsolutePath
(
)
)
,
new
FsPermission
(
)
,
false
)
;
File
logDir
=
new
File
(
workSpace
.
getAbsoluteFile
(
)
,
)
;
files
.
mkdir
(
new
Path
(
logDir
.
getAbsolutePath
(
)
)
,
new
FsPermission
(
)
,
false
)
;
String
exec_path
=
System
.
getProperty
(
)
;
if
(
exec_path
!=
null
&&
!
exec_path
.
isEmpty
(
)
)
{
conf
=
new
Configuration
(
false
)
;
conf
.
setClass
(
,
org
.
apache
.
hadoop
.
fs
.
local
.
LocalFs
.
class
,
org
.
apache
.
hadoop
.
fs
.
AbstractFileSystem
.
class
)
;
appSubmitter
=
System
.
getProperty
(
)
;
if
(
appSubmitter
==
null
||
appSubmitter
.
isEmpty
(
)
)
{
appSubmitter
=
;
}
conf
.
set
(
YarnConfiguration
.
NM_NONSECURE_MODE_LOCAL_USER_KEY
,
appSubmitter
)
;
assertThat
(
result
.
get
(
9
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
12
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
13
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
14
)
)
.
isEqualTo
(
String
.
format
(
,
mockExec
.
getConf
(
)
.
get
(
YarnConfiguration
.
NM_LOG_DIRS
)
)
)
;
assertThat
(
result
.
get
(
15
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
16
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
17
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
18
)
)
.
isEqualTo
(
+
)
;
assertThat
(
result
.
get
(
19
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
20
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
21
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
22
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
23
)
)
.
isEqualTo
(
)
;
assertThat
(
result
.
get
(
24
)
)
.
isEqualTo
(
)
;
}
catch
(
InterruptedException
e
)
{
nm
.
init
(
conf
)
;
Object
[
]
services
=
nm
.
getServices
(
)
.
toArray
(
)
;
Object
lastService
=
services
[
services
.
length
-
1
]
;
Assert
.
assertTrue
(
,
lastService
instanceof
NodeStatusUpdater
)
;
new
Thread
(
)
{
public
void
run
(
)
{
try
{
nm
.
start
(
)
;
}
catch
(
Throwable
e
)
{
TestNodeStatusUpdater
.
this
.
nmStartError
=
e
;
throw
new
YarnRuntimeException
(
e
)
;
}
}
}
.
start
(
)
;
System
.
out
.
println
(
+
nm
.
getServiceState
(
)
)
;
int
waitCount
=
0
;
while
(
nm
.
getServiceState
(
)
==
STATE
.
INITED
&&
waitCount
++
!=
50
)
{
@
Test
public
void
testSignalContainerToContainerManager
(
)
throws
Exception
{
nm
=
new
NodeManager
(
)
{
@
Override
protected
NodeStatusUpdater
createNodeStatusUpdater
(
Context
context
,
Dispatcher
dispatcher
,
NodeHealthCheckerService
healthChecker
)
{
return
new
MyNodeStatusUpdater
(
context
,
dispatcher
,
healthChecker
,
metrics
,
true
)
;
}
@
Override
protected
ContainerManagerImpl
createContainerManager
(
Context
context
,
ContainerExecutor
exec
,
DeletionService
del
,
NodeStatusUpdater
nodeStatusUpdater
,
ApplicationACLsManager
aclsManager
,
LocalDirsHandlerService
diskhandler
)
{
return
new
MyContainerManager
(
context
,
exec
,
del
,
nodeStatusUpdater
,
metrics
,
diskhandler
)
;
}
}
;
YarnConfiguration
conf
=
createNMConfig
(
)
;
nm
.
init
(
conf
)
;
nm
.
start
(
)
;
System
.
out
.
println
(
+
nm
.
getServiceState
(
)
)
;
int
waitCount
=
0
;
while
(
nm
.
getServiceState
(
)
==
STATE
.
INITED
&&
waitCount
++
!=
20
)
{
LOG
.
info
(
)
;
if
(
nmStartError
!=
null
)
{
LOG
.
info
(
)
;
Server
resourceTracker
=
getMockResourceTracker
(
resource
)
;
resourceTracker
.
start
(
)
;
LOG
.
info
(
)
;
NodeManager
nodeManager
=
new
NodeManager
(
)
;
YarnConfiguration
nmConf
=
new
YarnConfiguration
(
)
;
nmConf
.
setSocketAddr
(
YarnConfiguration
.
RM_RESOURCE_TRACKER_ADDRESS
,
resourceTracker
.
getListenerAddress
(
)
)
;
nmConf
.
set
(
YarnConfiguration
.
NM_LOCALIZER_ADDRESS
,
)
;
nodeManager
.
init
(
nmConf
)
;
nodeManager
.
start
(
)
;
LOG
.
info
(
)
;
ContainerManager
containerManager
=
nodeManager
.
getContainerManager
(
)
;
ContainersMonitor
containerMonitor
=
containerManager
.
getContainersMonitor
(
)
;
assertEquals
(
8
,
containerMonitor
.
getVCoresAllocatedForContainers
(
)
)
;
assertEquals
(
8
*
GB
,
containerMonitor
.
getPmemAllocatedForContainers
(
)
)
;
YarnConfiguration
nmConf
=
new
YarnConfiguration
(
)
;
nmConf
.
setSocketAddr
(
YarnConfiguration
.
RM_RESOURCE_TRACKER_ADDRESS
,
resourceTracker
.
getListenerAddress
(
)
)
;
nmConf
.
set
(
YarnConfiguration
.
NM_LOCALIZER_ADDRESS
,
)
;
nodeManager
.
init
(
nmConf
)
;
nodeManager
.
start
(
)
;
LOG
.
info
(
)
;
ContainerManager
containerManager
=
nodeManager
.
getContainerManager
(
)
;
ContainersMonitor
containerMonitor
=
containerManager
.
getContainersMonitor
(
)
;
assertEquals
(
8
,
containerMonitor
.
getVCoresAllocatedForContainers
(
)
)
;
assertEquals
(
8
*
GB
,
containerMonitor
.
getPmemAllocatedForContainers
(
)
)
;
LOG
.
info
(
,
resource
)
;
GenericTestUtils
.
waitFor
(
(
)
->
containerMonitor
.
getVCoresAllocatedForContainers
(
)
==
1
,
100
,
2
*
1000
)
;
assertEquals
(
8
*
GB
,
containerMonitor
.
getPmemAllocatedForContainers
(
)
)
;
resource
.
setVirtualCores
(
5
)
;
resource
.
setMemorySize
(
4
*
1024
)
;
protected
<
T
,
R
>
List
<
R
>
runInParallel
(
List
<
T
>
testContexts
,
final
Function
<
T
,
R
>
func
)
{
ExecutorCompletionService
<
R
>
completionService
=
new
ExecutorCompletionService
<
R
>
(
this
.
getThreadPool
(
)
)
;
protected
<
T
,
R
>
List
<
R
>
runInParallel
(
List
<
T
>
testContexts
,
final
Function
<
T
,
R
>
func
)
{
ExecutorCompletionService
<
R
>
completionService
=
new
ExecutorCompletionService
<
R
>
(
this
.
getThreadPool
(
)
)
;
LOG
.
info
(
+
testContexts
.
size
(
)
)
;
for
(
int
index
=
0
;
index
<
testContexts
.
size
(
)
;
index
++
)
{
final
T
testContext
=
testContexts
.
get
(
index
)
;
protected
<
T
,
R
>
List
<
R
>
runInParallel
(
List
<
T
>
testContexts
,
final
Function
<
T
,
R
>
func
)
{
ExecutorCompletionService
<
R
>
completionService
=
new
ExecutorCompletionService
<
R
>
(
this
.
getThreadPool
(
)
)
;
LOG
.
info
(
+
testContexts
.
size
(
)
)
;
for
(
int
index
=
0
;
index
<
testContexts
.
size
(
)
;
index
++
)
{
final
T
testContext
=
testContexts
.
get
(
index
)
;
LOG
.
info
(
+
testContext
.
toString
(
)
)
;
completionService
.
submit
(
new
Callable
<
R
>
(
)
{
@
Override
public
R
call
(
)
throws
Exception
{
protected
<
T
,
R
>
List
<
R
>
runInParallel
(
List
<
T
>
testContexts
,
final
Function
<
T
,
R
>
func
)
{
ExecutorCompletionService
<
R
>
completionService
=
new
ExecutorCompletionService
<
R
>
(
this
.
getThreadPool
(
)
)
;
LOG
.
info
(
+
testContexts
.
size
(
)
)
;
for
(
int
index
=
0
;
index
<
testContexts
.
size
(
)
;
index
++
)
{
final
T
testContext
=
testContexts
.
get
(
index
)
;
LOG
.
info
(
+
testContext
.
toString
(
)
)
;
completionService
.
submit
(
new
Callable
<
R
>
(
)
{
@
Override
public
R
call
(
)
throws
Exception
{
LOG
.
info
(
+
testContext
.
toString
(
)
)
;
R
response
=
null
;
try
{
response
=
func
.
invoke
(
testContext
)
;
final
T
testContext
=
testContexts
.
get
(
index
)
;
LOG
.
info
(
+
testContext
.
toString
(
)
)
;
completionService
.
submit
(
new
Callable
<
R
>
(
)
{
@
Override
public
R
call
(
)
throws
Exception
{
LOG
.
info
(
+
testContext
.
toString
(
)
)
;
R
response
=
null
;
try
{
response
=
func
.
invoke
(
testContext
)
;
LOG
.
info
(
+
testContext
.
toString
(
)
)
;
}
catch
(
Throwable
ex
)
{
LOG
.
error
(
+
testContext
)
;
response
=
null
;
}
return
response
;
}
}
)
;
}
ArrayList
<
R
>
responseList
=
new
ArrayList
<
R
>
(
)
;
response
=
func
.
invoke
(
testContext
)
;
LOG
.
info
(
+
testContext
.
toString
(
)
)
;
}
catch
(
Throwable
ex
)
{
LOG
.
error
(
+
testContext
)
;
response
=
null
;
}
return
response
;
}
}
)
;
}
ArrayList
<
R
>
responseList
=
new
ArrayList
<
R
>
(
)
;
LOG
.
info
(
+
testContexts
.
size
(
)
)
;
for
(
int
i
=
0
;
i
<
testContexts
.
size
(
)
;
++
i
)
{
try
{
final
Future
<
R
>
future
=
completionService
.
take
(
)
;
final
R
response
=
future
.
get
(
3000
,
TimeUnit
.
MILLISECONDS
)
;
responseList
.
add
(
response
)
;
}
catch
(
Throwable
e
)
{
protected
<
T
>
List
<
RegisterApplicationMasterResponseInfo
<
T
>>
registerApplicationMastersInParallel
(
final
ArrayList
<
T
>
testContexts
)
{
List
<
RegisterApplicationMasterResponseInfo
<
T
>>
responses
=
runInParallel
(
testContexts
,
new
Function
<
T
,
RegisterApplicationMasterResponseInfo
<
T
>>
(
)
{
@
Override
public
RegisterApplicationMasterResponseInfo
<
T
>
invoke
(
T
testContext
)
{
RegisterApplicationMasterResponseInfo
<
T
>
response
=
null
;
try
{
int
index
=
testContexts
.
indexOf
(
testContext
)
;
response
=
new
RegisterApplicationMasterResponseInfo
<
T
>
(
registerApplicationMaster
(
index
)
,
testContext
)
;
Assert
.
assertNotNull
(
response
.
getResponse
(
)
)
;
Assert
.
assertEquals
(
Integer
.
toString
(
index
)
,
response
.
getResponse
(
)
.
getQueue
(
)
)
;
protected
<
T
>
List
<
FinishApplicationMasterResponseInfo
<
T
>>
finishApplicationMastersInParallel
(
final
ArrayList
<
T
>
testContexts
)
{
List
<
FinishApplicationMasterResponseInfo
<
T
>>
responses
=
runInParallel
(
testContexts
,
new
Function
<
T
,
FinishApplicationMasterResponseInfo
<
T
>>
(
)
{
@
Override
public
FinishApplicationMasterResponseInfo
<
T
>
invoke
(
T
testContext
)
{
FinishApplicationMasterResponseInfo
<
T
>
response
=
null
;
try
{
response
=
new
FinishApplicationMasterResponseInfo
<
T
>
(
finishApplicationMaster
(
testContexts
.
indexOf
(
testContext
)
,
FinalApplicationStatus
.
SUCCEEDED
)
,
testContext
)
;
Assert
.
assertNotNull
(
response
.
getResponse
(
)
)
;
private
ArrayList
<
String
>
CreateTestRequestIdentifiers
(
int
numberOfRequests
)
{
ArrayList
<
String
>
testContexts
=
new
ArrayList
<
String
>
(
)
;
private
ArrayList
<
String
>
CreateTestRequestIdentifiers
(
int
numberOfRequests
)
{
ArrayList
<
String
>
testContexts
=
new
ArrayList
<
String
>
(
)
;
LOG
.
info
(
+
numberOfRequests
+
)
;
for
(
int
ep
=
0
;
ep
<
numberOfRequests
;
ep
++
)
{
testContexts
.
add
(
+
Integer
.
toString
(
ep
)
)
;
@
Test
public
void
testFinishMulitpleApplicationMastersInParallel
(
)
throws
Exception
{
int
numberOfRequests
=
5
;
ArrayList
<
String
>
testContexts
=
new
ArrayList
<
String
>
(
)
;
@
Test
public
void
testFinishMulitpleApplicationMastersInParallel
(
)
throws
Exception
{
int
numberOfRequests
=
5
;
ArrayList
<
String
>
testContexts
=
new
ArrayList
<
String
>
(
)
;
LOG
.
info
(
+
numberOfRequests
+
)
;
for
(
int
i
=
0
;
i
<
numberOfRequests
;
i
++
)
{
testContexts
.
add
(
+
Integer
.
toString
(
i
)
)
;
@
Test
public
void
testAllocateAndReleaseContainersForMultipleAMInParallel
(
)
throws
Exception
{
int
numberOfApps
=
6
;
ArrayList
<
Integer
>
tempAppIds
=
new
ArrayList
<
Integer
>
(
)
;
for
(
int
i
=
0
;
i
<
numberOfApps
;
i
++
)
{
tempAppIds
.
add
(
new
Integer
(
i
)
)
;
}
final
ArrayList
<
Integer
>
appIds
=
tempAppIds
;
List
<
Integer
>
responses
=
runInParallel
(
appIds
,
new
Function
<
Integer
,
Integer
>
(
)
{
@
Override
public
Integer
invoke
(
Integer
testAppId
)
{
try
{
RegisterApplicationMasterResponse
registerResponse
=
registerApplicationMaster
(
testAppId
)
;
Assert
.
assertNotNull
(
,
registerResponse
)
;
List
<
Container
>
containers
=
getContainersAndAssert
(
testAppId
,
10
)
;
releaseContainersAndAssert
(
testAppId
,
containers
)
;
LOG
.
info
(
+
testAppId
)
;
}
catch
(
Throwable
ex
)
{
List
<
Container
>
containers
=
new
ArrayList
<
Container
>
(
numberOfResourceRequests
)
;
List
<
ResourceRequest
>
askList
=
new
ArrayList
<
ResourceRequest
>
(
numberOfResourceRequests
)
;
for
(
int
testAppId
=
0
;
testAppId
<
numberOfResourceRequests
;
testAppId
++
)
{
askList
.
add
(
createResourceRequest
(
+
Integer
.
toString
(
testAppId
)
,
6000
,
2
,
testAppId
%
5
,
1
)
)
;
}
allocateRequest
.
setAskList
(
askList
)
;
AllocateResponse
allocateResponse
=
allocate
(
appId
,
allocateRequest
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
Assert
.
assertNull
(
,
allocateResponse
.
getAMRMToken
(
)
)
;
containers
.
addAll
(
allocateResponse
.
getAllocatedContainers
(
)
)
;
int
numHeartbeat
=
0
;
while
(
containers
.
size
(
)
<
askList
.
size
(
)
&&
numHeartbeat
++
<
10
)
{
allocateResponse
=
allocate
(
appId
,
Records
.
newRecord
(
AllocateRequest
.
class
)
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
Assert
.
assertNull
(
,
allocateResponse
.
getAMRMToken
(
)
)
;
containers
.
addAll
(
allocateResponse
.
getAllocatedContainers
(
)
)
;
List
<
ResourceRequest
>
askList
=
new
ArrayList
<
ResourceRequest
>
(
numberOfResourceRequests
)
;
for
(
int
testAppId
=
0
;
testAppId
<
numberOfResourceRequests
;
testAppId
++
)
{
askList
.
add
(
createResourceRequest
(
+
Integer
.
toString
(
testAppId
)
,
6000
,
2
,
testAppId
%
5
,
1
)
)
;
}
allocateRequest
.
setAskList
(
askList
)
;
AllocateResponse
allocateResponse
=
allocate
(
appId
,
allocateRequest
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
Assert
.
assertNull
(
,
allocateResponse
.
getAMRMToken
(
)
)
;
containers
.
addAll
(
allocateResponse
.
getAllocatedContainers
(
)
)
;
int
numHeartbeat
=
0
;
while
(
containers
.
size
(
)
<
askList
.
size
(
)
&&
numHeartbeat
++
<
10
)
{
allocateResponse
=
allocate
(
appId
,
Records
.
newRecord
(
AllocateRequest
.
class
)
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
Assert
.
assertNull
(
,
allocateResponse
.
getAMRMToken
(
)
)
;
containers
.
addAll
(
allocateResponse
.
getAllocatedContainers
(
)
)
;
LOG
.
info
(
+
Integer
.
toString
(
allocateResponse
.
getAllocatedContainers
(
)
.
size
(
)
)
)
;
}
allocateRequest
.
setReleaseList
(
relList
)
;
AllocateResponse
allocateResponse
=
allocate
(
appId
,
allocateRequest
)
;
Assert
.
assertNotNull
(
allocateResponse
)
;
Assert
.
assertNull
(
,
allocateResponse
.
getAMRMToken
(
)
)
;
List
<
ContainerId
>
containersForReleasedContainerIds
=
new
ArrayList
<
>
(
)
;
List
<
ContainerId
>
newlyFinished
=
getCompletedContainerIds
(
allocateResponse
.
getCompletedContainersStatuses
(
)
)
;
containersForReleasedContainerIds
.
addAll
(
newlyFinished
)
;
int
numHeartbeat
=
0
;
while
(
containersForReleasedContainerIds
.
size
(
)
<
relList
.
size
(
)
&&
numHeartbeat
++
<
10
)
{
allocateResponse
=
allocate
(
appId
,
Records
.
newRecord
(
AllocateRequest
.
class
)
)
;
Assert
.
assertNotNull
(
allocateResponse
)
;
Assert
.
assertNull
(
,
allocateResponse
.
getAMRMToken
(
)
)
;
newlyFinished
=
getCompletedContainerIds
(
allocateResponse
.
getCompletedContainersStatuses
(
)
)
;
containersForReleasedContainerIds
.
addAll
(
newlyFinished
)
;
LOG
.
info
(
+
Integer
.
toString
(
allocateResponse
.
getAllocatedContainers
(
)
.
size
(
)
)
)
;
AllocateResponse
allocateResponse
=
interceptor
.
allocate
(
allocateRequest
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
checkAMRMToken
(
allocateResponse
.
getAMRMToken
(
)
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
containers
.
addAll
(
allocateResponse
.
getAllocatedContainers
(
)
)
;
LOG
.
info
(
+
Integer
.
toString
(
allocateResponse
.
getAllocatedContainers
(
)
.
size
(
)
)
)
;
int
numHeartbeat
=
0
;
while
(
containers
.
size
(
)
<
numberOfAllocationExcepted
&&
numHeartbeat
++
<
10
)
{
allocateRequest
=
Records
.
newRecord
(
AllocateRequest
.
class
)
;
allocateRequest
.
setResponseId
(
lastResponseId
)
;
allocateResponse
=
interceptor
.
allocate
(
allocateRequest
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
checkAMRMToken
(
allocateResponse
.
getAMRMToken
(
)
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
interceptor
.
drainAllAsyncQueue
(
false
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
checkAMRMToken
(
allocateResponse
.
getAMRMToken
(
)
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
containers
.
addAll
(
allocateResponse
.
getAllocatedContainers
(
)
)
;
LOG
.
info
(
+
Integer
.
toString
(
allocateResponse
.
getAllocatedContainers
(
)
.
size
(
)
)
)
;
int
numHeartbeat
=
0
;
while
(
containers
.
size
(
)
<
numberOfAllocationExcepted
&&
numHeartbeat
++
<
10
)
{
allocateRequest
=
Records
.
newRecord
(
AllocateRequest
.
class
)
;
allocateRequest
.
setResponseId
(
lastResponseId
)
;
allocateResponse
=
interceptor
.
allocate
(
allocateRequest
)
;
Assert
.
assertNotNull
(
,
allocateResponse
)
;
checkAMRMToken
(
allocateResponse
.
getAMRMToken
(
)
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
interceptor
.
drainAllAsyncQueue
(
false
)
;
containers
.
addAll
(
allocateResponse
.
getAllocatedContainers
(
)
)
;
private
void
releaseContainersAndAssert
(
List
<
Container
>
containers
)
throws
Exception
{
Assert
.
assertTrue
(
containers
.
size
(
)
>
0
)
;
AllocateRequest
allocateRequest
=
Records
.
newRecord
(
AllocateRequest
.
class
)
;
List
<
ContainerId
>
relList
=
new
ArrayList
<
ContainerId
>
(
containers
.
size
(
)
)
;
for
(
Container
container
:
containers
)
{
relList
.
add
(
container
.
getId
(
)
)
;
}
allocateRequest
.
setReleaseList
(
relList
)
;
allocateRequest
.
setResponseId
(
lastResponseId
)
;
AllocateResponse
allocateResponse
=
interceptor
.
allocate
(
allocateRequest
)
;
Assert
.
assertNotNull
(
allocateResponse
)
;
checkAMRMToken
(
allocateResponse
.
getAMRMToken
(
)
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
List
<
ContainerId
>
containersForReleasedContainerIds
=
new
ArrayList
<
ContainerId
>
(
)
;
List
<
ContainerId
>
newlyFinished
=
getCompletedContainerIds
(
allocateResponse
.
getCompletedContainersStatuses
(
)
)
;
containersForReleasedContainerIds
.
addAll
(
newlyFinished
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
List
<
ContainerId
>
containersForReleasedContainerIds
=
new
ArrayList
<
ContainerId
>
(
)
;
List
<
ContainerId
>
newlyFinished
=
getCompletedContainerIds
(
allocateResponse
.
getCompletedContainersStatuses
(
)
)
;
containersForReleasedContainerIds
.
addAll
(
newlyFinished
)
;
LOG
.
info
(
+
Integer
.
toString
(
newlyFinished
.
size
(
)
)
)
;
int
numHeartbeat
=
0
;
while
(
containersForReleasedContainerIds
.
size
(
)
<
relList
.
size
(
)
&&
numHeartbeat
++
<
10
)
{
allocateRequest
=
Records
.
newRecord
(
AllocateRequest
.
class
)
;
allocateRequest
.
setResponseId
(
lastResponseId
)
;
allocateResponse
=
interceptor
.
allocate
(
allocateRequest
)
;
Assert
.
assertNotNull
(
allocateResponse
)
;
checkAMRMToken
(
allocateResponse
.
getAMRMToken
(
)
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
interceptor
.
drainAllAsyncQueue
(
false
)
;
newlyFinished
=
getCompletedContainerIds
(
allocateResponse
.
getCompletedContainersStatuses
(
)
)
;
List
<
ContainerId
>
containersForReleasedContainerIds
=
new
ArrayList
<
ContainerId
>
(
)
;
List
<
ContainerId
>
newlyFinished
=
getCompletedContainerIds
(
allocateResponse
.
getCompletedContainersStatuses
(
)
)
;
containersForReleasedContainerIds
.
addAll
(
newlyFinished
)
;
LOG
.
info
(
+
Integer
.
toString
(
newlyFinished
.
size
(
)
)
)
;
int
numHeartbeat
=
0
;
while
(
containersForReleasedContainerIds
.
size
(
)
<
relList
.
size
(
)
&&
numHeartbeat
++
<
10
)
{
allocateRequest
=
Records
.
newRecord
(
AllocateRequest
.
class
)
;
allocateRequest
.
setResponseId
(
lastResponseId
)
;
allocateResponse
=
interceptor
.
allocate
(
allocateRequest
)
;
Assert
.
assertNotNull
(
allocateResponse
)
;
checkAMRMToken
(
allocateResponse
.
getAMRMToken
(
)
)
;
lastResponseId
=
allocateResponse
.
getResponseId
(
)
;
interceptor
.
drainAllAsyncQueue
(
false
)
;
newlyFinished
=
getCompletedContainerIds
(
allocateResponse
.
getCompletedContainersStatuses
(
)
)
;
containersForReleasedContainerIds
.
addAll
(
newlyFinished
)
;
registerReq
.
setHost
(
Integer
.
toString
(
testAppId
)
)
;
registerReq
.
setRpcPort
(
testAppId
)
;
registerReq
.
setTrackingUrl
(
)
;
UserGroupInformation
ugi
=
interceptor
.
getUGIWithToken
(
interceptor
.
getAttemptId
(
)
)
;
ugi
.
doAs
(
new
PrivilegedExceptionAction
<
Object
>
(
)
{
@
Override
public
Object
run
(
)
throws
Exception
{
RegisterApplicationMasterResponse
registerResponse
=
interceptor
.
registerApplicationMaster
(
registerReq
)
;
Assert
.
assertNotNull
(
registerResponse
)
;
lastResponseId
=
0
;
Assert
.
assertEquals
(
0
,
interceptor
.
getUnmanagedAMPoolSize
(
)
)
;
registerSubCluster
(
SubClusterId
.
newInstance
(
)
)
;
registerSubCluster
(
SubClusterId
.
newInstance
(
HOME_SC_ID
)
)
;
int
numberOfContainers
=
3
;
List
<
Container
>
containers
=
getContainersAndAssert
(
numberOfContainers
,
numberOfContainers
*
2
)
;
for
(
Container
c
:
containers
)
{
@
Before
public
void
setup
(
)
throws
IOException
{
localFS
.
delete
(
new
Path
(
localDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
tmpDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
localLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
remoteLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localDir
.
mkdir
(
)
;
tmpDir
.
mkdir
(
)
;
localLogDir
.
mkdir
(
)
;
remoteLogDir
.
mkdir
(
)
;
@
Before
public
void
setup
(
)
throws
IOException
{
localFS
.
delete
(
new
Path
(
localDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
tmpDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
localLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
remoteLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localDir
.
mkdir
(
)
;
tmpDir
.
mkdir
(
)
;
localLogDir
.
mkdir
(
)
;
remoteLogDir
.
mkdir
(
)
;
LOG
.
info
(
+
localDir
.
getAbsolutePath
(
)
)
;
protected
DeletionService
createDeletionService
(
)
{
return
new
DeletionService
(
exec
)
{
@
Override
public
void
delete
(
DeletionTask
deletionTask
)
{
public
static
void
waitForContainerState
(
ContainerManagementProtocol
containerManager
,
ContainerId
containerID
,
List
<
ContainerState
>
finalStates
,
int
timeOutMax
)
throws
InterruptedException
,
YarnException
,
IOException
{
List
<
ContainerId
>
list
=
new
ArrayList
<
ContainerId
>
(
)
;
list
.
add
(
containerID
)
;
GetContainerStatusesRequest
request
=
GetContainerStatusesRequest
.
newInstance
(
list
)
;
ContainerStatus
containerStatus
=
null
;
HashSet
<
ContainerState
>
fStates
=
new
HashSet
<
>
(
finalStates
)
;
int
timeoutSecs
=
0
;
do
{
Thread
.
sleep
(
1000
)
;
containerStatus
=
containerManager
.
getContainerStatuses
(
request
)
.
getContainerStatuses
(
)
.
get
(
0
)
;
public
static
void
waitForApplicationState
(
ContainerManagerImpl
containerManager
,
ApplicationId
appID
,
ApplicationState
finalState
)
throws
InterruptedException
{
Application
app
=
containerManager
.
getContext
(
)
.
getApplications
(
)
.
get
(
appID
)
;
int
timeout
=
0
;
while
(
!
(
app
.
getApplicationState
(
)
.
equals
(
finalState
)
)
&&
timeout
++
<
15
)
{
@
Override
@
Before
public
void
setup
(
)
throws
IOException
{
localFS
.
delete
(
new
Path
(
localDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
tmpDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
localLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
remoteLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localDir
.
mkdir
(
)
;
tmpDir
.
mkdir
(
)
;
localLogDir
.
mkdir
(
)
;
remoteLogDir
.
mkdir
(
)
;
@
Override
@
Before
public
void
setup
(
)
throws
IOException
{
localFS
.
delete
(
new
Path
(
localDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
tmpDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
localLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localFS
.
delete
(
new
Path
(
remoteLogDir
.
getAbsolutePath
(
)
)
,
true
)
;
localDir
.
mkdir
(
)
;
tmpDir
.
mkdir
(
)
;
localLogDir
.
mkdir
(
)
;
remoteLogDir
.
mkdir
(
)
;
LOG
.
info
(
+
localDir
.
getAbsolutePath
(
)
)
;
StartContainersRequest
allRequests
=
StartContainersRequest
.
newInstance
(
list
)
;
containerManager
.
startContainers
(
allRequests
)
;
int
timeoutSecs
=
0
;
while
(
!
processStartFile
.
exists
(
)
&&
timeoutSecs
++
<
20
)
{
Thread
.
sleep
(
1000
)
;
LOG
.
info
(
)
;
}
Assert
.
assertTrue
(
,
processStartFile
.
exists
(
)
)
;
BufferedReader
reader
=
new
BufferedReader
(
new
FileReader
(
processStartFile
)
)
;
String
pid
=
reader
.
readLine
(
)
.
trim
(
)
;
Assert
.
assertEquals
(
null
,
reader
.
readLine
(
)
)
;
reader
.
close
(
)
;
reader
=
new
BufferedReader
(
new
FileReader
(
childProcessStartFile
)
)
;
String
child
=
reader
.
readLine
(
)
.
trim
(
)
;
Assert
.
assertEquals
(
null
,
reader
.
readLine
(
)
)
;
reader
.
close
(
)
;
List
<
PrivilegedOperation
>
ops
=
new
ArrayList
<
>
(
)
;
ops
.
add
(
opTasksNone
)
;
ops
.
add
(
opDisallowed
)
;
try
{
PrivilegedOperationExecutor
.
squashCGroupOperations
(
ops
)
;
Assert
.
fail
(
)
;
}
catch
(
PrivilegedOperationException
e
)
{
LOG
.
info
(
+
e
)
;
}
ops
.
clear
(
)
;
ops
.
add
(
opTasksNone
)
;
ops
.
add
(
opTasksInvalid
)
;
try
{
PrivilegedOperationExecutor
.
squashCGroupOperations
(
ops
)
;
Assert
.
fail
(
)
;
}
catch
(
PrivilegedOperationException
e
)
{
File
emptyMtab
=
createEmptyCgroups
(
)
;
try
{
CGroupsHandler
cGroupsHandler
=
new
CGroupsHandlerImpl
(
createMountConfiguration
(
)
,
privilegedOperationExecutorMock
,
emptyMtab
.
getAbsolutePath
(
)
)
;
PrivilegedOperation
expectedOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
MOUNT_CGROUPS
)
;
String
controllerKV
=
controller
.
getName
(
)
+
+
tmpPath
+
Path
.
SEPARATOR
+
controller
.
getName
(
)
;
expectedOp
.
appendArgs
(
hierarchy
,
controllerKV
)
;
cGroupsHandler
.
initializeCGroupController
(
controller
)
;
try
{
ArgumentCaptor
<
PrivilegedOperation
>
opCaptor
=
ArgumentCaptor
.
forClass
(
PrivilegedOperation
.
class
)
;
verify
(
privilegedOperationExecutorMock
)
.
executePrivilegedOperation
(
opCaptor
.
capture
(
)
,
eq
(
false
)
)
;
Assert
.
assertEquals
(
expectedOp
,
opCaptor
.
getValue
(
)
)
;
verifyNoMoreInteractions
(
privilegedOperationExecutorMock
)
;
cGroupsHandler
.
initializeCGroupController
(
controller
)
;
verifyNoMoreInteractions
(
privilegedOperationExecutorMock
)
;
}
catch
(
PrivilegedOperationException
e
)
{
PrivilegedOperation
expectedOp
=
new
PrivilegedOperation
(
PrivilegedOperation
.
OperationType
.
MOUNT_CGROUPS
)
;
String
controllerKV
=
controller
.
getName
(
)
+
+
tmpPath
+
Path
.
SEPARATOR
+
controller
.
getName
(
)
;
expectedOp
.
appendArgs
(
hierarchy
,
controllerKV
)
;
cGroupsHandler
.
initializeCGroupController
(
controller
)
;
try
{
ArgumentCaptor
<
PrivilegedOperation
>
opCaptor
=
ArgumentCaptor
.
forClass
(
PrivilegedOperation
.
class
)
;
verify
(
privilegedOperationExecutorMock
)
.
executePrivilegedOperation
(
opCaptor
.
capture
(
)
,
eq
(
false
)
)
;
Assert
.
assertEquals
(
expectedOp
,
opCaptor
.
getValue
(
)
)
;
verifyNoMoreInteractions
(
privilegedOperationExecutorMock
)
;
cGroupsHandler
.
initializeCGroupController
(
controller
)
;
verifyNoMoreInteractions
(
privilegedOperationExecutorMock
)
;
}
catch
(
PrivilegedOperationException
e
)
{
LOG
.
error
(
+
e
)
;
assertTrue
(
,
false
)
;
}
}
catch
(
ResourceHandlerException
e
)
{
try
{
String
path
=
cGroupsHandler
.
createCGroup
(
controller
,
testCGroup
)
;
assertTrue
(
new
File
(
expectedPath
)
.
exists
(
)
)
;
Assert
.
assertEquals
(
expectedPath
,
path
)
;
String
param
=
;
String
paramValue
=
;
cGroupsHandler
.
updateCGroupParam
(
controller
,
testCGroup
,
param
,
paramValue
)
;
String
paramPath
=
expectedPath
+
Path
.
SEPARATOR
+
controller
.
getName
(
)
+
+
param
;
File
paramFile
=
new
File
(
paramPath
)
;
assertTrue
(
paramFile
.
exists
(
)
)
;
try
{
Assert
.
assertEquals
(
paramValue
,
new
String
(
Files
.
readAllBytes
(
paramFile
.
toPath
(
)
)
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
e
)
;
Assert
.
fail
(
)
;
conf
.
setBoolean
(
YarnConfiguration
.
NM_RECOVERY_ENABLED
,
true
)
;
TrafficController
trafficController
=
new
TrafficController
(
conf
,
privilegedOperationExecutorMock
)
;
try
{
when
(
privilegedOperationExecutorMock
.
executePrivilegedOperation
(
any
(
PrivilegedOperation
.
class
)
,
eq
(
true
)
)
)
.
thenReturn
(
DEFAULT_TC_STATE_EXAMPLE
)
;
trafficController
.
bootstrap
(
DEVICE
,
ROOT_BANDWIDTH_MBIT
,
YARN_BANDWIDTH_MBIT
)
;
ArgumentCaptor
<
PrivilegedOperation
>
readOpCaptor
=
ArgumentCaptor
.
forClass
(
PrivilegedOperation
.
class
)
;
verify
(
privilegedOperationExecutorMock
,
times
(
1
)
)
.
executePrivilegedOperation
(
readOpCaptor
.
capture
(
)
,
eq
(
true
)
)
;
List
<
PrivilegedOperation
>
readOps
=
readOpCaptor
.
getAllValues
(
)
;
verifyTrafficControlOperation
(
readOps
.
get
(
0
)
,
PrivilegedOperation
.
OperationType
.
TC_READ_STATE
,
Arrays
.
asList
(
READ_QDISC_CMD
,
READ_FILTER_CMD
,
READ_CLASS_CMD
)
)
;
ArgumentCaptor
<
PrivilegedOperation
>
writeOpCaptor
=
ArgumentCaptor
.
forClass
(
PrivilegedOperation
.
class
)
;
verify
(
privilegedOperationExecutorMock
,
times
(
2
)
)
.
executePrivilegedOperation
(
writeOpCaptor
.
capture
(
)
,
eq
(
false
)
)
;
List
<
PrivilegedOperation
>
writeOps
=
writeOpCaptor
.
getAllValues
(
)
;
verifyTrafficControlOperation
(
writeOps
.
get
(
0
)
,
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
,
Arrays
.
asList
(
WIPE_STATE_CMD
)
)
;
verifyTrafficControlOperation
(
writeOps
.
get
(
1
)
,
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
,
Arrays
.
asList
(
ADD_ROOT_QDISC_CMD
,
ADD_CGROUP_FILTER_CMD
,
ADD_ROOT_CLASS_CMD
,
ADD_DEFAULT_CLASS_CMD
,
ADD_YARN_CLASS_CMD
)
)
;
}
catch
(
ResourceHandlerException
|
PrivilegedOperationException
|
IOException
e
)
{
Assert
.
assertTrue
(
classId
>=
MIN_CONTAINER_CLASS_ID
)
;
Assert
.
assertEquals
(
String
.
format
(
FORMAT_CONTAINER_CLASS_STR
,
classId
)
,
trafficController
.
getStringForNetClsClassId
(
classId
)
)
;
TrafficController
.
BatchBuilder
builder
=
trafficController
.
new
BatchBuilder
(
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
)
.
addContainerClass
(
classId
,
CONTAINER_BANDWIDTH_MBIT
,
false
)
;
PrivilegedOperation
addClassOp
=
builder
.
commitBatchToTempFile
(
)
;
String
expectedAddClassCmd
=
String
.
format
(
FORMAT_ADD_CONTAINER_CLASS_TO_DEVICE
,
classId
,
YARN_BANDWIDTH_MBIT
)
;
verifyTrafficControlOperation
(
addClassOp
,
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
,
Arrays
.
asList
(
expectedAddClassCmd
)
)
;
TrafficController
.
BatchBuilder
strictModeBuilder
=
trafficController
.
new
BatchBuilder
(
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
)
.
addContainerClass
(
classId
,
CONTAINER_BANDWIDTH_MBIT
,
true
)
;
PrivilegedOperation
addClassStrictModeOp
=
strictModeBuilder
.
commitBatchToTempFile
(
)
;
String
expectedAddClassStrictModeCmd
=
String
.
format
(
FORMAT_ADD_CONTAINER_CLASS_TO_DEVICE
,
classId
,
CONTAINER_BANDWIDTH_MBIT
)
;
verifyTrafficControlOperation
(
addClassStrictModeOp
,
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
,
Arrays
.
asList
(
expectedAddClassStrictModeCmd
)
)
;
TrafficController
.
BatchBuilder
deleteBuilder
=
trafficController
.
new
BatchBuilder
(
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
)
.
deleteContainerClass
(
classId
)
;
PrivilegedOperation
deleteClassOp
=
deleteBuilder
.
commitBatchToTempFile
(
)
;
String
expectedDeleteClassCmd
=
String
.
format
(
FORAMT_DELETE_CONTAINER_CLASS_FROM_DEVICE
,
classId
)
;
verifyTrafficControlOperation
(
deleteClassOp
,
PrivilegedOperation
.
OperationType
.
TC_MODIFY_STATE
,
Arrays
.
asList
(
expectedDeleteClassCmd
)
)
;
}
catch
(
ResourceHandlerException
|
IOException
e
)
{
when
(
mockApplicationId
.
toString
(
)
)
.
thenReturn
(
)
;
when
(
appAttemptId
.
getApplicationId
(
)
)
.
thenReturn
(
mockApplicationId
)
;
when
(
cId
.
getApplicationAttemptId
(
)
)
.
thenReturn
(
appAttemptId
)
;
when
(
container
.
getLaunchContext
(
)
)
.
thenReturn
(
context
)
;
when
(
context
.
getEnvironment
(
)
)
.
thenReturn
(
env
)
;
when
(
container
.
getUser
(
)
)
.
thenReturn
(
submittingUser
)
;
String
uid
=
;
String
gid
=
;
Shell
.
ShellCommandExecutor
shexec1
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
,
,
runAsUser
}
)
;
Shell
.
ShellCommandExecutor
shexec2
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
,
,
runAsUser
}
)
;
Shell
.
ShellCommandExecutor
shexec3
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
,
,
runAsUser
}
)
;
try
{
shexec1
.
execute
(
)
;
uid
=
shexec1
.
getOutput
(
)
.
replaceAll
(
,
)
;
}
catch
(
Exception
e
)
{
when
(
container
.
getUser
(
)
)
.
thenReturn
(
submittingUser
)
;
String
uid
=
;
String
gid
=
;
Shell
.
ShellCommandExecutor
shexec1
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
,
,
runAsUser
}
)
;
Shell
.
ShellCommandExecutor
shexec2
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
,
,
runAsUser
}
)
;
Shell
.
ShellCommandExecutor
shexec3
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
,
,
runAsUser
}
)
;
try
{
shexec1
.
execute
(
)
;
uid
=
shexec1
.
getOutput
(
)
.
replaceAll
(
,
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
shexec2
.
execute
(
)
;
gid
=
shexec2
.
getOutput
(
)
.
replaceAll
(
,
)
;
}
catch
(
Exception
e
)
{
Shell
.
ShellCommandExecutor
shexec3
=
new
Shell
.
ShellCommandExecutor
(
new
String
[
]
{
,
,
runAsUser
}
)
;
try
{
shexec1
.
execute
(
)
;
uid
=
shexec1
.
getOutput
(
)
.
replaceAll
(
,
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
shexec2
.
execute
(
)
;
gid
=
shexec2
.
getOutput
(
)
.
replaceAll
(
,
)
;
}
catch
(
Exception
e
)
{
LOG
.
info
(
+
e
)
;
}
try
{
shexec3
.
execute
(
)
;
groups
=
shexec3
.
getOutput
(
)
.
replace
(
,
)
.
split
(
)
;
}
catch
(
Exception
e
)
{
ConcurrentMap
<
ContainerId
,
Container
>
containerMap
=
mock
(
ConcurrentMap
.
class
)
;
when
(
mockNMContext
.
getLocalDirsHandler
(
)
)
.
thenReturn
(
localDirsHandler
)
;
when
(
mockNMContext
.
getResourcePluginManager
(
)
)
.
thenReturn
(
resourcePluginManager
)
;
when
(
mockNMContext
.
getContainers
(
)
)
.
thenReturn
(
containerMap
)
;
when
(
containerMap
.
get
(
any
(
)
)
)
.
thenReturn
(
container
)
;
ContainerManager
mockContainerManager
=
mock
(
ContainerManager
.
class
)
;
ResourceLocalizationService
mockLocalzationService
=
mock
(
ResourceLocalizationService
.
class
)
;
LocalizedResource
mockLocalizedResource
=
mock
(
LocalizedResource
.
class
)
;
when
(
mockLocalizedResource
.
getLocalPath
(
)
)
.
thenReturn
(
new
Path
(
)
)
;
when
(
mockLocalzationService
.
getLocalizedResource
(
any
(
)
,
anyString
(
)
,
any
(
)
)
)
.
thenReturn
(
mockLocalizedResource
)
;
when
(
mockContainerManager
.
getResourceLocalizationService
(
)
)
.
thenReturn
(
mockLocalzationService
)
;
when
(
mockNMContext
.
getContainerManager
(
)
)
.
thenReturn
(
mockContainerManager
)
;
try
{
when
(
localDirsHandler
.
getLocalPathForWrite
(
anyString
(
)
)
)
.
thenReturn
(
new
Path
(
tmpPath
)
)
;
}
catch
(
IOException
ioe
)
{
Assert
.
assertEquals
(
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
+
String
.
join
(
,
groups
)
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
+
+
+
+
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
+
uidGidPair
,
dockerCommands
.
get
(
counter
++
)
)
;
Assert
.
assertEquals
(
,
dockerCommands
.
get
(
counter
)
)
;
env
.
put
(
DockerLinuxContainerRuntime
.
ENV_DOCKER_CONTAINER_NETWORK
,
customNetwork3
)
;
try
{
runtime
.
launchContainer
(
builder
.
build
(
)
)
;
Assert
.
fail
(
+
customNetwork3
+
)
;
}
catch
(
ContainerExecutionException
e
)
{
ConcurrentMap
<
ContainerId
,
Container
>
containerMap
=
mock
(
ConcurrentMap
.
class
)
;
when
(
mockNMContext
.
getLocalDirsHandler
(
)
)
.
thenReturn
(
localDirsHandler
)
;
when
(
mockNMContext
.
getResourcePluginManager
(
)
)
.
thenReturn
(
resourcePluginManager
)
;
when
(
mockNMContext
.
getContainers
(
)
)
.
thenReturn
(
containerMap
)
;
when
(
containerMap
.
get
(
any
(
)
)
)
.
thenReturn
(
container
)
;
ContainerManager
mockContainerManager
=
mock
(
ContainerManager
.
class
)
;
ResourceLocalizationService
mockLocalzationService
=
mock
(
ResourceLocalizationService
.
class
)
;
LocalizedResource
mockLocalizedResource
=
mock
(
LocalizedResource
.
class
)
;
when
(
mockLocalizedResource
.
getLocalPath
(
)
)
.
thenReturn
(
new
Path
(
)
)
;
when
(
mockLocalzationService
.
getLocalizedResource
(
any
(
)
,
anyString
(
)
,
any
(
)
)
)
.
thenReturn
(
mockLocalizedResource
)
;
when
(
mockContainerManager
.
getResourceLocalizationService
(
)
)
.
thenReturn
(
mockLocalzationService
)
;
when
(
mockNMContext
.
getContainerManager
(
)
)
.
thenReturn
(
mockContainerManager
)
;
try
{
when
(
localDirsHandler
.
getLocalPathForWrite
(
anyString
(
)
)
)
.
thenReturn
(
new
Path
(
tmpPath
)
)
;
}
catch
(
IOException
ioe
)
{
private
void
writeContainerLogs
(
File
appLogDir
,
ContainerId
containerId
,
String
[
]
fileName
,
String
[
]
emptyFiles
)
throws
IOException
{
String
containerStr
=
containerId
.
toString
(
)
;
File
containerLogDir
=
new
File
(
appLogDir
,
containerStr
)
;
boolean
created
=
containerLogDir
.
mkdirs
(
)
;
LogKey
key
=
new
LogKey
(
)
;
valueStream
=
reader
.
next
(
key
)
;
while
(
valueStream
!=
null
)
{
LOG
.
info
(
+
key
.
toString
(
)
)
;
Map
<
String
,
String
>
perContainerMap
=
new
HashMap
<
String
,
String
>
(
)
;
logMap
.
put
(
key
.
toString
(
)
,
perContainerMap
)
;
while
(
true
)
{
try
{
ByteArrayOutputStream
baos
=
new
ByteArrayOutputStream
(
)
;
PrintStream
ps
=
new
PrintStream
(
baos
)
;
LogReader
.
readAContainerLogsForALogType
(
valueStream
,
ps
)
;
String
writtenLines
[
]
=
baos
.
toString
(
)
.
split
(
System
.
getProperty
(
)
)
;
Assert
.
assertEquals
(
,
writtenLines
[
0
]
.
substring
(
0
,
8
)
)
;
String
fileType
=
writtenLines
[
0
]
.
substring
(
8
)
;
fileTypes
.
add
(
fileType
)
;
private
int
numOfLogsAvailable
(
LogAggregationService
logAggregationService
,
ApplicationId
appId
,
boolean
sizeLimited
,
String
lastLogFile
)
throws
IOException
{
Path
appLogDir
=
logAggregationService
.
getLogAggregationFileController
(
conf
)
.
getRemoteAppLogDir
(
appId
,
this
.
user
)
;
RemoteIterator
<
FileStatus
>
nodeFiles
=
null
;
try
{
Path
qualifiedLogDir
=
FileContext
.
getFileContext
(
this
.
conf
)
.
makeQualified
(
appLogDir
)
;
nodeFiles
=
FileContext
.
getFileContext
(
qualifiedLogDir
.
toUri
(
)
,
this
.
conf
)
.
listStatus
(
appLogDir
)
;
}
catch
(
FileNotFoundException
fnf
)
{
LOG
.
info
(
+
fnf
)
;
return
-
1
;
}
int
count
=
0
;
while
(
nodeFiles
.
hasNext
(
)
)
{
FileStatus
status
=
nodeFiles
.
next
(
)
;
String
filename
=
status
.
getPath
(
)
.
getName
(
)
;
if
(
filename
.
contains
(
LogAggregationUtils
.
TMP_FILE_SUFFIX
)
||
(
lastLogFile
!=
null
&&
filename
.
contains
(
lastLogFile
)
&&
sizeLimited
)
)
{
LOG
.
info
(
+
filename
)
;
try
{
Path
qualifiedLogDir
=
FileContext
.
getFileContext
(
this
.
conf
)
.
makeQualified
(
appLogDir
)
;
nodeFiles
=
FileContext
.
getFileContext
(
qualifiedLogDir
.
toUri
(
)
,
this
.
conf
)
.
listStatus
(
appLogDir
)
;
}
catch
(
FileNotFoundException
fnf
)
{
LOG
.
info
(
+
fnf
)
;
return
-
1
;
}
int
count
=
0
;
while
(
nodeFiles
.
hasNext
(
)
)
{
FileStatus
status
=
nodeFiles
.
next
(
)
;
String
filename
=
status
.
getPath
(
)
.
getName
(
)
;
if
(
filename
.
contains
(
LogAggregationUtils
.
TMP_FILE_SUFFIX
)
||
(
lastLogFile
!=
null
&&
filename
.
contains
(
lastLogFile
)
&&
sizeLimited
)
)
{
LOG
.
info
(
+
filename
)
;
LOG
.
info
(
+
lastLogFile
)
;
return
-
1
;
}
if
(
filename
.
contains
(
LogAggregationUtils
.
getNodeString
(
logAggregationService
.
getNodeId
(
)
)
)
)
{
totalBoostAgainstMedian
+=
bstAgainstMedian
;
totalBoostAgainstMin
+=
bstAgainstMinimum
;
count
++
;
if
(
maxBoostAgainstMedian
<
bstAgainstMedian
)
{
maxBoostAgainstMedian
=
bstAgainstMedian
;
}
if
(
maxBoostAgainstMin
<
bstAgainstMinimum
)
{
maxBoostAgainstMin
=
bstAgainstMinimum
;
}
totalBoostAgainstMinCertainModel
+=
bstAgainstMinimum
;
totalBoostAgainstMedianCertainModel
+=
bstAgainstMedian
;
if
(
maxBoostAgainstMinCertainModel
<
bstAgainstMinimum
)
{
maxBoostAgainstMinCertainModel
=
bstAgainstMinimum
;
}
if
(
maxBoostAgainstMedianCertainModel
<
bstAgainstMedian
)
{
maxBoostAgainstMedianCertainModel
=
bstAgainstMedian
;
}
countOfEachModel
++
;
}
}
LOG
.
info
(
+
,
model
,
maxBoostAgainstMedianCertainModel
)
;
totalBoostAgainstMin
+=
bstAgainstMinimum
;
count
++
;
if
(
maxBoostAgainstMedian
<
bstAgainstMedian
)
{
maxBoostAgainstMedian
=
bstAgainstMedian
;
}
if
(
maxBoostAgainstMin
<
bstAgainstMinimum
)
{
maxBoostAgainstMin
=
bstAgainstMinimum
;
}
totalBoostAgainstMinCertainModel
+=
bstAgainstMinimum
;
totalBoostAgainstMedianCertainModel
+=
bstAgainstMedian
;
if
(
maxBoostAgainstMinCertainModel
<
bstAgainstMinimum
)
{
maxBoostAgainstMinCertainModel
=
bstAgainstMinimum
;
}
if
(
maxBoostAgainstMedianCertainModel
<
bstAgainstMedian
)
{
maxBoostAgainstMedianCertainModel
=
bstAgainstMedian
;
}
countOfEachModel
++
;
}
}
LOG
.
info
(
+
,
model
,
maxBoostAgainstMedianCertainModel
)
;
LOG
.
info
(
+
,
model
,
totalBoostAgainstMedianCertainModel
/
countOfEachModel
)
;
count
++
;
if
(
maxBoostAgainstMedian
<
bstAgainstMedian
)
{
maxBoostAgainstMedian
=
bstAgainstMedian
;
}
if
(
maxBoostAgainstMin
<
bstAgainstMinimum
)
{
maxBoostAgainstMin
=
bstAgainstMinimum
;
}
totalBoostAgainstMinCertainModel
+=
bstAgainstMinimum
;
totalBoostAgainstMedianCertainModel
+=
bstAgainstMedian
;
if
(
maxBoostAgainstMinCertainModel
<
bstAgainstMinimum
)
{
maxBoostAgainstMinCertainModel
=
bstAgainstMinimum
;
}
if
(
maxBoostAgainstMedianCertainModel
<
bstAgainstMedian
)
{
maxBoostAgainstMedianCertainModel
=
bstAgainstMedian
;
}
countOfEachModel
++
;
}
}
LOG
.
info
(
+
,
model
,
maxBoostAgainstMedianCertainModel
)
;
LOG
.
info
(
+
,
model
,
totalBoostAgainstMedianCertainModel
/
countOfEachModel
)
;
LOG
.
info
(
,
model
,
maxBoostAgainstMinCertainModel
)
;
}
if
(
maxBoostAgainstMin
<
bstAgainstMinimum
)
{
maxBoostAgainstMin
=
bstAgainstMinimum
;
}
totalBoostAgainstMinCertainModel
+=
bstAgainstMinimum
;
totalBoostAgainstMedianCertainModel
+=
bstAgainstMedian
;
if
(
maxBoostAgainstMinCertainModel
<
bstAgainstMinimum
)
{
maxBoostAgainstMinCertainModel
=
bstAgainstMinimum
;
}
if
(
maxBoostAgainstMedianCertainModel
<
bstAgainstMedian
)
{
maxBoostAgainstMedianCertainModel
=
bstAgainstMedian
;
}
countOfEachModel
++
;
}
}
LOG
.
info
(
+
,
model
,
maxBoostAgainstMedianCertainModel
)
;
LOG
.
info
(
+
,
model
,
totalBoostAgainstMedianCertainModel
/
countOfEachModel
)
;
LOG
.
info
(
,
model
,
maxBoostAgainstMinCertainModel
)
;
LOG
.
info
(
+
,
model
,
totalBoostAgainstMinCertainModel
/
countOfEachModel
)
;
}
LOG
.
info
(
+
maxBoostAgainstMedian
)
;
LOG
.
info
(
+
+
totalBoostAgainstMedian
/
count
)
;
@
Override
public
void
onWebSocketText
(
String
message
)
{
@
Test
public
void
testWebServerWithServlet
(
)
{
int
port
=
startNMWebAppServer
(
)
;
LOG
.
info
(
+
port
)
;
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
)
.
append
(
port
)
.
append
(
)
;
String
dest
=
sb
.
toString
(
)
;
WebSocketClient
client
=
new
WebSocketClient
(
)
;
try
{
ContainerShellClientSocketTest
socket
=
new
ContainerShellClientSocketTest
(
)
;
client
.
start
(
)
;
URI
echoUri
=
new
URI
(
dest
)
;
Future
<
Session
>
future
=
client
.
connect
(
socket
,
echoUri
)
;
Session
session
=
future
.
get
(
)
;
session
.
getRemote
(
)
.
sendString
(
)
;
session
.
close
(
)
;
client
.
stop
(
)
;
}
catch
(
Throwable
t
)
{
ContainerShellClientSocketTest
socket
=
new
ContainerShellClientSocketTest
(
)
;
client
.
start
(
)
;
URI
echoUri
=
new
URI
(
dest
)
;
Future
<
Session
>
future
=
client
.
connect
(
socket
,
echoUri
)
;
Session
session
=
future
.
get
(
)
;
session
.
getRemote
(
)
.
sendString
(
)
;
session
.
close
(
)
;
client
.
stop
(
)
;
}
catch
(
Throwable
t
)
{
LOG
.
error
(
,
t
)
;
}
finally
{
try
{
client
.
stop
(
)
;
server
.
close
(
)
;
}
catch
(
Exception
e
)
{
@
SuppressWarnings
(
)
@
Override
public
UpdateNodeResourceResponse
updateNodeResource
(
UpdateNodeResourceRequest
request
)
throws
YarnException
,
IOException
{
final
String
operation
=
;
UserGroupInformation
user
=
checkAcls
(
operation
)
;
checkRMStatus
(
user
.
getShortUserName
(
)
,
operation
,
)
;
Map
<
NodeId
,
ResourceOption
>
nodeResourceMap
=
request
.
getNodeResourceMap
(
)
;
Set
<
NodeId
>
nodeIds
=
nodeResourceMap
.
keySet
(
)
;
for
(
NodeId
nodeId
:
nodeIds
)
{
RMNode
node
=
this
.
rm
.
getRMContext
(
)
.
getRMNodes
(
)
.
get
(
nodeId
)
;
if
(
node
==
null
)
{
for
(
NodeId
nodeId
:
nodeIds
)
{
RMNode
node
=
this
.
rm
.
getRMContext
(
)
.
getRMNodes
(
)
.
get
(
nodeId
)
;
if
(
node
==
null
)
{
LOG
.
error
(
+
+
nodeId
)
;
throw
RPCUtil
.
getRemoteException
(
+
+
nodeId
)
;
}
}
boolean
allSuccess
=
true
;
for
(
Map
.
Entry
<
NodeId
,
ResourceOption
>
entry
:
nodeResourceMap
.
entrySet
(
)
)
{
ResourceOption
newResourceOption
=
entry
.
getValue
(
)
;
NodeId
nodeId
=
entry
.
getKey
(
)
;
RMNode
node
=
this
.
rm
.
getRMContext
(
)
.
getRMNodes
(
)
.
get
(
nodeId
)
;
if
(
node
==
null
)
{
LOG
.
warn
(
+
nodeId
)
;
allSuccess
=
false
;
}
else
{
this
.
rm
.
getRMContext
(
)
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMNodeResourceUpdateEvent
(
nodeId
,
newResourceOption
)
)
;
private
void
addPlacementConstraintHandler
(
Configuration
conf
)
{
String
placementConstraintsHandler
=
conf
.
get
(
YarnConfiguration
.
RM_PLACEMENT_CONSTRAINTS_HANDLER
,
YarnConfiguration
.
DISABLED_RM_PLACEMENT_CONSTRAINTS_HANDLER
)
;
if
(
placementConstraintsHandler
.
equals
(
YarnConfiguration
.
DISABLED_RM_PLACEMENT_CONSTRAINTS_HANDLER
)
)
{
@
Override
public
FinishApplicationMasterResponse
finishApplicationMaster
(
FinishApplicationMasterRequest
request
)
throws
YarnException
,
IOException
{
ApplicationAttemptId
applicationAttemptId
=
YarnServerSecurityUtils
.
authorizeRequest
(
)
.
getApplicationAttemptId
(
)
;
ApplicationId
appId
=
applicationAttemptId
.
getApplicationId
(
)
;
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
if
(
timelineServiceV2Enabled
)
{
(
(
RMAppImpl
)
rmApp
)
.
removeCollectorData
(
)
;
}
if
(
rmApp
.
isAppFinalStateStored
(
)
)
{
LOG
.
info
(
rmApp
.
getApplicationId
(
)
+
)
;
return
FinishApplicationMasterResponse
.
newInstance
(
true
)
;
}
AllocateResponseLock
lock
=
responseMap
.
get
(
applicationAttemptId
)
;
if
(
lock
==
null
)
{
throwApplicationDoesNotExistInCacheException
(
applicationAttemptId
)
;
}
synchronized
(
lock
)
{
if
(
!
hasApplicationMasterRegistered
(
applicationAttemptId
)
)
{
String
message
=
+
appId
;
private
void
throwApplicationDoesNotExistInCacheException
(
ApplicationAttemptId
appAttemptId
)
throws
InvalidApplicationMasterRequestException
{
String
message
=
+
appAttemptId
;
@
Override
public
AllocateResponse
allocate
(
AllocateRequest
request
)
throws
YarnException
,
IOException
{
AMRMTokenIdentifier
amrmTokenIdentifier
=
YarnServerSecurityUtils
.
authorizeRequest
(
)
;
ApplicationAttemptId
appAttemptId
=
amrmTokenIdentifier
.
getApplicationAttemptId
(
)
;
this
.
amLivelinessMonitor
.
receivedPing
(
appAttemptId
)
;
AllocateResponseLock
lock
=
responseMap
.
get
(
appAttemptId
)
;
if
(
lock
==
null
)
{
String
message
=
+
appAttemptId
+
;
if
(
!
hasApplicationMasterRegistered
(
appAttemptId
)
)
{
String
message
=
+
appAttemptId
+
+
;
throw
new
ApplicationMasterNotRegisteredException
(
message
)
;
}
if
(
AMRMClientUtils
.
getNextResponseId
(
request
.
getResponseId
(
)
)
==
lastResponse
.
getResponseId
(
)
)
{
return
lastResponse
;
}
else
if
(
request
.
getResponseId
(
)
!=
lastResponse
.
getResponseId
(
)
)
{
throw
new
InvalidApplicationMasterRequestException
(
AMRMClientUtils
.
assembleInvalidResponseIdExceptionMessage
(
appAttemptId
,
lastResponse
.
getResponseId
(
)
,
request
.
getResponseId
(
)
)
)
;
}
AllocateResponse
response
=
recordFactory
.
newRecordInstance
(
AllocateResponse
.
class
)
;
this
.
amsProcessingChain
.
allocate
(
amrmTokenIdentifier
.
getApplicationAttemptId
(
)
,
request
,
response
)
;
MasterKeyData
nextMasterKey
=
this
.
rmContext
.
getAMRMTokenSecretManager
(
)
.
getNextMasterKeyData
(
)
;
if
(
nextMasterKey
!=
null
&&
nextMasterKey
.
getMasterKey
(
)
.
getKeyId
(
)
!=
amrmTokenIdentifier
.
getKeyId
(
)
)
{
RMApp
app
=
this
.
rmContext
.
getRMApps
(
)
.
get
(
appAttemptId
.
getApplicationId
(
)
)
;
RMAppAttempt
appAttempt
=
app
.
getRMAppAttempt
(
appAttemptId
)
;
RMAppAttemptImpl
appAttemptImpl
=
(
RMAppAttemptImpl
)
appAttempt
;
Token
<
AMRMTokenIdentifier
>
amrmToken
=
appAttempt
.
getAMRMToken
(
)
;
public
void
registerAppAttempt
(
ApplicationAttemptId
attemptId
)
{
AllocateResponse
response
=
recordFactory
.
newRecordInstance
(
AllocateResponse
.
class
)
;
response
.
setResponseId
(
AMRMClientUtils
.
PRE_REGISTER_RESPONSE_ID
)
;
public
void
unregisterAttempt
(
ApplicationAttemptId
attemptId
)
{
ApplicationId
getNewApplicationId
(
)
{
ApplicationId
applicationId
=
org
.
apache
.
hadoop
.
yarn
.
server
.
utils
.
BuilderUtils
.
newApplicationId
(
recordFactory
,
ResourceManager
.
getClusterTimeStamp
(
)
,
applicationCounter
.
incrementAndGet
(
)
)
;
LOG
.
warn
(
,
ie
)
;
RMAuditLogger
.
logFailure
(
user
,
AuditConstants
.
SUBMIT_APP_REQUEST
,
ie
.
getMessage
(
)
,
,
,
applicationId
,
callerContext
,
submissionContext
.
getQueue
(
)
)
;
throw
RPCUtil
.
getRemoteException
(
ie
)
;
}
checkTags
(
submissionContext
.
getApplicationTags
(
)
)
;
if
(
timelineServiceV2Enabled
)
{
String
value
=
null
;
try
{
for
(
String
tag
:
submissionContext
.
getApplicationTags
(
)
)
{
if
(
tag
.
startsWith
(
TimelineUtils
.
FLOW_RUN_ID_TAG_PREFIX
+
)
||
tag
.
startsWith
(
TimelineUtils
.
FLOW_RUN_ID_TAG_PREFIX
.
toLowerCase
(
)
+
)
)
{
value
=
tag
.
substring
(
TimelineUtils
.
FLOW_RUN_ID_TAG_PREFIX
.
length
(
)
+
1
)
;
Long
.
valueOf
(
value
)
;
}
}
}
catch
(
NumberFormatException
e
)
{
LOG
.
warn
(
+
value
+
,
e
)
;
RMAuditLogger
.
logFailure
(
user
,
AuditConstants
.
SUBMIT_APP_REQUEST
,
e
.
getMessage
(
)
,
,
,
applicationId
,
submissionContext
.
getQueue
(
)
)
;
throw
RPCUtil
.
getRemoteException
(
e
)
;
if
(
timelineServiceV2Enabled
)
{
String
value
=
null
;
try
{
for
(
String
tag
:
submissionContext
.
getApplicationTags
(
)
)
{
if
(
tag
.
startsWith
(
TimelineUtils
.
FLOW_RUN_ID_TAG_PREFIX
+
)
||
tag
.
startsWith
(
TimelineUtils
.
FLOW_RUN_ID_TAG_PREFIX
.
toLowerCase
(
)
+
)
)
{
value
=
tag
.
substring
(
TimelineUtils
.
FLOW_RUN_ID_TAG_PREFIX
.
length
(
)
+
1
)
;
Long
.
valueOf
(
value
)
;
}
}
}
catch
(
NumberFormatException
e
)
{
LOG
.
warn
(
+
value
+
,
e
)
;
RMAuditLogger
.
logFailure
(
user
,
AuditConstants
.
SUBMIT_APP_REQUEST
,
e
.
getMessage
(
)
,
,
,
applicationId
,
submissionContext
.
getQueue
(
)
)
;
throw
RPCUtil
.
getRemoteException
(
e
)
;
}
}
if
(
rmContext
.
getRMApps
(
)
.
get
(
applicationId
)
!=
null
)
{
LOG
.
info
(
+
applicationId
)
;
return
SubmitApplicationResponse
.
newInstance
(
)
;
}
ByteBuffer
tokenConf
=
submissionContext
.
getAMContainerSpec
(
)
.
getTokensConf
(
)
;
RMAuditLogger
.
ArgsBuilder
arguments
=
new
RMAuditLogger
.
ArgsBuilder
(
)
.
append
(
Keys
.
QUEUENAME
,
request
.
getQueueName
(
)
)
.
append
(
Keys
.
INCLUDEAPPS
,
String
.
valueOf
(
request
.
getIncludeApplications
(
)
)
)
.
append
(
Keys
.
INCLUDECHILDQUEUES
,
String
.
valueOf
(
request
.
getIncludeChildQueues
(
)
)
)
.
append
(
Keys
.
RECURSIVE
,
String
.
valueOf
(
request
.
getRecursive
(
)
)
)
;
try
{
QueueInfo
queueInfo
=
scheduler
.
getQueueInfo
(
request
.
getQueueName
(
)
,
request
.
getIncludeChildQueues
(
)
,
request
.
getRecursive
(
)
)
;
List
<
ApplicationReport
>
appReports
=
EMPTY_APPS_REPORT
;
if
(
request
.
getIncludeApplications
(
)
)
{
List
<
ApplicationAttemptId
>
apps
=
scheduler
.
getAppsInQueue
(
request
.
getQueueName
(
)
)
;
appReports
=
new
ArrayList
<
ApplicationReport
>
(
apps
.
size
(
)
)
;
for
(
ApplicationAttemptId
app
:
apps
)
{
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
app
.
getApplicationId
(
)
)
;
if
(
rmApp
!=
null
)
{
if
(
!
checkAccess
(
callerUGI
,
rmApp
.
getUser
(
)
,
ApplicationAccessType
.
VIEW_APP
,
rmApp
)
)
{
continue
;
}
appReports
.
add
(
rmApp
.
createAndGetApplicationReport
(
callerUGI
.
getUserName
(
)
,
true
)
)
;
}
}
}
queueInfo
.
setApplications
(
appReports
)
;
response
.
setQueueInfo
(
queueInfo
)
;
private
void
refreshScheduler
(
String
planName
,
ReservationDefinition
contract
,
String
reservationId
)
{
if
(
(
contract
.
getArrival
(
)
-
clock
.
getTime
(
)
)
<
reservationSystem
.
getPlanFollowerTimeStep
(
)
)
{
private
void
refreshScheduler
(
String
planName
,
ReservationDefinition
contract
,
String
reservationId
)
{
if
(
(
contract
.
getArrival
(
)
-
clock
.
getTime
(
)
)
<
reservationSystem
.
getPlanFollowerTimeStep
(
)
)
{
LOG
.
debug
(
+
,
reservationId
)
;
reservationSystem
.
synchronizePlan
(
planName
,
true
)
;
public
synchronized
void
remove
(
NodeId
nodeId
)
{
DecommissioningNodeContext
context
=
decomNodes
.
get
(
nodeId
)
;
if
(
context
!=
null
)
{
@
Override
public
void
registerApplicationMaster
(
ApplicationAttemptId
applicationAttemptId
,
RegisterApplicationMasterRequest
request
,
RegisterApplicationMasterResponse
response
)
throws
IOException
,
YarnException
{
RMApp
app
=
getRmContext
(
)
.
getRMApps
(
)
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
}
if
(
app
.
getApplicationSubmissionContext
(
)
.
getKeepContainersAcrossApplicationAttempts
(
)
)
{
List
<
Container
>
transferredContainers
=
getScheduler
(
)
.
getTransferredContainers
(
applicationAttemptId
)
;
if
(
!
transferredContainers
.
isEmpty
(
)
)
{
response
.
setContainersFromPreviousAttempts
(
transferredContainers
)
;
rmContext
.
getNMTokenSecretManager
(
)
.
clearNodeSetForAttempt
(
applicationAttemptId
)
;
List
<
NMToken
>
nmTokens
=
new
ArrayList
<
NMToken
>
(
)
;
for
(
Container
container
:
transferredContainers
)
{
try
{
NMToken
token
=
getRmContext
(
)
.
getNMTokenSecretManager
(
)
.
createAndGetNMToken
(
app
.
getUser
(
)
,
applicationAttemptId
,
container
)
;
if
(
null
!=
token
)
{
nmTokens
.
add
(
token
)
;
}
}
catch
(
IllegalArgumentException
e
)
{
if
(
e
.
getCause
(
)
instanceof
UnknownHostException
)
{
throw
(
UnknownHostException
)
e
.
getCause
(
)
;
}
}
}
response
.
setNMTokensFromPreviousAttempts
(
nmTokens
)
;
List
<
RMNode
>
nodesToDecom
=
new
ArrayList
<
RMNode
>
(
)
;
HostDetails
hostDetails
;
gracefulDecommissionableNodes
.
clear
(
)
;
if
(
graceful
)
{
hostDetails
=
hostsReader
.
getLazyLoadedHostDetails
(
)
;
}
else
{
hostDetails
=
hostsReader
.
getHostDetails
(
)
;
}
Set
<
String
>
includes
=
hostDetails
.
getIncludedHosts
(
)
;
Map
<
String
,
Integer
>
excludes
=
hostDetails
.
getExcludedMap
(
)
;
for
(
RMNode
n
:
this
.
rmContext
.
getRMNodes
(
)
.
values
(
)
)
{
NodeState
s
=
n
.
getState
(
)
;
boolean
isExcluded
=
!
isValidNode
(
n
.
getHostName
(
)
,
includes
,
excludes
.
keySet
(
)
)
;
String
nodeStr
=
+
n
.
getNodeID
(
)
+
+
s
;
if
(
!
isExcluded
)
{
if
(
s
==
NodeState
.
DECOMMISSIONING
)
{
else
{
hostDetails
=
hostsReader
.
getHostDetails
(
)
;
}
Set
<
String
>
includes
=
hostDetails
.
getIncludedHosts
(
)
;
Map
<
String
,
Integer
>
excludes
=
hostDetails
.
getExcludedMap
(
)
;
for
(
RMNode
n
:
this
.
rmContext
.
getRMNodes
(
)
.
values
(
)
)
{
NodeState
s
=
n
.
getState
(
)
;
boolean
isExcluded
=
!
isValidNode
(
n
.
getHostName
(
)
,
includes
,
excludes
.
keySet
(
)
)
;
String
nodeStr
=
+
n
.
getNodeID
(
)
+
+
s
;
if
(
!
isExcluded
)
{
if
(
s
==
NodeState
.
DECOMMISSIONING
)
{
LOG
.
info
(
+
nodeStr
)
;
nodesToRecom
.
add
(
n
)
;
}
}
else
{
if
(
graceful
)
{
Integer
timeoutToUse
=
(
excludes
.
get
(
n
.
getHostName
(
)
)
!=
null
)
?
excludes
.
get
(
n
.
getHostName
(
)
)
:
timeout
;
Map
<
String
,
Integer
>
excludes
=
hostDetails
.
getExcludedMap
(
)
;
for
(
RMNode
n
:
this
.
rmContext
.
getRMNodes
(
)
.
values
(
)
)
{
NodeState
s
=
n
.
getState
(
)
;
boolean
isExcluded
=
!
isValidNode
(
n
.
getHostName
(
)
,
includes
,
excludes
.
keySet
(
)
)
;
String
nodeStr
=
+
n
.
getNodeID
(
)
+
+
s
;
if
(
!
isExcluded
)
{
if
(
s
==
NodeState
.
DECOMMISSIONING
)
{
LOG
.
info
(
+
nodeStr
)
;
nodesToRecom
.
add
(
n
)
;
}
}
else
{
if
(
graceful
)
{
Integer
timeoutToUse
=
(
excludes
.
get
(
n
.
getHostName
(
)
)
!=
null
)
?
excludes
.
get
(
n
.
getHostName
(
)
)
:
timeout
;
if
(
s
!=
NodeState
.
DECOMMISSIONED
&&
s
!=
NodeState
.
DECOMMISSIONING
)
{
LOG
.
info
(
+
nodeStr
)
;
nodesToDecom
.
add
(
n
)
;
String
nodeStr
=
+
n
.
getNodeID
(
)
+
+
s
;
if
(
!
isExcluded
)
{
if
(
s
==
NodeState
.
DECOMMISSIONING
)
{
LOG
.
info
(
+
nodeStr
)
;
nodesToRecom
.
add
(
n
)
;
}
}
else
{
if
(
graceful
)
{
Integer
timeoutToUse
=
(
excludes
.
get
(
n
.
getHostName
(
)
)
!=
null
)
?
excludes
.
get
(
n
.
getHostName
(
)
)
:
timeout
;
if
(
s
!=
NodeState
.
DECOMMISSIONED
&&
s
!=
NodeState
.
DECOMMISSIONING
)
{
LOG
.
info
(
+
nodeStr
)
;
nodesToDecom
.
add
(
n
)
;
gracefulDecommissionableNodes
.
add
(
n
)
;
}
else
if
(
s
==
NodeState
.
DECOMMISSIONING
&&
!
Objects
.
equals
(
n
.
getDecommissioningTimeout
(
)
,
timeoutToUse
)
)
{
LOG
.
info
(
+
nodeStr
+
+
timeoutToUse
)
;
nodesToDecom
.
add
(
n
)
;
LOG
.
info
(
+
nodeStr
)
;
nodesToRecom
.
add
(
n
)
;
}
}
else
{
if
(
graceful
)
{
Integer
timeoutToUse
=
(
excludes
.
get
(
n
.
getHostName
(
)
)
!=
null
)
?
excludes
.
get
(
n
.
getHostName
(
)
)
:
timeout
;
if
(
s
!=
NodeState
.
DECOMMISSIONED
&&
s
!=
NodeState
.
DECOMMISSIONING
)
{
LOG
.
info
(
+
nodeStr
)
;
nodesToDecom
.
add
(
n
)
;
gracefulDecommissionableNodes
.
add
(
n
)
;
}
else
if
(
s
==
NodeState
.
DECOMMISSIONING
&&
!
Objects
.
equals
(
n
.
getDecommissioningTimeout
(
)
,
timeoutToUse
)
)
{
LOG
.
info
(
+
nodeStr
+
+
timeoutToUse
)
;
nodesToDecom
.
add
(
n
)
;
gracefulDecommissionableNodes
.
add
(
n
)
;
}
else
{
LOG
.
info
(
+
nodeStr
)
;
protected
synchronized
void
checkAppNumCompletedLimit
(
)
{
while
(
completedAppsInStateStore
>
this
.
maxCompletedAppsInStateStore
)
{
ApplicationId
removeId
=
completedApps
.
get
(
completedApps
.
size
(
)
-
completedAppsInStateStore
)
;
RMApp
removeApp
=
rmContext
.
getRMApps
(
)
.
get
(
removeId
)
;
@
Override
public
void
handle
(
RMAppManagerEvent
event
)
{
ApplicationId
applicationId
=
event
.
getApplicationId
(
)
;
private
void
copyPlacementQueueToSubmissionContext
(
ApplicationPlacementContext
placementContext
,
ApplicationSubmissionContext
context
)
{
if
(
placementContext
!=
null
&&
!
StringUtils
.
equalsIgnoreCase
(
context
.
getQueue
(
)
,
placementContext
.
getQueue
(
)
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
ContainerId
containerId
,
Resource
resource
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
ContainerId
containerId
,
Resource
resource
,
String
queueName
,
String
partition
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
InetAddress
ip
,
ArgsBuilder
args
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
CallerContext
callerContext
,
String
queueName
,
String
partition
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
ApplicationAttemptId
attemptId
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
CallerContext
callerContext
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
CallerContext
callerContext
,
String
queueName
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
,
InetAddress
ip
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
,
ApplicationId
appId
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
public
static
void
logSuccess
(
String
user
,
String
operation
,
String
target
)
{
if
(
LOG
.
isInfoEnabled
(
)
)
{
protected
ResourceScheduler
createScheduler
(
)
{
String
schedulerClassName
=
conf
.
get
(
YarnConfiguration
.
RM_SCHEDULER
,
YarnConfiguration
.
DEFAULT_RM_SCHEDULER
)
;
protected
SystemServiceManager
createServiceManager
(
)
{
String
schedulerClassName
=
YarnConfiguration
.
DEFAULT_YARN_API_SYSTEM_SERVICES_CLASS
;
builder
.
withAttribute
(
WebAppProxy
.
PROXY_HOST_ATTRIBUTE
,
proxyParts
[
0
]
)
;
}
WebAppContext
uiWebAppContext
=
null
;
if
(
getConfig
(
)
.
getBoolean
(
YarnConfiguration
.
YARN_WEBAPP_UI2_ENABLE
,
YarnConfiguration
.
DEFAULT_YARN_WEBAPP_UI2_ENABLE
)
)
{
String
onDiskPath
=
getConfig
(
)
.
get
(
YarnConfiguration
.
YARN_WEBAPP_UI2_WARFILE_PATH
)
;
uiWebAppContext
=
new
WebAppContext
(
)
;
uiWebAppContext
.
setContextPath
(
UI2_WEBAPP_NAME
)
;
if
(
null
==
onDiskPath
)
{
String
war
=
+
VersionInfo
.
getVersion
(
)
+
;
URL
url
=
getClass
(
)
.
getClassLoader
(
)
.
getResource
(
war
)
;
if
(
null
==
url
)
{
onDiskPath
=
getWebAppsPath
(
)
;
}
else
{
onDiskPath
=
url
.
getFile
(
)
;
}
}
if
(
onDiskPath
==
null
||
onDiskPath
.
isEmpty
(
)
)
{
LOG
.
error
(
)
;
if
(
getConfig
(
)
.
getBoolean
(
YarnConfiguration
.
YARN_WEBAPP_UI2_ENABLE
,
YarnConfiguration
.
DEFAULT_YARN_WEBAPP_UI2_ENABLE
)
)
{
String
onDiskPath
=
getConfig
(
)
.
get
(
YarnConfiguration
.
YARN_WEBAPP_UI2_WARFILE_PATH
)
;
uiWebAppContext
=
new
WebAppContext
(
)
;
uiWebAppContext
.
setContextPath
(
UI2_WEBAPP_NAME
)
;
if
(
null
==
onDiskPath
)
{
String
war
=
+
VersionInfo
.
getVersion
(
)
+
;
URL
url
=
getClass
(
)
.
getClassLoader
(
)
.
getResource
(
war
)
;
if
(
null
==
url
)
{
onDiskPath
=
getWebAppsPath
(
)
;
}
else
{
onDiskPath
=
url
.
getFile
(
)
;
}
}
if
(
onDiskPath
==
null
||
onDiskPath
.
isEmpty
(
)
)
{
LOG
.
error
(
)
;
}
else
{
if
(
onDiskPath
.
endsWith
(
)
)
{
if
(
argv
.
length
>=
1
)
{
if
(
argv
[
0
]
.
equals
(
)
)
{
deleteRMStateStore
(
conf
)
;
}
else
if
(
argv
[
0
]
.
equals
(
)
)
{
deleteRMConfStore
(
conf
)
;
}
else
if
(
argv
[
0
]
.
equals
(
)
&&
argv
.
length
==
2
)
{
removeApplication
(
conf
,
argv
[
1
]
)
;
}
else
{
printUsage
(
System
.
err
)
;
}
}
else
{
ResourceManager
resourceManager
=
new
ResourceManager
(
)
;
ShutdownHookManager
.
get
(
)
.
addShutdownHook
(
new
CompositeServiceShutdownHook
(
resourceManager
)
,
SHUTDOWN_HOOK_PRIORITY
)
;
resourceManager
.
init
(
conf
)
;
resourceManager
.
start
(
)
;
}
}
catch
(
Throwable
t
)
{
@
VisibleForTesting
static
void
removeApplication
(
Configuration
conf
,
String
applicationId
)
throws
Exception
{
RMStateStore
rmStore
=
RMStateStoreFactory
.
getStore
(
conf
)
;
rmStore
.
setResourceManager
(
new
ResourceManager
(
)
)
;
rmStore
.
init
(
conf
)
;
rmStore
.
start
(
)
;
try
{
ApplicationId
removeAppId
=
ApplicationId
.
fromString
(
applicationId
)
;
@
SuppressWarnings
(
)
@
VisibleForTesting
void
handleNMContainerStatus
(
NMContainerStatus
containerStatus
,
NodeId
nodeId
)
{
ApplicationAttemptId
appAttemptId
=
containerStatus
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
;
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
appAttemptId
.
getApplicationId
(
)
)
;
if
(
rmApp
==
null
)
{
@
SuppressWarnings
(
)
@
Override
public
RegisterNodeManagerResponse
registerNodeManager
(
RegisterNodeManagerRequest
request
)
throws
YarnException
,
IOException
{
NodeId
nodeId
=
request
.
getNodeId
(
)
;
String
host
=
nodeId
.
getHost
(
)
;
int
cmPort
=
nodeId
.
getPort
(
)
;
int
httpPort
=
request
.
getHttpPort
(
)
;
Resource
capability
=
request
.
getResource
(
)
;
String
nodeManagerVersion
=
request
.
getNMVersion
(
)
;
Resource
physicalResource
=
request
.
getPhysicalResource
(
)
;
NodeStatus
nodeStatus
=
request
.
getNodeStatus
(
)
;
RegisterNodeManagerResponse
response
=
recordFactory
.
newRecordInstance
(
RegisterNodeManagerResponse
.
class
)
;
if
(
!
minimumNodeManagerVersion
.
equals
(
)
)
{
if
(
minimumNodeManagerVersion
.
equals
(
)
)
{
minimumNodeManagerVersion
=
YarnVersionInfo
.
getVersion
(
)
;
}
if
(
(
nodeManagerVersion
==
null
)
||
(
VersionUtil
.
compareVersions
(
nodeManagerVersion
,
minimumNodeManagerVersion
)
)
<
0
)
{
String
message
=
+
nodeManagerVersion
+
+
minimumNodeManagerVersion
+
+
;
String
message
=
+
nodeManagerVersion
+
+
minimumNodeManagerVersion
+
+
;
LOG
.
info
(
message
)
;
response
.
setDiagnosticsMessage
(
message
)
;
response
.
setNodeAction
(
NodeAction
.
SHUTDOWN
)
;
return
response
;
}
}
if
(
checkIpHostnameInRegistration
)
{
InetSocketAddress
nmAddress
=
NetUtils
.
createSocketAddrForHost
(
host
,
cmPort
)
;
InetAddress
inetAddress
=
Server
.
getRemoteIp
(
)
;
if
(
inetAddress
!=
null
&&
nmAddress
.
isUnresolved
(
)
)
{
final
String
message
=
+
inetAddress
.
getHostAddress
(
)
+
+
host
+
;
LOG
.
warn
(
+
message
)
;
response
.
setDiagnosticsMessage
(
message
)
;
response
.
setNodeAction
(
NodeAction
.
SHUTDOWN
)
;
return
response
;
}
}
if
(
!
this
.
nodesListManager
.
isValidNode
(
host
)
&&
!
isNodeInDecommissioning
(
nodeId
)
)
{
LOG
.
warn
(
+
message
)
;
response
.
setDiagnosticsMessage
(
message
)
;
response
.
setNodeAction
(
NodeAction
.
SHUTDOWN
)
;
return
response
;
}
}
if
(
!
this
.
nodesListManager
.
isValidNode
(
host
)
&&
!
isNodeInDecommissioning
(
nodeId
)
)
{
String
message
=
+
host
+
;
LOG
.
info
(
message
)
;
response
.
setDiagnosticsMessage
(
message
)
;
response
.
setNodeAction
(
NodeAction
.
SHUTDOWN
)
;
return
response
;
}
String
nid
=
nodeId
.
toString
(
)
;
Resource
dynamicLoadCapability
=
loadNodeResourceFromDRConfiguration
(
nid
)
;
if
(
dynamicLoadCapability
!=
null
)
{
LOG
.
debug
(
+
,
nid
,
capability
,
dynamicLoadCapability
)
;
capability
=
dynamicLoadCapability
;
}
String
nid
=
nodeId
.
toString
(
)
;
Resource
dynamicLoadCapability
=
loadNodeResourceFromDRConfiguration
(
nid
)
;
if
(
dynamicLoadCapability
!=
null
)
{
LOG
.
debug
(
+
,
nid
,
capability
,
dynamicLoadCapability
)
;
capability
=
dynamicLoadCapability
;
response
.
setResource
(
capability
)
;
}
if
(
capability
.
getMemorySize
(
)
<
minAllocMb
||
capability
.
getVirtualCores
(
)
<
minAllocVcores
)
{
String
message
=
+
host
+
+
+
capability
+
+
minAllocMb
+
+
minAllocVcores
+
;
LOG
.
info
(
message
)
;
response
.
setDiagnosticsMessage
(
message
)
;
response
.
setNodeAction
(
NodeAction
.
SHUTDOWN
)
;
return
response
;
}
response
.
setContainerTokenMasterKey
(
containerTokenSecretManager
.
getCurrentKey
(
)
)
;
response
.
setNMTokenMasterKey
(
nmTokenSecretManager
.
getCurrentKey
(
)
)
;
RMNode
rmNode
=
new
RMNodeImpl
(
nodeId
,
rmContext
,
host
,
cmPort
,
httpPort
,
resolve
(
host
)
,
capability
,
nodeManagerVersion
,
physicalResource
)
;
LOG
.
debug
(
+
,
nid
,
capability
,
dynamicLoadCapability
)
;
capability
=
dynamicLoadCapability
;
response
.
setResource
(
capability
)
;
}
if
(
capability
.
getMemorySize
(
)
<
minAllocMb
||
capability
.
getVirtualCores
(
)
<
minAllocVcores
)
{
String
message
=
+
host
+
+
+
capability
+
+
minAllocMb
+
+
minAllocVcores
+
;
LOG
.
info
(
message
)
;
response
.
setDiagnosticsMessage
(
message
)
;
response
.
setNodeAction
(
NodeAction
.
SHUTDOWN
)
;
return
response
;
}
response
.
setContainerTokenMasterKey
(
containerTokenSecretManager
.
getCurrentKey
(
)
)
;
response
.
setNMTokenMasterKey
(
nmTokenSecretManager
.
getCurrentKey
(
)
)
;
RMNode
rmNode
=
new
RMNodeImpl
(
nodeId
,
rmContext
,
host
,
cmPort
,
httpPort
,
resolve
(
host
)
,
capability
,
nodeManagerVersion
,
physicalResource
)
;
RMNode
oldNode
=
this
.
rmContext
.
getRMNodes
(
)
.
putIfAbsent
(
nodeId
,
rmNode
)
;
if
(
oldNode
==
null
)
{
RMNodeStartedEvent
startEvent
=
new
RMNodeStartedEvent
(
nodeId
,
request
.
getNMContainerStatuses
(
)
,
request
.
getRunningApplications
(
)
,
nodeStatus
)
;
LOG
.
debug
(
+
+
nodeId
+
+
request
.
getLogAggregationReportsForApps
(
)
.
size
(
)
)
;
}
startEvent
.
setLogAggregationReportsForApps
(
request
.
getLogAggregationReportsForApps
(
)
)
;
}
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
startEvent
)
;
}
else
{
LOG
.
info
(
+
host
)
;
this
.
nmLivelinessMonitor
.
unregister
(
nodeId
)
;
if
(
CollectionUtils
.
isEmpty
(
request
.
getRunningApplications
(
)
)
&&
rmNode
.
getState
(
)
!=
NodeState
.
DECOMMISSIONING
&&
rmNode
.
getHttpPort
(
)
!=
oldNode
.
getHttpPort
(
)
)
{
switch
(
rmNode
.
getState
(
)
)
{
case
RUNNING
:
ClusterMetrics
.
getMetrics
(
)
.
decrNumActiveNodes
(
)
;
break
;
case
UNHEALTHY
:
ClusterMetrics
.
getMetrics
(
)
.
decrNumUnhealthyNMs
(
)
;
break
;
default
:
LOG
.
debug
(
)
;
}
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
NodeRemovedSchedulerEvent
(
rmNode
)
)
;
this
.
rmContext
.
getRMNodes
(
)
.
put
(
nodeId
,
rmNode
)
;
@
SuppressWarnings
(
)
@
Override
public
NodeHeartbeatResponse
nodeHeartbeat
(
NodeHeartbeatRequest
request
)
throws
YarnException
,
IOException
{
NodeStatus
remoteNodeStatus
=
request
.
getNodeStatus
(
)
;
NodeId
nodeId
=
remoteNodeStatus
.
getNodeId
(
)
;
if
(
!
this
.
nodesListManager
.
isValidNode
(
nodeId
.
getHost
(
)
)
&&
!
isNodeInDecommissioning
(
nodeId
)
)
{
String
message
=
+
nodeId
+
+
nodeId
.
getHost
(
)
;
NodeStatus
remoteNodeStatus
=
request
.
getNodeStatus
(
)
;
NodeId
nodeId
=
remoteNodeStatus
.
getNodeId
(
)
;
if
(
!
this
.
nodesListManager
.
isValidNode
(
nodeId
.
getHost
(
)
)
&&
!
isNodeInDecommissioning
(
nodeId
)
)
{
String
message
=
+
nodeId
+
+
nodeId
.
getHost
(
)
;
LOG
.
info
(
message
)
;
return
YarnServerBuilderUtils
.
newNodeHeartbeatResponse
(
NodeAction
.
SHUTDOWN
,
message
)
;
}
RMNode
rmNode
=
this
.
rmContext
.
getRMNodes
(
)
.
get
(
nodeId
)
;
if
(
rmNode
==
null
)
{
String
message
=
+
remoteNodeStatus
.
getNodeId
(
)
;
LOG
.
info
(
message
)
;
return
YarnServerBuilderUtils
.
newNodeHeartbeatResponse
(
NodeAction
.
RESYNC
,
message
)
;
}
this
.
nmLivelinessMonitor
.
receivedPing
(
nodeId
)
;
this
.
decommissioningWatcher
.
update
(
rmNode
,
remoteNodeStatus
)
;
NodeHeartbeatResponse
lastNodeHeartbeatResponse
=
rmNode
.
getLastNodeHeartBeatResponse
(
)
;
if
(
getNextResponseId
(
remoteNodeStatus
.
getResponseId
(
)
)
==
lastNodeHeartbeatResponse
.
getResponseId
(
)
)
{
LOG
.
info
(
message
)
;
return
YarnServerBuilderUtils
.
newNodeHeartbeatResponse
(
NodeAction
.
SHUTDOWN
,
message
)
;
}
RMNode
rmNode
=
this
.
rmContext
.
getRMNodes
(
)
.
get
(
nodeId
)
;
if
(
rmNode
==
null
)
{
String
message
=
+
remoteNodeStatus
.
getNodeId
(
)
;
LOG
.
info
(
message
)
;
return
YarnServerBuilderUtils
.
newNodeHeartbeatResponse
(
NodeAction
.
RESYNC
,
message
)
;
}
this
.
nmLivelinessMonitor
.
receivedPing
(
nodeId
)
;
this
.
decommissioningWatcher
.
update
(
rmNode
,
remoteNodeStatus
)
;
NodeHeartbeatResponse
lastNodeHeartbeatResponse
=
rmNode
.
getLastNodeHeartBeatResponse
(
)
;
if
(
getNextResponseId
(
remoteNodeStatus
.
getResponseId
(
)
)
==
lastNodeHeartbeatResponse
.
getResponseId
(
)
)
{
LOG
.
info
(
+
rmNode
.
getNodeAddress
(
)
+
+
remoteNodeStatus
.
getResponseId
(
)
)
;
return
lastNodeHeartbeatResponse
;
}
else
if
(
remoteNodeStatus
.
getResponseId
(
)
!=
lastNodeHeartbeatResponse
.
getResponseId
(
)
)
{
String
message
=
+
lastNodeHeartbeatResponse
.
getResponseId
(
)
+
+
remoteNodeStatus
.
getResponseId
(
)
;
String
message
=
+
remoteNodeStatus
.
getNodeId
(
)
;
LOG
.
info
(
message
)
;
return
YarnServerBuilderUtils
.
newNodeHeartbeatResponse
(
NodeAction
.
RESYNC
,
message
)
;
}
this
.
nmLivelinessMonitor
.
receivedPing
(
nodeId
)
;
this
.
decommissioningWatcher
.
update
(
rmNode
,
remoteNodeStatus
)
;
NodeHeartbeatResponse
lastNodeHeartbeatResponse
=
rmNode
.
getLastNodeHeartBeatResponse
(
)
;
if
(
getNextResponseId
(
remoteNodeStatus
.
getResponseId
(
)
)
==
lastNodeHeartbeatResponse
.
getResponseId
(
)
)
{
LOG
.
info
(
+
rmNode
.
getNodeAddress
(
)
+
+
remoteNodeStatus
.
getResponseId
(
)
)
;
return
lastNodeHeartbeatResponse
;
}
else
if
(
remoteNodeStatus
.
getResponseId
(
)
!=
lastNodeHeartbeatResponse
.
getResponseId
(
)
)
{
String
message
=
+
lastNodeHeartbeatResponse
.
getResponseId
(
)
+
+
remoteNodeStatus
.
getResponseId
(
)
;
LOG
.
info
(
message
)
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMNodeEvent
(
nodeId
,
RMNodeEventType
.
REBOOTING
)
)
;
return
YarnServerBuilderUtils
.
newNodeHeartbeatResponse
(
NodeAction
.
RESYNC
,
message
)
;
}
if
(
rmNode
.
getState
(
)
==
NodeState
.
DECOMMISSIONING
&&
decommissioningWatcher
.
checkReadyToBeDecommissioned
(
rmNode
.
getNodeID
(
)
)
)
{
Map
<
ApplicationId
,
RMApp
>
rmApps
=
rmContext
.
getRMApps
(
)
;
for
(
Map
.
Entry
<
ApplicationId
,
AppCollectorData
>
entry
:
registeringCollectorsMap
.
entrySet
(
)
)
{
ApplicationId
appId
=
entry
.
getKey
(
)
;
AppCollectorData
collectorData
=
entry
.
getValue
(
)
;
if
(
collectorData
!=
null
)
{
if
(
!
collectorData
.
isStamped
(
)
)
{
collectorData
.
setRMIdentifier
(
ResourceManager
.
getClusterTimeStamp
(
)
)
;
collectorData
.
setVersion
(
timelineCollectorVersion
.
getAndIncrement
(
)
)
;
}
RMApp
rmApp
=
rmApps
.
get
(
appId
)
;
if
(
rmApp
==
null
)
{
LOG
.
warn
(
+
appId
+
)
;
}
else
{
synchronized
(
rmApp
)
{
AppCollectorData
previousCollectorData
=
rmApp
.
getCollectorData
(
)
;
if
(
AppCollectorData
.
happensBefore
(
previousCollectorData
,
collectorData
)
)
{
@
SuppressWarnings
(
)
@
Override
public
UnRegisterNodeManagerResponse
unRegisterNodeManager
(
UnRegisterNodeManagerRequest
request
)
throws
YarnException
,
IOException
{
UnRegisterNodeManagerResponse
response
=
recordFactory
.
newRecordInstance
(
UnRegisterNodeManagerResponse
.
class
)
;
NodeId
nodeId
=
request
.
getNodeId
(
)
;
RMNode
rmNode
=
this
.
rmContext
.
getRMNodes
(
)
.
get
(
nodeId
)
;
if
(
rmNode
==
null
)
{
private
void
updateNodeLabelsFromNMReport
(
Set
<
String
>
nodeLabels
,
NodeId
nodeId
)
throws
IOException
{
try
{
Map
<
NodeId
,
Set
<
String
>>
labelsUpdate
=
new
HashMap
<
NodeId
,
Set
<
String
>>
(
)
;
labelsUpdate
.
put
(
nodeId
,
nodeLabels
)
;
this
.
rmContext
.
getNodeLabelManager
(
)
.
replaceLabelsOnNode
(
labelsUpdate
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
populateTokenSequenceNo
(
NodeHeartbeatRequest
request
,
NodeHeartbeatResponse
nodeHeartBeatResponse
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
protected
void
handleWritingApplicationHistoryEvent
(
WritingApplicationHistoryEvent
event
)
{
switch
(
event
.
getType
(
)
)
{
case
APP_START
:
WritingApplicationStartEvent
wasEvent
=
(
WritingApplicationStartEvent
)
event
;
try
{
writer
.
applicationStarted
(
wasEvent
.
getApplicationStartData
(
)
)
;
writer
.
applicationStarted
(
wasEvent
.
getApplicationStartData
(
)
)
;
LOG
.
info
(
+
wasEvent
.
getApplicationId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
wasEvent
.
getApplicationId
(
)
)
;
}
break
;
case
APP_FINISH
:
WritingApplicationFinishEvent
wafEvent
=
(
WritingApplicationFinishEvent
)
event
;
try
{
writer
.
applicationFinished
(
wafEvent
.
getApplicationFinishData
(
)
)
;
LOG
.
info
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
break
;
case
APP_ATTEMPT_START
:
WritingApplicationAttemptStartEvent
waasEvent
=
(
WritingApplicationAttemptStartEvent
)
event
;
try
{
writer
.
applicationAttemptStarted
(
waasEvent
.
getApplicationAttemptStartData
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
wasEvent
.
getApplicationId
(
)
)
;
}
break
;
case
APP_FINISH
:
WritingApplicationFinishEvent
wafEvent
=
(
WritingApplicationFinishEvent
)
event
;
try
{
writer
.
applicationFinished
(
wafEvent
.
getApplicationFinishData
(
)
)
;
LOG
.
info
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
break
;
case
APP_ATTEMPT_START
:
WritingApplicationAttemptStartEvent
waasEvent
=
(
WritingApplicationAttemptStartEvent
)
event
;
try
{
writer
.
applicationAttemptStarted
(
waasEvent
.
getApplicationAttemptStartData
(
)
)
;
LOG
.
info
(
+
waasEvent
.
getApplicationAttemptId
(
)
)
;
}
catch
(
IOException
e
)
{
case
APP_FINISH
:
WritingApplicationFinishEvent
wafEvent
=
(
WritingApplicationFinishEvent
)
event
;
try
{
writer
.
applicationFinished
(
wafEvent
.
getApplicationFinishData
(
)
)
;
LOG
.
info
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
break
;
case
APP_ATTEMPT_START
:
WritingApplicationAttemptStartEvent
waasEvent
=
(
WritingApplicationAttemptStartEvent
)
event
;
try
{
writer
.
applicationAttemptStarted
(
waasEvent
.
getApplicationAttemptStartData
(
)
)
;
LOG
.
info
(
+
waasEvent
.
getApplicationAttemptId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
waasEvent
.
getApplicationAttemptId
(
)
)
;
}
break
;
case
APP_ATTEMPT_FINISH
:
WritingApplicationAttemptFinishEvent
waafEvent
=
(
WritingApplicationAttemptFinishEvent
)
event
;
writer
.
applicationFinished
(
wafEvent
.
getApplicationFinishData
(
)
)
;
LOG
.
info
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
wafEvent
.
getApplicationId
(
)
)
;
}
break
;
case
APP_ATTEMPT_START
:
WritingApplicationAttemptStartEvent
waasEvent
=
(
WritingApplicationAttemptStartEvent
)
event
;
try
{
writer
.
applicationAttemptStarted
(
waasEvent
.
getApplicationAttemptStartData
(
)
)
;
LOG
.
info
(
+
waasEvent
.
getApplicationAttemptId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
waasEvent
.
getApplicationAttemptId
(
)
)
;
}
break
;
case
APP_ATTEMPT_FINISH
:
WritingApplicationAttemptFinishEvent
waafEvent
=
(
WritingApplicationAttemptFinishEvent
)
event
;
try
{
writer
.
applicationAttemptFinished
(
waafEvent
.
getApplicationAttemptFinishData
(
)
)
;
case
APP_ATTEMPT_START
:
WritingApplicationAttemptStartEvent
waasEvent
=
(
WritingApplicationAttemptStartEvent
)
event
;
try
{
writer
.
applicationAttemptStarted
(
waasEvent
.
getApplicationAttemptStartData
(
)
)
;
LOG
.
info
(
+
waasEvent
.
getApplicationAttemptId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
waasEvent
.
getApplicationAttemptId
(
)
)
;
}
break
;
case
APP_ATTEMPT_FINISH
:
WritingApplicationAttemptFinishEvent
waafEvent
=
(
WritingApplicationAttemptFinishEvent
)
event
;
try
{
writer
.
applicationAttemptFinished
(
waafEvent
.
getApplicationAttemptFinishData
(
)
)
;
LOG
.
info
(
+
waafEvent
.
getApplicationAttemptId
(
)
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
waafEvent
.
getApplicationAttemptId
(
)
)
;
}
break
;
case
CONTAINER_START
:
WritingContainerStartEvent
wcsEvent
=
(
WritingContainerStartEvent
)
event
;
private
void
launch
(
)
throws
IOException
,
YarnException
{
connect
(
)
;
ContainerId
masterContainerID
=
masterContainer
.
getId
(
)
;
ApplicationSubmissionContext
applicationContext
=
application
.
getSubmissionContext
(
)
;
private
void
launch
(
)
throws
IOException
,
YarnException
{
connect
(
)
;
ContainerId
masterContainerID
=
masterContainer
.
getId
(
)
;
ApplicationSubmissionContext
applicationContext
=
application
.
getSubmissionContext
(
)
;
LOG
.
info
(
+
masterContainer
+
+
application
.
getAppAttemptId
(
)
)
;
ContainerLaunchContext
launchContext
=
createAMContainerLaunchContext
(
applicationContext
,
masterContainerID
)
;
StartContainerRequest
scRequest
=
StartContainerRequest
.
newInstance
(
launchContext
,
masterContainer
.
getContainerToken
(
)
)
;
List
<
StartContainerRequest
>
list
=
new
ArrayList
<
StartContainerRequest
>
(
)
;
list
.
add
(
scRequest
)
;
StartContainersRequest
allRequests
=
StartContainersRequest
.
newInstance
(
list
)
;
StartContainersResponse
response
=
containerMgrProxy
.
startContainers
(
allRequests
)
;
if
(
response
.
getFailedRequests
(
)
!=
null
&&
response
.
getFailedRequests
(
)
.
containsKey
(
masterContainerID
)
)
{
Throwable
t
=
response
.
getFailedRequests
(
)
.
get
(
masterContainerID
)
.
deSerialize
(
)
;
parseAndThrowException
(
t
)
;
}
else
{
LOG
.
info
(
+
application
.
getAppAttemptId
(
)
)
;
launch
(
)
;
handler
.
handle
(
new
RMAppAttemptEvent
(
application
.
getAppAttemptId
(
)
,
RMAppAttemptEventType
.
LAUNCHED
,
System
.
currentTimeMillis
(
)
)
)
;
}
catch
(
Exception
ie
)
{
onAMLaunchFailed
(
masterContainer
.
getId
(
)
,
ie
)
;
}
break
;
case
CLEANUP
:
try
{
LOG
.
info
(
+
application
.
getAppAttemptId
(
)
)
;
cleanup
(
)
;
}
catch
(
IOException
ie
)
{
LOG
.
info
(
,
ie
)
;
}
catch
(
YarnException
e
)
{
StringBuilder
sb
=
new
StringBuilder
(
)
;
sb
.
append
(
masterContainer
.
getId
(
)
.
toString
(
)
)
.
append
(
)
;
if
(
!
e
.
getMessage
(
)
.
contains
(
sb
.
toString
(
)
)
)
{
@
SuppressWarnings
(
)
protected
void
onAMLaunchFailed
(
ContainerId
containerId
,
Exception
ie
)
{
String
message
=
+
application
.
getAppAttemptId
(
)
+
+
StringUtils
.
stringifyException
(
ie
)
;
@
Override
public
ResourceBlacklistRequest
getBlacklistUpdates
(
)
{
ResourceBlacklistRequest
ret
;
List
<
String
>
blacklist
=
new
ArrayList
<
>
(
blacklistNodes
)
;
final
int
currentBlacklistSize
=
blacklist
.
size
(
)
;
final
double
failureThreshold
=
this
.
blacklistDisableFailureThreshold
*
numberOfNodeManagerHosts
;
if
(
currentBlacklistSize
<
failureThreshold
)
{
@
Override
public
synchronized
void
run
(
)
{
try
{
updateClusterState
(
)
;
SubClusterHeartbeatRequest
request
=
SubClusterHeartbeatRequest
.
newInstance
(
subClusterId
,
SubClusterState
.
SC_RUNNING
,
capability
)
;
stateStoreService
.
subClusterHeartbeat
(
request
)
;
private
void
registerAndInitializeHeartbeat
(
)
{
String
clientRMAddress
=
getServiceAddress
(
rmContext
.
getClientRMService
(
)
.
getBindAddress
(
)
)
;
String
amRMAddress
=
getServiceAddress
(
rmContext
.
getApplicationMasterService
(
)
.
getBindAddress
(
)
)
;
String
rmAdminAddress
=
getServiceAddress
(
config
.
getSocketAddr
(
YarnConfiguration
.
RM_ADMIN_ADDRESS
,
YarnConfiguration
.
DEFAULT_RM_ADMIN_ADDRESS
,
YarnConfiguration
.
DEFAULT_RM_ADMIN_PORT
)
)
;
String
webAppAddress
=
getServiceAddress
(
NetUtils
.
createSocketAddr
(
WebAppUtils
.
getRMWebAppURLWithScheme
(
config
)
)
)
;
SubClusterInfo
subClusterInfo
=
SubClusterInfo
.
newInstance
(
subClusterId
,
amRMAddress
,
clientRMAddress
,
rmAdminAddress
,
webAppAddress
,
SubClusterState
.
SC_NEW
,
ResourceManager
.
getClusterTimeStamp
(
)
,
)
;
try
{
registerSubCluster
(
SubClusterRegisterRequest
.
newInstance
(
subClusterInfo
)
)
;
private
void
putEntity
(
TimelineEntity
entity
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
putEntity
(
TimelineEntity
entity
,
ApplicationId
appId
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
putEntity
(
TimelineEntity
entity
,
ApplicationId
appId
)
{
try
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
entity
+
+
TimelineUtils
.
dumpTimelineRecordtoJSON
(
entity
)
)
;
}
TimelineCollector
timelineCollector
=
rmTimelineCollectorManager
.
get
(
appId
)
;
if
(
timelineCollector
!=
null
)
{
TimelineEntities
entities
=
new
TimelineEntities
(
)
;
entities
.
addEntity
(
entity
)
;
timelineCollector
.
putEntities
(
entities
,
UserGroupInformation
.
getCurrentUser
(
)
)
;
}
else
{
LOG
.
debug
(
+
entity
)
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
entity
)
;
LOG
.
debug
(
,
entity
,
e
)
;
}
catch
(
Exception
e
)
{
private
void
silentlyStopSchedulingMonitor
(
String
name
)
{
SchedulingMonitor
mon
=
runningSchedulingMonitors
.
get
(
name
)
;
try
{
mon
.
stop
(
)
;
@
Override
public
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
selectCandidates
(
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
selectedCandidates
,
Resource
clusterResource
,
Resource
totalPreemptionAllowed
)
{
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
curCandidates
=
new
HashMap
<
>
(
)
;
preemptableAmountCalculator
.
computeIdealAllocation
(
clusterResource
,
totalPreemptionAllowed
)
;
CapacitySchedulerPreemptionUtils
.
deductPreemptableResourcesBasedSelectedCandidates
(
preemptionContext
,
selectedCandidates
)
;
List
<
RMContainer
>
skippedAMContainerlist
=
new
ArrayList
<
>
(
)
;
for
(
String
queueName
:
preemptionContext
.
getLeafQueueNames
(
)
)
{
if
(
preemptionContext
.
getQueueByPartition
(
queueName
,
RMNodeLabelsManager
.
NO_LABEL
)
.
preemptionDisabled
)
{
Collection
<
FiCaSchedulerApp
>
apps
=
tq
.
leafQueue
.
getAllApplications
(
)
;
if
(
apps
.
size
(
)
==
1
)
{
return
;
}
PriorityQueue
<
TempAppPerPartition
>
orderedByPriority
=
createTempAppForResCalculation
(
tq
,
apps
,
clusterResource
,
perUserAMUsed
)
;
TreeSet
<
TempAppPerPartition
>
orderedApps
=
calculateIdealAssignedResourcePerApp
(
clusterResource
,
tq
,
selectedCandidates
,
queueReassignableResource
,
orderedByPriority
)
;
Resource
maxIntraQueuePreemptable
=
Resources
.
multiply
(
tq
.
getGuaranteed
(
)
,
maxAllowablePreemptLimit
)
;
if
(
Resources
.
greaterThan
(
rc
,
clusterResource
,
maxIntraQueuePreemptable
,
tq
.
getActuallyToBePreempted
(
)
)
)
{
Resources
.
subtractFrom
(
maxIntraQueuePreemptable
,
tq
.
getActuallyToBePreempted
(
)
)
;
}
else
{
maxIntraQueuePreemptable
=
Resource
.
newInstance
(
0
,
0
)
;
}
Resource
preemptionLimit
=
Resources
.
min
(
rc
,
clusterResource
,
maxIntraQueuePreemptable
,
totalPreemptedResourceAllowed
)
;
calculateToBePreemptedResourcePerApp
(
clusterResource
,
orderedApps
,
Resources
.
clone
(
preemptionLimit
)
)
;
tq
.
addAllApps
(
orderedApps
)
;
validateOutSameAppPriorityFromDemand
(
clusterResource
,
(
TreeSet
<
TempAppPerPartition
>
)
orderedApps
,
tq
.
getUsersPerPartition
(
)
,
context
.
getIntraQueuePreemptionOrderPolicy
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
return
;
}
PriorityQueue
<
TempAppPerPartition
>
orderedByPriority
=
createTempAppForResCalculation
(
tq
,
apps
,
clusterResource
,
perUserAMUsed
)
;
TreeSet
<
TempAppPerPartition
>
orderedApps
=
calculateIdealAssignedResourcePerApp
(
clusterResource
,
tq
,
selectedCandidates
,
queueReassignableResource
,
orderedByPriority
)
;
Resource
maxIntraQueuePreemptable
=
Resources
.
multiply
(
tq
.
getGuaranteed
(
)
,
maxAllowablePreemptLimit
)
;
if
(
Resources
.
greaterThan
(
rc
,
clusterResource
,
maxIntraQueuePreemptable
,
tq
.
getActuallyToBePreempted
(
)
)
)
{
Resources
.
subtractFrom
(
maxIntraQueuePreemptable
,
tq
.
getActuallyToBePreempted
(
)
)
;
}
else
{
maxIntraQueuePreemptable
=
Resource
.
newInstance
(
0
,
0
)
;
}
Resource
preemptionLimit
=
Resources
.
min
(
rc
,
clusterResource
,
maxIntraQueuePreemptable
,
totalPreemptedResourceAllowed
)
;
calculateToBePreemptedResourcePerApp
(
clusterResource
,
orderedApps
,
Resources
.
clone
(
preemptionLimit
)
)
;
tq
.
addAllApps
(
orderedApps
)
;
validateOutSameAppPriorityFromDemand
(
clusterResource
,
(
TreeSet
<
TempAppPerPartition
>
)
orderedApps
,
tq
.
getUsersPerPartition
(
)
,
context
.
getIntraQueuePreemptionOrderPolicy
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
tq
.
queueName
+
+
tq
.
partition
)
;
for
(
TempAppPerPartition
tmpApp
:
tq
.
getApps
(
)
)
{
used
=
(
used
==
null
)
?
Resources
.
createResource
(
0
,
0
)
:
used
;
amUsed
=
(
amUsed
==
null
)
?
Resources
.
createResource
(
0
,
0
)
:
amUsed
;
pending
=
(
pending
==
null
)
?
Resources
.
createResource
(
0
,
0
)
:
pending
;
reserved
=
(
reserved
==
null
)
?
Resources
.
createResource
(
0
,
0
)
:
reserved
;
HashSet
<
String
>
partitions
=
new
HashSet
<
String
>
(
app
.
getAppAttemptResourceUsage
(
)
.
getNodePartitionsSet
(
)
)
;
partitions
.
addAll
(
app
.
getTotalPendingRequestsPerPartition
(
)
.
keySet
(
)
)
;
TempAppPerPartition
tmpApp
=
new
TempAppPerPartition
(
app
,
Resources
.
clone
(
used
)
,
Resources
.
clone
(
amUsed
)
,
Resources
.
clone
(
reserved
)
,
Resources
.
clone
(
pending
)
)
;
tmpApp
.
idealAssigned
=
Resources
.
createResource
(
0
,
0
)
;
String
userName
=
app
.
getUser
(
)
;
TempUserPerPartition
tmpUser
=
usersPerPartition
.
get
(
userName
)
;
if
(
tmpUser
==
null
)
{
ResourceUsage
userResourceUsage
=
tq
.
leafQueue
.
getUser
(
userName
)
.
getResourceUsage
(
)
;
Resource
userSpecificAmUsed
=
perUserAMUsed
.
get
(
userName
)
;
amUsed
=
(
userSpecificAmUsed
==
null
)
?
Resources
.
none
(
)
:
userSpecificAmUsed
;
tmpUser
=
new
TempUserPerPartition
(
tq
.
leafQueue
.
getUser
(
userName
)
,
tq
.
queueName
,
Resources
.
clone
(
userResourceUsage
.
getUsed
(
partition
)
)
,
Resources
.
clone
(
amUsed
)
,
Resources
.
clone
(
userResourceUsage
.
getReserved
(
partition
)
)
,
Resources
.
none
(
)
)
;
private
void
initializeUsageAndUserLimitForCompute
(
Resource
clusterResource
,
String
partition
,
LeafQueue
leafQueue
,
Map
<
String
,
Resource
>
rollingResourceUsagePerUser
)
{
for
(
String
user
:
leafQueue
.
getAllUsers
(
)
)
{
rollingResourceUsagePerUser
.
put
(
user
,
Resources
.
clone
(
leafQueue
.
getUser
(
user
)
.
getResourceUsage
(
)
.
getUsed
(
partition
)
)
)
;
private
void
preemptFromLeastStarvedApp
(
LeafQueue
leafQueue
,
FiCaSchedulerApp
app
,
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
selectedCandidates
,
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
curCandidates
,
Resource
clusterResource
,
Resource
totalPreemptedResourceAllowed
,
Map
<
String
,
Resource
>
resToObtainByPartition
,
Map
<
String
,
Resource
>
rollingResourceUsagePerUser
)
{
List
<
RMContainer
>
liveContainers
=
new
ArrayList
<
>
(
app
.
getLiveContainers
(
)
)
;
sortContainers
(
liveContainers
)
;
private
void
preemptFromLeastStarvedApp
(
LeafQueue
leafQueue
,
FiCaSchedulerApp
app
,
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
selectedCandidates
,
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
curCandidates
,
Resource
clusterResource
,
Resource
totalPreemptedResourceAllowed
,
Map
<
String
,
Resource
>
resToObtainByPartition
,
Map
<
String
,
Resource
>
rollingResourceUsagePerUser
)
{
List
<
RMContainer
>
liveContainers
=
new
ArrayList
<
>
(
app
.
getLiveContainers
(
)
)
;
sortContainers
(
liveContainers
)
;
LOG
.
debug
(
+
,
totalPreemptedResourceAllowed
)
;
Resource
rollingUsedResourcePerUser
=
rollingResourceUsagePerUser
.
get
(
app
.
getUser
(
)
)
;
for
(
RMContainer
c
:
liveContainers
)
{
if
(
resToObtainByPartition
.
isEmpty
(
)
)
{
return
;
}
if
(
CapacitySchedulerPreemptionUtils
.
isContainerAlreadySelected
(
c
,
selectedCandidates
)
)
{
continue
;
}
if
(
null
!=
preemptionContext
.
getKillableContainers
(
)
&&
preemptionContext
.
getKillableContainers
(
)
.
contains
(
c
.
getContainerId
(
)
)
)
{
continue
;
}
if
(
c
.
isAMContainer
(
)
)
{
continue
;
}
if
(
fifoPreemptionComputePlugin
.
skipContainerBasedOnIntraQueuePolicy
(
app
,
clusterResource
,
rollingUsedResourcePerUser
,
c
)
)
{
private
void
calculateResToObtainByPartitionForLeafQueues
(
Set
<
String
>
leafQueueNames
,
Resource
clusterResource
)
{
for
(
String
queueName
:
leafQueueNames
)
{
if
(
context
.
getQueueByPartition
(
queueName
,
RMNodeLabelsManager
.
NO_LABEL
)
.
preemptionDisabled
)
{
candidatesSelectionPolicies
.
add
(
new
QueuePriorityContainerCandidateSelector
(
this
)
)
;
}
boolean
selectCandidatesForResevedContainers
=
config
.
getBoolean
(
CapacitySchedulerConfiguration
.
PREEMPTION_SELECT_CANDIDATES_FOR_RESERVED_CONTAINERS
,
CapacitySchedulerConfiguration
.
DEFAULT_PREEMPTION_SELECT_CANDIDATES_FOR_RESERVED_CONTAINERS
)
;
if
(
selectCandidatesForResevedContainers
)
{
candidatesSelectionPolicies
.
add
(
new
ReservedContainerCandidatesSelector
(
this
)
)
;
}
boolean
additionalPreemptionBasedOnReservedResource
=
config
.
getBoolean
(
CapacitySchedulerConfiguration
.
ADDITIONAL_RESOURCE_BALANCE_BASED_ON_RESERVED_CONTAINERS
,
CapacitySchedulerConfiguration
.
DEFAULT_ADDITIONAL_RESOURCE_BALANCE_BASED_ON_RESERVED_CONTAINERS
)
;
candidatesSelectionPolicies
.
add
(
new
FifoCandidatesSelector
(
this
,
additionalPreemptionBasedOnReservedResource
,
false
)
)
;
boolean
isPreemptionToBalanceRequired
=
config
.
getBoolean
(
CapacitySchedulerConfiguration
.
PREEMPTION_TO_BALANCE_QUEUES_BEYOND_GUARANTEED
,
CapacitySchedulerConfiguration
.
DEFAULT_PREEMPTION_TO_BALANCE_QUEUES_BEYOND_GUARANTEED
)
;
long
maximumKillWaitTimeForPreemptionToQueueBalance
=
config
.
getLong
(
CapacitySchedulerConfiguration
.
MAX_WAIT_BEFORE_KILL_FOR_QUEUE_BALANCE_PREEMPTION
,
CapacitySchedulerConfiguration
.
DEFAULT_MAX_WAIT_BEFORE_KILL_FOR_QUEUE_BALANCE_PREEMPTION
)
;
if
(
isPreemptionToBalanceRequired
)
{
PreemptionCandidatesSelector
selector
=
new
FifoCandidatesSelector
(
this
,
false
,
true
)
;
selector
.
setMaximumKillWaitTime
(
maximumKillWaitTimeForPreemptionToQueueBalance
)
;
candidatesSelectionPolicies
.
add
(
selector
)
;
}
boolean
isIntraQueuePreemptionEnabled
=
config
.
getBoolean
(
CapacitySchedulerConfiguration
.
INTRAQUEUE_PREEMPTION_ENABLED
,
CapacitySchedulerConfiguration
.
DEFAULT_INTRAQUEUE_PREEMPTION_ENABLED
)
;
if
(
isIntraQueuePreemptionEnabled
)
{
candidatesSelectionPolicies
.
add
(
new
IntraQueueCandidatesSelector
(
this
)
)
;
for
(
String
partitionToLookAt
:
allPartitions
)
{
cloneQueues
(
root
,
Resources
.
clone
(
nlm
.
getResourceByLabel
(
partitionToLookAt
,
clusterResources
)
)
,
partitionToLookAt
)
;
}
}
this
.
leafQueueNames
=
ImmutableSet
.
copyOf
(
getLeafQueueNames
(
getQueueByPartition
(
CapacitySchedulerConfiguration
.
ROOT
,
RMNodeLabelsManager
.
NO_LABEL
)
)
)
;
Resource
totalPreemptionAllowed
=
Resources
.
multiply
(
clusterResources
,
percentageClusterPreemptionAllowed
)
;
partitionToUnderServedQueues
.
clear
(
)
;
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
toPreempt
=
new
HashMap
<
>
(
)
;
Map
<
PreemptionCandidatesSelector
,
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>>
toPreemptPerSelector
=
new
HashMap
<
>
(
)
;
;
for
(
PreemptionCandidatesSelector
selector
:
candidatesSelectionPolicies
)
{
long
startTime
=
0
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
MessageFormat
.
format
(
,
selector
.
getClass
(
)
.
getName
(
)
)
)
;
startTime
=
clock
.
getTime
(
)
;
}
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
curCandidates
=
selector
.
selectCandidates
(
toPreempt
,
clusterResources
,
totalPreemptionAllowed
)
;
toPreemptPerSelector
.
putIfAbsent
(
selector
,
curCandidates
)
;
Map
<
PreemptionCandidatesSelector
,
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>>
toPreemptPerSelector
=
new
HashMap
<
>
(
)
;
;
for
(
PreemptionCandidatesSelector
selector
:
candidatesSelectionPolicies
)
{
long
startTime
=
0
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
MessageFormat
.
format
(
,
selector
.
getClass
(
)
.
getName
(
)
)
)
;
startTime
=
clock
.
getTime
(
)
;
}
Map
<
ApplicationAttemptId
,
Set
<
RMContainer
>>
curCandidates
=
selector
.
selectCandidates
(
toPreempt
,
clusterResources
,
totalPreemptionAllowed
)
;
toPreemptPerSelector
.
putIfAbsent
(
selector
,
curCandidates
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
MessageFormat
.
format
(
,
selector
.
getClass
(
)
.
getName
(
)
,
clock
.
getTime
(
)
-
startTime
)
)
;
int
totalSelected
=
0
;
int
curSelected
=
0
;
for
(
Set
<
RMContainer
>
set
:
toPreempt
.
values
(
)
)
{
totalSelected
+=
set
.
size
(
)
;
for
(
String
q2
:
preemptionContext
.
getLeafQueueNames
(
)
)
{
if
(
q1
.
compareTo
(
q2
)
<
0
)
{
TempQueuePerPartition
tq1
=
preemptionContext
.
getQueueByPartition
(
q1
,
RMNodeLabelsManager
.
NO_LABEL
)
;
TempQueuePerPartition
tq2
=
preemptionContext
.
getQueueByPartition
(
q2
,
RMNodeLabelsManager
.
NO_LABEL
)
;
List
<
TempQueuePerPartition
>
path1
=
getPathToRoot
(
tq1
)
;
List
<
TempQueuePerPartition
>
path2
=
getPathToRoot
(
tq2
)
;
int
i
=
path1
.
size
(
)
-
1
;
int
j
=
path2
.
size
(
)
-
1
;
while
(
path1
.
get
(
i
)
.
queueName
.
equals
(
path2
.
get
(
j
)
.
queueName
)
)
{
i
--
;
j
--
;
}
int
p1
=
path1
.
get
(
i
)
.
relativePriority
;
int
p2
=
path2
.
get
(
j
)
.
relativePriority
;
if
(
p1
<
p2
)
{
priorityDigraph
.
put
(
q2
,
q1
,
true
)
;
TempQueuePerPartition
tq2
=
preemptionContext
.
getQueueByPartition
(
q2
,
RMNodeLabelsManager
.
NO_LABEL
)
;
List
<
TempQueuePerPartition
>
path1
=
getPathToRoot
(
tq1
)
;
List
<
TempQueuePerPartition
>
path2
=
getPathToRoot
(
tq2
)
;
int
i
=
path1
.
size
(
)
-
1
;
int
j
=
path2
.
size
(
)
-
1
;
while
(
path1
.
get
(
i
)
.
queueName
.
equals
(
path2
.
get
(
j
)
.
queueName
)
)
{
i
--
;
j
--
;
}
int
p1
=
path1
.
get
(
i
)
.
relativePriority
;
int
p2
=
path2
.
get
(
j
)
.
relativePriority
;
if
(
p1
<
p2
)
{
priorityDigraph
.
put
(
q2
,
q1
,
true
)
;
LOG
.
debug
(
,
q2
,
q1
)
;
}
else
if
(
p2
<
p1
)
{
priorityDigraph
.
put
(
q1
,
q2
,
true
)
;
}
}
}
Collections
.
sort
(
reservedContainers
,
CONTAINER_CREATION_TIME_COMPARATOR
)
;
long
currentTime
=
System
.
currentTimeMillis
(
)
;
for
(
RMContainer
reservedContainer
:
reservedContainers
)
{
if
(
currentTime
-
reservedContainer
.
getCreationTime
(
)
<
minTimeout
)
{
continue
;
}
FiCaSchedulerNode
node
=
preemptionContext
.
getScheduler
(
)
.
getNode
(
reservedContainer
.
getReservedNode
(
)
)
;
if
(
null
==
node
)
{
continue
;
}
List
<
RMContainer
>
newlySelectedToBePreemptContainers
=
new
ArrayList
<
>
(
)
;
String
demandingQueueName
=
reservedContainer
.
getQueueName
(
)
;
boolean
demandingQueueSatisfied
=
isQueueSatisfied
(
demandingQueueName
,
node
.
getPartition
(
)
)
;
boolean
canPreempt
=
false
;
if
(
!
demandingQueueSatisfied
)
{
canPreempt
=
canPreemptEnoughResourceForAsked
(
reservedContainer
.
getReservedResource
(
)
,
demandingQueueName
,
node
,
false
,
newlySelectedToBePreemptContainers
)
;
}
if
(
canPreempt
)
{
Collections
.
sort
(
sortedRunningContainers
,
new
Comparator
<
RMContainer
>
(
)
{
@
Override
public
int
compare
(
RMContainer
o1
,
RMContainer
o2
)
{
return
-
1
*
o1
.
getContainerId
(
)
.
compareTo
(
o2
.
getContainerId
(
)
)
;
}
}
)
;
boolean
canAllocateReservedContainer
=
false
;
Resource
cur
=
Resources
.
add
(
available
,
node
.
getTotalKillableResources
(
)
)
;
String
partition
=
node
.
getPartition
(
)
;
if
(
Resources
.
fitsIn
(
rc
,
reservedContainer
.
getReservedResource
(
)
,
cur
)
)
{
return
null
;
}
float
amPreemptionCost
=
0f
;
for
(
RMContainer
c
:
sortedRunningContainers
)
{
String
containerQueueName
=
c
.
getQueueName
(
)
;
if
(
killableContainers
.
containsKey
(
c
.
getContainerId
(
)
)
)
{
continue
;
}
if
(
c
.
isAMContainer
(
)
)
{
@
Override
public
void
init
(
Configuration
config
,
RMContext
rmContext
,
ResourceScheduler
scheduler
)
{
this
.
conf
=
config
;
this
.
context
=
rmContext
;
this
.
scheduler
=
scheduler
;
this
.
throwOnInvariantViolation
=
conf
.
getBoolean
(
InvariantsChecker
.
THROW_ON_VIOLATION
,
false
)
;
this
.
monitoringInterval
=
conf
.
getLong
(
InvariantsChecker
.
INVARIANT_MONITOR_INTERVAL
,
1000L
)
;
break
;
case
REPLACE
:
clusterAttributes
.
putAll
(
newAttributesToBeAdded
)
;
replaceNodeToAttribute
(
nodeHost
,
attributePrefix
,
node
.
getAttributes
(
)
,
attributes
)
;
node
.
replaceAttributes
(
attributes
,
attributePrefix
)
;
break
;
default
:
break
;
}
logMsg
.
append
(
)
.
append
(
entry
.
getKey
(
)
)
.
append
(
)
.
append
(
StringUtils
.
join
(
entry
.
getValue
(
)
.
keySet
(
)
,
)
)
.
append
(
)
;
}
LOG
.
debug
(
,
logMsg
)
;
if
(
null
!=
dispatcher
&&
NodeAttribute
.
PREFIX_CENTRALIZED
.
equals
(
attributePrefix
)
)
{
dispatcher
.
getEventHandler
(
)
.
handle
(
new
NodeAttributesStoreEvent
(
nodeAttributeMapping
,
op
)
)
;
}
Map
<
String
,
Set
<
NodeAttribute
>>
newNodeToAttributesMap
=
new
HashMap
<
String
,
Set
<
NodeAttribute
>>
(
)
;
nodeAttributeMapping
.
forEach
(
(
k
,
v
)
->
{
Host
node
=
nodeCollections
.
get
(
k
)
;
newNodeToAttributesMap
.
put
(
k
,
node
.
attributes
.
keySet
(
)
)
;
}
)
;
public
static
void
verifyCentralizedNodeLabelConfEnabled
(
String
operation
,
boolean
isCentralizedNodeLabelConfiguration
)
throws
IOException
{
if
(
!
isCentralizedNodeLabelConfiguration
)
{
String
msg
=
String
.
format
(
+
,
operation
)
;
}
QueueMapping
newMapping
=
validateAndGetAutoCreatedQueueMapping
(
queueManager
,
mapping
)
;
if
(
newMapping
==
null
)
{
throw
new
IOException
(
+
mapping
.
getQueue
(
)
)
;
}
newMappings
.
add
(
newMapping
)
;
}
else
{
QueueMapping
newMapping
=
validateAndGetQueueMapping
(
queueManager
,
queue
,
mapping
)
;
newMappings
.
add
(
newMapping
)
;
}
}
else
{
QueueMapping
newMapping
=
validateAndGetAutoCreatedQueueMapping
(
queueManager
,
mapping
)
;
if
(
newMapping
!=
null
)
{
newMappings
.
add
(
newMapping
)
;
}
else
{
newMappings
.
add
(
mapping
)
;
}
}
}
if
(
newMappings
.
size
(
)
>
0
)
{
this
.
mappings
=
newMappings
;
@
Override
public
ApplicationPlacementContext
getPlacementForApp
(
ApplicationSubmissionContext
asc
,
String
user
)
throws
YarnException
{
String
queueName
=
asc
.
getQueue
(
)
;
String
applicationName
=
asc
.
getApplicationName
(
)
;
if
(
mappings
!=
null
&&
mappings
.
size
(
)
>
0
)
{
try
{
ApplicationPlacementContext
mappedQueue
=
getAppPlacementContext
(
user
,
applicationName
)
;
if
(
mappedQueue
!=
null
)
{
if
(
queueName
.
equals
(
YarnConfiguration
.
DEFAULT_QUEUE_NAME
)
||
queueName
.
equals
(
mappedQueue
.
getQueue
(
)
)
||
overrideWithQueueMappings
)
{
if
(
!
(
scheduler
instanceof
CapacityScheduler
)
)
{
throw
new
IOException
(
)
;
}
LOG
.
info
(
,
getClass
(
)
.
getSimpleName
(
)
)
;
CapacitySchedulerContext
csContext
=
(
CapacitySchedulerContext
)
scheduler
;
queueManager
=
csContext
.
getCapacitySchedulerQueueManager
(
)
;
CapacitySchedulerConfiguration
conf
=
csContext
.
getConfiguration
(
)
;
overrideWithQueueMappings
=
conf
.
getOverrideWithQueueMappings
(
)
;
if
(
groups
==
null
)
{
groups
=
Groups
.
getUserToGroupsMappingService
(
conf
)
;
}
MappingRuleValidationContext
validationContext
=
buildValidationContext
(
)
;
mappingRules
=
conf
.
getMappingRules
(
)
;
for
(
MappingRule
rule
:
mappingRules
)
{
try
{
rule
.
validate
(
validationContext
)
;
}
catch
(
YarnException
e
)
{
CapacitySchedulerContext
csContext
=
(
CapacitySchedulerContext
)
scheduler
;
queueManager
=
csContext
.
getCapacitySchedulerQueueManager
(
)
;
CapacitySchedulerConfiguration
conf
=
csContext
.
getConfiguration
(
)
;
overrideWithQueueMappings
=
conf
.
getOverrideWithQueueMappings
(
)
;
if
(
groups
==
null
)
{
groups
=
Groups
.
getUserToGroupsMappingService
(
conf
)
;
}
MappingRuleValidationContext
validationContext
=
buildValidationContext
(
)
;
mappingRules
=
conf
.
getMappingRules
(
)
;
for
(
MappingRule
rule
:
mappingRules
)
{
try
{
rule
.
validate
(
validationContext
)
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
+
,
rule
,
e
.
getMessage
(
)
)
;
if
(
failOnConfigError
)
{
throw
new
IOException
(
e
)
;
@
Override
public
ApplicationPlacementContext
getPlacementForApp
(
ApplicationSubmissionContext
asc
,
String
user
)
throws
YarnException
{
String
appQueue
=
asc
.
getQueue
(
)
;
if
(
appQueue
!=
null
&&
!
appQueue
.
equals
(
YarnConfiguration
.
DEFAULT_QUEUE_NAME
)
&&
!
appQueue
.
equals
(
YarnConfiguration
.
DEFAULT_QUEUE_FULL_NAME
)
&&
!
overrideWithQueueMappings
)
{
VariableContext
variables
;
try
{
variables
=
createVariableContext
(
asc
,
user
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
throw
new
YarnException
(
e
)
;
}
for
(
MappingRule
rule
:
mappingRules
)
{
MappingRuleResult
result
=
evaluateRule
(
rule
,
variables
)
;
switch
(
result
.
getResult
(
)
)
{
case
PLACE_TO_DEFAULT
:
return
placeToDefault
(
asc
,
variables
,
rule
)
;
case
PLACE
:
return
placeToQueue
(
asc
,
rule
,
result
)
;
case
REJECT
:
LOG
.
info
(
+
,
asc
.
getApplicationName
(
)
,
rule
)
;
throw
new
YarnException
(
+
)
;
case
SKIP
:
break
;
default
:
LOG
.
error
(
,
result
)
;
private
ApplicationPlacementContext
placeToQueue
(
ApplicationSubmissionContext
asc
,
MappingRule
rule
,
MappingRuleResult
result
)
{
private
ApplicationPlacementContext
placeToDefault
(
ApplicationSubmissionContext
asc
,
VariableContext
variables
,
MappingRule
rule
)
throws
YarnException
{
try
{
String
queueName
=
validateAndNormalizeQueue
(
variables
.
replacePathVariables
(
)
)
;
@
Override
public
void
setConfig
(
Element
conf
)
{
createQueue
=
getCreateFlag
(
conf
)
;
if
(
conf
!=
null
)
{
defaultQueueName
=
conf
.
getAttribute
(
)
;
if
(
!
isValidQueueName
(
defaultQueueName
)
)
{
@
Override
public
void
setConfig
(
Boolean
create
)
{
createQueue
=
create
;
defaultQueueName
=
assureRoot
(
YarnConfiguration
.
DEFAULT_QUEUE_NAME
)
;
public
static
PlacementRule
getPlacementRule
(
String
ruleStr
,
Configuration
conf
)
throws
ClassNotFoundException
{
Class
<
?
extends
PlacementRule
>
ruleClass
=
Class
.
forName
(
ruleStr
)
.
asSubclass
(
PlacementRule
.
class
)
;
public
static
PlacementRule
getPlacementRule
(
Class
<
?
extends
PlacementRule
>
ruleClass
,
Object
initArg
)
{
@
Override
public
ApplicationPlacementContext
getPlacementForApp
(
ApplicationSubmissionContext
asc
,
String
user
)
throws
YarnException
{
final
Set
<
String
>
groupSet
;
try
{
groupSet
=
groupProvider
.
getGroupsSet
(
user
)
;
}
catch
(
IOException
ioe
)
{
throw
new
YarnException
(
,
ioe
)
;
}
String
parentQueue
=
null
;
PlacementRule
parentRule
=
getParentRule
(
)
;
if
(
parentRule
!=
null
)
{
LOG
.
debug
(
,
parentRule
.
getName
(
)
)
;
ApplicationPlacementContext
parent
=
parentRule
.
getPlacementForApp
(
asc
,
user
)
;
if
(
parent
==
null
||
getQueueManager
(
)
.
getQueue
(
parent
.
getQueue
(
)
)
instanceof
FSLeafQueue
)
{
LOG
.
debug
(
)
;
return
null
;
}
parentQueue
=
parent
.
getQueue
(
)
;
@
Override
public
ApplicationPlacementContext
getPlacementForApp
(
ApplicationSubmissionContext
asc
,
String
user
)
throws
YarnException
{
String
queueName
=
asc
.
getQueue
(
)
;
if
(
!
isValidQueueName
(
queueName
)
)
{
private
ApplicationPlacementContext
getPlacementForUser
(
String
user
)
throws
IOException
{
for
(
QueueMapping
mapping
:
mappings
)
{
if
(
mapping
.
getType
(
)
.
equals
(
MappingType
.
USER
)
)
{
if
(
mapping
.
getSource
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
mapping
.
getParentQueue
(
)
!=
null
&&
mapping
.
getParentQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
&&
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
mapping
.
getSource
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
mapping
.
getParentQueue
(
)
!=
null
&&
mapping
.
getParentQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
&&
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getContextForGroupParent
(
user
,
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getParentQueue
(
)
!=
null
&&
mapping
.
getParentQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
&&
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getContextForGroupParent
(
user
,
mapping
,
getSecondaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
user
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
return
getContextForGroupParent
(
user
,
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getParentQueue
(
)
!=
null
&&
mapping
.
getParentQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
&&
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getContextForGroupParent
(
user
,
mapping
,
getSecondaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
user
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getContextForGroupParent
(
user
,
mapping
,
getSecondaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
user
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getSecondaryGroup
(
user
)
)
;
else
if
(
mapping
.
getQueue
(
)
.
equals
(
CURRENT_USER_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
user
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getSecondaryGroup
(
user
)
)
;
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
else
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getSecondaryGroup
(
user
)
)
;
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
)
;
}
}
if
(
user
.
equals
(
mapping
.
getSource
(
)
)
)
{
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getSecondaryGroup
(
user
)
)
;
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
)
;
}
}
if
(
user
.
equals
(
mapping
.
getSource
(
)
)
)
{
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getSecondaryGroup
(
user
)
)
;
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
)
;
}
}
if
(
user
.
equals
(
mapping
.
getSource
(
)
)
)
{
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
String
secondaryGroup
=
getSecondaryGroup
(
user
)
;
if
(
secondaryGroup
!=
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
return
getPlacementContext
(
mapping
)
;
}
}
if
(
user
.
equals
(
mapping
.
getSource
(
)
)
)
{
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
String
secondaryGroup
=
getSecondaryGroup
(
user
)
;
if
(
secondaryGroup
!=
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
secondaryGroup
)
;
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
user
)
;
}
if
(
user
.
equals
(
mapping
.
getSource
(
)
)
)
{
if
(
mapping
.
getQueue
(
)
.
equals
(
PRIMARY_GROUP_MAPPING
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
getPrimaryGroup
(
user
)
)
;
}
else
if
(
mapping
.
getQueue
(
)
.
equals
(
SECONDARY_GROUP_MAPPING
)
)
{
String
secondaryGroup
=
getSecondaryGroup
(
user
)
;
if
(
secondaryGroup
!=
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
user
)
;
}
return
getPlacementContext
(
mapping
,
secondaryGroup
)
;
}
else
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
,
user
)
;
}
return
null
;
@
Override
public
ApplicationPlacementContext
getPlacementForApp
(
ApplicationSubmissionContext
asc
,
String
user
)
throws
YarnException
{
String
queueName
=
asc
.
getQueue
(
)
;
ApplicationId
applicationId
=
asc
.
getApplicationId
(
)
;
if
(
mappings
!=
null
&&
mappings
.
size
(
)
>
0
)
{
try
{
ApplicationPlacementContext
mappedQueue
=
getPlacementForUser
(
user
)
;
if
(
mappedQueue
!=
null
)
{
if
(
queueName
.
equals
(
YarnConfiguration
.
DEFAULT_QUEUE_NAME
)
||
queueName
.
equals
(
mappedQueue
.
getQueue
(
)
)
||
overrideWithQueueMappings
)
{
@
Override
public
ApplicationPlacementContext
getPlacementForApp
(
ApplicationSubmissionContext
asc
,
String
user
)
throws
YarnException
{
String
queueName
;
String
cleanUser
=
cleanName
(
user
)
;
PlacementRule
parentRule
=
getParentRule
(
)
;
if
(
parentRule
!=
null
)
{
public
void
start
(
Configuration
conf
)
{
this
.
hostsFilePath
=
conf
.
get
(
YarnConfiguration
.
RM_SUBMISSION_PREPROCESSOR_FILE_PATH
,
YarnConfiguration
.
DEFAULT_RM_SUBMISSION_PREPROCESSOR_FILE_PATH
)
;
int
refreshPeriod
=
conf
.
getInt
(
YarnConfiguration
.
RM_SUBMISSION_PREPROCESSOR_REFRESH_INTERVAL_MS
,
YarnConfiguration
.
DEFAULT_RM_SUBMISSION_PREPROCESSOR_REFRESH_INTERVAL_MS
)
;
}
else
{
FileInputStream
fileInputStream
=
new
FileInputStream
(
hostFile
)
;
BufferedReader
reader
=
null
;
Map
<
String
,
Map
<
ContextProp
,
String
>>
tempHostCommands
=
new
HashMap
<
>
(
)
;
try
{
reader
=
new
BufferedReader
(
new
InputStreamReader
(
fileInputStream
,
StandardCharsets
.
UTF_8
)
)
;
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
String
[
]
commands
=
line
.
split
(
)
;
if
(
commands
!=
null
&&
commands
.
length
>
1
)
{
String
host
=
commands
[
0
]
.
trim
(
)
;
if
(
host
.
startsWith
(
)
)
{
continue
;
}
Map
<
ContextProp
,
String
>
cMap
=
null
;
for
(
int
i
=
1
;
i
<
commands
.
length
;
i
++
)
{
String
line
;
while
(
(
line
=
reader
.
readLine
(
)
)
!=
null
)
{
String
[
]
commands
=
line
.
split
(
)
;
if
(
commands
!=
null
&&
commands
.
length
>
1
)
{
String
host
=
commands
[
0
]
.
trim
(
)
;
if
(
host
.
startsWith
(
)
)
{
continue
;
}
Map
<
ContextProp
,
String
>
cMap
=
null
;
for
(
int
i
=
1
;
i
<
commands
.
length
;
i
++
)
{
String
[
]
cSplit
=
commands
[
i
]
.
split
(
)
;
if
(
cSplit
==
null
||
cSplit
.
length
!=
2
)
{
LOG
.
error
(
,
commands
[
i
]
)
;
continue
;
}
if
(
cMap
==
null
)
{
cMap
=
new
HashMap
<
>
(
)
;
private
boolean
checkAndRemovePartialRecord
(
Path
record
)
throws
IOException
{
if
(
record
.
getName
(
)
.
endsWith
(
)
)
{
@
Override
public
synchronized
void
storeApplicationStateInternal
(
ApplicationId
appId
,
ApplicationStateData
appStateDataPB
)
throws
Exception
{
Path
appDirPath
=
getAppDir
(
rmAppRoot
,
appId
)
;
mkdirsWithRetries
(
appDirPath
)
;
Path
nodeCreatePath
=
getNodePath
(
appDirPath
,
appId
.
toString
(
)
)
;
@
Override
public
synchronized
void
updateApplicationStateInternal
(
ApplicationId
appId
,
ApplicationStateData
appStateDataPB
)
throws
Exception
{
Path
appDirPath
=
getAppDir
(
rmAppRoot
,
appId
)
;
Path
nodeCreatePath
=
getNodePath
(
appDirPath
,
appId
.
toString
(
)
)
;
@
Override
public
synchronized
void
storeApplicationAttemptStateInternal
(
ApplicationAttemptId
appAttemptId
,
ApplicationAttemptStateData
attemptStateDataPB
)
throws
Exception
{
Path
appDirPath
=
getAppDir
(
rmAppRoot
,
appAttemptId
.
getApplicationId
(
)
)
;
Path
nodeCreatePath
=
getNodePath
(
appDirPath
,
appAttemptId
.
toString
(
)
)
;
@
Override
public
synchronized
void
updateApplicationAttemptStateInternal
(
ApplicationAttemptId
appAttemptId
,
ApplicationAttemptStateData
attemptStateDataPB
)
throws
Exception
{
Path
appDirPath
=
getAppDir
(
rmAppRoot
,
appAttemptId
.
getApplicationId
(
)
)
;
Path
nodeCreatePath
=
getNodePath
(
appDirPath
,
appAttemptId
.
toString
(
)
)
;
@
Override
public
synchronized
void
removeApplicationAttemptInternal
(
ApplicationAttemptId
appAttemptId
)
throws
Exception
{
Path
appDirPath
=
getAppDir
(
rmAppRoot
,
appAttemptId
.
getApplicationId
(
)
)
;
Path
nodeRemovePath
=
getNodePath
(
appDirPath
,
appAttemptId
.
toString
(
)
)
;
@
Override
public
synchronized
void
removeApplicationStateInternal
(
ApplicationStateData
appState
)
throws
Exception
{
ApplicationId
appId
=
appState
.
getApplicationSubmissionContext
(
)
.
getApplicationId
(
)
;
Path
nodeRemovePath
=
getAppDir
(
rmAppRoot
,
appId
)
;
@
Override
public
synchronized
void
removeRMDelegationTokenState
(
RMDelegationTokenIdentifier
identifier
)
throws
Exception
{
Path
nodeCreatePath
=
getNodePath
(
rmDTSecretManagerRoot
,
DELEGATION_TOKEN_PREFIX
+
identifier
.
getSequenceNumber
(
)
)
;
private
void
storeOrUpdateRMDelegationTokenState
(
RMDelegationTokenIdentifier
identifier
,
Long
renewDate
,
boolean
isUpdate
)
throws
Exception
{
Path
nodeCreatePath
=
getNodePath
(
rmDTSecretManagerRoot
,
DELEGATION_TOKEN_PREFIX
+
identifier
.
getSequenceNumber
(
)
)
;
RMDelegationTokenIdentifierData
identifierData
=
new
RMDelegationTokenIdentifierData
(
identifier
,
renewDate
)
;
if
(
isUpdate
)
{
@
Override
public
synchronized
void
storeRMDTMasterKeyState
(
DelegationKey
masterKey
)
throws
Exception
{
Path
nodeCreatePath
=
getNodePath
(
rmDTSecretManagerRoot
,
DELEGATION_KEY_PREFIX
+
masterKey
.
getKeyId
(
)
)
;
ByteArrayOutputStream
os
=
new
ByteArrayOutputStream
(
)
;
try
(
DataOutputStream
fsOut
=
new
DataOutputStream
(
os
)
)
{
@
Override
public
synchronized
void
removeRMDTMasterKeyState
(
DelegationKey
masterKey
)
throws
Exception
{
Path
nodeCreatePath
=
getNodePath
(
rmDTSecretManagerRoot
,
DELEGATION_KEY_PREFIX
+
masterKey
.
getKeyId
(
)
)
;
@
Override
protected
void
storeReservationState
(
ReservationAllocationStateProto
reservationAllocation
,
String
planName
,
String
reservationIdName
)
throws
Exception
{
Path
planCreatePath
=
getNodePath
(
reservationRoot
,
planName
)
;
mkdirsWithRetries
(
planCreatePath
)
;
Path
reservationPath
=
getNodePath
(
planCreatePath
,
reservationIdName
)
;
@
Override
protected
void
removeReservationState
(
String
planName
,
String
reservationIdName
)
throws
Exception
{
Path
planCreatePath
=
getNodePath
(
reservationRoot
,
planName
)
;
Path
reservationPath
=
getNodePath
(
planCreatePath
,
reservationIdName
)
;
@
Override
protected
void
startInternal
(
)
throws
Exception
{
Path
storeRoot
=
createStorageDir
(
)
;
Options
options
=
new
Options
(
)
;
options
.
createIfMissing
(
false
)
;
String
planReservationString
=
key
.
substring
(
RM_RESERVATION_KEY_PREFIX
.
length
(
)
)
;
String
[
]
parts
=
planReservationString
.
split
(
SEPARATOR
)
;
if
(
parts
.
length
!=
2
)
{
LOG
.
warn
(
+
key
)
;
continue
;
}
String
planName
=
parts
[
0
]
;
String
reservationName
=
parts
[
1
]
;
ReservationAllocationStateProto
allocationState
=
ReservationAllocationStateProto
.
parseFrom
(
entry
.
getValue
(
)
)
;
if
(
!
rmState
.
getReservationState
(
)
.
containsKey
(
planName
)
)
{
rmState
.
getReservationState
(
)
.
put
(
planName
,
new
HashMap
<
ReservationId
,
ReservationAllocationStateProto
>
(
)
)
;
}
ReservationId
reservationId
=
ReservationId
.
parseReservationId
(
reservationName
)
;
rmState
.
getReservationState
(
)
.
get
(
planName
)
.
put
(
reservationId
,
allocationState
)
;
numReservations
++
;
}
}
catch
(
DBException
e
)
{
throw
new
IOException
(
e
)
;
private
void
loadRMDTSecretManagerState
(
RMState
state
)
throws
IOException
{
int
numKeys
=
loadRMDTSecretManagerKeys
(
state
)
;
private
void
loadRMDTSecretManagerState
(
RMState
state
)
throws
IOException
{
int
numKeys
=
loadRMDTSecretManagerKeys
(
state
)
;
LOG
.
info
(
+
numKeys
+
)
;
int
numTokens
=
loadRMDTSecretManagerTokens
(
state
)
;
rmState
.
appState
.
put
(
appId
,
appState
)
;
String
attemptNodePrefix
=
getApplicationNodeKey
(
appId
)
+
SEPARATOR
;
while
(
iter
.
hasNext
(
)
)
{
Entry
<
byte
[
]
,
byte
[
]
>
entry
=
iter
.
peekNext
(
)
;
String
key
=
asString
(
entry
.
getKey
(
)
)
;
if
(
!
key
.
startsWith
(
attemptNodePrefix
)
)
{
break
;
}
String
attemptId
=
key
.
substring
(
attemptNodePrefix
.
length
(
)
)
;
if
(
attemptId
.
startsWith
(
ApplicationAttemptId
.
appAttemptIdStrPrefix
)
)
{
ApplicationAttemptStateData
attemptState
=
createAttemptState
(
attemptId
,
entry
.
getValue
(
)
)
;
appState
.
attempts
.
put
(
attemptState
.
getAttemptId
(
)
,
attemptState
)
;
}
else
{
LOG
.
warn
(
+
key
)
;
}
iter
.
next
(
)
;
}
int
numAttempts
=
appState
.
attempts
.
size
(
)
;
@
Override
protected
void
storeApplicationStateInternal
(
ApplicationId
appId
,
ApplicationStateData
appStateData
)
throws
IOException
{
String
key
=
getApplicationNodeKey
(
appId
)
;
@
Override
protected
void
storeApplicationAttemptStateInternal
(
ApplicationAttemptId
attemptId
,
ApplicationAttemptStateData
attemptStateData
)
throws
IOException
{
String
key
=
getApplicationAttemptNodeKey
(
attemptId
)
;
@
Override
public
synchronized
void
removeApplicationAttemptInternal
(
ApplicationAttemptId
attemptId
)
throws
IOException
{
String
attemptKey
=
getApplicationAttemptNodeKey
(
attemptId
)
;
@
Override
protected
void
storeReservationState
(
ReservationAllocationStateProto
reservationAllocation
,
String
planName
,
String
reservationIdName
)
throws
Exception
{
try
{
try
(
WriteBatch
batch
=
db
.
createWriteBatch
(
)
)
{
String
key
=
getReservationNodeKey
(
planName
,
reservationIdName
)
;
@
Override
protected
void
removeReservationState
(
String
planName
,
String
reservationIdName
)
throws
Exception
{
try
{
try
(
WriteBatch
batch
=
db
.
createWriteBatch
(
)
)
{
String
reservationKey
=
getReservationNodeKey
(
planName
,
reservationIdName
)
;
batch
.
delete
(
bytes
(
reservationKey
)
)
;
private
void
storeOrUpdateRMDT
(
RMDelegationTokenIdentifier
tokenId
,
Long
renewDate
,
boolean
isUpdate
)
throws
IOException
{
String
tokenKey
=
getRMDTTokenNodeKey
(
tokenId
)
;
RMDelegationTokenIdentifierData
tokenData
=
new
RMDelegationTokenIdentifierData
(
tokenId
,
renewDate
)
;
@
Override
protected
void
removeRMDelegationTokenState
(
RMDelegationTokenIdentifier
tokenId
)
throws
IOException
{
String
tokenKey
=
getRMDTTokenNodeKey
(
tokenId
)
;
@
Override
protected
void
storeRMDTMasterKeyState
(
DelegationKey
masterKey
)
throws
IOException
{
String
dbKey
=
getRMDTMasterKeyNodeKey
(
masterKey
)
;
@
Override
protected
void
removeRMDTMasterKeyState
(
DelegationKey
masterKey
)
throws
IOException
{
String
dbKey
=
getRMDTMasterKeyNodeKey
(
masterKey
)
;
@
Override
public
void
deleteStore
(
)
throws
IOException
{
Path
root
=
getStorageDir
(
)
;
@
Override
public
synchronized
void
removeApplication
(
ApplicationId
removeAppId
)
throws
IOException
{
String
appKey
=
getApplicationNodeKey
(
removeAppId
)
;
@
VisibleForTesting
int
getNumEntriesInDatabase
(
)
throws
IOException
{
int
numEntries
=
0
;
try
(
LeveldbIterator
iter
=
new
LeveldbIterator
(
db
)
)
{
iter
.
seekToFirst
(
)
;
while
(
iter
.
hasNext
(
)
)
{
Entry
<
byte
[
]
,
byte
[
]
>
entry
=
iter
.
next
(
)
;
@
Override
public
synchronized
void
updateApplicationStateInternal
(
ApplicationId
appId
,
ApplicationStateData
appState
)
throws
Exception
{
@
Override
public
synchronized
void
removeApplicationAttemptInternal
(
ApplicationAttemptId
appAttemptId
)
throws
Exception
{
ApplicationStateData
appState
=
state
.
getApplicationState
(
)
.
get
(
appAttemptId
.
getApplicationId
(
)
)
;
ApplicationAttemptStateData
attemptState
=
appState
.
attempts
.
remove
(
appAttemptId
)
;
private
void
storeOrUpdateRMDT
(
RMDelegationTokenIdentifier
rmDTIdentifier
,
Long
renewDate
,
boolean
isUpdate
)
throws
Exception
{
Map
<
RMDelegationTokenIdentifier
,
Long
>
rmDTState
=
state
.
rmSecretManagerState
.
getTokenState
(
)
;
if
(
rmDTState
.
containsKey
(
rmDTIdentifier
)
)
{
IOException
e
=
new
IOException
(
+
rmDTIdentifier
+
)
;
@
Override
public
synchronized
void
removeRMDelegationTokenState
(
RMDelegationTokenIdentifier
rmDTIdentifier
)
throws
Exception
{
Map
<
RMDelegationTokenIdentifier
,
Long
>
rmDTState
=
state
.
rmSecretManagerState
.
getTokenState
(
)
;
rmDTState
.
remove
(
rmDTIdentifier
)
;
@
Override
protected
synchronized
void
updateRMDelegationTokenState
(
RMDelegationTokenIdentifier
rmDTIdentifier
,
Long
renewDate
)
throws
Exception
{
removeRMDelegationTokenState
(
rmDTIdentifier
)
;
storeOrUpdateRMDT
(
rmDTIdentifier
,
renewDate
,
true
)
;
@
Override
public
synchronized
void
storeRMDTMasterKeyState
(
DelegationKey
delegationKey
)
throws
Exception
{
Set
<
DelegationKey
>
rmDTMasterKeyState
=
state
.
rmSecretManagerState
.
getMasterKeyState
(
)
;
if
(
rmDTMasterKeyState
.
contains
(
delegationKey
)
)
{
IOException
e
=
new
IOException
(
+
delegationKey
.
getKeyId
(
)
+
)
;
@
Override
public
synchronized
void
removeRMDTMasterKeyState
(
DelegationKey
delegationKey
)
throws
Exception
{
@
Override
protected
synchronized
void
storeReservationState
(
ReservationAllocationStateProto
reservationAllocation
,
String
planName
,
String
reservationIdName
)
throws
Exception
{
@
Override
protected
synchronized
void
removeReservationState
(
String
planName
,
String
reservationIdName
)
throws
Exception
{
public
void
checkVersion
(
)
throws
Exception
{
Version
loadedVersion
=
loadVersion
(
)
;
protected
void
handleStoreEvent
(
RMStateStoreEvent
event
)
{
this
.
writeLock
.
lock
(
)
;
try
{
protected
void
handleStoreEvent
(
RMStateStoreEvent
event
)
{
this
.
writeLock
.
lock
(
)
;
try
{
LOG
.
debug
(
,
event
.
getType
(
)
)
;
final
RMStateStoreState
oldState
=
getRMStateStoreState
(
)
;
this
.
stateMachine
.
doTransition
(
event
.
getType
(
)
,
event
)
;
if
(
oldState
!=
getRMStateStoreState
(
)
)
{
@
SuppressWarnings
(
)
private
boolean
notifyStoreOperationFailedInternal
(
Exception
failureCause
)
{
boolean
isFenced
=
false
;
public
static
RMStateStore
getStore
(
Configuration
conf
)
{
Class
<
?
extends
RMStateStore
>
storeClass
=
conf
.
getClass
(
YarnConfiguration
.
RM_STORE
,
MemoryRMStateStore
.
class
,
RMStateStore
.
class
)
;
}
fencingNodePath
=
getNodePath
(
zkRootNodePath
,
FENCING_LOCK
)
;
zkSessionTimeout
=
conf
.
getInt
(
YarnConfiguration
.
RM_ZK_TIMEOUT_MS
,
YarnConfiguration
.
DEFAULT_RM_ZK_TIMEOUT_MS
)
;
zknodeLimit
=
conf
.
getInt
(
YarnConfiguration
.
RM_ZK_ZNODE_SIZE_LIMIT_BYTES
,
YarnConfiguration
.
DEFAULT_RM_ZK_ZNODE_SIZE_LIMIT_BYTES
)
;
appIdNodeSplitIndex
=
conf
.
getInt
(
YarnConfiguration
.
ZK_APPID_NODE_SPLIT_INDEX
,
YarnConfiguration
.
DEFAULT_ZK_APPID_NODE_SPLIT_INDEX
)
;
if
(
appIdNodeSplitIndex
<
0
||
appIdNodeSplitIndex
>
4
)
{
LOG
.
info
(
+
appIdNodeSplitIndex
+
+
YarnConfiguration
.
ZK_APPID_NODE_SPLIT_INDEX
+
+
+
YarnConfiguration
.
DEFAULT_ZK_APPID_NODE_SPLIT_INDEX
)
;
appIdNodeSplitIndex
=
YarnConfiguration
.
DEFAULT_ZK_APPID_NODE_SPLIT_INDEX
;
}
zkAcl
=
ZKCuratorManager
.
getZKAcls
(
conf
)
;
if
(
HAUtil
.
isHAEnabled
(
conf
)
)
{
String
zkRootNodeAclConf
=
HAUtil
.
getConfValueForRMInstance
(
YarnConfiguration
.
ZK_RM_STATE_STORE_ROOT_NODE_ACL
,
conf
)
;
if
(
zkRootNodeAclConf
!=
null
)
{
zkRootNodeAclConf
=
ZKUtil
.
resolveConfIndirection
(
zkRootNodeAclConf
)
;
try
{
zkRootNodeAcl
=
ZKUtil
.
parseACLs
(
zkRootNodeAclConf
)
;
}
catch
(
ZKUtil
.
BadAclFormatException
bafe
)
{
private
void
loadReservationSystemState
(
RMState
rmState
)
throws
Exception
{
List
<
String
>
planNodes
=
getChildren
(
reservationRoot
)
;
for
(
String
planName
:
planNodes
)
{
private
void
loadReservationSystemState
(
RMState
rmState
)
throws
Exception
{
List
<
String
>
planNodes
=
getChildren
(
reservationRoot
)
;
for
(
String
planName
:
planNodes
)
{
LOG
.
debug
(
,
planName
)
;
String
planNodePath
=
getNodePath
(
reservationRoot
,
planName
)
;
List
<
String
>
reservationNodes
=
getChildren
(
planNodePath
)
;
for
(
String
reservationNodeName
:
reservationNodes
)
{
String
reservationNodePath
=
getNodePath
(
planNodePath
,
reservationNodeName
)
;
if
(
tokenRoot
==
null
)
{
continue
;
}
List
<
String
>
childNodes
=
getChildren
(
tokenRoot
)
;
boolean
dtNodeFound
=
false
;
for
(
String
childNodeName
:
childNodes
)
{
if
(
childNodeName
.
startsWith
(
DELEGATION_TOKEN_PREFIX
)
)
{
dtNodeFound
=
true
;
String
parentNodePath
=
getNodePath
(
tokenRoot
,
childNodeName
)
;
if
(
splitIndex
==
0
)
{
loadDelegationTokenFromNode
(
rmState
,
parentNodePath
)
;
}
else
{
List
<
String
>
leafNodes
=
getChildren
(
parentNodePath
)
;
for
(
String
leafNodeName
:
leafNodes
)
{
loadDelegationTokenFromNode
(
rmState
,
getNodePath
(
parentNodePath
,
leafNodeName
)
)
;
}
}
}
else
if
(
splitIndex
==
0
&&
!
(
childNodeName
.
equals
(
)
||
childNodeName
.
equals
(
)
||
childNodeName
.
equals
(
)
||
childNodeName
.
equals
(
)
)
)
{
private
void
loadRMAppStateFromAppNode
(
RMState
rmState
,
String
appNodePath
,
String
appIdStr
)
throws
Exception
{
byte
[
]
appData
=
getData
(
appNodePath
)
;
continue
;
}
List
<
String
>
childNodes
=
getChildren
(
appRoot
)
;
boolean
appNodeFound
=
false
;
for
(
String
childNodeName
:
childNodes
)
{
if
(
childNodeName
.
startsWith
(
ApplicationId
.
appIdStrPrefix
)
)
{
appNodeFound
=
true
;
if
(
splitIndex
==
0
)
{
loadRMAppStateFromAppNode
(
rmState
,
getNodePath
(
appRoot
,
childNodeName
)
,
childNodeName
)
;
}
else
{
String
parentNodePath
=
getNodePath
(
appRoot
,
childNodeName
)
;
List
<
String
>
leafNodes
=
getChildren
(
parentNodePath
)
;
for
(
String
leafNodeName
:
leafNodes
)
{
String
appIdStr
=
childNodeName
+
leafNodeName
;
loadRMAppStateFromAppNode
(
rmState
,
getNodePath
(
parentNodePath
,
leafNodeName
)
,
appIdStr
)
;
}
}
}
else
if
(
!
childNodeName
.
equals
(
RM_APP_ROOT_HIERARCHIES
)
)
{
@
Override
public
synchronized
void
storeApplicationStateInternal
(
ApplicationId
appId
,
ApplicationStateData
appStateDataPB
)
throws
Exception
{
String
nodeCreatePath
=
getLeafAppIdNodePath
(
appId
.
toString
(
)
,
true
)
;
ZnodeSplitInfo
alternatePathInfo
=
getAlternateAppPath
(
appId
.
toString
(
)
)
;
if
(
alternatePathInfo
!=
null
)
{
nodeUpdatePath
=
alternatePathInfo
.
path
;
}
else
{
pathExists
=
false
;
if
(
appIdNodeSplitIndex
!=
0
)
{
String
rootNode
=
getSplitZnodeParent
(
nodeUpdatePath
,
appIdNodeSplitIndex
)
;
if
(
!
exists
(
rootNode
)
)
{
zkManager
.
safeCreate
(
rootNode
,
null
,
zkAcl
,
CreateMode
.
PERSISTENT
,
zkAcl
,
fencingNodePath
)
;
}
}
}
}
LOG
.
debug
(
,
appId
,
nodeUpdatePath
)
;
byte
[
]
appStateData
=
appStateDataPB
.
getProto
(
)
.
toByteArray
(
)
;
if
(
pathExists
)
{
zkManager
.
safeSetData
(
nodeUpdatePath
,
appStateData
,
-
1
,
zkAcl
,
fencingNodePath
)
;
}
else
{
zkManager
.
safeCreate
(
nodeUpdatePath
,
appStateData
,
zkAcl
,
CreateMode
.
PERSISTENT
,
zkAcl
,
fencingNodePath
)
;
if
(
alternatePathInfo
==
null
)
{
if
(
operation
==
AppAttemptOp
.
REMOVE
)
{
return
;
}
else
{
throw
new
YarnRuntimeException
(
+
+
appId
+
)
;
}
}
else
{
appDirPath
=
alternatePathInfo
.
path
;
}
}
String
path
=
getNodePath
(
appDirPath
,
appAttemptId
.
toString
(
)
)
;
byte
[
]
attemptStateData
=
(
attemptStateDataPB
==
null
)
?
null
:
attemptStateDataPB
.
getProto
(
)
.
toByteArray
(
)
;
LOG
.
debug
(
,
operation
,
appAttemptId
,
path
)
;
switch
(
operation
)
{
case
UPDATE
:
if
(
exists
(
path
)
)
{
zkManager
.
safeSetData
(
path
,
attemptStateData
,
-
1
,
zkAcl
,
fencingNodePath
)
;
}
else
{
zkManager
.
safeCreate
(
path
,
attemptStateData
,
zkAcl
,
CreateMode
.
PERSISTENT
,
zkAcl
,
fencingNodePath
)
;
@
Override
protected
synchronized
void
storeRMDelegationTokenState
(
RMDelegationTokenIdentifier
rmDTIdentifier
,
Long
renewDate
)
throws
Exception
{
String
nodeCreatePath
=
getLeafDelegationTokenNodePath
(
rmDTIdentifier
.
getSequenceNumber
(
)
,
true
)
;
@
Override
protected
synchronized
void
storeRMDelegationTokenState
(
RMDelegationTokenIdentifier
rmDTIdentifier
,
Long
renewDate
)
throws
Exception
{
String
nodeCreatePath
=
getLeafDelegationTokenNodePath
(
rmDTIdentifier
.
getSequenceNumber
(
)
,
true
)
;
LOG
.
debug
(
,
DELEGATION_TOKEN_PREFIX
,
rmDTIdentifier
.
getSequenceNumber
(
)
)
;
RMDelegationTokenIdentifierData
identifierData
=
new
RMDelegationTokenIdentifierData
(
rmDTIdentifier
,
renewDate
)
;
ByteArrayOutputStream
seqOs
=
new
ByteArrayOutputStream
(
)
;
try
(
DataOutputStream
seqOut
=
new
DataOutputStream
(
seqOs
)
)
{
SafeTransaction
trx
=
zkManager
.
createTransaction
(
zkAcl
,
fencingNodePath
)
;
trx
.
create
(
nodeCreatePath
,
identifierData
.
toByteArray
(
)
,
zkAcl
,
CreateMode
.
PERSISTENT
)
;
seqOut
.
writeInt
(
rmDTIdentifier
.
getSequenceNumber
(
)
)
;
@
Override
protected
synchronized
void
storeRMDTMasterKeyState
(
DelegationKey
delegationKey
)
throws
Exception
{
String
nodeCreatePath
=
getNodePath
(
dtMasterKeysRootPath
,
DELEGATION_KEY_PREFIX
+
delegationKey
.
getKeyId
(
)
)
;
@
Override
protected
synchronized
void
removeRMDTMasterKeyState
(
DelegationKey
delegationKey
)
throws
Exception
{
String
nodeRemovePath
=
getNodePath
(
dtMasterKeysRootPath
,
DELEGATION_KEY_PREFIX
+
delegationKey
.
getKeyId
(
)
)
;
@
Override
protected
synchronized
void
removeReservationState
(
String
planName
,
String
reservationIdName
)
throws
Exception
{
String
planNodePath
=
getNodePath
(
reservationRoot
,
planName
)
;
String
reservationPath
=
getNodePath
(
planNodePath
,
reservationIdName
)
;
private
void
addOrUpdateReservationState
(
ReservationAllocationStateProto
reservationAllocation
,
String
planName
,
String
reservationIdName
,
SafeTransaction
trx
,
boolean
isUpdate
)
throws
Exception
{
String
planCreatePath
=
getNodePath
(
reservationRoot
,
planName
)
;
String
reservationPath
=
getNodePath
(
planCreatePath
,
reservationIdName
)
;
byte
[
]
reservationData
=
reservationAllocation
.
toByteArray
(
)
;
if
(
!
exists
(
planCreatePath
)
)
{
@
Override
public
ReservationId
getNewReservationId
(
)
{
writeLock
.
lock
(
)
;
try
{
ReservationId
resId
=
ReservationId
.
newInstance
(
ResourceManager
.
getClusterTimeStamp
(
)
,
resCounter
.
incrementAndGet
(
)
)
;
protected
Plan
initializePlan
(
String
planQueueName
)
throws
YarnException
{
String
planQueuePath
=
getPlanQueuePath
(
planQueueName
)
;
SharingPolicy
adPolicy
=
getAdmissionPolicy
(
planQueuePath
)
;
adPolicy
.
init
(
planQueuePath
,
getReservationSchedulerConfiguration
(
)
)
;
Resource
minAllocation
=
getMinAllocation
(
)
;
Resource
maxAllocation
=
getMaxAllocation
(
)
;
ResourceCalculator
rescCalc
=
getResourceCalculator
(
)
;
Resource
totCap
=
getPlanQueueCapacity
(
planQueueName
)
;
Plan
plan
=
new
InMemoryPlan
(
getRootQueueMetrics
(
)
,
adPolicy
,
getAgent
(
planQueuePath
)
,
totCap
,
planStepSize
,
rescCalc
,
minAllocation
,
maxAllocation
,
planQueueName
,
getReplanner
(
planQueuePath
)
,
getReservationSchedulerConfiguration
(
)
.
getMoveOnExpiry
(
planQueuePath
)
,
maxPeriodicity
,
rmContext
)
;
protected
Planner
getReplanner
(
String
planQueueName
)
{
ReservationSchedulerConfiguration
reservationConfig
=
getReservationSchedulerConfiguration
(
)
;
String
plannerClassName
=
reservationConfig
.
getReplanner
(
planQueueName
)
;
protected
ReservationAgent
getAgent
(
String
queueName
)
{
ReservationSchedulerConfiguration
reservationConfig
=
getReservationSchedulerConfiguration
(
)
;
String
agentClassName
=
reservationConfig
.
getReservationAgent
(
queueName
)
;
protected
SharingPolicy
getAdmissionPolicy
(
String
queueName
)
{
ReservationSchedulerConfiguration
reservationConfig
=
getReservationSchedulerConfiguration
(
)
;
String
admissionPolicyClassName
=
reservationConfig
.
getReservationAdmissionPolicy
(
queueName
)
;
@
Override
public
synchronized
void
synchronizePlan
(
Plan
plan
,
boolean
shouldReplan
)
{
String
planQueueName
=
plan
.
getQueueName
(
)
;
curReservationNames
.
remove
(
reservationId
)
;
}
else
{
expired
.
add
(
reservationId
)
;
}
}
cleanupExpiredQueues
(
planQueueName
,
plan
.
getMoveOnExpiry
(
)
,
expired
,
defReservationQueue
)
;
float
totalAssignedCapacity
=
0f
;
if
(
currentReservations
!=
null
)
{
try
{
setQueueEntitlement
(
planQueueName
,
defReservationQueue
,
0f
,
1.0f
)
;
}
catch
(
YarnException
e
)
{
LOG
.
warn
(
,
planQueueName
,
e
)
;
}
List
<
ReservationAllocation
>
sortedAllocations
=
sortByDelta
(
new
ArrayList
<
ReservationAllocation
>
(
currentReservations
)
,
now
,
plan
)
;
for
(
ReservationAllocation
res
:
sortedAllocations
)
{
String
currResId
=
res
.
getReservationId
(
)
.
toString
(
)
;
if
(
curReservationNames
.
contains
(
currResId
)
)
{
addReservationQueue
(
planQueueName
,
planQueue
,
currResId
)
;
try
{
setQueueEntitlement
(
planQueueName
,
defReservationQueue
,
0f
,
1.0f
)
;
}
catch
(
YarnException
e
)
{
LOG
.
warn
(
,
planQueueName
,
e
)
;
}
List
<
ReservationAllocation
>
sortedAllocations
=
sortByDelta
(
new
ArrayList
<
ReservationAllocation
>
(
currentReservations
)
,
now
,
plan
)
;
for
(
ReservationAllocation
res
:
sortedAllocations
)
{
String
currResId
=
res
.
getReservationId
(
)
.
toString
(
)
;
if
(
curReservationNames
.
contains
(
currResId
)
)
{
addReservationQueue
(
planQueueName
,
planQueue
,
currResId
)
;
}
Resource
capToAssign
=
res
.
getResourcesAtTime
(
now
)
;
float
targetCapacity
=
0f
;
if
(
planResources
.
getMemorySize
(
)
>
0
&&
planResources
.
getVirtualCores
(
)
>
0
)
{
if
(
shouldResize
)
{
capToAssign
=
calculateReservationToPlanProportion
(
plan
.
getResourceCalculator
(
)
,
planResources
,
reservedResources
,
capToAssign
)
;
}
targetCapacity
=
calculateReservationToPlanRatio
(
plan
.
getResourceCalculator
(
)
,
clusterResources
,
planResources
,
capToAssign
)
;
addReservationQueue
(
planQueueName
,
planQueue
,
currResId
)
;
}
Resource
capToAssign
=
res
.
getResourcesAtTime
(
now
)
;
float
targetCapacity
=
0f
;
if
(
planResources
.
getMemorySize
(
)
>
0
&&
planResources
.
getVirtualCores
(
)
>
0
)
{
if
(
shouldResize
)
{
capToAssign
=
calculateReservationToPlanProportion
(
plan
.
getResourceCalculator
(
)
,
planResources
,
reservedResources
,
capToAssign
)
;
}
targetCapacity
=
calculateReservationToPlanRatio
(
plan
.
getResourceCalculator
(
)
,
clusterResources
,
planResources
,
capToAssign
)
;
}
LOG
.
debug
(
,
capToAssign
,
currResId
,
targetCapacity
)
;
float
maxCapacity
=
1.0f
;
if
(
res
.
containsGangs
(
)
)
{
maxCapacity
=
targetCapacity
;
}
try
{
setQueueEntitlement
(
planQueueName
,
currResId
,
targetCapacity
,
maxCapacity
)
;
}
catch
(
YarnException
e
)
{
LOG
.
warn
(
,
currResId
,
planQueueName
,
e
)
;
@
Override
protected
Queue
getPlanQueue
(
String
planQueueName
)
{
Queue
planQueue
=
fs
.
getQueueManager
(
)
.
getParentQueue
(
planQueueName
,
false
)
;
if
(
planQueue
==
null
)
{
@
Override
public
boolean
addReservation
(
ReservationAllocation
reservation
,
boolean
isRecovering
)
throws
PlanningException
{
InMemoryReservationAllocation
inMemReservation
=
(
InMemoryReservationAllocation
)
reservation
;
if
(
inMemReservation
.
getUser
(
)
==
null
)
{
String
errMsg
=
+
inMemReservation
.
getReservationId
(
)
+
;
writeLock
.
lock
(
)
;
try
{
if
(
reservationTable
.
containsKey
(
inMemReservation
.
getReservationId
(
)
)
)
{
String
errMsg
=
+
inMemReservation
.
getReservationId
(
)
+
;
LOG
.
error
(
errMsg
)
;
throw
new
IllegalArgumentException
(
errMsg
)
;
}
if
(
!
isRecovering
)
{
policy
.
validate
(
this
,
inMemReservation
)
;
reservation
.
setAcceptanceTimestamp
(
clock
.
getTime
(
)
)
;
if
(
rmStateStore
!=
null
)
{
rmStateStore
.
storeNewReservation
(
ReservationSystemUtil
.
buildStateProto
(
inMemReservation
)
,
getQueueName
(
)
,
inMemReservation
.
getReservationId
(
)
.
toString
(
)
)
;
}
}
ReservationInterval
searchInterval
=
new
ReservationInterval
(
inMemReservation
.
getStartTime
(
)
,
inMemReservation
.
getEndTime
(
)
)
;
Set
<
InMemoryReservationAllocation
>
reservations
=
currentReservations
.
get
(
searchInterval
)
;
if
(
reservations
==
null
)
{
reservations
=
new
HashSet
<
InMemoryReservationAllocation
>
(
)
;
throw
new
IllegalArgumentException
(
errMsg
)
;
}
if
(
!
isRecovering
)
{
policy
.
validate
(
this
,
inMemReservation
)
;
reservation
.
setAcceptanceTimestamp
(
clock
.
getTime
(
)
)
;
if
(
rmStateStore
!=
null
)
{
rmStateStore
.
storeNewReservation
(
ReservationSystemUtil
.
buildStateProto
(
inMemReservation
)
,
getQueueName
(
)
,
inMemReservation
.
getReservationId
(
)
.
toString
(
)
)
;
}
}
ReservationInterval
searchInterval
=
new
ReservationInterval
(
inMemReservation
.
getStartTime
(
)
,
inMemReservation
.
getEndTime
(
)
)
;
Set
<
InMemoryReservationAllocation
>
reservations
=
currentReservations
.
get
(
searchInterval
)
;
if
(
reservations
==
null
)
{
reservations
=
new
HashSet
<
InMemoryReservationAllocation
>
(
)
;
}
if
(
!
reservations
.
add
(
inMemReservation
)
)
{
LOG
.
error
(
,
inMemReservation
.
getReservationId
(
)
)
;
return
false
;
}
currentReservations
.
put
(
searchInterval
,
reservations
)
;
reservationTable
.
put
(
inMemReservation
.
getReservationId
(
)
,
inMemReservation
)
;
@
Override
public
boolean
updateReservation
(
ReservationAllocation
reservation
)
throws
PlanningException
{
writeLock
.
lock
(
)
;
boolean
result
=
false
;
try
{
ReservationId
resId
=
reservation
.
getReservationId
(
)
;
ReservationAllocation
currReservation
=
getReservationById
(
resId
)
;
if
(
currReservation
==
null
)
{
String
errMsg
=
+
resId
+
;
boolean
result
=
false
;
try
{
ReservationId
resId
=
reservation
.
getReservationId
(
)
;
ReservationAllocation
currReservation
=
getReservationById
(
resId
)
;
if
(
currReservation
==
null
)
{
String
errMsg
=
+
resId
+
;
LOG
.
error
(
errMsg
)
;
throw
new
IllegalArgumentException
(
errMsg
)
;
}
policy
.
validate
(
this
,
reservation
)
;
if
(
!
removeReservation
(
currReservation
)
)
{
LOG
.
error
(
,
reservation
.
getReservationId
(
)
)
;
return
result
;
}
try
{
result
=
addReservation
(
reservation
,
false
)
;
}
catch
(
PlanningException
e
)
{
ReservationId
resId
=
reservation
.
getReservationId
(
)
;
ReservationAllocation
currReservation
=
getReservationById
(
resId
)
;
if
(
currReservation
==
null
)
{
String
errMsg
=
+
resId
+
;
LOG
.
error
(
errMsg
)
;
throw
new
IllegalArgumentException
(
errMsg
)
;
}
policy
.
validate
(
this
,
reservation
)
;
if
(
!
removeReservation
(
currReservation
)
)
{
LOG
.
error
(
,
reservation
.
getReservationId
(
)
)
;
return
result
;
}
try
{
result
=
addReservation
(
reservation
,
false
)
;
}
catch
(
PlanningException
e
)
{
LOG
.
error
(
,
reservation
.
getReservationId
(
)
,
e
.
getMessage
(
)
)
;
}
if
(
result
)
{
LOG
.
error
(
errMsg
)
;
throw
new
IllegalArgumentException
(
errMsg
)
;
}
policy
.
validate
(
this
,
reservation
)
;
if
(
!
removeReservation
(
currReservation
)
)
{
LOG
.
error
(
,
reservation
.
getReservationId
(
)
)
;
return
result
;
}
try
{
result
=
addReservation
(
reservation
,
false
)
;
}
catch
(
PlanningException
e
)
{
LOG
.
error
(
,
reservation
.
getReservationId
(
)
,
e
.
getMessage
(
)
)
;
}
if
(
result
)
{
LOG
.
info
(
,
reservation
.
getReservationId
(
)
)
;
return
result
;
}
else
{
addReservation
(
currReservation
,
false
)
;
Set
<
InMemoryReservationAllocation
>
reservations
=
currentReservations
.
get
(
searchInterval
)
;
if
(
reservations
!=
null
)
{
if
(
rmStateStore
!=
null
)
{
rmStateStore
.
removeReservation
(
getQueueName
(
)
,
reservation
.
getReservationId
(
)
.
toString
(
)
)
;
}
if
(
!
reservations
.
remove
(
reservation
)
)
{
LOG
.
error
(
,
reservation
.
getReservationId
(
)
)
;
return
false
;
}
if
(
reservations
.
isEmpty
(
)
)
{
currentReservations
.
remove
(
searchInterval
)
;
}
}
else
{
String
errMsg
=
+
reservation
.
getReservationId
(
)
+
;
LOG
.
error
(
errMsg
)
;
throw
new
IllegalArgumentException
(
errMsg
)
;
}
reservationTable
.
remove
(
reservation
.
getReservationId
(
)
)
;
decrementAllocation
(
reservation
)
;
@
Override
public
boolean
deleteReservation
(
ReservationId
reservationID
)
{
writeLock
.
lock
(
)
;
try
{
ReservationAllocation
reservation
=
getReservationById
(
reservationID
)
;
if
(
reservation
==
null
)
{
String
errMsg
=
+
reservationID
+
;
@
Override
public
void
archiveCompletedReservations
(
long
tick
)
{
@
Override
public
boolean
createReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
,
ReservationDefinition
contract
)
throws
PlanningException
{
@
Override
public
boolean
createReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
,
ReservationDefinition
contract
)
throws
PlanningException
{
LOG
.
info
(
+
contract
)
;
try
{
boolean
res
=
planner
.
createReservation
(
reservationId
,
user
,
plan
,
contract
)
;
if
(
res
)
{
@
Override
public
boolean
updateReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
,
ReservationDefinition
contract
)
throws
PlanningException
{
@
Override
public
boolean
deleteReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
)
throws
PlanningException
{
@
Override
public
void
init
(
Configuration
conf
)
{
allocateLeft
=
conf
.
getBoolean
(
FAVOR_EARLY_ALLOCATION
,
DEFAULT_GREEDY_FAVOR_EARLY_ALLOCATION
)
;
if
(
allocateLeft
)
{
@
Override
public
boolean
createReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
,
ReservationDefinition
contract
)
throws
PlanningException
{
@
Override
public
boolean
createReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
,
ReservationDefinition
contract
)
throws
PlanningException
{
LOG
.
info
(
+
contract
)
;
try
{
boolean
res
=
planner
.
createReservation
(
reservationId
,
user
,
plan
,
contract
)
;
if
(
res
)
{
@
Override
public
boolean
updateReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
,
ReservationDefinition
contract
)
throws
PlanningException
{
@
Override
public
boolean
deleteReservation
(
ReservationId
reservationId
,
String
user
,
Plan
plan
)
throws
PlanningException
{
public
void
setVcoresPerNode
(
String
node
,
int
vcores
)
{
setInt
(
getNodePrefix
(
node
)
+
VCORES
,
vcores
)
;
public
void
setMemoryPerNode
(
String
node
,
int
memory
)
{
setInt
(
getNodePrefix
(
node
)
+
MEMORY
,
memory
)
;
public
void
setOverCommitTimeoutPerNode
(
String
node
,
int
overCommitTimeout
)
{
setInt
(
getNodePrefix
(
node
)
+
OVERCOMMIT_TIMEOUT
,
overCommitTimeout
)
;
resourcesFile
=
tmp
.
getPath
(
)
;
}
}
ObjectMapper
mapper
=
new
ObjectMapper
(
)
;
Map
data
=
mapper
.
readValue
(
new
File
(
resourcesFile
)
,
Map
.
class
)
;
Iterator
iterator
=
data
.
entrySet
(
)
.
iterator
(
)
;
while
(
iterator
.
hasNext
(
)
)
{
Map
.
Entry
entry
=
(
Map
.
Entry
)
iterator
.
next
(
)
;
String
profileName
=
entry
.
getKey
(
)
.
toString
(
)
;
if
(
profileName
.
isEmpty
(
)
)
{
throw
new
IOException
(
)
;
}
if
(
profileName
.
equals
(
MINIMUM_PROFILE
)
||
profileName
.
equals
(
MAXIMUM_PROFILE
)
)
{
throw
new
IOException
(
String
.
format
(
+
,
MINIMUM_PROFILE
,
MAXIMUM_PROFILE
,
sourceFile
)
)
;
}
if
(
entry
.
getValue
(
)
instanceof
Map
)
{
Map
profileInfo
=
(
Map
)
entry
.
getValue
(
)
;
if
(
!
profileInfo
.
containsKey
(
MEMORY
)
||
!
profileInfo
.
containsKey
(
VCORES
)
)
{
throw
new
IOException
(
+
profileName
+
+
MEMORY
+
+
VCORES
+
)
;
Map
.
Entry
entry
=
(
Map
.
Entry
)
iterator
.
next
(
)
;
String
profileName
=
entry
.
getKey
(
)
.
toString
(
)
;
if
(
profileName
.
isEmpty
(
)
)
{
throw
new
IOException
(
)
;
}
if
(
profileName
.
equals
(
MINIMUM_PROFILE
)
||
profileName
.
equals
(
MAXIMUM_PROFILE
)
)
{
throw
new
IOException
(
String
.
format
(
+
,
MINIMUM_PROFILE
,
MAXIMUM_PROFILE
,
sourceFile
)
)
;
}
if
(
entry
.
getValue
(
)
instanceof
Map
)
{
Map
profileInfo
=
(
Map
)
entry
.
getValue
(
)
;
if
(
!
profileInfo
.
containsKey
(
MEMORY
)
||
!
profileInfo
.
containsKey
(
VCORES
)
)
{
throw
new
IOException
(
+
profileName
+
+
MEMORY
+
+
VCORES
+
)
;
}
Resource
resource
=
parseResource
(
profileInfo
)
;
profiles
.
put
(
profileName
,
resource
)
;
LOG
.
info
(
+
profileName
+
+
resource
)
;
}
}
profiles
.
put
(
MINIMUM_PROFILE
,
ResourceUtils
.
getResourceTypesMinimumAllocation
(
)
)
;
profiles
.
put
(
MAXIMUM_PROFILE
,
ResourceUtils
.
getResourceTypesMaximumAllocation
(
)
)
;
@
Override
public
void
handle
(
RMAppEvent
event
)
{
this
.
writeLock
.
lock
(
)
;
try
{
ApplicationId
appID
=
event
.
getApplicationId
(
)
;
@
Override
public
void
recover
(
RMState
state
)
{
ApplicationStateData
appState
=
state
.
getApplicationState
(
)
.
get
(
getApplicationId
(
)
)
;
this
.
recoveredFinalState
=
appState
.
getState
(
)
;
if
(
recoveredFinalState
==
null
)
{
private
void
processNodeUpdate
(
RMAppNodeUpdateType
type
,
RMNode
node
)
{
NodeState
nodeState
=
node
.
getState
(
)
;
updatedNodes
.
put
(
node
,
RMAppNodeUpdateType
.
convertToNodeUpdateType
(
type
)
)
;
static
void
appAdminClientCleanUp
(
RMAppImpl
app
)
{
try
{
AppAdminClient
client
=
AppAdminClient
.
createAppAdminClient
(
app
.
applicationType
,
app
.
conf
)
;
int
result
=
client
.
actionCleanUp
(
app
.
name
,
app
.
user
)
;
if
(
result
==
0
)
{
private
int
getDiagnosticsLimitKCOrThrow
(
final
Configuration
configuration
)
{
try
{
final
int
diagnosticsLimitKC
=
configuration
.
getInt
(
YarnConfiguration
.
APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC
,
YarnConfiguration
.
DEFAULT_APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC
)
;
if
(
diagnosticsLimitKC
<=
0
)
{
final
String
message
=
String
.
format
(
DIAGNOSTIC_LIMIT_CONFIG_ERROR_MESSAGE
,
YarnConfiguration
.
APP_ATTEMPT_DIAGNOSTICS_LIMIT_KC
,
diagnosticsLimitKC
)
;
@
Override
public
void
handle
(
RMAppAttemptEvent
event
)
{
this
.
writeLock
.
lock
(
)
;
try
{
ApplicationAttemptId
appAttemptID
=
event
.
getApplicationAttemptId
(
)
;
finalTrackingUrl
=
sanitizeTrackingUrl
(
unregisterEvent
.
getFinalTrackingUrl
(
)
)
;
finalStatus
=
unregisterEvent
.
getFinalApplicationStatus
(
)
;
break
;
case
CONTAINER_FINISHED
:
RMAppAttemptContainerFinishedEvent
finishEvent
=
(
RMAppAttemptContainerFinishedEvent
)
event
;
diags
.
append
(
getAMContainerCrashedDiagnostics
(
finishEvent
)
)
;
exitStatus
=
finishEvent
.
getContainerStatus
(
)
.
getExitStatus
(
)
;
break
;
case
KILL
:
break
;
case
FAIL
:
diags
.
append
(
event
.
getDiagnosticMsg
(
)
)
;
break
;
case
EXPIRE
:
diags
.
append
(
getAMExpiredDiagnostics
(
event
)
)
;
break
;
default
:
break
;
}
AggregateAppResourceUsage
resUsage
=
this
.
attemptMetrics
.
getAggregateAppResourceUsage
(
)
;
RMStateStore
rmStore
=
rmContext
.
getStateStore
(
)
;
@
Override
public
void
handle
(
RMContainerEvent
event
)
{
@
VisibleForTesting
protected
void
onInvalidStateTransition
(
RMContainerEventType
rmContainerEventType
,
RMContainerState
state
)
{
public
void
handle
(
RMNodeEvent
event
)
{
private
void
handleReportedIncreasedContainers
(
List
<
Container
>
reportedIncreasedContainers
)
{
for
(
Container
container
:
reportedIncreasedContainers
)
{
ContainerId
containerId
=
container
.
getId
(
)
;
if
(
containersToClean
.
contains
(
containerId
)
)
{
private
void
handleContainerStatus
(
List
<
ContainerStatus
>
containerStatuses
)
{
List
<
ContainerStatus
>
newlyLaunchedContainers
=
new
ArrayList
<
ContainerStatus
>
(
)
;
List
<
ContainerStatus
>
newlyCompletedContainers
=
new
ArrayList
<
ContainerStatus
>
(
)
;
List
<
Map
.
Entry
<
ApplicationId
,
ContainerStatus
>>
needUpdateContainers
=
new
ArrayList
<
Map
.
Entry
<
ApplicationId
,
ContainerStatus
>>
(
)
;
int
numRemoteRunningContainers
=
0
;
for
(
ContainerStatus
remoteContainer
:
containerStatuses
)
{
ContainerId
containerId
=
remoteContainer
.
getContainerId
(
)
;
if
(
containersToClean
.
contains
(
containerId
)
)
{
private
void
handleContainerStatus
(
List
<
ContainerStatus
>
containerStatuses
)
{
List
<
ContainerStatus
>
newlyLaunchedContainers
=
new
ArrayList
<
ContainerStatus
>
(
)
;
List
<
ContainerStatus
>
newlyCompletedContainers
=
new
ArrayList
<
ContainerStatus
>
(
)
;
List
<
Map
.
Entry
<
ApplicationId
,
ContainerStatus
>>
needUpdateContainers
=
new
ArrayList
<
Map
.
Entry
<
ApplicationId
,
ContainerStatus
>>
(
)
;
int
numRemoteRunningContainers
=
0
;
for
(
ContainerStatus
remoteContainer
:
containerStatuses
)
{
ContainerId
containerId
=
remoteContainer
.
getContainerId
(
)
;
if
(
containersToClean
.
contains
(
containerId
)
)
{
LOG
.
info
(
+
containerId
+
+
)
;
continue
;
}
ApplicationId
containerAppId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
if
(
finishedApplications
.
contains
(
containerAppId
)
)
{
LOG
.
info
(
+
containerId
+
+
)
;
continue
;
}
else
if
(
!
runningApplications
.
contains
(
containerAppId
)
)
{
protected
void
containerLaunchedOnNode
(
ContainerId
containerId
,
SchedulerNode
node
)
{
readLock
.
lock
(
)
;
try
{
SchedulerApplicationAttempt
application
=
getCurrentAttemptForContainer
(
containerId
)
;
if
(
application
==
null
)
{
protected
void
containerIncreasedOnNode
(
ContainerId
containerId
,
SchedulerNode
node
,
Container
increasedContainerReportedByNM
)
{
SchedulerApplicationAttempt
application
=
getCurrentAttemptForContainer
(
containerId
)
;
if
(
application
==
null
)
{
@
Override
public
SchedulerAppReport
getSchedulerAppInfo
(
ApplicationAttemptId
appAttemptId
)
{
SchedulerApplicationAttempt
attempt
=
getApplicationAttempt
(
appAttemptId
)
;
if
(
attempt
==
null
)
{
@
Override
public
ApplicationResourceUsageReport
getAppResourceUsageReport
(
ApplicationAttemptId
appAttemptId
)
{
SchedulerApplicationAttempt
attempt
=
getApplicationAttempt
(
appAttemptId
)
;
if
(
attempt
==
null
)
{
for
(
NMContainerStatus
container
:
containerReports
)
{
ApplicationId
appId
=
container
.
getContainerId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
appId
)
;
if
(
rmApp
==
null
)
{
LOG
.
error
(
+
container
+
)
;
killOrphanContainerOnNode
(
nm
,
container
)
;
continue
;
}
SchedulerApplication
<
T
>
schedulerApp
=
applications
.
get
(
appId
)
;
if
(
schedulerApp
==
null
)
{
LOG
.
info
(
+
container
+
+
+
rmApp
.
getState
(
)
)
;
killOrphanContainerOnNode
(
nm
,
container
)
;
continue
;
}
LOG
.
info
(
+
container
)
;
SchedulerApplicationAttempt
schedulerAttempt
=
schedulerApp
.
getCurrentAppAttempt
(
)
;
if
(
!
rmApp
.
getApplicationSubmissionContext
(
)
.
getKeepContainersAcrossApplicationAttempts
(
)
)
{
killOrphanContainerOnNode
(
nm
,
container
)
;
continue
;
}
}
Queue
queue
=
schedulerApp
.
getQueue
(
)
;
String
queueName
=
queue
instanceof
CSQueue
?
(
(
CSQueue
)
queue
)
.
getQueuePath
(
)
:
queue
.
getQueueName
(
)
;
RMContainer
rmContainer
=
recoverAndCreateContainer
(
container
,
nm
,
queueName
)
;
rmContainer
.
handle
(
new
RMContainerRecoverEvent
(
container
.
getContainerId
(
)
,
container
)
)
;
SchedulerNode
schedulerNode
=
nodeTracker
.
getNode
(
nm
.
getNodeID
(
)
)
;
schedulerNode
.
recoverContainer
(
rmContainer
)
;
Queue
queueToRecover
=
schedulerAttempt
.
getQueue
(
)
;
queueToRecover
.
recoverContainer
(
getClusterResource
(
)
,
schedulerAttempt
,
rmContainer
)
;
schedulerAttempt
.
recoverContainer
(
schedulerNode
,
rmContainer
)
;
RMAppAttempt
appAttempt
=
rmApp
.
getCurrentAppAttempt
(
)
;
if
(
appAttempt
!=
null
)
{
Container
masterContainer
=
appAttempt
.
getMasterContainer
(
)
;
if
(
masterContainer
!=
null
&&
masterContainer
.
getId
(
)
.
equals
(
rmContainer
.
getContainerId
(
)
)
)
{
@
VisibleForTesting
@
Private
public
void
completedContainer
(
RMContainer
rmContainer
,
ContainerStatus
containerStatus
,
RMContainerEventType
event
)
{
if
(
rmContainer
==
null
)
{
protected
void
releaseContainers
(
List
<
ContainerId
>
containers
,
SchedulerApplicationAttempt
attempt
)
{
for
(
ContainerId
containerId
:
containers
)
{
RMContainer
rmContainer
=
getRMContainer
(
containerId
)
;
if
(
rmContainer
==
null
)
{
if
(
System
.
currentTimeMillis
(
)
-
ResourceManager
.
getClusterTimeStamp
(
)
<
nmExpireInterval
)
{
public
void
updateNodeResource
(
RMNode
nm
,
ResourceOption
resourceOption
)
{
writeLock
.
lock
(
)
;
try
{
SchedulerNode
node
=
getSchedulerNode
(
nm
.
getNodeID
(
)
)
;
Resource
newResource
=
resourceOption
.
getResource
(
)
;
final
int
timeout
=
resourceOption
.
getOverCommitTimeout
(
)
;
Resource
oldResource
=
node
.
getTotalResource
(
)
;
if
(
!
oldResource
.
equals
(
newResource
)
)
{
rmContext
.
getNodeLabelManager
(
)
.
updateNodeResource
(
nm
.
getNodeID
(
)
,
newResource
)
;
for
(
Container
container
:
newlyIncreasedContainers
)
{
containerIncreasedOnNode
(
container
.
getId
(
)
,
schedulerNode
,
container
)
;
}
for
(
Map
.
Entry
<
ApplicationId
,
ContainerStatus
>
c
:
updateExistContainers
)
{
SchedulerApplication
<
T
>
app
=
applications
.
get
(
c
.
getKey
(
)
)
;
ContainerId
containerId
=
c
.
getValue
(
)
.
getContainerId
(
)
;
if
(
app
==
null
||
app
.
getCurrentAppAttempt
(
)
==
null
)
{
continue
;
}
RMContainer
rmContainer
=
app
.
getCurrentAppAttempt
(
)
.
getRMContainer
(
containerId
)
;
if
(
rmContainer
==
null
)
{
continue
;
}
if
(
rmContainer
.
getExposedPorts
(
)
!=
null
&&
rmContainer
.
getExposedPorts
(
)
.
size
(
)
>
0
)
{
continue
;
}
String
strExposedPorts
=
c
.
getValue
(
)
.
getExposedPorts
(
)
;
if
(
null
!=
strExposedPorts
&&
!
strExposedPorts
.
isEmpty
(
)
)
{
Gson
gson
=
new
Gson
(
)
;
private
int
updateCompletedContainers
(
List
<
ContainerStatus
>
completedContainers
,
Resource
releasedResources
,
NodeId
nodeId
,
SchedulerNode
schedulerNode
)
{
int
releasedContainers
=
0
;
List
<
ContainerId
>
untrackedContainerIdList
=
new
ArrayList
<
ContainerId
>
(
)
;
for
(
ContainerStatus
completedContainer
:
completedContainers
)
{
ContainerId
containerId
=
completedContainer
.
getContainerId
(
)
;
protected
void
nodeUpdate
(
RMNode
nm
)
{
LOG
.
debug
(
,
nm
,
getClusterResource
(
)
)
;
SchedulerNode
schedulerNode
=
getNode
(
nm
.
getNodeID
(
)
)
;
List
<
ContainerStatus
>
completedContainers
=
updateNewContainerInfo
(
nm
,
schedulerNode
)
;
if
(
schedulerNode
!=
null
)
{
schedulerNode
.
notifyNodeUpdate
(
)
;
}
Resource
releasedResources
=
Resource
.
newInstance
(
0
,
0
)
;
int
releasedContainers
=
updateCompletedContainers
(
completedContainers
,
releasedResources
,
nm
.
getNodeID
(
)
,
schedulerNode
)
;
if
(
nm
.
getState
(
)
==
NodeState
.
DECOMMISSIONING
&&
schedulerNode
!=
null
&&
schedulerNode
.
getTotalResource
(
)
.
compareTo
(
schedulerNode
.
getAllocatedResource
(
)
)
!=
0
)
{
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMNodeResourceUpdateEvent
(
nm
.
getNodeID
(
)
,
ResourceOption
.
newInstance
(
schedulerNode
.
getAllocatedResource
(
)
,
0
)
)
)
;
}
updateSchedulerHealthInformation
(
releasedResources
,
releasedContainers
)
;
if
(
schedulerNode
!=
null
)
{
updateNodeResourceUtilization
(
nm
,
schedulerNode
)
;
}
if
(
schedulerNode
!=
null
)
{
signalContainersIfOvercommitted
(
schedulerNode
,
true
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
kill
)
{
eventType
=
SchedulerEventType
.
MARK_CONTAINER_FOR_KILLABLE
;
if
(
!
schedulerNode
.
isOvercommitTimedOut
(
)
)
{
return
;
}
}
ResourceCalculator
rc
=
getResourceCalculator
(
)
;
Resource
unallocated
=
Resource
.
newInstance
(
schedulerNode
.
getUnallocatedResource
(
)
)
;
if
(
Resources
.
fitsIn
(
rc
,
ZERO_RESOURCE
,
unallocated
)
)
{
return
;
}
LOG
.
info
(
,
schedulerNode
.
getNodeID
(
)
,
unallocated
)
;
for
(
RMContainer
container
:
schedulerNode
.
getContainersToKill
(
)
)
{
LOG
.
info
(
,
eventType
,
container
.
getContainerId
(
)
,
container
.
getAllocatedResource
(
)
)
;
ApplicationAttemptId
appId
=
container
.
getApplicationAttemptId
(
)
;
ContainerPreemptEvent
event
=
new
ContainerPreemptEvent
(
appId
,
container
,
eventType
)
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
event
)
;
Resources
.
addTo
(
unallocated
,
container
.
getAllocatedResource
(
)
)
;
protected
void
handleContainerUpdates
(
SchedulerApplicationAttempt
appAttempt
,
ContainerUpdates
updates
)
{
List
<
UpdateContainerRequest
>
promotionRequests
=
updates
.
getPromotionRequests
(
)
;
if
(
promotionRequests
!=
null
&&
!
promotionRequests
.
isEmpty
(
)
)
{
protected
void
handleContainerUpdates
(
SchedulerApplicationAttempt
appAttempt
,
ContainerUpdates
updates
)
{
List
<
UpdateContainerRequest
>
promotionRequests
=
updates
.
getPromotionRequests
(
)
;
if
(
promotionRequests
!=
null
&&
!
promotionRequests
.
isEmpty
(
)
)
{
LOG
.
info
(
+
promotionRequests
)
;
handleIncreaseRequests
(
appAttempt
,
promotionRequests
)
;
}
List
<
UpdateContainerRequest
>
increaseRequests
=
updates
.
getIncreaseRequests
(
)
;
if
(
increaseRequests
!=
null
&&
!
increaseRequests
.
isEmpty
(
)
)
{
LOG
.
info
(
+
increaseRequests
)
;
handleIncreaseRequests
(
appAttempt
,
increaseRequests
)
;
}
List
<
UpdateContainerRequest
>
demotionRequests
=
updates
.
getDemotionRequests
(
)
;
if
(
demotionRequests
!=
null
&&
!
demotionRequests
.
isEmpty
(
)
)
{
LOG
.
info
(
+
demotionRequests
)
;
handleDecreaseRequests
(
appAttempt
,
demotionRequests
)
;
}
List
<
UpdateContainerRequest
>
decreaseRequests
=
updates
.
getDecreaseRequests
(
)
;
if
(
decreaseRequests
!=
null
&&
!
decreaseRequests
.
isEmpty
(
)
)
{
protected
void
rollbackContainerUpdate
(
ContainerId
containerId
)
{
RMContainer
rmContainer
=
getRMContainer
(
containerId
)
;
if
(
rmContainer
==
null
)
{
public
Resource
getMinimumAllocation
(
)
{
Resource
ret
=
ResourceUtils
.
getResourceTypesMinimumAllocation
(
)
;
public
Resource
getMaximumAllocation
(
)
{
Resource
ret
=
ResourceUtils
.
getResourceTypesMaximumAllocation
(
)
;
@
Lock
(
{
Queue
.
class
,
SchedulerApplicationAttempt
.
class
}
)
@
Override
synchronized
public
void
activateApplication
(
String
user
,
ApplicationId
applicationId
)
{
Set
<
ApplicationId
>
userApps
=
usersApplications
.
get
(
user
)
;
if
(
userApps
==
null
)
{
userApps
=
new
HashSet
<
ApplicationId
>
(
)
;
usersApplications
.
put
(
user
,
userApps
)
;
++
activeUsers
;
metrics
.
incrActiveUsers
(
)
;
public
static
void
updateMetrics
(
ApplicationId
applicationId
,
NodeType
type
,
SchedulerNode
node
,
RMContainer
containerAllocated
,
String
user
,
Queue
queue
)
{
public
static
ConfigurationMutationACLPolicy
getPolicy
(
Configuration
conf
)
{
Class
<
?
extends
ConfigurationMutationACLPolicy
>
policyClass
=
conf
.
getClass
(
YarnConfiguration
.
RM_SCHEDULER_MUTATION_ACL_POLICY_CLASS
,
DefaultConfigurationMutationACLPolicy
.
class
,
ConfigurationMutationACLPolicy
.
class
)
;
@
SuppressWarnings
(
)
public
boolean
canDelete
(
String
queueName
)
{
SchedulerQueue
<
T
>
queue
=
queueManager
.
getQueue
(
queueName
)
;
if
(
queue
==
null
)
{
public
static
boolean
isPlaceBlacklisted
(
SchedulerApplicationAttempt
application
,
SchedulerNode
node
,
Logger
log
)
{
if
(
application
.
isPlaceBlacklisted
(
node
.
getNodeName
(
)
)
)
{
public
void
addRMContainer
(
ContainerId
id
,
RMContainer
rmContainer
)
{
writeLock
.
lock
(
)
;
try
{
if
(
!
getApplicationAttemptId
(
)
.
equals
(
rmContainer
.
getApplicationAttemptId
(
)
)
&&
!
liveContainers
.
containsKey
(
id
)
)
{
private
static
void
normalizeNodeLabelExpressionInRequest
(
ResourceRequest
resReq
,
QueueInfo
queueInfo
)
{
String
labelExp
=
resReq
.
getNodeLabelExpression
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
void
normalizeNodeLabelExpressionInRequest
(
ResourceRequest
resReq
,
QueueInfo
queueInfo
)
{
String
labelExp
=
resReq
.
getNodeLabelExpression
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
labelExp
)
;
private
static
boolean
checkResource
(
ResourceInformation
requestedRI
,
Resource
availableResource
)
{
final
ResourceInformation
availableRI
=
availableResource
.
getResourceInformation
(
requestedRI
.
getName
(
)
)
;
long
requestedResourceValue
=
requestedRI
.
getValue
(
)
;
long
availableResourceValue
=
availableRI
.
getValue
(
)
;
int
unitsRelation
=
UnitsConversionUtil
.
compareUnits
(
requestedRI
.
getUnits
(
)
,
availableRI
.
getUnits
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
static
boolean
checkResource
(
ResourceInformation
requestedRI
,
Resource
availableResource
)
{
final
ResourceInformation
availableRI
=
availableResource
.
getResourceInformation
(
requestedRI
.
getName
(
)
)
;
long
requestedResourceValue
=
requestedRI
.
getValue
(
)
;
long
availableResourceValue
=
availableRI
.
getValue
(
)
;
int
unitsRelation
=
UnitsConversionUtil
.
compareUnits
(
requestedRI
.
getUnits
(
)
,
availableRI
.
getUnits
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
requestedRI
)
;
private
static
boolean
checkResource
(
ResourceInformation
requestedRI
,
Resource
availableResource
)
{
final
ResourceInformation
availableRI
=
availableResource
.
getResourceInformation
(
requestedRI
.
getName
(
)
)
;
long
requestedResourceValue
=
requestedRI
.
getValue
(
)
;
long
availableResourceValue
=
availableRI
.
getValue
(
)
;
int
unitsRelation
=
UnitsConversionUtil
.
compareUnits
(
requestedRI
.
getUnits
(
)
,
availableRI
.
getUnits
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
requestedRI
)
;
LOG
.
debug
(
+
availableRI
)
;
private
static
boolean
checkResource
(
ResourceInformation
requestedRI
,
Resource
availableResource
)
{
final
ResourceInformation
availableRI
=
availableResource
.
getResourceInformation
(
requestedRI
.
getName
(
)
)
;
long
requestedResourceValue
=
requestedRI
.
getValue
(
)
;
long
availableResourceValue
=
availableRI
.
getValue
(
)
;
int
unitsRelation
=
UnitsConversionUtil
.
compareUnits
(
requestedRI
.
getUnits
(
)
,
availableRI
.
getUnits
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
requestedRI
)
;
LOG
.
debug
(
+
availableRI
)
;
LOG
.
debug
(
+
unitsRelation
)
;
}
if
(
unitsRelation
<
0
)
{
availableResourceValue
=
UnitsConversionUtil
.
convert
(
availableRI
.
getUnits
(
)
,
requestedRI
.
getUnits
(
)
,
availableRI
.
getValue
(
)
)
;
}
else
if
(
unitsRelation
>
0
)
{
requestedResourceValue
=
UnitsConversionUtil
.
convert
(
requestedRI
.
getUnits
(
)
,
availableRI
.
getUnits
(
)
,
requestedRI
.
getValue
(
)
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
requestedResourceValue
)
;
return
;
}
if
(
rmContext
.
getScheduler
(
)
instanceof
CapacityScheduler
)
{
CapacityScheduler
cs
=
(
CapacityScheduler
)
rmContext
.
getScheduler
(
)
;
if
(
!
cs
.
isMultiNodePlacementEnabled
(
)
)
{
int
numNodes
=
rmContext
.
getRMNodes
(
)
.
size
(
)
;
int
newAppActivitiesMaxQueueLength
;
int
numAsyncSchedulerThreads
=
cs
.
getNumAsyncSchedulerThreads
(
)
;
if
(
numAsyncSchedulerThreads
>
0
)
{
newAppActivitiesMaxQueueLength
=
Math
.
max
(
configuredAppActivitiesMaxQueueLength
,
numNodes
*
numAsyncSchedulerThreads
)
;
}
else
{
newAppActivitiesMaxQueueLength
=
Math
.
max
(
configuredAppActivitiesMaxQueueLength
,
(
int
)
(
numNodes
*
1.2
)
)
;
}
if
(
appActivitiesMaxQueueLength
!=
newAppActivitiesMaxQueueLength
)
{
LOG
.
info
(
+
+
,
appActivitiesMaxQueueLength
,
newAppActivitiesMaxQueueLength
,
configuredAppActivitiesMaxQueueLength
,
numNodes
,
numAsyncSchedulerThreads
)
;
appActivitiesMaxQueueLength
=
newAppActivitiesMaxQueueLength
;
}
}
else
if
(
appActivitiesMaxQueueLength
!=
configuredAppActivitiesMaxQueueLength
)
{
}
}
Iterator
<
Map
.
Entry
<
ApplicationId
,
Queue
<
AppAllocation
>>>
iteApp
=
completedAppAllocations
.
entrySet
(
)
.
iterator
(
)
;
while
(
iteApp
.
hasNext
(
)
)
{
Map
.
Entry
<
ApplicationId
,
Queue
<
AppAllocation
>>
appAllocation
=
iteApp
.
next
(
)
;
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
appAllocation
.
getKey
(
)
)
;
if
(
rmApp
==
null
||
rmApp
.
getFinalApplicationStatus
(
)
!=
FinalApplicationStatus
.
UNDEFINED
)
{
iteApp
.
remove
(
)
;
}
else
{
Iterator
<
AppAllocation
>
appActivitiesIt
=
appAllocation
.
getValue
(
)
.
iterator
(
)
;
while
(
appActivitiesIt
.
hasNext
(
)
)
{
if
(
curTS
-
appActivitiesIt
.
next
(
)
.
getTime
(
)
>
appActivitiesTTL
)
{
appActivitiesIt
.
remove
(
)
;
}
else
{
break
;
}
}
if
(
appAllocation
.
getValue
(
)
.
isEmpty
(
)
)
{
iteApp
.
remove
(
)
;
Iterator
<
Map
.
Entry
<
ApplicationId
,
Queue
<
AppAllocation
>>>
iteApp
=
completedAppAllocations
.
entrySet
(
)
.
iterator
(
)
;
while
(
iteApp
.
hasNext
(
)
)
{
Map
.
Entry
<
ApplicationId
,
Queue
<
AppAllocation
>>
appAllocation
=
iteApp
.
next
(
)
;
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
appAllocation
.
getKey
(
)
)
;
if
(
rmApp
==
null
||
rmApp
.
getFinalApplicationStatus
(
)
!=
FinalApplicationStatus
.
UNDEFINED
)
{
iteApp
.
remove
(
)
;
}
else
{
Iterator
<
AppAllocation
>
appActivitiesIt
=
appAllocation
.
getValue
(
)
.
iterator
(
)
;
while
(
appActivitiesIt
.
hasNext
(
)
)
{
if
(
curTS
-
appActivitiesIt
.
next
(
)
.
getTime
(
)
>
appActivitiesTTL
)
{
appActivitiesIt
.
remove
(
)
;
}
else
{
break
;
}
}
if
(
appAllocation
.
getValue
(
)
.
isEmpty
(
)
)
{
iteApp
.
remove
(
)
;
protected
void
updateConfigurableResourceRequirement
(
String
queuePath
,
Resource
clusterResource
)
{
CapacitySchedulerConfiguration
conf
=
csContext
.
getConfiguration
(
)
;
Set
<
String
>
configuredNodelabels
=
conf
.
getConfiguredNodeLabels
(
queuePath
)
;
for
(
String
label
:
configuredNodelabels
)
{
Resource
minResource
=
getMinimumAbsoluteResource
(
queuePath
,
label
)
;
Resource
maxResource
=
getMaximumAbsoluteResource
(
queuePath
,
label
)
;
LOG
.
debug
(
,
capacityConfigType
,
getQueuePath
(
)
)
;
CapacityConfigType
localType
=
checkConfigTypeIsAbsoluteResource
(
queuePath
,
label
)
?
CapacityConfigType
.
ABSOLUTE_RESOURCE
:
CapacityConfigType
.
PERCENTAGE
;
if
(
this
.
capacityConfigType
.
equals
(
CapacityConfigType
.
NONE
)
)
{
this
.
capacityConfigType
=
localType
;
try
{
Resource
currentLimitResource
=
getCurrentLimitResource
(
nodePartition
,
clusterResource
,
currentResourceLimits
,
schedulingMode
)
;
Resource
nowTotalUsed
=
queueUsage
.
getUsed
(
nodePartition
)
;
Resource
usedExceptKillable
=
nowTotalUsed
;
if
(
hasChildQueues
(
)
)
{
usedExceptKillable
=
Resources
.
subtract
(
nowTotalUsed
,
getTotalKillableResource
(
nodePartition
)
)
;
}
currentResourceLimits
.
setHeadroom
(
Resources
.
subtract
(
currentLimitResource
,
usedExceptKillable
)
)
;
if
(
Resources
.
greaterThanOrEqual
(
resourceCalculator
,
clusterResource
,
usedExceptKillable
,
currentLimitResource
)
)
{
if
(
this
.
reservationsContinueLooking
&&
Resources
.
greaterThan
(
resourceCalculator
,
clusterResource
,
resourceCouldBeUnreserved
,
Resources
.
none
(
)
)
)
{
Resource
newTotalWithoutReservedResource
=
Resources
.
subtract
(
usedExceptKillable
,
resourceCouldBeUnreserved
)
;
if
(
Resources
.
lessThan
(
resourceCalculator
,
clusterResource
,
newTotalWithoutReservedResource
,
currentLimitResource
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
getQueuePath
(
)
+
+
queueUsage
.
getUsed
(
)
+
+
clusterResource
+
+
resourceCouldBeUnreserved
+
+
newTotalWithoutReservedResource
+
+
currentLimitResource
)
;
}
return
true
;
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
Resource
usedExceptKillable
=
nowTotalUsed
;
if
(
hasChildQueues
(
)
)
{
usedExceptKillable
=
Resources
.
subtract
(
nowTotalUsed
,
getTotalKillableResource
(
nodePartition
)
)
;
}
currentResourceLimits
.
setHeadroom
(
Resources
.
subtract
(
currentLimitResource
,
usedExceptKillable
)
)
;
if
(
Resources
.
greaterThanOrEqual
(
resourceCalculator
,
clusterResource
,
usedExceptKillable
,
currentLimitResource
)
)
{
if
(
this
.
reservationsContinueLooking
&&
Resources
.
greaterThan
(
resourceCalculator
,
clusterResource
,
resourceCouldBeUnreserved
,
Resources
.
none
(
)
)
)
{
Resource
newTotalWithoutReservedResource
=
Resources
.
subtract
(
usedExceptKillable
,
resourceCouldBeUnreserved
)
;
if
(
Resources
.
lessThan
(
resourceCalculator
,
clusterResource
,
newTotalWithoutReservedResource
,
currentLimitResource
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
getQueuePath
(
)
+
+
queueUsage
.
getUsed
(
)
+
+
clusterResource
+
+
resourceCouldBeUnreserved
+
+
newTotalWithoutReservedResource
+
+
currentLimitResource
)
;
}
return
true
;
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
getQueuePath
(
)
+
+
nodePartition
+
+
queueUsage
.
getUsed
(
nodePartition
)
+
+
clusterResource
+
+
resourceCouldBeUnreserved
+
+
currentLimitResource
+
+
usedExceptKillable
)
;
}
return
false
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
ContainerAllocationProposal
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
allocation
=
request
.
getFirstAllocatedOrReservedContainer
(
)
;
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
schedulerContainer
=
allocation
.
getAllocatedOrReservedContainer
(
)
;
if
(
allocation
.
getAllocateFromReservedContainer
(
)
==
null
)
{
Resource
required
=
allocation
.
getAllocatedOrReservedResource
(
)
;
Resource
netAllocated
=
Resources
.
subtract
(
required
,
request
.
getTotalReleasedResource
(
)
)
;
readLock
.
lock
(
)
;
try
{
String
partition
=
schedulerContainer
.
getNodePartition
(
)
;
Resource
maxResourceLimit
;
if
(
allocation
.
getSchedulingMode
(
)
==
SchedulingMode
.
RESPECT_PARTITION_EXCLUSIVITY
)
{
maxResourceLimit
=
getQueueMaxResource
(
partition
)
;
}
else
{
maxResourceLimit
=
labelManager
.
getResourceByLabel
(
schedulerContainer
.
getNodePartition
(
)
,
cluster
)
;
}
if
(
!
Resources
.
fitsIn
(
resourceCalculator
,
Resources
.
add
(
queueUsage
.
getUsed
(
partition
)
,
netAllocated
)
,
maxResourceLimit
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
boolean
exceedQueueMaxParallelApps
(
AbstractCSQueue
queue
)
{
while
(
queue
!=
null
)
{
if
(
queue
.
getNumRunnableApps
(
)
>=
queue
.
getMaxParallelApps
(
)
)
{
FiCaSchedulerApp
next
=
iter
.
next
(
)
;
if
(
next
==
prev
)
{
continue
;
}
if
(
checkRunnabilityWithUpdate
(
next
)
)
{
LeafQueue
nextQueue
=
next
.
getCSLeafQueue
(
)
;
LOG
.
info
(
,
next
.
getApplicationAttemptId
(
)
,
nextQueue
)
;
trackRunnableApp
(
next
)
;
FiCaSchedulerApp
appSched
=
next
;
nextQueue
.
submitApplicationAttempt
(
next
,
next
.
getUser
(
)
)
;
noLongerPendingApps
.
add
(
appSched
)
;
if
(
noLongerPendingApps
.
size
(
)
>=
maxRunnableApps
)
{
break
;
}
}
prev
=
next
;
}
for
(
FiCaSchedulerApp
appSched
:
noLongerPendingApps
)
{
if
(
!
(
appSched
.
getCSLeafQueue
(
)
.
removeNonRunnableApp
(
appSched
)
)
)
{
continue
;
}
if
(
checkRunnabilityWithUpdate
(
next
)
)
{
LeafQueue
nextQueue
=
next
.
getCSLeafQueue
(
)
;
LOG
.
info
(
,
next
.
getApplicationAttemptId
(
)
,
nextQueue
)
;
trackRunnableApp
(
next
)
;
FiCaSchedulerApp
appSched
=
next
;
nextQueue
.
submitApplicationAttempt
(
next
,
next
.
getUser
(
)
)
;
noLongerPendingApps
.
add
(
appSched
)
;
if
(
noLongerPendingApps
.
size
(
)
>=
maxRunnableApps
)
{
break
;
}
}
prev
=
next
;
}
for
(
FiCaSchedulerApp
appSched
:
noLongerPendingApps
)
{
if
(
!
(
appSched
.
getCSLeafQueue
(
)
.
removeNonRunnableApp
(
appSched
)
)
)
{
LOG
.
error
(
+
,
appSched
.
getApplicationAttemptId
(
)
)
;
}
if
(
!
usersNonRunnableApps
.
remove
(
appSched
.
getUser
(
)
,
appSched
)
)
{
scheduleAsynchronously
=
this
.
conf
.
getScheduleAynschronously
(
)
;
asyncScheduleInterval
=
this
.
conf
.
getLong
(
ASYNC_SCHEDULER_INTERVAL
,
DEFAULT_ASYNC_SCHEDULER_INTERVAL
)
;
this
.
assignMultipleEnabled
=
this
.
conf
.
getAssignMultipleEnabled
(
)
;
this
.
maxAssignPerHeartbeat
=
this
.
conf
.
getMaxAssignPerHeartbeat
(
)
;
int
maxAsyncSchedulingThreads
=
this
.
conf
.
getInt
(
CapacitySchedulerConfiguration
.
SCHEDULE_ASYNCHRONOUSLY_MAXIMUM_THREAD
,
1
)
;
maxAsyncSchedulingThreads
=
Math
.
max
(
maxAsyncSchedulingThreads
,
1
)
;
if
(
scheduleAsynchronously
)
{
asyncSchedulerThreads
=
new
ArrayList
<
>
(
)
;
for
(
int
i
=
0
;
i
<
maxAsyncSchedulingThreads
;
i
++
)
{
asyncSchedulerThreads
.
add
(
new
AsyncScheduleThread
(
this
)
)
;
}
resourceCommitterService
=
new
ResourceCommitterService
(
this
)
;
asyncMaxPendingBacklogs
=
this
.
conf
.
getInt
(
CapacitySchedulerConfiguration
.
SCHEDULE_ASYNCHRONOUSLY_MAXIMUM_PENDING_BACKLOGS
,
CapacitySchedulerConfiguration
.
DEFAULT_SCHEDULE_ASYNCHRONOUSLY_MAXIMUM_PENDING_BACKLOGS
)
;
}
offswitchPerHeartbeatLimit
=
this
.
conf
.
getOffSwitchPerHeartbeatLimit
(
)
;
multiNodePlacementEnabled
=
this
.
conf
.
getMultiNodePlacementEnabled
(
)
;
if
(
rmContext
.
getMultiNodeSortingManager
(
)
!=
null
)
{
CSQueue
queue
=
getOrCreateQueueFromPlacementContext
(
applicationId
,
user
,
queueName
,
placementContext
,
true
)
;
if
(
queue
==
null
)
{
if
(
!
getConfiguration
(
)
.
shouldAppFailFast
(
getConfig
(
)
)
)
{
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
KILL
,
+
+
queueName
+
)
)
;
return
;
}
else
{
String
queueErrorMsg
=
+
queueName
+
+
+
+
+
+
;
LOG
.
error
(
FATAL
,
queueErrorMsg
)
;
throw
new
QueueInvalidException
(
queueErrorMsg
)
;
}
}
if
(
!
(
queue
instanceof
LeafQueue
)
)
{
if
(
!
getConfiguration
(
)
.
shouldAppFailFast
(
getConfig
(
)
)
)
{
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
KILL
,
+
+
queueName
+
)
)
;
return
;
}
else
{
String
queueErrorMsg
=
+
queueName
+
+
+
+
+
;
}
}
if
(
!
(
queue
instanceof
LeafQueue
)
)
{
if
(
!
getConfiguration
(
)
.
shouldAppFailFast
(
getConfig
(
)
)
)
{
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
KILL
,
+
+
queueName
+
)
)
;
return
;
}
else
{
String
queueErrorMsg
=
+
queueName
+
+
+
+
+
;
LOG
.
error
(
FATAL
,
queueErrorMsg
)
;
throw
new
QueueInvalidException
(
queueErrorMsg
)
;
}
}
if
(
queue
.
getState
(
)
==
QueueState
.
STOPPED
)
{
(
(
LeafQueue
)
queue
)
.
recoverDrainingState
(
)
;
}
try
{
queue
.
submitApplication
(
applicationId
,
user
,
queueName
)
;
}
catch
(
AccessControlException
ace
)
{
}
queue
.
getMetrics
(
)
.
submitApp
(
user
)
;
SchedulerApplication
<
FiCaSchedulerApp
>
application
=
new
SchedulerApplication
<
FiCaSchedulerApp
>
(
queue
,
user
,
priority
)
;
}
if
(
!
(
queue
instanceof
LeafQueue
)
)
{
if
(
!
getConfiguration
(
)
.
shouldAppFailFast
(
getConfig
(
)
)
)
{
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
KILL
,
+
+
queueName
+
)
)
;
return
;
}
else
{
String
queueErrorMsg
=
+
queueName
+
+
+
+
+
;
LOG
.
error
(
FATAL
,
queueErrorMsg
)
;
throw
new
QueueInvalidException
(
queueErrorMsg
)
;
}
}
if
(
queue
.
getState
(
)
==
QueueState
.
STOPPED
)
{
(
(
LeafQueue
)
queue
)
.
recoverDrainingState
(
)
;
}
try
{
queue
.
submitApplication
(
applicationId
,
user
,
queueName
)
;
}
catch
(
AccessControlException
ace
)
{
}
queue
.
getMetrics
(
)
.
submitApp
(
user
)
;
SchedulerApplication
<
FiCaSchedulerApp
>
application
=
new
SchedulerApplication
<
FiCaSchedulerApp
>
(
queue
,
user
,
priority
)
;
CSQueue
queue
=
getQueue
(
queueName
)
;
if
(
queue
==
null
)
{
if
(
placementContext
!=
null
&&
placementContext
.
hasParentQueue
(
)
)
{
try
{
return
autoCreateLeafQueue
(
placementContext
)
;
}
catch
(
YarnException
|
IOException
e
)
{
if
(
isRecovery
)
{
if
(
!
getConfiguration
(
)
.
shouldAppFailFast
(
getConfig
(
)
)
)
{
LOG
.
error
(
+
queueName
+
,
e
)
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
KILL
,
+
+
queueName
+
)
)
;
}
else
{
String
queueErrorMsg
=
+
queueName
+
+
;
LOG
.
error
(
FATAL
,
queueErrorMsg
,
e
)
;
throw
new
QueueInvalidException
(
queueErrorMsg
)
;
}
}
else
{
return
;
}
else
if
(
queue
instanceof
AutoCreatedLeafQueue
&&
queue
.
getParent
(
)
instanceof
ManagedParentQueue
)
{
if
(
placementContext
==
null
)
{
String
message
=
+
applicationId
+
+
user
+
+
queueName
+
+
+
QUEUE_MAPPING
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
APP_REJECTED
,
message
)
)
;
return
;
}
else
if
(
!
queue
.
getParent
(
)
.
getQueueShortName
(
)
.
equals
(
placementContext
.
getParentQueue
(
)
)
&&
!
queue
.
getParent
(
)
.
getQueuePath
(
)
.
equals
(
placementContext
.
getParentQueue
(
)
)
)
{
String
message
=
+
placementContext
.
getQueue
(
)
+
+
+
queue
.
getParent
(
)
.
getQueueShortName
(
)
+
+
CapacitySchedulerConfiguration
.
QUEUE_MAPPING
+
+
+
placementContext
.
getParentQueue
(
)
+
+
user
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
APP_REJECTED
,
message
)
)
;
return
;
}
}
try
{
priority
=
workflowPriorityMappingsMgr
.
mapWorkflowPriorityForApp
(
applicationId
,
queue
,
user
,
priority
)
;
}
catch
(
YarnException
e
)
{
String
message
=
+
applicationId
+
+
user
+
+
e
.
getMessage
(
)
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
APP_REJECTED
,
message
)
)
;
return
;
}
else
if
(
!
queue
.
getParent
(
)
.
getQueueShortName
(
)
.
equals
(
placementContext
.
getParentQueue
(
)
)
&&
!
queue
.
getParent
(
)
.
getQueuePath
(
)
.
equals
(
placementContext
.
getParentQueue
(
)
)
)
{
String
message
=
+
placementContext
.
getQueue
(
)
+
+
+
queue
.
getParent
(
)
.
getQueueShortName
(
)
+
+
CapacitySchedulerConfiguration
.
QUEUE_MAPPING
+
+
+
placementContext
.
getParentQueue
(
)
+
+
user
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
APP_REJECTED
,
message
)
)
;
return
;
}
}
try
{
priority
=
workflowPriorityMappingsMgr
.
mapWorkflowPriorityForApp
(
applicationId
,
queue
,
user
,
priority
)
;
}
catch
(
YarnException
e
)
{
String
message
=
+
applicationId
+
+
user
+
+
e
.
getMessage
(
)
;
this
.
rmContext
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
new
RMAppEvent
(
applicationId
,
RMAppEventType
.
APP_REJECTED
,
message
)
)
;
return
;
}
try
{
queue
.
submitApplication
(
applicationId
,
user
,
queueName
)
;
}
catch
(
AccessControlException
ace
)
{
LOG
.
info
(
+
applicationId
+
+
queueName
+
+
user
,
ace
)
;
writeLock
.
lock
(
)
;
try
{
SchedulerApplication
<
FiCaSchedulerApp
>
application
=
applications
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
if
(
application
==
null
)
{
LOG
.
warn
(
+
applicationAttemptId
.
getApplicationId
(
)
+
)
;
return
;
}
CSQueue
queue
=
(
CSQueue
)
application
.
getQueue
(
)
;
FiCaSchedulerApp
attempt
=
new
FiCaSchedulerApp
(
applicationAttemptId
,
application
.
getUser
(
)
,
queue
,
queue
.
getAbstractUsersManager
(
)
,
rmContext
,
application
.
getPriority
(
)
,
isAttemptRecovering
,
activitiesManager
)
;
if
(
transferStateFromPreviousAttempt
)
{
attempt
.
transferStateFromPreviousAttempt
(
application
.
getCurrentAppAttempt
(
)
)
;
}
application
.
setCurrentAppAttempt
(
attempt
)
;
attempt
.
setPriority
(
application
.
getPriority
(
)
)
;
maxRunningEnforcer
.
checkRunnabilityWithUpdate
(
attempt
)
;
maxRunningEnforcer
.
trackApp
(
attempt
)
;
queue
.
submitApplicationAttempt
(
attempt
,
application
.
getUser
(
)
)
;
SchedulerApplication
<
FiCaSchedulerApp
>
application
=
applications
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
if
(
application
==
null
)
{
LOG
.
warn
(
+
applicationAttemptId
.
getApplicationId
(
)
+
)
;
return
;
}
CSQueue
queue
=
(
CSQueue
)
application
.
getQueue
(
)
;
FiCaSchedulerApp
attempt
=
new
FiCaSchedulerApp
(
applicationAttemptId
,
application
.
getUser
(
)
,
queue
,
queue
.
getAbstractUsersManager
(
)
,
rmContext
,
application
.
getPriority
(
)
,
isAttemptRecovering
,
activitiesManager
)
;
if
(
transferStateFromPreviousAttempt
)
{
attempt
.
transferStateFromPreviousAttempt
(
application
.
getCurrentAppAttempt
(
)
)
;
}
application
.
setCurrentAppAttempt
(
attempt
)
;
attempt
.
setPriority
(
application
.
getPriority
(
)
)
;
maxRunningEnforcer
.
checkRunnabilityWithUpdate
(
attempt
)
;
maxRunningEnforcer
.
trackApp
(
attempt
)
;
queue
.
submitApplicationAttempt
(
attempt
,
application
.
getUser
(
)
)
;
LOG
.
info
(
+
applicationAttemptId
+
+
application
.
getUser
(
)
+
+
queue
.
getQueuePath
(
)
)
;
if
(
isAttemptRecovering
)
{
private
void
doneApplicationAttempt
(
ApplicationAttemptId
applicationAttemptId
,
RMAppAttemptState
rmAppAttemptFinalState
,
boolean
keepContainers
)
{
writeLock
.
lock
(
)
;
try
{
private
void
doneApplicationAttempt
(
ApplicationAttemptId
applicationAttemptId
,
RMAppAttemptState
rmAppAttemptFinalState
,
boolean
keepContainers
)
{
writeLock
.
lock
(
)
;
try
{
LOG
.
info
(
+
applicationAttemptId
+
+
+
rmAppAttemptFinalState
)
;
FiCaSchedulerApp
attempt
=
getApplicationAttempt
(
applicationAttemptId
)
;
SchedulerApplication
<
FiCaSchedulerApp
>
application
=
applications
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
if
(
application
==
null
||
attempt
==
null
)
{
SchedulerApplication
<
FiCaSchedulerApp
>
application
=
applications
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
if
(
application
==
null
||
attempt
==
null
)
{
LOG
.
info
(
+
applicationAttemptId
+
)
;
return
;
}
for
(
RMContainer
rmContainer
:
attempt
.
getLiveContainers
(
)
)
{
if
(
keepContainers
&&
rmContainer
.
getState
(
)
.
equals
(
RMContainerState
.
RUNNING
)
)
{
LOG
.
info
(
+
rmContainer
.
getContainerId
(
)
)
;
continue
;
}
super
.
completedContainer
(
rmContainer
,
SchedulerUtils
.
createAbnormalContainerStatus
(
rmContainer
.
getContainerId
(
)
,
SchedulerUtils
.
COMPLETED_APPLICATION
)
,
RMContainerEventType
.
KILL
)
;
}
for
(
RMContainer
rmContainer
:
attempt
.
getReservedContainers
(
)
)
{
super
.
completedContainer
(
rmContainer
,
SchedulerUtils
.
createAbnormalContainerStatus
(
rmContainer
.
getContainerId
(
)
,
)
,
RMContainerEventType
.
KILL
)
;
}
attempt
.
stop
(
rmAppAttemptFinalState
)
;
Queue
queue
=
attempt
.
getQueue
(
)
;
CSQueue
csQueue
=
(
CSQueue
)
queue
;
if
(
!
(
csQueue
instanceof
LeafQueue
)
)
{
@
Override
@
Lock
(
Lock
.
NoLock
.
class
)
public
Allocation
allocate
(
ApplicationAttemptId
applicationAttemptId
,
List
<
ResourceRequest
>
ask
,
List
<
SchedulingRequest
>
schedulingRequests
,
List
<
ContainerId
>
release
,
List
<
String
>
blacklistAdditions
,
List
<
String
>
blacklistRemovals
,
ContainerUpdates
updateRequests
)
{
FiCaSchedulerApp
application
=
getApplicationAttempt
(
applicationAttemptId
)
;
if
(
application
==
null
)
{
}
if
(
!
application
.
getApplicationAttemptId
(
)
.
equals
(
applicationAttemptId
)
)
{
LOG
.
error
(
+
+
applicationAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
handleContainerUpdates
(
application
,
updateRequests
)
;
releaseContainers
(
release
,
application
)
;
LeafQueue
updateDemandForQueue
=
null
;
normalizeResourceRequests
(
ask
)
;
normalizeSchedulingRequests
(
schedulingRequests
)
;
Allocation
allocation
;
application
.
getWriteLock
(
)
.
lock
(
)
;
try
{
if
(
application
.
isStopped
(
)
)
{
return
EMPTY_ALLOCATION
;
}
if
(
!
ask
.
isEmpty
(
)
||
(
schedulingRequests
!=
null
&&
!
schedulingRequests
.
isEmpty
(
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
return
EMPTY_ALLOCATION
;
}
if
(
!
ask
.
isEmpty
(
)
||
(
schedulingRequests
!=
null
&&
!
schedulingRequests
.
isEmpty
(
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
applicationAttemptId
+
+
ask
.
size
(
)
)
;
application
.
showRequests
(
)
;
}
if
(
application
.
updateResourceRequests
(
ask
)
||
application
.
updateSchedulingRequests
(
schedulingRequests
)
)
{
updateDemandForQueue
=
(
LeafQueue
)
application
.
getQueue
(
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
)
;
application
.
showRequests
(
)
;
}
}
application
.
updateBlacklist
(
blacklistAdditions
,
blacklistRemovals
)
;
allocation
=
application
.
getAllocation
(
getResourceCalculator
(
)
,
getClusterResource
(
)
,
getMinimumResourceCapability
(
)
)
;
}
finally
{
application
.
getWriteLock
(
)
.
unlock
(
)
;
}
if
(
updateDemandForQueue
!=
null
&&
!
application
.
isWaitingForAMContainer
(
)
)
{
int
assignedContainers
=
0
;
CandidateNodeSet
<
FiCaSchedulerNode
>
candidates
=
getCandidateNodeSet
(
node
)
;
CSAssignment
assignment
=
allocateContainersToNode
(
candidates
,
withNodeHeartbeat
)
;
if
(
null
!=
assignment
&&
withNodeHeartbeat
)
{
if
(
assignment
.
getType
(
)
==
NodeType
.
OFF_SWITCH
)
{
offswitchCount
++
;
}
if
(
Resources
.
greaterThan
(
calculator
,
getClusterResource
(
)
,
assignment
.
getResource
(
)
,
Resources
.
none
(
)
)
)
{
assignedContainers
++
;
}
while
(
canAllocateMore
(
assignment
,
offswitchCount
,
assignedContainers
)
)
{
assignment
=
allocateContainersToNode
(
candidates
,
true
)
;
if
(
null
!=
assignment
&&
assignment
.
getType
(
)
==
NodeType
.
OFF_SWITCH
)
{
offswitchCount
++
;
}
if
(
null
!=
assignment
&&
Resources
.
greaterThan
(
calculator
,
getClusterResource
(
)
,
assignment
.
getResource
(
)
,
Resources
.
none
(
)
)
)
{
assignedContainers
++
;
}
}
if
(
offswitchCount
>=
offswitchPerHeartbeatLimit
)
{
private
CSAssignment
allocateContainerOnSingleNode
(
CandidateNodeSet
<
FiCaSchedulerNode
>
candidates
,
FiCaSchedulerNode
node
,
boolean
withNodeHeartbeat
)
{
private
CSAssignment
allocateContainerOnSingleNode
(
CandidateNodeSet
<
FiCaSchedulerNode
>
candidates
,
FiCaSchedulerNode
node
,
boolean
withNodeHeartbeat
)
{
LOG
.
debug
(
,
node
.
getNodeName
(
)
,
node
.
getUnallocatedResource
(
)
)
;
if
(
getNode
(
node
.
getNodeID
(
)
)
!=
node
)
{
private
void
allocateFromReservedContainer
(
FiCaSchedulerNode
node
,
boolean
withNodeHeartbeat
,
RMContainer
reservedContainer
)
{
FiCaSchedulerApp
reservedApplication
=
getCurrentAttemptForContainer
(
reservedContainer
.
getContainerId
(
)
)
;
if
(
reservedApplication
==
null
)
{
@
Override
protected
void
completedContainerInternal
(
RMContainer
rmContainer
,
ContainerStatus
containerStatus
,
RMContainerEventType
event
)
{
Container
container
=
rmContainer
.
getContainer
(
)
;
ContainerId
containerId
=
container
.
getId
(
)
;
FiCaSchedulerApp
application
=
getCurrentAttemptForContainer
(
container
.
getId
(
)
)
;
ApplicationId
appId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
if
(
application
==
null
)
{
@
Override
public
void
killReservedContainer
(
RMContainer
container
)
{
@
Override
public
void
markContainerForPreemption
(
ApplicationAttemptId
aid
,
RMContainer
cont
)
{
public
void
markContainerForKillable
(
RMContainer
killableContainer
)
{
writeLock
.
lock
(
)
;
try
{
private
void
markContainerForNonKillable
(
RMContainer
nonKillableContainer
)
{
writeLock
.
lock
(
)
;
try
{
@
Override
public
boolean
checkAccess
(
UserGroupInformation
callerUGI
,
QueueACL
acl
,
String
queueName
)
{
CSQueue
queue
=
getQueue
(
queueName
)
;
if
(
queue
==
null
)
{
@
Override
public
void
removeQueue
(
String
queueName
)
throws
SchedulerDynamicEditException
{
writeLock
.
lock
(
)
;
try
{
catch
(
AccessControlException
e
)
{
throw
new
YarnException
(
e
)
;
}
FiCaSchedulerApp
app
=
application
.
getCurrentAppAttempt
(
)
;
if
(
app
!=
null
)
{
for
(
RMContainer
rmContainer
:
app
.
getLiveContainers
(
)
)
{
source
.
detachContainer
(
getClusterResource
(
)
,
app
,
rmContainer
)
;
dest
.
attachContainer
(
getClusterResource
(
)
,
app
,
rmContainer
)
;
}
for
(
RMContainer
rmContainer
:
app
.
getReservedContainers
(
)
)
{
source
.
detachContainer
(
getClusterResource
(
)
,
app
,
rmContainer
)
;
dest
.
attachContainer
(
getClusterResource
(
)
,
app
,
rmContainer
)
;
}
if
(
!
app
.
isStopped
(
)
)
{
source
.
finishApplicationAttempt
(
app
,
sourceQueueName
)
;
dest
.
submitApplicationAttempt
(
app
,
user
,
true
)
;
}
app
.
move
(
dest
)
;
}
source
.
appFinished
(
)
;
SchedulerApplication
<
FiCaSchedulerApp
>
application
=
applications
.
get
(
applicationId
)
;
if
(
application
==
null
)
{
throw
new
YarnException
(
+
applicationId
+
)
;
}
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
applicationId
)
;
appPriority
=
checkAndGetApplicationPriority
(
newPriority
,
user
,
rmApp
.
getQueue
(
)
,
applicationId
)
;
if
(
application
.
getPriority
(
)
.
equals
(
appPriority
)
)
{
future
.
set
(
null
)
;
return
appPriority
;
}
rmApp
.
getApplicationSubmissionContext
(
)
.
setPriority
(
appPriority
)
;
ApplicationStateData
appState
=
ApplicationStateData
.
newInstance
(
rmApp
.
getSubmitTime
(
)
,
rmApp
.
getStartTime
(
)
,
rmApp
.
getApplicationSubmissionContext
(
)
,
rmApp
.
getUser
(
)
,
rmApp
.
getCallerContext
(
)
)
;
appState
.
setApplicationTimeouts
(
rmApp
.
getApplicationTimeouts
(
)
)
;
appState
.
setLaunchTime
(
rmApp
.
getLaunchTime
(
)
)
;
rmContext
.
getStateStore
(
)
.
updateApplicationStateSynchronously
(
appState
,
false
,
future
)
;
LeafQueue
queue
=
(
LeafQueue
)
getQueue
(
rmApp
.
getQueue
(
)
)
;
queue
.
updateApplicationPriority
(
application
,
appPriority
)
;
attemptId
=
request
.
getContainersToRelease
(
)
.
get
(
0
)
.
getSchedulerApplicationAttempt
(
)
.
getApplicationAttemptId
(
)
;
}
}
LOG
.
debug
(
,
request
)
;
boolean
isSuccess
=
false
;
if
(
attemptId
!=
null
)
{
FiCaSchedulerApp
app
=
getApplicationAttempt
(
attemptId
)
;
if
(
app
!=
null
&&
attemptId
.
equals
(
app
.
getApplicationAttemptId
(
)
)
)
{
if
(
app
.
accept
(
cluster
,
request
,
updatePending
)
&&
app
.
apply
(
cluster
,
request
,
updatePending
)
)
{
long
commitSuccess
=
System
.
nanoTime
(
)
-
commitStart
;
CapacitySchedulerMetrics
.
getMetrics
(
)
.
addCommitSuccess
(
commitSuccess
)
;
LOG
.
info
(
)
;
isSuccess
=
true
;
}
else
{
long
commitFailed
=
System
.
nanoTime
(
)
-
commitStart
;
CapacitySchedulerMetrics
.
getMetrics
(
)
.
addCommitFailure
(
commitFailed
)
;
LOG
.
info
(
)
;
public
boolean
moveReservedContainer
(
RMContainer
toBeMovedContainer
,
FiCaSchedulerNode
targetNode
)
{
writeLock
.
lock
(
)
;
try
{
public
boolean
moveReservedContainer
(
RMContainer
toBeMovedContainer
,
FiCaSchedulerNode
targetNode
)
{
writeLock
.
lock
(
)
;
try
{
LOG
.
debug
(
,
toBeMovedContainer
,
targetNode
.
getNodeID
(
)
)
;
FiCaSchedulerNode
sourceNode
=
getNode
(
toBeMovedContainer
.
getNodeId
(
)
)
;
if
(
null
==
sourceNode
)
{
writeLock
.
lock
(
)
;
try
{
LOG
.
debug
(
,
toBeMovedContainer
,
targetNode
.
getNodeID
(
)
)
;
FiCaSchedulerNode
sourceNode
=
getNode
(
toBeMovedContainer
.
getNodeId
(
)
)
;
if
(
null
==
sourceNode
)
{
LOG
.
debug
(
,
toBeMovedContainer
.
getNodeId
(
)
)
;
return
false
;
}
if
(
getNode
(
targetNode
.
getNodeID
(
)
)
!=
targetNode
)
{
LOG
.
debug
(
+
)
;
return
false
;
}
if
(
targetNode
.
getReservedContainer
(
)
!=
null
)
{
LOG
.
debug
(
+
)
;
return
false
;
}
FiCaSchedulerApp
app
=
getApplicationAttempt
(
toBeMovedContainer
.
getApplicationAttemptId
(
)
)
;
if
(
null
==
app
)
{
@
Override
public
long
getMaximumApplicationLifetime
(
String
queueName
)
{
CSQueue
queue
=
getQueue
(
queueName
)
;
if
(
queue
==
null
||
!
(
queue
instanceof
LeafQueue
)
)
{
if
(
isAmbiguous
(
queueName
)
)
{
public
static
void
validateQueueHierarchy
(
CSQueueStore
queues
,
CSQueueStore
newQueues
,
CapacitySchedulerConfiguration
newConf
)
throws
IOException
{
for
(
CSQueue
oldQueue
:
queues
.
getQueues
(
)
)
{
if
(
!
(
AbstractAutoCreatedLeafQueue
.
class
.
isAssignableFrom
(
oldQueue
.
getClass
(
)
)
)
)
{
String
queuePath
=
oldQueue
.
getQueuePath
(
)
;
CSQueue
newQueue
=
newQueues
.
get
(
queuePath
)
;
String
configPrefix
=
newConf
.
getQueuePrefix
(
oldQueue
.
getQueuePath
(
)
)
;
String
state
=
newConf
.
get
(
configPrefix
+
)
;
QueueState
newQueueState
=
null
;
if
(
state
!=
null
)
{
try
{
newQueueState
=
QueueState
.
valueOf
(
state
)
;
}
catch
(
Exception
ex
)
{
LOG
.
warn
(
+
oldQueue
.
getQueuePath
(
)
)
;
}
}
if
(
null
==
newQueue
)
{
if
(
oldQueue
.
getState
(
)
==
QueueState
.
STOPPED
||
newQueueState
==
QueueState
.
STOPPED
)
{
}
catch
(
Exception
ex
)
{
LOG
.
warn
(
+
oldQueue
.
getQueuePath
(
)
)
;
}
}
if
(
null
==
newQueue
)
{
if
(
oldQueue
.
getState
(
)
==
QueueState
.
STOPPED
||
newQueueState
==
QueueState
.
STOPPED
)
{
LOG
.
info
(
+
queuePath
+
+
)
;
}
else
{
throw
new
IOException
(
oldQueue
.
getQueuePath
(
)
+
+
+
+
oldQueue
.
getState
(
)
)
;
}
}
else
if
(
!
oldQueue
.
getQueuePath
(
)
.
equals
(
newQueue
.
getQueuePath
(
)
)
)
{
throw
new
IOException
(
queuePath
+
+
oldQueue
.
getQueuePath
(
)
+
+
newQueue
.
getQueuePath
(
)
+
)
;
}
else
if
(
oldQueue
instanceof
ParentQueue
&&
!
(
oldQueue
instanceof
ManagedParentQueue
)
&&
newQueue
instanceof
ManagedParentQueue
)
{
throw
new
IOException
(
+
oldQueue
.
getQueuePath
(
)
+
+
+
)
;
}
else
if
(
oldQueue
instanceof
ManagedParentQueue
&&
!
(
newQueue
instanceof
ManagedParentQueue
)
)
{
throw
new
IOException
(
+
oldQueue
.
getQueuePath
(
)
+
+
+
CapacitySchedulerConfiguration
.
AUTO_CREATE_CHILD_QUEUE_ENABLED
+
)
;
}
else
if
(
oldQueue
instanceof
LeafQueue
&&
newQueue
instanceof
ParentQueue
)
{
if
(
oldQueue
.
getState
(
)
==
QueueState
.
STOPPED
||
newQueueState
==
QueueState
.
STOPPED
)
{
}
if
(
null
==
newQueue
)
{
if
(
oldQueue
.
getState
(
)
==
QueueState
.
STOPPED
||
newQueueState
==
QueueState
.
STOPPED
)
{
LOG
.
info
(
+
queuePath
+
+
)
;
}
else
{
throw
new
IOException
(
oldQueue
.
getQueuePath
(
)
+
+
+
+
oldQueue
.
getState
(
)
)
;
}
}
else
if
(
!
oldQueue
.
getQueuePath
(
)
.
equals
(
newQueue
.
getQueuePath
(
)
)
)
{
throw
new
IOException
(
queuePath
+
+
oldQueue
.
getQueuePath
(
)
+
+
newQueue
.
getQueuePath
(
)
+
)
;
}
else
if
(
oldQueue
instanceof
ParentQueue
&&
!
(
oldQueue
instanceof
ManagedParentQueue
)
&&
newQueue
instanceof
ManagedParentQueue
)
{
throw
new
IOException
(
+
oldQueue
.
getQueuePath
(
)
+
+
+
)
;
}
else
if
(
oldQueue
instanceof
ManagedParentQueue
&&
!
(
newQueue
instanceof
ManagedParentQueue
)
)
{
throw
new
IOException
(
+
oldQueue
.
getQueuePath
(
)
+
+
+
CapacitySchedulerConfiguration
.
AUTO_CREATE_CHILD_QUEUE_ENABLED
+
)
;
}
else
if
(
oldQueue
instanceof
LeafQueue
&&
newQueue
instanceof
ParentQueue
)
{
if
(
oldQueue
.
getState
(
)
==
QueueState
.
STOPPED
||
newQueueState
==
QueueState
.
STOPPED
)
{
LOG
.
info
(
+
oldQueue
.
getQueuePath
(
)
+
)
;
}
else
{
public
void
setUserLimit
(
String
queue
,
int
userLimit
)
{
setInt
(
getQueuePrefix
(
queue
)
+
USER_LIMIT
,
userLimit
)
;
public
String
[
]
getQueues
(
String
queue
)
{
public
void
setQueues
(
String
queue
,
String
[
]
subQueues
)
{
set
(
getQueuePrefix
(
queue
)
+
QUEUES
,
StringUtils
.
arrayToString
(
subQueues
)
)
;
public
void
setReservable
(
String
queue
,
boolean
isReservable
)
{
setBoolean
(
getQueuePrefix
(
queue
)
+
IS_RESERVABLE
,
isReservable
)
;
@
Private
protected
AutoCreatedQueueManagementPolicy
getAutoCreatedQueueManagementPolicyClass
(
String
queueName
)
{
String
queueManagementPolicyClassName
=
getAutoCreatedQueueManagementPolicy
(
queueName
)
;
return
Resources
.
none
(
)
;
}
Resource
resource
=
Resource
.
newInstance
(
0L
,
0
)
;
Matcher
matcher
=
RESOURCE_PATTERN
.
matcher
(
resourceString
)
;
if
(
matcher
.
find
(
)
)
{
String
subGroup
=
matcher
.
group
(
0
)
;
if
(
subGroup
.
trim
(
)
.
isEmpty
(
)
)
{
return
Resources
.
none
(
)
;
}
subGroup
=
subGroup
.
substring
(
1
,
subGroup
.
length
(
)
-
1
)
;
for
(
String
kvPair
:
subGroup
.
trim
(
)
.
split
(
)
)
{
String
[
]
splits
=
kvPair
.
split
(
)
;
if
(
splits
!=
null
&&
splits
.
length
>
1
)
{
updateResourceValuesFromConfig
(
resourceTypes
,
resource
,
splits
)
;
}
}
}
if
(
resource
.
getMemorySize
(
)
==
0L
)
{
return
Resources
.
none
(
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
public
void
initializeQueues
(
CapacitySchedulerConfiguration
conf
)
throws
IOException
{
root
=
parseQueue
(
this
.
csContext
,
conf
,
null
,
CapacitySchedulerConfiguration
.
ROOT
,
queues
,
queues
,
NOOP
)
;
setQueueAcls
(
authorizer
,
appPriorityACLManager
,
queues
)
;
labelManager
.
reinitializeQueueLabels
(
getQueueToLabels
(
)
)
;
this
.
queueStateManager
.
initialize
(
this
)
;
queue
=
new
ManagedParentQueue
(
csContext
,
queueName
,
parent
,
oldQueues
.
get
(
fullQueueName
)
)
;
}
else
{
queue
=
new
LeafQueue
(
csContext
,
queueName
,
parent
,
oldQueues
.
get
(
fullQueueName
)
)
;
queue
=
hook
.
hook
(
queue
)
;
}
}
else
{
if
(
isReservableQueue
)
{
throw
new
IllegalStateException
(
+
fullQueueName
)
;
}
ParentQueue
parentQueue
;
if
(
isAutoCreateEnabled
)
{
parentQueue
=
new
ManagedParentQueue
(
csContext
,
queueName
,
parent
,
oldQueues
.
get
(
fullQueueName
)
)
;
}
else
{
parentQueue
=
new
ParentQueue
(
csContext
,
queueName
,
parent
,
oldQueues
.
get
(
fullQueueName
)
)
;
}
queue
=
hook
.
hook
(
parentQueue
)
;
List
<
CSQueue
>
childQueues
=
new
ArrayList
<
>
(
)
;
for
(
String
childQueueName
:
childQueueNames
)
{
}
nodeLocalityDelay
=
schedConf
.
getNodeLocalityDelay
(
)
;
rackLocalityAdditionalDelay
=
schedConf
.
getRackLocalityAdditionalDelay
(
)
;
rackLocalityFullReset
=
schedConf
.
getRackLocalityFullReset
(
)
;
this
.
minimumAllocationFactor
=
Resources
.
ratio
(
resourceCalculator
,
Resources
.
subtract
(
maximumAllocation
,
minimumAllocation
)
,
maximumAllocation
)
;
StringBuilder
aclsString
=
new
StringBuilder
(
)
;
for
(
Map
.
Entry
<
AccessType
,
AccessControlList
>
e
:
acls
.
entrySet
(
)
)
{
aclsString
.
append
(
e
.
getKey
(
)
+
+
e
.
getValue
(
)
.
getAclString
(
)
)
;
}
StringBuilder
labelStrBuilder
=
new
StringBuilder
(
)
;
if
(
accessibleLabels
!=
null
)
{
for
(
String
s
:
accessibleLabels
)
{
labelStrBuilder
.
append
(
s
)
.
append
(
)
;
}
}
defaultAppPriorityPerQueue
=
Priority
.
newInstance
(
conf
.
getDefaultApplicationPriorityConfPerQueue
(
getQueuePath
(
)
)
)
;
int
queueUL
=
Math
.
min
(
100
,
conf
.
getUserLimit
(
getQueuePath
(
)
)
)
;
for
(
Entry
<
String
,
Float
>
e
:
getUserWeights
(
)
.
entrySet
(
)
)
{
float
val
=
e
.
getValue
(
)
.
floatValue
(
)
;
public
void
validateSubmitApplication
(
ApplicationId
applicationId
,
String
userName
,
String
queue
)
throws
AccessControlException
{
writeLock
.
lock
(
)
;
try
{
if
(
getState
(
)
!=
QueueState
.
RUNNING
)
{
String
msg
=
+
getQueuePath
(
)
+
+
applicationId
;
throw
new
AccessControlException
(
msg
)
;
}
if
(
getNumApplications
(
)
>=
getMaxApplications
(
)
)
{
String
msg
=
+
getQueuePath
(
)
+
+
getNumApplications
(
)
+
+
+
applicationId
;
LOG
.
info
(
msg
)
;
throw
new
AccessControlException
(
msg
)
;
}
User
user
=
usersManager
.
getUserAndAddIfAbsent
(
userName
)
;
if
(
user
.
getTotalApplications
(
)
>=
getMaxApplicationsPerUser
(
)
)
{
String
msg
=
+
getQueuePath
(
)
+
+
user
.
getTotalApplications
(
)
+
+
userName
+
+
applicationId
;
LOG
.
info
(
msg
)
;
throw
new
AccessControlException
(
msg
)
;
}
}
finally
{
writeLock
.
unlock
(
)
;
}
try
{
getParent
(
)
.
validateSubmitApplication
(
applicationId
,
userName
,
queue
)
;
}
catch
(
AccessControlException
ace
)
{
public
Resource
getUserAMResourceLimitPerPartition
(
String
nodePartition
,
String
userName
)
{
float
userWeight
=
1.0f
;
if
(
userName
!=
null
&&
getUser
(
userName
)
!=
null
)
{
userWeight
=
getUser
(
userName
)
.
getWeight
(
)
;
}
readLock
.
lock
(
)
;
try
{
float
effectiveUserLimit
=
Math
.
max
(
usersManager
.
getUserLimit
(
)
/
100.0f
,
1.0f
/
Math
.
max
(
getAbstractUsersManager
(
)
.
getNumActiveUsers
(
)
,
1
)
)
;
float
preWeightedUserLimit
=
effectiveUserLimit
;
effectiveUserLimit
=
Math
.
min
(
effectiveUserLimit
*
userWeight
,
1.0f
)
;
Resource
queuePartitionResource
=
getEffectiveCapacity
(
nodePartition
)
;
Resource
userAMLimit
=
Resources
.
multiplyAndNormalizeUp
(
resourceCalculator
,
queuePartitionResource
,
queueCapacities
.
getMaxAMResourcePercentage
(
nodePartition
)
*
effectiveUserLimit
*
usersManager
.
getUserLimitFactor
(
)
,
minimumAllocation
)
;
userAMLimit
=
Resources
.
min
(
resourceCalculator
,
lastClusterResource
,
userAMLimit
,
Resources
.
clone
(
getAMResourceLimitPerPartition
(
nodePartition
)
)
)
;
Resource
preWeighteduserAMLimit
=
Resources
.
multiplyAndNormalizeUp
(
resourceCalculator
,
queuePartitionResource
,
queueCapacities
.
getMaxAMResourcePercentage
(
nodePartition
)
*
preWeightedUserLimit
*
usersManager
.
getUserLimitFactor
(
)
,
minimumAllocation
)
;
preWeighteduserAMLimit
=
Resources
.
min
(
resourceCalculator
,
lastClusterResource
,
preWeighteduserAMLimit
,
Resources
.
clone
(
getAMResourceLimitPerPartition
(
nodePartition
)
)
)
;
queueUsage
.
setUserAMLimit
(
nodePartition
,
preWeighteduserAMLimit
)
;
protected
void
activateApplications
(
)
{
writeLock
.
lock
(
)
;
try
{
Map
<
String
,
Resource
>
userAmPartitionLimit
=
new
HashMap
<
String
,
Resource
>
(
)
;
for
(
String
nodePartition
:
getNodeLabelsForQueue
(
)
)
{
calculateAndGetAMResourceLimitPerPartition
(
nodePartition
)
;
}
for
(
Iterator
<
FiCaSchedulerApp
>
fsApp
=
getPendingAppsOrderingPolicy
(
)
.
getAssignmentIterator
(
IteratorSelector
.
EMPTY_ITERATOR_SELECTOR
)
;
fsApp
.
hasNext
(
)
;
)
{
FiCaSchedulerApp
application
=
fsApp
.
next
(
)
;
ApplicationId
applicationId
=
application
.
getApplicationId
(
)
;
String
partitionName
=
application
.
getAppAMNodePartitionName
(
)
;
Resource
amLimit
=
getAMResourceLimitPerPartition
(
partitionName
)
;
if
(
amLimit
==
null
)
{
amLimit
=
calculateAndGetAMResourceLimitPerPartition
(
partitionName
)
;
}
Resource
amIfStarted
=
Resources
.
add
(
application
.
getAMResource
(
partitionName
)
,
queueUsage
.
getAMUsed
(
partitionName
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
for
(
Iterator
<
FiCaSchedulerApp
>
fsApp
=
getPendingAppsOrderingPolicy
(
)
.
getAssignmentIterator
(
IteratorSelector
.
EMPTY_ITERATOR_SELECTOR
)
;
fsApp
.
hasNext
(
)
;
)
{
FiCaSchedulerApp
application
=
fsApp
.
next
(
)
;
ApplicationId
applicationId
=
application
.
getApplicationId
(
)
;
String
partitionName
=
application
.
getAppAMNodePartitionName
(
)
;
Resource
amLimit
=
getAMResourceLimitPerPartition
(
partitionName
)
;
if
(
amLimit
==
null
)
{
amLimit
=
calculateAndGetAMResourceLimitPerPartition
(
partitionName
)
;
}
Resource
amIfStarted
=
Resources
.
add
(
application
.
getAMResource
(
partitionName
)
,
queueUsage
.
getAMUsed
(
partitionName
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
application
.
getId
(
)
+
+
application
.
getAMResource
(
partitionName
)
+
+
maxAMResourcePerQueuePercent
+
+
amLimit
+
+
lastClusterResource
+
+
amIfStarted
+
+
partitionName
)
;
}
if
(
!
resourceCalculator
.
fitsIn
(
amIfStarted
,
amLimit
)
)
{
if
(
getNumActiveApplications
(
)
<
1
||
(
Resources
.
lessThanOrEqual
(
resourceCalculator
,
lastClusterResource
,
queueUsage
.
getAMUsed
(
partitionName
)
,
Resources
.
none
(
)
)
)
)
{
LOG
.
warn
(
+
+
+
)
;
}
else
{
application
.
updateAMContainerDiagnostics
(
AMState
.
INACTIVATED
,
CSAMContainerLaunchDiagnosticsConstants
.
QUEUE_AM_RESOURCE_LIMIT_EXCEED
)
;
if
(
!
resourceCalculator
.
fitsIn
(
amIfStarted
,
amLimit
)
)
{
if
(
getNumActiveApplications
(
)
<
1
||
(
Resources
.
lessThanOrEqual
(
resourceCalculator
,
lastClusterResource
,
queueUsage
.
getAMUsed
(
partitionName
)
,
Resources
.
none
(
)
)
)
)
{
LOG
.
warn
(
+
+
+
)
;
}
else
{
application
.
updateAMContainerDiagnostics
(
AMState
.
INACTIVATED
,
CSAMContainerLaunchDiagnosticsConstants
.
QUEUE_AM_RESOURCE_LIMIT_EXCEED
)
;
LOG
.
debug
(
+
,
applicationId
,
amIfStarted
,
amLimit
)
;
continue
;
}
}
User
user
=
getUser
(
application
.
getUser
(
)
)
;
Resource
userAMLimit
=
userAmPartitionLimit
.
get
(
partitionName
)
;
if
(
userAMLimit
==
null
)
{
userAMLimit
=
getUserAMResourceLimitPerPartition
(
partitionName
,
application
.
getUser
(
)
)
;
userAmPartitionLimit
.
put
(
partitionName
,
userAMLimit
)
;
}
Resource
userAmIfStarted
=
Resources
.
add
(
application
.
getAMResource
(
partitionName
)
,
user
.
getConsumedAMResources
(
partitionName
)
)
;
if
(
!
resourceCalculator
.
fitsIn
(
userAmIfStarted
,
userAMLimit
)
)
{
if
(
getNumActiveApplications
(
)
<
1
||
(
Resources
.
lessThanOrEqual
(
resourceCalculator
,
lastClusterResource
,
queueUsage
.
getAMUsed
(
partitionName
)
,
Resources
.
none
(
)
)
)
)
{
Resource
userAMLimit
=
userAmPartitionLimit
.
get
(
partitionName
)
;
if
(
userAMLimit
==
null
)
{
userAMLimit
=
getUserAMResourceLimitPerPartition
(
partitionName
,
application
.
getUser
(
)
)
;
userAmPartitionLimit
.
put
(
partitionName
,
userAMLimit
)
;
}
Resource
userAmIfStarted
=
Resources
.
add
(
application
.
getAMResource
(
partitionName
)
,
user
.
getConsumedAMResources
(
partitionName
)
)
;
if
(
!
resourceCalculator
.
fitsIn
(
userAmIfStarted
,
userAMLimit
)
)
{
if
(
getNumActiveApplications
(
)
<
1
||
(
Resources
.
lessThanOrEqual
(
resourceCalculator
,
lastClusterResource
,
queueUsage
.
getAMUsed
(
partitionName
)
,
Resources
.
none
(
)
)
)
)
{
LOG
.
warn
(
+
+
+
)
;
}
else
{
application
.
updateAMContainerDiagnostics
(
AMState
.
INACTIVATED
,
CSAMContainerLaunchDiagnosticsConstants
.
USER_AM_RESOURCE_LIMIT_EXCEED
)
;
LOG
.
debug
(
+
,
applicationId
,
user
,
userAmIfStarted
,
userAMLimit
)
;
continue
;
}
}
user
.
activateApplication
(
)
;
orderingPolicy
.
addSchedulableEntity
(
application
)
;
application
.
updateAMContainerDiagnostics
(
AMState
.
ACTIVATED
,
null
)
;
private
void
addApplicationAttempt
(
FiCaSchedulerApp
application
,
User
user
)
{
writeLock
.
lock
(
)
;
try
{
applicationAttemptMap
.
put
(
application
.
getApplicationAttemptId
(
)
,
application
)
;
if
(
application
.
isRunnable
(
)
)
{
runnableApps
.
add
(
application
)
;
try
{
applicationAttemptMap
.
put
(
application
.
getApplicationAttemptId
(
)
,
application
)
;
if
(
application
.
isRunnable
(
)
)
{
runnableApps
.
add
(
application
)
;
LOG
.
debug
(
,
application
.
getApplicationAttemptId
(
)
)
;
}
else
{
nonRunnableApps
.
add
(
application
)
;
LOG
.
info
(
+
,
application
.
getApplicationAttemptId
(
)
)
;
return
;
}
user
.
submitApplication
(
)
;
getPendingAppsOrderingPolicy
(
)
.
addSchedulableEntity
(
application
)
;
if
(
Resources
.
greaterThan
(
resourceCalculator
,
lastClusterResource
,
lastClusterResource
,
Resources
.
none
(
)
)
)
{
activateApplications
(
)
;
}
else
{
application
.
updateAMContainerDiagnostics
(
AMState
.
INACTIVATED
,
CSAMContainerLaunchDiagnosticsConstants
.
CLUSTER_RESOURCE_EMPTY
)
;
private
void
removeApplicationAttempt
(
FiCaSchedulerApp
application
,
String
userName
)
{
writeLock
.
lock
(
)
;
try
{
User
user
=
usersManager
.
getUserAndAddIfAbsent
(
userName
)
;
boolean
runnable
=
runnableApps
.
remove
(
application
)
;
if
(
!
runnable
)
{
if
(
!
removeNonRunnableApp
(
application
)
)
{
if
(
!
removeNonRunnableApp
(
application
)
)
{
LOG
.
error
(
+
application
+
+
getQueuePath
(
)
)
;
}
}
String
partitionName
=
application
.
getAppAMNodePartitionName
(
)
;
boolean
wasActive
=
orderingPolicy
.
removeSchedulableEntity
(
application
)
;
if
(
!
wasActive
)
{
pendingOrderingPolicy
.
removeSchedulableEntity
(
application
)
;
}
else
{
queueUsage
.
decAMUsed
(
partitionName
,
application
.
getAMResource
(
partitionName
)
)
;
user
.
getResourceUsage
(
)
.
decAMUsed
(
partitionName
,
application
.
getAMResource
(
partitionName
)
)
;
metrics
.
decAMUsed
(
partitionName
,
application
.
getUser
(
)
,
application
.
getAMResource
(
partitionName
)
)
;
}
applicationAttemptMap
.
remove
(
application
.
getApplicationAttemptId
(
)
)
;
user
.
finishApplication
(
wasActive
)
;
if
(
user
.
getTotalApplications
(
)
==
0
)
{
usersManager
.
removeUser
(
application
.
getUser
(
)
)
;
}
activateApplications
(
)
;
@
Override
public
CSAssignment
assignContainers
(
Resource
clusterResource
,
CandidateNodeSet
<
FiCaSchedulerNode
>
candidates
,
ResourceLimits
currentResourceLimits
,
SchedulingMode
schedulingMode
)
{
updateCurrentResourceLimits
(
currentResourceLimits
,
clusterResource
)
;
FiCaSchedulerNode
node
=
CandidateNodeSetUtils
.
getSingleNode
(
candidates
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
}
CachedUserLimit
cul
=
userLimits
.
get
(
application
.
getUser
(
)
)
;
Resource
cachedUserLimit
=
null
;
if
(
cul
!=
null
)
{
cachedUserLimit
=
cul
.
userLimit
;
}
Resource
userLimit
=
computeUserLimitAndSetHeadroom
(
application
,
clusterResource
,
candidates
.
getPartition
(
)
,
schedulingMode
,
cachedUserLimit
)
;
if
(
cul
==
null
)
{
cul
=
new
CachedUserLimit
(
userLimit
)
;
userLimits
.
put
(
application
.
getUser
(
)
,
cul
)
;
}
boolean
userAssignable
=
true
;
if
(
!
cul
.
canAssign
&&
Resources
.
fitsIn
(
appReserved
,
cul
.
reservation
)
)
{
userAssignable
=
false
;
}
else
{
userAssignable
=
canAssignToUser
(
clusterResource
,
application
.
getUser
(
)
,
userLimit
,
application
,
candidates
.
getPartition
(
)
,
currentResourceLimits
)
;
if
(
!
userAssignable
&&
Resources
.
fitsIn
(
cul
.
reservation
,
appReserved
)
)
{
cul
.
canAssign
=
false
;
@
Override
public
boolean
accept
(
Resource
cluster
,
ResourceCommitRequest
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
request
)
{
ContainerAllocationProposal
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
allocation
=
request
.
getFirstAllocatedOrReservedContainer
(
)
;
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
schedulerContainer
=
allocation
.
getAllocatedOrReservedContainer
(
)
;
if
(
allocation
.
getAllocateFromReservedContainer
(
)
==
null
)
{
readLock
.
lock
(
)
;
try
{
FiCaSchedulerApp
app
=
schedulerContainer
.
getSchedulerApplicationAttempt
(
)
;
String
username
=
app
.
getUser
(
)
;
String
p
=
schedulerContainer
.
getNodePartition
(
)
;
Resource
userLimit
=
computeUserLimitAndSetHeadroom
(
app
,
cluster
,
p
,
allocation
.
getSchedulingMode
(
)
,
null
)
;
User
user
=
getUser
(
username
)
;
if
(
user
==
null
)
{
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
schedulerContainer
=
allocation
.
getAllocatedOrReservedContainer
(
)
;
if
(
allocation
.
getAllocateFromReservedContainer
(
)
==
null
)
{
readLock
.
lock
(
)
;
try
{
FiCaSchedulerApp
app
=
schedulerContainer
.
getSchedulerApplicationAttempt
(
)
;
String
username
=
app
.
getUser
(
)
;
String
p
=
schedulerContainer
.
getNodePartition
(
)
;
Resource
userLimit
=
computeUserLimitAndSetHeadroom
(
app
,
cluster
,
p
,
allocation
.
getSchedulingMode
(
)
,
null
)
;
User
user
=
getUser
(
username
)
;
if
(
user
==
null
)
{
LOG
.
debug
(
,
username
)
;
return
false
;
}
Resource
usedResource
=
Resources
.
clone
(
user
.
getUsed
(
p
)
)
;
Resources
.
subtractFrom
(
usedResource
,
request
.
getTotalReleasedResource
(
)
)
;
if
(
Resources
.
greaterThan
(
resourceCalculator
,
cluster
,
usedResource
,
userLimit
)
)
{
@
Lock
(
{
LeafQueue
.
class
}
)
Resource
computeUserLimitAndSetHeadroom
(
FiCaSchedulerApp
application
,
Resource
clusterResource
,
String
nodePartition
,
SchedulingMode
schedulingMode
,
Resource
userLimit
)
{
String
user
=
application
.
getUser
(
)
;
User
queueUser
=
getUser
(
user
)
;
if
(
queueUser
==
null
)
{
@
Private
protected
boolean
canAssignToUser
(
Resource
clusterResource
,
String
userName
,
Resource
limit
,
FiCaSchedulerApp
application
,
String
nodePartition
,
ResourceLimits
currentResourceLimits
)
{
readLock
.
lock
(
)
;
try
{
User
user
=
getUser
(
userName
)
;
if
(
user
==
null
)
{
try
{
User
user
=
getUser
(
userName
)
;
if
(
user
==
null
)
{
LOG
.
debug
(
,
userName
)
;
return
false
;
}
currentResourceLimits
.
setAmountNeededUnreserve
(
Resources
.
none
(
)
)
;
if
(
Resources
.
greaterThan
(
resourceCalculator
,
clusterResource
,
user
.
getUsed
(
nodePartition
)
,
limit
)
)
{
if
(
this
.
reservationsContinueLooking
)
{
if
(
Resources
.
lessThanOrEqual
(
resourceCalculator
,
clusterResource
,
Resources
.
subtract
(
user
.
getUsed
(
)
,
application
.
getCurrentReservation
(
)
)
,
limit
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
userName
+
+
getQueuePath
(
)
+
+
+
user
.
getUsed
(
)
+
+
application
.
getCurrentReservation
(
)
+
+
limit
)
;
}
Resource
amountNeededToUnreserve
=
Resources
.
subtract
(
user
.
getUsed
(
nodePartition
)
,
limit
)
;
currentResourceLimits
.
setAmountNeededUnreserve
(
amountNeededToUnreserve
)
;
return
true
;
}
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
{
super
.
allocateResource
(
clusterResource
,
resource
,
nodePartition
)
;
if
(
null
!=
rmContainer
&&
rmContainer
.
getNodeLabelExpression
(
)
.
equals
(
RMNodeLabelsManager
.
NO_LABEL
)
&&
!
nodePartition
.
equals
(
RMNodeLabelsManager
.
NO_LABEL
)
)
{
TreeSet
<
RMContainer
>
rmContainers
=
null
;
if
(
null
==
(
rmContainers
=
ignorePartitionExclusivityRMContainers
.
get
(
nodePartition
)
)
)
{
rmContainers
=
new
TreeSet
<
>
(
)
;
ignorePartitionExclusivityRMContainers
.
put
(
nodePartition
,
rmContainers
)
;
}
rmContainers
.
add
(
rmContainer
)
;
}
String
userName
=
application
.
getUser
(
)
;
User
user
=
usersManager
.
updateUserResourceUsage
(
userName
,
resource
,
nodePartition
,
true
)
;
Resource
partitionHeadroom
=
Resources
.
createResource
(
0
,
0
)
;
if
(
metrics
.
getUserMetrics
(
userName
)
!=
null
)
{
partitionHeadroom
=
getHeadroom
(
user
,
cachedResourceLimitsForHeadroom
.
getLimit
(
)
,
clusterResource
,
getResourceLimitForActiveUsers
(
userName
,
clusterResource
,
nodePartition
,
SchedulingMode
.
RESPECT_PARTITION_EXCLUSIVITY
)
,
nodePartition
)
;
}
metrics
.
setAvailableResourcesToUser
(
nodePartition
,
userName
,
partitionHeadroom
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
try
{
super
.
releaseResource
(
clusterResource
,
resource
,
nodePartition
)
;
if
(
null
!=
rmContainer
&&
rmContainer
.
getNodeLabelExpression
(
)
.
equals
(
RMNodeLabelsManager
.
NO_LABEL
)
&&
!
nodePartition
.
equals
(
RMNodeLabelsManager
.
NO_LABEL
)
)
{
if
(
ignorePartitionExclusivityRMContainers
.
containsKey
(
nodePartition
)
)
{
Set
<
RMContainer
>
rmContainers
=
ignorePartitionExclusivityRMContainers
.
get
(
nodePartition
)
;
rmContainers
.
remove
(
rmContainer
)
;
if
(
rmContainers
.
isEmpty
(
)
)
{
ignorePartitionExclusivityRMContainers
.
remove
(
nodePartition
)
;
}
}
}
String
userName
=
application
.
getUser
(
)
;
User
user
=
usersManager
.
updateUserResourceUsage
(
userName
,
resource
,
nodePartition
,
false
)
;
Resource
partitionHeadroom
=
Resources
.
createResource
(
0
,
0
)
;
if
(
metrics
.
getUserMetrics
(
userName
)
!=
null
)
{
partitionHeadroom
=
getHeadroom
(
user
,
cachedResourceLimitsForHeadroom
.
getLimit
(
)
,
clusterResource
,
getResourceLimitForActiveUsers
(
userName
,
clusterResource
,
nodePartition
,
SchedulingMode
.
RESPECT_PARTITION_EXCLUSIVITY
)
,
nodePartition
)
;
}
metrics
.
setAvailableResourcesToUser
(
nodePartition
,
userName
,
partitionHeadroom
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
attachContainer
(
Resource
clusterResource
,
FiCaSchedulerApp
application
,
RMContainer
rmContainer
)
{
if
(
application
!=
null
&&
rmContainer
!=
null
&&
rmContainer
.
getExecutionType
(
)
==
ExecutionType
.
GUARANTEED
)
{
FiCaSchedulerNode
node
=
scheduler
.
getNode
(
rmContainer
.
getContainer
(
)
.
getNodeId
(
)
)
;
allocateResource
(
clusterResource
,
application
,
rmContainer
.
getContainer
(
)
.
getResource
(
)
,
node
.
getPartition
(
)
,
rmContainer
)
;
@
Override
public
void
detachContainer
(
Resource
clusterResource
,
FiCaSchedulerApp
application
,
RMContainer
rmContainer
)
{
if
(
application
!=
null
&&
rmContainer
!=
null
&&
rmContainer
.
getExecutionType
(
)
==
ExecutionType
.
GUARANTEED
)
{
FiCaSchedulerNode
node
=
scheduler
.
getNode
(
rmContainer
.
getContainer
(
)
.
getNodeId
(
)
)
;
releaseResource
(
clusterResource
,
application
,
rmContainer
.
getContainer
(
)
.
getResource
(
)
,
node
.
getPartition
(
)
,
rmContainer
)
;
validate
(
newlyParsedQueue
)
;
shouldFailAutoCreationWhenGuaranteedCapacityExceeded
=
csContext
.
getConfiguration
(
)
.
getShouldFailAutoQueueCreationWhenGuaranteedCapacityExceeded
(
getQueuePath
(
)
)
;
if
(
shouldFailAutoCreationWhenGuaranteedCapacityExceeded
)
{
float
childCap
=
sumOfChildCapacities
(
)
;
if
(
getCapacity
(
)
<
childCap
)
{
throw
new
IOException
(
+
childCap
+
+
getQueuePath
(
)
+
+
getCapacity
(
)
+
+
+
)
;
}
}
leafQueueTemplate
=
initializeLeafQueueConfigs
(
)
.
build
(
)
;
super
.
reinitialize
(
newlyParsedQueue
,
clusterResource
)
;
for
(
CSQueue
res
:
this
.
getChildQueues
(
)
)
{
res
.
reinitialize
(
res
,
clusterResource
)
;
}
reinitializeQueueManagementPolicy
(
)
;
final
List
<
QueueManagementChange
>
queueManagementChanges
=
queueManagementPolicy
.
computeQueueManagementChanges
(
)
;
validateAndApplyQueueManagementChanges
(
queueManagementChanges
)
;
LOG
.
info
(
+
,
queueName
,
super
.
getCapacity
(
)
,
super
.
getMaximumCapacity
(
)
)
;
}
catch
(
YarnException
ye
)
{
ParentQueue
newlyParsedParentQueue
=
(
ParentQueue
)
newlyParsedQueue
;
setupQueueConfigs
(
clusterResource
)
;
Map
<
String
,
CSQueue
>
currentChildQueues
=
getQueuesMap
(
childQueues
)
;
Map
<
String
,
CSQueue
>
newChildQueues
=
getQueuesMap
(
newlyParsedParentQueue
.
childQueues
)
;
for
(
Map
.
Entry
<
String
,
CSQueue
>
e
:
newChildQueues
.
entrySet
(
)
)
{
String
newChildQueueName
=
e
.
getKey
(
)
;
CSQueue
newChildQueue
=
e
.
getValue
(
)
;
CSQueue
childQueue
=
currentChildQueues
.
get
(
newChildQueueName
)
;
if
(
childQueue
!=
null
)
{
if
(
(
childQueue
instanceof
LeafQueue
&&
newChildQueue
instanceof
ParentQueue
)
||
(
childQueue
instanceof
ParentQueue
&&
newChildQueue
instanceof
LeafQueue
)
)
{
newChildQueue
.
setParent
(
this
)
;
currentChildQueues
.
put
(
newChildQueueName
,
newChildQueue
)
;
CapacitySchedulerQueueManager
queueManager
=
this
.
csContext
.
getCapacitySchedulerQueueManager
(
)
;
queueManager
.
addQueue
(
newChildQueueName
,
newChildQueue
)
;
continue
;
for
(
Map
.
Entry
<
String
,
CSQueue
>
e
:
newChildQueues
.
entrySet
(
)
)
{
String
newChildQueueName
=
e
.
getKey
(
)
;
CSQueue
newChildQueue
=
e
.
getValue
(
)
;
CSQueue
childQueue
=
currentChildQueues
.
get
(
newChildQueueName
)
;
if
(
childQueue
!=
null
)
{
if
(
(
childQueue
instanceof
LeafQueue
&&
newChildQueue
instanceof
ParentQueue
)
||
(
childQueue
instanceof
ParentQueue
&&
newChildQueue
instanceof
LeafQueue
)
)
{
newChildQueue
.
setParent
(
this
)
;
currentChildQueues
.
put
(
newChildQueueName
,
newChildQueue
)
;
CapacitySchedulerQueueManager
queueManager
=
this
.
csContext
.
getCapacitySchedulerQueueManager
(
)
;
queueManager
.
addQueue
(
newChildQueueName
,
newChildQueue
)
;
continue
;
}
childQueue
.
reinitialize
(
newChildQueue
,
clusterResource
)
;
LOG
.
info
(
getQueuePath
(
)
+
+
childQueue
)
;
}
else
{
newChildQueue
.
setParent
(
this
)
;
private
void
addApplication
(
ApplicationId
applicationId
,
String
user
)
{
writeLock
.
lock
(
)
;
try
{
++
numApplications
;
private
void
removeApplication
(
ApplicationId
applicationId
,
String
user
)
{
writeLock
.
lock
(
)
;
try
{
--
numApplications
;
@
Override
public
CSAssignment
assignContainers
(
Resource
clusterResource
,
CandidateNodeSet
<
FiCaSchedulerNode
>
candidates
,
ResourceLimits
resourceLimits
,
SchedulingMode
schedulingMode
)
{
FiCaSchedulerNode
node
=
CandidateNodeSetUtils
.
getSingleNode
(
candidates
)
;
if
(
schedulingMode
==
SchedulingMode
.
RESPECT_PARTITION_EXCLUSIVITY
&&
!
accessibleToPartition
(
candidates
.
getPartition
(
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
now
=
System
.
currentTimeMillis
(
)
;
if
(
now
-
this
.
lastSkipQueueDebugLoggingTimestamp
>
1000
)
{
FiCaSchedulerNode
node
=
CandidateNodeSetUtils
.
getSingleNode
(
candidates
)
;
if
(
schedulingMode
==
SchedulingMode
.
RESPECT_PARTITION_EXCLUSIVITY
&&
!
accessibleToPartition
(
candidates
.
getPartition
(
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
now
=
System
.
currentTimeMillis
(
)
;
if
(
now
-
this
.
lastSkipQueueDebugLoggingTimestamp
>
1000
)
{
LOG
.
debug
(
+
getQueuePath
(
)
+
+
candidates
.
getPartition
(
)
)
;
this
.
lastSkipQueueDebugLoggingTimestamp
=
now
;
}
}
ActivitiesLogger
.
QUEUE
.
recordQueueActivity
(
activitiesManager
,
node
,
getParentName
(
)
,
getQueuePath
(
)
,
ActivityState
.
REJECTED
,
ActivityDiagnosticConstant
.
QUEUE_NOT_ABLE_TO_ACCESS_PARTITION
)
;
if
(
rootQueue
)
{
ActivitiesLogger
.
NODE
.
finishSkippedNodeAllocation
(
activitiesManager
,
node
)
;
}
return
CSAssignment
.
NULL_ASSIGNMENT
;
}
if
(
!
super
.
hasPendingResourceRequest
(
candidates
.
getPartition
(
)
,
clusterResource
,
schedulingMode
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
long
now
=
System
.
currentTimeMillis
(
)
;
if
(
now
-
this
.
lastSkipQueueDebugLoggingTimestamp
>
1000
)
{
ActivitiesLogger
.
NODE
.
finishSkippedNodeAllocation
(
activitiesManager
,
node
)
;
}
break
;
}
CSAssignment
assignedToChild
=
assignContainersToChildQueues
(
clusterResource
,
candidates
,
resourceLimits
,
schedulingMode
)
;
assignment
.
setType
(
assignedToChild
.
getType
(
)
)
;
assignment
.
setRequestLocalityType
(
assignedToChild
.
getRequestLocalityType
(
)
)
;
assignment
.
setExcessReservation
(
assignedToChild
.
getExcessReservation
(
)
)
;
assignment
.
setContainersToKill
(
assignedToChild
.
getContainersToKill
(
)
)
;
assignment
.
setFulfilledReservation
(
assignedToChild
.
isFulfilledReservation
(
)
)
;
assignment
.
setFulfilledReservedContainer
(
assignedToChild
.
getFulfilledReservedContainer
(
)
)
;
if
(
Resources
.
greaterThan
(
resourceCalculator
,
clusterResource
,
assignedToChild
.
getResource
(
)
,
Resources
.
none
(
)
)
)
{
ActivitiesLogger
.
QUEUE
.
recordQueueActivity
(
activitiesManager
,
node
,
getParentName
(
)
,
getQueuePath
(
)
,
ActivityState
.
ACCEPTED
,
ActivityDiagnosticConstant
.
EMPTY
)
;
boolean
isReserved
=
assignedToChild
.
getAssignmentInformation
(
)
.
getReservationDetails
(
)
!=
null
&&
!
assignedToChild
.
getAssignmentInformation
(
)
.
getReservationDetails
(
)
.
isEmpty
(
)
;
if
(
rootQueue
)
{
ActivitiesLogger
.
NODE
.
finishAllocatedNodeAllocation
(
activitiesManager
,
node
,
assignedToChild
.
getAssignmentInformation
(
)
.
getFirstAllocatedOrReservedContainerId
(
)
,
isReserved
?
AllocationState
.
RESERVED
:
AllocationState
.
ALLOCATED
)
;
}
Resources
.
addTo
(
assignment
.
getResource
(
)
,
assignedToChild
.
getResource
(
)
)
;
}
break
;
}
CSAssignment
assignedToChild
=
assignContainersToChildQueues
(
clusterResource
,
candidates
,
resourceLimits
,
schedulingMode
)
;
assignment
.
setType
(
assignedToChild
.
getType
(
)
)
;
assignment
.
setRequestLocalityType
(
assignedToChild
.
getRequestLocalityType
(
)
)
;
assignment
.
setExcessReservation
(
assignedToChild
.
getExcessReservation
(
)
)
;
assignment
.
setContainersToKill
(
assignedToChild
.
getContainersToKill
(
)
)
;
assignment
.
setFulfilledReservation
(
assignedToChild
.
isFulfilledReservation
(
)
)
;
assignment
.
setFulfilledReservedContainer
(
assignedToChild
.
getFulfilledReservedContainer
(
)
)
;
if
(
Resources
.
greaterThan
(
resourceCalculator
,
clusterResource
,
assignedToChild
.
getResource
(
)
,
Resources
.
none
(
)
)
)
{
ActivitiesLogger
.
QUEUE
.
recordQueueActivity
(
activitiesManager
,
node
,
getParentName
(
)
,
getQueuePath
(
)
,
ActivityState
.
ACCEPTED
,
ActivityDiagnosticConstant
.
EMPTY
)
;
boolean
isReserved
=
assignedToChild
.
getAssignmentInformation
(
)
.
getReservationDetails
(
)
!=
null
&&
!
assignedToChild
.
getAssignmentInformation
(
)
.
getReservationDetails
(
)
.
isEmpty
(
)
;
if
(
rootQueue
)
{
ActivitiesLogger
.
NODE
.
finishAllocatedNodeAllocation
(
activitiesManager
,
node
,
assignedToChild
.
getAssignmentInformation
(
)
.
getFirstAllocatedOrReservedContainerId
(
)
,
isReserved
?
AllocationState
.
RESERVED
:
AllocationState
.
ALLOCATED
)
;
}
Resources
.
addTo
(
assignment
.
getResource
(
)
,
assignedToChild
.
getResource
(
)
)
;
Resources
.
addTo
(
assignment
.
getAssignmentInformation
(
)
.
getAllocated
(
)
,
assignedToChild
.
getAssignmentInformation
(
)
.
getAllocated
(
)
)
;
private
CSAssignment
assignContainersToChildQueues
(
Resource
cluster
,
CandidateNodeSet
<
FiCaSchedulerNode
>
candidates
,
ResourceLimits
limits
,
SchedulingMode
schedulingMode
)
{
CSAssignment
assignment
=
CSAssignment
.
NULL_ASSIGNMENT
;
printChildQueues
(
)
;
for
(
Iterator
<
CSQueue
>
iter
=
sortAndGetChildrenAllocationIterator
(
candidates
.
getPartition
(
)
)
;
iter
.
hasNext
(
)
;
)
{
CSQueue
childQueue
=
iter
.
next
(
)
;
LOG
.
debug
(
+
childQueue
.
getQueuePath
(
)
+
+
childQueue
+
+
childAssignment
.
getResource
(
)
+
+
childAssignment
.
getType
(
)
)
;
}
if
(
Resources
.
greaterThan
(
resourceCalculator
,
cluster
,
childAssignment
.
getResource
(
)
,
Resources
.
none
(
)
)
)
{
assignment
=
childAssignment
;
break
;
}
else
if
(
childAssignment
.
getSkippedType
(
)
==
CSAssignment
.
SkippedType
.
QUEUE_LIMIT
)
{
if
(
assignment
.
getSkippedType
(
)
!=
CSAssignment
.
SkippedType
.
QUEUE_LIMIT
)
{
assignment
=
childAssignment
;
}
Resource
blockedHeadroom
=
null
;
if
(
childQueue
instanceof
LeafQueue
)
{
blockedHeadroom
=
childLimits
.
getHeadroom
(
)
;
}
else
{
blockedHeadroom
=
childLimits
.
getBlockedHeadroom
(
)
;
}
Resource
resourceToSubtract
=
Resources
.
max
(
resourceCalculator
,
cluster
,
blockedHeadroom
,
Resources
.
none
(
)
)
;
limits
.
addBlockedHeadroom
(
resourceToSubtract
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
internalReleaseResource
(
Resource
clusterResource
,
FiCaSchedulerNode
node
,
Resource
releasedResource
)
{
writeLock
.
lock
(
)
;
try
{
super
.
releaseResource
(
clusterResource
,
releasedResource
,
node
.
getPartition
(
)
)
;
else
{
if
(
Resources
.
lessThan
(
rc
,
clusterResource
,
queueResourceQuotas
.
getEffectiveMinResource
(
label
)
,
configuredMinResources
)
)
{
numeratorForMinRatio
=
queueResourceQuotas
.
getEffectiveMinResource
(
label
)
;
}
}
Map
<
String
,
Float
>
effectiveMinRatioPerResource
=
getEffectiveMinRatioPerResource
(
configuredMinResources
,
numeratorForMinRatio
)
;
for
(
CSQueue
childQueue
:
getChildQueues
(
)
)
{
Resource
minResource
=
childQueue
.
getQueueResourceQuotas
(
)
.
getConfiguredMinResource
(
label
)
;
if
(
childQueue
.
getCapacityConfigType
(
)
.
equals
(
CapacityConfigType
.
ABSOLUTE_RESOURCE
)
)
{
childQueue
.
getQueueResourceQuotas
(
)
.
setEffectiveMinResource
(
label
,
getMinResourceNormalized
(
childQueue
.
getQueuePath
(
)
,
effectiveMinRatioPerResource
,
minResource
)
)
;
Resource
parentMaxRes
=
queueResourceQuotas
.
getConfiguredMaxResource
(
label
)
;
if
(
parent
!=
null
&&
parentMaxRes
.
equals
(
Resources
.
none
(
)
)
)
{
parentMaxRes
=
parent
.
getQueueResourceQuotas
(
)
.
getEffectiveMaxResource
(
label
)
;
}
Resource
childMaxResource
=
childQueue
.
getQueueResourceQuotas
(
)
.
getConfiguredMaxResource
(
label
)
;
Resource
effMaxResource
=
Resources
.
min
(
resourceCalculator
,
resourceByLabel
,
childMaxResource
.
equals
(
Resources
.
none
(
)
)
?
parentMaxRes
:
childMaxResource
,
parentMaxRes
)
;
childQueue
.
getQueueResourceQuotas
(
)
.
setEffectiveMaxResource
(
label
,
Resources
.
clone
(
effMaxResource
)
)
;
deriveCapacityFromAbsoluteConfigurations
(
label
,
clusterResource
,
rc
,
childQueue
)
;
private
Resource
getMinResourceNormalized
(
String
name
,
Map
<
String
,
Float
>
effectiveMinRatio
,
Resource
minResource
)
{
Resource
ret
=
Resource
.
newInstance
(
minResource
)
;
int
maxLength
=
ResourceUtils
.
getNumberOfCountableResourceTypes
(
)
;
for
(
int
i
=
0
;
i
<
maxLength
;
i
++
)
{
ResourceInformation
nResourceInformation
=
minResource
.
getResourceInformation
(
i
)
;
Float
ratio
=
effectiveMinRatio
.
get
(
nResourceInformation
.
getName
(
)
)
;
if
(
ratio
!=
null
)
{
ret
.
setResourceValue
(
i
,
(
long
)
(
nResourceInformation
.
getValue
(
)
*
ratio
.
floatValue
(
)
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
childQueue
.
getQueueCapacities
(
)
.
setAbsoluteCapacity
(
label
,
childQueue
.
getQueueCapacities
(
)
.
getCapacity
(
label
)
*
getQueueCapacities
(
)
.
getAbsoluteCapacity
(
label
)
)
;
childQueue
.
getQueueCapacities
(
)
.
setAbsoluteMaximumCapacity
(
label
,
childQueue
.
getQueueCapacities
(
)
.
getMaximumCapacity
(
label
)
*
getQueueCapacities
(
)
.
getAbsoluteMaximumCapacity
(
label
)
)
;
if
(
childQueue
instanceof
LeafQueue
)
{
LeafQueue
leafQueue
=
(
LeafQueue
)
childQueue
;
CapacitySchedulerConfiguration
conf
=
csContext
.
getConfiguration
(
)
;
int
maxApplications
=
conf
.
getMaximumApplicationsPerQueue
(
childQueue
.
getQueuePath
(
)
)
;
if
(
maxApplications
<
0
)
{
int
maxGlobalPerQueueApps
=
conf
.
getGlobalMaximumApplicationsPerQueue
(
)
;
if
(
maxGlobalPerQueueApps
>
0
)
{
maxApplications
=
(
int
)
(
maxGlobalPerQueueApps
*
childQueue
.
getQueueCapacities
(
)
.
getAbsoluteCapacity
(
label
)
)
;
}
else
{
maxApplications
=
(
int
)
(
conf
.
getMaximumSystemApplications
(
)
*
childQueue
.
getQueueCapacities
(
)
.
getAbsoluteCapacity
(
label
)
)
;
}
}
leafQueue
.
setMaxApplications
(
maxApplications
)
;
int
maxApplicationsPerUser
=
Math
.
min
(
maxApplications
,
(
int
)
(
maxApplications
*
(
leafQueue
.
getUsersManager
(
)
.
getUserLimit
(
)
/
100.0f
)
*
leafQueue
.
getUsersManager
(
)
.
getUserLimitFactor
(
)
)
)
;
leafQueue
.
setMaxApplicationsPerUser
(
maxApplicationsPerUser
)
;
@
Override
public
void
attachContainer
(
Resource
clusterResource
,
FiCaSchedulerApp
application
,
RMContainer
rmContainer
)
{
if
(
application
!=
null
)
{
FiCaSchedulerNode
node
=
scheduler
.
getNode
(
rmContainer
.
getContainer
(
)
.
getNodeId
(
)
)
;
allocateResource
(
clusterResource
,
rmContainer
.
getContainer
(
)
.
getResource
(
)
,
node
.
getPartition
(
)
)
;
@
Override
public
void
detachContainer
(
Resource
clusterResource
,
FiCaSchedulerApp
application
,
RMContainer
rmContainer
)
{
if
(
application
!=
null
)
{
FiCaSchedulerNode
node
=
scheduler
.
getNode
(
rmContainer
.
getContainer
(
)
.
getNodeId
(
)
)
;
super
.
releaseResource
(
clusterResource
,
rmContainer
.
getContainer
(
)
.
getResource
(
)
,
node
.
getPartition
(
)
)
;
public
void
apply
(
Resource
cluster
,
ResourceCommitRequest
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
request
)
{
if
(
request
.
anythingAllocatedOrReserved
(
)
)
{
ContainerAllocationProposal
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
allocation
=
request
.
getFirstAllocatedOrReservedContainer
(
)
;
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
schedulerContainer
=
allocation
.
getAllocatedOrReservedContainer
(
)
;
if
(
allocation
.
getAllocateFromReservedContainer
(
)
==
null
)
{
writeLock
.
lock
(
)
;
try
{
allocateResource
(
cluster
,
allocation
.
getAllocatedOrReservedResource
(
)
,
schedulerContainer
.
getNodePartition
(
)
)
;
List
<
QueueManagementChange
>
queueManagementChanges
=
Collections
.
emptyList
(
)
;
if
(
!
parentQueue
.
shouldFailAutoCreationWhenGuaranteedCapacityExceeded
(
)
)
{
AutoCreatedQueueManagementPolicy
policyClazz
=
parentQueue
.
getAutoCreatedQueueManagementPolicy
(
)
;
long
startTime
=
0
;
try
{
startTime
=
clock
.
getTime
(
)
;
queueManagementChanges
=
policyClazz
.
computeQueueManagementChanges
(
)
;
if
(
queueManagementChanges
.
size
(
)
>
0
)
{
QueueManagementChangeEvent
queueManagementChangeEvent
=
new
QueueManagementChangeEvent
(
parentQueue
,
queueManagementChanges
)
;
scheduler
.
getRMContext
(
)
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
queueManagementChangeEvent
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
policyClazz
.
getClass
(
)
.
getName
(
)
,
clock
.
getTime
(
)
-
startTime
)
;
if
(
queueManagementChanges
.
size
(
)
>
0
)
{
LOG
.
debug
(
+
+
,
parentQueue
.
getQueuePath
(
)
,
queueManagementChanges
.
size
(
)
<
25
?
queueManagementChanges
.
toString
(
)
:
queueManagementChanges
.
size
(
)
)
;
}
}
}
catch
(
YarnException
e
)
{
AutoCreatedQueueManagementPolicy
policyClazz
=
parentQueue
.
getAutoCreatedQueueManagementPolicy
(
)
;
long
startTime
=
0
;
try
{
startTime
=
clock
.
getTime
(
)
;
queueManagementChanges
=
policyClazz
.
computeQueueManagementChanges
(
)
;
if
(
queueManagementChanges
.
size
(
)
>
0
)
{
QueueManagementChangeEvent
queueManagementChangeEvent
=
new
QueueManagementChangeEvent
(
parentQueue
,
queueManagementChanges
)
;
scheduler
.
getRMContext
(
)
.
getDispatcher
(
)
.
getEventHandler
(
)
.
handle
(
queueManagementChangeEvent
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
,
policyClazz
.
getClass
(
)
.
getName
(
)
,
clock
.
getTime
(
)
-
startTime
)
;
if
(
queueManagementChanges
.
size
(
)
>
0
)
{
LOG
.
debug
(
+
+
,
parentQueue
.
getQueuePath
(
)
,
queueManagementChanges
.
size
(
)
<
25
?
queueManagementChanges
.
toString
(
)
:
queueManagementChanges
.
size
(
)
)
;
}
}
}
catch
(
YarnException
e
)
{
LOG
.
error
(
+
+
parentQueue
.
getQueuePath
(
)
,
e
)
;
}
}
else
{
Resource
consumed
=
Resources
.
multiplyAndNormalizeUp
(
resourceCalculator
,
partitionResource
,
getUsageRatio
(
nodePartition
)
,
lQueue
.
getMinimumAllocation
(
)
)
;
Resource
currentCapacity
=
Resources
.
lessThan
(
resourceCalculator
,
partitionResource
,
consumed
,
queueCapacity
)
?
queueCapacity
:
Resources
.
add
(
consumed
,
required
)
;
float
usersSummedByWeight
=
activeUsersTimesWeights
;
Resource
resourceUsed
=
Resources
.
add
(
totalResUsageForActiveUsers
.
getUsed
(
nodePartition
)
,
required
)
;
if
(
!
activeUser
)
{
resourceUsed
=
currentCapacity
;
usersSummedByWeight
=
allUsersTimesWeights
;
}
Resource
userLimitResource
=
Resources
.
max
(
resourceCalculator
,
partitionResource
,
Resources
.
divideAndCeil
(
resourceCalculator
,
resourceUsed
,
usersSummedByWeight
)
,
Resources
.
divideAndCeil
(
resourceCalculator
,
Resources
.
multiplyAndRoundDown
(
currentCapacity
,
getUserLimit
(
)
)
,
100
)
)
;
Resource
maxUserLimit
=
Resources
.
none
(
)
;
if
(
schedulingMode
==
SchedulingMode
.
RESPECT_PARTITION_EXCLUSIVITY
)
{
maxUserLimit
=
Resources
.
multiplyAndRoundDown
(
queueCapacity
,
getUserLimitFactor
(
)
)
;
}
else
if
(
schedulingMode
==
SchedulingMode
.
IGNORE_PARTITION_EXCLUSIVITY
)
{
maxUserLimit
=
partitionResource
;
}
userLimitResource
=
Resources
.
roundUp
(
resourceCalculator
,
Resources
.
min
(
resourceCalculator
,
partitionResource
,
userLimitResource
,
maxUserLimit
)
,
lQueue
.
getMinimumAllocation
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
updateNonActiveUsersResourceUsage
(
String
userName
)
{
this
.
writeLock
.
lock
(
)
;
try
{
User
user
=
getUser
(
userName
)
;
if
(
user
==
null
)
return
;
ResourceUsage
resourceUsage
=
user
.
getResourceUsage
(
)
;
if
(
activeUsersSet
.
contains
(
userName
)
)
{
activeUsersSet
.
remove
(
userName
)
;
nonActiveUsersSet
.
add
(
userName
)
;
activeUsersTimesWeights
=
sumActiveUsersTimesWeights
(
)
;
for
(
String
partition
:
resourceUsage
.
getNodePartitionsSet
(
)
)
{
totalResUsageForActiveUsers
.
decUsed
(
partition
,
resourceUsage
.
getUsed
(
partition
)
)
;
totalResUsageForNonActiveUsers
.
incUsed
(
partition
,
resourceUsage
.
getUsed
(
partition
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
VisibleForTesting
public
void
initialize
(
CapacityScheduler
scheduler
)
throws
IOException
{
this
.
scheduler
=
scheduler
;
this
.
conf
=
scheduler
.
getConfiguration
(
)
;
boolean
overrideWithWorkflowPriorityMappings
=
conf
.
getOverrideWithWorkflowPriorityMappings
(
)
;
protected
CSAssignment
getCSAssignmentFromAllocateResult
(
Resource
clusterResource
,
ContainerAllocation
result
,
RMContainer
rmContainer
,
FiCaSchedulerNode
node
)
{
CSAssignment
.
SkippedType
skipped
=
(
result
.
getAllocationState
(
)
==
AllocationState
.
APP_SKIPPED
)
?
CSAssignment
.
SkippedType
.
OTHER
:
CSAssignment
.
SkippedType
.
NONE
;
CSAssignment
assignment
=
new
CSAssignment
(
skipped
)
;
assignment
.
setApplication
(
application
)
;
assignment
.
setExcessReservation
(
result
.
getContainerToBeUnreserved
(
)
)
;
assignment
.
setRequestLocalityType
(
result
.
requestLocalityType
)
;
if
(
Resources
.
greaterThan
(
rc
,
clusterResource
,
result
.
getResourceToBeAllocated
(
)
,
Resources
.
none
(
)
)
)
{
Resource
allocatedResource
=
result
.
getResourceToBeAllocated
(
)
;
RMContainer
updatedContainer
=
result
.
getUpdatedContainer
(
)
;
assignment
.
setResource
(
allocatedResource
)
;
assignment
.
setType
(
result
.
getContainerNodeType
(
)
)
;
if
(
result
.
getAllocationState
(
)
==
AllocationState
.
RESERVED
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
Resource
allocatedResource
=
result
.
getResourceToBeAllocated
(
)
;
RMContainer
updatedContainer
=
result
.
getUpdatedContainer
(
)
;
assignment
.
setResource
(
allocatedResource
)
;
assignment
.
setType
(
result
.
getContainerNodeType
(
)
)
;
if
(
result
.
getAllocationState
(
)
==
AllocationState
.
RESERVED
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
application
.
getApplicationId
(
)
+
+
allocatedResource
+
+
appInfo
.
getQueueName
(
)
+
+
clusterResource
)
;
}
assignment
.
getAssignmentInformation
(
)
.
addReservationDetails
(
updatedContainer
,
application
.
getCSLeafQueue
(
)
.
getQueuePath
(
)
)
;
assignment
.
getAssignmentInformation
(
)
.
incrReservations
(
)
;
Resources
.
addTo
(
assignment
.
getAssignmentInformation
(
)
.
getReserved
(
)
,
allocatedResource
)
;
if
(
rmContainer
!=
null
)
{
ActivitiesLogger
.
APP
.
finishSkippedAppAllocationRecording
(
activitiesManager
,
application
.
getApplicationId
(
)
,
ActivityState
.
SKIPPED
,
ActivityDiagnosticConstant
.
EMPTY
)
;
}
else
{
ActivitiesLogger
.
APP
.
finishAllocatedAppAllocationRecording
(
activitiesManager
,
application
.
getApplicationId
(
)
,
updatedContainer
.
getContainerId
(
)
,
ActivityState
.
RESERVED
,
ActivityDiagnosticConstant
.
EMPTY
)
;
}
}
else
if
(
result
.
getAllocationState
(
)
==
AllocationState
.
ALLOCATED
)
{
}
if
(
schedulingMode
==
SchedulingMode
.
IGNORE_PARTITION_EXCLUSIVITY
)
{
if
(
application
.
isWaitingForAMContainer
(
)
)
{
LOG
.
debug
(
+
,
application
.
getApplicationAttemptId
(
)
)
;
application
.
updateAppSkipNodeDiagnostics
(
)
;
ActivitiesLogger
.
APP
.
recordSkippedAppActivityWithoutAllocation
(
activitiesManager
,
node
,
application
,
schedulerKey
,
ActivityDiagnosticConstant
.
REQUEST_SKIPPED_IN_IGNORE_EXCLUSIVITY_MODE
,
ActivityLevel
.
REQUEST
)
;
return
ContainerAllocation
.
APP_SKIPPED
;
}
}
Optional
<
DiagnosticsCollector
>
dcOpt
=
activitiesManager
==
null
?
Optional
.
empty
(
)
:
activitiesManager
.
getOptionalDiagnosticsCollector
(
)
;
if
(
!
appInfo
.
precheckNode
(
schedulerKey
,
node
,
schedulingMode
,
dcOpt
)
)
{
ActivitiesLogger
.
APP
.
recordSkippedAppActivityWithoutAllocation
(
activitiesManager
,
node
,
application
,
schedulerKey
,
ActivityDiagnosticConstant
.
NODE_DO_NOT_MATCH_PARTITION_OR_PLACEMENT_CONSTRAINTS
+
ActivitiesManager
.
getDiagnostics
(
dcOpt
)
,
ActivityLevel
.
NODE
)
;
return
ContainerAllocation
.
PRIORITY_SKIPPED
;
}
if
(
!
application
.
getCSLeafQueue
(
)
.
getReservationContinueLooking
(
)
)
{
if
(
!
shouldAllocOrReserveNewContainer
(
schedulerKey
,
required
)
)
{
LOG
.
debug
(
)
;
ActivitiesLogger
.
APP
.
recordSkippedAppActivityWithoutAllocation
(
activitiesManager
,
node
,
application
,
schedulerKey
,
ActivityDiagnosticConstant
.
REQUEST_SKIPPED_BECAUSE_OF_RESERVATION
,
ActivityLevel
.
REQUEST
)
;
return
ContainerAllocation
.
PRIORITY_SKIPPED
;
if
(
!
application
.
getCSLeafQueue
(
)
.
getReservationContinueLooking
(
)
)
{
if
(
!
shouldAllocOrReserveNewContainer
(
schedulerKey
,
required
)
)
{
LOG
.
debug
(
)
;
ActivitiesLogger
.
APP
.
recordSkippedAppActivityWithoutAllocation
(
activitiesManager
,
node
,
application
,
schedulerKey
,
ActivityDiagnosticConstant
.
REQUEST_SKIPPED_BECAUSE_OF_RESERVATION
,
ActivityLevel
.
REQUEST
)
;
return
ContainerAllocation
.
PRIORITY_SKIPPED
;
}
}
if
(
!
checkHeadroom
(
clusterResource
,
resourceLimits
,
required
,
node
.
getPartition
(
)
)
)
{
LOG
.
debug
(
,
required
)
;
ActivitiesLogger
.
APP
.
recordAppActivityWithoutAllocation
(
activitiesManager
,
node
,
application
,
schedulerKey
,
ActivityDiagnosticConstant
.
QUEUE_DO_NOT_HAVE_ENOUGH_HEADROOM
,
ActivityState
.
REJECTED
,
ActivityLevel
.
REQUEST
)
;
return
ContainerAllocation
.
QUEUE_SKIPPED
;
}
int
missedNonPartitionedRequestSchedulingOpportunity
=
0
;
AppPlacementAllocator
appPlacementAllocator
=
appInfo
.
getAppPlacementAllocator
(
schedulerKey
)
;
if
(
null
==
appPlacementAllocator
)
{
ActivitiesLogger
.
APP
.
recordSkippedAppActivityWithoutAllocation
(
activitiesManager
,
node
,
application
,
schedulerKey
,
ActivityDiagnosticConstant
.
REQUEST_SKIPPED_BECAUSE_NULL_ANY_REQUEST
,
ActivityLevel
.
REQUEST
)
;
return
ContainerAllocation
.
PRIORITY_SKIPPED
;
}
String
requestPartition
=
appPlacementAllocator
.
getPrimaryRequestedNodePartition
(
)
;
private
ContainerAllocation
assignContainer
(
Resource
clusterResource
,
FiCaSchedulerNode
node
,
SchedulerRequestKey
schedulerKey
,
PendingAsk
pendingAsk
,
NodeType
type
,
RMContainer
rmContainer
,
SchedulingMode
schedulingMode
,
ResourceLimits
currentResoureLimits
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
boolean
shouldAllocOrReserveNewContainer
(
SchedulerRequestKey
schedulerKey
,
Resource
required
)
{
int
requiredContainers
=
application
.
getOutstandingAsksCount
(
schedulerKey
)
;
int
reservedContainers
=
application
.
getNumReservedContainers
(
schedulerKey
)
;
int
starvation
=
0
;
if
(
reservedContainers
>
0
)
{
float
nodeFactor
=
Resources
.
ratio
(
rc
,
required
,
application
.
getCSLeafQueue
(
)
.
getMaximumAllocation
(
)
)
;
starvation
=
(
int
)
(
(
application
.
getReReservations
(
schedulerKey
)
/
(
float
)
reservedContainers
)
*
(
1.0f
-
(
Math
.
min
(
nodeFactor
,
application
.
getCSLeafQueue
(
)
.
getMinimumAllocationFactor
(
)
)
)
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
CSAssignment
assignContainers
(
Resource
clusterResource
,
CandidateNodeSet
<
FiCaSchedulerNode
>
candidates
,
SchedulingMode
schedulingMode
,
ResourceLimits
resourceLimits
,
RMContainer
reservedContainer
)
{
FiCaSchedulerNode
node
=
CandidateNodeSetUtils
.
getSingleNode
(
candidates
)
;
if
(
reservedContainer
==
null
)
{
if
(
!
application
.
hasPendingResourceRequest
(
candidates
.
getPartition
(
)
,
schedulingMode
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
String
pathName
=
path
.
getName
(
)
;
return
pathName
.
startsWith
(
YarnConfiguration
.
CS_CONFIGURATION_FILE
)
&&
!
pathName
.
endsWith
(
TMP
)
;
}
}
;
Configuration
conf
=
new
Configuration
(
fsConf
)
;
String
schedulerConfPathStr
=
conf
.
get
(
YarnConfiguration
.
SCHEDULER_CONFIGURATION_FS_PATH
)
;
if
(
schedulerConfPathStr
==
null
||
schedulerConfPathStr
.
isEmpty
(
)
)
{
throw
new
IOException
(
YarnConfiguration
.
SCHEDULER_CONFIGURATION_FS_PATH
+
)
;
}
this
.
schedulerConfDir
=
new
Path
(
schedulerConfPathStr
)
;
String
scheme
=
schedulerConfDir
.
toUri
(
)
.
getScheme
(
)
;
if
(
scheme
==
null
)
{
scheme
=
FileSystem
.
getDefaultUri
(
conf
)
.
getScheme
(
)
;
}
if
(
scheme
!=
null
)
{
String
disableCacheName
=
String
.
format
(
,
scheme
)
;
conf
.
setBoolean
(
disableCacheName
,
true
)
;
}
this
.
fileSystem
=
this
.
schedulerConfDir
.
getFileSystem
(
conf
)
;
return
pathName
.
startsWith
(
YarnConfiguration
.
CS_CONFIGURATION_FILE
)
&&
!
pathName
.
endsWith
(
TMP
)
;
}
}
;
Configuration
conf
=
new
Configuration
(
fsConf
)
;
String
schedulerConfPathStr
=
conf
.
get
(
YarnConfiguration
.
SCHEDULER_CONFIGURATION_FS_PATH
)
;
if
(
schedulerConfPathStr
==
null
||
schedulerConfPathStr
.
isEmpty
(
)
)
{
throw
new
IOException
(
YarnConfiguration
.
SCHEDULER_CONFIGURATION_FS_PATH
+
)
;
}
this
.
schedulerConfDir
=
new
Path
(
schedulerConfPathStr
)
;
String
scheme
=
schedulerConfDir
.
toUri
(
)
.
getScheme
(
)
;
if
(
scheme
==
null
)
{
scheme
=
FileSystem
.
getDefaultUri
(
conf
)
.
getScheme
(
)
;
}
if
(
scheme
!=
null
)
{
String
disableCacheName
=
String
.
format
(
,
scheme
)
;
conf
.
setBoolean
(
disableCacheName
,
true
)
;
}
this
.
fileSystem
=
this
.
schedulerConfDir
.
getFileSystem
(
conf
)
;
this
.
maxVersion
=
conf
.
getInt
(
YarnConfiguration
.
SCHEDULER_CONFIGURATION_FS_MAX_VERSION
,
YarnConfiguration
.
DEFAULT_SCHEDULER_CONFIGURATION_FS_MAX_VERSION
)
;
private
void
finalizeFileSystemFile
(
)
throws
IOException
{
Path
finalConfigPath
=
getFinalConfigPath
(
tempConfigPath
)
;
fileSystem
.
rename
(
tempConfigPath
,
finalConfigPath
)
;
private
void
removeTmpConfigFile
(
)
throws
IOException
{
fileSystem
.
delete
(
tempConfigPath
,
true
)
;
@
VisibleForTesting
private
Path
writeTmpConfig
(
Configuration
vSchedConf
)
throws
IOException
{
long
start
=
Time
.
monotonicNow
(
)
;
String
tempSchedulerConfigFile
=
YarnConfiguration
.
CS_CONFIGURATION_FILE
+
+
System
.
currentTimeMillis
(
)
+
TMP
;
Path
tempSchedulerConfigPath
=
new
Path
(
this
.
schedulerConfDir
,
tempSchedulerConfigFile
)
;
try
(
FSDataOutputStream
outputStream
=
fileSystem
.
create
(
tempSchedulerConfigPath
)
)
{
cleanConfigurationFile
(
)
;
vSchedConf
.
writeXml
(
outputStream
)
;
public
void
checkVersion
(
)
throws
Exception
{
Version
loadedVersion
=
getConfStoreVersion
(
)
;
Version
currentVersion
=
getCurrentVersion
(
)
;
public
static
YarnConfigurationStore
getStore
(
Configuration
conf
)
{
String
store
=
conf
.
get
(
YarnConfiguration
.
SCHEDULER_CONFIGURATION_STORE_CLASS
,
YarnConfiguration
.
MEMORY_CONFIGURATION_STORE
)
;
switch
(
store
)
{
case
YarnConfiguration
.
MEMORY_CONFIGURATION_STORE
:
return
new
InMemoryConfigurationStore
(
)
;
case
YarnConfiguration
.
LEVELDB_CONFIGURATION_STORE
:
return
new
LeveldbConfigurationStore
(
)
;
case
YarnConfiguration
.
ZK_CONFIGURATION_STORE
:
return
new
ZKConfigurationStore
(
)
;
case
YarnConfiguration
.
FS_CONFIGURATION_STORE
:
return
new
FSSchedulerConfigurationStore
(
)
;
default
:
Class
<
?
extends
YarnConfigurationStore
>
storeClass
=
conf
.
getClass
(
YarnConfiguration
.
SCHEDULER_CONFIGURATION_STORE_CLASS
,
InMemoryConfigurationStore
.
class
,
YarnConfigurationStore
.
class
)
;
private
void
initializeLeafQueueTemplate
(
ManagedParentQueue
parentQueue
)
throws
IOException
{
leafQueueTemplate
=
parentQueue
.
getLeafQueueTemplate
(
)
;
leafQueueTemplateCapacities
=
leafQueueTemplate
.
getQueueCapacities
(
)
;
Set
<
String
>
parentQueueLabels
=
parentQueue
.
getNodeLabelsForQueue
(
)
;
for
(
String
nodeLabel
:
leafQueueTemplateCapacities
.
getExistingNodeLabels
(
)
)
{
if
(
!
parentQueueLabels
.
contains
(
nodeLabel
)
)
{
@
Override
public
List
<
QueueManagementChange
>
computeQueueManagementChanges
(
)
throws
SchedulerDynamicEditException
{
updateLeafQueueState
(
)
;
readLock
.
lock
(
)
;
try
{
List
<
QueueManagementChange
>
queueManagementChanges
=
new
ArrayList
<
>
(
)
;
List
<
FiCaSchedulerApp
>
pendingApps
=
getSortedPendingApplications
(
)
;
Map
<
String
,
QueueCapacities
>
leafQueueEntitlements
=
new
HashMap
<
>
(
)
;
for
(
String
nodeLabel
:
leafQueueTemplateNodeLabels
)
{
float
parentAbsoluteCapacity
=
managedParentQueue
.
getQueueCapacities
(
)
.
getAbsoluteCapacity
(
nodeLabel
)
;
float
leafQueueTemplateAbsoluteCapacity
=
leafQueueTemplateCapacities
.
getAbsoluteCapacity
(
nodeLabel
)
;
Map
<
String
,
QueueCapacities
>
deactivatedLeafQueues
=
deactivateLeafQueuesIfInActive
(
managedParentQueue
,
nodeLabel
,
leafQueueEntitlements
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
deactivatedLeafQueues
.
size
(
)
>
0
)
{
try
{
List
<
QueueManagementChange
>
queueManagementChanges
=
new
ArrayList
<
>
(
)
;
List
<
FiCaSchedulerApp
>
pendingApps
=
getSortedPendingApplications
(
)
;
Map
<
String
,
QueueCapacities
>
leafQueueEntitlements
=
new
HashMap
<
>
(
)
;
for
(
String
nodeLabel
:
leafQueueTemplateNodeLabels
)
{
float
parentAbsoluteCapacity
=
managedParentQueue
.
getQueueCapacities
(
)
.
getAbsoluteCapacity
(
nodeLabel
)
;
float
leafQueueTemplateAbsoluteCapacity
=
leafQueueTemplateCapacities
.
getAbsoluteCapacity
(
nodeLabel
)
;
Map
<
String
,
QueueCapacities
>
deactivatedLeafQueues
=
deactivateLeafQueuesIfInActive
(
managedParentQueue
,
nodeLabel
,
leafQueueEntitlements
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
deactivatedLeafQueues
.
size
(
)
>
0
)
{
LOG
.
debug
(
+
,
managedParentQueue
.
getQueuePath
(
)
,
nodeLabel
,
deactivatedLeafQueues
.
size
(
)
>
25
?
deactivatedLeafQueues
.
size
(
)
:
deactivatedLeafQueues
)
;
}
}
float
deactivatedCapacity
=
getTotalDeactivatedCapacity
(
deactivatedLeafQueues
,
nodeLabel
)
;
float
sumOfChildQueueActivatedCapacity
=
parentQueueState
.
getAbsoluteActivatedChildQueueCapacity
(
nodeLabel
)
;
float
availableCapacity
=
parentAbsoluteCapacity
-
sumOfChildQueueActivatedCapacity
+
deactivatedCapacity
+
EPSILON
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
float
parentAbsoluteCapacity
=
managedParentQueue
.
getQueueCapacities
(
)
.
getAbsoluteCapacity
(
nodeLabel
)
;
float
leafQueueTemplateAbsoluteCapacity
=
leafQueueTemplateCapacities
.
getAbsoluteCapacity
(
nodeLabel
)
;
Map
<
String
,
QueueCapacities
>
deactivatedLeafQueues
=
deactivateLeafQueuesIfInActive
(
managedParentQueue
,
nodeLabel
,
leafQueueEntitlements
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
deactivatedLeafQueues
.
size
(
)
>
0
)
{
LOG
.
debug
(
+
,
managedParentQueue
.
getQueuePath
(
)
,
nodeLabel
,
deactivatedLeafQueues
.
size
(
)
>
25
?
deactivatedLeafQueues
.
size
(
)
:
deactivatedLeafQueues
)
;
}
}
float
deactivatedCapacity
=
getTotalDeactivatedCapacity
(
deactivatedLeafQueues
,
nodeLabel
)
;
float
sumOfChildQueueActivatedCapacity
=
parentQueueState
.
getAbsoluteActivatedChildQueueCapacity
(
nodeLabel
)
;
float
availableCapacity
=
parentAbsoluteCapacity
-
sumOfChildQueueActivatedCapacity
+
deactivatedCapacity
+
EPSILON
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
managedParentQueue
.
getQueuePath
(
)
+
+
nodeLabel
+
+
parentAbsoluteCapacity
+
+
leafQueueTemplateAbsoluteCapacity
+
+
deactivatedCapacity
+
+
sumOfChildQueueActivatedCapacity
+
+
availableCapacity
)
;
}
if
(
availableCapacity
>=
leafQueueTemplateAbsoluteCapacity
)
{
if
(
pendingApps
.
size
(
)
>
0
)
{
int
maxLeafQueuesTobeActivated
=
getMaxLeavesToBeActivated
(
availableCapacity
,
leafQueueTemplateAbsoluteCapacity
,
pendingApps
.
size
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
writeLock
.
lock
(
)
;
try
{
Set
<
String
>
newPartitions
=
new
HashSet
<
>
(
)
;
Set
<
String
>
newQueues
=
new
HashSet
<
>
(
)
;
for
(
CSQueue
newQueue
:
managedParentQueue
.
getChildQueues
(
)
)
{
if
(
newQueue
instanceof
LeafQueue
)
{
for
(
String
nodeLabel
:
leafQueueTemplateNodeLabels
)
{
leafQueueState
.
createLeafQueueStateIfNotExists
(
(
LeafQueue
)
newQueue
,
nodeLabel
)
;
newPartitions
.
add
(
nodeLabel
)
;
}
newQueues
.
add
(
newQueue
.
getQueuePath
(
)
)
;
}
}
for
(
Iterator
<
Map
.
Entry
<
String
,
Map
<
String
,
LeafQueueStatePerPartition
>>>
itr
=
leafQueueState
.
getLeafQueueStateMap
(
)
.
entrySet
(
)
.
iterator
(
)
;
itr
.
hasNext
(
)
;
)
{
Map
.
Entry
<
String
,
Map
<
String
,
LeafQueueStatePerPartition
>>
e
=
itr
.
next
(
)
;
String
partition
=
e
.
getKey
(
)
;
if
(
!
newPartitions
.
contains
(
partition
)
)
{
itr
.
remove
(
)
;
leafQueueState
.
createLeafQueueStateIfNotExists
(
(
LeafQueue
)
newQueue
,
nodeLabel
)
;
newPartitions
.
add
(
nodeLabel
)
;
}
newQueues
.
add
(
newQueue
.
getQueuePath
(
)
)
;
}
}
for
(
Iterator
<
Map
.
Entry
<
String
,
Map
<
String
,
LeafQueueStatePerPartition
>>>
itr
=
leafQueueState
.
getLeafQueueStateMap
(
)
.
entrySet
(
)
.
iterator
(
)
;
itr
.
hasNext
(
)
;
)
{
Map
.
Entry
<
String
,
Map
<
String
,
LeafQueueStatePerPartition
>>
e
=
itr
.
next
(
)
;
String
partition
=
e
.
getKey
(
)
;
if
(
!
newPartitions
.
contains
(
partition
)
)
{
itr
.
remove
(
)
;
LOG
.
info
(
managedParentQueue
.
getQueuePath
(
)
+
+
partition
+
+
)
;
}
else
{
Map
<
String
,
LeafQueueStatePerPartition
>
queues
=
e
.
getValue
(
)
;
for
(
Iterator
<
Map
.
Entry
<
String
,
LeafQueueStatePerPartition
>>
queueItr
=
queues
.
entrySet
(
)
.
iterator
(
)
;
queueItr
.
hasNext
(
)
;
)
{
String
queue
=
queueItr
.
next
(
)
.
getKey
(
)
;
if
(
!
newQueues
.
contains
(
queue
)
)
{
queueItr
.
remove
(
)
;
try
{
for
(
QueueManagementChange
queueManagementChange
:
queueManagementChanges
)
{
AutoCreatedLeafQueueConfig
updatedQueueTemplate
=
queueManagementChange
.
getUpdatedQueueTemplate
(
)
;
CSQueue
queue
=
queueManagementChange
.
getQueue
(
)
;
if
(
!
(
queue
instanceof
AutoCreatedLeafQueue
)
)
{
throw
new
SchedulerDynamicEditException
(
+
+
queue
.
getClass
(
)
.
getName
(
)
)
;
}
AutoCreatedLeafQueue
leafQueue
=
(
AutoCreatedLeafQueue
)
queue
;
for
(
String
nodeLabel
:
updatedQueueTemplate
.
getQueueCapacities
(
)
.
getExistingNodeLabels
(
)
)
{
if
(
updatedQueueTemplate
.
getQueueCapacities
(
)
.
getCapacity
(
nodeLabel
)
>
0
)
{
if
(
isActive
(
leafQueue
,
nodeLabel
)
)
{
LOG
.
debug
(
,
leafQueue
.
getQueuePath
(
)
)
;
}
else
{
activate
(
leafQueue
,
nodeLabel
)
;
}
}
else
{
if
(
!
isActive
(
leafQueue
,
nodeLabel
)
)
{
private
boolean
anyContainerInFinalState
(
ResourceCommitRequest
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
request
)
{
for
(
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
c
:
request
.
getContainersToRelease
(
)
)
{
if
(
rmContainerInFinalState
(
c
.
getRmContainer
(
)
)
)
{
if
(
rmContainerInFinalState
(
c
.
getRmContainer
(
)
)
)
{
LOG
.
debug
(
,
c
.
getRmContainer
(
)
)
;
return
true
;
}
}
for
(
ContainerAllocationProposal
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
c
:
request
.
getContainersToAllocate
(
)
)
{
for
(
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
r
:
c
.
getToRelease
(
)
)
{
if
(
rmContainerInFinalState
(
r
.
getRmContainer
(
)
)
)
{
LOG
.
debug
(
+
,
r
.
getRmContainer
(
)
)
;
return
true
;
}
}
if
(
null
!=
c
.
getAllocateFromReservedContainer
(
)
)
{
if
(
rmContainerInFinalState
(
c
.
getAllocateFromReservedContainer
(
)
.
getRmContainer
(
)
)
)
{
LOG
.
debug
(
,
c
.
getAllocateFromReservedContainer
(
)
.
getRmContainer
(
)
)
;
return
true
;
}
}
}
for
(
ContainerAllocationProposal
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
c
:
request
.
getContainersToReserve
(
)
)
{
for
(
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
r
:
c
.
getToRelease
(
)
)
{
if
(
rmContainerInFinalState
(
r
.
getRmContainer
(
)
)
)
{
return
false
;
}
}
if
(
allocation
.
getAllocateFromReservedContainer
(
)
!=
null
&&
reservedContainerOnNode
==
null
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
allocation
.
getAllocateFromReservedContainer
(
)
.
getRmContainer
(
)
.
getContainerId
(
)
+
)
;
}
return
false
;
}
Resource
availableResource
=
Resources
.
clone
(
schedulerContainer
.
getSchedulerNode
(
)
.
getUnallocatedResource
(
)
)
;
if
(
allocation
.
getToRelease
(
)
!=
null
&&
!
allocation
.
getToRelease
(
)
.
isEmpty
(
)
)
{
for
(
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
releaseContainer
:
allocation
.
getToRelease
(
)
)
{
if
(
releaseContainer
.
getRmContainer
(
)
.
getState
(
)
==
RMContainerState
.
RESERVED
&&
releaseContainer
.
getRmContainer
(
)
!=
releaseContainer
.
getSchedulerNode
(
)
.
getReservedContainer
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
releaseContainer
.
getRmContainer
(
)
.
getContainerId
(
)
+
+
releaseContainer
.
getSchedulerNode
(
)
.
getNodeID
(
)
+
+
releaseContainer
.
getSchedulerNode
(
)
.
getReservedContainer
(
)
)
;
}
return
false
;
}
if
(
releaseContainer
.
getRmContainer
(
)
.
getState
(
)
!=
RMContainerState
.
RESERVED
&&
releaseContainer
.
getSchedulerNode
(
)
==
schedulerContainer
.
getSchedulerNode
(
)
)
{
Resources
.
addTo
(
availableResource
,
releaseContainer
.
getRmContainer
(
)
.
getAllocatedResource
(
)
)
;
}
}
}
if
(
!
Resources
.
fitsIn
(
rc
,
allocation
.
getAllocatedOrReservedResource
(
)
,
availableResource
)
)
{
readLock
.
lock
(
)
;
try
{
if
(
anyContainerInFinalState
(
request
)
)
{
return
false
;
}
if
(
request
.
anythingAllocatedOrReserved
(
)
)
{
ContainerAllocationProposal
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
allocation
=
request
.
getFirstAllocatedOrReservedContainer
(
)
;
SchedulerContainer
<
FiCaSchedulerApp
,
FiCaSchedulerNode
>
schedulerContainer
=
allocation
.
getAllocatedOrReservedContainer
(
)
;
if
(
schedulerContainer
.
getSchedulerNode
(
)
.
getRMNode
(
)
.
getState
(
)
!=
NodeState
.
RUNNING
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
schedulerContainer
.
getSchedulerNode
(
)
.
getNodeID
(
)
+
+
schedulerContainer
.
getSchedulerNode
(
)
.
getRMNode
(
)
.
getState
(
)
+
)
;
}
return
false
;
}
if
(
schedulerContainer
.
isAllocated
(
)
)
{
containerRequest
=
schedulerContainer
.
getRmContainer
(
)
.
getContainerRequest
(
)
;
if
(
checkPending
&&
!
appSchedulingInfo
.
checkAllocation
(
allocation
.
getAllocationLocalityType
(
)
,
schedulerContainer
.
getSchedulerNode
(
)
,
schedulerContainer
.
getSchedulerRequestKey
(
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
schedulerContainer
.
getSchedulerNode
(
)
.
getRMNode
(
)
.
getState
(
)
!=
NodeState
.
RUNNING
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
schedulerContainer
.
getSchedulerNode
(
)
.
getNodeID
(
)
+
+
schedulerContainer
.
getSchedulerNode
(
)
.
getRMNode
(
)
.
getState
(
)
+
)
;
}
return
false
;
}
if
(
schedulerContainer
.
isAllocated
(
)
)
{
containerRequest
=
schedulerContainer
.
getRmContainer
(
)
.
getContainerRequest
(
)
;
if
(
checkPending
&&
!
appSchedulingInfo
.
checkAllocation
(
allocation
.
getAllocationLocalityType
(
)
,
schedulerContainer
.
getSchedulerNode
(
)
,
schedulerContainer
.
getSchedulerRequestKey
(
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
allocation
.
getAllocationLocalityType
(
)
+
+
schedulerContainer
.
getSchedulerNode
(
)
+
+
schedulerContainer
.
getSchedulerRequestKey
(
)
+
+
getApplicationAttemptId
(
)
)
;
}
return
false
;
}
if
(
!
commonCheckContainerAllocation
(
allocation
,
schedulerContainer
)
)
{
return
false
;
}
}
else
{
if
(
schedulerContainer
.
getRmContainer
(
)
.
getState
(
)
==
RMContainerState
.
RESERVED
)
{
if
(
schedulerContainer
.
getRmContainer
(
)
!=
schedulerContainer
.
getSchedulerNode
(
)
.
getReservedContainer
(
)
)
{
if
(
allocation
.
getAllocateFromReservedContainer
(
)
!=
null
)
{
RMContainer
reservedContainer
=
allocation
.
getAllocateFromReservedContainer
(
)
.
getRmContainer
(
)
;
unreserve
(
schedulerContainer
.
getSchedulerRequestKey
(
)
,
schedulerContainer
.
getSchedulerNode
(
)
,
reservedContainer
)
;
}
addToNewlyAllocatedContainers
(
schedulerContainer
.
getSchedulerNode
(
)
,
rmContainer
)
;
liveContainers
.
put
(
containerId
,
rmContainer
)
;
if
(
updatePending
)
{
ContainerRequest
containerRequest
=
appSchedulingInfo
.
allocate
(
allocation
.
getAllocationLocalityType
(
)
,
schedulerContainer
.
getSchedulerNode
(
)
,
schedulerContainer
.
getSchedulerRequestKey
(
)
,
schedulerContainer
.
getRmContainer
(
)
)
;
(
(
RMContainerImpl
)
rmContainer
)
.
setContainerRequest
(
containerRequest
)
;
if
(
containerRequest
!=
null
&&
containerRequest
.
getSchedulingRequest
(
)
!=
null
)
{
(
(
RMContainerImpl
)
rmContainer
)
.
setAllocationTags
(
containerRequest
.
getSchedulingRequest
(
)
.
getAllocationTags
(
)
)
;
}
}
else
{
AppSchedulingInfo
.
updateMetrics
(
getApplicationId
(
)
,
allocation
.
getAllocationLocalityType
(
)
,
schedulerContainer
.
getSchedulerNode
(
)
,
schedulerContainer
.
getRmContainer
(
)
,
getUser
(
)
,
getQueue
(
)
)
;
}
attemptResourceUsage
.
incUsed
(
schedulerContainer
.
getNodePartition
(
)
,
allocation
.
getAllocatedOrReservedResource
(
)
)
;
rmContainer
.
handle
(
new
RMContainerEvent
(
containerId
,
RMContainerEventType
.
START
)
)
;
schedulerContainer
.
getSchedulerNode
(
)
.
allocateContainer
(
rmContainer
)
;
ContainerRequest
containerRequest
=
appSchedulingInfo
.
allocate
(
allocation
.
getAllocationLocalityType
(
)
,
schedulerContainer
.
getSchedulerNode
(
)
,
schedulerContainer
.
getSchedulerRequestKey
(
)
,
schedulerContainer
.
getRmContainer
(
)
)
;
(
(
RMContainerImpl
)
rmContainer
)
.
setContainerRequest
(
containerRequest
)
;
if
(
containerRequest
!=
null
&&
containerRequest
.
getSchedulingRequest
(
)
!=
null
)
{
(
(
RMContainerImpl
)
rmContainer
)
.
setAllocationTags
(
containerRequest
.
getSchedulingRequest
(
)
.
getAllocationTags
(
)
)
;
}
}
else
{
AppSchedulingInfo
.
updateMetrics
(
getApplicationId
(
)
,
allocation
.
getAllocationLocalityType
(
)
,
schedulerContainer
.
getSchedulerNode
(
)
,
schedulerContainer
.
getRmContainer
(
)
,
getUser
(
)
,
getQueue
(
)
)
;
}
attemptResourceUsage
.
incUsed
(
schedulerContainer
.
getNodePartition
(
)
,
allocation
.
getAllocatedOrReservedResource
(
)
)
;
rmContainer
.
handle
(
new
RMContainerEvent
(
containerId
,
RMContainerEventType
.
START
)
)
;
schedulerContainer
.
getSchedulerNode
(
)
.
allocateContainer
(
rmContainer
)
;
incNumAllocatedContainers
(
allocation
.
getAllocationLocalityType
(
)
,
allocation
.
getRequestLocalityType
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
containerId
.
getApplicationAttemptId
(
)
+
+
containerId
+
+
rmContainer
.
getAllocatedNode
(
)
.
getHost
(
)
+
+
allocation
.
getAllocationLocalityType
(
)
)
;
}
String
partition
=
schedulerContainer
.
getSchedulerNode
(
)
.
getPartition
(
)
;
if
(
partition
!=
null
&&
partition
.
isEmpty
(
)
)
{
partition
=
null
;
@
VisibleForTesting
public
NodeId
getNodeIdToUnreserve
(
SchedulerRequestKey
schedulerKey
,
Resource
resourceNeedUnreserve
,
ResourceCalculator
resourceCalculator
)
{
Map
<
NodeId
,
RMContainer
>
reservedContainers
=
this
.
reservedContainers
.
get
(
schedulerKey
)
;
if
(
(
reservedContainers
!=
null
)
&&
(
!
reservedContainers
.
isEmpty
(
)
)
)
{
for
(
Map
.
Entry
<
NodeId
,
RMContainer
>
entry
:
reservedContainers
.
entrySet
(
)
)
{
NodeId
nodeId
=
entry
.
getKey
(
)
;
RMContainer
reservedContainer
=
entry
.
getValue
(
)
;
Resource
reservedResource
=
reservedContainer
.
getReservedResource
(
)
;
if
(
Resources
.
fitsIn
(
resourceCalculator
,
resourceNeedUnreserve
,
reservedResource
)
)
{
LOG
.
debug
(
+
)
;
return
false
;
}
Map
<
NodeId
,
RMContainer
>
map
=
reservedContainers
.
get
(
reservedContainer
.
getReservedSchedulerKey
(
)
)
;
if
(
null
==
map
)
{
LOG
.
debug
(
)
;
return
false
;
}
if
(
sourceNode
.
getReservedContainer
(
)
!=
reservedContainer
)
{
LOG
.
debug
(
)
;
return
false
;
}
synchronized
(
targetNode
)
{
if
(
targetNode
.
getReservedContainer
(
)
!=
null
)
{
LOG
.
debug
(
)
;
}
try
{
targetNode
.
reserveResource
(
this
,
reservedContainer
.
getReservedSchedulerKey
(
)
,
reservedContainer
)
;
}
catch
(
IllegalStateException
e
)
{
protected
synchronized
void
allocateContainer
(
RMContainer
rmContainer
,
boolean
launchedOnNode
)
{
super
.
allocateContainer
(
rmContainer
,
launchedOnNode
)
;
final
Container
container
=
rmContainer
.
getContainer
(
)
;
Map
<
String
,
PlacementConstraint
>
constraintsForApp
=
new
HashMap
<
>
(
)
;
readLock
.
lock
(
)
;
try
{
if
(
appConstraints
.
get
(
appId
)
!=
null
)
{
LOG
.
warn
(
,
appId
)
;
return
;
}
for
(
Map
.
Entry
<
Set
<
String
>
,
PlacementConstraint
>
entry
:
constraintMap
.
entrySet
(
)
)
{
Set
<
String
>
sourceTags
=
entry
.
getKey
(
)
;
PlacementConstraint
constraint
=
entry
.
getValue
(
)
;
if
(
validateConstraint
(
sourceTags
,
constraint
)
)
{
String
sourceTag
=
getValidSourceTag
(
sourceTags
)
;
constraintsForApp
.
put
(
sourceTag
,
constraint
)
;
}
}
}
finally
{
readLock
.
unlock
(
)
;
}
if
(
constraintsForApp
.
isEmpty
(
)
)
{
@
Override
public
void
addConstraint
(
ApplicationId
appId
,
Set
<
String
>
sourceTags
,
PlacementConstraint
placementConstraint
,
boolean
replace
)
{
writeLock
.
lock
(
)
;
try
{
Map
<
String
,
PlacementConstraint
>
constraintsForApp
=
appConstraints
.
get
(
appId
)
;
if
(
constraintsForApp
==
null
)
{
private
void
addConstraintToMap
(
Map
<
String
,
PlacementConstraint
>
constraintMap
,
Set
<
String
>
sourceTags
,
PlacementConstraint
placementConstraint
,
boolean
replace
)
{
if
(
validateConstraint
(
sourceTags
,
placementConstraint
)
)
{
String
sourceTag
=
getValidSourceTag
(
sourceTags
)
;
if
(
constraintMap
.
get
(
sourceTag
)
==
null
||
replace
)
{
if
(
replace
)
{
@
Override
public
Map
<
Set
<
String
>
,
PlacementConstraint
>
getConstraints
(
ApplicationId
appId
)
{
readLock
.
lock
(
)
;
try
{
if
(
appConstraints
.
get
(
appId
)
==
null
)
{
private
static
boolean
getNodeConstraintEvaluatedResult
(
SchedulerNode
schedulerNode
,
NodeAttributeOpCode
opCode
,
NodeAttribute
requestAttribute
)
{
if
(
schedulerNode
.
getNodeAttributes
(
)
==
null
||
!
schedulerNode
.
getNodeAttributes
(
)
.
contains
(
requestAttribute
)
)
{
if
(
opCode
==
NodeAttributeOpCode
.
NE
)
{
LOG
.
debug
(
+
,
requestAttribute
,
schedulerNode
.
getNodeID
(
)
)
;
return
true
;
}
LOG
.
debug
(
+
,
requestAttribute
,
schedulerNode
.
getNodeID
(
)
)
;
return
false
;
}
boolean
found
=
false
;
for
(
Iterator
<
NodeAttribute
>
it
=
schedulerNode
.
getNodeAttributes
(
)
.
iterator
(
)
;
it
.
hasNext
(
)
;
)
{
NodeAttribute
nodeAttribute
=
it
.
next
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
requestAttribute
+
+
requestAttribute
.
getAttributeValue
(
)
+
+
nodeAttribute
.
getAttributeValue
(
)
)
;
}
if
(
requestAttribute
.
equals
(
nodeAttribute
)
)
{
if
(
isOpCodeMatches
(
requestAttribute
,
nodeAttribute
,
opCode
)
)
{
LOG
.
debug
(
,
requestAttribute
,
schedulerNode
.
getNodeID
(
)
)
;
found
=
true
;
return
found
;
}
}
}
if
(
!
found
)
{
private
static
boolean
canSatisfyConstraints
(
ApplicationId
appId
,
PlacementConstraint
constraint
,
SchedulerNode
node
,
AllocationTagsManager
atm
,
Optional
<
DiagnosticsCollector
>
dcOpt
)
throws
InvalidAllocationTagsQueryException
{
if
(
constraint
==
null
)
{
private
static
NodeAttribute
getNodeConstraintFromRequest
(
String
attrKey
,
String
attrString
)
{
NodeAttribute
nodeAttribute
=
null
;
private
void
processPlacementConstraints
(
ApplicationId
applicationId
,
Map
<
Set
<
String
>
,
PlacementConstraint
>
appPlacementConstraints
)
{
if
(
appPlacementConstraints
!=
null
&&
!
appPlacementConstraints
.
isEmpty
(
)
)
{
ConstraintPlacementAlgorithm
algorithm
=
null
;
if
(
instances
!=
null
&&
!
instances
.
isEmpty
(
)
)
{
algorithm
=
instances
.
get
(
0
)
;
}
else
{
algorithm
=
new
DefaultPlacementAlgorithm
(
)
;
}
LOG
.
info
(
,
algorithm
.
getClass
(
)
.
getName
(
)
)
;
String
iteratorName
=
(
(
RMContextImpl
)
amsContext
)
.
getYarnConfiguration
(
)
.
get
(
YarnConfiguration
.
RM_PLACEMENT_CONSTRAINTS_ALGORITHM_ITERATOR
,
BatchedRequests
.
IteratorType
.
SERIAL
.
name
(
)
)
;
LOG
.
info
(
,
iteratorName
)
;
try
{
iteratorType
=
BatchedRequests
.
IteratorType
.
valueOf
(
iteratorName
)
;
}
catch
(
IllegalArgumentException
e
)
{
throw
new
YarnRuntimeException
(
,
e
)
;
}
int
algoPSize
=
(
(
RMContextImpl
)
amsContext
)
.
getYarnConfiguration
(
)
.
getInt
(
YarnConfiguration
.
RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE
,
YarnConfiguration
.
DEFAULT_RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE
)
;
this
.
placementDispatcher
=
new
PlacementDispatcher
(
)
;
this
.
placementDispatcher
.
init
(
(
(
RMContextImpl
)
amsContext
)
,
algorithm
,
algoPSize
)
;
}
else
{
algorithm
=
new
DefaultPlacementAlgorithm
(
)
;
}
LOG
.
info
(
,
algorithm
.
getClass
(
)
.
getName
(
)
)
;
String
iteratorName
=
(
(
RMContextImpl
)
amsContext
)
.
getYarnConfiguration
(
)
.
get
(
YarnConfiguration
.
RM_PLACEMENT_CONSTRAINTS_ALGORITHM_ITERATOR
,
BatchedRequests
.
IteratorType
.
SERIAL
.
name
(
)
)
;
LOG
.
info
(
,
iteratorName
)
;
try
{
iteratorType
=
BatchedRequests
.
IteratorType
.
valueOf
(
iteratorName
)
;
}
catch
(
IllegalArgumentException
e
)
{
throw
new
YarnRuntimeException
(
,
e
)
;
}
int
algoPSize
=
(
(
RMContextImpl
)
amsContext
)
.
getYarnConfiguration
(
)
.
getInt
(
YarnConfiguration
.
RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE
,
YarnConfiguration
.
DEFAULT_RM_PLACEMENT_CONSTRAINTS_ALGORITHM_POOL_SIZE
)
;
this
.
placementDispatcher
=
new
PlacementDispatcher
(
)
;
this
.
placementDispatcher
.
init
(
(
(
RMContextImpl
)
amsContext
)
,
algorithm
,
algoPSize
)
;
LOG
.
info
(
,
algoPSize
)
;
int
schedPSize
=
(
(
RMContextImpl
)
amsContext
)
.
getYarnConfiguration
(
)
.
getInt
(
YarnConfiguration
.
RM_PLACEMENT_CONSTRAINTS_SCHEDULER_POOL_SIZE
,
YarnConfiguration
.
DEFAULT_RM_PLACEMENT_CONSTRAINTS_SCHEDULER_POOL_SIZE
)
;
this
.
schedulingThreadPool
=
Executors
.
newFixedThreadPool
(
schedPSize
)
;
void
dispatch
(
final
BatchedRequests
batchedRequests
)
{
final
ConstraintPlacementAlgorithmOutputCollector
collector
=
this
;
Runnable
placingTask
=
(
)
->
{
@
Override
public
void
collect
(
ConstraintPlacementAlgorithmOutput
placement
)
{
if
(
!
placement
.
getPlacedRequests
(
)
.
isEmpty
(
)
)
{
List
<
PlacedSchedulingRequest
>
processed
=
placedRequests
.
computeIfAbsent
(
placement
.
getApplicationId
(
)
,
k
->
new
ArrayList
<
>
(
)
)
;
synchronized
(
processed
)
{
@
Override
public
List
<
Container
>
allocateContainers
(
ResourceBlacklistRequest
blackList
,
List
<
ResourceRequest
>
oppResourceReqs
,
ApplicationAttemptId
applicationAttemptId
,
OpportunisticContainerContext
opportContext
,
long
rmIdentifier
,
String
appSubmitter
)
throws
YarnException
{
updateBlacklist
(
blackList
,
opportContext
)
;
opportContext
.
addToOutstandingReqs
(
oppResourceReqs
)
;
Set
<
String
>
nodeBlackList
=
new
HashSet
<
>
(
opportContext
.
getBlacklist
(
)
)
;
List
<
Container
>
allocatedContainers
=
new
ArrayList
<
>
(
)
;
int
maxAllocationsPerAMHeartbeat
=
getMaxAllocationsPerAMHeartbeat
(
)
;
List
<
Map
<
Resource
,
List
<
Allocation
>>>
allocations
=
new
ArrayList
<
>
(
)
;
for
(
SchedulerRequestKey
schedulerKey
:
opportContext
.
getOutstandingOpReqs
(
)
.
descendingKeySet
(
)
)
{
int
remAllocs
=
-
1
;
if
(
maxAllocationsPerAMHeartbeat
>
0
)
{
remAllocs
=
maxAllocationsPerAMHeartbeat
-
getTotalAllocations
(
allocations
)
;
if
(
remAllocs
<=
0
)
{
@
SuppressWarnings
(
)
private
List
<
Container
>
allocateNodeLocal
(
EnrichedResourceRequest
enrichedAsk
,
String
nodeLocation
,
int
toAllocate
,
long
rmIdentifier
,
AllocationParams
appParams
,
ContainerIdGenerator
idCounter
,
Set
<
String
>
blacklist
,
ApplicationAttemptId
id
,
String
userName
,
Map
<
Resource
,
List
<
Allocation
>>
allocations
)
throws
YarnException
{
List
<
Container
>
allocatedContainers
=
new
ArrayList
<
>
(
)
;
while
(
toAllocate
>
0
)
{
RMNode
node
=
nodeQueueLoadMonitor
.
selectLocalNode
(
nodeLocation
,
blacklist
)
;
if
(
node
!=
null
)
{
toAllocate
--
;
Container
container
=
createContainer
(
rmIdentifier
,
appParams
,
idCounter
,
id
,
userName
,
allocations
,
nodeLocation
,
enrichedAsk
.
getRequest
(
)
,
convertToRemoteNode
(
node
)
)
;
allocatedContainers
.
add
(
container
)
;
@
SuppressWarnings
(
)
private
List
<
Container
>
allocateRackLocal
(
EnrichedResourceRequest
enrichedAsk
,
String
rackLocation
,
int
toAllocate
,
long
rmIdentifier
,
AllocationParams
appParams
,
ContainerIdGenerator
idCounter
,
Set
<
String
>
blacklist
,
ApplicationAttemptId
id
,
String
userName
,
Map
<
Resource
,
List
<
Allocation
>>
allocations
)
throws
YarnException
{
List
<
Container
>
allocatedContainers
=
new
ArrayList
<
>
(
)
;
while
(
toAllocate
>
0
)
{
RMNode
node
=
nodeQueueLoadMonitor
.
selectRackLocalNode
(
rackLocation
,
blacklist
)
;
if
(
node
!=
null
)
{
toAllocate
--
;
Container
container
=
createContainer
(
rmIdentifier
,
appParams
,
idCounter
,
id
,
userName
,
allocations
,
rackLocation
,
enrichedAsk
.
getRequest
(
)
,
convertToRemoteNode
(
node
)
)
;
allocatedContainers
.
add
(
container
)
;
metrics
.
incrRackLocalOppContainers
(
)
;
@
SuppressWarnings
(
)
private
List
<
Container
>
allocateAny
(
EnrichedResourceRequest
enrichedAsk
,
int
toAllocate
,
long
rmIdentifier
,
AllocationParams
appParams
,
ContainerIdGenerator
idCounter
,
Set
<
String
>
blacklist
,
ApplicationAttemptId
id
,
String
userName
,
Map
<
Resource
,
List
<
Allocation
>>
allocations
)
throws
YarnException
{
List
<
Container
>
allocatedContainers
=
new
ArrayList
<
>
(
)
;
while
(
toAllocate
>
0
)
{
RMNode
node
=
nodeQueueLoadMonitor
.
selectAnyNode
(
blacklist
)
;
if
(
node
!=
null
)
{
toAllocate
--
;
Container
container
=
createContainer
(
rmIdentifier
,
appParams
,
idCounter
,
id
,
userName
,
allocations
,
ResourceRequest
.
ANY
,
enrichedAsk
.
getRequest
(
)
,
convertToRemoteNode
(
node
)
)
;
allocatedContainers
.
add
(
container
)
;
metrics
.
incrOffSwitchOppContainers
(
)
;
@
Override
public
void
updateNode
(
RMNode
rmNode
)
{
protected
void
onNewNodeAdded
(
RMNode
rmNode
,
OpportunisticContainersStatus
status
)
{
int
opportQueueCapacity
=
status
.
getOpportQueueCapacity
(
)
;
int
estimatedQueueWaitTime
=
status
.
getEstimatedQueueWaitTime
(
)
;
int
waitQueueLength
=
status
.
getWaitQueueLength
(
)
;
if
(
rmNode
.
getState
(
)
!=
NodeState
.
DECOMMISSIONING
&&
(
estimatedQueueWaitTime
!=
-
1
||
comparator
==
LoadComparator
.
QUEUE_LENGTH
)
)
{
this
.
clusterNodes
.
put
(
rmNode
.
getNodeID
(
)
,
new
ClusterNode
(
rmNode
.
getNodeID
(
)
)
.
setQueueWaitTime
(
estimatedQueueWaitTime
)
.
setQueueLength
(
waitQueueLength
)
.
setNodeLabels
(
rmNode
.
getNodeLabels
(
)
)
.
setQueueCapacity
(
opportQueueCapacity
)
)
;
protected
void
onExistingNodeUpdated
(
RMNode
rmNode
,
ClusterNode
clusterNode
,
OpportunisticContainersStatus
status
)
{
int
estimatedQueueWaitTime
=
status
.
getEstimatedQueueWaitTime
(
)
;
int
waitQueueLength
=
status
.
getWaitQueueLength
(
)
;
if
(
rmNode
.
getState
(
)
!=
NodeState
.
DECOMMISSIONING
&&
(
estimatedQueueWaitTime
!=
-
1
||
comparator
==
LoadComparator
.
QUEUE_LENGTH
)
)
{
clusterNode
.
setQueueWaitTime
(
estimatedQueueWaitTime
)
.
setQueueLength
(
waitQueueLength
)
.
setNodeLabels
(
rmNode
.
getNodeLabels
(
)
)
.
updateTimestamp
(
)
;
@
Override
public
void
updateNodeResource
(
RMNode
rmNode
,
ResourceOption
resourceOption
)
{
this
.
allocFile
=
getAllocationFile
(
conf
)
;
if
(
this
.
allocFile
!=
null
)
{
this
.
fs
=
allocFile
.
getFileSystem
(
conf
)
;
reloadThread
=
new
Thread
(
(
)
->
{
while
(
running
)
{
try
{
synchronized
(
this
)
{
reloadListener
.
onCheck
(
)
;
}
long
time
=
clock
.
getTime
(
)
;
long
lastModified
=
fs
.
getFileStatus
(
allocFile
)
.
getModificationTime
(
)
;
if
(
lastModified
>
lastSuccessfulReload
&&
time
>
lastModified
+
ALLOC_RELOAD_WAIT_MS
)
{
try
{
reloadAllocations
(
)
;
}
catch
(
Exception
ex
)
{
if
(
!
lastReloadAttemptFailed
)
{
reloadListener
.
onCheck
(
)
;
}
long
time
=
clock
.
getTime
(
)
;
long
lastModified
=
fs
.
getFileStatus
(
allocFile
)
.
getModificationTime
(
)
;
if
(
lastModified
>
lastSuccessfulReload
&&
time
>
lastModified
+
ALLOC_RELOAD_WAIT_MS
)
{
try
{
reloadAllocations
(
)
;
}
catch
(
Exception
ex
)
{
if
(
!
lastReloadAttemptFailed
)
{
LOG
.
error
(
+
,
ex
)
;
}
lastReloadAttemptFailed
=
true
;
}
}
else
if
(
lastModified
==
0l
)
{
if
(
!
lastReloadAttemptFailed
)
{
LOG
.
warn
(
+
+
fs
.
exists
(
allocFile
)
)
;
}
lastReloadAttemptFailed
=
true
;
}
}
catch
(
IOException
e
)
{
void
containerCompleted
(
RMContainer
rmContainer
,
ContainerStatus
containerStatus
,
RMContainerEventType
event
)
{
writeLock
.
lock
(
)
;
try
{
Container
container
=
rmContainer
.
getContainer
(
)
;
ContainerId
containerId
=
container
.
getId
(
)
;
if
(
liveContainers
.
remove
(
containerId
)
==
null
)
{
@
Override
public
Resource
getHeadroom
(
)
{
final
FSQueue
fsQueue
=
getQueue
(
)
;
SchedulingPolicy
policy
=
fsQueue
.
getPolicy
(
)
;
Resource
queueFairShare
=
fsQueue
.
getFairShare
(
)
;
Resource
queueUsage
=
fsQueue
.
getResourceUsage
(
)
;
Resource
clusterResource
=
this
.
scheduler
.
getClusterResource
(
)
;
Resource
clusterUsage
=
this
.
scheduler
.
getRootQueueMetrics
(
)
.
getAllocatedResources
(
)
;
Resource
clusterAvailableResources
=
Resources
.
subtract
(
clusterResource
,
clusterUsage
)
;
subtractResourcesOnBlacklistedNodes
(
clusterAvailableResources
)
;
Resource
queueMaxAvailableResources
=
Resources
.
subtract
(
fsQueue
.
getMaxShare
(
)
,
queueUsage
)
;
Resource
maxAvailableResource
=
Resources
.
componentwiseMin
(
clusterAvailableResources
,
queueMaxAvailableResources
)
;
Resource
headroom
=
policy
.
getHeadroom
(
queueFairShare
,
queueUsage
,
maxAvailableResource
)
;
this
.
resetAllowedLocalityLevel
(
schedulerKey
,
type
)
;
}
}
if
(
getOutstandingAsksCount
(
schedulerKey
)
<=
0
)
{
return
null
;
}
container
=
reservedContainer
;
if
(
container
==
null
)
{
container
=
createContainer
(
node
,
pendingAsk
.
getPerAllocationResource
(
)
,
schedulerKey
)
;
}
rmContainer
=
new
RMContainerImpl
(
container
,
schedulerKey
,
getApplicationAttemptId
(
)
,
node
.
getNodeID
(
)
,
appSchedulingInfo
.
getUser
(
)
,
rmContext
)
;
(
(
RMContainerImpl
)
rmContainer
)
.
setQueueName
(
this
.
getQueueName
(
)
)
;
addToNewlyAllocatedContainers
(
node
,
rmContainer
)
;
liveContainers
.
put
(
container
.
getId
(
)
,
rmContainer
)
;
ContainerRequest
containerRequest
=
appSchedulingInfo
.
allocate
(
type
,
node
,
schedulerKey
,
rmContainer
)
;
this
.
attemptResourceUsage
.
incUsed
(
container
.
getResource
(
)
)
;
getQueue
(
)
.
incUsedResource
(
container
.
getResource
(
)
)
;
(
(
RMContainerImpl
)
rmContainer
)
.
setContainerRequest
(
containerRequest
)
;
rmContainer
.
handle
(
new
RMContainerEvent
(
container
.
getId
(
)
,
RMContainerEventType
.
START
)
)
;
private
boolean
reserve
(
Resource
perAllocationResource
,
FSSchedulerNode
node
,
Container
reservedContainer
,
NodeType
type
,
SchedulerRequestKey
schedulerKey
)
{
RMContainer
nodeReservedContainer
=
node
.
getReservedContainer
(
)
;
boolean
reservableForThisApp
=
nodeReservedContainer
==
null
||
nodeReservedContainer
.
getApplicationAttemptId
(
)
.
equals
(
getApplicationAttemptId
(
)
)
;
if
(
reservableForThisApp
&&
!
reservationExceedsThreshold
(
node
,
type
)
)
{
private
boolean
reservationExceedsThreshold
(
FSSchedulerNode
node
,
NodeType
type
)
{
if
(
type
!=
NodeType
.
NODE_LOCAL
)
{
int
existingReservations
=
getNumReservations
(
node
.
getRackName
(
)
,
type
==
NodeType
.
OFF_SWITCH
)
;
int
totalAvailNodes
=
(
type
==
NodeType
.
OFF_SWITCH
)
?
scheduler
.
getNumClusterNodes
(
)
:
scheduler
.
getNumNodesInRack
(
node
.
getRackName
(
)
)
;
int
numAllowedReservations
=
(
int
)
Math
.
ceil
(
totalAvailNodes
*
scheduler
.
getReservableNodesRatio
(
)
)
;
if
(
existingReservations
>=
numAllowedReservations
)
{
DecimalFormat
df
=
new
DecimalFormat
(
)
;
df
.
setMaximumFractionDigits
(
2
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
Resources
.
fitsIn
(
capability
,
available
)
)
{
RMContainer
allocatedContainer
=
allocate
(
type
,
node
,
schedulerKey
,
pendingAsk
,
reservedContainer
)
;
if
(
allocatedContainer
==
null
)
{
if
(
reserved
)
{
unreserve
(
schedulerKey
,
node
)
;
}
LOG
.
debug
(
+
,
capability
,
available
)
;
return
Resources
.
none
(
)
;
}
if
(
reserved
)
{
unreserve
(
schedulerKey
,
node
)
;
}
node
.
allocateContainer
(
allocatedContainer
)
;
if
(
!
isAmRunning
(
)
&&
!
getUnmanagedAM
(
)
)
{
setAMResource
(
capability
)
;
getQueue
(
)
.
addAMResourceUsage
(
capability
)
;
setAmRunning
(
true
)
;
}
return
capability
;
return
Resources
.
none
(
)
;
}
if
(
reserved
)
{
unreserve
(
schedulerKey
,
node
)
;
}
node
.
allocateContainer
(
allocatedContainer
)
;
if
(
!
isAmRunning
(
)
&&
!
getUnmanagedAM
(
)
)
{
setAMResource
(
capability
)
;
getQueue
(
)
.
addAMResourceUsage
(
capability
)
;
setAmRunning
(
true
)
;
}
return
capability
;
}
LOG
.
debug
(
+
,
capability
)
;
if
(
isReservable
(
capability
)
&&
!
node
.
isPreemptedForApp
(
this
)
&&
reserve
(
pendingAsk
.
getPerAllocationResource
(
)
,
node
,
reservedContainer
,
type
,
schedulerKey
)
)
{
updateAMDiagnosticMsg
(
capability
,
+
)
;
LOG
.
debug
(
,
getName
(
)
)
;
return
FairScheduler
.
CONTAINER_RESERVED
;
}
else
{
boolean
assignReservedContainer
(
FSSchedulerNode
node
)
{
RMContainer
rmContainer
=
node
.
getReservedContainer
(
)
;
SchedulerRequestKey
reservedSchedulerKey
=
rmContainer
.
getReservedSchedulerKey
(
)
;
if
(
!
isValidReservation
(
node
)
)
{
@
Override
public
Resource
assignContainer
(
FSSchedulerNode
node
)
{
if
(
isOverAMShareLimit
(
)
)
{
PendingAsk
amAsk
=
appSchedulingInfo
.
getNextPendingAsk
(
)
;
updateAMDiagnosticMsg
(
amAsk
.
getPerAllocationResource
(
)
,
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
Resource
assignContainer
(
FSSchedulerNode
node
)
{
Resource
assigned
=
none
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
void
updateDemand
(
)
{
writeLock
.
lock
(
)
;
try
{
demand
=
Resources
.
createResource
(
0
)
;
for
(
FSQueue
childQueue
:
childQueues
)
{
childQueue
.
updateDemand
(
)
;
Resource
toAdd
=
childQueue
.
getDemand
(
)
;
demand
=
Resources
.
add
(
demand
,
toAdd
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
Resource
assignContainer
(
FSSchedulerNode
node
)
{
Resource
assigned
=
Resources
.
none
(
)
;
if
(
!
assignContainerPreCheck
(
node
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
List
<
RMContainer
>
identifyContainersToPreempt
(
FSAppAttempt
starvedApp
)
{
List
<
RMContainer
>
containersToPreempt
=
new
ArrayList
<
>
(
)
;
for
(
ResourceRequest
rr
:
starvedApp
.
getStarvedResourceRequests
(
)
)
{
List
<
FSSchedulerNode
>
potentialNodes
=
scheduler
.
getNodeTracker
(
)
.
getNodesByResourceName
(
rr
.
getResourceName
(
)
)
;
for
(
int
i
=
0
;
i
<
rr
.
getNumContainers
(
)
;
i
++
)
{
PreemptableContainers
bestContainers
=
getBestPreemptableContainers
(
rr
,
potentialNodes
)
;
if
(
bestContainers
!=
null
)
{
List
<
RMContainer
>
containers
=
bestContainers
.
getAllContainers
(
)
;
if
(
containers
.
size
(
)
>
0
)
{
containersToPreempt
.
addAll
(
containers
)
;
trackPreemptionsAgainstNode
(
containers
,
starvedApp
)
;
for
(
RMContainer
container
:
containers
)
{
FSAppAttempt
app
=
scheduler
.
getSchedulerApp
(
container
.
getApplicationAttemptId
(
)
)
;
private
PreemptableContainers
identifyContainersToPreemptOnNode
(
Resource
request
,
FSSchedulerNode
node
,
int
maxAMContainers
)
{
PreemptableContainers
preemptableContainers
=
new
PreemptableContainers
(
maxAMContainers
)
;
List
<
RMContainer
>
containersToCheck
=
node
.
getRunningContainersWithAMsAtTheEnd
(
)
;
containersToCheck
.
removeAll
(
node
.
getContainersForPreemption
(
)
)
;
Resource
potential
=
Resources
.
subtractFromNonNegative
(
Resources
.
clone
(
node
.
getUnallocatedResource
(
)
)
,
node
.
getTotalReserved
(
)
)
;
for
(
RMContainer
container
:
containersToCheck
)
{
FSAppAttempt
app
=
scheduler
.
getSchedulerApp
(
container
.
getApplicationAttemptId
(
)
)
;
if
(
app
==
null
)
{
@
Override
public
void
setFairShare
(
Resource
fairShare
)
{
this
.
fairShare
=
fairShare
;
metrics
.
setFairShare
(
fairShare
)
;
boolean
assignContainerPreCheck
(
FSSchedulerNode
node
)
{
if
(
node
.
getReservedContainer
(
)
!=
null
)
{
boolean
fitsInMaxShare
(
Resource
additionalResource
)
{
Resource
usagePlusAddition
=
Resources
.
add
(
getResourceUsage
(
)
,
additionalResource
)
;
if
(
!
Resources
.
fitsIn
(
usagePlusAddition
,
getMaxShare
(
)
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
protected
synchronized
void
allocateContainer
(
RMContainer
rmContainer
,
boolean
launchedOnNode
)
{
super
.
allocateContainer
(
rmContainer
,
launchedOnNode
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
final
Container
container
=
rmContainer
.
getContainer
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
final
Container
container
=
rmContainer
.
getContainer
(
)
;
LOG
.
debug
(
+
container
.
getId
(
)
+
+
container
.
getResource
(
)
+
+
getRMNode
(
)
.
getNodeAddress
(
)
+
+
getNumContainers
(
)
+
+
getAllocatedResource
(
)
+
+
getUnallocatedResource
(
)
+
)
;
}
Resource
allocated
=
rmContainer
.
getAllocatedResource
(
)
;
if
(
!
Resources
.
isNone
(
allocated
)
)
{
FSAppAttempt
app
=
appIdToAppMap
.
get
(
rmContainer
.
getApplicationAttemptId
(
)
)
;
if
(
app
!=
null
)
{
Resource
reserved
=
resourcesPreemptedForApp
.
get
(
app
)
;
Resource
fulfilled
=
Resources
.
componentwiseMin
(
reserved
,
allocated
)
;
Resources
.
subtractFrom
(
reserved
,
fulfilled
)
;
Resources
.
subtractFrom
(
totalResourcesPreempted
,
fulfilled
)
;
if
(
Resources
.
isNone
(
reserved
)
)
{
resourcesPreemptedForApp
.
remove
(
app
)
;
appIdToAppMap
.
remove
(
rmContainer
.
getApplicationAttemptId
(
)
)
;
}
}
}
else
{
private
void
dumpSchedulerState
(
)
{
FSQueue
rootQueue
=
queueMgr
.
getRootQueue
(
)
;
Resource
clusterResource
=
getClusterResource
(
)
;
private
void
dumpSchedulerState
(
)
{
FSQueue
rootQueue
=
queueMgr
.
getRootQueue
(
)
;
Resource
clusterResource
=
getClusterResource
(
)
;
STATE_DUMP_LOG
.
debug
(
+
clusterResource
+
+
rootMetrics
.
getAllocatedResources
(
)
+
+
Resource
.
newInstance
(
rootMetrics
.
getAvailableMB
(
)
,
rootMetrics
.
getAvailableVirtualCores
(
)
)
+
+
rootQueue
.
getDemand
(
)
)
;
if
(
!
isAppRecovering
)
{
rejectApplicationWithMessage
(
applicationId
,
queueName
+
)
;
return
;
}
queueName
=
;
queue
=
queueMgr
.
getLeafQueue
(
queueName
,
true
,
applicationId
)
;
}
if
(
!
isAppRecovering
)
{
UserGroupInformation
userUgi
=
UserGroupInformation
.
createRemoteUser
(
user
)
;
if
(
!
queue
.
hasAccess
(
QueueACL
.
SUBMIT_APPLICATIONS
,
userUgi
)
&&
!
queue
.
hasAccess
(
QueueACL
.
ADMINISTER_QUEUE
,
userUgi
)
)
{
String
msg
=
+
user
+
+
applicationId
+
+
queueName
;
rejectApplicationWithMessage
(
applicationId
,
msg
)
;
queue
.
removeAssignedApp
(
applicationId
)
;
return
;
}
}
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
applicationId
)
;
if
(
rmApp
!=
null
)
{
rmApp
.
setQueue
(
queueName
)
;
rejectApplicationWithMessage
(
applicationId
,
msg
)
;
queue
.
removeAssignedApp
(
applicationId
)
;
return
;
}
}
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
applicationId
)
;
if
(
rmApp
!=
null
)
{
rmApp
.
setQueue
(
queueName
)
;
}
else
{
LOG
.
error
(
+
applicationId
+
)
;
}
if
(
!
isAppRecovering
&&
rmApp
!=
null
&&
rmApp
.
getAMResourceRequests
(
)
!=
null
)
{
List
<
MaxResourceValidationResult
>
invalidAMResourceRequests
=
validateResourceRequests
(
rmApp
.
getAMResourceRequests
(
)
,
queue
)
;
if
(
!
invalidAMResourceRequests
.
isEmpty
(
)
)
{
String
msg
=
String
.
format
(
+
+
+
,
applicationId
,
queueName
,
invalidAMResourceRequests
,
queue
.
getMaxShare
(
)
)
;
rejectApplicationWithMessage
(
applicationId
,
msg
)
;
queue
.
removeAssignedApp
(
applicationId
)
;
return
;
return
;
}
}
RMApp
rmApp
=
rmContext
.
getRMApps
(
)
.
get
(
applicationId
)
;
if
(
rmApp
!=
null
)
{
rmApp
.
setQueue
(
queueName
)
;
}
else
{
LOG
.
error
(
+
applicationId
+
)
;
}
if
(
!
isAppRecovering
&&
rmApp
!=
null
&&
rmApp
.
getAMResourceRequests
(
)
!=
null
)
{
List
<
MaxResourceValidationResult
>
invalidAMResourceRequests
=
validateResourceRequests
(
rmApp
.
getAMResourceRequests
(
)
,
queue
)
;
if
(
!
invalidAMResourceRequests
.
isEmpty
(
)
)
{
String
msg
=
String
.
format
(
+
+
+
,
applicationId
,
queueName
,
invalidAMResourceRequests
,
queue
.
getMaxShare
(
)
)
;
rejectApplicationWithMessage
(
applicationId
,
msg
)
;
queue
.
removeAssignedApp
(
applicationId
)
;
return
;
}
}
SchedulerApplication
<
FSAppAttempt
>
application
=
new
SchedulerApplication
<
>
(
queue
,
user
)
;
applications
.
put
(
applicationId
,
application
)
;
try
{
SchedulerApplication
<
FSAppAttempt
>
application
=
applications
.
get
(
applicationAttemptId
.
getApplicationId
(
)
)
;
String
user
=
application
.
getUser
(
)
;
FSLeafQueue
queue
=
(
FSLeafQueue
)
application
.
getQueue
(
)
;
FSAppAttempt
attempt
=
new
FSAppAttempt
(
this
,
applicationAttemptId
,
user
,
queue
,
new
ActiveUsersManager
(
getRootQueueMetrics
(
)
)
,
rmContext
)
;
if
(
transferStateFromPreviousAttempt
)
{
attempt
.
transferStateFromPreviousAttempt
(
application
.
getCurrentAppAttempt
(
)
)
;
}
application
.
setCurrentAppAttempt
(
attempt
)
;
boolean
runnable
=
maxRunningEnforcer
.
canAppBeRunnable
(
queue
,
attempt
)
;
queue
.
addApp
(
attempt
,
runnable
)
;
if
(
runnable
)
{
maxRunningEnforcer
.
trackRunnableApp
(
attempt
)
;
}
else
{
maxRunningEnforcer
.
trackNonRunnableApp
(
attempt
)
;
}
queue
.
getMetrics
(
)
.
submitAppAttempt
(
user
)
;
String
user
=
application
.
getUser
(
)
;
FSLeafQueue
queue
=
(
FSLeafQueue
)
application
.
getQueue
(
)
;
FSAppAttempt
attempt
=
new
FSAppAttempt
(
this
,
applicationAttemptId
,
user
,
queue
,
new
ActiveUsersManager
(
getRootQueueMetrics
(
)
)
,
rmContext
)
;
if
(
transferStateFromPreviousAttempt
)
{
attempt
.
transferStateFromPreviousAttempt
(
application
.
getCurrentAppAttempt
(
)
)
;
}
application
.
setCurrentAppAttempt
(
attempt
)
;
boolean
runnable
=
maxRunningEnforcer
.
canAppBeRunnable
(
queue
,
attempt
)
;
queue
.
addApp
(
attempt
,
runnable
)
;
if
(
runnable
)
{
maxRunningEnforcer
.
trackRunnableApp
(
attempt
)
;
}
else
{
maxRunningEnforcer
.
trackNonRunnableApp
(
attempt
)
;
}
queue
.
getMetrics
(
)
.
submitAppAttempt
(
user
)
;
LOG
.
info
(
+
applicationAttemptId
+
+
user
)
;
if
(
isAttemptRecovering
)
{
private
void
removeApplicationAttempt
(
ApplicationAttemptId
applicationAttemptId
,
RMAppAttemptState
rmAppAttemptFinalState
,
boolean
keepContainers
)
{
writeLock
.
lock
(
)
;
try
{
private
void
removeApplicationAttempt
(
ApplicationAttemptId
applicationAttemptId
,
RMAppAttemptState
rmAppAttemptFinalState
,
boolean
keepContainers
)
{
writeLock
.
lock
(
)
;
try
{
LOG
.
info
(
+
applicationAttemptId
+
+
rmAppAttemptFinalState
)
;
FSAppAttempt
attempt
=
getApplicationAttempt
(
applicationAttemptId
)
;
if
(
attempt
==
null
)
{
@
Override
protected
void
completedContainerInternal
(
RMContainer
rmContainer
,
ContainerStatus
containerStatus
,
RMContainerEventType
event
)
{
writeLock
.
lock
(
)
;
try
{
Container
container
=
rmContainer
.
getContainer
(
)
;
FSAppAttempt
application
=
getCurrentAttemptForContainer
(
container
.
getId
(
)
)
;
ApplicationId
appId
=
container
.
getId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
if
(
application
==
null
)
{
@
Override
protected
void
completedContainerInternal
(
RMContainer
rmContainer
,
ContainerStatus
containerStatus
,
RMContainerEventType
event
)
{
writeLock
.
lock
(
)
;
try
{
Container
container
=
rmContainer
.
getContainer
(
)
;
FSAppAttempt
application
=
getCurrentAttemptForContainer
(
container
.
getId
(
)
)
;
ApplicationId
appId
=
container
.
getId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
if
(
application
==
null
)
{
LOG
.
info
(
+
container
+
+
appId
+
+
event
)
;
return
;
}
NodeId
nodeID
=
container
.
getNodeId
(
)
;
FSSchedulerNode
node
=
getFSSchedulerNode
(
nodeID
)
;
if
(
rmContainer
.
getState
(
)
==
RMContainerState
.
RESERVED
)
{
if
(
node
!=
null
)
{
application
.
unreserve
(
rmContainer
.
getReservedSchedulerKey
(
)
,
node
)
;
}
else
{
if
(
application
==
null
)
{
LOG
.
info
(
+
container
+
+
appId
+
+
event
)
;
return
;
}
NodeId
nodeID
=
container
.
getNodeId
(
)
;
FSSchedulerNode
node
=
getFSSchedulerNode
(
nodeID
)
;
if
(
rmContainer
.
getState
(
)
==
RMContainerState
.
RESERVED
)
{
if
(
node
!=
null
)
{
application
.
unreserve
(
rmContainer
.
getReservedSchedulerKey
(
)
,
node
)
;
}
else
{
LOG
.
debug
(
,
nodeID
)
;
}
}
else
{
application
.
containerCompleted
(
rmContainer
,
containerStatus
,
event
)
;
if
(
node
!=
null
)
{
node
.
releaseContainer
(
rmContainer
.
getContainerId
(
)
,
false
)
;
}
else
{
}
NodeId
nodeID
=
container
.
getNodeId
(
)
;
FSSchedulerNode
node
=
getFSSchedulerNode
(
nodeID
)
;
if
(
rmContainer
.
getState
(
)
==
RMContainerState
.
RESERVED
)
{
if
(
node
!=
null
)
{
application
.
unreserve
(
rmContainer
.
getReservedSchedulerKey
(
)
,
node
)
;
}
else
{
LOG
.
debug
(
,
nodeID
)
;
}
}
else
{
application
.
containerCompleted
(
rmContainer
,
containerStatus
,
event
)
;
if
(
node
!=
null
)
{
node
.
releaseContainer
(
rmContainer
.
getContainerId
(
)
,
false
)
;
}
else
{
LOG
.
debug
(
,
nodeID
)
;
}
updateRootQueueMetrics
(
)
;
}
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
addNode
(
List
<
NMContainerStatus
>
containerReports
,
RMNode
node
)
{
writeLock
.
lock
(
)
;
try
{
FSSchedulerNode
schedulerNode
=
new
FSSchedulerNode
(
node
,
usePortForNodeName
)
;
nodeTracker
.
addNode
(
schedulerNode
)
;
triggerUpdate
(
)
;
Resource
clusterResource
=
getClusterResource
(
)
;
queueMgr
.
getRootQueue
(
)
.
setSteadyFairShare
(
clusterResource
)
;
queueMgr
.
getRootQueue
(
)
.
recomputeSteadyShares
(
)
;
private
void
removeNode
(
RMNode
rmNode
)
{
writeLock
.
lock
(
)
;
try
{
NodeId
nodeId
=
rmNode
.
getNodeID
(
)
;
FSSchedulerNode
node
=
nodeTracker
.
getNode
(
nodeId
)
;
if
(
node
==
null
)
{
LOG
.
error
(
+
nodeId
)
;
return
;
}
List
<
RMContainer
>
runningContainers
=
node
.
getCopiedListOfRunningContainers
(
)
;
for
(
RMContainer
container
:
runningContainers
)
{
super
.
completedContainer
(
container
,
SchedulerUtils
.
createAbnormalContainerStatus
(
container
.
getContainerId
(
)
,
SchedulerUtils
.
LOST_CONTAINER
)
,
RMContainerEventType
.
KILL
)
;
node
.
releaseContainer
(
container
.
getContainerId
(
)
,
true
)
;
}
RMContainer
reservedContainer
=
node
.
getReservedContainer
(
)
;
if
(
reservedContainer
!=
null
)
{
super
.
completedContainer
(
reservedContainer
,
SchedulerUtils
.
createAbnormalContainerStatus
(
reservedContainer
.
getContainerId
(
)
,
SchedulerUtils
.
LOST_CONTAINER
)
,
RMContainerEventType
.
KILL
)
;
}
nodeTracker
.
removeNode
(
nodeId
)
;
Resource
clusterResource
=
getClusterResource
(
)
;
queueMgr
.
getRootQueue
(
)
.
setSteadyFairShare
(
clusterResource
)
;
queueMgr
.
getRootQueue
(
)
.
recomputeSteadyShares
(
)
;
updateRootQueueMetrics
(
)
;
triggerUpdate
(
)
;
@
VisibleForTesting
@
Override
public
void
killContainer
(
RMContainer
container
)
{
ContainerStatus
status
=
SchedulerUtils
.
createKilledContainerStatus
(
container
.
getContainerId
(
)
,
)
;
@
Override
public
Allocation
allocate
(
ApplicationAttemptId
appAttemptId
,
List
<
ResourceRequest
>
ask
,
List
<
SchedulingRequest
>
schedulingRequests
,
List
<
ContainerId
>
release
,
List
<
String
>
blacklistAdditions
,
List
<
String
>
blacklistRemovals
,
ContainerUpdates
updateRequests
)
{
FSAppAttempt
application
=
getSchedulerApp
(
appAttemptId
)
;
if
(
application
==
null
)
{
LOG
.
error
(
+
+
appAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
ApplicationId
applicationId
=
application
.
getApplicationId
(
)
;
FSLeafQueue
queue
=
application
.
getQueue
(
)
;
List
<
MaxResourceValidationResult
>
invalidAsks
=
validateResourceRequests
(
ask
,
queue
)
;
if
(
!
invalidAsks
.
isEmpty
(
)
)
{
throw
new
SchedulerInvalidResoureRequestException
(
String
.
format
(
+
+
,
applicationId
,
queue
.
getName
(
)
,
invalidAsks
)
)
;
}
handleContainerUpdates
(
application
,
updateRequests
)
;
normalizeResourceRequests
(
ask
,
queue
.
getName
(
)
)
;
application
.
recordContainerRequestTime
(
getClock
(
)
.
getTime
(
)
)
;
releaseContainers
(
release
,
application
)
;
ReentrantReadWriteLock
.
WriteLock
lock
=
application
.
getWriteLock
(
)
;
lock
.
lock
(
)
;
try
{
if
(
!
ask
.
isEmpty
(
)
)
{
}
handleContainerUpdates
(
application
,
updateRequests
)
;
normalizeResourceRequests
(
ask
,
queue
.
getName
(
)
)
;
application
.
recordContainerRequestTime
(
getClock
(
)
.
getTime
(
)
)
;
releaseContainers
(
release
,
application
)
;
ReentrantReadWriteLock
.
WriteLock
lock
=
application
.
getWriteLock
(
)
;
lock
.
lock
(
)
;
try
{
if
(
!
ask
.
isEmpty
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
+
appAttemptId
+
+
application
.
getApplicationId
(
)
)
;
}
application
.
showRequests
(
)
;
application
.
updateResourceRequests
(
ask
)
;
application
.
showRequests
(
)
;
}
}
finally
{
lock
.
unlock
(
)
;
final
NodeId
nodeID
=
(
node
!=
null
?
node
.
getNodeID
(
)
:
null
)
;
if
(
!
nodeTracker
.
exists
(
nodeID
)
)
{
LOG
.
info
(
+
nodeID
+
)
;
return
;
}
assignPreemptedContainers
(
node
)
;
FSAppAttempt
reservedAppSchedulable
=
node
.
getReservedAppSchedulable
(
)
;
boolean
validReservation
=
false
;
if
(
reservedAppSchedulable
!=
null
)
{
validReservation
=
reservedAppSchedulable
.
assignReservedContainer
(
node
)
;
}
if
(
!
validReservation
)
{
int
assignedContainers
=
0
;
Resource
assignedResource
=
Resources
.
clone
(
Resources
.
none
(
)
)
;
Resource
maxResourcesToAssign
=
Resources
.
multiply
(
node
.
getUnallocatedResource
(
)
,
0.5f
)
;
while
(
node
.
getReservedContainer
(
)
==
null
)
{
Resource
assignment
=
queueMgr
.
getRootQueue
(
)
.
assignContainer
(
node
)
;
private
void
rejectApplicationWithMessage
(
ApplicationId
applicationId
,
String
msg
)
{
@
Override
public
boolean
checkAccess
(
UserGroupInformation
callerUGI
,
QueueACL
acl
,
String
queueName
)
{
readLock
.
lock
(
)
;
try
{
FSQueue
queue
=
getQueueManager
(
)
.
getQueue
(
queueName
)
;
if
(
queue
==
null
)
{
List
<
FSAppAttempt
>
noLongerPendingApps
=
new
ArrayList
<
FSAppAttempt
>
(
)
;
while
(
iter
.
hasNext
(
)
)
{
FSAppAttempt
next
=
iter
.
next
(
)
;
if
(
next
==
prev
)
{
continue
;
}
if
(
canAppBeRunnable
(
next
.
getQueue
(
)
,
next
)
)
{
trackRunnableApp
(
next
)
;
FSAppAttempt
appSched
=
next
;
next
.
getQueue
(
)
.
addApp
(
appSched
,
true
)
;
noLongerPendingApps
.
add
(
appSched
)
;
if
(
noLongerPendingApps
.
size
(
)
>=
maxRunnableApps
)
{
break
;
}
}
prev
=
next
;
}
for
(
FSAppAttempt
appSched
:
noLongerPendingApps
)
{
if
(
!
appSched
.
getQueue
(
)
.
removeNonRunnableApp
(
appSched
)
)
{
FSAppAttempt
next
=
iter
.
next
(
)
;
if
(
next
==
prev
)
{
continue
;
}
if
(
canAppBeRunnable
(
next
.
getQueue
(
)
,
next
)
)
{
trackRunnableApp
(
next
)
;
FSAppAttempt
appSched
=
next
;
next
.
getQueue
(
)
.
addApp
(
appSched
,
true
)
;
noLongerPendingApps
.
add
(
appSched
)
;
if
(
noLongerPendingApps
.
size
(
)
>=
maxRunnableApps
)
{
break
;
}
}
prev
=
next
;
}
for
(
FSAppAttempt
appSched
:
noLongerPendingApps
)
{
if
(
!
appSched
.
getQueue
(
)
.
removeNonRunnableApp
(
appSched
)
)
{
LOG
.
error
(
+
+
appSched
+
)
;
}
if
(
!
usersNonRunnableApps
.
remove
(
appSched
.
getUser
(
)
,
appSched
)
)
{
private
FSQueue
createNewQueues
(
FSQueueType
queueType
,
FSParentQueue
topParent
,
List
<
String
>
newQueueNames
)
{
AllocationConfiguration
queueConf
=
scheduler
.
getAllocationConfiguration
(
)
;
Iterator
<
String
>
i
=
newQueueNames
.
iterator
(
)
;
FSParentQueue
parent
=
topParent
;
FSQueue
queue
=
null
;
while
(
i
.
hasNext
(
)
)
{
FSParentQueue
newParent
=
null
;
String
queueName
=
i
.
next
(
)
;
SchedulingPolicy
childPolicy
=
scheduler
.
getAllocationConfiguration
(
)
.
getSchedulingPolicy
(
queueName
)
;
if
(
!
parent
.
getPolicy
(
)
.
isChildPolicyAllowed
(
childPolicy
)
)
{
FSParentQueue
parent
=
topParent
;
FSQueue
queue
=
null
;
while
(
i
.
hasNext
(
)
)
{
FSParentQueue
newParent
=
null
;
String
queueName
=
i
.
next
(
)
;
SchedulingPolicy
childPolicy
=
scheduler
.
getAllocationConfiguration
(
)
.
getSchedulingPolicy
(
queueName
)
;
if
(
!
parent
.
getPolicy
(
)
.
isChildPolicyAllowed
(
childPolicy
)
)
{
LOG
.
error
(
+
queueName
+
+
)
;
return
null
;
}
if
(
!
i
.
hasNext
(
)
&&
(
queueType
!=
FSQueueType
.
PARENT
)
)
{
FSLeafQueue
leafQueue
=
new
FSLeafQueue
(
queueName
,
scheduler
,
parent
)
;
leafQueues
.
add
(
leafQueue
)
;
queue
=
leafQueue
;
}
else
{
if
(
childPolicy
instanceof
FifoPolicy
)
{
private
static
PlacementRule
getParentRule
(
Element
parent
,
FairScheduler
fs
)
throws
AllocationConfigurationException
{
siteConf
.
set
(
YarnConfiguration
.
FS_BASED_RM_CONF_STORE
,
outputDir
)
;
ConfigurationProvider
provider
=
new
FileSystemBasedConfigurationProvider
(
)
;
provider
.
init
(
siteConf
)
;
rmContext
.
setConfigurationProvider
(
provider
)
;
RMNodeLabelsManager
mgr
=
new
RMNodeLabelsManager
(
)
;
mgr
.
init
(
siteConf
)
;
rmContext
.
setNodeLabelManager
(
mgr
)
;
try
(
CapacityScheduler
cs
=
new
CapacityScheduler
(
)
)
{
cs
.
setConf
(
siteConf
)
;
cs
.
setRMContext
(
rmContext
)
;
cs
.
serviceInit
(
csConfig
)
;
cs
.
serviceStart
(
)
;
LOG
.
info
(
)
;
cs
.
serviceStop
(
)
;
}
catch
(
Exception
e
)
{
public
void
printDryRunResults
(
)
{
LOG
.
info
(
)
;
LOG
.
info
(
)
;
LOG
.
info
(
)
;
int
noOfErrors
=
errors
.
size
(
)
;
int
noOfWarnings
=
warnings
.
size
(
)
;
public
void
printDryRunResults
(
)
{
LOG
.
info
(
)
;
LOG
.
info
(
)
;
LOG
.
info
(
)
;
int
noOfErrors
=
errors
.
size
(
)
;
int
noOfWarnings
=
warnings
.
size
(
)
;
LOG
.
info
(
,
noOfErrors
)
;
static
void
logAndStdErr
(
Throwable
t
,
String
msg
)
{
static
void
logAndStdErr
(
Throwable
t
,
String
msg
)
{
LOG
.
debug
(
,
t
)
;
private
void
loadConversionRules
(
String
rulesFile
)
throws
IOException
{
if
(
rulesFile
!=
null
)
{
private
void
setActionForProperty
(
String
property
)
{
String
action
=
properties
.
getProperty
(
property
)
;
if
(
action
==
null
)
{
@
Override
public
boolean
isChildPolicyAllowed
(
SchedulingPolicy
childPolicy
)
{
if
(
childPolicy
instanceof
DominantResourceFairnessPolicy
)
{
if
(
isStopped
)
{
return
null
;
}
if
(
getOutstandingAsksCount
(
schedulerKey
)
<=
0
)
{
return
null
;
}
RMContainer
rmContainer
=
new
RMContainerImpl
(
container
,
schedulerKey
,
this
.
getApplicationAttemptId
(
)
,
node
.
getNodeID
(
)
,
appSchedulingInfo
.
getUser
(
)
,
this
.
rmContext
,
node
.
getPartition
(
)
)
;
(
(
RMContainerImpl
)
rmContainer
)
.
setQueueName
(
this
.
getQueueName
(
)
)
;
updateAMContainerDiagnostics
(
AMState
.
ASSIGNED
,
null
)
;
addToNewlyAllocatedContainers
(
node
,
rmContainer
)
;
ContainerId
containerId
=
container
.
getId
(
)
;
liveContainers
.
put
(
containerId
,
rmContainer
)
;
ContainerRequest
containerRequest
=
appSchedulingInfo
.
allocate
(
type
,
node
,
schedulerKey
,
rmContainer
)
;
attemptResourceUsage
.
incUsed
(
node
.
getPartition
(
)
,
container
.
getResource
(
)
)
;
(
(
RMContainerImpl
)
rmContainer
)
.
setContainerRequest
(
containerRequest
)
;
rmContainer
.
handle
(
new
RMContainerEvent
(
containerId
,
RMContainerEventType
.
START
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
Allocation
allocate
(
ApplicationAttemptId
applicationAttemptId
,
List
<
ResourceRequest
>
ask
,
List
<
SchedulingRequest
>
schedulingRequests
,
List
<
ContainerId
>
release
,
List
<
String
>
blacklistAdditions
,
List
<
String
>
blacklistRemovals
,
ContainerUpdates
updateRequests
)
{
FifoAppAttempt
application
=
getApplicationAttempt
(
applicationAttemptId
)
;
if
(
application
==
null
)
{
@
Override
public
Allocation
allocate
(
ApplicationAttemptId
applicationAttemptId
,
List
<
ResourceRequest
>
ask
,
List
<
SchedulingRequest
>
schedulingRequests
,
List
<
ContainerId
>
release
,
List
<
String
>
blacklistAdditions
,
List
<
String
>
blacklistRemovals
,
ContainerUpdates
updateRequests
)
{
FifoAppAttempt
application
=
getApplicationAttempt
(
applicationAttemptId
)
;
if
(
application
==
null
)
{
LOG
.
error
(
+
applicationAttemptId
.
getApplicationId
(
)
)
;
return
EMPTY_ALLOCATION
;
}
if
(
!
application
.
getApplicationAttemptId
(
)
.
equals
(
applicationAttemptId
)
)
{
LOG
.
error
(
+
+
applicationAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
normalizeResourceRequests
(
ask
)
;
releaseContainers
(
release
,
application
)
;
synchronized
(
application
)
{
if
(
application
.
isStopped
(
)
)
{
LOG
.
info
(
+
+
applicationAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
if
(
!
ask
.
isEmpty
(
)
)
{
LOG
.
error
(
+
applicationAttemptId
.
getApplicationId
(
)
)
;
return
EMPTY_ALLOCATION
;
}
if
(
!
application
.
getApplicationAttemptId
(
)
.
equals
(
applicationAttemptId
)
)
{
LOG
.
error
(
+
+
applicationAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
normalizeResourceRequests
(
ask
)
;
releaseContainers
(
release
,
application
)
;
synchronized
(
application
)
{
if
(
application
.
isStopped
(
)
)
{
LOG
.
info
(
+
+
applicationAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
if
(
!
ask
.
isEmpty
(
)
)
{
LOG
.
debug
(
+
+
applicationAttemptId
+
+
application
)
;
application
.
showRequests
(
)
;
application
.
updateResourceRequests
(
ask
)
;
}
if
(
!
application
.
getApplicationAttemptId
(
)
.
equals
(
applicationAttemptId
)
)
{
LOG
.
error
(
+
+
applicationAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
normalizeResourceRequests
(
ask
)
;
releaseContainers
(
release
,
application
)
;
synchronized
(
application
)
{
if
(
application
.
isStopped
(
)
)
{
LOG
.
info
(
+
+
applicationAttemptId
)
;
return
EMPTY_ALLOCATION
;
}
if
(
!
ask
.
isEmpty
(
)
)
{
LOG
.
debug
(
+
+
applicationAttemptId
+
+
application
)
;
application
.
showRequests
(
)
;
application
.
updateResourceRequests
(
ask
)
;
LOG
.
debug
(
+
+
applicationAttemptId
+
+
application
)
;
application
.
showRequests
(
)
;
@
VisibleForTesting
public
synchronized
void
addApplication
(
ApplicationId
applicationId
,
String
queue
,
String
user
,
boolean
isAppRecovering
)
{
SchedulerApplication
<
FifoAppAttempt
>
application
=
new
SchedulerApplication
<
>
(
DEFAULT_QUEUE
,
user
)
;
applications
.
put
(
applicationId
,
application
)
;
metrics
.
submitApp
(
user
)
;
@
VisibleForTesting
public
synchronized
void
addApplication
(
ApplicationId
applicationId
,
String
queue
,
String
user
,
boolean
isAppRecovering
)
{
SchedulerApplication
<
FifoAppAttempt
>
application
=
new
SchedulerApplication
<
>
(
DEFAULT_QUEUE
,
user
)
;
applications
.
put
(
applicationId
,
application
)
;
metrics
.
submitApp
(
user
)
;
LOG
.
info
(
+
applicationId
+
+
user
+
+
applications
.
size
(
)
)
;
if
(
isAppRecovering
)
{
private
int
assignContainer
(
FiCaSchedulerNode
node
,
FifoAppAttempt
application
,
SchedulerRequestKey
schedulerKey
,
int
assignableContainers
,
Resource
capability
,
NodeType
type
)
{
}
break
;
case
APP_ADDED
:
{
AppAddedSchedulerEvent
appAddedEvent
=
(
AppAddedSchedulerEvent
)
event
;
addApplication
(
appAddedEvent
.
getApplicationId
(
)
,
appAddedEvent
.
getQueue
(
)
,
appAddedEvent
.
getUser
(
)
,
appAddedEvent
.
getIsAppRecovering
(
)
)
;
}
break
;
case
APP_REMOVED
:
{
AppRemovedSchedulerEvent
appRemovedEvent
=
(
AppRemovedSchedulerEvent
)
event
;
doneApplication
(
appRemovedEvent
.
getApplicationID
(
)
,
appRemovedEvent
.
getFinalState
(
)
)
;
}
break
;
case
APP_ATTEMPT_ADDED
:
{
AppAttemptAddedSchedulerEvent
appAttemptAddedEvent
=
(
AppAttemptAddedSchedulerEvent
)
event
;
addApplicationAttempt
(
appAttemptAddedEvent
.
getApplicationAttemptId
(
)
,
appAttemptAddedEvent
.
getTransferStateFromPreviousAttempt
(
)
,
appAttemptAddedEvent
.
getIsAttemptRecovering
(
)
)
;
}
break
;
case
APP_ATTEMPT_REMOVED
:
{
AppAttemptRemovedSchedulerEvent
appAttemptRemovedEvent
=
(
AppAttemptRemovedSchedulerEvent
)
event
;
@
Lock
(
FifoScheduler
.
class
)
@
Override
protected
synchronized
void
completedContainerInternal
(
RMContainer
rmContainer
,
ContainerStatus
containerStatus
,
RMContainerEventType
event
)
{
Container
container
=
rmContainer
.
getContainer
(
)
;
FifoAppAttempt
application
=
getCurrentAttemptForContainer
(
container
.
getId
(
)
)
;
ApplicationId
appId
=
container
.
getId
(
)
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
FiCaSchedulerNode
node
=
(
FiCaSchedulerNode
)
getNode
(
container
.
getNodeId
(
)
)
;
if
(
application
==
null
)
{
@
VisibleForTesting
@
Override
public
void
killContainer
(
RMContainer
container
)
{
ContainerStatus
status
=
SchedulerUtils
.
createKilledContainerStatus
(
container
.
getContainerId
(
)
,
)
;
public
void
initialize
(
AppSchedulingInfo
appSchedulingInfo
,
SchedulerRequestKey
schedulerRequestKey
,
RMContext
rmContext
)
{
this
.
appSchedulingInfo
=
appSchedulingInfo
;
this
.
rmContext
=
rmContext
;
this
.
schedulerRequestKey
=
schedulerRequestKey
;
multiNodeSortPolicyName
=
appSchedulingInfo
.
getApplicationSchedulingEnvs
(
)
.
get
(
ApplicationSchedulingConfig
.
ENV_MULTI_NODE_SORTING_POLICY_CLASS
)
;
multiNodeSortingManager
=
(
MultiNodeSortingManager
<
N
>
)
rmContext
.
getMultiNodeSortingManager
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
@
Override
public
boolean
precheckNode
(
SchedulerNode
schedulerNode
,
SchedulingMode
schedulingMode
,
Optional
<
DiagnosticsCollector
>
dcOpt
)
{
@
Override
public
void
showRequests
(
)
{
for
(
ResourceRequest
request
:
resourceRequestMap
.
values
(
)
)
{
if
(
request
.
getNumContainers
(
)
>
0
)
{
public
void
registerMultiNodePolicyNames
(
boolean
isMultiNodePlacementEnabled
,
Set
<
MultiNodePolicySpec
>
multiNodePlacementPolicies
)
{
this
.
policySpecs
.
addAll
(
multiNodePlacementPolicies
)
;
this
.
multiNodePlacementEnabled
=
isMultiNodePlacementEnabled
;
if
(
recoverContainer
)
{
newNumAllocations
=
existingNumAllocations
+
1
;
}
else
{
newNumAllocations
=
sizing
.
getNumAllocations
(
)
;
}
sizing
.
setNumAllocations
(
existingNumAllocations
)
;
if
(
!
schedulingRequest
.
equals
(
newSchedulingRequest
)
)
{
sizing
.
setNumAllocations
(
newNumAllocations
)
;
throw
new
SchedulerInvalidResoureRequestException
(
+
+
+
schedulingRequest
.
toString
(
)
+
+
newSchedulingRequest
.
toString
(
)
+
+
+
+
)
;
}
else
{
if
(
newNumAllocations
==
existingNumAllocations
)
{
return
null
;
}
}
sizing
.
setNumAllocations
(
newNumAllocations
)
;
if
(
newNumAllocations
<
0
)
{
throw
new
SchedulerInvalidResoureRequestException
(
+
)
;
}
PendingAskUpdateResult
updateResult
=
new
PendingAskUpdateResult
(
new
PendingAsk
(
schedulingRequest
.
getResourceSizing
(
)
)
,
new
PendingAsk
(
newSchedulingRequest
.
getResourceSizing
(
)
)
,
targetNodePartition
,
targetNodePartition
)
;
@
Override
public
void
showRequests
(
)
{
readLock
.
lock
(
)
;
try
{
if
(
schedulingRequest
!=
null
)
{
public
void
applicationMasterFinished
(
ApplicationAttemptId
appAttemptId
)
{
this
.
writeLock
.
lock
(
)
;
try
{
public
Token
<
AMRMTokenIdentifier
>
createAndGetAMRMToken
(
ApplicationAttemptId
appAttemptId
)
{
this
.
writeLock
.
lock
(
)
;
try
{
public
void
addPersistedPassword
(
Token
<
AMRMTokenIdentifier
>
token
)
throws
IOException
{
this
.
writeLock
.
lock
(
)
;
try
{
AMRMTokenIdentifier
identifier
=
token
.
decodeIdentifier
(
)
;
@
Override
public
byte
[
]
retrievePassword
(
AMRMTokenIdentifier
identifier
)
throws
InvalidToken
{
this
.
readLock
.
lock
(
)
;
try
{
ApplicationAttemptId
applicationAttemptId
=
identifier
.
getApplicationAttemptId
(
)
;
@
Override
@
Private
protected
byte
[
]
createPassword
(
AMRMTokenIdentifier
identifier
)
{
this
.
readLock
.
lock
(
)
;
try
{
ApplicationAttemptId
applicationAttemptId
=
identifier
.
getApplicationAttemptId
(
)
;
serviceStateLock
.
writeLock
(
)
.
lock
(
)
;
try
{
isServiceStarted
=
false
;
this
.
renewerService
.
shutdown
(
)
;
}
finally
{
serviceStateLock
.
writeLock
(
)
.
unlock
(
)
;
}
dtCancelThread
.
interrupt
(
)
;
try
{
dtCancelThread
.
join
(
1000
)
;
}
catch
(
InterruptedException
e
)
{
e
.
printStackTrace
(
)
;
}
if
(
tokenKeepAliveEnabled
&&
delayedRemovalThread
!=
null
)
{
delayedRemovalThread
.
interrupt
(
)
;
try
{
delayedRemovalThread
.
join
(
1000
)
;
private
void
handleAppSubmitEvent
(
AbstractDelegationTokenRenewerAppEvent
evt
)
throws
IOException
,
InterruptedException
{
ApplicationId
applicationId
=
evt
.
getApplicationId
(
)
;
Credentials
ts
=
evt
.
getCredentials
(
)
;
boolean
shouldCancelAtEnd
=
evt
.
shouldCancelAtEnd
(
)
;
if
(
ts
==
null
)
{
return
;
}
LOG
.
debug
(
,
applicationId
)
;
Collection
<
Token
<
?
>>
tokens
=
ts
.
getAllTokens
(
)
;
long
now
=
System
.
currentTimeMillis
(
)
;
appTokens
.
put
(
applicationId
,
Collections
.
synchronizedSet
(
new
HashSet
<
DelegationTokenToRenew
>
(
)
)
)
;
Set
<
DelegationTokenToRenew
>
tokenList
=
new
HashSet
<
DelegationTokenToRenew
>
(
)
;
boolean
hasHdfsToken
=
false
;
for
(
Token
<
?
>
token
:
tokens
)
{
if
(
token
.
isManaged
(
)
)
{
if
(
token
.
getKind
(
)
.
equals
(
HDFS_DELEGATION_KIND
)
)
{
long
now
=
System
.
currentTimeMillis
(
)
;
appTokens
.
put
(
applicationId
,
Collections
.
synchronizedSet
(
new
HashSet
<
DelegationTokenToRenew
>
(
)
)
)
;
Set
<
DelegationTokenToRenew
>
tokenList
=
new
HashSet
<
DelegationTokenToRenew
>
(
)
;
boolean
hasHdfsToken
=
false
;
for
(
Token
<
?
>
token
:
tokens
)
{
if
(
token
.
isManaged
(
)
)
{
if
(
token
.
getKind
(
)
.
equals
(
HDFS_DELEGATION_KIND
)
)
{
LOG
.
info
(
applicationId
+
+
token
)
;
hasHdfsToken
=
true
;
}
if
(
skipTokenRenewal
(
token
)
)
{
continue
;
}
DelegationTokenToRenew
dttr
=
allTokens
.
get
(
token
)
;
if
(
dttr
==
null
)
{
Configuration
tokenConf
;
if
(
evt
.
tokenConf
!=
null
)
{
}
DelegationTokenToRenew
dttr
=
allTokens
.
get
(
token
)
;
if
(
dttr
==
null
)
{
Configuration
tokenConf
;
if
(
evt
.
tokenConf
!=
null
)
{
tokenConf
=
evt
.
tokenConf
;
LOG
.
info
(
+
+
tokenConf
.
size
(
)
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
for
(
Iterator
<
Map
.
Entry
<
String
,
String
>>
itor
=
tokenConf
.
iterator
(
)
;
itor
.
hasNext
(
)
;
)
{
Map
.
Entry
<
String
,
String
>
entry
=
itor
.
next
(
)
;
LOG
.
debug
(
,
entry
.
getKey
(
)
,
entry
.
getValue
(
)
)
;
}
}
}
else
{
tokenConf
=
getConfig
(
)
;
}
dttr
=
new
DelegationTokenToRenew
(
Arrays
.
asList
(
applicationId
)
,
token
,
tokenConf
,
now
,
shouldCancelAtEnd
,
evt
.
getUser
(
)
)
;
try
{
renewToken
(
dttr
)
;
@
VisibleForTesting
protected
void
setTimerForTokenRenewal
(
DelegationTokenToRenew
token
)
throws
IOException
{
long
expiresIn
=
token
.
expirationDate
-
System
.
currentTimeMillis
(
)
;
if
(
expiresIn
<=
0
)
{
synchronized
(
dttr
.
referringAppIds
)
{
applicationIds
=
new
HashSet
<
>
(
dttr
.
referringAppIds
)
;
dttr
.
referringAppIds
.
clear
(
)
;
}
for
(
ApplicationId
appId
:
applicationIds
)
{
Set
<
DelegationTokenToRenew
>
tokenSet
=
appTokens
.
get
(
appId
)
;
if
(
tokenSet
==
null
||
tokenSet
.
isEmpty
(
)
)
{
continue
;
}
Iterator
<
DelegationTokenToRenew
>
iter
=
tokenSet
.
iterator
(
)
;
synchronized
(
tokenSet
)
{
while
(
iter
.
hasNext
(
)
)
{
DelegationTokenToRenew
t
=
iter
.
next
(
)
;
if
(
t
.
token
.
getKind
(
)
.
equals
(
HDFS_DELEGATION_KIND
)
)
{
iter
.
remove
(
)
;
allTokens
.
remove
(
t
.
token
)
;
t
.
cancelTimer
(
)
;
private
void
removeFailedDelegationToken
(
DelegationTokenToRenew
t
)
{
Collection
<
ApplicationId
>
applicationIds
=
t
.
referringAppIds
;
synchronized
(
applicationIds
)
{
private
void
removeApplicationFromRenewal
(
ApplicationId
applicationId
)
{
rmContext
.
getSystemCredentialsForApps
(
)
.
remove
(
applicationId
)
;
Set
<
DelegationTokenToRenew
>
tokens
=
appTokens
.
remove
(
applicationId
)
;
if
(
tokens
!=
null
&&
!
tokens
.
isEmpty
(
)
)
{
synchronized
(
tokens
)
{
Iterator
<
DelegationTokenToRenew
>
it
=
tokens
.
iterator
(
)
;
while
(
it
.
hasNext
(
)
)
{
DelegationTokenToRenew
dttr
=
it
.
next
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
TimerTask
getTimerTask
(
AbstractDelegationTokenRenewerAppEvent
evt
)
{
return
new
TimerTask
(
)
{
@
Override
public
void
run
(
)
{
public
void
clearNodeSetForAttempt
(
ApplicationAttemptId
attemptId
)
{
super
.
writeLock
.
lock
(
)
;
try
{
HashSet
<
NodeId
>
nodeSet
=
this
.
appAttemptToNodeKeyMap
.
get
(
attemptId
)
;
if
(
nodeSet
!=
null
)
{
public
NMToken
createAndGetNMToken
(
String
applicationSubmitter
,
ApplicationAttemptId
appAttemptId
,
Container
container
)
{
this
.
writeLock
.
lock
(
)
;
try
{
HashSet
<
NodeId
>
nodeSet
=
this
.
appAttemptToNodeKeyMap
.
get
(
appAttemptId
)
;
NMToken
nmToken
=
null
;
if
(
nodeSet
!=
null
)
{
if
(
!
nodeSet
.
contains
(
container
.
getNodeId
(
)
)
)
{
@
Override
protected
void
storeNewMasterKey
(
DelegationKey
newKey
)
{
try
{
@
Override
protected
void
removeStoredMasterKey
(
DelegationKey
key
)
{
try
{
@
Override
protected
void
storeNewToken
(
RMDelegationTokenIdentifier
identifier
,
long
renewDate
)
{
try
{
@
Override
protected
void
updateStoredToken
(
RMDelegationTokenIdentifier
id
,
long
renewDate
)
{
try
{
@
Override
protected
void
removeStoredToken
(
RMDelegationTokenIdentifier
ident
)
throws
IOException
{
try
{
private
void
initCsiAdaptorCache
(
final
Map
<
String
,
CsiAdaptorProtocol
>
adaptorMap
,
Configuration
conf
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
String
[
]
addresses
=
conf
.
getStrings
(
YarnConfiguration
.
NM_CSI_ADAPTOR_ADDRESSES
)
;
if
(
addresses
!=
null
&&
addresses
.
length
>
0
)
{
for
(
String
addr
:
addresses
)
{
private
void
initCsiAdaptorCache
(
final
Map
<
String
,
CsiAdaptorProtocol
>
adaptorMap
,
Configuration
conf
)
throws
IOException
,
YarnException
{
LOG
.
info
(
)
;
String
[
]
addresses
=
conf
.
getStrings
(
YarnConfiguration
.
NM_CSI_ADAPTOR_ADDRESSES
)
;
if
(
addresses
!=
null
&&
addresses
.
length
>
0
)
{
for
(
String
addr
:
addresses
)
{
LOG
.
info
(
+
addr
)
;
InetSocketAddress
address
=
NetUtils
.
createSocketAddr
(
addr
)
;
YarnRPC
rpc
=
YarnRPC
.
create
(
conf
)
;
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
CsiAdaptorProtocol
adaptorClient
=
NMProxy
.
createNMProxy
(
conf
,
CsiAdaptorProtocol
.
class
,
currentUser
,
rpc
,
address
)
;
String
[
]
addresses
=
conf
.
getStrings
(
YarnConfiguration
.
NM_CSI_ADAPTOR_ADDRESSES
)
;
if
(
addresses
!=
null
&&
addresses
.
length
>
0
)
{
for
(
String
addr
:
addresses
)
{
LOG
.
info
(
+
addr
)
;
InetSocketAddress
address
=
NetUtils
.
createSocketAddr
(
addr
)
;
YarnRPC
rpc
=
YarnRPC
.
create
(
conf
)
;
UserGroupInformation
currentUser
=
UserGroupInformation
.
getCurrentUser
(
)
;
CsiAdaptorProtocol
adaptorClient
=
NMProxy
.
createNMProxy
(
conf
,
CsiAdaptorProtocol
.
class
,
currentUser
,
rpc
,
address
)
;
LOG
.
info
(
+
addr
)
;
GetPluginInfoResponse
response
=
adaptorClient
.
getPluginInfo
(
GetPluginInfoRequest
.
newInstance
(
)
)
;
if
(
!
Strings
.
isNullOrEmpty
(
response
.
getDriverName
(
)
)
)
{
String
driverName
=
response
.
getDriverName
(
)
;
if
(
adaptorMap
.
containsKey
(
driverName
)
)
{
throw
new
YarnException
(
+
+
driverName
)
;
}
adaptorMap
.
put
(
driverName
,
adaptorClient
)
;
@
Override
public
ScheduledFuture
<
VolumeProvisioningResults
>
schedule
(
VolumeProvisioningTask
volumeProvisioningTask
,
int
delaySecond
)
{
this
.
writeLock
.
lock
(
)
;
try
{
VolumeId
volumeId
=
event
.
getVolumeId
(
)
;
if
(
volumeId
==
null
)
{
LOG
.
warn
(
+
event
.
getType
(
)
.
name
(
)
+
)
;
return
;
}
LOG
.
info
(
+
event
.
getType
(
)
.
name
(
)
+
+
volumeId
.
toString
(
)
)
;
VolumeState
oldState
=
null
;
VolumeState
newState
=
null
;
try
{
oldState
=
stateMachine
.
getCurrentState
(
)
;
newState
=
stateMachine
.
doTransition
(
event
.
getType
(
)
,
event
)
;
}
catch
(
InvalidStateTransitionException
e
)
{
LOG
.
warn
(
+
oldState
+
+
event
.
getType
(
)
+
+
+
volumeId
+
,
e
)
;
}
if
(
newState
!=
null
&&
oldState
!=
newState
)
{
ApplicationId
appId
=
null
;
ApplicationAttemptId
appAttemptId
=
null
;
ContainerId
containerId
=
null
;
switch
(
type
)
{
case
:
try
{
appId
=
Apps
.
toAppID
(
parts
[
3
]
)
;
}
catch
(
YarnRuntimeException
|
NumberFormatException
e
)
{
LOG
.
debug
(
,
parts
[
3
]
,
e
)
;
return
redirectPath
;
}
if
(
!
context
.
getRMApps
(
)
.
containsKey
(
appId
)
)
{
redirectPath
=
pjoin
(
ahsPageURLPrefix
,
,
appId
)
;
}
break
;
case
:
try
{
appAttemptId
=
ApplicationAttemptId
.
fromString
(
parts
[
3
]
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
debug
(
,
parts
[
3
]
,
e
)
;
return
redirectPath
;
}
if
(
!
context
.
getRMApps
(
)
.
containsKey
(
appId
)
)
{
redirectPath
=
pjoin
(
ahsPageURLPrefix
,
,
appId
)
;
}
break
;
case
:
try
{
appAttemptId
=
ApplicationAttemptId
.
fromString
(
parts
[
3
]
)
;
}
catch
(
IllegalArgumentException
e
)
{
LOG
.
debug
(
,
parts
[
3
]
,
e
)
;
return
redirectPath
;
}
if
(
!
context
.
getRMApps
(
)
.
containsKey
(
appAttemptId
.
getApplicationId
(
)
)
)
{
redirectPath
=
pjoin
(
ahsPageURLPrefix
,
,
appAttemptId
)
;
}
break
;
case
:
try
{
containerId
=
ContainerId
.
fromString
(
parts
[
3
]
)
;
@
POST
@
Path
(
RMWSConsts
.
NODE_RESOURCE
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
,
MediaType
.
APPLICATION_XML
}
)
@
Produces
(
{
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
,
MediaType
.
APPLICATION_XML
+
+
JettyUtils
.
UTF_8
}
)
public
ResourceInfo
updateNodeResource
(
@
Context
HttpServletRequest
hsr
,
@
PathParam
(
RMWSConsts
.
NODEID
)
String
nodeId
,
ResourceOptionInfo
resourceOption
)
throws
AuthorizationException
{
UserGroupInformation
callerUGI
=
getCallerUserGroupInformation
(
hsr
,
true
)
;
initForWritableEndpoints
(
callerUGI
,
false
)
;
RMNode
rmNode
=
getRMNode
(
nodeId
)
;
Map
<
NodeId
,
ResourceOption
>
nodeResourceMap
=
Collections
.
singletonMap
(
rmNode
.
getNodeID
(
)
,
resourceOption
.
getResourceOption
(
)
)
;
UpdateNodeResourceRequest
updateRequest
=
UpdateNodeResourceRequest
.
newInstance
(
nodeResourceMap
)
;
try
{
RMContext
rmContext
=
this
.
rm
.
getRMContext
(
)
;
AdminService
admin
=
rmContext
.
getRMAdminService
(
)
;
admin
.
updateNodeResource
(
updateRequest
)
;
}
catch
(
YarnException
e
)
{
String
message
=
+
rmNode
.
getNodeID
(
)
+
;
LOG
.
error
(
message
,
e
)
;
throw
new
YarnRuntimeException
(
message
,
e
)
;
}
catch
(
IOException
e
)
{
int
limitNum
=
-
1
;
if
(
limit
!=
null
)
{
try
{
limitNum
=
Integer
.
parseInt
(
limit
)
;
if
(
limitNum
<=
0
)
{
return
new
AppActivitiesInfo
(
,
appId
)
;
}
}
catch
(
NumberFormatException
e
)
{
return
new
AppActivitiesInfo
(
,
appId
)
;
}
}
double
maxTime
=
3.0
;
if
(
time
!=
null
)
{
if
(
time
.
contains
(
)
)
{
maxTime
=
Double
.
parseDouble
(
time
)
;
}
else
{
maxTime
=
Double
.
parseDouble
(
time
+
)
;
}
}
ApplicationId
applicationId
;
resp
=
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
RenewDelegationTokenResponse
>
(
)
{
@
Override
public
RenewDelegationTokenResponse
run
(
)
throws
YarnException
{
return
rm
.
getClientRMService
(
)
.
renewDelegationToken
(
req
)
;
}
}
)
;
}
catch
(
UndeclaredThrowableException
ue
)
{
if
(
ue
.
getCause
(
)
instanceof
YarnException
)
{
if
(
ue
.
getCause
(
)
.
getCause
(
)
instanceof
InvalidToken
)
{
throw
new
BadRequestException
(
ue
.
getCause
(
)
.
getCause
(
)
.
getMessage
(
)
)
;
}
else
if
(
ue
.
getCause
(
)
.
getCause
(
)
instanceof
org
.
apache
.
hadoop
.
security
.
AccessControlException
)
{
return
Response
.
status
(
Status
.
FORBIDDEN
)
.
entity
(
ue
.
getCause
(
)
.
getCause
(
)
.
getMessage
(
)
)
.
build
(
)
;
}
LOG
.
info
(
,
ue
)
;
throw
ue
;
}
LOG
.
info
(
,
ue
)
;
throw
ue
;
}
catch
(
Exception
e
)
{
return
Response
.
status
(
Status
.
FORBIDDEN
)
.
entity
(
ye
.
getMessage
(
)
)
.
build
(
)
;
}
Token
<
RMDelegationTokenIdentifier
>
token
=
extractToken
(
hsr
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
dToken
=
BuilderUtils
.
newDelegationToken
(
token
.
getIdentifier
(
)
,
token
.
getKind
(
)
.
toString
(
)
,
token
.
getPassword
(
)
,
token
.
getService
(
)
.
toString
(
)
)
;
final
CancelDelegationTokenRequest
req
=
CancelDelegationTokenRequest
.
newInstance
(
dToken
)
;
try
{
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
CancelDelegationTokenResponse
>
(
)
{
@
Override
public
CancelDelegationTokenResponse
run
(
)
throws
IOException
,
YarnException
{
return
rm
.
getClientRMService
(
)
.
cancelDelegationToken
(
req
)
;
}
}
)
;
}
catch
(
UndeclaredThrowableException
ue
)
{
if
(
ue
.
getCause
(
)
instanceof
YarnException
)
{
if
(
ue
.
getCause
(
)
.
getCause
(
)
instanceof
InvalidToken
)
{
throw
new
BadRequestException
(
ue
.
getCause
(
)
.
getCause
(
)
.
getMessage
(
)
)
;
}
else
if
(
ue
.
getCause
(
)
.
getCause
(
)
instanceof
org
.
apache
.
hadoop
.
security
.
AccessControlException
)
{
return
Response
.
status
(
Status
.
FORBIDDEN
)
.
entity
(
ue
.
getCause
(
)
.
getCause
(
)
.
getMessage
(
)
)
.
build
(
)
;
Token
<
RMDelegationTokenIdentifier
>
token
=
extractToken
(
hsr
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
dToken
=
BuilderUtils
.
newDelegationToken
(
token
.
getIdentifier
(
)
,
token
.
getKind
(
)
.
toString
(
)
,
token
.
getPassword
(
)
,
token
.
getService
(
)
.
toString
(
)
)
;
final
CancelDelegationTokenRequest
req
=
CancelDelegationTokenRequest
.
newInstance
(
dToken
)
;
try
{
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
CancelDelegationTokenResponse
>
(
)
{
@
Override
public
CancelDelegationTokenResponse
run
(
)
throws
IOException
,
YarnException
{
return
rm
.
getClientRMService
(
)
.
cancelDelegationToken
(
req
)
;
}
}
)
;
}
catch
(
UndeclaredThrowableException
ue
)
{
if
(
ue
.
getCause
(
)
instanceof
YarnException
)
{
if
(
ue
.
getCause
(
)
.
getCause
(
)
instanceof
InvalidToken
)
{
throw
new
BadRequestException
(
ue
.
getCause
(
)
.
getCause
(
)
.
getMessage
(
)
)
;
}
else
if
(
ue
.
getCause
(
)
.
getCause
(
)
instanceof
org
.
apache
.
hadoop
.
security
.
AccessControlException
)
{
return
Response
.
status
(
Status
.
FORBIDDEN
)
.
entity
(
ue
.
getCause
(
)
.
getCause
(
)
.
getMessage
(
)
)
.
build
(
)
;
}
LOG
.
info
(
,
ue
)
;
UserGroupInformation
callerUGI
=
getCallerUserGroupInformation
(
hsr
,
true
)
;
initForWritableEndpoints
(
callerUGI
,
true
)
;
ResourceScheduler
scheduler
=
rm
.
getResourceScheduler
(
)
;
if
(
scheduler
instanceof
MutableConfScheduler
&&
(
(
MutableConfScheduler
)
scheduler
)
.
isConfigurationMutable
(
)
)
{
try
{
MutableConfigurationProvider
mutableConfigurationProvider
=
(
(
MutableConfScheduler
)
scheduler
)
.
getMutableConfProvider
(
)
;
mutableConfigurationProvider
.
formatConfigurationInStore
(
conf
)
;
try
{
rm
.
getRMContext
(
)
.
getRMAdminService
(
)
.
refreshQueues
(
)
;
}
catch
(
IOException
|
YarnException
e
)
{
LOG
.
error
(
,
e
)
;
mutableConfigurationProvider
.
revertToOldConfig
(
conf
)
;
throw
e
;
}
return
Response
.
status
(
Status
.
OK
)
.
entity
(
+
)
.
build
(
)
;
}
catch
(
Exception
e
)
{
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
Void
>
(
)
{
@
Override
public
Void
run
(
)
throws
Exception
{
MutableConfigurationProvider
provider
=
(
(
MutableConfScheduler
)
scheduler
)
.
getMutableConfProvider
(
)
;
if
(
!
provider
.
getAclMutationPolicy
(
)
.
isMutationAllowed
(
callerUGI
,
mutationInfo
)
)
{
throw
new
org
.
apache
.
hadoop
.
security
.
AccessControlException
(
+
)
;
}
LogMutation
logMutation
=
provider
.
logAndApplyMutation
(
callerUGI
,
mutationInfo
)
;
try
{
rm
.
getRMContext
(
)
.
getRMAdminService
(
)
.
refreshQueues
(
)
;
}
catch
(
IOException
|
YarnException
e
)
{
provider
.
confirmPendingMutation
(
logMutation
,
false
)
;
throw
e
;
}
provider
.
confirmPendingMutation
(
logMutation
,
true
)
;
return
null
;
}
}
)
;
}
catch
(
IOException
e
)
{
public
synchronized
void
addTask
(
Task
task
)
{
SchedulerRequestKey
schedulerKey
=
task
.
getSchedulerKey
(
)
;
Map
<
String
,
ResourceRequest
>
requests
=
this
.
requests
.
get
(
schedulerKey
)
;
if
(
requests
==
null
)
{
requests
=
new
HashMap
<
String
,
ResourceRequest
>
(
)
;
this
.
requests
.
put
(
schedulerKey
,
requests
)
;
public
synchronized
List
<
Container
>
getResources
(
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
,
applicationId
,
ask
.
size
(
)
)
;
for
(
ResourceRequest
request
:
ask
)
{
private
synchronized
void
assign
(
SchedulerRequestKey
schedulerKey
,
NodeType
type
,
List
<
Container
>
containers
)
throws
IOException
,
YarnException
{
for
(
Iterator
<
Container
>
i
=
containers
.
iterator
(
)
;
i
.
hasNext
(
)
;
)
{
Container
container
=
i
.
next
(
)
;
String
host
=
container
.
getNodeId
(
)
.
toString
(
)
;
if
(
Resources
.
equals
(
requestSpec
.
get
(
schedulerKey
)
,
container
.
getResource
(
)
)
)
{
for
(
Iterator
<
Task
>
t
=
tasks
.
get
(
schedulerKey
)
.
iterator
(
)
;
t
.
hasNext
(
)
;
)
{
Task
task
=
t
.
next
(
)
;
if
(
task
.
getState
(
)
==
State
.
PENDING
&&
task
.
canSchedule
(
type
,
host
)
)
{
NodeManager
nodeManager
=
getNodeManager
(
host
)
;
task
.
start
(
nodeManager
,
container
.
getId
(
)
)
;
i
.
remove
(
)
;
Resources
.
addTo
(
used
,
container
.
getResource
(
)
)
;
private
void
updateResourceRequests
(
Map
<
String
,
ResourceRequest
>
requests
,
NodeType
type
,
Task
task
)
{
if
(
type
==
NodeType
.
NODE_LOCAL
)
{
for
(
String
host
:
task
.
getHosts
(
)
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
updateResourceRequest
(
ResourceRequest
request
)
{
request
.
setNumContainers
(
request
.
getNumContainers
(
)
-
1
)
;
ask
.
remove
(
request
)
;
ask
.
add
(
ResourceRequest
.
clone
(
request
)
)
;
driver
.
put
(
testCapacityDRConf
,
EnumSet
.
of
(
YarnServiceProtos
.
SchedulerResourceTypes
.
CPU
,
YarnServiceProtos
.
SchedulerResourceTypes
.
MEMORY
)
)
;
driver
.
put
(
testCapacityDefConf
,
EnumSet
.
of
(
YarnServiceProtos
.
SchedulerResourceTypes
.
MEMORY
)
)
;
driver
.
put
(
testFairDefConf
,
EnumSet
.
of
(
YarnServiceProtos
.
SchedulerResourceTypes
.
MEMORY
,
YarnServiceProtos
.
SchedulerResourceTypes
.
CPU
)
)
;
for
(
Map
.
Entry
<
YarnConfiguration
,
EnumSet
<
YarnServiceProtos
.
SchedulerResourceTypes
>>
entry
:
driver
.
entrySet
(
)
)
{
EnumSet
<
YarnServiceProtos
.
SchedulerResourceTypes
>
expectedValue
=
entry
.
getValue
(
)
;
MockRM
rm
=
new
MockRM
(
entry
.
getKey
(
)
)
;
rm
.
start
(
)
;
MockNM
nm1
=
rm
.
registerNode
(
DEFAULT_HOST
+
+
DEFAULT_PORT
,
6
*
GB
)
;
RMApp
app1
=
MockRMAppSubmitter
.
submitWithMemory
(
2048
,
rm
)
;
Thread
.
sleep
(
1000
)
;
nm1
.
nodeHeartbeat
(
true
)
;
RMAppAttempt
attempt1
=
app1
.
getCurrentAppAttempt
(
)
;
MockAM
am1
=
rm
.
sendAMLaunched
(
attempt1
.
getAppAttemptId
(
)
)
;
RegisterApplicationMasterResponse
resp
=
am1
.
registerAppAttempt
(
)
;
EnumSet
<
YarnServiceProtos
.
SchedulerResourceTypes
>
types
=
resp
.
getSchedulerResourceTypes
(
)
;
public
void
waitForContainerToComplete
(
RMAppAttempt
attempt
,
NMContainerStatus
completedContainer
)
throws
InterruptedException
{
drainEventsImplicitly
(
)
;
int
timeWaiting
=
0
;
while
(
timeWaiting
<
TIMEOUT_MS_FOR_CONTAINER_AND_NODE
)
{
List
<
ContainerStatus
>
containers
=
attempt
.
getJustFinishedContainers
(
)
;
nm
.
nodeHeartbeat
(
true
)
;
}
drainEventsImplicitly
(
)
;
container
=
getResourceScheduler
(
)
.
getRMContainer
(
containerId
)
;
LOG
.
info
(
+
containerId
+
+
containerState
+
)
;
Thread
.
sleep
(
WAIT_MS_PER_LOOP
)
;
timeWaiting
+=
WAIT_MS_PER_LOOP
;
}
while
(
!
containerState
.
equals
(
container
.
getState
(
)
)
)
{
if
(
timeWaiting
>=
timeoutMsecs
)
{
return
false
;
}
LOG
.
info
(
+
containerId
+
+
container
.
getState
(
)
+
+
containerState
)
;
for
(
MockNM
nm
:
nms
)
{
nm
.
nodeHeartbeat
(
true
)
;
}
drainEventsImplicitly
(
)
;
Thread
.
sleep
(
WAIT_MS_PER_LOOP
)
;
timeWaiting
+=
WAIT_MS_PER_LOOP
;
@
SuppressWarnings
(
)
private
static
void
waitForSchedulerAppAttemptAdded
(
ApplicationAttemptId
attemptId
,
MockRM
rm
)
throws
InterruptedException
{
int
tick
=
0
;
rm
.
drainEventsImplicitly
(
)
;
while
(
null
==
(
(
AbstractYarnScheduler
)
rm
.
getResourceScheduler
(
)
)
.
getApplicationAttempt
(
attemptId
)
&&
tick
<
50
)
{
Thread
.
sleep
(
100
)
;
if
(
tick
%
10
==
0
)
{
public
static
MockAM
launchAM
(
RMApp
app
,
MockRM
rm
,
MockNM
nm
)
throws
Exception
{
rm
.
drainEventsImplicitly
(
)
;
RMAppAttempt
attempt
=
waitForAttemptScheduled
(
app
,
rm
)
;
public
static
MockAM
launchUAM
(
RMApp
app
,
MockRM
rm
,
MockNM
nm
)
throws
Exception
{
rm
.
drainEventsImplicitly
(
)
;
rm
.
waitForState
(
app
.
getApplicationId
(
)
,
RMAppState
.
ACCEPTED
)
;
RMAppAttempt
attempt
=
app
.
getCurrentAppAttempt
(
)
;
waitForSchedulerAppAttemptAdded
(
attempt
.
getAppAttemptId
(
)
,
rm
)
;
throw
RPCUtil
.
getRemoteException
(
e
)
;
}
ContainerId
containerID
=
tokenId
.
getContainerID
(
)
;
ApplicationId
applicationId
=
containerID
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
List
<
Container
>
applicationContainers
=
containers
.
get
(
applicationId
)
;
if
(
applicationContainers
==
null
)
{
applicationContainers
=
new
ArrayList
<
Container
>
(
)
;
containers
.
put
(
applicationId
,
applicationContainers
)
;
}
for
(
Container
container
:
applicationContainers
)
{
if
(
container
.
getId
(
)
.
compareTo
(
containerID
)
==
0
)
{
throw
new
IllegalStateException
(
+
containerID
+
+
containerManagerAddress
)
;
}
}
Container
container
=
BuilderUtils
.
newContainer
(
containerID
,
this
.
nodeId
,
nodeHttpAddress
,
tokenId
.
getResource
(
)
,
null
,
null
)
;
ContainerStatus
containerStatus
=
BuilderUtils
.
newContainerStatus
(
container
.
getId
(
)
,
ContainerState
.
NEW
,
,
-
1000
,
container
.
getResource
(
)
)
;
applicationContainers
.
add
(
container
)
;
containerStatusMap
.
put
(
container
,
containerStatus
)
;
Resources
.
subtractFrom
(
available
,
tokenId
.
getResource
(
)
)
;
}
}
try
{
heartbeat
(
)
;
}
catch
(
IOException
ioe
)
{
throw
RPCUtil
.
getRemoteException
(
ioe
)
;
}
int
ctr
=
0
;
Container
container
=
null
;
for
(
Iterator
<
Container
>
i
=
applicationContainers
.
iterator
(
)
;
i
.
hasNext
(
)
;
)
{
container
=
i
.
next
(
)
;
if
(
container
.
getId
(
)
.
compareTo
(
containerID
)
==
0
)
{
i
.
remove
(
)
;
++
ctr
;
}
}
if
(
ctr
!=
1
)
{
throw
new
IllegalStateException
(
+
containerID
+
+
ctr
+
)
;
}
Resources
.
addTo
(
available
,
container
.
getResource
(
)
)
;
Resources
.
subtractFrom
(
used
,
container
.
getResource
(
)
)
;
private
void
handleAdministerException
(
Exception
e
,
String
user
,
String
queue
,
String
operation
)
{
private
void
waitForLaunchedState
(
RMAppAttempt
attempt
)
throws
InterruptedException
{
int
waitCount
=
0
;
while
(
attempt
.
getAppAttemptState
(
)
!=
RMAppAttemptState
.
LAUNCHED
&&
waitCount
++
<
40
)
{
Container
mockContainer
=
mock
(
Container
.
class
)
;
NodeId
mockNodeId
=
mock
(
NodeId
.
class
)
;
String
host
=
;
when
(
mockNodeId
.
getHost
(
)
)
.
thenReturn
(
host
)
;
when
(
mockContainer
.
getNodeId
(
)
)
.
thenReturn
(
mockNodeId
)
;
when
(
mockRMAppAttempt
.
getMasterContainer
(
)
)
.
thenReturn
(
mockContainer
)
;
when
(
app
.
getCurrentAppAttempt
(
)
)
.
thenReturn
(
mockRMAppAttempt
)
;
Map
<
String
,
Long
>
resourceSecondsMap
=
new
HashMap
<
>
(
)
;
resourceSecondsMap
.
put
(
ResourceInformation
.
MEMORY_MB
.
getName
(
)
,
16384L
)
;
resourceSecondsMap
.
put
(
ResourceInformation
.
VCORES
.
getName
(
)
,
64L
)
;
RMAppMetrics
metrics
=
new
RMAppMetrics
(
Resource
.
newInstance
(
1234
,
56
)
,
10
,
1
,
resourceSecondsMap
,
new
HashMap
<
>
(
)
,
1234
)
;
when
(
app
.
getRMAppMetrics
(
)
)
.
thenReturn
(
metrics
)
;
when
(
app
.
getDiagnostics
(
)
)
.
thenReturn
(
new
StringBuilder
(
)
)
;
RMAppManager
.
ApplicationSummary
.
SummaryBuilder
summary
=
new
RMAppManager
.
ApplicationSummary
(
)
.
createAppSummary
(
app
)
;
String
msg
=
summary
.
toString
(
)
;
final
GetApplicationReportRequest
appReportRequest
=
recordFactory
.
newRecordInstance
(
GetApplicationReportRequest
.
class
)
;
appReportRequest
.
setApplicationId
(
applicationId
)
;
final
KillApplicationRequest
finishAppRequest
=
recordFactory
.
newRecordInstance
(
KillApplicationRequest
.
class
)
;
finishAppRequest
.
setApplicationId
(
applicationId
)
;
ApplicationClientProtocol
enemyRmClient
=
getRMClientForUser
(
ENEMY
)
;
ApplicationReport
appReport
=
enemyRmClient
.
getApplicationReport
(
appReportRequest
)
.
getApplicationReport
(
)
;
verifyEnemyAppReport
(
appReport
)
;
List
<
ApplicationReport
>
appReports
=
enemyRmClient
.
getApplications
(
recordFactory
.
newRecordInstance
(
GetApplicationsRequest
.
class
)
)
.
getApplicationList
(
)
;
Assert
.
assertEquals
(
,
4
,
appReports
.
size
(
)
)
;
for
(
ApplicationReport
report
:
appReports
)
{
verifyEnemyAppReport
(
report
)
;
}
try
{
enemyRmClient
.
forceKillApplication
(
finishAppRequest
)
;
Assert
.
fail
(
)
;
}
catch
(
YarnException
e
)
{
MockRM
rm
=
new
MockRM
(
)
;
rm
.
start
(
)
;
MockNM
nm1
=
rm
.
registerNode
(
,
5000
)
;
RMApp
app
=
MockRMAppSubmitter
.
submitWithMemory
(
2000
,
rm
)
;
nm1
.
nodeHeartbeat
(
true
)
;
RMAppAttempt
attempt
=
app
.
getCurrentAppAttempt
(
)
;
MockAM
am
=
rm
.
sendAMLaunched
(
attempt
.
getAppAttemptId
(
)
)
;
am
.
registerAppAttempt
(
)
;
int
request
=
2
;
am
.
allocate
(
,
1000
,
request
,
new
ArrayList
<
ContainerId
>
(
)
)
;
nm1
.
nodeHeartbeat
(
true
)
;
List
<
Container
>
conts
=
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
;
int
contReceived
=
conts
.
size
(
)
;
int
waitCount
=
0
;
while
(
contReceived
<
request
&&
waitCount
++
<
200
)
{
Thread
.
sleep
(
100
)
;
conts
=
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
;
contReceived
+=
conts
.
size
(
)
;
nm1
.
nodeHeartbeat
(
true
)
;
}
Assert
.
assertEquals
(
request
,
contReceived
)
;
am
.
unregisterAppAttempt
(
)
;
NodeHeartbeatResponse
resp
=
nm1
.
nodeHeartbeat
(
attempt
.
getAppAttemptId
(
)
,
1
,
ContainerState
.
COMPLETE
)
;
rm
.
waitForState
(
am
.
getApplicationAttemptId
(
)
,
RMAppAttemptState
.
FINISHED
)
;
resp
=
nm1
.
nodeHeartbeat
(
true
)
;
List
<
ContainerId
>
containersToCleanup
=
resp
.
getContainersToCleanup
(
)
;
List
<
ApplicationId
>
appsToCleanup
=
resp
.
getApplicationsToCleanup
(
)
;
int
numCleanedContainers
=
containersToCleanup
.
size
(
)
;
int
numCleanedApps
=
appsToCleanup
.
size
(
)
;
waitCount
=
0
;
while
(
(
numCleanedContainers
<
2
||
numCleanedApps
<
1
)
&&
waitCount
++
<
200
)
{
rm
.
start
(
)
;
MockNM
nm1
=
rm
.
registerNode
(
,
5000
)
;
RMApp
app
=
MockRMAppSubmitter
.
submitWithMemory
(
2000
,
rm
)
;
nm1
.
nodeHeartbeat
(
true
)
;
RMAppAttempt
attempt
=
app
.
getCurrentAppAttempt
(
)
;
MockAM
am
=
rm
.
sendAMLaunched
(
attempt
.
getAppAttemptId
(
)
)
;
am
.
registerAppAttempt
(
)
;
int
request
=
2
;
am
.
allocate
(
,
1000
,
request
,
new
ArrayList
<
ContainerId
>
(
)
)
;
rm
.
drainEvents
(
)
;
nm1
.
nodeHeartbeat
(
true
)
;
List
<
Container
>
conts
=
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
;
int
contReceived
=
conts
.
size
(
)
;
int
waitCount
=
0
;
while
(
contReceived
<
request
&&
waitCount
++
<
200
)
{
protected
void
waitForContainerCleanup
(
MockRM
rm
,
MockNM
nm
,
NodeHeartbeatResponse
resp
)
throws
Exception
{
int
waitCount
=
0
,
cleanedConts
=
0
;
List
<
ContainerId
>
contsToClean
;
do
{
rm
.
drainEvents
(
)
;
contsToClean
=
resp
.
getContainersToCleanup
(
)
;
cleanedConts
+=
contsToClean
.
size
(
)
;
if
(
cleanedConts
>=
1
)
{
break
;
}
Thread
.
sleep
(
100
)
;
resp
=
nm
.
nodeHeartbeat
(
true
)
;
}
while
(
waitCount
++
<
200
)
;
if
(
contsToClean
.
isEmpty
(
)
)
{
LOG
.
error
(
)
;
}
else
{
protected
ClientRMService
createClientRMService
(
)
{
return
new
ClientRMService
(
this
.
rmContext
,
scheduler
,
this
.
rmAppManager
,
this
.
applicationACLsManager
,
this
.
queueACLsManager
,
this
.
getRMContext
(
)
.
getRMDelegationTokenSecretManager
(
)
)
;
}
}
;
rm
.
start
(
)
;
int
nodeMemory
=
1024
;
MockNM
nm1
=
rm
.
registerNode
(
,
nodeMemory
)
;
rm
.
sendNodeStarted
(
nm1
)
;
nm1
.
nodeHeartbeat
(
true
)
;
rm
.
waitForState
(
nm1
.
getNodeId
(
)
,
NodeState
.
RUNNING
)
;
Integer
decommissioningTimeout
=
600
;
rm
.
sendNodeGracefulDecommission
(
nm1
,
decommissioningTimeout
)
;
rm
.
waitForState
(
nm1
.
getNodeId
(
)
,
NodeState
.
DECOMMISSIONING
)
;
Configuration
conf
=
new
Configuration
(
)
;
YarnRPC
rpc
=
YarnRPC
.
create
(
conf
)
;
InetSocketAddress
rmAddress
=
rm
.
getClientRMService
(
)
.
getBindAddress
(
)
;
}
}
;
rm
.
start
(
)
;
NodeLabel
labelX
=
NodeLabel
.
newInstance
(
,
false
)
;
NodeLabel
labelY
=
NodeLabel
.
newInstance
(
)
;
RMNodeLabelsManager
labelsMgr
=
rm
.
getRMContext
(
)
.
getNodeLabelManager
(
)
;
labelsMgr
.
addToCluserNodeLabels
(
ImmutableSet
.
of
(
labelX
,
labelY
)
)
;
NodeId
node1
=
NodeId
.
newInstance
(
,
1234
)
;
NodeId
node2
=
NodeId
.
newInstance
(
,
1234
)
;
Map
<
NodeId
,
Set
<
String
>>
map
=
new
HashMap
<
NodeId
,
Set
<
String
>>
(
)
;
map
.
put
(
node1
,
ImmutableSet
.
of
(
)
)
;
map
.
put
(
node2
,
ImmutableSet
.
of
(
)
)
;
labelsMgr
.
replaceLabelsOnNode
(
map
)
;
Configuration
conf
=
new
Configuration
(
)
;
YarnRPC
rpc
=
YarnRPC
.
create
(
conf
)
;
InetSocketAddress
rmAddress
=
rm
.
getClientRMService
(
)
.
getBindAddress
(
)
;
}
}
;
rm
.
start
(
)
;
NodeAttributesManager
mgr
=
rm
.
getRMContext
(
)
.
getNodeAttributesManager
(
)
;
NodeId
host1
=
NodeId
.
newInstance
(
,
0
)
;
NodeId
host2
=
NodeId
.
newInstance
(
,
0
)
;
NodeAttribute
gpu
=
NodeAttribute
.
newInstance
(
NodeAttribute
.
PREFIX_CENTRALIZED
,
,
NodeAttributeType
.
STRING
,
)
;
NodeAttribute
os
=
NodeAttribute
.
newInstance
(
NodeAttribute
.
PREFIX_CENTRALIZED
,
,
NodeAttributeType
.
STRING
,
)
;
NodeAttribute
docker
=
NodeAttribute
.
newInstance
(
NodeAttribute
.
PREFIX_DISTRIBUTED
,
,
NodeAttributeType
.
STRING
,
)
;
Map
<
String
,
Set
<
NodeAttribute
>>
nodes
=
new
HashMap
<
>
(
)
;
nodes
.
put
(
host1
.
getHost
(
)
,
ImmutableSet
.
of
(
gpu
,
os
)
)
;
nodes
.
put
(
host2
.
getHost
(
)
,
ImmutableSet
.
of
(
docker
)
)
;
mgr
.
addNodeAttributes
(
nodes
)
;
Configuration
conf
=
new
Configuration
(
)
;
YarnRPC
rpc
=
YarnRPC
.
create
(
conf
)
;
InetSocketAddress
rmAddress
=
rm
.
getClientRMService
(
)
.
getBindAddress
(
)
;
yarnConf
.
setClass
(
CapacitySchedulerConfiguration
.
RESOURCE_CALCULATOR_CLASS
,
DominantResourceCalculator
.
class
,
ResourceCalculator
.
class
)
;
MockRM
rm
=
new
MockRM
(
yarnConf
)
{
protected
ClientRMService
createClientRMService
(
)
{
return
new
ClientRMService
(
this
.
rmContext
,
scheduler
,
this
.
rmAppManager
,
this
.
applicationACLsManager
,
this
.
queueACLsManager
,
this
.
getRMContext
(
)
.
getRMDelegationTokenSecretManager
(
)
)
;
}
}
;
rm
.
start
(
)
;
Resource
resource
=
BuilderUtils
.
newResource
(
1024
,
1
)
;
resource
.
setResourceInformation
(
,
ResourceInformation
.
newInstance
(
,
,
1024
)
)
;
resource
.
setResourceInformation
(
,
ResourceInformation
.
newInstance
(
,
,
1
)
)
;
resource
.
setResourceInformation
(
,
ResourceInformation
.
newInstance
(
,
,
1
)
)
;
MockNM
node
=
rm
.
registerNode
(
,
resource
)
;
node
.
nodeHeartbeat
(
true
)
;
Configuration
conf
=
new
Configuration
(
)
;
YarnRPC
rpc
=
YarnRPC
.
create
(
conf
)
;
InetSocketAddress
rmAddress
=
rm
.
getClientRMService
(
)
.
getBindAddress
(
)
;
@
Test
public
void
testDelegationToken
(
)
throws
IOException
,
InterruptedException
{
final
YarnConfiguration
conf
=
new
YarnConfiguration
(
)
;
conf
.
set
(
YarnConfiguration
.
RM_PRINCIPAL
,
)
;
conf
.
set
(
CommonConfigurationKeysPublic
.
HADOOP_SECURITY_AUTHENTICATION
,
)
;
UserGroupInformation
.
setConfiguration
(
conf
)
;
ResourceScheduler
scheduler
=
createMockScheduler
(
conf
)
;
long
initialInterval
=
10000l
;
long
maxLifetime
=
20000l
;
long
renewInterval
=
10000l
;
RMDelegationTokenSecretManager
rmDtSecretManager
=
createRMDelegationTokenSecretManager
(
initialInterval
,
maxLifetime
,
renewInterval
)
;
rmDtSecretManager
.
startThreads
(
)
;
long
maxLifetime
=
20000l
;
long
renewInterval
=
10000l
;
RMDelegationTokenSecretManager
rmDtSecretManager
=
createRMDelegationTokenSecretManager
(
initialInterval
,
maxLifetime
,
renewInterval
)
;
rmDtSecretManager
.
startThreads
(
)
;
LOG
.
info
(
+
initialInterval
+
+
maxLifetime
+
+
renewInterval
)
;
final
ClientRMService
clientRMService
=
new
ClientRMServiceForTest
(
conf
,
scheduler
,
rmDtSecretManager
)
;
clientRMService
.
init
(
conf
)
;
clientRMService
.
start
(
)
;
ApplicationClientProtocol
clientRMWithDT
=
null
;
try
{
UserGroupInformation
loggedInUser
=
UserGroupInformation
.
createRemoteUser
(
)
;
Assert
.
assertEquals
(
,
loggedInUser
.
getShortUserName
(
)
)
;
loggedInUser
.
setAuthenticationMethod
(
AuthenticationMethod
.
KERBEROS
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
token
=
getDelegationToken
(
loggedInUser
,
clientRMService
,
loggedInUser
.
getShortUserName
(
)
)
;
long
tokenFetchTime
=
System
.
currentTimeMillis
(
)
;
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
token
=
getDelegationToken
(
loggedInUser
,
clientRMService
,
loggedInUser
.
getShortUserName
(
)
)
;
long
tokenFetchTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
tokenFetchTime
)
;
clientRMWithDT
=
getClientRMProtocolWithDT
(
token
,
clientRMService
.
getBindAddress
(
)
,
,
conf
)
;
GetNewApplicationRequest
request
=
Records
.
newRecord
(
GetNewApplicationRequest
.
class
)
;
try
{
clientRMWithDT
.
getNewApplication
(
request
)
;
}
catch
(
IOException
e
)
{
fail
(
+
e
)
;
}
catch
(
YarnException
e
)
{
fail
(
+
e
)
;
}
while
(
System
.
currentTimeMillis
(
)
<
tokenFetchTime
+
initialInterval
/
2
)
{
Thread
.
sleep
(
500l
)
;
}
long
nextExpTime
=
renewDelegationToken
(
loggedInUser
,
clientRMService
,
token
)
;
long
renewalTime
=
System
.
currentTimeMillis
(
)
;
}
while
(
System
.
currentTimeMillis
(
)
<
tokenFetchTime
+
initialInterval
/
2
)
{
Thread
.
sleep
(
500l
)
;
}
long
nextExpTime
=
renewDelegationToken
(
loggedInUser
,
clientRMService
,
token
)
;
long
renewalTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
renewalTime
+
+
nextExpTime
)
;
while
(
System
.
currentTimeMillis
(
)
>
tokenFetchTime
+
initialInterval
&&
System
.
currentTimeMillis
(
)
<
nextExpTime
)
{
Thread
.
sleep
(
500l
)
;
}
Thread
.
sleep
(
50l
)
;
try
{
clientRMWithDT
.
getNewApplication
(
request
)
;
}
catch
(
IOException
e
)
{
fail
(
+
e
)
;
}
catch
(
YarnException
e
)
{
fail
(
+
e
)
;
}
while
(
System
.
currentTimeMillis
(
)
<
renewalTime
+
renewInterval
)
{
clientRMWithDT
.
getNewApplication
(
request
)
;
fail
(
)
;
}
catch
(
Exception
e
)
{
assertEquals
(
InvalidToken
.
class
.
getName
(
)
,
e
.
getClass
(
)
.
getName
(
)
)
;
assertTrue
(
e
.
getMessage
(
)
.
contains
(
)
)
;
}
if
(
clientRMWithDT
!=
null
)
{
RPC
.
stopProxy
(
clientRMWithDT
)
;
clientRMWithDT
=
null
;
}
token
=
getDelegationToken
(
loggedInUser
,
clientRMService
,
loggedInUser
.
getShortUserName
(
)
)
;
tokenFetchTime
=
System
.
currentTimeMillis
(
)
;
LOG
.
info
(
+
tokenFetchTime
)
;
clientRMWithDT
=
getClientRMProtocolWithDT
(
token
,
clientRMService
.
getBindAddress
(
)
,
,
conf
)
;
request
=
Records
.
newRecord
(
GetNewApplicationRequest
.
class
)
;
try
{
clientRMWithDT
.
getNewApplication
(
request
)
;
MockNM
nm1
=
rm
.
registerNode
(
,
5120
)
;
MockNM
nm2
=
rm
.
registerNode
(
,
10240
)
;
RMApp
app
=
MockRMAppSubmitter
.
submitWithMemory
(
2000
,
rm
)
;
nm1
.
nodeHeartbeat
(
true
)
;
RMAppAttempt
attempt
=
app
.
getCurrentAppAttempt
(
)
;
MockAM
am
=
rm
.
sendAMLaunched
(
attempt
.
getAppAttemptId
(
)
)
;
am
.
registerAppAttempt
(
)
;
int
request
=
13
;
am
.
allocate
(
,
1000
,
request
,
new
ArrayList
<
ContainerId
>
(
)
)
;
List
<
Container
>
conts
=
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
;
int
contReceived
=
conts
.
size
(
)
;
while
(
contReceived
<
3
)
{
nm1
.
nodeHeartbeat
(
true
)
;
conts
.
addAll
(
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
)
;
contReceived
=
conts
.
size
(
)
;
List
<
Container
>
conts
=
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
;
int
contReceived
=
conts
.
size
(
)
;
while
(
contReceived
<
3
)
{
nm1
.
nodeHeartbeat
(
true
)
;
conts
.
addAll
(
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
)
;
contReceived
=
conts
.
size
(
)
;
LOG
.
info
(
+
contReceived
+
+
3
)
;
Thread
.
sleep
(
WAIT_SLEEP_MS
)
;
}
Assert
.
assertEquals
(
3
,
conts
.
size
(
)
)
;
conts
=
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
;
contReceived
=
conts
.
size
(
)
;
while
(
contReceived
<
10
)
{
nm2
.
nodeHeartbeat
(
true
)
;
conts
.
addAll
(
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
)
;
contReceived
=
conts
.
size
(
)
;
allocateContainersAndValidateNMTokens
(
am
,
containersReceivedForNM1
,
4
,
nmTokens
,
nm1
)
;
Assert
.
assertEquals
(
1
,
nmTokens
.
size
(
)
)
;
MockNM
nm2
=
rm
.
registerNode
(
,
10000
)
;
nm2
.
nodeHeartbeat
(
true
)
;
ArrayList
<
Container
>
containersReceivedForNM2
=
new
ArrayList
<
Container
>
(
)
;
response
=
am
.
allocate
(
,
1000
,
2
,
releaseContainerList
)
;
Assert
.
assertEquals
(
0
,
response
.
getAllocatedContainers
(
)
.
size
(
)
)
;
allocateContainersAndValidateNMTokens
(
am
,
containersReceivedForNM2
,
2
,
nmTokens
,
nm2
)
;
Assert
.
assertEquals
(
2
,
nmTokens
.
size
(
)
)
;
nm2
=
rm
.
registerNode
(
,
10000
)
;
Map
<
NodeId
,
RMNode
>
nodes
=
rm
.
getRMContext
(
)
.
getRMNodes
(
)
;
while
(
nodes
.
get
(
nm2
.
getNodeId
(
)
)
.
getLastNodeHeartBeatResponse
(
)
.
getResponseId
(
)
>
0
)
{
Thread
.
sleep
(
WAIT_SLEEP_MS
)
;
}
int
interval
=
40
;
while
(
nmTokenSecretManager
.
isApplicationAttemptNMTokenPresent
(
attempt
.
getAppAttemptId
(
)
,
nm2
.
getNodeId
(
)
)
&&
interval
--
>
0
)
{
@
Test
(
timeout
=
60000
)
public
void
testRMRestartWaitForPreviousSucceededAttempt
(
)
throws
Exception
{
conf
.
setInt
(
YarnConfiguration
.
RM_AM_MAX_ATTEMPTS
,
2
)
;
MemoryRMStateStore
memStore
=
new
MockMemoryRMStateStore
(
)
{
int
count
=
0
;
@
Override
public
void
updateApplicationStateInternal
(
ApplicationId
appId
,
ApplicationStateData
appStateData
)
throws
Exception
{
if
(
count
==
1
)
{
@
Test
public
void
testFailoverAndSubmitReservation
(
)
throws
Exception
{
startRMs
(
)
;
addNodeCapacityToPlan
(
rm1
,
102400
,
100
)
;
explicitFailover
(
)
;
addNodeCapacityToPlan
(
rm2
,
102400
,
100
)
;
ClientRMService
clientService
=
rm2
.
getClientRMService
(
)
;
ReservationId
reservationID
=
getNewReservation
(
clientService
)
.
getReservationId
(
)
;
ReservationSubmissionRequest
request
=
createReservationSubmissionRequest
(
reservationID
)
;
ReservationSubmissionResponse
response
=
null
;
try
{
response
=
clientService
.
submitReservation
(
request
)
;
}
catch
(
Exception
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
Assert
.
assertNotNull
(
response
)
;
Assert
.
assertNotNull
(
reservationID
)
;
try
{
response
=
clientService
.
submitReservation
(
request
)
;
}
catch
(
Exception
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
Assert
.
assertNotNull
(
response
)
;
Assert
.
assertNotNull
(
resID1
)
;
LOG
.
info
(
+
resID1
)
;
ReservationId
resID2
=
getNewReservation
(
clientService
)
.
getReservationId
(
)
;
request
.
setReservationId
(
resID2
)
;
try
{
response
=
clientService
.
submitReservation
(
request
)
;
}
catch
(
Exception
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
Assert
.
assertNotNull
(
response
)
;
Assert
.
assertNotNull
(
resID2
)
;
ReservationId
resID2
=
getNewReservation
(
clientService
)
.
getReservationId
(
)
;
request
.
setReservationId
(
resID2
)
;
try
{
response
=
clientService
.
submitReservation
(
request
)
;
}
catch
(
Exception
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
}
Assert
.
assertNotNull
(
response
)
;
Assert
.
assertNotNull
(
resID2
)
;
LOG
.
info
(
+
resID2
)
;
ReservationId
resID3
=
getNewReservation
(
clientService
)
.
getReservationId
(
)
;
request
.
setReservationId
(
resID3
)
;
try
{
response
=
clientService
.
submitReservation
(
request
)
;
}
catch
(
Exception
e
)
{
Assert
.
fail
(
e
.
getMessage
(
)
)
;
if
(
rm
.
getResourceScheduler
(
)
.
getClass
(
)
==
FairScheduler
.
class
)
{
fs
=
(
FairScheduler
)
rm
.
getResourceScheduler
(
)
;
}
rm
.
start
(
)
;
MockNM
nm1
=
rm
.
registerNode
(
,
5000
)
;
RMApp
app
=
MockRMAppSubmitter
.
submitWithMemory
(
2000
,
rm
)
;
nm1
.
nodeHeartbeat
(
true
)
;
RMAppAttempt
attempt
=
app
.
getCurrentAppAttempt
(
)
;
MockAM
am
=
rm
.
sendAMLaunched
(
attempt
.
getAppAttemptId
(
)
)
;
am
.
registerAppAttempt
(
)
;
final
int
request
=
2
;
am
.
allocate
(
,
1000
,
request
,
new
ArrayList
<
ContainerId
>
(
)
)
;
nm1
.
nodeHeartbeat
(
true
)
;
List
<
Container
>
conts
=
new
ArrayList
<
>
(
request
)
;
int
waitCount
=
0
;
while
(
conts
.
size
(
)
<
request
&&
waitCount
++
<
200
)
{
int
waitCount
=
0
;
while
(
conts
.
size
(
)
<
request
&&
waitCount
++
<
200
)
{
LOG
.
info
(
+
conts
.
size
(
)
+
+
request
)
;
Thread
.
sleep
(
100
)
;
List
<
Container
>
allocation
=
am
.
allocate
(
new
ArrayList
<
ResourceRequest
>
(
)
,
new
ArrayList
<
ContainerId
>
(
)
)
.
getAllocatedContainers
(
)
;
conts
.
addAll
(
allocation
)
;
if
(
fs
!=
null
)
{
nm1
.
nodeHeartbeat
(
true
)
;
}
}
Assert
.
assertEquals
(
request
,
conts
.
size
(
)
)
;
for
(
Container
container
:
conts
)
{
rm
.
signalToContainer
(
container
.
getId
(
)
,
SignalContainerCommand
.
OUTPUT_THREAD_DUMP
)
;
}
NodeHeartbeatResponse
resp
;
List
<
SignalContainerRequest
>
contsToSignal
;
int
signaledConts
=
0
;
waitCount
=
0
;
doAnswer
(
new
Answer
<
Integer
>
(
)
{
@
Override
public
Integer
answer
(
InvocationOnMock
invocation
)
throws
Throwable
{
return
cId
.
compareTo
(
(
(
RMContainer
)
invocation
.
getArguments
(
)
[
0
]
)
.
getContainerId
(
)
)
;
}
}
)
.
when
(
rmc
)
.
compareTo
(
any
(
RMContainer
.
class
)
)
;
if
(
containerId
==
1
)
{
when
(
rmc
.
isAMContainer
(
)
)
.
thenReturn
(
true
)
;
when
(
app
.
getAMResource
(
label
)
)
.
thenReturn
(
res
)
;
when
(
app
.
getAppAMNodePartitionName
(
)
)
.
thenReturn
(
label
)
;
}
if
(
reserved
)
{
reservedContainers
.
add
(
rmc
)
;
when
(
rmc
.
getReservedResource
(
)
)
.
thenReturn
(
res
)
;
}
else
{
liveContainers
.
add
(
rmc
)
;
}
addContainerToSchedulerNode
(
host
,
rmc
,
reserved
)
;
String
partition
=
null
;
HashMap
<
String
,
HashMap
<
String
,
HashMap
<
String
,
ResourceUsage
>>>
userResourceUsagePerLabel
=
new
HashMap
<
>
(
)
;
LeafQueue
queue
=
null
;
int
mulp
=
-
1
;
for
(
String
a
:
appsConfig
.
split
(
)
)
{
String
[
]
strs
=
a
.
split
(
)
;
String
queueName
=
strs
[
0
]
;
if
(
mulp
<=
0
&&
strs
.
length
>
2
&&
strs
[
2
]
!=
null
)
{
mulp
=
100
/
(
new
Integer
(
strs
[
2
]
)
.
intValue
(
)
)
;
}
List
<
RMContainer
>
liveContainers
=
new
ArrayList
<
RMContainer
>
(
)
;
List
<
RMContainer
>
reservedContainers
=
new
ArrayList
<
RMContainer
>
(
)
;
ApplicationId
appId
=
ApplicationId
.
newInstance
(
0L
,
id
)
;
ApplicationAttemptId
appAttemptId
=
ApplicationAttemptId
.
newInstance
(
appId
,
1
)
;
FiCaSchedulerApp
app
=
mock
(
FiCaSchedulerApp
.
class
)
;
when
(
app
.
getAMResource
(
anyString
(
)
)
)
.
thenReturn
(
Resources
.
createResource
(
0
,
0
)
)
;
mockContainers
(
strs
[
1
]
,
app
,
appAttemptId
,
queueName
,
reservedContainers
,
liveContainers
)
;
userResourceUsage
.
put
(
app
.
getUser
(
)
,
usage
)
;
}
usage
.
incAMUsed
(
app
.
getAMResource
(
label
)
)
;
usage
.
incUsed
(
app
.
getAppAttemptResourceUsage
(
)
.
getUsed
(
label
)
)
;
id
++
;
}
for
(
String
label
:
userResourceUsagePerLabel
.
keySet
(
)
)
{
for
(
String
queueName
:
userMap
.
keySet
(
)
)
{
queue
=
(
LeafQueue
)
nameToCSQueues
.
get
(
queueName
)
;
Resource
totResoucePerPartition
=
partitionToResource
.
get
(
)
;
Resource
capacity
=
Resources
.
multiply
(
totResoucePerPartition
,
queue
.
getQueueCapacities
(
)
.
getAbsoluteCapacity
(
)
)
;
HashSet
<
String
>
users
=
userMap
.
get
(
queue
.
getQueueName
(
)
)
;
if
(
users
==
null
)
{
users
=
userMap
.
get
(
queue
.
getQueuePath
(
)
)
;
}
when
(
queue
.
getAllUsers
(
)
)
.
thenReturn
(
users
)
;
Resource
userLimit
;
if
(
mulp
>
0
)
{
FiCaSchedulerNode
sn
=
mock
(
FiCaSchedulerNode
.
class
)
;
when
(
sn
.
getNodeID
(
)
)
.
thenReturn
(
nodeId
)
;
when
(
sn
.
getPartition
(
)
)
.
thenReturn
(
partition
)
;
Resource
totalRes
=
Resources
.
createResource
(
0
)
;
if
(
arr
.
length
>
1
)
{
String
res
=
arr
[
1
]
;
if
(
res
.
contains
(
)
)
{
String
resSring
=
res
.
substring
(
res
.
indexOf
(
)
+
.
length
(
)
)
;
totalRes
=
parseResourceFromString
(
resSring
)
;
}
}
when
(
sn
.
getTotalResource
(
)
)
.
thenReturn
(
totalRes
)
;
when
(
sn
.
getUnallocatedResource
(
)
)
.
thenReturn
(
Resources
.
clone
(
totalRes
)
)
;
when
(
sn
.
getTotalKillableResources
(
)
)
.
thenReturn
(
Resources
.
none
(
)
)
;
List
<
RMContainer
>
liveContainers
=
new
ArrayList
<
>
(
)
;
when
(
sn
.
getCopiedListOfRunningContainers
(
)
)
.
thenReturn
(
liveContainers
)
;
nodeIdToSchedulerNodes
.
put
(
nodeId
,
sn
)
;
private
void
mockNodeLabelsManager
(
String
nodeLabelsConfigStr
)
throws
IOException
{
String
[
]
partitionConfigArr
=
nodeLabelsConfigStr
.
split
(
)
;
clusterResource
=
Resources
.
createResource
(
0
)
;
for
(
String
p
:
partitionConfigArr
)
{
String
partitionName
=
p
.
substring
(
0
,
p
.
indexOf
(
)
)
;
Resource
res
=
parseResourceFromString
(
p
.
substring
(
p
.
indexOf
(
)
+
1
,
p
.
indexOf
(
)
)
)
;
boolean
exclusivity
=
Boolean
.
valueOf
(
p
.
substring
(
p
.
indexOf
(
)
+
1
,
p
.
length
(
)
)
)
;
when
(
nlm
.
getResourceByLabel
(
eq
(
partitionName
)
,
any
(
Resource
.
class
)
)
)
.
thenReturn
(
res
)
;
when
(
nlm
.
isExclusiveNodeLabel
(
eq
(
partitionName
)
)
)
.
thenReturn
(
exclusivity
)
;
partitionToResource
.
put
(
partitionName
,
res
)
;
private
void
setupQueue
(
CSQueue
queue
,
String
q
,
String
[
]
queueExprArray
,
int
idx
)
{
queuePath
=
ROOT
;
}
String
queueName
=
getQueueName
(
q
)
;
when
(
queue
.
getQueueName
(
)
)
.
thenReturn
(
queueName
)
;
ParentQueue
parentQueue
=
getParentQueue
(
queueExprArray
,
idx
,
myLevel
)
;
if
(
null
!=
parentQueue
)
{
when
(
queue
.
getParent
(
)
)
.
thenReturn
(
parentQueue
)
;
parentQueue
.
getChildQueues
(
)
.
add
(
queue
)
;
queuePath
=
parentQueue
.
getQueuePath
(
)
+
+
queueName
;
}
when
(
queue
.
getQueuePath
(
)
)
.
thenReturn
(
queuePath
)
;
QueueCapacities
qc
=
new
QueueCapacities
(
0
==
myLevel
)
;
ResourceUsage
ru
=
new
ResourceUsage
(
)
;
QueueResourceQuotas
qr
=
new
QueueResourceQuotas
(
)
;
when
(
queue
.
getQueueCapacities
(
)
)
.
thenReturn
(
qc
)
;
when
(
queue
.
getQueueResourceUsage
(
)
)
.
thenReturn
(
ru
)
;
when
(
queue
.
getQueueResourceQuotas
(
)
)
.
thenReturn
(
qr
)
;
qr
.
setEffectiveMaxResource
(
parseResourceFromString
(
values
[
1
]
.
trim
(
)
)
)
;
qr
.
setEffectiveMinResource
(
parseResourceFromString
(
values
[
0
]
.
trim
(
)
)
)
;
qr
.
setEffectiveMaxResource
(
partitionName
,
parseResourceFromString
(
values
[
1
]
.
trim
(
)
)
)
;
qr
.
setEffectiveMinResource
(
partitionName
,
parseResourceFromString
(
values
[
0
]
.
trim
(
)
)
)
;
when
(
queue
.
getUsedCapacity
(
)
)
.
thenReturn
(
used
)
;
when
(
queue
.
getEffectiveCapacity
(
partitionName
)
)
.
thenReturn
(
parseResourceFromString
(
values
[
0
]
.
trim
(
)
)
)
;
when
(
queue
.
getEffectiveMaxCapacity
(
partitionName
)
)
.
thenReturn
(
parseResourceFromString
(
values
[
1
]
.
trim
(
)
)
)
;
ru
.
setPending
(
partitionName
,
pending
)
;
Resource
reserved
=
Resources
.
none
(
)
;
if
(
values
.
length
==
5
)
{
reserved
=
parseResourceFromString
(
values
[
4
]
.
trim
(
)
)
;
ru
.
setReserved
(
partitionName
,
reserved
)
;
}
if
(
!
isParent
(
queueExprArray
,
idx
)
)
{
LeafQueue
lq
=
(
LeafQueue
)
queue
;
when
(
lq
.
getTotalPendingResourcesConsideringUserLimit
(
isA
(
Resource
.
class
)
,
isA
(
String
.
class
)
,
eq
(
false
)
)
)
.
thenReturn
(
pending
)
;
@
Test
public
void
testPeriodicCapacity
(
)
{
int
[
]
alloc
=
{
10
,
7
,
5
,
2
,
0
}
;
long
[
]
timeSteps
=
{
0L
,
5L
,
10L
,
15L
,
19L
}
;
RLESparseResourceAllocation
rleSparseVector
=
ReservationSystemTestUtil
.
generateRLESparseResourceAllocation
(
alloc
,
timeSteps
)
;
PeriodicRLESparseResourceAllocation
periodicVector
=
new
PeriodicRLESparseResourceAllocation
(
rleSparseVector
,
20L
)
;
@
Test
public
void
testMaxPeriodicCapacity
(
)
{
int
[
]
alloc
=
{
2
,
5
,
7
,
10
,
3
,
4
,
6
,
8
}
;
long
[
]
timeSteps
=
{
0L
,
1L
,
2L
,
3L
,
4L
,
5L
,
6L
,
7L
}
;
RLESparseResourceAllocation
rleSparseVector
=
ReservationSystemTestUtil
.
generateRLESparseResourceAllocation
(
alloc
,
timeSteps
)
;
PeriodicRLESparseResourceAllocation
periodicVector
=
new
PeriodicRLESparseResourceAllocation
(
rleSparseVector
,
8L
)
;
@
Test
public
void
testPartialRemoval
(
)
{
ResourceCalculator
resCalc
=
new
DefaultResourceCalculator
(
)
;
RLESparseResourceAllocation
rleSparseVector
=
new
RLESparseResourceAllocation
(
resCalc
)
;
ReservationInterval
riAdd
=
new
ReservationInterval
(
10
,
20
)
;
Resource
rr
=
Resource
.
newInstance
(
1024
*
100
,
100
)
;
ReservationInterval
riAdd2
=
new
ReservationInterval
(
20
,
30
)
;
Resource
rr2
=
Resource
.
newInstance
(
1024
*
200
,
200
)
;
ReservationInterval
riRemove
=
new
ReservationInterval
(
12
,
25
)
;
@
Test
public
void
testPartialRemoval
(
)
{
ResourceCalculator
resCalc
=
new
DefaultResourceCalculator
(
)
;
RLESparseResourceAllocation
rleSparseVector
=
new
RLESparseResourceAllocation
(
resCalc
)
;
ReservationInterval
riAdd
=
new
ReservationInterval
(
10
,
20
)
;
Resource
rr
=
Resource
.
newInstance
(
1024
*
100
,
100
)
;
ReservationInterval
riAdd2
=
new
ReservationInterval
(
20
,
30
)
;
Resource
rr2
=
Resource
.
newInstance
(
1024
*
200
,
200
)
;
ReservationInterval
riRemove
=
new
ReservationInterval
(
12
,
25
)
;
LOG
.
info
(
rleSparseVector
.
toString
(
)
)
;
rleSparseVector
.
addInterval
(
riAdd
,
rr
)
;
rleSparseVector
.
addInterval
(
riAdd2
,
rr2
)
;
@
Test
public
void
testPartialRemoval
(
)
{
ResourceCalculator
resCalc
=
new
DefaultResourceCalculator
(
)
;
RLESparseResourceAllocation
rleSparseVector
=
new
RLESparseResourceAllocation
(
resCalc
)
;
ReservationInterval
riAdd
=
new
ReservationInterval
(
10
,
20
)
;
Resource
rr
=
Resource
.
newInstance
(
1024
*
100
,
100
)
;
ReservationInterval
riAdd2
=
new
ReservationInterval
(
20
,
30
)
;
Resource
rr2
=
Resource
.
newInstance
(
1024
*
200
,
200
)
;
ReservationInterval
riRemove
=
new
ReservationInterval
(
12
,
25
)
;
LOG
.
info
(
rleSparseVector
.
toString
(
)
)
;
rleSparseVector
.
addInterval
(
riAdd
,
rr
)
;
rleSparseVector
.
addInterval
(
riAdd2
,
rr2
)
;
LOG
.
info
(
rleSparseVector
.
toString
(
)
)
;
rleSparseVector
.
removeInterval
(
riRemove
,
rr
)
;
Resource
rr2
=
Resource
.
newInstance
(
1024
*
200
,
200
)
;
ReservationInterval
riRemove
=
new
ReservationInterval
(
12
,
25
)
;
LOG
.
info
(
rleSparseVector
.
toString
(
)
)
;
rleSparseVector
.
addInterval
(
riAdd
,
rr
)
;
rleSparseVector
.
addInterval
(
riAdd2
,
rr2
)
;
LOG
.
info
(
rleSparseVector
.
toString
(
)
)
;
rleSparseVector
.
removeInterval
(
riRemove
,
rr
)
;
LOG
.
info
(
rleSparseVector
.
toString
(
)
)
;
Assert
.
assertEquals
(
102400
,
rleSparseVector
.
getCapacityAtTime
(
10
)
.
getMemorySize
(
)
)
;
Assert
.
assertEquals
(
0
,
rleSparseVector
.
getCapacityAtTime
(
13
)
.
getMemorySize
(
)
)
;
Assert
.
assertEquals
(
0
,
rleSparseVector
.
getCapacityAtTime
(
19
)
.
getMemorySize
(
)
)
;
Assert
.
assertEquals
(
102400
,
rleSparseVector
.
getCapacityAtTime
(
21
)
.
getMemorySize
(
)
)
;
Assert
.
assertEquals
(
2
*
102400
,
rleSparseVector
.
getCapacityAtTime
(
26
)
.
getMemorySize
(
)
)
;
ReservationInterval
riRemove2
=
new
ReservationInterval
(
9
,
13
)
;
rleSparseVector
.
removeInterval
(
riRemove2
,
rr
)
;
@
Test
public
void
testZeroAllocation
(
)
{
ResourceCalculator
resCalc
=
new
DefaultResourceCalculator
(
)
;
RLESparseResourceAllocation
rleSparseVector
=
new
RLESparseResourceAllocation
(
resCalc
)
;
rleSparseVector
.
addInterval
(
new
ReservationInterval
(
0
,
Long
.
MAX_VALUE
)
,
Resource
.
newInstance
(
0
,
0
)
)
;
@
Test
public
void
testMaxPeriodicCapacity
(
)
{
long
[
]
timeSteps
=
{
0L
,
1L
,
2L
,
3L
,
4L
,
5L
,
6L
,
7L
}
;
int
[
]
alloc
=
{
2
,
5
,
7
,
10
,
3
,
4
,
6
,
8
}
;
RLESparseResourceAllocation
rleSparseVector
=
ReservationSystemTestUtil
.
generateRLESparseResourceAllocation
(
alloc
,
timeSteps
)
;
@
Test
public
void
testGetMinimumCapacityInInterval
(
)
{
long
[
]
timeSteps
=
{
0L
,
1L
,
2L
,
3L
,
4L
,
5L
,
6L
,
7L
}
;
int
[
]
alloc
=
{
2
,
5
,
7
,
10
,
3
,
4
,
0
,
8
}
;
RLESparseResourceAllocation
rleSparseVector
=
ReservationSystemTestUtil
.
generateRLESparseResourceAllocation
(
alloc
,
timeSteps
)
;
Assert
.
fail
(
)
;
}
catch
(
YarnException
e
)
{
Assert
.
assertNull
(
plan
)
;
String
message
=
e
.
getMessage
(
)
;
Assert
.
assertTrue
(
message
.
startsWith
(
)
)
;
LOG
.
info
(
message
)
;
}
request
=
createSimpleReservationSubmissionRequest
(
1
,
1
,
1
,
50
,
3
,
)
;
plan
=
null
;
try
{
plan
=
rrValidator
.
validateReservationSubmissionRequest
(
rSystem
,
request
,
ReservationSystemTestUtil
.
getNewReservationId
(
)
)
;
Assert
.
fail
(
)
;
}
catch
(
YarnException
e
)
{
Assert
.
assertNull
(
plan
)
;
String
message
=
e
.
getMessage
(
)
;
Assert
.
assertTrue
(
message
.
startsWith
(
)
)
;
Assert
.
fail
(
)
;
}
catch
(
YarnException
e
)
{
Assert
.
assertNull
(
plan
)
;
String
message
=
e
.
getMessage
(
)
;
Assert
.
assertTrue
(
message
.
startsWith
(
)
)
;
LOG
.
info
(
message
)
;
}
request
=
createSimpleReservationUpdateRequest
(
1
,
1
,
1
,
50
,
3
,
)
;
plan
=
null
;
try
{
plan
=
rrValidator
.
validateReservationUpdateRequest
(
rSystem
,
request
)
;
Assert
.
fail
(
)
;
}
catch
(
YarnException
e
)
{
Assert
.
assertNull
(
plan
)
;
String
message
=
e
.
getMessage
(
)
;
Assert
.
assertTrue
(
message
.
startsWith
(
)
)
;
@
Before
public
void
setup
(
)
throws
Exception
{
long
seed
=
rand
.
nextLong
(
)
;
rand
.
setSeed
(
seed
)
;
@
Before
public
void
setup
(
)
throws
Exception
{
long
seed
=
rand
.
nextLong
(
)
;
rand
.
setSeed
(
seed
)
;
@
Before
public
void
setup
(
)
throws
Exception
{
long
seed
=
rand
.
nextLong
(
)
;
rand
.
setSeed
(
seed
)
;
private
void
logAssertingMessage
(
MetricsSource
source
)
{
String
queueName
=
(
(
QueueMetrics
)
source
)
.
queueName
;
Map
<
String
,
QueueMetrics
>
users
=
(
(
QueueMetrics
)
source
)
.
users
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
logAssertingMessage
(
MetricsSource
source
)
{
String
queueName
=
(
(
QueueMetrics
)
source
)
.
queueName
;
Map
<
String
,
QueueMetrics
>
users
=
(
(
QueueMetrics
)
source
)
.
users
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
private
void
waitForLaunchedState
(
RMAppAttempt
attempt
)
throws
InterruptedException
{
int
waitCount
=
0
;
while
(
attempt
.
getAppAttemptState
(
)
!=
RMAppAttemptState
.
LAUNCHED
&&
waitCount
++
<
20
)
{
YarnConfiguration
conf
=
new
YarnConfiguration
(
)
;
CapacitySchedulerContext
csContext
=
mock
(
CapacitySchedulerContext
.
class
)
;
when
(
csContext
.
getConfiguration
(
)
)
.
thenReturn
(
csConf
)
;
when
(
csContext
.
getConf
(
)
)
.
thenReturn
(
conf
)
;
when
(
csContext
.
getMinimumResourceCapability
(
)
)
.
thenReturn
(
Resources
.
createResource
(
GB
,
1
)
)
;
when
(
csContext
.
getMaximumResourceCapability
(
)
)
.
thenReturn
(
Resources
.
createResource
(
16
*
GB
,
16
)
)
;
when
(
csContext
.
getResourceCalculator
(
)
)
.
thenReturn
(
resourceCalculator
)
;
when
(
csContext
.
getRMContext
(
)
)
.
thenReturn
(
rmContext
)
;
when
(
csContext
.
getPreemptionManager
(
)
)
.
thenReturn
(
new
PreemptionManager
(
)
)
;
Resource
clusterResource
=
Resources
.
createResource
(
100
*
16
*
GB
,
100
*
16
)
;
when
(
csContext
.
getClusterResource
(
)
)
.
thenReturn
(
clusterResource
)
;
CSQueueStore
queues
=
new
CSQueueStore
(
)
;
CSQueue
root
=
CapacitySchedulerQueueManager
.
parseQueue
(
csContext
,
csConf
,
null
,
,
queues
,
queues
,
TestUtils
.
spyHook
)
;
root
.
updateClusterResource
(
clusterResource
,
new
ResourceLimits
(
clusterResource
)
)
;
LeafQueue
queue
=
(
LeafQueue
)
queues
.
get
(
A
)
;
nodeUpdate
(
nm_0
)
;
nodeUpdate
(
nm_1
)
;
application_0
.
schedule
(
)
;
checkApplicationResourceUsage
(
1
*
GB
,
application_0
)
;
application_1
.
schedule
(
)
;
checkApplicationResourceUsage
(
3
*
GB
,
application_1
)
;
checkNodeResourceUsage
(
4
*
GB
,
nm_0
)
;
checkNodeResourceUsage
(
0
*
GB
,
nm_1
)
;
LOG
.
info
(
)
;
Task
task_1_1
=
new
Task
(
application_1
,
priority_0
,
new
String
[
]
{
ResourceRequest
.
ANY
}
)
;
application_1
.
addTask
(
task_1_1
)
;
application_1
.
schedule
(
)
;
Task
task_0_1
=
new
Task
(
application_0
,
priority_0
,
new
String
[
]
{
host_0
,
host_1
}
)
;
application_0
.
addTask
(
task_0_1
)
;
application_0
.
schedule
(
)
;
application_0
.
schedule
(
)
;
checkApplicationResourceUsage
(
1
*
GB
,
application_0
)
;
application_1
.
schedule
(
)
;
checkApplicationResourceUsage
(
3
*
GB
,
application_1
)
;
checkNodeResourceUsage
(
4
*
GB
,
nm_0
)
;
checkNodeResourceUsage
(
0
*
GB
,
nm_1
)
;
LOG
.
info
(
)
;
Task
task_1_1
=
new
Task
(
application_1
,
priority_0
,
new
String
[
]
{
ResourceRequest
.
ANY
}
)
;
application_1
.
addTask
(
task_1_1
)
;
application_1
.
schedule
(
)
;
Task
task_0_1
=
new
Task
(
application_0
,
priority_0
,
new
String
[
]
{
host_0
,
host_1
}
)
;
application_0
.
addTask
(
task_0_1
)
;
application_0
.
schedule
(
)
;
LOG
.
info
(
+
nm_0
.
getHostName
(
)
)
;
nodeUpdate
(
nm_0
)
;
conf
.
setClass
(
YarnConfiguration
.
RM_SCHEDULER
,
CapacityScheduler
.
class
,
ResourceScheduler
.
class
)
;
MyContainerManager
containerManager
=
new
MyContainerManager
(
)
;
final
MockRMWithAMS
rm
=
new
MockRMWithAMS
(
conf
,
containerManager
)
;
rm
.
start
(
)
;
MockNM
nm1
=
rm
.
registerNode
(
,
5120
)
;
Map
<
ApplicationAccessType
,
String
>
acls
=
new
HashMap
<
ApplicationAccessType
,
String
>
(
2
)
;
acls
.
put
(
ApplicationAccessType
.
VIEW_APP
,
)
;
MockRMAppSubmissionData
data
=
MockRMAppSubmissionData
.
Builder
.
createWithMemory
(
1024
,
rm
)
.
withAppName
(
)
.
withUser
(
)
.
withAcls
(
acls
)
.
build
(
)
;
RMApp
app
=
MockRMAppSubmitter
.
submit
(
rm
,
data
)
;
nm1
.
nodeHeartbeat
(
true
)
;
RMAppAttempt
attempt
=
app
.
getCurrentAppAttempt
(
)
;
ApplicationAttemptId
applicationAttemptId
=
attempt
.
getAppAttemptId
(
)
;
int
msecToWait
=
10000
;
int
msecToSleep
=
100
;
while
(
attempt
.
getAppAttemptState
(
)
!=
RMAppAttemptState
.
LAUNCHED
&&
msecToWait
>
0
)
{
String
b1QTobeDeleted
=
;
LeafQueue
csB1Queue
=
Mockito
.
spy
(
(
LeafQueue
)
queues
.
get
(
b1QTobeDeleted
)
)
;
when
(
csB1Queue
.
getState
(
)
)
.
thenReturn
(
QueueState
.
DRAINING
)
.
thenReturn
(
QueueState
.
STOPPED
)
;
cs
.
getCapacitySchedulerQueueManager
(
)
.
addQueue
(
b1QTobeDeleted
,
csB1Queue
)
;
conf
=
new
CapacitySchedulerConfiguration
(
)
;
setupQueueConfigurationWithOutB1
(
conf
)
;
try
{
cs
.
reinitialize
(
conf
,
mockContext
)
;
fail
(
+
)
;
}
catch
(
IOException
e
)
{
}
conf
=
new
CapacitySchedulerConfiguration
(
)
;
setupQueueConfigurationWithOutB1
(
conf
)
;
try
{
cs
.
reinitialize
(
conf
,
mockContext
)
;
}
catch
(
IOException
e
)
{
conf
.
setQueues
(
AGROUP
,
new
String
[
]
{
}
)
;
conf
.
setCapacity
(
AGROUP_A
,
100f
)
;
conf
.
setUserLimitFactor
(
AGROUP_A
,
100.0f
)
;
conf
.
setUserLimitFactor
(
C
,
1.0f
)
;
conf
.
setAutoCreateChildQueueEnabled
(
C
,
true
)
;
conf
.
setAutoCreatedLeafQueueConfigCapacity
(
C
,
50.0f
)
;
conf
.
setAutoCreatedLeafQueueConfigMaxCapacity
(
C
,
100.0f
)
;
conf
.
setAutoCreatedLeafQueueConfigUserLimit
(
C
,
100
)
;
conf
.
setAutoCreatedLeafQueueConfigUserLimitFactor
(
C
,
3.0f
)
;
conf
.
setAutoCreatedLeafQueueTemplateCapacityByLabel
(
C
,
NODEL_LABEL_GPU
,
NODE_LABEL_GPU_TEMPLATE_CAPACITY
)
;
conf
.
setAutoCreatedLeafQueueTemplateMaxCapacity
(
C
,
NODEL_LABEL_GPU
,
100.0f
)
;
conf
.
setAutoCreatedLeafQueueTemplateCapacityByLabel
(
C
,
NODEL_LABEL_SSD
,
NODEL_LABEL_SSD_TEMPLATE_CAPACITY
)
;
conf
.
setAutoCreatedLeafQueueTemplateMaxCapacity
(
C
,
NODEL_LABEL_SSD
,
100.0f
)
;
conf
.
setDefaultNodeLabelExpression
(
C
,
NODEL_LABEL_GPU
)
;
conf
.
setAutoCreatedLeafQueueConfigDefaultNodeLabelExpression
(
C
,
NODEL_LABEL_SSD
)
;
conf
.
setAutoCreatedLeafQueueConfigCapacity
(
D
,
10.0f
)
;
conf
.
setAutoCreatedLeafQueueConfigMaxCapacity
(
D
,
100.0f
)
;
conf
.
setAutoCreatedLeafQueueConfigUserLimit
(
D
,
3
)
;
conf
.
setAutoCreatedLeafQueueConfigUserLimitFactor
(
D
,
100
)
;
conf
.
set
(
CapacitySchedulerConfiguration
.
PREFIX
+
C
+
DOT
+
CapacitySchedulerConfiguration
.
AUTO_CREATED_LEAF_QUEUE_TEMPLATE_PREFIX
+
DOT
+
CapacitySchedulerConfiguration
.
ORDERING_POLICY
,
FAIR_APP_ORDERING_POLICY
)
;
accessibleNodeLabelsOnC
.
add
(
NODEL_LABEL_GPU
)
;
accessibleNodeLabelsOnC
.
add
(
NODEL_LABEL_SSD
)
;
accessibleNodeLabelsOnC
.
add
(
NO_LABEL
)
;
conf
.
setAccessibleNodeLabels
(
C
,
accessibleNodeLabelsOnC
)
;
conf
.
setAccessibleNodeLabels
(
ROOT
,
accessibleNodeLabelsOnC
)
;
conf
.
setCapacityByLabel
(
ROOT
,
NODEL_LABEL_GPU
,
100f
)
;
conf
.
setCapacityByLabel
(
ROOT
,
NODEL_LABEL_SSD
,
100f
)
;
conf
.
setAccessibleNodeLabels
(
C
,
accessibleNodeLabelsOnC
)
;
conf
.
setCapacityByLabel
(
C
,
NODEL_LABEL_GPU
,
100f
)
;
conf
.
setCapacityByLabel
(
C
,
NODEL_LABEL_SSD
,
100f
)
;
protected
void
validateDeactivatedQueueEntitlement
(
CSQueue
parentQueue
,
String
leafQueueName
,
Map
<
String
,
Float
>
expectedTotalChildQueueAbsCapacity
,
List
<
QueueManagementChange
>
queueManagementChanges
)
throws
SchedulerDynamicEditException
{
QueueEntitlement
expectedEntitlement
=
new
QueueEntitlement
(
0.0f
,
1.0f
)
;
ManagedParentQueue
autoCreateEnabledParentQueue
=
(
ManagedParentQueue
)
parentQueue
;
AutoCreatedLeafQueue
leafQueue
=
(
AutoCreatedLeafQueue
)
cs
.
getQueue
(
leafQueueName
)
;
GuaranteedOrZeroCapacityOverTimePolicy
policy
=
(
GuaranteedOrZeroCapacityOverTimePolicy
)
autoCreateEnabledParentQueue
.
getAutoCreatedQueueManagementPolicy
(
)
;
Map
<
String
,
QueueEntitlement
>
expectedEntitlements
=
new
HashMap
<
>
(
)
;
for
(
String
label
:
accessibleNodeLabelsOnC
)
{
@
Test
(
timeout
=
60000
)
public
void
testExcessReservationThanNodeManagerCapacity
(
)
throws
Exception
{
@
SuppressWarnings
(
)
MockRM
rm
=
new
MockRM
(
conf
)
;
rm
.
start
(
)
;
MockNM
nm1
=
rm
.
registerNode
(
,
2
*
GB
,
4
)
;
MockNM
nm2
=
rm
.
registerNode
(
,
3
*
GB
,
4
)
;
nm1
.
nodeHeartbeat
(
true
)
;
nm2
.
nodeHeartbeat
(
true
)
;
int
waitCount
=
20
;
int
size
=
rm
.
getRMContext
(
)
.
getRMNodes
(
)
.
size
(
)
;
while
(
(
size
=
rm
.
getRMContext
(
)
.
getRMNodes
(
)
.
size
(
)
)
!=
2
&&
waitCount
--
>
0
)
{
Resource
clusterResource
=
Resources
.
createResource
(
numNodes
*
(
8
*
GB
)
,
numNodes
*
100
)
;
when
(
csContext
.
getNumClusterNodes
(
)
)
.
thenReturn
(
numNodes
)
;
when
(
csContext
.
getClusterResource
(
)
)
.
thenReturn
(
clusterResource
)
;
root
.
updateClusterResource
(
clusterResource
,
new
ResourceLimits
(
clusterResource
)
)
;
Priority
priority
=
TestUtils
.
createMockPriority
(
1
)
;
app0
.
updateResourceRequests
(
Collections
.
singletonList
(
TestUtils
.
createResourceRequest
(
ResourceRequest
.
ANY
,
1
*
GB
,
40
,
10
,
true
,
priority
,
recordFactory
,
NO_LABEL
)
)
)
;
app2
.
updateResourceRequests
(
Collections
.
singletonList
(
TestUtils
.
createResourceRequest
(
ResourceRequest
.
ANY
,
2
*
GB
,
10
,
10
,
true
,
priority
,
recordFactory
,
NO_LABEL
)
)
)
;
b
.
setUserLimit
(
50
)
;
b
.
setUserLimitFactor
(
2
)
;
User
queueUser0
=
b
.
getUser
(
user0
)
;
User
queueUser1
=
b
.
getUser
(
user1
)
;
assertEquals
(
,
2
,
b
.
getAbstractUsersManager
(
)
.
getNumActiveUsers
(
)
)
;
CSAssignment
assign
;
do
{
assign
=
b
.
assignContainers
(
clusterResource
,
node0
,
new
ResourceLimits
(
clusterResource
)
,
SchedulingMode
.
RESPECT_PARTITION_EXCLUSIVITY
)
;
public
static
FiCaSchedulerNode
getMockNode
(
String
host
,
String
rack
,
int
port
,
int
memory
,
int
vcores
)
{
NodeId
nodeId
=
NodeId
.
newInstance
(
host
,
port
)
;
RMNode
rmNode
=
mock
(
RMNode
.
class
)
;
when
(
rmNode
.
getNodeID
(
)
)
.
thenReturn
(
nodeId
)
;
when
(
rmNode
.
getTotalCapability
(
)
)
.
thenReturn
(
Resources
.
createResource
(
memory
,
vcores
)
)
;
when
(
rmNode
.
getNodeAddress
(
)
)
.
thenReturn
(
host
+
+
port
)
;
when
(
rmNode
.
getHostName
(
)
)
.
thenReturn
(
host
)
;
when
(
rmNode
.
getRackName
(
)
)
.
thenReturn
(
rack
)
;
when
(
rmNode
.
getState
(
)
)
.
thenReturn
(
NodeState
.
RUNNING
)
;
FiCaSchedulerNode
node
=
spy
(
new
FiCaSchedulerNode
(
rmNode
,
false
)
)
;
private
static
void
printTags
(
Collection
<
MockNM
>
nodes
,
AllocationTagsManager
atm
)
{
for
(
MockNM
nm
:
nodes
)
{
Map
<
String
,
Long
>
nmTags
=
atm
.
getAllocationTagsWithCount
(
nm
.
getNodeId
(
)
)
;
StringBuffer
sb
=
new
StringBuffer
(
)
;
if
(
nmTags
!=
null
)
{
nmTags
.
forEach
(
(
tag
,
count
)
->
sb
.
append
(
tag
+
+
count
+
)
)
;
@
Test
public
void
testRoundRobinSimpleAllocation
(
)
throws
Exception
{
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
1
,
ResourceRequest
.
ANY
,
1
)
,
createResourceRequest
(
2
,
ResourceRequest
.
ANY
,
1
)
,
createResourceRequest
(
3
,
ResourceRequest
.
ANY
,
1
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
3
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testNodeLocalAllocation
(
)
throws
Exception
{
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
1
,
ResourceRequest
.
ANY
,
1
)
,
createResourceRequest
(
2
,
,
1
)
,
createResourceRequest
(
2
,
,
1
)
,
createResourceRequest
(
2
,
ResourceRequest
.
ANY
,
1
)
,
createResourceRequest
(
3
,
,
1
)
,
createResourceRequest
(
3
,
,
1
)
,
createResourceRequest
(
3
,
ResourceRequest
.
ANY
,
1
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testNodeLocalAllocationSameSchedulerKey
(
)
throws
Exception
{
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
2
,
ResourceRequest
.
ANY
,
2
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testOffSwitchAllocationWhenNoNodeOrRack
(
)
throws
Exception
{
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
2
,
ResourceRequest
.
ANY
,
2
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
NodeQueueLoadMonitor
selector
=
createNodeQueueLoadMonitor
(
Arrays
.
asList
(
,
,
,
)
,
Arrays
.
asList
(
,
,
,
)
,
Arrays
.
asList
(
4
,
4
,
4
,
4
)
,
Arrays
.
asList
(
5
,
5
,
5
,
5
)
)
;
allocator
.
setNodeQueueLoadMonitor
(
selector
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeat
(
)
throws
Exception
{
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
2
,
,
3
)
,
createResourceRequest
(
2
,
,
3
)
,
createResourceRequest
(
2
,
ResourceRequest
.
ANY
,
3
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeat
(
)
throws
Exception
{
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
2
,
,
3
)
,
createResourceRequest
(
2
,
,
3
)
,
createResourceRequest
(
2
,
ResourceRequest
.
ANY
,
3
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeatDifferentSchedKey
(
)
throws
Exception
{
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
1
,
ResourceRequest
.
ANY
,
1
)
,
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
3
,
,
2
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeatDifferentSchedKey
(
)
throws
Exception
{
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
1
,
ResourceRequest
.
ANY
,
1
)
,
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
3
,
,
2
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testMaxAllocationsPerAMHeartbeatDifferentSchedKey
(
)
throws
Exception
{
allocator
.
setMaxAllocationsPerAMHeartbeat
(
2
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
1
,
ResourceRequest
.
ANY
,
1
)
,
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
3
,
,
2
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
LOG
.
info
(
,
containers
)
;
assertEquals
(
2
,
containers
.
size
(
)
)
;
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
new
ArrayList
<
>
(
)
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testAllocationLatencyMetrics
(
)
throws
Exception
{
oppCntxt
=
spy
(
oppCntxt
)
;
OpportunisticSchedulerMetrics
metrics
=
mock
(
OpportunisticSchedulerMetrics
.
class
)
;
when
(
oppCntxt
.
getOppSchedulerMetrics
(
)
)
.
thenReturn
(
metrics
)
;
List
<
ResourceRequest
>
reqs
=
Arrays
.
asList
(
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
2
,
,
2
)
,
createResourceRequest
(
2
,
ResourceRequest
.
ANY
,
2
)
)
;
ApplicationAttemptId
appAttId
=
ApplicationAttemptId
.
newInstance
(
ApplicationId
.
newInstance
(
0L
,
1
)
,
1
)
;
allocator
.
setNodeQueueLoadMonitor
(
createNodeQueueLoadMonitor
(
3
,
2
,
5
)
)
;
List
<
Container
>
containers
=
allocator
.
allocateContainers
(
EMPTY_BLACKLIST_REQUEST
,
reqs
,
appAttId
,
oppCntxt
,
1L
,
)
;
@
Test
public
void
testConvertFSConfigurationDefaults
(
)
throws
Exception
{
setupFSConfigConversionFiles
(
true
)
;
ArgumentCaptor
<
FSConfigToCSConfigConverterParams
>
conversionParams
=
ArgumentCaptor
.
forClass
(
FSConfigToCSConfigConverterParams
.
class
)
;
FSConfigToCSConfigArgumentHandler
argumentHandler
=
createArgumentHandler
(
)
;
String
[
]
args
=
getArgumentsAsArrayWithDefaults
(
,
FSConfigConverterTestCommons
.
FS_ALLOC_FILE
,
,
FSConfigConverterTestCommons
.
CONVERSION_RULES_FILE
)
;
argumentHandler
.
parseAndConvert
(
args
)
;
verify
(
mockConverter
)
.
convert
(
conversionParams
.
capture
(
)
)
;
FSConfigToCSConfigConverterParams
params
=
conversionParams
.
getValue
(
)
;
@
Test
public
void
testConvertFSConfigurationWithConsoleParam
(
)
throws
Exception
{
setupFSConfigConversionFiles
(
true
)
;
ArgumentCaptor
<
FSConfigToCSConfigConverterParams
>
conversionParams
=
ArgumentCaptor
.
forClass
(
FSConfigToCSConfigConverterParams
.
class
)
;
FSConfigToCSConfigArgumentHandler
argumentHandler
=
createArgumentHandler
(
)
;
String
[
]
args
=
getArgumentsAsArrayWithDefaults
(
,
FSConfigConverterTestCommons
.
FS_ALLOC_FILE
,
,
FSConfigConverterTestCommons
.
CONVERSION_RULES_FILE
,
)
;
argumentHandler
.
parseAndConvert
(
args
)
;
verify
(
mockConverter
)
.
convert
(
conversionParams
.
capture
(
)
)
;
FSConfigToCSConfigConverterParams
params
=
conversionParams
.
getValue
(
)
;
@
Test
public
void
testConvertFSConfigurationClusterResource
(
)
throws
Exception
{
setupFSConfigConversionFiles
(
true
)
;
ArgumentCaptor
<
FSConfigToCSConfigConverterParams
>
conversionParams
=
ArgumentCaptor
.
forClass
(
FSConfigToCSConfigConverterParams
.
class
)
;
FSConfigToCSConfigArgumentHandler
argumentHandler
=
createArgumentHandler
(
)
;
String
[
]
args
=
getArgumentsAsArrayWithDefaults
(
,
FSConfigConverterTestCommons
.
FS_ALLOC_FILE
,
,
FSConfigConverterTestCommons
.
CONVERSION_RULES_FILE
,
,
,
)
;
argumentHandler
.
parseAndConvert
(
args
)
;
verify
(
mockConverter
)
.
convert
(
conversionParams
.
capture
(
)
)
;
FSConfigToCSConfigConverterParams
params
=
conversionParams
.
getValue
(
)
;
Assert
.
assertEquals
(
nm1
.
getNodeId
(
)
,
allocated1
.
get
(
0
)
.
getNodeId
(
)
)
;
report_nm1
=
rm
.
getResourceScheduler
(
)
.
getNodeReport
(
nm1
.
getNodeId
(
)
)
;
Assert
.
assertEquals
(
0
,
report_nm1
.
getAvailableResource
(
)
.
getMemorySize
(
)
)
;
Assert
.
assertEquals
(
4
*
GB
,
report_nm1
.
getUsedResource
(
)
.
getMemorySize
(
)
)
;
Container
c1
=
allocated1
.
get
(
0
)
;
Assert
.
assertEquals
(
2
*
GB
,
c1
.
getResource
(
)
.
getMemorySize
(
)
)
;
Map
<
NodeId
,
ResourceOption
>
nodeResourceMap
=
new
HashMap
<
NodeId
,
ResourceOption
>
(
)
;
nodeResourceMap
.
put
(
nm1
.
getNodeId
(
)
,
ResourceOption
.
newInstance
(
Resource
.
newInstance
(
2
*
GB
,
1
)
,
-
1
)
)
;
UpdateNodeResourceRequest
request
=
UpdateNodeResourceRequest
.
newInstance
(
nodeResourceMap
)
;
rm
.
getAdminService
(
)
.
updateNodeResource
(
request
)
;
waitCount
=
0
;
while
(
waitCount
++
!=
20
)
{
report_nm1
=
rm
.
getResourceScheduler
(
)
.
getNodeReport
(
nm1
.
getNodeId
(
)
)
;
if
(
null
!=
report_nm1
&&
report_nm1
.
getAvailableResource
(
)
.
getMemorySize
(
)
!=
0
)
{
break
;
UpdateNodeResourceRequest
request
=
UpdateNodeResourceRequest
.
newInstance
(
nodeResourceMap
)
;
rm
.
getAdminService
(
)
.
updateNodeResource
(
request
)
;
waitCount
=
0
;
while
(
waitCount
++
!=
20
)
{
report_nm1
=
rm
.
getResourceScheduler
(
)
.
getNodeReport
(
nm1
.
getNodeId
(
)
)
;
if
(
null
!=
report_nm1
&&
report_nm1
.
getAvailableResource
(
)
.
getMemorySize
(
)
!=
0
)
{
break
;
}
LOG
.
info
(
+
waitCount
+
)
;
Thread
.
sleep
(
1000
)
;
}
report_nm1
=
rm
.
getResourceScheduler
(
)
.
getNodeReport
(
nm1
.
getNodeId
(
)
)
;
Assert
.
assertEquals
(
4
*
GB
,
report_nm1
.
getUsedResource
(
)
.
getMemorySize
(
)
)
;
Assert
.
assertEquals
(
-
2
*
GB
,
report_nm1
.
getAvailableResource
(
)
.
getMemorySize
(
)
)
;
ContainerStatus
containerStatus
=
BuilderUtils
.
newContainerStatus
(
c1
.
getId
(
)
,
ContainerState
.
COMPLETE
,
,
0
,
c1
.
getResource
(
)
)
;
nm1
.
containerStatus
(
containerStatus
)
;
waitCount
=
0
;
@
Test
(
timeout
=
60000
)
public
void
testDTRenewal
(
)
throws
Exception
{
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
conf
)
;
@
Test
(
timeout
=
60000
)
public
void
testDTRenewal
(
)
throws
Exception
{
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
conf
)
;
LOG
.
info
(
+
(
Object
)
dfs
.
hashCode
(
)
+
+
conf
.
hashCode
(
)
)
;
MyToken
token1
,
token2
,
token3
;
token1
=
dfs
.
getDelegationToken
(
)
;
token2
=
dfs
.
getDelegationToken
(
)
;
token3
=
dfs
.
getDelegationToken
(
)
;
Renewer
.
tokenToRenewIn2Sec
=
token1
;
delegationTokenRenewer
.
addApplicationAsync
(
applicationId_0
,
ts
,
true
,
,
new
Configuration
(
)
)
;
waitForEventsToGetProcessed
(
delegationTokenRenewer
)
;
int
numberOfExpectedRenewals
=
3
+
1
;
int
attempts
=
10
;
while
(
attempts
--
>
0
)
{
try
{
Thread
.
sleep
(
3
*
1000
)
;
}
catch
(
InterruptedException
e
)
{
}
if
(
Renewer
.
counter
==
numberOfExpectedRenewals
)
break
;
}
LOG
.
info
(
+
dfs
.
hashCode
(
)
+
+
Renewer
.
counter
+
+
Renewer
.
lastRenewed
)
;
assertEquals
(
,
numberOfExpectedRenewals
,
Renewer
.
counter
)
;
assertEquals
(
,
Renewer
.
lastRenewed
,
token1
)
;
ts
=
new
Credentials
(
)
;
MyToken
token4
=
dfs
.
getDelegationToken
(
)
;
Renewer
.
tokenToRenewIn2Sec
=
token4
;
assertEquals
(
,
numberOfExpectedRenewals
,
Renewer
.
counter
)
;
assertEquals
(
,
Renewer
.
lastRenewed
,
token1
)
;
ts
=
new
Credentials
(
)
;
MyToken
token4
=
dfs
.
getDelegationToken
(
)
;
Renewer
.
tokenToRenewIn2Sec
=
token4
;
LOG
.
info
(
+
token4
+
)
;
String
nn4
=
DelegationTokenRenewer
.
SCHEME
+
;
ts
.
addToken
(
new
Text
(
nn4
)
,
token4
)
;
ApplicationId
applicationId_1
=
BuilderUtils
.
newApplicationId
(
0
,
1
)
;
delegationTokenRenewer
.
addApplicationAsync
(
applicationId_1
,
ts
,
true
,
,
new
Configuration
(
)
)
;
waitForEventsToGetProcessed
(
delegationTokenRenewer
)
;
delegationTokenRenewer
.
applicationFinished
(
applicationId_1
)
;
waitForEventsToGetProcessed
(
delegationTokenRenewer
)
;
numberOfExpectedRenewals
=
Renewer
.
counter
;
try
{
@
Test
(
timeout
=
60000
)
public
void
testAppRejectionWithCancelledDelegationToken
(
)
throws
Exception
{
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
conf
)
;
@
Test
(
timeout
=
60000
)
public
void
testAppTokenWithNonRenewer
(
)
throws
Exception
{
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
conf
)
;
@
Test
(
timeout
=
60000
)
public
void
testDTRenewalWithNoCancel
(
)
throws
Exception
{
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
conf
)
;
@
Test
(
timeout
=
60000
)
public
void
testDTRenewalWithNoCancel
(
)
throws
Exception
{
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
conf
)
;
LOG
.
info
(
+
(
Object
)
dfs
.
hashCode
(
)
+
+
conf
.
hashCode
(
)
)
;
Credentials
ts
=
new
Credentials
(
)
;
MyToken
token1
=
dfs
.
getDelegationToken
(
)
;
Renewer
.
tokenToRenewIn2Sec
=
token1
;
lconf
.
setBoolean
(
YarnConfiguration
.
RM_DELEGATION_TOKEN_ALWAYS_CANCEL
,
true
)
;
DelegationTokenRenewer
localDtr
=
createNewDelegationTokenRenewer
(
lconf
,
counter
)
;
RMContext
mockContext
=
mock
(
RMContext
.
class
)
;
when
(
mockContext
.
getSystemCredentialsForApps
(
)
)
.
thenReturn
(
new
ConcurrentHashMap
<
ApplicationId
,
SystemCredentialsForAppsProto
>
(
)
)
;
ClientRMService
mockClientRMService
=
mock
(
ClientRMService
.
class
)
;
when
(
mockContext
.
getClientRMService
(
)
)
.
thenReturn
(
mockClientRMService
)
;
when
(
mockContext
.
getDelegationTokenRenewer
(
)
)
.
thenReturn
(
localDtr
)
;
when
(
mockContext
.
getDispatcher
(
)
)
.
thenReturn
(
dispatcher
)
;
InetSocketAddress
sockAddr
=
InetSocketAddress
.
createUnresolved
(
,
1234
)
;
when
(
mockClientRMService
.
getBindAddress
(
)
)
.
thenReturn
(
sockAddr
)
;
localDtr
.
setDelegationTokenRenewerPoolTracker
(
false
)
;
localDtr
.
setRMContext
(
mockContext
)
;
localDtr
.
init
(
lconf
)
;
localDtr
.
start
(
)
;
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
lconf
)
;
ClientRMService
mockClientRMService
=
mock
(
ClientRMService
.
class
)
;
when
(
mockContext
.
getClientRMService
(
)
)
.
thenReturn
(
mockClientRMService
)
;
when
(
mockContext
.
getDelegationTokenRenewer
(
)
)
.
thenReturn
(
localDtr
)
;
when
(
mockContext
.
getDispatcher
(
)
)
.
thenReturn
(
dispatcher
)
;
InetSocketAddress
sockAddr
=
InetSocketAddress
.
createUnresolved
(
,
1234
)
;
when
(
mockClientRMService
.
getBindAddress
(
)
)
.
thenReturn
(
sockAddr
)
;
localDtr
.
setDelegationTokenRenewerPoolTracker
(
false
)
;
localDtr
.
setRMContext
(
mockContext
)
;
localDtr
.
init
(
lconf
)
;
localDtr
.
start
(
)
;
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
lconf
)
;
LOG
.
info
(
+
(
Object
)
dfs
.
hashCode
(
)
+
+
lconf
.
hashCode
(
)
)
;
Credentials
ts
=
new
Credentials
(
)
;
MyToken
token1
=
dfs
.
getDelegationToken
(
)
;
Renewer
.
tokenToRenewIn2Sec
=
token1
;
lconf
.
setLong
(
YarnConfiguration
.
RM_DELAYED_DELEGATION_TOKEN_REMOVAL_INTERVAL_MS
,
1000l
)
;
DelegationTokenRenewer
localDtr
=
createNewDelegationTokenRenewer
(
lconf
,
counter
)
;
RMContext
mockContext
=
mock
(
RMContext
.
class
)
;
when
(
mockContext
.
getSystemCredentialsForApps
(
)
)
.
thenReturn
(
new
ConcurrentHashMap
<
ApplicationId
,
SystemCredentialsForAppsProto
>
(
)
)
;
ClientRMService
mockClientRMService
=
mock
(
ClientRMService
.
class
)
;
when
(
mockContext
.
getClientRMService
(
)
)
.
thenReturn
(
mockClientRMService
)
;
when
(
mockContext
.
getDelegationTokenRenewer
(
)
)
.
thenReturn
(
localDtr
)
;
when
(
mockContext
.
getDispatcher
(
)
)
.
thenReturn
(
dispatcher
)
;
InetSocketAddress
sockAddr
=
InetSocketAddress
.
createUnresolved
(
,
1234
)
;
when
(
mockClientRMService
.
getBindAddress
(
)
)
.
thenReturn
(
sockAddr
)
;
localDtr
.
setDelegationTokenRenewerPoolTracker
(
false
)
;
localDtr
.
setRMContext
(
mockContext
)
;
localDtr
.
init
(
lconf
)
;
localDtr
.
start
(
)
;
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
lconf
)
;
lconf
.
setLong
(
YarnConfiguration
.
RM_DELAYED_DELEGATION_TOKEN_REMOVAL_INTERVAL_MS
,
1000l
)
;
DelegationTokenRenewer
localDtr
=
createNewDelegationTokenRenewer
(
conf
,
counter
)
;
RMContext
mockContext
=
mock
(
RMContext
.
class
)
;
when
(
mockContext
.
getSystemCredentialsForApps
(
)
)
.
thenReturn
(
new
ConcurrentHashMap
<
ApplicationId
,
SystemCredentialsForAppsProto
>
(
)
)
;
ClientRMService
mockClientRMService
=
mock
(
ClientRMService
.
class
)
;
when
(
mockContext
.
getClientRMService
(
)
)
.
thenReturn
(
mockClientRMService
)
;
when
(
mockContext
.
getDelegationTokenRenewer
(
)
)
.
thenReturn
(
localDtr
)
;
when
(
mockContext
.
getDispatcher
(
)
)
.
thenReturn
(
dispatcher
)
;
InetSocketAddress
sockAddr
=
InetSocketAddress
.
createUnresolved
(
,
1234
)
;
when
(
mockClientRMService
.
getBindAddress
(
)
)
.
thenReturn
(
sockAddr
)
;
localDtr
.
setDelegationTokenRenewerPoolTracker
(
false
)
;
localDtr
.
setRMContext
(
mockContext
)
;
localDtr
.
init
(
lconf
)
;
localDtr
.
start
(
)
;
MyFS
dfs
=
(
MyFS
)
FileSystem
.
get
(
lconf
)
;
private
DelegationTokenRenewer
createNewDelegationTokenRenewerForTimeout
(
Configuration
config
,
final
AtomicInteger
renewerCounter
,
final
AtomicBoolean
renewDelay
)
{
DelegationTokenRenewer
renew
=
new
DelegationTokenRenewer
(
)
{
@
Override
protected
ThreadPoolExecutor
createNewThreadPoolService
(
Configuration
configuration
)
{
ThreadPoolExecutor
pool
=
new
ThreadPoolExecutor
(
5
,
5
,
3L
,
TimeUnit
.
SECONDS
,
new
LinkedBlockingQueue
<
Runnable
>
(
)
)
{
@
Override
public
Future
<
?
>
submit
(
Runnable
r
)
{
renewerCounter
.
incrementAndGet
(
)
;
return
super
.
submit
(
r
)
;
}
}
;
return
pool
;
}
@
Override
protected
void
renewToken
(
final
DelegationTokenToRenew
dttr
)
throws
IOException
{
try
{
if
(
renewDelay
.
get
(
)
)
{
Thread
.
sleep
(
config
.
getTimeDuration
(
YarnConfiguration
.
RM_DT_RENEWER_THREAD_TIMEOUT
,
YarnConfiguration
.
DEFAULT_RM_DT_RENEWER_THREAD_TIMEOUT
,
TimeUnit
.
MILLISECONDS
)
*
2
)
;
}
super
.
renewToken
(
dttr
)
;
}
catch
(
InterruptedException
e
)
{
public
void
verifyClusterSchedulerFifo
(
JSONObject
json
)
throws
JSONException
,
Exception
{
assertEquals
(
+
json
,
1
,
json
.
length
(
)
)
;
JSONObject
info
=
json
.
getJSONObject
(
)
;
assertEquals
(
+
info
,
1
,
info
.
length
(
)
)
;
info
=
info
.
getJSONObject
(
)
;
@
Test
public
void
testUpdateQueue
(
)
throws
Exception
{
WebResource
r
=
resource
(
)
;
ClientResponse
response
;
SchedConfUpdateInfo
updateInfo
=
new
SchedConfUpdateInfo
(
)
;
Map
<
String
,
String
>
updateParam
=
new
HashMap
<
>
(
)
;
updateParam
.
put
(
CapacitySchedulerConfiguration
.
MAXIMUM_AM_RESOURCE_SUFFIX
,
)
;
QueueConfigInfo
aUpdateInfo
=
new
QueueConfigInfo
(
,
updateParam
)
;
updateInfo
.
getUpdateQueueInfo
(
)
.
add
(
aUpdateInfo
)
;
CapacityScheduler
cs
=
(
CapacityScheduler
)
rm
.
getResourceScheduler
(
)
;
assertEquals
(
CapacitySchedulerConfiguration
.
DEFAULT_MAXIMUM_APPLICATIONMASTERS_RESOURCE_PERCENT
,
cs
.
getConfiguration
(
)
.
getMaximumApplicationMasterResourcePerQueuePercent
(
)
,
0.001f
)
;
response
=
r
.
path
(
)
.
path
(
)
.
path
(
)
.
path
(
)
.
queryParam
(
,
userName
)
.
accept
(
MediaType
.
APPLICATION_JSON
)
.
entity
(
YarnWebServiceUtils
.
toJson
(
updateInfo
,
SchedConfUpdateInfo
.
class
)
,
MediaType
.
APPLICATION_JSON
)
.
put
(
ClientResponse
.
class
)
;
private
void
logResponse
(
)
{
String
responseStr
=
response
.
getEntity
(
String
.
class
)
;
private
void
logResponse
(
)
{
String
responseStr
=
response
.
getEntity
(
String
.
class
)
;
LOG
.
info
(
,
path
.
toString
(
)
,
responseStr
)
;
private
void
logResponse
(
Document
doc
)
{
String
responseStr
=
response
.
getEntity
(
String
.
class
)
;
private
void
logResponse
(
Document
doc
)
{
String
responseStr
=
response
.
getEntity
(
String
.
class
)
;
LOG
.
info
(
,
path
.
toString
(
)
,
responseStr
)
;
@
Public
@
Unstable
public
static
void
logAndThrowException
(
String
errMsg
,
Throwable
t
)
throws
YarnException
{
if
(
t
!=
null
)
{
@
Override
public
GetNewApplicationResponse
getNewApplication
(
GetNewApplicationRequest
request
)
throws
YarnException
,
IOException
{
long
startTime
=
clock
.
getTime
(
)
;
Map
<
SubClusterId
,
SubClusterInfo
>
subClustersActive
=
federationFacade
.
getSubClusters
(
true
)
;
for
(
int
i
=
0
;
i
<
numSubmitRetries
;
++
i
)
{
SubClusterId
subClusterId
=
getRandomActiveSubCluster
(
subClustersActive
)
;
LOG
.
debug
(
,
i
,
subClusterId
)
;
ApplicationClientProtocol
clientRMProxy
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
GetNewApplicationResponse
response
=
null
;
try
{
response
=
clientRMProxy
.
getNewApplication
(
request
)
;
}
catch
(
Exception
e
)
{
LOG
.
warn
(
+
subClusterId
.
getId
(
)
,
e
)
;
}
if
(
response
!=
null
)
{
long
stopTime
=
clock
.
getTime
(
)
;
routerMetrics
.
succeededAppsCreated
(
stopTime
-
startTime
)
;
return
response
;
}
else
{
subClustersActive
.
remove
(
subClusterId
)
;
}
}
routerMetrics
.
incrAppsFailedCreated
(
)
;
String
errMsg
=
;
LOG
.
info
(
+
applicationId
+
+
i
+
+
subClusterId
)
;
ApplicationHomeSubCluster
appHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
applicationId
,
subClusterId
)
;
if
(
i
==
0
)
{
try
{
subClusterId
=
federationFacade
.
addApplicationHomeSubCluster
(
appHomeSubCluster
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
String
message
=
+
applicationId
+
;
RouterServerUtil
.
logAndThrowException
(
message
,
e
)
;
}
}
else
{
try
{
federationFacade
.
updateApplicationHomeSubCluster
(
appHomeSubCluster
)
;
}
catch
(
YarnException
e
)
{
String
message
=
+
applicationId
+
;
SubClusterId
subClusterIdInStateStore
=
federationFacade
.
getApplicationHomeSubCluster
(
applicationId
)
;
}
else
{
try
{
federationFacade
.
updateApplicationHomeSubCluster
(
appHomeSubCluster
)
;
}
catch
(
YarnException
e
)
{
String
message
=
+
applicationId
+
;
SubClusterId
subClusterIdInStateStore
=
federationFacade
.
getApplicationHomeSubCluster
(
applicationId
)
;
if
(
subClusterId
==
subClusterIdInStateStore
)
{
LOG
.
info
(
+
applicationId
+
+
subClusterId
)
;
}
else
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
RouterServerUtil
.
logAndThrowException
(
message
,
e
)
;
}
}
}
ApplicationClientProtocol
clientRMProxy
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
SubmitApplicationResponse
response
=
null
;
try
{
response
=
clientRMProxy
.
submitApplication
(
request
)
;
@
Override
public
KillApplicationResponse
forceKillApplication
(
KillApplicationRequest
request
)
throws
YarnException
,
IOException
{
long
startTime
=
clock
.
getTime
(
)
;
if
(
request
==
null
||
request
.
getApplicationId
(
)
==
null
)
{
routerMetrics
.
incrAppsFailedKilled
(
)
;
RouterServerUtil
.
logAndThrowException
(
,
null
)
;
}
ApplicationId
applicationId
=
request
.
getApplicationId
(
)
;
SubClusterId
subClusterId
=
null
;
try
{
subClusterId
=
federationFacade
.
getApplicationHomeSubCluster
(
request
.
getApplicationId
(
)
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedKilled
(
)
;
RouterServerUtil
.
logAndThrowException
(
+
applicationId
+
,
e
)
;
}
ApplicationClientProtocol
clientRMProxy
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
KillApplicationResponse
response
=
null
;
try
{
RouterServerUtil
.
logAndThrowException
(
,
null
)
;
}
ApplicationId
applicationId
=
request
.
getApplicationId
(
)
;
SubClusterId
subClusterId
=
null
;
try
{
subClusterId
=
federationFacade
.
getApplicationHomeSubCluster
(
request
.
getApplicationId
(
)
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedKilled
(
)
;
RouterServerUtil
.
logAndThrowException
(
+
applicationId
+
,
e
)
;
}
ApplicationClientProtocol
clientRMProxy
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
KillApplicationResponse
response
=
null
;
try
{
LOG
.
info
(
+
applicationId
+
+
subClusterId
)
;
response
=
clientRMProxy
.
forceKillApplication
(
request
)
;
}
catch
(
Exception
e
)
{
routerMetrics
.
incrAppsFailedKilled
(
)
;
SubClusterId
subClusterId
=
null
;
try
{
subClusterId
=
federationFacade
.
getApplicationHomeSubCluster
(
request
.
getApplicationId
(
)
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedKilled
(
)
;
RouterServerUtil
.
logAndThrowException
(
+
applicationId
+
,
e
)
;
}
ApplicationClientProtocol
clientRMProxy
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
KillApplicationResponse
response
=
null
;
try
{
LOG
.
info
(
+
applicationId
+
+
subClusterId
)
;
response
=
clientRMProxy
.
forceKillApplication
(
request
)
;
}
catch
(
Exception
e
)
{
routerMetrics
.
incrAppsFailedKilled
(
)
;
LOG
.
error
(
+
request
.
getApplicationId
(
)
+
+
subClusterId
.
getId
(
)
,
e
)
;
throw
e
;
if
(
request
==
null
||
request
.
getApplicationId
(
)
==
null
)
{
routerMetrics
.
incrAppsFailedRetrieved
(
)
;
RouterServerUtil
.
logAndThrowException
(
,
null
)
;
}
SubClusterId
subClusterId
=
null
;
try
{
subClusterId
=
federationFacade
.
getApplicationHomeSubCluster
(
request
.
getApplicationId
(
)
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedRetrieved
(
)
;
RouterServerUtil
.
logAndThrowException
(
+
request
.
getApplicationId
(
)
+
,
e
)
;
}
ApplicationClientProtocol
clientRMProxy
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
GetApplicationReportResponse
response
=
null
;
try
{
response
=
clientRMProxy
.
getApplicationReport
(
request
)
;
}
catch
(
Exception
e
)
{
routerMetrics
.
incrAppsFailedRetrieved
(
)
;
}
SubClusterId
subClusterId
=
null
;
try
{
subClusterId
=
federationFacade
.
getApplicationHomeSubCluster
(
request
.
getApplicationId
(
)
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedRetrieved
(
)
;
RouterServerUtil
.
logAndThrowException
(
+
request
.
getApplicationId
(
)
+
,
e
)
;
}
ApplicationClientProtocol
clientRMProxy
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
GetApplicationReportResponse
response
=
null
;
try
{
response
=
clientRMProxy
.
getApplicationReport
(
request
)
;
}
catch
(
Exception
e
)
{
routerMetrics
.
incrAppsFailedRetrieved
(
)
;
LOG
.
error
(
+
request
.
getApplicationId
(
)
+
+
subClusterId
.
getId
(
)
,
e
)
;
throw
e
;
}
if
(
response
==
null
)
{
ApplicationClientProtocol
protocol
=
getClientRMProxyForSubCluster
(
subClusterId
)
;
Method
method
=
ApplicationClientProtocol
.
class
.
getMethod
(
request
.
getMethodName
(
)
,
request
.
getTypes
(
)
)
;
return
method
.
invoke
(
protocol
,
request
.
getParams
(
)
)
;
}
}
)
;
}
Map
<
SubClusterId
,
R
>
results
=
new
TreeMap
<
>
(
)
;
try
{
futures
.
addAll
(
executorService
.
invokeAll
(
callables
)
)
;
for
(
int
i
=
0
;
i
<
futures
.
size
(
)
;
i
++
)
{
SubClusterId
subClusterId
=
clusterIds
.
get
(
i
)
;
try
{
Future
<
Object
>
future
=
futures
.
get
(
i
)
;
Object
result
=
future
.
get
(
)
;
results
.
put
(
subClusterId
,
clazz
.
cast
(
result
)
)
;
}
catch
(
ExecutionException
ex
)
{
Throwable
cause
=
ex
.
getCause
(
)
;
private
RequestInterceptorChainWrapper
initializePipeline
(
String
user
)
{
synchronized
(
this
.
userPipelineMap
)
{
if
(
this
.
userPipelineMap
.
containsKey
(
user
)
)
{
private
RequestInterceptorChainWrapper
initializePipeline
(
String
user
)
{
synchronized
(
this
.
userPipelineMap
)
{
if
(
this
.
userPipelineMap
.
containsKey
(
user
)
)
{
Map
<
SubClusterId
,
SubClusterInfo
>
subClustersInfo
=
facade
.
getSubClusters
(
true
)
;
List
<
SubClusterInfo
>
subclusters
=
new
ArrayList
<
>
(
)
;
subclusters
.
addAll
(
subClustersInfo
.
values
(
)
)
;
Comparator
<
?
super
SubClusterInfo
>
cmp
=
new
Comparator
<
SubClusterInfo
>
(
)
{
@
Override
public
int
compare
(
SubClusterInfo
o1
,
SubClusterInfo
o2
)
{
return
o1
.
getSubClusterId
(
)
.
compareTo
(
o2
.
getSubClusterId
(
)
)
;
}
}
;
Collections
.
sort
(
subclusters
,
cmp
)
;
for
(
SubClusterInfo
subcluster
:
subclusters
)
{
SubClusterId
subClusterId
=
subcluster
.
getSubClusterId
(
)
;
String
webAppAddress
=
subcluster
.
getRMWebServiceAddress
(
)
;
String
capability
=
subcluster
.
getCapability
(
)
;
ClusterMetricsInfo
subClusterInfo
=
getClusterMetricsInfo
(
capability
)
;
tbody
.
tr
(
)
.
td
(
)
.
a
(
+
webAppAddress
,
subClusterId
.
toString
(
)
)
.
__
(
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getAppsSubmitted
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getAppsPending
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getAppsRunning
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getAppsFailed
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getAppsKilled
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getAppsCompleted
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getContainersAllocated
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getReservedContainers
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getPendingContainers
(
)
)
)
.
td
(
StringUtils
.
byteDesc
(
subClusterInfo
.
getAvailableMB
(
)
*
BYTES_IN_MB
)
)
.
td
(
StringUtils
.
byteDesc
(
subClusterInfo
.
getAllocatedMB
(
)
*
BYTES_IN_MB
)
)
.
td
(
StringUtils
.
byteDesc
(
subClusterInfo
.
getReservedMB
(
)
*
BYTES_IN_MB
)
)
.
td
(
StringUtils
.
byteDesc
(
subClusterInfo
.
getTotalMB
(
)
*
BYTES_IN_MB
)
)
.
td
(
Long
.
toString
(
subClusterInfo
.
getAvailableVirtualCores
(
)
)
)
.
td
(
Long
.
toString
(
subClusterInfo
.
getAllocatedVirtualCores
(
)
)
)
.
td
(
Long
.
toString
(
subClusterInfo
.
getReservedVirtualCores
(
)
)
)
.
td
(
Long
.
toString
(
subClusterInfo
.
getTotalVirtualCores
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getActiveNodes
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getLostNodes
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getDecommissionedNodes
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getUnhealthyNodes
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getRebootedNodes
(
)
)
)
.
td
(
Integer
.
toString
(
subClusterInfo
.
getTotalNodes
(
)
)
)
.
__
(
)
;
}
}
catch
(
YarnException
e
)
{
ApplicationId
applicationId
=
null
;
try
{
applicationId
=
ApplicationId
.
fromString
(
newApp
.
getApplicationId
(
)
)
;
}
catch
(
IllegalArgumentException
e
)
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
return
Response
.
status
(
Status
.
BAD_REQUEST
)
.
entity
(
e
.
getLocalizedMessage
(
)
)
.
build
(
)
;
}
List
<
SubClusterId
>
blacklist
=
new
ArrayList
<
SubClusterId
>
(
)
;
for
(
int
i
=
0
;
i
<
numSubmitRetries
;
++
i
)
{
ApplicationSubmissionContext
context
=
RMWebAppUtil
.
createAppSubmissionContext
(
newApp
,
this
.
getConf
(
)
)
;
SubClusterId
subClusterId
=
null
;
try
{
subClusterId
=
policyFacade
.
getHomeSubcluster
(
context
,
blacklist
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
return
Response
.
status
(
Status
.
SERVICE_UNAVAILABLE
)
.
entity
(
e
.
getLocalizedMessage
(
)
)
.
build
(
)
;
ApplicationHomeSubCluster
appHomeSubCluster
=
ApplicationHomeSubCluster
.
newInstance
(
applicationId
,
subClusterId
)
;
if
(
i
==
0
)
{
try
{
subClusterId
=
federationFacade
.
addApplicationHomeSubCluster
(
appHomeSubCluster
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
String
errMsg
=
+
applicationId
+
;
return
Response
.
status
(
Status
.
SERVICE_UNAVAILABLE
)
.
entity
(
errMsg
+
+
e
.
getLocalizedMessage
(
)
)
.
build
(
)
;
}
}
else
{
try
{
federationFacade
.
updateApplicationHomeSubCluster
(
appHomeSubCluster
)
;
}
catch
(
YarnException
e
)
{
String
errMsg
=
+
applicationId
+
;
SubClusterId
subClusterIdInStateStore
;
try
{
String
errMsg
=
+
applicationId
+
;
SubClusterId
subClusterIdInStateStore
;
try
{
subClusterIdInStateStore
=
federationFacade
.
getApplicationHomeSubCluster
(
applicationId
)
;
}
catch
(
YarnException
e1
)
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
return
Response
.
status
(
Status
.
SERVICE_UNAVAILABLE
)
.
entity
(
e1
.
getLocalizedMessage
(
)
)
.
build
(
)
;
}
if
(
subClusterId
==
subClusterIdInStateStore
)
{
LOG
.
info
(
,
applicationId
,
subClusterId
)
;
}
else
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
return
Response
.
status
(
Status
.
SERVICE_UNAVAILABLE
)
.
entity
(
errMsg
)
.
build
(
)
;
}
}
}
SubClusterInfo
subClusterInfo
;
try
{
subClusterInfo
=
federationFacade
.
getSubCluster
(
subClusterId
)
;
}
if
(
subClusterId
==
subClusterIdInStateStore
)
{
LOG
.
info
(
,
applicationId
,
subClusterId
)
;
}
else
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
return
Response
.
status
(
Status
.
SERVICE_UNAVAILABLE
)
.
entity
(
errMsg
)
.
build
(
)
;
}
}
}
SubClusterInfo
subClusterInfo
;
try
{
subClusterInfo
=
federationFacade
.
getSubCluster
(
subClusterId
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrAppsFailedSubmitted
(
)
;
return
Response
.
status
(
Status
.
SERVICE_UNAVAILABLE
)
.
entity
(
e
.
getLocalizedMessage
(
)
)
.
build
(
)
;
}
Response
response
=
null
;
try
{
response
=
getOrCreateInterceptorForSubCluster
(
subClusterId
,
subClusterInfo
.
getRMWebServiceAddress
(
)
)
.
submitApplication
(
newApp
,
hsr
)
;
}
catch
(
Exception
e
)
{
Map
<
SubClusterId
,
SubClusterInfo
>
subClustersActive
=
null
;
try
{
subClustersActive
=
federationFacade
.
getSubClusters
(
true
)
;
}
catch
(
YarnException
e
)
{
routerMetrics
.
incrMultipleAppsFailedRetrieved
(
)
;
return
null
;
}
CompletionService
<
AppsInfo
>
compSvc
=
new
ExecutorCompletionService
<
>
(
this
.
threadpool
)
;
final
HttpServletRequest
hsrCopy
=
clone
(
hsr
)
;
for
(
final
SubClusterInfo
info
:
subClustersActive
.
values
(
)
)
{
compSvc
.
submit
(
new
Callable
<
AppsInfo
>
(
)
{
@
Override
public
AppsInfo
call
(
)
{
DefaultRequestInterceptorREST
interceptor
=
getOrCreateInterceptorForSubCluster
(
info
.
getSubClusterId
(
)
,
info
.
getRMWebServiceAddress
(
)
)
;
AppsInfo
rmApps
=
interceptor
.
getApps
(
hsrCopy
,
stateQuery
,
statesQuery
,
finalStatusQuery
,
userQuery
,
queueQuery
,
count
,
startedBegin
,
startedEnd
,
finishBegin
,
finishEnd
,
applicationTypes
,
applicationTags
,
name
,
unselectedFields
)
;
if
(
rmApps
==
null
)
{
routerMetrics
.
incrMultipleAppsFailedRetrieved
(
)
;
final
Map
<
SubClusterId
,
SubClusterInfo
>
subClustersActive
;
try
{
subClustersActive
=
getActiveSubclusters
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
,
e
.
getMessage
(
)
)
;
return
new
NodesInfo
(
)
;
}
CompletionService
<
NodesInfo
>
compSvc
=
new
ExecutorCompletionService
<
NodesInfo
>
(
this
.
threadpool
)
;
for
(
final
SubClusterInfo
info
:
subClustersActive
.
values
(
)
)
{
compSvc
.
submit
(
new
Callable
<
NodesInfo
>
(
)
{
@
Override
public
NodesInfo
call
(
)
{
DefaultRequestInterceptorREST
interceptor
=
getOrCreateInterceptorForSubCluster
(
info
.
getSubClusterId
(
)
,
info
.
getRMWebServiceAddress
(
)
)
;
try
{
NodesInfo
nodesInfo
=
interceptor
.
getNodes
(
states
)
;
return
nodesInfo
;
}
catch
(
Exception
e
)
{
final
Map
<
SubClusterId
,
SubClusterInfo
>
subClustersActive
;
try
{
subClustersActive
=
getActiveSubclusters
(
)
;
}
catch
(
Exception
e
)
{
LOG
.
error
(
e
.
getLocalizedMessage
(
)
)
;
return
metrics
;
}
CompletionService
<
ClusterMetricsInfo
>
compSvc
=
new
ExecutorCompletionService
<
ClusterMetricsInfo
>
(
this
.
threadpool
)
;
for
(
final
SubClusterInfo
info
:
subClustersActive
.
values
(
)
)
{
compSvc
.
submit
(
new
Callable
<
ClusterMetricsInfo
>
(
)
{
@
Override
public
ClusterMetricsInfo
call
(
)
{
DefaultRequestInterceptorREST
interceptor
=
getOrCreateInterceptorForSubCluster
(
info
.
getSubClusterId
(
)
,
info
.
getRMWebServiceAddress
(
)
)
;
try
{
ClusterMetricsInfo
metrics
=
interceptor
.
getClusterMetricsInfo
(
)
;
return
metrics
;
}
catch
(
Exception
e
)
{
}
try
{
return
callerUGI
.
doAs
(
new
PrivilegedExceptionAction
<
T
>
(
)
{
@
SuppressWarnings
(
)
@
Override
public
T
run
(
)
{
Map
<
String
,
String
[
]
>
paramMap
=
null
;
if
(
hsr
!=
null
)
{
paramMap
=
hsr
.
getParameterMap
(
)
;
}
else
if
(
additionalParam
!=
null
)
{
paramMap
=
additionalParam
;
}
ClientResponse
response
=
RouterWebServiceUtil
.
invokeRMWebService
(
webApp
,
targetPath
,
method
,
(
hsr
==
null
)
?
null
:
hsr
.
getPathInfo
(
)
,
paramMap
,
formParam
,
getMediaTypeFromHttpServletRequest
(
hsr
,
returnType
)
,
conf
)
;
if
(
Response
.
class
.
equals
(
returnType
)
)
{
return
(
T
)
RouterWebServiceUtil
.
clientResponseToResponse
(
response
)
;
}
if
(
response
.
getStatus
(
)
==
SC_OK
)
{
return
response
.
getEntity
(
returnType
)
;
}
if
(
response
.
getStatus
(
)
==
SC_NO_CONTENT
)
{
try
{
private
RequestInterceptorChainWrapper
initializePipeline
(
String
user
)
{
synchronized
(
this
.
userPipelineMap
)
{
if
(
this
.
userPipelineMap
.
containsKey
(
user
)
)
{
super
.
setUpConfig
(
)
;
interceptor
=
new
TestableFederationClientInterceptor
(
)
;
stateStore
=
new
MemoryFederationStateStore
(
)
;
stateStore
.
init
(
this
.
getConf
(
)
)
;
FederationStateStoreFacade
.
getInstance
(
)
.
reinitialize
(
stateStore
,
getConf
(
)
)
;
stateStoreUtil
=
new
FederationStateStoreTestUtil
(
stateStore
)
;
interceptor
.
setConf
(
this
.
getConf
(
)
)
;
interceptor
.
init
(
user
)
;
subClusters
=
new
ArrayList
<
SubClusterId
>
(
)
;
try
{
for
(
int
i
=
0
;
i
<
NUM_SUBCLUSTER
;
i
++
)
{
SubClusterId
sc
=
SubClusterId
.
newInstance
(
Integer
.
toString
(
i
)
)
;
stateStoreUtil
.
registerSubCluster
(
sc
)
;
subClusters
.
add
(
sc
)
;
}
}
catch
(
YarnException
e
)
{
@
Test
public
void
testClientPipelineConcurrent
(
)
throws
InterruptedException
{
final
String
user
=
;
class
ClientTestThread
extends
Thread
{
private
ClientRequestInterceptor
interceptor
;
@
Override
public
void
run
(
)
{
try
{
interceptor
=
pipeline
(
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
e
.
printStackTrace
(
)
;
}
}
private
ClientRequestInterceptor
pipeline
(
)
throws
IOException
,
InterruptedException
{
return
UserGroupInformation
.
createRemoteUser
(
user
)
.
doAs
(
new
PrivilegedExceptionAction
<
ClientRequestInterceptor
>
(
)
{
@
Override
public
ClientRequestInterceptor
run
(
)
throws
Exception
{
RequestInterceptorChainWrapper
wrapper
=
getRouterClientRMService
(
)
.
getInterceptorChain
(
)
;
ClientRequestInterceptor
interceptor
=
wrapper
.
getRootInterceptor
(
)
;
Assert
.
assertNotNull
(
interceptor
)
;
@
Test
public
void
testRMAdminPipelineConcurrent
(
)
throws
InterruptedException
{
final
String
user
=
;
class
ClientTestThread
extends
Thread
{
private
RMAdminRequestInterceptor
interceptor
;
@
Override
public
void
run
(
)
{
try
{
interceptor
=
pipeline
(
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
e
.
printStackTrace
(
)
;
}
}
private
RMAdminRequestInterceptor
pipeline
(
)
throws
IOException
,
InterruptedException
{
return
UserGroupInformation
.
createRemoteUser
(
user
)
.
doAs
(
new
PrivilegedExceptionAction
<
RMAdminRequestInterceptor
>
(
)
{
@
Override
public
RMAdminRequestInterceptor
run
(
)
throws
Exception
{
RequestInterceptorChainWrapper
wrapper
=
getRouterRMAdminService
(
)
.
getInterceptorChain
(
)
;
RMAdminRequestInterceptor
interceptor
=
wrapper
.
getRootInterceptor
(
)
;
Assert
.
assertNotNull
(
interceptor
)
;
@
Override
public
Response
submitApplication
(
ApplicationSubmissionContextInfo
newApp
,
HttpServletRequest
hsr
)
throws
AuthorizationException
,
IOException
,
InterruptedException
{
validateRunning
(
)
;
ApplicationId
appId
=
ApplicationId
.
fromString
(
newApp
.
getApplicationId
(
)
)
;
super
.
setUpConfig
(
)
;
interceptor
=
new
TestableFederationInterceptorREST
(
)
;
stateStore
=
new
MemoryFederationStateStore
(
)
;
stateStore
.
init
(
this
.
getConf
(
)
)
;
FederationStateStoreFacade
.
getInstance
(
)
.
reinitialize
(
stateStore
,
this
.
getConf
(
)
)
;
stateStoreUtil
=
new
FederationStateStoreTestUtil
(
stateStore
)
;
interceptor
.
setConf
(
this
.
getConf
(
)
)
;
interceptor
.
init
(
user
)
;
subClusters
=
new
ArrayList
<
>
(
)
;
try
{
for
(
int
i
=
0
;
i
<
NUM_SUBCLUSTER
;
i
++
)
{
SubClusterId
sc
=
SubClusterId
.
newInstance
(
Integer
.
toString
(
i
)
)
;
stateStoreUtil
.
registerSubCluster
(
sc
)
;
subClusters
.
add
(
sc
)
;
}
}
catch
(
YarnException
e
)
{
private
void
setUpClusterMetrics
(
ClusterMetricsInfo
metrics
,
long
seed
)
{
@
Test
public
void
testWebPipelineConcurrent
(
)
throws
InterruptedException
{
final
String
user
=
;
class
ClientTestThread
extends
Thread
{
private
RESTRequestInterceptor
interceptor
;
@
Override
public
void
run
(
)
{
try
{
interceptor
=
pipeline
(
)
;
}
catch
(
IOException
|
InterruptedException
e
)
{
e
.
printStackTrace
(
)
;
}
}
private
RESTRequestInterceptor
pipeline
(
)
throws
IOException
,
InterruptedException
{
return
UserGroupInformation
.
createRemoteUser
(
user
)
.
doAs
(
new
PrivilegedExceptionAction
<
RESTRequestInterceptor
>
(
)
{
@
Override
public
RESTRequestInterceptor
run
(
)
throws
Exception
{
RequestInterceptorChainWrapper
wrapper
=
getInterceptorChain
(
user
)
;
RESTRequestInterceptor
interceptor
=
wrapper
.
getRootInterceptor
(
)
;
Assert
.
assertNotNull
(
interceptor
)
;
private
void
removeGlobalCleanerPidFile
(
)
{
try
{
FileSystem
fs
=
FileSystem
.
get
(
this
.
conf
)
;
String
root
=
conf
.
get
(
YarnConfiguration
.
SHARED_CACHE_ROOT
,
YarnConfiguration
.
DEFAULT_SHARED_CACHE_ROOT
)
;
Path
pidPath
=
new
Path
(
root
,
GLOBAL_CLEANER_PID
)
;
fs
.
delete
(
pidPath
,
false
)
;
void
process
(
)
{
metrics
.
reportCleaningStart
(
)
;
try
{
String
pattern
=
SharedCacheUtil
.
getCacheEntryGlobPattern
(
nestedLevel
)
;
FileStatus
[
]
resources
=
fs
.
globStatus
(
new
Path
(
root
,
pattern
)
)
;
int
numResources
=
resources
==
null
?
0
:
resources
.
length
;
LOG
.
info
(
+
numResources
+
)
;
long
beginMs
=
System
.
currentTimeMillis
(
)
;
if
(
resources
!=
null
)
{
for
(
FileStatus
resource
:
resources
)
{
if
(
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
LOG
.
warn
(
)
;
break
;
}
if
(
resource
.
isDirectory
(
)
)
{
processSingleResource
(
resource
)
;
}
else
{
LOG
.
warn
(
+
resource
.
getPath
(
)
.
toString
(
)
+
)
;
}
if
(
sleepTime
>
0
)
{
Thread
.
sleep
(
sleepTime
)
;
}
}
}
long
endMs
=
System
.
currentTimeMillis
(
)
;
long
durationMs
=
endMs
-
beginMs
;
if
(
resources
!=
null
)
{
for
(
FileStatus
resource
:
resources
)
{
if
(
Thread
.
currentThread
(
)
.
isInterrupted
(
)
)
{
LOG
.
warn
(
)
;
break
;
}
if
(
resource
.
isDirectory
(
)
)
{
processSingleResource
(
resource
)
;
}
else
{
LOG
.
warn
(
+
resource
.
getPath
(
)
.
toString
(
)
+
)
;
}
if
(
sleepTime
>
0
)
{
Thread
.
sleep
(
sleepTime
)
;
}
}
}
long
endMs
=
System
.
currentTimeMillis
(
)
;
long
durationMs
=
endMs
-
beginMs
;
LOG
.
info
(
+
numResources
+
+
durationMs
+
)
;
}
catch
(
IOException
e1
)
{
void
processSingleResource
(
FileStatus
resource
)
{
Path
path
=
resource
.
getPath
(
)
;
ResourceStatus
resourceStatus
=
ResourceStatus
.
INIT
;
if
(
path
.
toString
(
)
.
endsWith
(
RENAMED_SUFFIX
)
)
{
void
processSingleResource
(
FileStatus
resource
)
{
Path
path
=
resource
.
getPath
(
)
;
ResourceStatus
resourceStatus
=
ResourceStatus
.
INIT
;
if
(
path
.
toString
(
)
.
endsWith
(
RENAMED_SUFFIX
)
)
{
LOG
.
info
(
+
path
.
toString
(
)
+
)
;
try
{
if
(
fs
.
delete
(
path
,
true
)
)
{
resourceStatus
=
ResourceStatus
.
DELETED
;
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
path
,
e
)
;
}
}
else
{
String
key
=
path
.
getName
(
)
;
try
{
store
.
cleanResourceReferences
(
key
)
;
}
catch
(
YarnException
e
)
{
}
}
catch
(
IOException
e
)
{
LOG
.
error
(
+
path
,
e
)
;
}
}
else
{
String
key
=
path
.
getName
(
)
;
try
{
store
.
cleanResourceReferences
(
key
)
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
)
;
}
if
(
store
.
isResourceEvictable
(
key
,
resource
)
)
{
try
{
if
(
store
.
removeResource
(
key
)
)
{
boolean
deleted
=
removeResourceFromCacheFileSystem
(
path
)
;
if
(
deleted
)
{
resourceStatus
=
ResourceStatus
.
DELETED
;
}
else
{
}
else
{
String
key
=
path
.
getName
(
)
;
try
{
store
.
cleanResourceReferences
(
key
)
;
}
catch
(
YarnException
e
)
{
LOG
.
error
(
,
e
)
;
}
if
(
store
.
isResourceEvictable
(
key
,
resource
)
)
{
try
{
if
(
store
.
removeResource
(
key
)
)
{
boolean
deleted
=
removeResourceFromCacheFileSystem
(
path
)
;
if
(
deleted
)
{
resourceStatus
=
ResourceStatus
.
DELETED
;
}
else
{
LOG
.
error
(
+
+
path
)
;
resourceStatus
=
ResourceStatus
.
ERROR
;
private
boolean
removeResourceFromCacheFileSystem
(
Path
path
)
throws
IOException
{
Path
renamedPath
=
new
Path
(
path
.
toString
(
)
+
RENAMED_SUFFIX
)
;
if
(
fs
.
rename
(
path
,
renamedPath
)
)
{
private
void
bootstrap
(
Configuration
conf
)
throws
IOException
{
Map
<
String
,
String
>
initialCachedResources
=
getInitialCachedResources
(
FileSystem
.
get
(
conf
)
,
conf
)
;
FileStatus
[
]
entries
=
fs
.
globStatus
(
new
Path
(
root
,
pattern
)
)
;
int
numEntries
=
entries
==
null
?
0
:
entries
.
length
;
LOG
.
info
(
+
numEntries
+
+
)
;
Map
<
String
,
String
>
initialCachedEntries
=
new
HashMap
<
String
,
String
>
(
)
;
if
(
entries
!=
null
)
{
for
(
FileStatus
entry
:
entries
)
{
Path
file
=
entry
.
getPath
(
)
;
String
fileName
=
file
.
getName
(
)
;
if
(
entry
.
isFile
(
)
)
{
Path
parent
=
file
.
getParent
(
)
;
if
(
parent
!=
null
)
{
String
key
=
parent
.
getName
(
)
;
if
(
initialCachedEntries
.
containsKey
(
key
)
)
{
LOG
.
warn
(
+
key
+
+
initialCachedEntries
.
get
(
key
)
+
+
fileName
+
)
;
}
else
{
@
Override
protected
void
serviceStart
(
)
throws
Exception
{
SCMWebApp
scmWebApp
=
new
SCMWebApp
(
scm
)
;
this
.
webApp
=
WebApps
.
$for
(
)
.
at
(
bindAddress
)
.
start
(
scmWebApp
)
;
private
void
waitForContainerToFinishOnNM
(
ContainerId
containerId
)
throws
InterruptedException
{
Context
nmContext
=
yarnCluster
.
getNodeManager
(
0
)
.
getNMContext
(
)
;
final
int
timeout
=
4
*
60
*
1000
;
Container
waitContainer
=
nmContext
.
getContainers
(
)
.
get
(
containerId
)
;
if
(
waitContainer
!=
null
)
{
try
{
private
void
testDirsFailures
(
boolean
localORLogDirs
)
throws
IOException
{
String
dirType
=
localORLogDirs
?
:
;
String
dirsProperty
=
localORLogDirs
?
YarnConfiguration
.
NM_LOCAL_DIRS
:
YarnConfiguration
.
NM_LOG_DIRS
;
Configuration
conf
=
new
Configuration
(
)
;
conf
.
setLong
(
YarnConfiguration
.
NM_DISK_HEALTH_CHECK_INTERVAL_MS
,
TOO_HIGH_DISK_HEALTH_CHECK_INTERVAL
)
;
conf
.
setFloat
(
YarnConfiguration
.
NM_MIN_HEALTHY_DISKS_FRACTION
,
0.60F
)
;
if
(
yarnCluster
!=
null
)
{
yarnCluster
.
stop
(
)
;
FileUtil
.
fullyDelete
(
localFSDirBase
)
;
localFSDirBase
.
mkdirs
(
)
;
}
LOG
.
info
(
)
;
yarnCluster
=
new
MiniYARNCluster
(
TestDiskFailures
.
class
.
getName
(
)
,
1
,
numLocalDirs
,
numLogDirs
)
;
yarnCluster
.
init
(
conf
)
;
yarnCluster
.
start
(
)
;
NodeManager
nm
=
yarnCluster
.
getNodeManager
(
0
)
;
private
void
verifyDisksHealth
(
boolean
localORLogDirs
,
String
expectedDirs
,
boolean
isHealthy
)
{
dirsHandler
.
checkDirs
(
)
;
List
<
String
>
list
=
localORLogDirs
?
dirsHandler
.
getLocalDirs
(
)
:
dirsHandler
.
getLogDirs
(
)
;
String
seenDirs
=
StringUtils
.
join
(
,
list
)
;
private
void
verifyDisksHealth
(
boolean
localORLogDirs
,
String
expectedDirs
,
boolean
isHealthy
)
{
dirsHandler
.
checkDirs
(
)
;
List
<
String
>
list
=
localORLogDirs
?
dirsHandler
.
getLocalDirs
(
)
:
dirsHandler
.
getLogDirs
(
)
;
String
seenDirs
=
StringUtils
.
join
(
,
list
)
;
LOG
.
info
(
+
expectedDirs
)
;
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
metrics
=
EntityGroupFSTimelineStoreMetrics
.
create
(
)
;
summaryStore
=
createSummaryStore
(
)
;
addService
(
summaryStore
)
;
long
logRetainSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT
)
;
logRetainMillis
=
logRetainSecs
*
1000
;
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
metrics
=
EntityGroupFSTimelineStoreMetrics
.
create
(
)
;
summaryStore
=
createSummaryStore
(
)
;
addService
(
summaryStore
)
;
long
logRetainSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT
)
;
logRetainMillis
=
logRetainSecs
*
1000
;
LOG
.
info
(
,
logRetainSecs
)
;
long
unknownActiveSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT
)
;
unknownActiveMillis
=
unknownActiveSecs
*
1000
;
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
metrics
=
EntityGroupFSTimelineStoreMetrics
.
create
(
)
;
summaryStore
=
createSummaryStore
(
)
;
addService
(
summaryStore
)
;
long
logRetainSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT
)
;
logRetainMillis
=
logRetainSecs
*
1000
;
LOG
.
info
(
,
logRetainSecs
)
;
long
unknownActiveSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT
)
;
unknownActiveMillis
=
unknownActiveSecs
*
1000
;
LOG
.
info
(
,
unknownActiveSecs
)
;
appCacheMaxSize
=
conf
.
getInt
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT
)
;
metrics
=
EntityGroupFSTimelineStoreMetrics
.
create
(
)
;
summaryStore
=
createSummaryStore
(
)
;
addService
(
summaryStore
)
;
long
logRetainSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT
)
;
logRetainMillis
=
logRetainSecs
*
1000
;
LOG
.
info
(
,
logRetainSecs
)
;
long
unknownActiveSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT
)
;
unknownActiveMillis
=
unknownActiveSecs
*
1000
;
LOG
.
info
(
,
unknownActiveSecs
)
;
appCacheMaxSize
=
conf
.
getInt
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT
)
;
LOG
.
info
(
,
appCacheMaxSize
)
;
cachedLogs
=
Collections
.
synchronizedMap
(
new
LinkedHashMap
<
TimelineEntityGroupId
,
EntityCacheItem
>
(
appCacheMaxSize
+
1
,
0.75f
,
true
)
{
@
Override
protected
boolean
removeEldestEntry
(
Map
.
Entry
<
TimelineEntityGroupId
,
EntityCacheItem
>
eldest
)
{
if
(
super
.
size
(
)
>
appCacheMaxSize
)
{
TimelineEntityGroupId
groupId
=
eldest
.
getKey
(
)
;
addService
(
summaryStore
)
;
long
logRetainSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_RETAIN_SECONDS_DEFAULT
)
;
logRetainMillis
=
logRetainSecs
*
1000
;
LOG
.
info
(
,
logRetainSecs
)
;
long
unknownActiveSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_UNKNOWN_ACTIVE_SECONDS_DEFAULT
)
;
unknownActiveMillis
=
unknownActiveSecs
*
1000
;
LOG
.
info
(
,
unknownActiveSecs
)
;
appCacheMaxSize
=
conf
.
getInt
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_APP_CACHE_SIZE_DEFAULT
)
;
LOG
.
info
(
,
appCacheMaxSize
)
;
cachedLogs
=
Collections
.
synchronizedMap
(
new
LinkedHashMap
<
TimelineEntityGroupId
,
EntityCacheItem
>
(
appCacheMaxSize
+
1
,
0.75f
,
true
)
{
@
Override
protected
boolean
removeEldestEntry
(
Map
.
Entry
<
TimelineEntityGroupId
,
EntityCacheItem
>
eldest
)
{
if
(
super
.
size
(
)
>
appCacheMaxSize
)
{
TimelineEntityGroupId
groupId
=
eldest
.
getKey
(
)
;
LOG
.
debug
(
,
groupId
)
;
EntityCacheItem
cacheItem
=
eldest
.
getValue
(
)
;
Collection
<
String
>
pluginNames
=
conf
.
getTrimmedStringCollection
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_CLASSES
)
;
String
pluginClasspath
=
conf
.
getTrimmed
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_CLASSPATH
)
;
String
[
]
systemClasses
=
conf
.
getTrimmedStrings
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_SYSTEM_CLASSES
)
;
List
<
TimelineEntityGroupPlugin
>
pluginList
=
new
LinkedList
<
TimelineEntityGroupPlugin
>
(
)
;
ClassLoader
customClassLoader
=
null
;
if
(
pluginClasspath
!=
null
&&
pluginClasspath
.
length
(
)
>
0
)
{
try
{
customClassLoader
=
createPluginClassLoader
(
pluginClasspath
,
systemClasses
)
;
}
catch
(
IOException
ioe
)
{
LOG
.
warn
(
,
ioe
)
;
}
}
for
(
final
String
name
:
pluginNames
)
{
LOG
.
debug
(
,
name
)
;
TimelineEntityGroupPlugin
cacheIdPlugin
=
null
;
try
{
if
(
customClassLoader
!=
null
)
{
summaryTdm
.
init
(
conf
)
;
addService
(
summaryTdm
)
;
super
.
serviceStart
(
)
;
if
(
!
fs
.
exists
(
activeRootPath
)
)
{
fs
.
mkdirs
(
activeRootPath
)
;
fs
.
setPermission
(
activeRootPath
,
ACTIVE_DIR_PERMISSION
)
;
}
if
(
!
fs
.
exists
(
doneRootPath
)
)
{
fs
.
mkdirs
(
doneRootPath
)
;
fs
.
setPermission
(
doneRootPath
,
DONE_DIR_PERMISSION
)
;
}
objMapper
=
new
ObjectMapper
(
)
;
objMapper
.
setAnnotationIntrospector
(
new
JaxbAnnotationIntrospector
(
TypeFactory
.
defaultInstance
(
)
)
)
;
jsonFactory
=
new
MappingJsonFactory
(
objMapper
)
;
final
long
scanIntervalSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS_DEFAULT
)
;
final
long
cleanerIntervalSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS_DEFAULT
)
;
final
int
numThreads
=
conf
.
getInt
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS_DEFAULT
)
;
addService
(
summaryTdm
)
;
super
.
serviceStart
(
)
;
if
(
!
fs
.
exists
(
activeRootPath
)
)
{
fs
.
mkdirs
(
activeRootPath
)
;
fs
.
setPermission
(
activeRootPath
,
ACTIVE_DIR_PERMISSION
)
;
}
if
(
!
fs
.
exists
(
doneRootPath
)
)
{
fs
.
mkdirs
(
doneRootPath
)
;
fs
.
setPermission
(
doneRootPath
,
DONE_DIR_PERMISSION
)
;
}
objMapper
=
new
ObjectMapper
(
)
;
objMapper
.
setAnnotationIntrospector
(
new
JaxbAnnotationIntrospector
(
TypeFactory
.
defaultInstance
(
)
)
)
;
jsonFactory
=
new
MappingJsonFactory
(
objMapper
)
;
final
long
scanIntervalSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS_DEFAULT
)
;
final
long
cleanerIntervalSecs
=
conf
.
getLong
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_CLEANER_INTERVAL_SECONDS_DEFAULT
)
;
final
int
numThreads
=
conf
.
getInt
(
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS
,
YarnConfiguration
.
TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_THREADS_DEFAULT
)
;
LOG
.
info
(
,
activeRootPath
,
scanIntervalSecs
)
;
int
scanActiveLogs
(
Path
dir
)
throws
IOException
{
RemoteIterator
<
FileStatus
>
iter
=
list
(
dir
)
;
int
logsToScanCount
=
0
;
while
(
iter
.
hasNext
(
)
)
{
FileStatus
stat
=
iter
.
next
(
)
;
String
name
=
stat
.
getPath
(
)
.
getName
(
)
;
ApplicationId
appId
=
parseApplicationId
(
name
)
;
if
(
appId
!=
null
)
{
private
AppLogs
getAndSetAppLogs
(
ApplicationId
applicationId
)
throws
IOException
{
if
(
fs
.
exists
(
appDirPath
)
)
{
appState
=
AppState
.
COMPLETED
;
}
else
{
appDirPath
=
getActiveAppPath
(
applicationId
)
;
if
(
fs
.
exists
(
appDirPath
)
)
{
appState
=
AppState
.
ACTIVE
;
}
else
{
RemoteIterator
<
FileStatus
>
iter
=
list
(
activeRootPath
)
;
while
(
iter
.
hasNext
(
)
)
{
Path
child
=
new
Path
(
iter
.
next
(
)
.
getPath
(
)
.
getName
(
)
,
applicationId
.
toString
(
)
)
;
appDirPath
=
new
Path
(
activeRootPath
,
child
)
;
if
(
fs
.
exists
(
appDirPath
)
)
{
appState
=
AppState
.
ACTIVE
;
break
;
}
}
}
}
if
(
appState
!=
AppState
.
UNKNOWN
)
{
private
void
deleteDir
(
Path
path
)
{
try
{
private
static
boolean
shouldCleanAppLogDir
(
Path
appLogPath
,
long
now
,
FileSystem
fs
,
long
logRetainMillis
)
throws
IOException
{
RemoteIterator
<
FileStatus
>
iter
=
fs
.
listStatusIterator
(
appLogPath
)
;
while
(
iter
.
hasNext
(
)
)
{
FileStatus
stat
=
iter
.
next
(
)
;
if
(
now
-
stat
.
getModificationTime
(
)
<=
logRetainMillis
)
{
private
List
<
TimelineStore
>
getTimelineStoresFromCacheIds
(
Set
<
TimelineEntityGroupId
>
groupIds
,
String
entityType
,
List
<
EntityCacheItem
>
cacheItems
)
throws
IOException
{
List
<
TimelineStore
>
stores
=
new
LinkedList
<
TimelineStore
>
(
)
;
for
(
TimelineEntityGroupId
groupId
:
groupIds
)
{
TimelineStore
storeForId
=
getCachedStore
(
groupId
,
cacheItems
)
;
if
(
storeForId
!=
null
)
{
protected
List
<
TimelineStore
>
getTimelineStoresForRead
(
String
entityId
,
String
entityType
,
List
<
EntityCacheItem
>
cacheItems
)
throws
IOException
{
Set
<
TimelineEntityGroupId
>
groupIds
=
new
HashSet
<
TimelineEntityGroupId
>
(
)
;
for
(
TimelineEntityGroupPlugin
cacheIdPlugin
:
cacheIdPlugins
)
{
private
List
<
TimelineStore
>
getTimelineStoresForRead
(
String
entityType
,
NameValuePair
primaryFilter
,
Collection
<
NameValuePair
>
secondaryFilters
,
List
<
EntityCacheItem
>
cacheItems
)
throws
IOException
{
Set
<
TimelineEntityGroupId
>
groupIds
=
new
HashSet
<
TimelineEntityGroupId
>
(
)
;
for
(
TimelineEntityGroupPlugin
cacheIdPlugin
:
cacheIdPlugins
)
{
Set
<
TimelineEntityGroupId
>
idsFromPlugin
=
cacheIdPlugin
.
getTimelineEntityGroupId
(
entityType
,
primaryFilter
,
secondaryFilters
)
;
if
(
idsFromPlugin
!=
null
)
{
private
TimelineStore
getCachedStore
(
TimelineEntityGroupId
groupId
,
List
<
EntityCacheItem
>
cacheItems
)
throws
IOException
{
EntityCacheItem
cacheItem
;
synchronized
(
this
.
cachedLogs
)
{
cacheItem
=
this
.
cachedLogs
.
get
(
groupId
)
;
if
(
cacheItem
==
null
)
{
private
TimelineStore
getCachedStore
(
TimelineEntityGroupId
groupId
,
List
<
EntityCacheItem
>
cacheItems
)
throws
IOException
{
EntityCacheItem
cacheItem
;
synchronized
(
this
.
cachedLogs
)
{
cacheItem
=
this
.
cachedLogs
.
get
(
groupId
)
;
if
(
cacheItem
==
null
)
{
LOG
.
debug
(
,
groupId
)
;
cacheItem
=
new
EntityCacheItem
(
groupId
,
getConfig
(
)
)
;
AppLogs
appLogs
=
getAndSetAppLogs
(
groupId
.
getApplicationId
(
)
)
;
if
(
appLogs
!=
null
)
{
synchronized
(
this
.
cachedLogs
)
{
cacheItem
=
this
.
cachedLogs
.
get
(
groupId
)
;
if
(
cacheItem
==
null
)
{
LOG
.
debug
(
,
groupId
)
;
cacheItem
=
new
EntityCacheItem
(
groupId
,
getConfig
(
)
)
;
AppLogs
appLogs
=
getAndSetAppLogs
(
groupId
.
getApplicationId
(
)
)
;
if
(
appLogs
!=
null
)
{
LOG
.
debug
(
,
appLogs
,
groupId
)
;
cacheItem
.
setAppLogs
(
appLogs
)
;
this
.
cachedLogs
.
put
(
groupId
,
cacheItem
)
;
}
else
{
LOG
.
warn
(
,
groupId
)
;
}
}
}
TimelineStore
store
=
null
;
if
(
cacheItem
.
getAppLogs
(
)
!=
null
)
{
AppLogs
appLogs
=
cacheItem
.
getAppLogs
(
)
;
@
Override
public
TimelineEntities
getEntities
(
String
entityType
,
Long
limit
,
Long
windowStart
,
Long
windowEnd
,
String
fromId
,
Long
fromTs
,
NameValuePair
primaryFilter
,
Collection
<
NameValuePair
>
secondaryFilters
,
EnumSet
<
Field
>
fieldsToRetrieve
,
CheckAcl
checkAcl
)
throws
IOException
{
@
Override
public
TimelineEntities
getEntities
(
String
entityType
,
Long
limit
,
Long
windowStart
,
Long
windowEnd
,
String
fromId
,
Long
fromTs
,
NameValuePair
primaryFilter
,
Collection
<
NameValuePair
>
secondaryFilters
,
EnumSet
<
Field
>
fieldsToRetrieve
,
CheckAcl
checkAcl
)
throws
IOException
{
LOG
.
debug
(
,
entityType
,
primaryFilter
)
;
List
<
EntityCacheItem
>
relatedCacheItems
=
new
ArrayList
<
>
(
)
;
List
<
TimelineStore
>
stores
=
getTimelineStoresForRead
(
entityType
,
primaryFilter
,
secondaryFilters
,
relatedCacheItems
)
;
TimelineEntities
returnEntities
=
new
TimelineEntities
(
)
;
for
(
TimelineStore
store
:
stores
)
{
@
Override
public
TimelineEntity
getEntity
(
String
entityId
,
String
entityType
,
EnumSet
<
Field
>
fieldsToRetrieve
)
throws
IOException
{
@
Override
public
TimelineEntity
getEntity
(
String
entityId
,
String
entityType
,
EnumSet
<
Field
>
fieldsToRetrieve
)
throws
IOException
{
LOG
.
debug
(
,
entityType
,
entityId
)
;
List
<
EntityCacheItem
>
relatedCacheItems
=
new
ArrayList
<
>
(
)
;
List
<
TimelineStore
>
stores
=
getTimelineStoresForRead
(
entityId
,
entityType
,
relatedCacheItems
)
;
for
(
TimelineStore
store
:
stores
)
{
@
Override
public
TimelineEvents
getEntityTimelines
(
String
entityType
,
SortedSet
<
String
>
entityIds
,
Long
limit
,
Long
windowStart
,
Long
windowEnd
,
Set
<
String
>
eventTypes
)
throws
IOException
{
public
long
parseForStore
(
TimelineDataManager
tdm
,
Path
appDirPath
,
boolean
appCompleted
,
JsonFactory
jsonFactory
,
ObjectMapper
objMapper
,
FileSystem
fs
)
throws
IOException
{
public
long
parseForStore
(
TimelineDataManager
tdm
,
Path
appDirPath
,
boolean
appCompleted
,
JsonFactory
jsonFactory
,
ObjectMapper
objMapper
,
FileSystem
fs
)
throws
IOException
{
LOG
.
debug
(
,
appDirPath
,
attemptDirName
)
;
Path
logPath
=
getPath
(
appDirPath
)
;
FileStatus
status
=
fs
.
getFileStatus
(
logPath
)
;
long
numParsed
=
0
;
if
(
status
!=
null
)
{
long
startTime
=
Time
.
monotonicNow
(
)
;
try
{
public
long
parseForStore
(
TimelineDataManager
tdm
,
Path
appDirPath
,
boolean
appCompleted
,
JsonFactory
jsonFactory
,
ObjectMapper
objMapper
,
FileSystem
fs
)
throws
IOException
{
LOG
.
debug
(
,
appDirPath
,
attemptDirName
)
;
Path
logPath
=
getPath
(
appDirPath
)
;
FileStatus
status
=
fs
.
getFileStatus
(
logPath
)
;
long
numParsed
=
0
;
if
(
status
!=
null
)
{
long
startTime
=
Time
.
monotonicNow
(
)
;
try
{
LOG
.
debug
(
,
logPath
,
offset
)
;
long
count
=
parsePath
(
tdm
,
logPath
,
appCompleted
,
jsonFactory
,
objMapper
,
fs
)
;
ArrayList
<
TimelineEntity
>
entityList
=
new
ArrayList
<
TimelineEntity
>
(
1
)
;
long
bytesParsed
;
long
bytesParsedLastBatch
=
0
;
boolean
postError
=
false
;
try
{
MappingIterator
<
TimelineEntity
>
iter
=
objMapper
.
readValues
(
parser
,
TimelineEntity
.
class
)
;
while
(
iter
.
hasNext
(
)
)
{
TimelineEntity
entity
=
iter
.
next
(
)
;
String
etype
=
entity
.
getEntityType
(
)
;
String
eid
=
entity
.
getEntityId
(
)
;
LOG
.
trace
(
,
etype
)
;
++
count
;
bytesParsed
=
parser
.
getCurrentLocation
(
)
.
getCharOffset
(
)
+
1
;
LOG
.
trace
(
,
bytesParsed
)
;
try
{
@
Override
public
void
createTimelineSchema
(
String
[
]
args
)
{
try
{
Configuration
conf
=
new
YarnConfiguration
(
)
;
@
Override
public
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
DocumentStoreVendor
storeType
=
DocumentStoreUtils
.
getStoreVendor
(
conf
)
;
@
Override
public
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
storeType
=
DocumentStoreUtils
.
getStoreVendor
(
conf
)
;
@
Override
public
TimelineWriteResponse
write
(
TimelineCollectorContext
context
,
TimelineEntities
data
,
UserGroupInformation
callerUgi
)
{
public
TimelineEntityDocument
readDocument
(
TimelineReaderContext
context
)
throws
IOException
{
public
List
<
TimelineEntityDocument
>
readDocuments
(
TimelineReaderContext
context
,
long
documentsSize
)
throws
IOException
{
List
<
TimelineEntityDocument
>
entityDocs
=
new
ArrayList
<
>
(
)
;
public
Set
<
String
>
fetchEntityTypes
(
TimelineReaderContext
context
)
{
@
Override
public
Set
<
String
>
fetchEntityTypes
(
String
collectionName
,
TimelineReaderContext
context
)
{
StringBuilder
queryStrBuilder
=
new
StringBuilder
(
)
;
queryStrBuilder
.
append
(
String
.
format
(
SELECT_DISTINCT_TYPES_FROM_COLLECTION
,
collectionName
)
)
;
String
sqlQuery
=
addPredicates
(
context
,
collectionName
,
queryStrBuilder
)
;
private
List
<
TimelineDoc
>
queryDocuments
(
String
collectionName
,
TimelineReaderContext
context
,
final
Class
<
TimelineDoc
>
docClass
,
final
long
maxDocumentsSize
)
{
final
String
sqlQuery
=
buildQueryWithPredicates
(
context
,
collectionName
,
maxDocumentsSize
)
;
if
(
!
DocumentStoreUtils
.
isNullOrEmpty
(
context
.
getFlowName
(
)
)
)
{
hasPredicate
=
true
;
queryStrBuilder
.
append
(
AND_OPERATOR
)
.
append
(
String
.
format
(
CONTAINS_FUNC_FOR_ID
,
context
.
getFlowName
(
)
)
)
;
}
if
(
!
DocumentStoreUtils
.
isNullOrEmpty
(
context
.
getAppId
(
)
)
)
{
hasPredicate
=
true
;
queryStrBuilder
.
append
(
AND_OPERATOR
)
.
append
(
String
.
format
(
CONTAINS_FUNC_FOR_ID
,
context
.
getAppId
(
)
)
)
;
}
if
(
!
DocumentStoreUtils
.
isNullOrEmpty
(
context
.
getEntityId
(
)
)
)
{
hasPredicate
=
true
;
queryStrBuilder
.
append
(
AND_OPERATOR
)
.
append
(
String
.
format
(
CONTAINS_FUNC_FOR_ID
,
context
.
getEntityId
(
)
)
)
;
}
if
(
context
.
getFlowRunId
(
)
!=
null
)
{
hasPredicate
=
true
;
queryStrBuilder
.
append
(
AND_OPERATOR
)
.
append
(
String
.
format
(
CONTAINS_FUNC_FOR_ID
,
context
.
getFlowRunId
(
)
)
)
;
}
if
(
!
DocumentStoreUtils
.
isNullOrEmpty
(
context
.
getEntityType
(
)
)
)
{
hasPredicate
=
true
;
queryStrBuilder
.
append
(
AND_OPERATOR
)
.
append
(
String
.
format
(
CONTAINS_FUNC_FOR_TYPE
,
context
.
getEntityType
(
)
)
)
;
@
Override
public
void
createDatabase
(
)
{
Observable
<
ResourceResponse
<
Database
>>
databaseReadObs
=
client
.
readDatabase
(
String
.
format
(
DATABASE_LINK
,
databaseName
)
,
null
)
;
Observable
<
ResourceResponse
<
Database
>>
databaseExistenceObs
=
databaseReadObs
.
doOnNext
(
databaseResourceResponse
->
LOG
.
info
(
,
databaseName
)
)
.
onErrorResumeNext
(
throwable
->
{
if
(
throwable
instanceof
DocumentClientException
)
{
DocumentClientException
de
=
(
DocumentClientException
)
throwable
;
if
(
de
.
getStatusCode
(
)
==
404
)
{
@
Override
public
void
createCollection
(
final
String
collectionName
)
{
@
Override
public
void
createCollection
(
final
String
collectionName
)
{
LOG
.
info
(
,
collectionName
,
databaseName
)
;
client
.
queryCollections
(
String
.
format
(
DATABASE_LINK
,
databaseName
)
,
new
SqlQuerySpec
(
QUERY_COLLECTION_IF_EXISTS
,
new
SqlParameterCollection
(
new
SqlParameter
(
ID
,
collectionName
)
)
)
,
null
)
.
single
(
)
.
flatMap
(
(
Func1
<
FeedResponse
<
DocumentCollection
>
,
Observable
<
?
>>
)
page
->
{
if
(
page
.
getResults
(
)
.
isEmpty
(
)
)
{
DocumentCollection
collection
=
new
DocumentCollection
(
)
;
collection
.
setId
(
collectionName
)
;
@
Override
public
void
writeDocument
(
final
TimelineDoc
timelineDoc
,
final
CollectionType
collectionType
)
{
o
.
setArgName
(
)
;
o
.
setRequired
(
false
)
;
options
.
addOption
(
o
)
;
o
=
new
Option
(
SUB_APP_METRICS_TTL_OPTION_SHORT
,
,
true
,
)
;
o
.
setArgName
(
)
;
o
.
setRequired
(
false
)
;
options
.
addOption
(
o
)
;
o
=
new
Option
(
SKIP_EXISTING_TABLE_OPTION_SHORT
,
,
false
,
)
;
o
.
setRequired
(
false
)
;
options
.
addOption
(
o
)
;
CommandLineParser
parser
=
new
PosixParser
(
)
;
CommandLine
commandLine
=
null
;
try
{
commandLine
=
parser
.
parse
(
options
,
args
)
;
}
catch
(
Exception
e
)
{
@
Override
protected
void
serviceInit
(
Configuration
conf
)
throws
Exception
{
super
.
serviceInit
(
conf
)
;
Configuration
hbaseConf
=
HBaseTimelineStorageUtils
.
getTimelineServiceHBaseConf
(
conf
)
;
conn
=
ConnectionFactory
.
createConnection
(
hbaseConf
)
;
entityTable
=
new
EntityTableRW
(
)
.
getTableMutator
(
hbaseConf
,
conn
)
;
appToFlowTable
=
new
AppToFlowTableRW
(
)
.
getTableMutator
(
hbaseConf
,
conn
)
;
applicationTable
=
new
ApplicationTableRW
(
)
.
getTableMutator
(
hbaseConf
,
conn
)
;
flowRunTable
=
new
FlowRunTableRW
(
)
.
getTableMutator
(
hbaseConf
,
conn
)
;
flowActivityTable
=
new
FlowActivityTableRW
(
)
.
getTableMutator
(
hbaseConf
,
conn
)
;
subApplicationTable
=
new
SubApplicationTableRW
(
)
.
getTableMutator
(
hbaseConf
,
conn
)
;
domainTable
=
new
DomainTableRW
(
)
.
getTableMutator
(
hbaseConf
,
conn
)
;
UserGroupInformation
ugi
=
UserGroupInformation
.
isSecurityEnabled
(
)
?
UserGroupInformation
.
getLoginUser
(
)
:
UserGroupInformation
.
getCurrentUser
(
)
;
storageMonitor
=
new
HBaseStorageMonitor
(
conf
)
;
K
converterColumnKey
=
null
;
if
(
columnPrefixBytes
==
null
)
{
LOG
.
debug
(
)
;
try
{
converterColumnKey
=
keyConverter
.
decode
(
entry
.
getKey
(
)
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
error
(
,
iae
)
;
continue
;
}
}
else
{
byte
[
]
[
]
columnNameParts
=
Separator
.
QUALIFIERS
.
split
(
entry
.
getKey
(
)
,
2
)
;
byte
[
]
actualColumnPrefixBytes
=
columnNameParts
[
0
]
;
if
(
Bytes
.
equals
(
columnPrefixBytes
,
actualColumnPrefixBytes
)
&&
columnNameParts
.
length
==
2
)
{
try
{
converterColumnKey
=
keyConverter
.
decode
(
columnNameParts
[
1
]
)
;
}
catch
(
IllegalArgumentException
iae
)
{
K
converterColumnKey
=
null
;
if
(
columnPrefixBytes
==
null
)
{
try
{
converterColumnKey
=
keyConverter
.
decode
(
columnKey
)
;
}
catch
(
IllegalArgumentException
iae
)
{
LOG
.
error
(
,
iae
)
;
continue
;
}
}
else
{
byte
[
]
[
]
columnNameParts
=
Separator
.
QUALIFIERS
.
split
(
columnKey
,
2
)
;
if
(
columnNameParts
.
length
>
0
)
{
byte
[
]
actualColumnPrefixBytes
=
columnNameParts
[
0
]
;
if
(
Bytes
.
equals
(
columnPrefixBytes
,
actualColumnPrefixBytes
)
&&
columnNameParts
.
length
==
2
)
{
try
{
converterColumnKey
=
keyConverter
.
decode
(
columnNameParts
[
1
]
)
;
}
catch
(
IllegalArgumentException
iae
)
{
public
void
createTable
(
Admin
admin
,
Configuration
hbaseConf
)
throws
IOException
{
TableName
table
=
getTableName
(
hbaseConf
)
;
if
(
admin
.
tableExists
(
table
)
)
{
throw
new
IOException
(
+
table
.
getNameAsString
(
)
+
)
;
}
HTableDescriptor
flowRunTableDescp
=
new
HTableDescriptor
(
table
)
;
HColumnDescriptor
infoCF
=
new
HColumnDescriptor
(
FlowRunColumnFamily
.
INFO
.
getBytes
(
)
)
;
infoCF
.
setBloomFilterType
(
BloomType
.
ROWCOL
)
;
flowRunTableDescp
.
addFamily
(
infoCF
)
;
infoCF
.
setMinVersions
(
1
)
;
infoCF
.
setMaxVersions
(
DEFAULT_METRICS_MAX_VERSIONS
)
;
String
coprocessorJarPathStr
=
hbaseConf
.
get
(
YarnConfiguration
.
FLOW_RUN_COPROCESSOR_JAR_HDFS_LOCATION
,
YarnConfiguration
.
DEFAULT_HDFS_LOCATION_FLOW_RUN_COPROCESSOR_JAR
)
;
Path
coprocessorJarPath
=
new
Path
(
coprocessorJarPathStr
)
;
LOG
.
info
(
+
coprocessorJarPath
.
toString
(
)
)
;
flowRunTableDescp
.
addCoprocessor
(
+
,
coprocessorJarPath
,
Coprocessor
.
PRIORITY_USER
,
null
)
;
admin
.
createTable
(
flowRunTableDescp
)
;
public
Set
<
String
>
readEntityTypes
(
Configuration
hbaseConf
,
Connection
conn
)
throws
IOException
{
validateParams
(
)
;
augmentParams
(
hbaseConf
,
conn
)
;
Set
<
String
>
types
=
new
TreeSet
<
>
(
)
;
TimelineReaderContext
context
=
getContext
(
)
;
EntityRowKeyPrefix
prefix
=
new
EntityRowKeyPrefix
(
context
.
getClusterId
(
)
,
context
.
getUserId
(
)
,
context
.
getFlowName
(
)
,
context
.
getFlowRunId
(
)
,
context
.
getAppId
(
)
)
;
byte
[
]
currRowKey
=
prefix
.
getRowKeyPrefix
(
)
;
byte
[
]
nextRowKey
=
prefix
.
getRowKeyPrefix
(
)
;
nextRowKey
[
nextRowKey
.
length
-
1
]
++
;
FilterList
typeFilterList
=
new
FilterList
(
)
;
typeFilterList
.
addFilter
(
new
FirstKeyOnlyFilter
(
)
)
;
typeFilterList
.
addFilter
(
new
KeyOnlyFilter
(
)
)
;
typeFilterList
.
addFilter
(
new
PageFilter
(
1
)
)
;
typeFilterList
.
addFilter
(
new
FirstKeyOnlyFilter
(
)
)
;
typeFilterList
.
addFilter
(
new
KeyOnlyFilter
(
)
)
;
typeFilterList
.
addFilter
(
new
PageFilter
(
1
)
)
;
LOG
.
debug
(
,
typeFilterList
)
;
int
counter
=
0
;
while
(
true
)
{
try
(
ResultScanner
results
=
getResult
(
hbaseConf
,
conn
,
typeFilterList
,
currRowKey
,
nextRowKey
)
)
{
TimelineEntity
entity
=
parseEntityForType
(
results
.
next
(
)
)
;
if
(
entity
==
null
)
{
break
;
}
++
counter
;
if
(
!
types
.
add
(
entity
.
getType
(
)
)
)
{
LOG
.
warn
(
+
entity
.
getType
(
)
+
)
;
}
String
currType
=
entity
.
getType
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
typeFilterList
.
addFilter
(
new
KeyOnlyFilter
(
)
)
;
typeFilterList
.
addFilter
(
new
PageFilter
(
1
)
)
;
LOG
.
debug
(
,
typeFilterList
)
;
int
counter
=
0
;
while
(
true
)
{
try
(
ResultScanner
results
=
getResult
(
hbaseConf
,
conn
,
typeFilterList
,
currRowKey
,
nextRowKey
)
)
{
TimelineEntity
entity
=
parseEntityForType
(
results
.
next
(
)
)
;
if
(
entity
==
null
)
{
break
;
}
++
counter
;
if
(
!
types
.
add
(
entity
.
getType
(
)
)
)
{
LOG
.
warn
(
+
entity
.
getType
(
)
+
)
;
}
String
currType
=
entity
.
getType
(
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
LOG
.
debug
(
+
Arrays
.
toString
(
currRowKey
)
)
;
public
TimelineEntity
readEntity
(
Configuration
hbaseConf
,
Connection
conn
)
throws
IOException
{
validateParams
(
)
;
augmentParams
(
hbaseConf
,
conn
)
;
FilterList
filterList
=
constructFilterListBasedOnFields
(
new
HashSet
<
>
(
0
)
)
;
if
(
filterList
!=
null
)
{
public
Set
<
TimelineEntity
>
readEntities
(
Configuration
hbaseConf
,
Connection
conn
)
throws
IOException
{
validateParams
(
)
;
augmentParams
(
hbaseConf
,
conn
)
;
Set
<
TimelineEntity
>
entities
=
new
LinkedHashSet
<
>
(
)
;
FilterList
filterList
=
createFilterList
(
)
;
if
(
filterList
!=
null
)
{
@
Override
public
InternalScanner
preFlush
(
ObserverContext
<
RegionCoprocessorEnvironment
>
c
,
Store
store
,
InternalScanner
scanner
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
store
!=
null
)
{
@
Override
public
void
postFlush
(
ObserverContext
<
RegionCoprocessorEnvironment
>
c
,
Store
store
,
StoreFile
resultFile
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
store
!=
null
)
{
@
Override
public
InternalScanner
preCompact
(
ObserverContext
<
RegionCoprocessorEnvironment
>
e
,
Store
store
,
InternalScanner
scanner
,
ScanType
scanType
,
CompactionRequest
request
)
throws
IOException
{
FlowScannerOperation
requestOp
=
FlowScannerOperation
.
MINOR_COMPACTION
;
if
(
request
!=
null
)
{
requestOp
=
(
request
.
isMajor
(
)
?
FlowScannerOperation
.
MAJOR_COMPACTION
:
FlowScannerOperation
.
MINOR_COMPACTION
)
;
byte
[
]
currentColumnQualifier
=
CellUtil
.
cloneQualifier
(
cell
)
;
if
(
previousColumnQualifier
==
null
)
{
previousColumnQualifier
=
currentColumnQualifier
;
}
converter
=
getValueConverter
(
currentColumnQualifier
)
;
if
(
comp
.
compare
(
previousColumnQualifier
,
currentColumnQualifier
)
!=
0
)
{
addedCnt
+=
emitCells
(
cells
,
currentColumnCells
,
currentAggOp
,
converter
,
currentTimestamp
)
;
resetState
(
currentColumnCells
,
alreadySeenAggDim
)
;
previousColumnQualifier
=
currentColumnQualifier
;
currentAggOp
=
getCurrentAggOp
(
cell
)
;
converter
=
getValueConverter
(
currentColumnQualifier
)
;
}
collectCells
(
currentColumnCells
,
currentAggOp
,
cell
,
alreadySeenAggDim
,
converter
,
scannerContext
)
;
nextCell
(
scannerContext
)
;
}
if
(
(
!
currentColumnCells
.
isEmpty
(
)
)
&&
(
(
limit
<=
0
||
addedCnt
<
limit
)
)
)
{
addedCnt
+=
emitCells
(
cells
,
currentColumnCells
,
currentAggOp
,
converter
,
currentTimestamp
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
sum
=
converter
.
add
(
sum
,
currentValue
)
;
summationDone
=
true
;
if
(
LOG
.
isTraceEnabled
(
)
)
{
LOG
.
trace
(
+
sum
+
+
+
Bytes
.
toString
(
CellUtil
.
cloneQualifier
(
cell
)
)
+
+
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
cell
)
)
+
+
cell
.
getTimestamp
(
)
+
+
this
.
action
)
;
}
}
else
{
finalCells
.
add
(
cell
)
;
}
}
}
if
(
summationDone
)
{
Cell
anyCell
=
currentColumnCells
.
first
(
)
;
List
<
Tag
>
tags
=
new
ArrayList
<
Tag
>
(
)
;
Tag
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationOperation
.
SUM_FINAL
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationCompactionDimension
.
APPLICATION_ID
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
byte
[
]
tagByteArray
=
HBaseTimelineServerUtils
.
convertTagListToByteArray
(
tags
)
;
Cell
sumCell
=
HBaseTimelineServerUtils
.
createNewCell
(
CellUtil
.
cloneRow
(
anyCell
)
,
CellUtil
.
cloneFamily
(
anyCell
)
,
CellUtil
.
cloneQualifier
(
anyCell
)
,
TimestampGenerator
.
getSupplementedTimestamp
(
System
.
currentTimeMillis
(
)
,
FLOW_APP_ID
)
,
converter
.
encodeValue
(
sum
)
,
tagByteArray
)
;
if
(
LOG
.
isTraceEnabled
(
)
)
{
LOG
.
trace
(
+
sum
+
+
+
Bytes
.
toString
(
CellUtil
.
cloneQualifier
(
cell
)
)
+
+
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
cell
)
)
+
+
cell
.
getTimestamp
(
)
+
+
this
.
action
)
;
}
}
else
{
finalCells
.
add
(
cell
)
;
}
}
}
if
(
summationDone
)
{
Cell
anyCell
=
currentColumnCells
.
first
(
)
;
List
<
Tag
>
tags
=
new
ArrayList
<
Tag
>
(
)
;
Tag
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationOperation
.
SUM_FINAL
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationCompactionDimension
.
APPLICATION_ID
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
byte
[
]
tagByteArray
=
HBaseTimelineServerUtils
.
convertTagListToByteArray
(
tags
)
;
Cell
sumCell
=
HBaseTimelineServerUtils
.
createNewCell
(
CellUtil
.
cloneRow
(
anyCell
)
,
CellUtil
.
cloneFamily
(
anyCell
)
,
CellUtil
.
cloneQualifier
(
anyCell
)
,
TimestampGenerator
.
getSupplementedTimestamp
(
System
.
currentTimeMillis
(
)
,
FLOW_APP_ID
)
,
converter
.
encodeValue
(
sum
)
,
tagByteArray
)
;
finalCells
.
add
(
sumCell
)
;
if
(
LOG
.
isTraceEnabled
(
)
)
{
return
currentCell
;
}
try
{
Number
previouslyChosenCellValue
=
(
Number
)
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
previouslyChosenCell
)
)
;
Number
currentCellValue
=
(
Number
)
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
currentCell
)
)
;
switch
(
currentAggOp
)
{
case
GLOBAL_MIN
:
if
(
converter
.
compare
(
currentCellValue
,
previouslyChosenCellValue
)
<
0
)
{
return
currentCell
;
}
else
{
return
previouslyChosenCell
;
}
case
GLOBAL_MAX
:
if
(
converter
.
compare
(
currentCellValue
,
previouslyChosenCellValue
)
>
0
)
{
return
currentCell
;
}
else
{
return
previouslyChosenCell
;
}
default
:
return
currentCell
;
}
}
catch
(
IllegalArgumentException
iae
)
{
@
Override
public
InternalScanner
preFlush
(
ObserverContext
<
RegionCoprocessorEnvironment
>
c
,
Store
store
,
InternalScanner
scanner
,
FlushLifeCycleTracker
cycleTracker
)
throws
IOException
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
store
!=
null
)
{
@
Override
public
void
postFlush
(
ObserverContext
<
RegionCoprocessorEnvironment
>
c
,
Store
store
,
StoreFile
resultFile
,
FlushLifeCycleTracker
tracker
)
{
if
(
LOG
.
isDebugEnabled
(
)
)
{
if
(
store
!=
null
)
{
@
Override
public
InternalScanner
preCompact
(
ObserverContext
<
RegionCoprocessorEnvironment
>
e
,
Store
store
,
InternalScanner
scanner
,
ScanType
scanType
,
CompactionLifeCycleTracker
tracker
,
CompactionRequest
request
)
throws
IOException
{
FlowScannerOperation
requestOp
=
FlowScannerOperation
.
MINOR_COMPACTION
;
if
(
request
!=
null
)
{
requestOp
=
(
request
.
isMajor
(
)
?
FlowScannerOperation
.
MAJOR_COMPACTION
:
FlowScannerOperation
.
MINOR_COMPACTION
)
;
byte
[
]
currentColumnQualifier
=
CellUtil
.
cloneQualifier
(
cell
)
;
if
(
previousColumnQualifier
==
null
)
{
previousColumnQualifier
=
currentColumnQualifier
;
}
converter
=
getValueConverter
(
currentColumnQualifier
)
;
if
(
comp
.
compare
(
previousColumnQualifier
,
currentColumnQualifier
)
!=
0
)
{
addedCnt
+=
emitCells
(
cells
,
currentColumnCells
,
currentAggOp
,
converter
,
currentTimestamp
)
;
resetState
(
currentColumnCells
,
alreadySeenAggDim
)
;
previousColumnQualifier
=
currentColumnQualifier
;
currentAggOp
=
getCurrentAggOp
(
cell
)
;
converter
=
getValueConverter
(
currentColumnQualifier
)
;
}
collectCells
(
currentColumnCells
,
currentAggOp
,
cell
,
alreadySeenAggDim
,
converter
,
scannerContext
)
;
nextCell
(
scannerContext
)
;
}
if
(
(
!
currentColumnCells
.
isEmpty
(
)
)
&&
(
(
limit
<=
0
||
addedCnt
<
limit
)
)
)
{
addedCnt
+=
emitCells
(
cells
,
currentColumnCells
,
currentAggOp
,
converter
,
currentTimestamp
)
;
if
(
LOG
.
isDebugEnabled
(
)
)
{
summationDone
=
true
;
if
(
LOG
.
isTraceEnabled
(
)
)
{
LOG
.
trace
(
+
sum
+
+
+
Bytes
.
toString
(
CellUtil
.
cloneQualifier
(
cell
)
)
+
+
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
cell
)
)
+
+
cell
.
getTimestamp
(
)
+
+
this
.
action
)
;
}
}
else
{
finalCells
.
add
(
cell
)
;
}
}
}
if
(
summationDone
)
{
Cell
anyCell
=
currentColumnCells
.
first
(
)
;
List
<
Tag
>
tags
=
new
ArrayList
<
Tag
>
(
)
;
Tag
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationOperation
.
SUM_FINAL
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationCompactionDimension
.
APPLICATION_ID
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
byte
[
]
tagByteArray
=
HBaseTimelineServerUtils
.
convertTagListToByteArray
(
tags
)
;
Cell
sumCell
=
HBaseTimelineServerUtils
.
createNewCell
(
CellUtil
.
cloneRow
(
anyCell
)
,
CellUtil
.
cloneFamily
(
anyCell
)
,
CellUtil
.
cloneQualifier
(
anyCell
)
,
TimestampGenerator
.
getSupplementedTimestamp
(
System
.
currentTimeMillis
(
)
,
FLOW_APP_ID
)
,
converter
.
encodeValue
(
sum
)
,
tagByteArray
)
;
finalCells
.
add
(
sumCell
)
;
LOG
.
trace
(
+
sum
+
+
+
Bytes
.
toString
(
CellUtil
.
cloneQualifier
(
cell
)
)
+
+
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
cell
)
)
+
+
cell
.
getTimestamp
(
)
+
+
this
.
action
)
;
}
}
else
{
finalCells
.
add
(
cell
)
;
}
}
}
if
(
summationDone
)
{
Cell
anyCell
=
currentColumnCells
.
first
(
)
;
List
<
Tag
>
tags
=
new
ArrayList
<
Tag
>
(
)
;
Tag
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationOperation
.
SUM_FINAL
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
t
=
HBaseTimelineServerUtils
.
createTag
(
AggregationCompactionDimension
.
APPLICATION_ID
.
getTagType
(
)
,
Bytes
.
toBytes
(
FLOW_APP_ID
)
)
;
tags
.
add
(
t
)
;
byte
[
]
tagByteArray
=
HBaseTimelineServerUtils
.
convertTagListToByteArray
(
tags
)
;
Cell
sumCell
=
HBaseTimelineServerUtils
.
createNewCell
(
CellUtil
.
cloneRow
(
anyCell
)
,
CellUtil
.
cloneFamily
(
anyCell
)
,
CellUtil
.
cloneQualifier
(
anyCell
)
,
TimestampGenerator
.
getSupplementedTimestamp
(
System
.
currentTimeMillis
(
)
,
FLOW_APP_ID
)
,
converter
.
encodeValue
(
sum
)
,
tagByteArray
)
;
finalCells
.
add
(
sumCell
)
;
if
(
LOG
.
isTraceEnabled
(
)
)
{
LOG
.
trace
(
+
sum
+
+
Bytes
.
toString
(
CellUtil
.
cloneQualifier
(
sumCell
)
)
+
+
this
.
action
)
;
return
currentCell
;
}
try
{
Number
previouslyChosenCellValue
=
(
Number
)
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
previouslyChosenCell
)
)
;
Number
currentCellValue
=
(
Number
)
converter
.
decodeValue
(
CellUtil
.
cloneValue
(
currentCell
)
)
;
switch
(
currentAggOp
)
{
case
GLOBAL_MIN
:
if
(
converter
.
compare
(
currentCellValue
,
previouslyChosenCellValue
)
<
0
)
{
return
currentCell
;
}
else
{
return
previouslyChosenCell
;
}
case
GLOBAL_MAX
:
if
(
converter
.
compare
(
currentCellValue
,
previouslyChosenCellValue
)
>
0
)
{
return
currentCell
;
}
else
{
return
previouslyChosenCell
;
}
default
:
return
currentCell
;
}
}
catch
(
IllegalArgumentException
iae
)
{
if
(
host
==
null
||
host
.
isEmpty
(
)
)
{
bindAddress
=
conf
.
get
(
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_BIND_HOST
,
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_BIND_HOST
)
+
+
startPort
;
}
else
{
bindAddress
=
host
+
+
startPort
;
}
try
{
HttpServer2
.
Builder
builder
=
new
HttpServer2
.
Builder
(
)
.
setName
(
)
.
setConf
(
conf
)
.
addEndpoint
(
URI
.
create
(
(
YarnConfiguration
.
useHttps
(
conf
)
?
:
)
+
bindAddress
)
)
;
if
(
portRanges
!=
null
&&
!
portRanges
.
isEmpty
(
)
)
{
builder
.
setPortRanges
(
portRanges
)
;
}
if
(
YarnConfiguration
.
useHttps
(
conf
)
)
{
builder
=
WebAppUtils
.
loadSslConfiguration
(
builder
,
conf
)
;
}
timelineRestServer
=
builder
.
build
(
)
;
timelineRestServer
.
addJerseyResourcePackage
(
TimelineCollectorWebService
.
class
.
getPackage
(
)
.
getName
(
)
+
+
GenericExceptionHandler
.
class
.
getPackage
(
)
.
getName
(
)
+
+
YarnJacksonJaxbJsonProvider
.
class
.
getPackage
(
)
.
getName
(
)
,
)
;
timelineRestServer
.
setAttribute
(
COLLECTOR_MANAGER_ATTR_KEY
,
this
)
;
timelineRestServer
.
start
(
)
;
}
catch
(
Exception
e
)
{
private
void
reportNewCollectorInfoToNM
(
ApplicationId
appId
,
org
.
apache
.
hadoop
.
yarn
.
api
.
records
.
Token
token
)
throws
YarnException
,
IOException
{
ReportNewCollectorInfoRequest
request
=
ReportNewCollectorInfoRequest
.
newInstance
(
appId
,
this
.
timelineRestServerBindAddress
,
token
)
;
private
void
updateTimelineCollectorContext
(
ApplicationId
appId
,
TimelineCollector
collector
)
throws
YarnException
,
IOException
{
GetTimelineCollectorContextRequest
request
=
GetTimelineCollectorContextRequest
.
newInstance
(
appId
)
;
private
void
updateTimelineCollectorContext
(
ApplicationId
appId
,
TimelineCollector
collector
)
throws
YarnException
,
IOException
{
GetTimelineCollectorContextRequest
request
=
GetTimelineCollectorContextRequest
.
newInstance
(
appId
)
;
LOG
.
info
(
+
appId
)
;
GetTimelineCollectorContextResponse
response
=
getNMCollectorService
(
)
.
getTimelineCollectorContext
(
request
)
;
String
userId
=
response
.
getUserId
(
)
;
if
(
userId
!=
null
&&
!
userId
.
isEmpty
(
)
)
{
GetTimelineCollectorContextResponse
response
=
getNMCollectorService
(
)
.
getTimelineCollectorContext
(
request
)
;
String
userId
=
response
.
getUserId
(
)
;
if
(
userId
!=
null
&&
!
userId
.
isEmpty
(
)
)
{
LOG
.
debug
(
,
userId
)
;
collector
.
getTimelineEntityContext
(
)
.
setUserId
(
userId
)
;
}
String
flowName
=
response
.
getFlowName
(
)
;
if
(
flowName
!=
null
&&
!
flowName
.
isEmpty
(
)
)
{
LOG
.
debug
(
,
flowName
)
;
collector
.
getTimelineEntityContext
(
)
.
setFlowName
(
flowName
)
;
}
String
flowVersion
=
response
.
getFlowVersion
(
)
;
if
(
flowVersion
!=
null
&&
!
flowVersion
.
isEmpty
(
)
)
{
LOG
.
debug
(
,
flowVersion
)
;
collector
.
getTimelineEntityContext
(
)
.
setFlowVersion
(
flowVersion
)
;
}
long
flowRunId
=
response
.
getFlowRunId
(
)
;
if
(
flowRunId
!=
0L
)
{
@
VisibleForTesting
protected
CollectorNodemanagerProtocol
getNMCollectorService
(
)
{
if
(
nmCollectorService
==
null
)
{
synchronized
(
this
)
{
if
(
nmCollectorService
==
null
)
{
Configuration
conf
=
getConfig
(
)
;
InetSocketAddress
nmCollectorServiceAddress
=
conf
.
getSocketAddr
(
YarnConfiguration
.
NM_BIND_HOST
,
YarnConfiguration
.
NM_COLLECTOR_SERVICE_ADDRESS
,
YarnConfiguration
.
DEFAULT_NM_COLLECTOR_SERVICE_ADDRESS
,
YarnConfiguration
.
DEFAULT_NM_COLLECTOR_SERVICE_PORT
)
;
@
VisibleForTesting
protected
Future
removeApplicationCollector
(
final
ContainerId
containerId
)
{
final
ApplicationId
appId
=
containerId
.
getApplicationAttemptId
(
)
.
getApplicationId
(
)
;
return
scheduler
.
schedule
(
new
Runnable
(
)
{
public
void
run
(
)
{
boolean
shouldRemoveApplication
=
false
;
synchronized
(
appIdToContainerId
)
{
Set
<
ContainerId
>
masterContainers
=
appIdToContainerId
.
get
(
appId
)
;
if
(
masterContainers
==
null
)
{
public
TimelineWriteResponse
putEntities
(
TimelineEntities
entities
,
UserGroupInformation
callerUgi
)
throws
IOException
{
public
TimelineWriteResponse
putDomain
(
TimelineDomain
domain
,
UserGroupInformation
callerUgi
)
throws
IOException
{
public
void
putEntitiesAsync
(
TimelineEntities
entities
,
UserGroupInformation
callerUgi
)
throws
IOException
{
private
TimelineWriter
createTimelineWriter
(
final
Configuration
conf
)
{
String
timelineWriterClassName
=
conf
.
get
(
YarnConfiguration
.
TIMELINE_SERVICE_WRITER_CLASS
,
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_WRITER_CLASS
)
;
public
TimelineCollector
putIfAbsent
(
ApplicationId
appId
,
TimelineCollector
collector
)
{
TimelineCollector
collectorInTable
=
null
;
synchronized
(
collectors
)
{
collectorInTable
=
collectors
.
get
(
appId
)
;
if
(
collectorInTable
==
null
)
{
try
{
collector
.
init
(
getConfig
(
)
)
;
collector
.
setWriter
(
writer
)
;
collector
.
start
(
)
;
collectors
.
put
(
appId
,
collector
)
;
TimelineCollector
collectorInTable
=
null
;
synchronized
(
collectors
)
{
collectorInTable
=
collectors
.
get
(
appId
)
;
if
(
collectorInTable
==
null
)
{
try
{
collector
.
init
(
getConfig
(
)
)
;
collector
.
setWriter
(
writer
)
;
collector
.
start
(
)
;
collectors
.
put
(
appId
,
collector
)
;
LOG
.
info
(
+
appId
+
)
;
collectorInTable
=
collector
;
postPut
(
appId
,
collectorInTable
)
;
}
catch
(
Exception
e
)
{
throw
new
YarnRuntimeException
(
e
)
;
}
}
else
{
public
boolean
remove
(
ApplicationId
appId
)
{
TimelineCollector
collector
=
collectors
.
remove
(
appId
)
;
if
(
collector
==
null
)
{
@
PUT
@
Path
(
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
public
Response
putEntities
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
QueryParam
(
)
String
async
,
@
QueryParam
(
)
String
isSubAppEntities
,
@
QueryParam
(
)
String
appId
,
TimelineEntities
entities
)
{
init
(
res
)
;
UserGroupInformation
callerUgi
=
getUser
(
req
)
;
boolean
isAsync
=
async
!=
null
&&
async
.
trim
(
)
.
equalsIgnoreCase
(
)
;
if
(
callerUgi
==
null
)
{
String
msg
=
;
UserGroupInformation
callerUgi
=
getUser
(
req
)
;
boolean
isAsync
=
async
!=
null
&&
async
.
trim
(
)
.
equalsIgnoreCase
(
)
;
if
(
callerUgi
==
null
)
{
String
msg
=
;
LOG
.
error
(
msg
)
;
throw
new
ForbiddenException
(
msg
)
;
}
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
try
{
ApplicationId
appID
=
parseApplicationId
(
appId
)
;
if
(
appID
==
null
)
{
return
Response
.
status
(
Response
.
Status
.
BAD_REQUEST
)
.
build
(
)
;
}
NodeTimelineCollectorManager
collectorManager
=
(
NodeTimelineCollectorManager
)
context
.
getAttribute
(
NodeTimelineCollectorManager
.
COLLECTOR_MANAGER_ATTR_KEY
)
;
TimelineCollector
collector
=
collectorManager
.
get
(
appID
)
;
if
(
collector
==
null
)
{
if
(
appID
==
null
)
{
return
Response
.
status
(
Response
.
Status
.
BAD_REQUEST
)
.
build
(
)
;
}
NodeTimelineCollectorManager
collectorManager
=
(
NodeTimelineCollectorManager
)
context
.
getAttribute
(
NodeTimelineCollectorManager
.
COLLECTOR_MANAGER_ATTR_KEY
)
;
TimelineCollector
collector
=
collectorManager
.
get
(
appID
)
;
if
(
collector
==
null
)
{
LOG
.
error
(
+
appId
+
)
;
throw
new
NotFoundException
(
+
appId
+
)
;
}
if
(
isAsync
)
{
collector
.
putEntitiesAsync
(
processTimelineEntities
(
entities
,
appId
,
Boolean
.
valueOf
(
isSubAppEntities
)
)
,
callerUgi
)
;
}
else
{
collector
.
putEntities
(
processTimelineEntities
(
entities
,
appId
,
Boolean
.
valueOf
(
isSubAppEntities
)
)
,
callerUgi
)
;
}
succeeded
=
true
;
return
Response
.
ok
(
)
.
build
(
)
;
}
catch
(
NotFoundException
|
ForbiddenException
e
)
{
throw
new
WebApplicationException
(
e
,
Response
.
Status
.
INTERNAL_SERVER_ERROR
)
;
NodeTimelineCollectorManager
collectorManager
=
(
NodeTimelineCollectorManager
)
context
.
getAttribute
(
NodeTimelineCollectorManager
.
COLLECTOR_MANAGER_ATTR_KEY
)
;
TimelineCollector
collector
=
collectorManager
.
get
(
appID
)
;
if
(
collector
==
null
)
{
LOG
.
error
(
+
appId
+
)
;
throw
new
NotFoundException
(
+
appId
+
)
;
}
if
(
isAsync
)
{
collector
.
putEntitiesAsync
(
processTimelineEntities
(
entities
,
appId
,
Boolean
.
valueOf
(
isSubAppEntities
)
)
,
callerUgi
)
;
}
else
{
collector
.
putEntities
(
processTimelineEntities
(
entities
,
appId
,
Boolean
.
valueOf
(
isSubAppEntities
)
)
,
callerUgi
)
;
}
succeeded
=
true
;
return
Response
.
ok
(
)
.
build
(
)
;
}
catch
(
NotFoundException
|
ForbiddenException
e
)
{
throw
new
WebApplicationException
(
e
,
Response
.
Status
.
INTERNAL_SERVER_ERROR
)
;
}
catch
(
IOException
e
)
{
LOG
.
error
(
,
e
)
;
@
PUT
@
Path
(
)
@
Consumes
(
{
MediaType
.
APPLICATION_JSON
}
)
public
Response
putDomain
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
QueryParam
(
)
String
appId
,
TimelineDomain
domain
)
{
init
(
res
)
;
UserGroupInformation
callerUgi
=
getUser
(
req
)
;
if
(
callerUgi
==
null
)
{
String
msg
=
;
}
try
{
ApplicationId
appID
=
parseApplicationId
(
appId
)
;
if
(
appID
==
null
)
{
return
Response
.
status
(
Response
.
Status
.
BAD_REQUEST
)
.
build
(
)
;
}
NodeTimelineCollectorManager
collectorManager
=
(
NodeTimelineCollectorManager
)
context
.
getAttribute
(
NodeTimelineCollectorManager
.
COLLECTOR_MANAGER_ATTR_KEY
)
;
TimelineCollector
collector
=
collectorManager
.
get
(
appID
)
;
if
(
collector
==
null
)
{
LOG
.
error
(
+
appId
+
)
;
throw
new
NotFoundException
(
+
appId
+
)
;
}
domain
.
setOwner
(
callerUgi
.
getShortUserName
(
)
)
;
collector
.
putDomain
(
domain
,
callerUgi
)
;
return
Response
.
ok
(
)
.
build
(
)
;
}
catch
(
NotFoundException
e
)
{
throw
new
WebApplicationException
(
e
,
Response
.
Status
.
INTERNAL_SERVER_ERROR
)
;
}
catch
(
IOException
e
)
{
private
TimelineReader
createTimelineReaderStore
(
final
Configuration
conf
)
{
String
timelineReaderClassName
=
conf
.
get
(
YarnConfiguration
.
TIMELINE_SERVICE_READER_CLASS
,
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_READER_CLASS
)
;
if
(
host
==
null
||
host
.
isEmpty
(
)
)
{
hostProperty
=
YarnConfiguration
.
TIMELINE_SERVICE_BIND_HOST
;
}
String
bindAddress
=
WebAppUtils
.
getWebAppBindURL
(
conf
,
hostProperty
,
webAppURLWithoutScheme
)
;
LOG
.
info
(
+
bindAddress
)
;
try
{
String
httpScheme
=
WebAppUtils
.
getHttpSchemePrefix
(
conf
)
;
HttpServer2
.
Builder
builder
=
new
HttpServer2
.
Builder
(
)
.
setName
(
)
.
setConf
(
conf
)
.
addEndpoint
(
URI
.
create
(
httpScheme
+
bindAddress
)
)
;
if
(
httpScheme
.
equals
(
WebAppUtils
.
HTTPS_PREFIX
)
)
{
WebAppUtils
.
loadSslConfiguration
(
builder
,
conf
)
;
}
readerWebServer
=
builder
.
build
(
)
;
readerWebServer
.
addJerseyResourcePackage
(
TimelineReaderWebServices
.
class
.
getPackage
(
)
.
getName
(
)
+
+
GenericExceptionHandler
.
class
.
getPackage
(
)
.
getName
(
)
+
+
YarnJacksonJaxbJsonProvider
.
class
.
getPackage
(
)
.
getName
(
)
+
+
LogWebService
.
class
.
getPackage
(
)
.
getName
(
)
,
)
;
readerWebServer
.
setAttribute
(
TIMELINE_READER_MANAGER_ATTR
,
timelineReaderManager
)
;
readerWebServer
.
start
(
)
;
}
catch
(
Exception
e
)
{
String
msg
=
;
private
static
void
handleException
(
Exception
e
,
String
url
,
long
startTime
,
String
invalidNumMsg
)
throws
BadRequestException
,
WebApplicationException
{
long
endTime
=
Time
.
monotonicNow
(
)
;
long
endTime
=
Time
.
monotonicNow
(
)
;
LOG
.
info
(
+
url
+
+
(
endTime
-
startTime
)
+
)
;
if
(
e
instanceof
NumberFormatException
)
{
throw
new
BadRequestException
(
invalidNumMsg
+
)
;
}
else
if
(
e
instanceof
IllegalArgumentException
)
{
throw
new
BadRequestException
(
e
.
getMessage
(
)
==
null
?
:
e
.
getMessage
(
)
)
;
}
else
if
(
e
instanceof
NotFoundException
)
{
throw
(
NotFoundException
)
e
;
}
else
if
(
e
instanceof
TimelineParseException
)
{
throw
new
BadRequestException
(
e
.
getMessage
(
)
==
null
?
:
e
.
getMessage
(
)
)
;
}
else
if
(
e
instanceof
BadRequestException
)
{
throw
(
BadRequestException
)
e
;
}
else
if
(
e
instanceof
ForbiddenException
)
{
throw
(
ForbiddenException
)
e
;
}
else
{
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getEntities
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
uId
,
@
PathParam
(
)
String
entityType
,
@
QueryParam
(
)
String
limit
,
@
QueryParam
(
)
String
createdTimeStart
,
@
QueryParam
(
)
String
createdTimeEnd
,
@
QueryParam
(
)
String
relatesTo
,
@
QueryParam
(
)
String
isRelatedTo
,
@
QueryParam
(
)
String
infofilters
,
@
QueryParam
(
)
String
conffilters
,
@
QueryParam
(
)
String
metricfilters
,
@
QueryParam
(
)
String
eventfilters
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
,
@
QueryParam
(
)
String
fromId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
APPLICATION_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
context
.
setEntityType
(
TimelineReaderWebServicesUtils
.
parseStr
(
entityType
)
)
;
entities
=
timelineReaderManager
.
getEntities
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineEntityFilters
(
limit
,
createdTimeStart
,
createdTimeEnd
,
relatesTo
,
isRelatedTo
,
infofilters
,
conffilters
,
metricfilters
,
eventfilters
,
fromId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForGenericEntities
(
entities
,
callerUGI
,
entityType
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getEntities
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
appId
,
@
PathParam
(
)
String
entityType
,
@
QueryParam
(
)
String
userId
,
@
QueryParam
(
)
String
flowName
,
@
QueryParam
(
)
String
flowRunId
,
@
QueryParam
(
)
String
limit
,
@
QueryParam
(
)
String
createdTimeStart
,
@
QueryParam
(
)
String
createdTimeEnd
,
@
QueryParam
(
)
String
relatesTo
,
@
QueryParam
(
)
String
isRelatedTo
,
@
QueryParam
(
)
String
infofilters
,
@
QueryParam
(
)
String
conffilters
,
@
QueryParam
(
)
String
metricfilters
,
@
QueryParam
(
)
String
eventfilters
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
,
@
QueryParam
(
)
String
fromId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
TimelineReaderContext
context
=
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
appId
,
entityType
,
null
,
null
)
;
entities
=
timelineReaderManager
.
getEntities
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineEntityFilters
(
limit
,
createdTimeStart
,
createdTimeEnd
,
relatesTo
,
isRelatedTo
,
infofilters
,
conffilters
,
metricfilters
,
eventfilters
,
fromId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForGenericEntities
(
entities
,
callerUGI
,
entityType
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
TimelineEntity
getEntity
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
uId
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
GENERIC_ENTITY_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForGenericEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
TimelineEntity
entity
=
null
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
GENERIC_ENTITY_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForGenericEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
LOG
.
info
(
+
url
+
+
latency
+
)
;
}
if
(
entity
==
null
)
{
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
TimelineEntity
getEntity
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
appId
,
@
PathParam
(
)
String
entityType
,
@
PathParam
(
)
String
entityId
,
@
QueryParam
(
)
String
userId
,
@
QueryParam
(
)
String
flowName
,
@
QueryParam
(
)
String
flowRunId
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
,
@
QueryParam
(
)
String
entityIdPrefix
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
LOG
.
info
(
+
url
+
+
TimelineReaderWebServicesUtils
.
getUserName
(
callerUGI
)
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
entity
=
timelineReaderManager
.
getEntity
(
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
appId
,
entityType
,
entityIdPrefix
,
entityId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForGenericEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
entity
=
timelineReaderManager
.
getEntity
(
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
appId
,
entityType
,
entityIdPrefix
,
entityId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForGenericEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
LOG
.
info
(
+
url
+
+
latency
+
)
;
}
if
(
entity
==
null
)
{
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
TimelineEntity
getFlowRun
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
uId
,
@
QueryParam
(
)
String
metricsToRetrieve
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
FLOWRUN_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
checkAccess
(
timelineReaderManager
,
callerUGI
,
context
.
getUserId
(
)
)
;
context
.
setEntityType
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
)
;
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
null
,
metricsToRetrieve
,
null
,
null
,
null
,
null
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
FLOWRUN_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
checkAccess
(
timelineReaderManager
,
callerUGI
,
context
.
getUserId
(
)
)
;
context
.
setEntityType
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
)
;
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
null
,
metricsToRetrieve
,
null
,
null
,
null
,
null
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
LOG
.
info
(
+
url
+
+
latency
+
)
;
}
if
(
entity
==
null
)
{
@
GET
@
Path
(
+
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
TimelineEntity
getFlowRun
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
userId
,
@
PathParam
(
)
String
flowName
,
@
PathParam
(
)
String
flowRunId
,
@
QueryParam
(
)
String
metricsToRetrieve
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
TimelineReaderContext
context
=
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
null
,
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
null
,
null
)
;
checkAccess
(
timelineReaderManager
,
callerUGI
,
context
.
getUserId
(
)
)
;
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
null
,
metricsToRetrieve
,
null
,
null
,
null
,
null
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
TimelineReaderContext
context
=
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
null
,
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
null
,
null
)
;
checkAccess
(
timelineReaderManager
,
callerUGI
,
context
.
getUserId
(
)
)
;
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
null
,
metricsToRetrieve
,
null
,
null
,
null
,
null
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
LOG
.
info
(
+
url
+
+
latency
+
)
;
}
if
(
entity
==
null
)
{
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getFlowRuns
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
uId
,
@
QueryParam
(
)
String
limit
,
@
QueryParam
(
)
String
createdTimeStart
,
@
QueryParam
(
)
String
createdTimeEnd
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
fromId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
FLOW_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
checkAccess
(
timelineReaderManager
,
callerUGI
,
context
.
getUserId
(
)
)
;
context
.
setEntityType
(
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
)
;
entities
=
timelineReaderManager
.
getEntities
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineEntityFilters
(
limit
,
createdTimeStart
,
createdTimeEnd
,
null
,
null
,
null
,
null
,
null
,
null
,
fromId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
null
,
metricsToRetrieve
,
fields
,
null
,
null
,
null
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getFlowRuns
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
userId
,
@
PathParam
(
)
String
flowName
,
@
QueryParam
(
)
String
limit
,
@
QueryParam
(
)
String
createdTimeStart
,
@
QueryParam
(
)
String
createdTimeEnd
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
fromId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
TimelineReaderContext
timelineReaderContext
=
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
null
,
null
,
TimelineEntityType
.
YARN_FLOW_RUN
.
toString
(
)
,
null
,
null
)
;
checkAccess
(
timelineReaderManager
,
callerUGI
,
timelineReaderContext
.
getUserId
(
)
)
;
entities
=
timelineReaderManager
.
getEntities
(
timelineReaderContext
,
TimelineReaderWebServicesUtils
.
createTimelineEntityFilters
(
limit
,
createdTimeStart
,
createdTimeEnd
,
null
,
null
,
null
,
null
,
null
,
null
,
fromId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
null
,
metricsToRetrieve
,
fields
,
null
,
null
,
null
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getFlows
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
QueryParam
(
)
String
limit
,
@
QueryParam
(
)
String
dateRange
,
@
QueryParam
(
)
String
fromId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
DateRange
range
=
parseDateRange
(
dateRange
)
;
TimelineEntityFilters
entityFilters
=
TimelineReaderWebServicesUtils
.
createTimelineEntityFilters
(
limit
,
range
.
dateStart
,
range
.
dateEnd
,
null
,
null
,
null
,
null
,
null
,
null
,
fromId
)
;
entities
=
timelineReaderManager
.
getEntities
(
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
null
,
null
,
null
,
null
,
TimelineEntityType
.
YARN_FLOW_ACTIVITY
.
toString
(
)
,
null
,
null
)
,
entityFilters
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
null
,
null
,
null
,
null
,
null
,
null
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
TimelineEntity
getApp
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
uId
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
APPLICATION_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
context
.
setEntityType
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
)
;
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForAppEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
APPLICATION_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
context
.
setEntityType
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
)
;
entity
=
timelineReaderManager
.
getEntity
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForAppEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
LOG
.
info
(
+
url
+
+
latency
+
)
;
}
if
(
entity
==
null
)
{
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
TimelineEntity
getApp
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
appId
,
@
QueryParam
(
)
String
flowName
,
@
QueryParam
(
)
String
flowRunId
,
@
QueryParam
(
)
String
userId
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
LOG
.
info
(
+
url
+
+
TimelineReaderWebServicesUtils
.
getUserName
(
callerUGI
)
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
entity
=
timelineReaderManager
.
getEntity
(
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
appId
,
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
null
,
null
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForAppEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
TimelineEntity
entity
=
null
;
try
{
entity
=
timelineReaderManager
.
getEntity
(
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
appId
,
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
,
null
,
null
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForAppEntity
(
entity
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
LOG
.
info
(
+
url
+
+
latency
+
)
;
}
if
(
entity
==
null
)
{
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getFlowRunApps
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
uId
,
@
QueryParam
(
)
String
limit
,
@
QueryParam
(
)
String
createdTimeStart
,
@
QueryParam
(
)
String
createdTimeEnd
,
@
QueryParam
(
)
String
relatesTo
,
@
QueryParam
(
)
String
isRelatedTo
,
@
QueryParam
(
)
String
infofilters
,
@
QueryParam
(
)
String
conffilters
,
@
QueryParam
(
)
String
metricfilters
,
@
QueryParam
(
)
String
eventfilters
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
,
@
QueryParam
(
)
String
fromId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
TimelineReaderContext
context
=
TimelineUIDConverter
.
FLOWRUN_UID
.
decodeUID
(
uId
)
;
if
(
context
==
null
)
{
throw
new
BadRequestException
(
+
uId
)
;
}
checkAccess
(
timelineReaderManager
,
callerUGI
,
context
.
getUserId
(
)
)
;
context
.
setEntityType
(
TimelineEntityType
.
YARN_APPLICATION
.
toString
(
)
)
;
entities
=
timelineReaderManager
.
getEntities
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineEntityFilters
(
limit
,
createdTimeStart
,
createdTimeEnd
,
relatesTo
,
isRelatedTo
,
infofilters
,
conffilters
,
metricfilters
,
eventfilters
,
fromId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
)
public
Set
<
String
>
getEntityTypes
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
appId
,
@
QueryParam
(
)
String
flowName
,
@
QueryParam
(
)
String
flowRunId
,
@
QueryParam
(
)
String
userId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
String
>
results
=
null
;
try
{
TimelineReaderContext
context
=
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
userId
,
flowName
,
flowRunId
,
appId
,
null
,
null
,
null
)
;
results
=
timelineReaderManager
.
getEntityTypes
(
context
)
;
checkAccess
(
getTimelineReaderManager
(
)
,
callerUGI
,
context
.
getUserId
(
)
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntityTypesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getSubAppEntities
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
userId
,
@
PathParam
(
)
String
entityType
,
@
QueryParam
(
)
String
limit
,
@
QueryParam
(
)
String
createdTimeStart
,
@
QueryParam
(
)
String
createdTimeEnd
,
@
QueryParam
(
)
String
relatesTo
,
@
QueryParam
(
)
String
isRelatedTo
,
@
QueryParam
(
)
String
infofilters
,
@
QueryParam
(
)
String
conffilters
,
@
QueryParam
(
)
String
metricfilters
,
@
QueryParam
(
)
String
eventfilters
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
,
@
QueryParam
(
)
String
fromId
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
TimelineReaderContext
context
=
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
null
,
null
,
null
,
null
,
entityType
,
null
,
null
,
userId
)
;
entities
=
timelineReaderManager
.
getEntities
(
context
,
TimelineReaderWebServicesUtils
.
createTimelineEntityFilters
(
limit
,
createdTimeStart
,
createdTimeEnd
,
relatesTo
,
isRelatedTo
,
infofilters
,
conffilters
,
metricfilters
,
eventfilters
,
fromId
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForSubAppEntities
(
entities
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
@
GET
@
Path
(
)
@
Produces
(
MediaType
.
APPLICATION_JSON
+
+
JettyUtils
.
UTF_8
)
public
Set
<
TimelineEntity
>
getSubAppEntities
(
@
Context
HttpServletRequest
req
,
@
Context
HttpServletResponse
res
,
@
PathParam
(
)
String
clusterId
,
@
PathParam
(
)
String
userId
,
@
PathParam
(
)
String
entityType
,
@
PathParam
(
)
String
entityId
,
@
QueryParam
(
)
String
confsToRetrieve
,
@
QueryParam
(
)
String
metricsToRetrieve
,
@
QueryParam
(
)
String
fields
,
@
QueryParam
(
)
String
metricsLimit
,
@
QueryParam
(
)
String
metricsTimeStart
,
@
QueryParam
(
)
String
metricsTimeEnd
,
@
QueryParam
(
)
String
entityIdPrefix
)
{
String
url
=
req
.
getRequestURI
(
)
+
(
req
.
getQueryString
(
)
==
null
?
:
QUERY_STRING_SEP
+
req
.
getQueryString
(
)
)
;
UserGroupInformation
callerUGI
=
TimelineReaderWebServicesUtils
.
getUser
(
req
)
;
long
startTime
=
Time
.
monotonicNow
(
)
;
boolean
succeeded
=
false
;
init
(
res
)
;
TimelineReaderManager
timelineReaderManager
=
getTimelineReaderManager
(
)
;
Set
<
TimelineEntity
>
entities
=
null
;
try
{
TimelineReaderContext
context
=
TimelineReaderWebServicesUtils
.
createTimelineReaderContext
(
clusterId
,
null
,
null
,
null
,
null
,
entityType
,
entityIdPrefix
,
entityId
,
userId
)
;
entities
=
timelineReaderManager
.
getEntities
(
context
,
new
TimelineEntityFilters
.
Builder
(
)
.
build
(
)
,
TimelineReaderWebServicesUtils
.
createTimelineDataToRetrieve
(
confsToRetrieve
,
metricsToRetrieve
,
fields
,
metricsLimit
,
metricsTimeStart
,
metricsTimeEnd
)
)
;
checkAccessForSubAppEntities
(
entities
,
callerUGI
)
;
succeeded
=
true
;
}
catch
(
Exception
e
)
{
handleException
(
e
,
url
,
startTime
,
+
)
;
}
finally
{
long
latency
=
Time
.
monotonicNow
(
)
-
startTime
;
METRICS
.
addGetEntitiesLatency
(
latency
,
succeeded
)
;
static
boolean
validateAuthUserWithEntityUser
(
TimelineReaderManager
readerManager
,
UserGroupInformation
ugi
,
String
entityUser
)
{
String
authUser
=
TimelineReaderWebServicesUtils
.
getUserName
(
ugi
)
;
String
requestedUser
=
TimelineReaderWebServicesUtils
.
parseStr
(
entityUser
)
;
isWhitelistReadAuthEnabled
=
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_READ_AUTH_ENABLED
;
}
else
{
isWhitelistReadAuthEnabled
=
Boolean
.
valueOf
(
isWhitelistReadAuthEnabledStr
)
;
}
if
(
isWhitelistReadAuthEnabled
)
{
String
listAllowedUsers
=
conf
.
getInitParameter
(
YarnConfiguration
.
TIMELINE_SERVICE_READ_ALLOWED_USERS
)
;
if
(
StringUtils
.
isEmpty
(
listAllowedUsers
)
)
{
listAllowedUsers
=
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_READ_ALLOWED_USERS
;
}
LOG
.
info
(
+
listAllowedUsers
)
;
allowedUsersAclList
=
new
AccessControlList
(
listAllowedUsers
)
;
LOG
.
info
(
+
allowedUsersAclList
.
getUsers
(
)
)
;
String
adminAclListStr
=
conf
.
getInitParameter
(
YarnConfiguration
.
YARN_ADMIN_ACL
)
;
if
(
StringUtils
.
isEmpty
(
adminAclListStr
)
)
{
adminAclListStr
=
TimelineReaderWhitelistAuthorizationFilter
.
EMPTY_STRING
;
LOG
.
info
(
)
;
}
adminAclList
=
new
AccessControlList
(
adminAclListStr
)
;
@
VisibleForTesting
int
createTimelineSchema
(
String
[
]
args
,
Configuration
conf
)
throws
Exception
{
String
schemaCreatorClassName
=
conf
.
get
(
YarnConfiguration
.
TIMELINE_SERVICE_SCHEMA_CREATOR_CLASS
,
YarnConfiguration
.
DEFAULT_TIMELINE_SERVICE_SCHEMA_CREATOR_CLASS
)
;
SubjectPublicKeyInfo
subPubKeyInfo
=
SubjectPublicKeyInfo
.
getInstance
(
publicKey
.
getEncoded
(
)
)
;
X509v3CertificateBuilder
certBuilder
=
new
X509v3CertificateBuilder
(
issuer
,
new
BigInteger
(
64
,
srand
)
,
from
,
to
,
subject
,
subPubKeyInfo
)
;
AlgorithmIdentifier
digAlgId
=
new
DefaultDigestAlgorithmIdentifierFinder
(
)
.
find
(
SIG_ALG_ID
)
;
ContentSigner
contentSigner
;
try
{
contentSigner
=
new
BcRSAContentSignerBuilder
(
SIG_ALG_ID
,
digAlgId
)
.
build
(
PrivateKeyFactory
.
createKey
(
privateKey
.
getEncoded
(
)
)
)
;
}
catch
(
OperatorCreationException
oce
)
{
throw
new
GeneralSecurityException
(
oce
)
;
}
if
(
isCa
)
{
certBuilder
.
addExtension
(
Extension
.
basicConstraints
,
true
,
new
BasicConstraints
(
0
)
)
;
}
else
{
certBuilder
.
addExtension
(
Extension
.
basicConstraints
,
true
,
new
BasicConstraints
(
false
)
)
;
certBuilder
.
addExtension
(
Extension
.
authorityKeyIdentifier
,
false
,
new
JcaX509ExtensionUtils
(
)
.
createAuthorityKeyIdentifier
(
caCert
)
)
;
}
X509CertificateHolder
certHolder
=
certBuilder
.
build
(
contentSigner
)
;
X509Certificate
cert
=
new
JcaX509CertificateConverter
(
)
.
setProvider
(
)
.
getCertificate
(
certHolder
)
;
private
void
createCACertAndKeyPair
(
)
throws
GeneralSecurityException
,
IOException
{
Date
from
=
new
Date
(
)
;
Date
to
=
new
GregorianCalendar
(
2037
,
Calendar
.
DECEMBER
,
31
)
.
getTime
(
)
;
KeyPairGenerator
keyGen
=
KeyPairGenerator
.
getInstance
(
)
;
keyGen
.
initialize
(
2048
)
;
caKeyPair
=
keyGen
.
genKeyPair
(
)
;
String
subject
=
+
UUID
.
randomUUID
(
)
;
caCert
=
createCert
(
true
,
subject
,
subject
,
from
,
to
,
caKeyPair
.
getPublic
(
)
,
caKeyPair
.
getPrivate
(
)
)
;
public
static
void
sendRedirect
(
HttpServletRequest
request
,
HttpServletResponse
response
,
String
target
)
throws
IOException
{
}
else
{
LOG
.
warn
(
+
CommonConfigurationKeys
.
HADOOP_SECURITY_AUTHENTICATION
+
+
auth
)
;
}
String
proxy
=
WebAppUtils
.
getProxyHostAndPort
(
conf
)
;
String
[
]
proxyParts
=
proxy
.
split
(
)
;
proxyHost
=
proxyParts
[
0
]
;
fetcher
=
new
AppReportFetcher
(
conf
)
;
bindAddress
=
conf
.
get
(
YarnConfiguration
.
PROXY_ADDRESS
)
;
if
(
bindAddress
==
null
||
bindAddress
.
isEmpty
(
)
)
{
throw
new
YarnRuntimeException
(
YarnConfiguration
.
PROXY_ADDRESS
+
)
;
}
String
[
]
parts
=
StringUtils
.
split
(
bindAddress
,
':'
)
;
port
=
0
;
if
(
parts
.
length
==
2
)
{
bindAddress
=
parts
[
0
]
;
port
=
Integer
.
parseInt
(
parts
[
1
]
)
;
}
String
bindHost
=
conf
.
getTrimmed
(
YarnConfiguration
.
PROXY_BIND_HOST
,
null
)
;
}
String
proxy
=
WebAppUtils
.
getProxyHostAndPort
(
conf
)
;
String
[
]
proxyParts
=
proxy
.
split
(
)
;
proxyHost
=
proxyParts
[
0
]
;
fetcher
=
new
AppReportFetcher
(
conf
)
;
bindAddress
=
conf
.
get
(
YarnConfiguration
.
PROXY_ADDRESS
)
;
if
(
bindAddress
==
null
||
bindAddress
.
isEmpty
(
)
)
{
throw
new
YarnRuntimeException
(
YarnConfiguration
.
PROXY_ADDRESS
+
)
;
}
String
[
]
parts
=
StringUtils
.
split
(
bindAddress
,
':'
)
;
port
=
0
;
if
(
parts
.
length
==
2
)
{
bindAddress
=
parts
[
0
]
;
port
=
Integer
.
parseInt
(
parts
[
1
]
)
;
}
String
bindHost
=
conf
.
getTrimmed
(
YarnConfiguration
.
PROXY_BIND_HOST
,
null
)
;
if
(
bindHost
!=
null
)
{
LOG
.
debug
(
,
YarnConfiguration
.
PROXY_BIND_HOST
)
;
boolean
checkUser
=
securityEnabled
&&
(
!
userWasWarned
||
!
userApproved
)
;
FetchedAppReport
fetchedAppReport
;
try
{
fetchedAppReport
=
getFetchedAppReport
(
id
)
;
}
catch
(
ApplicationNotFoundException
e
)
{
fetchedAppReport
=
null
;
}
ApplicationReport
applicationReport
=
null
;
if
(
fetchedAppReport
!=
null
)
{
applicationReport
=
fetchedAppReport
.
getApplicationReport
(
)
;
}
if
(
applicationReport
==
null
)
{
LOG
.
warn
(
,
remoteUser
,
id
)
;
URI
toFetch
=
ProxyUriUtils
.
getUriFromTrackingPlugins
(
id
,
this
.
trackingUriPlugins
)
;
if
(
toFetch
!=
null
)
{
ProxyUtils
.
sendRedirect
(
req
,
resp
,
toFetch
.
toString
(
)
)
;
return
;
}
catch
(
ApplicationNotFoundException
e
)
{
fetchedAppReport
=
null
;
}
ApplicationReport
applicationReport
=
null
;
if
(
fetchedAppReport
!=
null
)
{
applicationReport
=
fetchedAppReport
.
getApplicationReport
(
)
;
}
if
(
applicationReport
==
null
)
{
LOG
.
warn
(
,
remoteUser
,
id
)
;
URI
toFetch
=
ProxyUriUtils
.
getUriFromTrackingPlugins
(
id
,
this
.
trackingUriPlugins
)
;
if
(
toFetch
!=
null
)
{
ProxyUtils
.
sendRedirect
(
req
,
resp
,
toFetch
.
toString
(
)
)
;
return
;
}
notFound
(
resp
,
+
appId
+
+
)
;
return
;
}
URI
trackingUri
=
getTrackingUri
(
req
,
resp
,
id
,
applicationReport
.
getOriginalTrackingUrl
(
)
,
fetchedAppReport
.
getAppReportSource
(
)
)
;
if
(
trackingUri
==
null
)
{
protected
Set
<
String
>
getProxyAddresses
(
)
throws
ServletException
{
long
now
=
Time
.
monotonicNow
(
)
;
synchronized
(
this
)
{
if
(
proxyAddresses
==
null
||
(
lastUpdate
+
updateInterval
)
<=
now
)
{
proxyAddresses
=
new
HashSet
<
>
(
)
;
for
(
String
proxyHost
:
proxyHosts
)
{
try
{
for
(
InetAddress
add
:
InetAddress
.
getAllByName
(
proxyHost
)
)
{
@
Override
public
void
doFilter
(
ServletRequest
req
,
ServletResponse
resp
,
FilterChain
chain
)
throws
IOException
,
ServletException
{
ProxyUtils
.
rejectNonHttpRequests
(
req
)
;
HttpServletRequest
httpReq
=
(
HttpServletRequest
)
req
;
HttpServletResponse
httpResp
=
(
HttpServletResponse
)
resp
;
insertPoint
+=
PROXY_PATH
.
length
(
)
;
redirect
.
insert
(
insertPoint
,
)
;
}
String
queryString
=
httpReq
.
getQueryString
(
)
;
if
(
queryString
!=
null
&&
!
queryString
.
isEmpty
(
)
)
{
redirect
.
append
(
)
;
redirect
.
append
(
queryString
)
;
}
ProxyUtils
.
sendRedirect
(
httpReq
,
httpResp
,
redirect
.
toString
(
)
)
;
}
else
{
String
user
=
null
;
if
(
httpReq
.
getCookies
(
)
!=
null
)
{
for
(
Cookie
c
:
httpReq
.
getCookies
(
)
)
{
if
(
WebAppProxyServlet
.
PROXY_USER_COOKIE_NAME
.
equals
(
c
.
getName
(
)
)
)
{
user
=
c
.
getValue
(
)
;
break
;
}
}
}
if
(
user
==
null
)
{
@
BeforeClass
public
static
void
start
(
)
throws
Exception
{
server
=
new
Server
(
0
)
;
(
(
QueuedThreadPool
)
server
.
getThreadPool
(
)
)
.
setMaxThreads
(
20
)
;
ServletContextHandler
context
=
new
ServletContextHandler
(
)
;
context
.
setContextPath
(
)
;
server
.
setHandler
(
context
)
;
context
.
addServlet
(
new
ServletHolder
(
TestServlet
.
class
)
,
)
;
(
(
ServerConnector
)
server
.
getConnectors
(
)
[
0
]
)
.
setHost
(
)
;
server
.
start
(
)
;
originalPort
=
(
(
ServerConnector
)
server
.
getConnectors
(
)
[
0
]
)
.
getLocalPort
(
)
;
proxyConn
=
(
HttpURLConnection
)
url
.
openConnection
(
)
;
proxyConn
.
connect
(
)
;
assertEquals
(
HttpURLConnection
.
HTTP_OK
,
proxyConn
.
getResponseCode
(
)
)
;
String
s
=
readInputStream
(
proxyConn
.
getInputStream
(
)
)
;
assertTrue
(
s
.
contains
(
)
)
;
assertTrue
(
s
.
contains
(
)
)
;
appReportFetcher
.
answer
=
3
;
proxyConn
=
(
HttpURLConnection
)
url
.
openConnection
(
)
;
proxyConn
.
setRequestProperty
(
,
)
;
proxyConn
.
connect
(
)
;
assertEquals
(
HttpURLConnection
.
HTTP_OK
,
proxyConn
.
getResponseCode
(
)
)
;
appReportFetcher
.
answer
=
5
;
URL
clientUrl
=
new
URL
(
+
proxyPort
+
)
;
proxyConn
=
(
HttpURLConnection
)
clientUrl
.
openConnection
(
)
;
proxyConn
.
connect
(
)
;
